{"question":"What are the environmental benefits of horse-powered farming versus tractors, and what specific tractor features should one consider for sustainable small-scale agriculture?","answer":"Horse-powered farming offers several environmental benefits compared to tractors: horses are fueled by grass and hay rather than consuming expensive gasoline, they produce organic fertilizer, and they can self-reproduce, unlike tractors which have costly fuel consumption and cause harmful pollution. For sustainable small-scale agriculture with tractors, key features to consider include: live hydraulics and power take-off (PTO), a three-point hitch for equipment compatibility, two sets of hydraulic outlets for modern farm machines, a diesel engine (preferable to gasoline), and ideally four-wheel drive for difficult terrain conditions. A tractor with 45-75 horsepower is typically suitable, and it should have less than 5,000 hours on the meter to avoid expensive repairs.","context":["Small Farm, Big Solution\nMother Nature has all the answers, say permaculture advocates, we just need to learn to cooperate\nBy Brian Lavendel, PhD\nSmack in the middle of the modern agricultural miracle that is America's dairyland lies a seemingly anachronistic little farm. Last spring, when nearly every other farmer in southeastern Wisconsin was maneuvering a plow-equipped tractor to put in a new crop of corn or soybeans, farmer Greg David guided Minnie and Sophie, his two Percheron draft horses, through the small fields of his 20-acre homestead, Prairie Dock Farm.\nDavid, a 48-year-old Wisconsin native, acknowledges that at first blush, it might seem easier to fire up an 85-horsepower tractor than to hitch up his team. But his decision to use the old-fashioned kind of \"horse power\" was informed by more than a quaint preference for working with animals. David is a permaculturist, and from a permaculture perspective, the benefits of using horses far outweigh a tractor's hidden costs. Factor in the combustion engine's pricey gasoline consumption and harmful pollution, permaculture advocates say, and equine power becomes very appealing: They're fueled by grass and hay, they create organic fertilizer, they even self-replicate. \"As many times as I've parked two tractors side by side,\" David says. \"I've never returned in the morning to find three tractors in the shed.\"\nThe permaculture farming methods David practices are, more than anything, a way of life. A comprehensive body of philosophical beliefs and pragmatic agricultural and design techniques, the concept of permaculture was first developed in the '70s by Australian ecologists David Holmgren and Bill Mollison.\nThe practice of permaculture takes organic farming a few steps further, partly by adding a set of ethics. Fundamentally, its adherents see individuals as a small piece of a large, interdependent world. Thus, they say, not only are we responsible for the results of our actions—such as pollution, overpopulation, and depletion of energy reserves—but also every living thing has an intrinsic value, independent of its apparent utility to humans.\nCombining knowledge from the disciplines of ecology, landscape design, environmental science and energy conservation, permaculture can be used for urban and community planning as well as for rural land use. It can help create productive ecosystems or restore degraded ones. Practitioners look for solutions among preindustrial, traditional knowledge and techniques from around the world, but they adapt them for specific local purposes. By using tools ranging from solar power and straw-bale construction to aquaponics (a system that combines fish farming with hydroponics, or growing plants without soil), permaculture seeks above all to form harmony with nature rather than control it. Or as farmer David puts it, \"I'd rather work with nature than agin' it.\"\nWith a cart on the back of his bike, David can carry a straw bale a half-mile out to the horses in no time. Natural synergy was exactly what founders Holmgren and Mollison had in mind when they started codifying \"permanent agriculture\" as part of their search for a more environmentally sustainable way to raise food. In the past 25 years, about 100,000 people in dozens of countries have learned permaculture's principles and technologies, according to Keith Johnson, editor of the Permaculture Activist. Precise North American statistics are extremely hard to come by, but it's estimated that tens of thousands already follow some \"permaculture\" practices, such as composting. In David's neck of the woods, a handful of other farmers are active practitioners. The movement continues to be most popular in less-developed regions of the world, such as Africa, Asia, and areas of Australia—partly because isolated local communities have fewer resources and tend to live closer to the land. Where soil, water, and hard-currency resources are most severely limited, permaculture is not a feel-good option, but rather a lifesaving strategy.\nWhen David and his wife Sandy, 38, bought this chunk of land along the meandering Rock River some 14 years ago, it resembled a barren weed field. \"There wasn't hardly a tree on it,\" says David, a slight, muscular man and county board member known for voicing alternative opinions on issues. Though David had always been interested in habitat restoration and wildlife, he says that at first, \"We went through lots of work to subdue Mother Nature.\" Then he picked up a book by Japanese farmer Masanobu Fukuoka, who practiced what he called \"natural farming\" back in the '20s, and became a key innovator of permaculture practices. Most important, Fukuoka demonstrated that digging, weeding, pest control, fertilizing and pruning are not necessary to grow food in abundance. The more David read, the more he was hooked.\nOne of the first times he actually applied permacultural guidelines was in choosing a site for his house. He chose a spot high above the river basin only after exploring the entire plot extensively, making a common-sense decision based on noise, views, light, wind currents, storm and drainage patterns, and animal life. He bermed the straw-bale cobb home partway into the earth to achieve a more constant interior temperature. Still under construction, his house will eventually incorporate passive solar and rainwater catchment. He planted shady, deciduous trees on the south side to cut summer sun, coniferous trees on the north side to break winter winds.\nAnother basic permaculture principle holds that species diversity, especially that of native species, increases the stability of an agricultural system, eliminating the need for pesticides to control disease and pests. Early on, David planted five acres of native prairie grasses and 8,000 trees. (Long-lived, low-maintenance, and multipurpose, trees are especially popular with permaculturalists.) These days, Prairie Dock Farm counts among its inhabitants pheasants, ducks, turkeys, garter snakes, toads, rabbits, foxes, coyotes, great horned owls, red-tailed hawks and a host of songbirds and insects. From a permaculture perspective, there are no \"pests.\" Rather, it's believed that species exist in ecosystems for a reason, and taking even one of them out will disrupt the entire system.\nEager to feed his family as well as make an adequate living, David established an organic vegetable farm and orchard that now produces everything from Jonafree and Macoun apples to Swiss chard. The abundant produce stocks the larders of a dozen local families from April to October, via a community-supported agriculture (CSA) arrangement, whereby subscribers pay annual fees that help David pay his bills. Clients also take home some of the 40 pigs and 400 or so free-range chickens he raises each year. True to permaculture's devotion to multifunction, the animals play broader roles on the farm. Pigs consume weeds as well as rotten vegetables and fruit and help turn the compost as they root through it. And chickens wander through the apple orchard helping keep bugs under control and loosening the soil, improving water and nutrient take-up by tree roots.\nTo maximize land-use efficiency, David grows plants of differing heights in a close, stair-stepping pattern, so that each plant occupies minimal space but gets sufficient sunlight, a practice permaculturalists call forest gardening. For instance, in one corner of the farm, David started by planting shrublike raspberries, then worked his way back, planting semidwarf fruit trees, which top out at 20 feet, and finally hazelnuts and pecans, which now tower above the rest. By observing growing seasons on the nearby prairie, David knows that nature works this way, too. \"In the beginning of the year, you have things at six, eight, ten inches,\" he says. \"And by the end of the season, you have stuff blooming eight or ten feet high.\"\nAmong permaculture's greatest ideals is the practice of doing less, especially when it concerns interfering with natural processes. Consider the simple genius of techniques such as sheet mulching. When starting a new garden bed, instead of digging or tilling the soil, David lets nature do the work. Starting a year early, he mows vegetation as low as possible, then covers it with cardboard or newsprint, followed by several inches of dead leaves, mulch or compost. Maintenance trucks from a nearby city regularly drop off waste leaves at Prairie Dock Farm, reducing the city's refuse problem and improving David's soil fertility at the same time. Over time, the plants underneath break down, yielding rich, loose soil in which to plant the following year. (For home gardeners, David suggests artfully arranging rocks around the perimeter of your sheet mulch, so that the area resembles a Zen rock garden.)\nFactor in a tractor's pricey gasoline consumption and harmful pollution, and equine power becomes very appealing. David has even figured out how to make his own personal fitness program serve higher permaculture goals. Instead of doing farm chores with an ATV four-wheeler, and then devoting free time to working out, David addressed both issues with one elegant solution: He equipped his mountain bike with a work trailer. \"With that cart on the back, I can carry a straw bale a half-mile out to the horses in no time flat,\" David says. Clearly tickled to have figured out a way to get exercise while doing chores, the former triathlete says the best part is: \"I have fun doing it.\"\nFull-time practitioners of permaculture such as David are probably likely to remain a small minority for the near future, at least in more developed areas of the world. But certainly many of the system's specific practices can be adapted for home and other use (see \"Rethinking the Lawn\"). Grinning at the absurdity of conventional agricultural's comparatively aggressive tactics, David says, \"Instead of trying to find ways to do this and to do that, permaculturalists find ways not to do this and not to do that.\"\nDavid's tread-lightly philosophy is shared among permaculturalists, who take seriously their roles as stewards of the land. \"This is gardening for future generations,\" says Madison, Wisconsin-based permaculture consultant and gardener Marian Farrior. \"You're not killing the soil; you're creating the soil. You're not putting chemicals into the water; you're purifying the water.\" If elements of international grassroots movements such as permaculture become mainstream, she says, there's hope for preserving species and groundwater. \"Humanity thinks we're above nature because we have these artificial environments,\" she says. \"When people learn how the nature around them works, they see how much we depend on clean water and oxygen to survive.\"\nIn the meantime, David says, practicing permaculture offers its own daily rewards. For most of the year, his wife Sandy works as an elementary school principal. \"On spring break, her colleagues go off to exotic locations down in Florida or somewhere or other,\" David says. \"But we're just happy grabbing a cup of coffee and walking through the prairie—we may see more wildlife than people who travel hundreds of miles to do it. It's very fulfilling, just an incredible feeling of contentment.\"","Selecting a Tractor for the Small Farm\nOK, you bought the farm, and you have moved onto the property, with excitement, and anticipating new activities in your farming ventures. There are a tremendous number of activities that need to be done on a small farm, regardless of the size. For the new or beginning farmer, one inevitable hurdle to overcome is how you will power your farm activities? You can farm with a number of power sources, such as walk behind small scale 2 wheel type machines, or with tractors, or you can custom hire work that needs to be done. If you decide that having a tractor fits into your operation, read on. This article will be the first in a series of how to select, acquire, operate and maintain smaller scale farm machinery. Often people ask me “What tractor should I buy?”, and “How much does a tractor cost?” As with all things agricultural, the answer is “it depends”. One of the first things that I ask a new farmer is “What do you want and need to do with a tractor, and, how much can you afford to spend?” Each activity mentioned in this article will require certain machines in addition to a tractor and will be addressed in later articles.\nThe following is a list of activities which might be occurring on a farm or homestead at any given time of the year and requiring specific tractors and machines:\nBrushhogging meadows and pastures for grazing management and to prevent the fields from growing back into forest. This is one of the most critical jobs on a small farm, as nature can be relentless in working to restore forests. If fields are let to grow for more than about three years, small trees and brush will begin to take over, and then can be extremely difficult and expensive to correct.\nHaymaking, which might be either small square bales, or large round bales of dry hay or baleage. The process can require mowing, tedding, raking, baling, wrapping high moisture bales, and hauling and unloading hay.\nTillage activities, such as plowing, disking, harrowing, planting, and of course, picking stones.\nPlanting crops such as forages, row crops such as corn and cereal grains, fruits and vegetables, and possibly food plots for wildlife.\nPulling a variety of 2 and 4 wheel wagons around on the farm for diverse activities, such as hauling crops and hay, firewood, and giving hayrides to people.\nHarvesting crops other than dry hay, such as chopping high moisture hay for haylage or greenchop, and vegetables.\nFront end loader work for snow removal, manure handling, feeding animals, moving materials around the farm, and loading and hauling hay and other crops.\nWorking in the woods skidding logs, hauling firewood, and hauling maple sap to the sugar house.\nI am going to make some assumptions before I begin to suggest which tractor might be a good choice for you. Assumption one: right now, you are going to try to do as many operations as possible on your farm with one tractor, while you’re getting started. As you build equity and experience, you may someday need another tractor or two, because as you will find out, not all tractors are suited to all purposes, but for now, we will focus on getting you this first tractor to do as many things as possible.\nAssumption two: you only have a given amount of money to spend, and that you might be looking for a good used tractor. New tractors can be prohibitively expensive; if you can afford one, good for you.\nAssumption three: you may be looking for a smaller to medium size tractor to get everything done; leaving the big tractors for those who truly need them.\nSo just what should we be looking for in a good used tractor? There are a myriad of features that we need to consider. To accomplish what we need to do, will our tractor have the features listed in Table 1?\nLive hydraulics and pto (power take off) are important; older tractors made in the forties and fifties frequently did not have “live” features, which meant if you pushed in the tractor’s clutch, the pto or hydraulics stopped working. This can be quite annoying when you are in thick crops and have to use the clutch, and have to start up again from a dead stop.\nNow that we have all of these features to think about, I will describe my version of a tractor that if I could only have one tractor, this is the type that I would look for, for my first tractor on a smaller farm.\n1. I would want a tractor made in the latter part of the twentieth century, at least from about 1970 onward. Older tractors made after World War II still abound, but lack many of the needed operational and safety features expected today. Narrow front ends from that era were very dangerous and lacked many of the safety features that we expect nowadays. These type of tractors can be very dangerous!\n2. I would prefer having a diesel engine; not many farm tractors come with gasoline engines anymore. Older tractors from the sixties and early seventies with gasoline engines can be quite aggravating to keep running smoothly on a year round basis, especially the ones made before the era of electronic ignition.\n3. It is mandatory to have a three point hitch. So much farm equipment exists which requires this feature making it an absolute requirement. Often you can purchase aftermarket three point hitch assemblies for older tractors but they tend to be expensive, awkward to use, and not very efficient.\n4. I would prefer two sets of live hydraulic outlets (this is required to run many farm machines today). If your tractor has only one set of hydraulic hoses, a second set can be added if and when your machinery needs dictate.\n5. A front end loader (often the front end loader ties up one set of hydraulic outlets leaving you only one other set to operate equipment).\n6. Not too many hours on the hour meter, preferably under 5,000 hours. Repairs and overhauls can be extremely expensive!\n7. If not a fully enclosed cab, at least a Roll Over Protective Structure (ROPS) should be mandatory, with a seat belt. A canopy is a nice addition on top of the ROPS. The ROPS is for roll over protection and can save lives, and the canopy can prevent you from baking in the sun during those hot summer days, and will keep some rain off of you.\n8. It would be nice to have four wheel drive; our long snowy winters and muddy rainy summer months, especially if the tractor has a front end loader, make you able to get into a lot of places you normally would not, and you can easily get stuck. There are two basic types of tractor operators, those who have been stuck, and those who are going to get stuck. Getting stuck and having to have the neighbors come pull you out frequently can strain neighborly relations.\n9. Live power take off. Some older obsolete tractors have “non-live” pto which makes controlling the actions of your equipment awkward and difficult.\n10. How much horse power? There are different ways to describe h.p. but for right now we can simply say that a tractor with between about 45 to 75 horsepower should fit our needs.\n11. How much will this tractor cost? Depending on the age, condition, and features present, you can expect to pay at least $5,000 up to $20,000 for a good used tractor such as I have described in this article. In general, the more features, the more costly the machine will be.\nIn future articles, I will discuss where we can find good used tractors as well as looking at some of the numerous machines we need on a farm.\nTwo very useful websites for those looking for tractors and farm machinery are tractor house and fast line. These two sites list machines for sale all over the country; looking through them will give the reader a good idea on what typical prices for machine will be."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:20a54265-5542-4cc3-b631-e65429d98d70>","<urn:uuid:8ba52b39-8789-424d-836f-1da0fbbd7384>"],"error":null}
{"question":"What are the traditional ways that collectors used to organize card collections, and how have modern digital tools changed collection management?","answer":"Traditionally, sports card collecting was simple and straightforward - collectors would organize their cards in monster boxes and nine-pocket pages, often meeting in person at local clubs to trade and discuss their collections. In contrast, modern collectors have access to digital tools that have transformed collection management. They can now use online platforms to create and share digital want lists through services like Google Docs or Dropbox, making their lists accessible anywhere. They can also connect with other collectors through social media, online forums, and blogs, enabling trading and discussion beyond geographical boundaries. However, the basic need for physical organization remains important - collectors are still advised to keep their cards properly stored and regularly maintain organization to avoid cluttered desks and overwhelming piles.","context":["By George Vrechek\nClub membership connotes selectivity and often requires the applicant’s subjection to social and financial scrutiny. Groucho Marx is attributed with making fun of the process by sending the Friars Club of Beverly Hills a telegram, “Please accept my resignation. I don’t care to belong to any club that will have me as a member.”\nHowever, card collectors clubs have never involved prospective members jumping through hoops for admission. If you want to join, you are in – once your check clears. Clubs began sprouting in the early 1970s as collectors started to find one another and were anxious to get together to whittle down their want-lists. Collectors met in living rooms and basements, moved on to restaurants, and graduated to VFW Halls and church basements. Hobby publications in the 1970s frequently reported the activities of local clubs as they formed and started shows.\nAs groups grew, they found the need to collect dues and fees to defray costs like room rentals, table rentals, publicity, newsletters and autograph guests. The clubs usually operated on a break-even concept; most organized themselves as not-for-profit entities. Not-for-profit clubs got to be involved in the exciting process of applying for not-for-profit tax status, filing required annual reports and returns, and complying with requirements for sales taxes.\nSome of the clubs morphed into for-profit promoter shows, others went out of existence, and a select few have kept at it, providing a different experience for local collectors.\nMore than one of the club directors I contacted mentioned something like, “As with most organizations, much is done by few.” While few areas of the country are covered by collectors clubs today, their very existence raises the question of whether such clubs would be successful again in other regions. I found four organizations identifying themselves as long-standing collectors clubs that are not online groups. If readers are involved in any other not-for-profit sports collectors clubs, please let us know and we’ll include your information in a future article.\nTWIN CITIES / MINNESOTA; tcscc.org\nThe Twin Cities Sports Collectors Club was started in 1975 by Larry Loeschen, Bud Tompkins, Lee Temanson and Dennis Moriarity – all Minneapolis/St. Paul area residents. Dave Bonde is the current club president. Former SCD columnist Temanson contacted the other three founders after finding their names in hobby publications like SCD and The Trader Speaks.\nTemanson’s history in the hobby\nTemanson, 75, started collecting in 1949 and never stopped. In the 1970s, he wrote a column called “Out There” for SCD about new card offerings. He also found time to co-own with Dave Nelsen the Great American Collectors Store outside Chicago for several years.\nTemanson had the ambitious goal of collecting one of every baseball card. He started buying T206s for 40 cents each in the 1960s and it seemed like anything was possible. When Topps came out with glossy coated “regular” cards described as the Tiffany set in 1987, Temanson finally threw in the towel on the one-of-every-card idea.\nThe four founding fathers first met at a Perkins Restaurant in 1975. Apparently any wives involved figured out very early in the process that it would not be a good idea to invite a mob of hungry collectors into their dining rooms or basements. Larry Loeschen knew a merchant at Apache Mall and the group moved their gatherings to the mall. The original arrangement was that you could join the club for $10 per year, which would entitle you to having a table at the mall for $1 per show. Of course, you could wander into the show at the mall for free. Membership quickly grew to more than 100 collectors with shows every month. Dennis Moriarity edited a newsletter. Dave Bonde contributed articles on autographs.\nAfter a 1987 tornado hit the Apache Mall, the club moved to other locations, eventually settling in at Valley West Mall in Bloomington. Several years ago the club direction had to be re-focused, and Lee Temanson returned to help operate the club along with Tom Dolan. Club President Dave Bonde has continued the tradition.\nDave Bonde’s report of current activity\nThere are usually 45 dealers or collectors with tables each month, and 700 people may spend time at the show. About 225 members pay dues of $15 for one year, $25 for two years or $60 for lifetime. Members get a reduced table rental rate at the monthly shows, reduced autograph fees, a fantasy baseball contest and receive a digital newsletter. There are other contests where members can win club bucks redeemable for anything at dealers’ tables.\nFour times a year club members are entitled to put cards or any collecting memorabilia in an oral auction run during the shows. After the auction, club members enjoy free pizza and pop. Bonde added, “Once a year, the club has an annual banquet at a local restaurant. Members can purchase tickets for a greatly reduced price, with the club picking up the remainder. A guest speaker is generally there, and door prizes are also given.”\nThe bulk of club expenses are paid by table rentals. Autograph guests will be brought in for modest fees. The club is proud of being a not-for-profit entity and having donated more than $10,000 over the last few years to local charities, nominated by members. Recently, former Twins manager Tom Kelly appeared and helped raise more than $3,000 for the Twins Community Fund.\nThe oldest club appears to be the Washington State Sports Collectors Association founded in 1973 by 10 charter members: Frank Caruso, Russ Dille, Ralph Winnie, Steve Mitchell, Ted Baker, John Eichmann, Dave Voorhees, Bob Senior, Larry Moe and Bud Baker. The first Convention was held on Aug. 25, 1973, at the Washington Athletic Club in Seattle.\nMonthly meetings are truly meetings rather than shows. Members gather at Razzis Pizzeria in Seattle; a trading session begins at 7 p.m., followed by a meeting at 7:30. They work toward putting on two shows a year at Meridian Park School in Shoreline, Wash. Their summer show has been running for 41 years. Admission is $2 and little kids are free. There are five officers, with Mark Clatterbuck as the president.\nAnnual dues of $20 entitle the 55 members to reduced table rates at the two annual shows, where about 30 people will take tables.\nClub treasurer Dave Peterson reports, “Our events are put on by collectors for collectors. We have a core group of dedicated members and dealers who help set up, break down and run the conventions. We do not allow Beanie Babies, comics, Pokémon, coins or Magic cards. This sets us apart from other shows . . . For a number of people, dealers and collectors, this is the only show they will do each year.”\nCollector Dave Fallen of Arlington, Wash., added his thoughts about the great shows: “One thing that is pretty special is that several local collectors have tables to sell their dupes or trade. They are there to talk and enjoy others in the hobby. It is the only place I can network with Seattle Rainiers and other PCL collectors. I really look forward to this show.”\nThe Wisconsin Sports Collectors Association has been gathering since 1975. There are monthly shows held at Gonzaga Hall in Milwaukee. Annual dues are $8, which permits free admission for 250 families during the year. Walk-in collectors pay $1 per show. There are around 50 people who have tables at each show.\nThe Kilps family\nThe late LeRoy Kilps helped start the club, and it is now run by his son Keith. Mike Rodell is the vice president, and Bob Ruesch is a director. Keith Kilps commented on the long-running status of the club and the family aspect: “Many adults who are now members were kids who came with their dads and now they bring their kids! It has been a great way to keep kids involved in a very expensive hobby nowadays. We try to come up with activities to keep the kids interested in the hobby and keep them coming back. There is a kids’ auction at noon, where the kids pay a quarter and can pick a prize. Many of the items that the kids choose are donated by the dealers. In our monthly newsletter, I add in a word search or a small activity for the kids to complete and bring to the next show for a prize.”\nKilps added, “When my dad started the club, along with three others, the thought was, ‘Hey we are collecting and trading cards in each other’s basements, let’s take it to a hall and make something of this.’ Any revenues in excess of expenses we have as a club are donated to various charities and functions each year. We sponsor several Relay For Life outings for a team, Tyler’s Crusaders, headed by one of our members in Kenosha, Wis. We also work with charities, including the Brewers Community Foundation, Juveniles Diabetes and March of Dimes.”\nMembers and volunteers\n“We have been thankful to keep a steady family membership list and see many returning faces at each show,” Kilps stated. “We do not have the funds to bring in high-priced autograph guests, however, we are able to bring in retired Braves, Brewers, Bucks, etc., at low autograph prices and in an environment where they can take time with fans and tell great stories. This year we have had Johnny Logan, Felix Mantilla and Sam Williams as guests.\nThe volunteer side is very positive as well. We are very blessed to have a big group that helps each month. It’s very much a family function. My mom runs the concession stand, my aunt and uncle run the admission door and my nephew is around to carry things and help out. This is how it has always been throughout the years, so taking over upon my dad’s sudden passing three years ago, I was very familiar with the way things operated.\n“Mike (Rodell) and I are also volunteers in our roles for the club. We have an auction during each show from 10 a.m.-12 p.m., and it is run completely by our volunteers. A few of them have been with the club for many years; some even came to shows when they were kids. There is definitely no shortage of people willing to help.”\nCollector Ken Goetsch of Watertown, Wis., gets to their shows frequently and describes them as very well run. “It’s really the only vintage show in Wisconsin,” he said.\nThe Greater Boston Sports Collectors Club was established in 1985. Of the 11 founding fathers, three are still active as board members. Annual $12 dues provide members with free admission to monthly meetings and an annual convention, as well as buffet dinners twice yearly.\nThe not-for-profit club has six directors and more than 200 members. Meetings are held at the Woburn Elks in Woburn, Mass. According to club treasurer Steve Knapp, the monthly meetings usually include a former athlete from a Boston team speaking to the group, followed by questions and answers and autographs. A few members set up tables at the meetings, but it’s not a big part of the evening. They also have an auction, which gives club members another venue to sell things.\nKnapp joined in 1994 and commented on the volunteer aspect of the club: “We have volunteers manning our club tables, which provide volunteers with an opportunity to spend some time with former major leaguers, and others sell admission tickets. We have taken to giving volunteers $25 restaurant gift cards the last couple of years, but I don’t really think that it’s a major reason people come to help. We have pretty much the same group of volunteers, year in and year out.”\nKnapp pointed out the administrative requirements for their club. “Our six-member board has clearly defined roles and complement each other very well. As treasurer, I keep the books, provide quarterly financial reports, submit documents to our tax accountants and pay all the bills. Our publicity person does all our advertising and printing and keeps our membership and mailing lists. Our vice president arranges for guests for our monthly meetings. We meet once a month as a board and generally work very well together.”\nSOUTHERN CALIFORNIA; socalsportscollectors.com\nThere were several clubs around Los Angeles in the 1970s which have all ceased operating. However, Jason Miller, Anthony Nex and Adam Warshaw thought it would be great to revive the concept. They have a name: Southern California Sports Collectors and a website; they are just starting to organize a club.\nFirst show this year\nMiller reported, “We just had our first show at Veterans Memorial Auditorium in Culver City. In some ways, I guess the show was a club of sorts. Of the 37 vendors, 50-60 percent were not dealers, but collectors setting up and selling dupes and even open to trades.”\nCollector John Stamper of West Hills, Calif., attended the show and added, “I spent six hours looking through cards and talking to dealers and collectors. It was a really good show.”\nMiller added, “We had some vision of six yearly meet-ups where somebody presents a set with intricacies, scarcities and things to watch out for in terms of fakes. Someone could take the baton every couple of months and then it would devolve into eating pizza and just generally talking about cards for an hour or so. If the topic of the month was E93, perhaps club members would bring their E93s, someone would talk about them and then maybe people would do a little trading.”\nLocal clubs died out\nAdam Warshaw was involved with the West Coast Card Club in the San Fernando Valley and used to take a table at monthly meetings in a social hall in Northridge.\nWarshaw recalled, “The great thing about the club-run events was that they were simple, cheap and quick; it was literally $20 for a table for a Thursday evening, which was something that an average collector could readily handle. The clubs all collapsed with the advent of card stores and professionally promoted conventions. Later, eBay and online auctions killed even the stores and shows. As a result, an area like Los Angeles went from having numerous card clubs with various events, to having several card shows a week, to having only one or two professionally promoted shows a year. For the last few years there has been almost nothing. Even those few shows were typically three or four-day events with correspondingly high table fees that made it impossible for average collectors to participate as dealers.”\nConcept for the future\nWarshaw continued, “What we hope to revive is the no-nonsense, one-day event where the table fees are low enough for weekend warriors to come out and have some fun and the focus is on the cards and memorabilia, not on autograph guests, museum-like displays and corporate giveaways.\n“Going forward (the next show is planned for October), we foresee having not only twice-yearly one-day shows, but perhaps smaller non-public club meetings in between so that people can hang out and ‘talk cards.’ We are also hoping to have informal auctions at the shows and meetings.”\nThoughts on clubs\nWhen I first found other collectors in 1981, I learned of a Chicagoland Sports Collectors Association “meeting.” Don Steinbach, Pat Quinn, Jim Rowe, Bill Loughman, Richard Egan and Richard Boe were among the early directors of the Chicago club. I arrived at the Hillside Holiday Inn expecting to find something that looked like a meeting. It looked to me like a flea market, which was fine for being able to buy cards, but I didn’t have a sense that there was any “clubbiness” involved.\nOf course, I had missed the previous 10 years of club formation and didn’t have any perspective as to how the hobby was evolving. Like some of the club founders featured in this article, I envisioned bringing in a box of duplicates and swapping them with fellow collectors to fill my wants as we chatted about collecting, something like we did as kids, but without the gum.\nIf I were still trying to finish a 1974 Topps set, for example, and other collectors were doing the same, this sitting down with a box of your duplicates might be productive.\nHowever, when your want-list consists of obscure and expensive cards, you aren’t likely to make a dent in your want-list by swapping with your local collectors each month. But if there were a way of having a table at a minimal cost, it would be an opportunity to at least sell off duplicates to defray the cost of buying cards you needed.\nDespite the modest chance of finding cards I need at a club meeting, I have always found it fun to meet with other collectors. I might learn something new or start a new set, but at a minimum I get a sense of camaraderie based on the hobby of sports collecting. We can’t all be nuts. Best wishes to the continued success of local collectors clubs.\nGeorge Vrechek is a freelance contributor to Sports Collectors Digest and can be contacted at email@example.com.","Collecting sports cards used to be simple. Every year, a handful of sets made their way to store shelves. Inserts were simple things like stickers or other one-per-pack premiums. Defining and chasing rookie cards wasn't a complicated process. Autographs were things you got at the park or arena, not in packs. Jersey were clothes and nothing more. They came with gum.\nYou get the point.\nIf you collect modern cards, you probably also know that days like these are, for the most part, gone. And that's not a judgement call. It's fact. And while the options available to collectors today are far more numerous, the hobby can also be daunting. Not only to new and returning collectors, but long-time hobbyists as well.\nIt's easy to lose your way amidst an ever accumulating stack of monster boxes and cluttered desks. The challenging of getting everything becomes too daunting. For some, there comes a breaking point where it becomes too much and they walk away. But it doesn't have to be this way. Unless sports cards are a significant part of your income, collecting should be fun. If it's not, something's wrong.\nWhether you're new to the hobby, a returning collector or a veteran, here are ten easy steps to building a better sports card collection.\n1. Find a Focus\nForget trying to get everything. You can't do it. So rather than ripping boxes and cases of every new product and tossing everything aside, find a focus for your sports card collection. Pick a theme or two and run with it. The options are endless and can be catered to your personal tastes. Some choose to build sets while others go after their favorite team. There are player collectors, autograph collectors and rookie card collectors. Many collectors are loyal to a specific brand. Others prospect with visions of future returns. There is no wrong focus.\nAnd don't be afraid to change your focus. As new sets come out, you may find you like a particular style of card. Or, you may choose to move on if your favorite player is traded or a set loses its appeal.\nBy getting a focus, you give yourself direction. It gives purpose to your collection. Without one, you'll likely find yourself throwing money around at random cards. And with today's prices, a lot of money can disappear very quickly.\n2. Set a Budget (and Stick to It)\nNext to finding a focus, setting a budget is probably the most important part of building your collection. It's easy to get carried away. Too easy. Deals pop up, rare cards surface, new sets come out -- all these vie for your extra cash. And if you're not careful and disciplined, you may find yourself eating more Kraft Dinner than you usually like.\nCards should always come from the money that's leftover after all the bills are paid. It sounds like a simple concept, but for many it's a tough proposition to keep.\nOnce you've decided how much you've got to spend on cards each month, stick to it. You might even want to create a simple spreadsheet or tally of your expenses.\nLook for ways to make you money go further. By shopping on eBay, you can often set your own price easily and without hassle. You can often negotiate at your local shop, but online auctions are a convenient way to hunt for bargains.\nAnd if some great deals pop up all at once? Pass some of them over. Or sell some of your collection. List on eBay. Set up at a local card show. Look at some other online sites. It has never been easier for collectors to turn their unwanted and extra cards into cash that you can then use to build your collection.\n3. Make a Wantlist\nOnce you've got your focus, compile a list of what you're looking for and type it out. Writing it on a napkin works, but there's a good chance your 5-year-old will come along and wipe the peanut butter and jelly off his face with it.\nTo take your wantlist a step further, upload it with something like Google Docs or onto Dropbox so it's with you anywhere you go. This is very handy when you stumble upon a shop while away on business or you hit up your local show.\nJust remember to update your wantlist as soon as it changes. It's easy to forget to cross something off once you've filed it away in a binder or box.\n4. Target the Whale\nWhatever your focus and no matter your budget, there's probably something rare and/or pricey that you've always wanted. Right now, it might be out of reach. But that doesn't mean you can't plan for it. Set aside a little bit of your card budget every month for a \"Whale Fund\" so that the next time one pops up, you'll be ready. Even if you're on an Opening Day budget, you can still own some Exquisite pieces.\nA big part of the thrill of collecting is the chase. It's great to have several inexpensive and easy-to-find pieces in your sites, but landing that cornerstone piece is one of the biggest rushes collectors can have.\n5. Make It a Family Affair\nAnything you can do with your family makes it that much more enjoyable. If the hobby is going to grow and thrive, it needs new collectors. Chances are, if your kids see that you're interested in something, they're likely to become interested as well. That might mean some compromises. Your daughter may not like baseball, but My Little Pony or Star Wars might be right up her alley. You might pledge your allegiance to the Yankees and your son might think Mr. Met is funny. Still, collecting cards is best shared.\nAnd don't think that it's just your kids you might want to get involved. You might even want to look for ways of getting your spouse or partner on board. My wife has always tolerated my collecting but she's never shown any interest. That was until we opened a box of Game of Thrones together. Like your kids, look for alternate options that might get them interested in busting packs with you.\n6. Try Something New\nYes, I know I said finding a focus was one of the most important things you could do with your collection. But you still need to try new things. That doesn't meant break a box of every new product. Rather, try some packs here and there. Trade for some cards from a sport you enjoy, but don't usually collect. Bounce back from a depressing episode of The Walking Dead with some cards from the show.\nExpanding your collecting horizons without going wild, can help your collection evolve. You may find you prefer a new brand. Or it may help solidify your current collecting direction.\n7. Get Social\nWhile there may not be as many collectors as there once was, it's hardly a ghost town either. Collecting is so much more enjoyable if you have someone to talk about it with. Even if you failed at getting your family on board, there's no excuse for not being social with the hobby. Join a forum, get on Twitter, start a blog. If you're a sports card hermit, things will get very boring.\nWhen many of my friends stopped collecting, I found a huge community online. Not only did it make collecting easier, it rejuvenated me and got me more excited about collecting than ever.\nWith the expansion of social media, getting connected has never been easier.\n8. Start Trading\nNow that you're connected with other collectors, you can leverage those relationships into getting some of the cards you want. By being social, you can also usually find someone who is looking for your extra cards.\nTrading is one of the purest forms of collecting. For many, the evolution of the hobby has taken it back to where it began. There are literally thousands of collectors online who swap cards. Some demand structured deals where every penny of \"book value\" is accounted for. Others are happy to send their friends things they need without any strings attached. Most are somewhere in between.\nNo matter how you trade, it's a fun, easy way to clear some clutter and knock things off of your wantlists.\n9. Clear Off Your Desk and Get Organized\nIt's depressing when you go to work and you find your desk covered with papers and other clutter. Likewise, it's tough to get excited about collecting when you're surrounded by stacks and stacks of unorganized cards.\nSo take a Saturday, stock up on supplies, plug in a movie and start getting organized. There is no right or wrong way to do it, just as long as the end result means everything has a place and you've got some piece of mind. That said, a couple of monster boxes and boxes of nine-pocket pages go a long way to getting things stored properly.\nAnd don't forget, once you're organized, don't the the piles creep up on you. It'll be the same problem all over again. Take a few minutes every week and make sure everything finds a place.\n10. Find a Local Shop\nFor card collectors, the local hobby shop is, ideally, the place where everybody knows your name. In a perfect world, it's a place where you can go talk about the hobby, debate the standings and buy some cards. A dwindling number of quality card stores makes that a lot tougher. But they're out there. And it's important you find one, even if it means a bit of a drive.\nFor some, this might mean heading to the next town over. That's fine. Just don't go as often as you might if they were on the block.\nShopping online is easy, convenient and, in most cases, offers both the best selection and prices. But you can't put a price tag on the card shop experience. It's the perfect way to try out new stuff a little bit at a time, socialize with other collectors and hunt for whales.\nAnd if the closest shop isn't any good? Keep on driving until you find one. Get the family together and make a road trip out of it.\nBuilding a better sports card collection is a lot of work. But the underlying theme is making it fun and enjoyable. For most of us, it's a hobby. If collecting is causing you stress, step back and look at why. All these ideas are easy and should bring back the fun.\nRelated Topics: How To: General"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:d7e1ebd5-a25e-47c3-a4ff-7f678ffc1b70>","<urn:uuid:ddbca7a4-ffbd-4120-9073-080ef16e921e>"],"error":null}
{"question":"What genetic evidence suggests that Native Americans descended from multiple founding populations rather than a single migration?","answer":"Genetic studies show that some Amazonian Native Americans descend partly from a founding population that carried ancestry more closely related to indigenous Australians, New Guineans and Andaman Islanders than to present-day Eurasians or Native Americans. This signature is not present to the same extent in Northern and Central Americans or in a 12,600-year-old Clovis genome, indicating a more diverse set of founding populations than previously thought.","context":["Amerindians Are Even More Genetically Diverse and Older Than You Thought\nScience DOI: 10.1126/science.aab3884\nGenomic evidence for the Pleistocene and recent population history of Native Americans\nRaghavan, Maanasa, Matthias Steinrücken, Kelley Harris, Stephan Schiffels, Simon Rasmussen, Michael DeGiorgio, Anders Albrechtsen, …Eske Willerslev.\nHow and when the Americas were populated remains contentious. Using ancient and modern genome-wide data, we find that the ancestors of all present-day Native Americans, including Athabascans and Amerindians, entered the Americas as a single migration wave from Siberia no earlier than 23 thousand years ago (KYA), and after no more than 8,000-year isolation period in Beringia. Following their arrival to the Americas, ancestral Native Americans diversified into two basal genetic branches around 13 KYA, one that is now dispersed across North and South America and the other is restricted to North America. Subsequent gene flow resulted in some Native Americans sharing ancestry with present-day East Asians (including Siberians) and, more distantly, Australo-Melanesians. Putative ‘Paleoamerican’ relict populations, including the historical Mexican Pericúes and South American Fuego-Patagonians, are not directly related to modern Australo-Melanesians as suggested by the Paleoamerican Model.\nNature (2015) doi:10.1038/nature14895\nGenetic evidence for two founding populations of the Americas\nSkoglund, Pontus, Swapan Mallick, Maria C. Bortolini, Niru Channagiri, Tabita Hunemeier, Maria L. Petzl-Erler, Francisco M. Salzano, Nick Patterson, and David Reich.\nGenetic studies have consistently indicated a single common origin of Native American groups from Central and South America. However, some morphological studies have suggested a more complex picture, whereby the northeast Asian affinities of present-day Native Americans contrast with a distinctive morphology seen in some of the earliest American skeletons, which share traits with present-day Australasians (indigenous groups in Australia, Melanesia, and island Southeast Asia). Here we analyse genome-wide data to show that some Amazonian Native Americans descend partly from a Native American founding population that carried ancestry more closely related to indigenous Australians, New Guineans and Andaman Islanders than to any present-day Eurasians or Native Americans. This signature is not present to the same extent, or at all, in present-day Northern and Central Americans or in a ~12,600-year-old Clovis-associated genome, suggesting a more diverse set of founding populations of the Americas than previously accepted.\nWhole-genome and ancient DNA studies continue to topple conventional paradigms, befuddle academic researchers and fulfill out-of-America predictions. The two brand new studies by teams from the Reich lab at Harvard and the Willerslev lab at the University of Copenhagen postulate no fewer than three ancestry components in Amerindians related to three major population clusters in the Old World. Just 10 years ago the opinion was split between those scholars who imagined genetic and cultural continuity between Amerindians and East Asians and the peopling of the Americas at 15,000 years ago and those who postulated discontinuity from the ancestors of modern East Asians and the isolation of proto-Amerindians for some 15,000 years in Beringia. The latter model known as the “Beringian Standstill Hypothesis” (Tamm et al. 2007) sought to explain the presence of unique genetic signatures in modern Amerindians which required time and geographic isolation from the ancestral East Asian pool to accrue and stabilize. The Yana Rhinoceros Horn site located in close proximity to the East Siberian Sea shore and dated at 30,000 YBP provided the material evidence and the lowest chronological horizon for a proto-Amerindian source population presumably locked in a northern refugium during the LGM times and waiting for the ice shield to melt before spreading into the New World. While the two mental models differed in the extent to which they allowed for discontinuity between East Asians and Amerindians, they both imagined a homogeneous East Asian gene pool yielding an even more homogeneous Amerindian population.\nWith the sequencing of the DNA from the Mal’ta boy located in South Siberia and dated at 24,000 YBP, this thinking proved to be false. The Mal’ta site itself was located thousands of miles south of the mouth of the Yana River. More importantly, its DNA showed affinity to modern Amerindians and West Eurasians to the exclusion of modern East Asians (Raghavan et al. 2014). So, during the LGM times distinct Amerindian ancestry was already detectable in a geographically northeast Asian sample, while East Asian ancestry was not. Contrary to the prediction of both the East Asian Continuity and the Beringian Standstill models, a distinctive Amerindian genetic signature predated a distinctive East Asian genetic signature in the heart of Siberia and its closest affinities were with modern Europeans and not East Asians. Amerindians turned out to be older, more heterogeneous and less East Asian than everybody thought. The academic community responded to this puzzling finding by postulating an extinct “Ancient Northern European” (ANE) population that admixed with an East Asian population to generate ancestors of modern Amerindians. The ANE signature was later also found west of the Urals in ancient Kostenki DNA at 36,000 YBP (Seguin-Orlando et al. 2014; also covered here) as well as across a wide range of modern European, Middle Eastern and Caucasus populations including the putative Yamnaya ancestors of Indo-European speakers (Haak et al. 2015). But the best living example of that ancient Eurasian population continue to be Amerindians. The genetic impact of ANE on West Eurasians is so significant that all of the ancient samples discovered in Europe, from La Brana foragers to Stuttgart farmers, as well as all of the modern European populations score closer to modern Amerindians than to modern East Asians or Australo-Melanesians.\nUp until now, Australo-Melanesians have never been a factor in the population genetic theories of the peopling of the Americas. Whether imagined as the earliest and sovereign wave of modern humans emanating from Africa along a “southern” migratory route or an early offshoot of an East Asian population, Australo-Melanesians have always been considered too old in terms of their divergence time and too southern in their geographic distribution to play a role in the peopling of the Americas via the Bering Strait bridge. Linguists, ethnologists, folklorists and ethnomusicologists, on the other hand, have long pointed out suggestive parallels between grammatical traits, mythological motifs, rituals and musical styles and instruments between some New World regions (especially, Amazonia but also North America) and the Sahul (see more here).\nPhysical anthropologists, too, advanced an argument that the earliest craniological material from the Americas is closer to Australo-Melanesians than to modern Amerindians or East Asians. This observation was so consistent across their Paleoindian sample that it warranted a formalization into a theory of two large-scale migrations to the Americas: the first one followed a coastal route and brought populations related to Australo-Melanesians from the deep south of the Circumpacific region, while the second one was derived from an inland source in northeast Asia (Neves & Hubbe 2005). Historic Fuegians and Pericues from Baha California were presented as the only surviving examples of the ancient Australo-Melanesian craniological pattern in the Americas (Gonzalez-Jose et al. 2003). However, ancient mtDNA extracted from Paleoindian skulls has invariably showed close affinities to all of modern Amerindians, thus undermining claims for a two-migration scenario (see, e.g., Chatters et al. 2014). Raghavan et al. (2015) re-examined the Paleoindian, Fuegian and Pericue craniological dataset and rejected the original conclusion:\n“The results of analyses based on craniometric data are, thus, highly sensitive to sample structure and the statistical approach and data filtering used. Our morphometric analyses suggest that these ancient samples are not true relicts of a distinct migration, as claimed, and hence do not support the Paleoamerican model.”\nBut while the Australo-Melanesian hypothesis cannot be defended using craniological material, Skoglund et al. (2015) and Raghavan et al. (2015) have now furnished whole-genome evidence for a distinct Australo-Melanesian, or Oceanic ancestry in modern Amerindians. (Note that, ironically, the craniologists failed to see the Australo-Melanesian signature in modern Amerindian skulls.) Raghavan et al. (2015) report results from their in-depth D statistic analysis of shared drift between various populations:\n“We found that some American populations, including the Aleutian Islanders, Surui, and Athabascans are closer to Australo-Melanesians compared to other Native Americans, such as North American Ojibwa, Cree and Algonquin, and the South American Purepecha, Arhuaco and Wayuu (fig. S10). The Surui are, in fact, one of closest Native American populations to East Asians and Australo-Melanesians, the latter including Papuans, non-Papuan Melanesians, Solomon Islanders, and South East Asian hunter-gatherers such as Aeta.”\nTwo examples from their Fig. S10 can be seen below.\nWhat it shows is that some Amerindian populations markedly shift in the direction of Papuans or Aeta compared to the majority of Amerindians. Importantly, this shift affects some of the same populations (e.g., Aleutians and Saqqaq) that also shift toward Han and Koryaks but, remarkably, the Australo-Melanesian pull is stronger than the East Asian pull! (see below from Fig. S10 in Raghavan et al. 2015, where negative D values are lower when Han and Koryaks are compared to Amerindians than when Papuans and Aeta are compared to them).\nImportantly, the Australo-Melanesian shift is displayed by populations from both South America and North America, so it’s a low-frequency but pan-American phenomenon. Raghavan et al. (2013) showed that some North American populations are more East Asian shifted and less ANE-shifted than Central and South American populations. Now, they present evidence that those populations are more Australo-Melanesian-shifted than East Asian-shifted.\nWorking with a different sample, Skoglund et al. (2015) echo Raghavan et al. (2015) findings. They write:\n“Andamanese Onge, Papuans, New Guineans, indigenous Australians and Mamanwa Negritos from the Philippines all share significantly more derived alleles with the Amazonians (4.6 . Z . 3.0 standard errors (s.e.) from zero). No population shares significantly more derived alleles with the Mesoamericans than with the Amazonians.”\nExtended Data Table 2 in Skoglund et al. (2015) (see below) shows this excess of Australo-Melanesian alleles (Z values positive) in Amazonians (Surui, Karitiana and Xavante) compared to Central American Indians (as proxies for other Amerindians including the 12,000-year-old Anzick sample from Montana).\nInterestingly, the “Australo-Melanesian” footprint in the Old World is geographically broad spanning South Asia, Southeast Asia as well as the Sahul. It’s clearly the pre-Mongoloid and pre-Austronesian “substrate” in the eastern provinces of the Old World.\nPredictably, Skoglund et al. (2015) and Raghavan et al. (2015) struggled to interpret these results. Raghavan et al. (2015) concluded:\n“The data presented here are consistent with a single initial migration of all Native Americans and with later gene flow from sources related to East Asians and, more distantly [this statement is contradicted by their own data, as I showed above. – G.D.], Australo-Melanesians.”\nBut the conclusion from Skoglund et al (2015) is radically different:\n“[O]ur results suggest that the genetic ancestry of Native Americans from Central and South America cannot be due to a single pulse of migration south of the Late Pleistocene ice sheets from a homogenous source population, and instead must reflect at least two streams of migration or alternatively a long drawn out period of gene flow from a structured Beringian or Northeast Asian source. The arrival of Population Y ancestry in the Americas must in any scenario have been ancient: while Population Y shows a distant genetic affinity to Andamanese, Australian and New Guinean populations, it is not particularly closely related to any of them, suggesting that the source of population Y in Eurasia no longer exists…”\nSkoglund et al. (2015) seem to be more reasonable in their judgment. Their phylogenetic tree with admixture arrows (see on the left) captures well the ever-more-complex prehistory of the New World.\nIt used to be that Amerindians were depicted as a simple offshoot of a Han-like population (see the tree on the right, from McEvoy et al. 2010, Fig. 1). The branch connecting them to East Asians has always been long but it was interpreted as the effect of a bottleneck induced by the Beringian Standstill and subsequent further isolation in a newly-colonized continent. Now, Amerindian ancestry, whether northern or southern, spans the whole gamut of extra-African genetic diversity. In addition, the Australo-Melanesian link in the New World is clearly connected to the discovery of a stronger Denisovan signal in Amerindians vs. East Asians (Qin & Stoneking 2015), which suggests that Amerindians must be older than the LGM time frame entertained by Raghavan et al. (2015).\nWhile Raghavan et al. (2015) dismiss the Australo-Melanesian hypothesis advanced by craniologists, Skoglund et al. hope that direct DNA analysis of the Paleoindian material from Amazonia will yield support to their finding. Importantly, just like the mythical “Ancient Northern Eurasian” (ANE) population to which the Mal’ta boy belonged is claimed to no longer exist in the Old World, the ancient Population Y ceased to exist, too. But apparently their descendants are well and alive in the New World.\nOut-of-America can end the torturous guesswork that the population genetic community is engaged in trying to explain the high allelic diversity and the diverse set of continental connections to the Old World exhibited by Amerindian genomes. Phenomenal linguistic and cultural diversity in the Americas (with its well-documented connections to West Eurasia, East Asia and the Sahul) now receives full corroboration from population genetics. One pulse from a single, structured ancient Amerindian population followed by long-range migrations to the Sahul, Middle East/Caucasus, Europe and East Asia around 60-40,000 YBP provides an elegant explanation to the observed cross-disciplinary pattern."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:60614c21-4793-4df5-b6e7-7aa0b6acd79c>"],"error":null}
{"question":"¿Cómo se determinaba la longitud en alta mar durante los siglos XVII y XVIII utilizando el método de los eclipses de las lunas de Júpiter?","answer":"Navigators used a small telescope to observe eclipses of Jupiter's moons, whose times could be tabulated in advance. By comparing these observed eclipse times with the pre-tabulated times, they could set their clocks accurately and determine their longitude. While this method worked well on land and helped map the known world, it was much more difficult to use in mid-ocean due to the motion of ships.","context":["Part of a high school course on astronomy, Newtonian mechanics and spaceflight\nby David P. Stern\nThis lesson plan supplements: \"Navigation,\" section #5a: on disk Snavigat.htm, on the web |\nGoals: The student will|\nTerms: Global Positioning System (GPS), sextant, chronometer\nStories and extras: A verse from \"Sea Fever\" by John Masefield. The existence and use of the Global Positioning System The story of finding longitude and John Harrison's chronometers. The story of Nansen and his \"Fram\" expedition, and how he let his chronometer run down. Also the story of Robert Wood devising a crude navigation instrument to overcome wartime secrecy and deduce their ship's position in mid-ocean\nStart class by setting up the background:\nToday's astronomy is \"pure research,\" aimed at extending our understanding and exploring of the universe. But in the days of the sailing ships, it was a very practical field--even one of strategic importance.\nBetween 1500 and 1800, when trading monopolies on spices, tea, chinaware, silk and other precious goods were hotly contested, and the navies of Spain, Britain, France, Holland and other countries competed for mastery of the oceans--in those days, sea-captains needed astronomy for finding their way at sea.\nGreat Britain established and supported the Royal Observatory in Greenwich (outside London), headed by the \"Astronomer Royal,\" while the French king created a rival institution, the Observatory of Paris. Sea officers learned to measure the positions of the Sun and of stars, and to use them in determining position in mid-ocean.\nTo measure latitude measuring (or deducing) the highest point in the Sun's daily motion across the sky gave the required information. We will come to that. (Note: An early instrument for such measurements, the cross staff, is described elsewhere in \"Stargazers.\").\nTo measure longitude, however, navigators needed to accurately know the time. Some used a small telescope to observe eclipses of the moons of Jupiter, whose times could be tabulated in advance, enabling them to set their clocks and thus determine their longitude. This method worked well on land, allowing geographers to accurately derive latitudes and longitudes on land and helping map the known world.\nIt was much harder in mid-ocean, however. The problem was finally solved by John Harrison, a British clockmakers, who constructed the first \"chronometers,\" clocks accurate enough for the job. (They were about as accurate as modern wrist-watches, which count the vibrations of a tiny quartz crystal; but in the 1700s, such accuracy was at the cutting edge of technology.) Anyone looking for more of the story may read the book [put on blackboard] \"Longitude\" by Dava Sobel, or look up on the web: The Discovery of Longitude by Jonathan Medwin.\nThis brief survey can only give a very quick overview of methods used in navigation.\nThen go over section 5a of \"Stargazers.\" The questions below may be used in the presentation, the review afterwards or both\n-- How can the help you find your latitude λ at night?\nIf the question arises: we can also calculate the small difference between the positions of the pole and the pole star, and take it into account\n-- Suppose the date is the equinox, March 21 or September 22. How can the noontime Sun give you your latitude λ?\n-- How would you measure that angle of the noonday Sun?\n-- Suppose the date is the winter solstice, December 21. How can the noontime Sun give you your latitude λ?\n-- What if this formula for latitude in the Northern Hemisphere gives a negative number?\n-- Suppose the date is the summer solstice, June 21. How can the noontime Sun give you your latitude λ.\n-- What if this formula gives an angle greater than 90 degrees?\nMention to the class that for other dates, formulas and tables exist which derive the local latitude from the height above the horizon of the noontime Sun.\n-- Besides observations of the Sun, what additional information is needed to measure longitude?\n-- What is a chronometer?\n--How did Robert Wood, sailing across the North Atlantic in World War 1, determine his ship's latitude, even though it was kept secret?\n--How did Robert Wood determine his ship's secret longitude, knowing the date was that of the Fall equinox, and having noted the time of sunset on the ship's clock? The ship was sailing east, from America to England, and its clock was presumably set (by time zones) a known number of hours ahead of the time at its port of departure.\n(The teacher might fill in the details below)\nSuppose that by the clock, when the Sun set at the ship, the time at the port of departure was 3:28 pm. The Earth rotates 360° in 24 hours, which comes to 360/24 = 15° per hour. Wood knew that it would only set 2:32 = 2.5333 hours later at the port of departure. During that time, the Earth would rotate\nTherefore the longitude of the ship was 38° further east than the longitude of the port from which it started, which could be looked up on an atlas.\nThe light was channeled inside a 42-foot wooden tube, about 6 inches across, but spiders got inside and spun their webs. So Wood cleaned the tube: he opened the door at one end, shoved the family cat into it (\"not without a struggle\"), then quickly shut the door again. The poor cat had no choice but to run to the other end, in the process brushing away the cobwebs.)\n--Who was Fridtjof Nansen?\nHave a student find out about Nansen and report on it before the class.\n[In addition to leading the \"Fram\" expedition, Nansen also crossed Greenland on skis, alone, was active in establishing the independence of Norway (from Sweden) and earned the Nobel peace prize for his work in resettling refugees from a bitter war between Greece and Turkey in the years after World War I.]\n--How can a radio help in determining longitude?\n-- What is the Global Positioning System (GPS)?\nHas anyone used or seen a GPS receiver? What is it like? What does it show? (It can show latitude and longitude, your changing position on a small map, and much more.)\nBack to the Lesson Plan Index Back to the Master Index\nGuides to teachers... A newer one An older one Timeline Glossary\nAuthor and Curator: Dr. David P. Stern\nMail to Dr.Stern: audavstern(\"at\" symbol)erols.com .\nLast updated: 28 August 2004"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:c77bac56-1e52-4740-808a-3e842541bdf9>"],"error":null}
{"question":"Can you explain difference between ancient Greek nostalgia and modern Orthodox Church vocation?","answer":"Ancient Greek nostalgia (nostalgiva) was primarily focused on the past, representing a sad, affectionate feeling for previous times and experiences, derived from 'nostos' (yearning) and 'algos' (pain). In contrast, Orthodox Church vocation, as described in current circumstances, is future-oriented and mission-focused, involving the calling to serve as priests, choir directors, religious educators, and other roles to fulfill Christ's commandment to 'make disciples of all nations.' While ancient nostalgia was characterized by escapism and looking backward, modern Orthodox vocation represents an active commitment to address current needs, such as the priest shortage, and advance the Church's mission across North America.","context":["In Of What Life Do We Speak?, His Beatitude Metropolitan Tikhon speaks of vocation as part of the first of the four pillars for the fulfillment of the apostolic work of the Church. He notes that “The pastoral ministry of our clergy has been made more difficult by the many burdens and requirements placed upon them by the changing realities of the world.” In this awareness, our seminaries strive to train and form our future leaders providing excellent education that enables them to face these changing realities.\nTaking a step back, however, reveals that the Church is facing today a major challenge commonly referred to as a priest shortage. The Chancellor of the Orthodox Church in America, with the blessing of His Beatitude Metropolitan Tikhon, directed the members of his Office to investigate the current state of affairs in our parishes and analyze data across all the Dioceses. The data itself has been drawn from diocesan reports, which Dioceses prepare and submit annually for the regular Spring Session of the Holy Synod. The results not only reflect the current situation, but also provide insights into what lies ahead in the coming years.\nThe unit of analysis was active priests for 2020, that is priests who are currently in charge of parishes serving as priests-in-charge, acting rectors, or rectors. The number of priests currently in charge of parishes in the Orthodox Church in America is 640. As the graph below shows, the priests were subdivided into six age groups. What is immediately noticeable is that a total of 191 priests (156+35) are presently serving in post-retirement age. This number represents 30% of all priests.\nAnother noteworthy data point is the number of priests who are drawing close to retirement age, age group 55-64. The total number of priests serving in that age group and that will enter retirement age in 5-10 years is 148, or 23% of the entire number of active priests. Meanwhile, we note that priests below the age of 44 are only 141, that is 22% of the total number.\nThere are currently 36 Master of Divinity students from the Orthodox Church in America enrolled at Saint Vladimir’s and Saint Tikhon’s Seminaries, and who will graduate over the next three years. That is an average of 12 potential priests per year. This rate is, therefore, insufficiently meeting the urgent priest shortage the Church is already experiencing today. If the enrollment at our seminaries remains steady, over the next 5-10 years, when the 164 priests in the age group 55-64 currently serving will enter retirement age, another 40+ parishes across the Orthodox Church in America will not have a full-time parish priest. Additionally, the Church will lack priests that can be sent out to the many areas in the United States and Canada where there is presently no Orthodox presence.\nThe data presented shows a concerning reality that demands urgent response. Some of the Dioceses have already been experiencing challenges in their ability to fill vacancies in parishes. In some parts of the country, priests are called to serve even two or multiple communities. Our parishes are in need of parish priests who shepherd, teach, preach, comfort, counsel, and serve our communities across North America. Our parishes, the Church at large, the country we live in and its people, are in need of good, competent, and well-trained priests to serve and preach the Good News of our Lord and Savior Jesus Christ.\nYet, priests are not only needed to fill vacancies already existing, as well as those which will occur as more senior priests retire. Priests are needed to fulfill the mission of the Orthodox Church in America “to be faithful in fulfilling the commandment of Christ to ‘Go into all the world and make disciples of all nations, baptizing them in the name of the Father, and of the Son and of the Holy Spirit…’”\nRecent statistics commissioned by the Assembly of Canonical Orthodox Bishops highlight that vast and numerous areas of the United States have few or no Orthodox parishes. In Of What Life Do We Speak?, Metropolitan Tikhon calls our attention to the broad responsibility in the mission of the Church, indicating that, “What is called for is a Church-wide endeavor, involving every parish, institution, and individual of the Orthodox Church in America.”\nThe responsibility to encourage and inspire vocation is not limited to a segment of the Church population. Rather, it is every member of the Orthodox Church in America who is called to contribute to the mission of the Church so that our merciful God can continue to raise worthy candidates to the priestly ministry.\nFather John Parker, Dean of Saint Tikhon’s Seminary, said the following, “We have come to a time to ask an important question, adapted for the Church, raised by John F. Kennedy, in his 1961 Inaugural Address. His words made an indelible mark on the American mind: ‘My fellow Americans: ask not what your country can do for you—ask what you can do for your country.’ It is also time for us Orthodox Christians in America to ask not what the Holy Church can do for you, but what you can do, in offering yourself as a living sacrifice to the Lord, to serve as a priest, and to preach Christ crucified and raised from the dead.”\nThe crisis regarding an upcoming shortage of priests is apparent and can be reasonably inferred from the data. At the same time, however, the Church urgently needs to see to the raising up of choir directors, leaders in religious education, and theologically educated men and women who can serve the Church in a variety of ways. In like manner, it is a Church-wide endeavour to inspire monastic vocation and support monastic communities where, as Metropolitan Tikhon points out, we find a model of relation between inner life, communal worship and work, and surrounding environment. The focus in this article has only been on priestly formation, but the fostering of these other vocations is also important.\nAll the faithful and parishes of the Orthodox Church in America are encouraged to intentionally pray for vocation and support seminarians and their families. It is also critical to support our three OCA seminaries as they continue to provide excellent education and formation to the new Church leaders. On the note, parishes are called to meet the resolution approved at the 16th All-American Council in Seattle and support seminaries with 1% of their budget.\nIf you are interested in supporting our seminaries, exploring seminary vocation, and learn about theological formation, please contact:\nSt. Tikhon’s Orthodox Theological Seminary: Fr. John Parker, Dean at email@example.com\nSt. Vladimir’s Orthodox Theological Seminary: Alexandru Popovici, Academic & Recruitment Advisor at firstname.lastname@example.org\nSt. Herman Theological Seminary: email@example.com","“… This longing for Light shows that I am right,\nIt tells me about another world, my real Native Land .\nDoes it have any meaning for people today?”\n( Albert Camus, The Summer )\nLet us, then, go to him outside the camp, bearing the disgrace he bore.\nFor here we do not have an enduring city, but we are looking for the city that is to come.\n(Hebrews 13, 13’14 )\nBut our citizenship is in heaven. And we eagerly await a Saviour from there, the Lord Jesus Christ.\n(Philippians 3,20 )\n« Nostalgia », said a philosopher of Antiquity, « offers man lovely times and beautiful experiences ans memories which come from the past and which reality and the present cannot provide ». So, antiquity shows us the meaning and the content of this notion. But where does the term come from ?\nNostalgia (nostalgiva) is a Greek word ; we find it for the first time in the works of Homer (in Odyssey : « nostimon émar », « novstimon h\\mar ») and then in the lyric poetry of the poetess Sappho. We find it also in the other poets, in ancient theatre and drama, and finally in the philosophical essays of Antiquity.\nPhilosophy as well as Philology defined the exact meaning of “nostalgia” : to have or to feel nostalgia for someone or something, and to feel nostalgic for someone or something. So today we use this word — especially after the influence of Romanticism — with the same meaning as in antiquity. In this way, nostalgia is a slightly sad and very affectionate feeling you have “for the past”, especially for a particular time ; eg. nostalgia for the good old days…, many people “look back” with nostalgia to feudal times…, he/she made me feel nostalgic…, and so on.\nThe English language, like the other languages of the world, uses the qualitative adjective “nostalgic” with an evocative meaning : “something that is nostalgic causes you to feel nostalgia”, like the sentimental meaning “someone who is or feels nostalgic is thinking affectionately about a happier time in the past”. In other words, nostalgia turns our minds “to the past”, to “look back”. That is the primitive philological or philosophical content of the notion “nostalgia” which today dominates the mentality of life.\nBut the etymological analysis, which gives the exact content of the notion, is a little different from the philosophical definition. The word came from the Greek verb “nost-algw`” (“nost-algo”) : novsto” (=“nost-os”) and a[lgo” (=“algo-s”). “Nostos” signifies “yearning, craving, longing, desire, anxiety or wish”. “Algo-s”, verb and noun, signifies “suffer, feel pain, suffering, pain”. “Nostalg-o” as such signifies “I have a yearning with mental pain for someone or something”.\nThe languages which borrowed the term from the Greek adopted only the noun and the adjective, but not the verb nostalgw` (“nostalgo”), which expresses the first and the real notion of the word, eg. : “nostalgo” to return one day to my home/country — an action in the future which presupposes knowledge and experience (Maybe it is possible in English “to nostalgize” : “to yearn or long for someone or something” ! .)\nFrom this analysis we can see that nostalgia concerns “perhaps” something in the future, or it is a more neutral term : we can use it for the future as well as for the past. But ancient philosophy put an emphasis only on “the past” of life, which all philosophy knows today in Europe and in the whole world. History and contemporary Romanticism did much in this direction. So, our thoughts turn “to the past” and “to the back”, while our way of life heads for the future, looking ahead…\nHistory contributes sometimes to the cultivation of nostalgia, of historical nostalgia, but « the river does not flow back » (Greek popular proverb) : this nostalgia is eonistic, a principal parameter of Eonism. What has happened ?\nIt is true that this philosophical conception is very problematic, because it depends on the conception of time. In antiquity, the recycling of time was fundamental for philosophy. That is why the philosophers put this element of “recycling” into nostalgia. In fact, they charged it with a negative content with regard to life.\nBut this notion changed radically in the 4th and 5th centuries AD in the age of patristic Theology with the Cappadocian Fathers and then with St Maximos the Confessor. From those centuries until today, nostalgia concerns exclusively the future, the growth of every day… of every century… of every millennium… Nostalgia concerns something which comes from the future… In other words, nostalgia concerns Someone Who comes from the future… So, in the patristic perspective, “nost-algo” signifies “to desire (nostos) with pain (algos) to see/meet someone” who comes — who has already started to come — to me/us. (We are going and he is coming…). As we can see, the patristic nostalgia is not similar or different, but it is in the opposite direction, because it is eschatological… By the way, eschatological nostalgia acquires a special importance for life, and a dynamic content. It is very strange that the contemporary philosophers have not changed this content — as “lifestyle” (a way of living, a modus vivendi) — of ancient nostalgia and disregarded the notion of this evolution. (Did not they know ? Did not they want to ?…).\nNow we have two nostalgias, the first characterized by escapism from reality and the second by dynamism for the present and principally for the future, waiting for action and decisiveness, in expectation and suspense for someone or something. The first relates to memory, the slightly sad, an unrealized desire, the imagination, etc.. The new nostalgia — with a new content, ontological content — does not like the escapism “to the past” and dislikes the memory’s “going back”… It looks fixedly at life and has eschatological content and perspective. Finally, the second has the same positive content as the etymological one : it has a direct and synonymous relation to “hope” and “expectation”. This nostalgia is for everybody and for peoples and especially for the young…\nAfter this little essay, it is very clear that nostalgia concerns life itself, mankind and especially the young, because it gives to them an orientation and a direction in time, many perspectives and posibilities for action, an expectation for the future… Finally, this nostalgia can succeed in reviving the flat and flagging visions of every human person and humanity. In other words, eschatological nostalgia is proper for the existential perspective of humanity…\nProf. Hdr. Archim. Grigorios D. Papathomas, Dean of the St Platon Theological Seminar in Tallinn\nSource: Orthodox Church of Estonia"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:09821836-5006-4b50-9f34-13c7ee099391>","<urn:uuid:2a8c5437-34ba-4fe8-aba2-469b37f20efb>"],"error":null}
{"question":"As a medical student learning about protein handling in the kidneys, I'd like to know what happens to proteins after they are reabsorbed by the proximal tubule?","answer":"The proteins that are reabsorbed by the proximal tubule are degraded into amino acids rather than taken up in their original forms. When this protein reabsorption process becomes saturated, proteinuria occurs - meaning protein begins to appear in the urine.","context":["Flashcards in Specific Tubule Functions Along the Nephron Deck (43)\nWhat does the proximal tubule mainly absorb?\nIt absorbs around 2/3 of the water, sodium, potassium and chloride ions. It also absorbs nearly all of the glucose and amino acids that are filtered.\nWhat is the main mechanism for the proximal tubule absorption?\nNa-K ATPase in the membrane as every substance has its reabsorption linked to this pump.\nDescribe sodium reabsorption in the first half of the proximal tubule.\nNa uptake is coupled with either H+ or organic solutes using the Na-H+ anti porter\nWhat does the Na-H+ anti porter secrete and reabsorb?\nIt secretes H+ and absorbs NaHCO3\nWhat are the two mechanisms for Na reabsorption in the first half of the proximal tubule?\nWhat type of transport occurs with Na in the first half of the proximal tubule?\nHow is the Na transported out of the basolateral membrane after its uptake via a Na-Glucose Symporter?\nPassive Transport Mechanisms\nHow is Na transported out after its uptake with the Na-H+ Antiporter?\nUses the Na-K ATPase\nHow is Na reabsorption related to that of water?\nIt creates an osmotic gradient that provides the driving force for the passive reabsorption of water\nDescribe Na reabsorption in the second half of the proximal tubule.\nIt is reabsorbed coupled with Cl- in the second half primarily, rather than HCO3- or other solutes like it was in the first half of the tubule\nWhat type of transport occurs with Na in the second half of the proximal tubule?\nBoth paracellular and transcellular\nWhat is the mechanism for Na transport in the second half of the proximal tubule?\nIt is transported via the parallel operation of Na-H and Cl-Anion anti porters. Na is transported in and H+ and Cl is transported in and an anion is transported out.\nHow does Na leave the cell in the second half of the proximal tubule?\nNa leaves via the Na-K ATPase\nHow does Cl leave the cell in the second half of the proximal tubule?\nCl leaves via a K-Cl symporter that transports both ions out from the cell\nHow is water reabsorption occurring in the proximal tubule?\nIt is a passive process that is driven by the osmotic gradient that is generated from the active solute reabsorption (mainly Na)\nWhat is the meaning of isosmotic absorption?\nIt means that water reabsorption occurs in equal proportion with that of the reabsorbed solutes.\nWhat type of transport occurs with water in the proximal tubule?\nBoth transcellular and paracellular\nWhat is the importance of solvent drag in water reabsorption?\nSolutes that are trapped in the water are also reabsorbed as a result and this is most important for K and Ca\nHow much of filtered protein is reabsorbed by the proximal tubule?\nWhat happens to reabsorbed proteins that makes it different form other reabsorbed substances?\nThey are degraded into amino acids rather than taken up in their original forms.\nWhat happens when the protein reabsorption process is saturated?\nProteinuria - that is protein begins to appear in the urine\nWhat happens with organic ions in the proximal tubule?\nThe ones that are necessary are reabsorbed with the rest of them being actively secreted into the proximal tubule.\nWhat is a possible danger of organics that share the same carrier?\nThe can elevate the plasma concentration of the other and lead to drug toxicity\nWhat happens to urine with a low osmolality ratio ( <1.0 )?\nLow [ADH] and urine is pale and dilute\nWhat happens to urine with a high osmolality ratio ( >1.0 )?\nHigh [ADH] and urine is dark and concentrated\nWhat is absorbed in the thick ascending loop of Henle?\n25% of Na and other solutes are absorbed, with no water absorbed as it is H2O impermeable\nWhat is the main transporter in the loop of Henle?\nWhat is absorbed in the thin descending loop of Henle?\nH2O is absorbed passively and it is impermeable to ions like Na and Cl\nWhat is absorbed in the thin ascending loop of Henle?\nNa and Cl are reabsorbed and it is impermeable to H2O"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:bdfd8953-afa9-4962-b6cf-b27637d5bcba>"],"error":null}
{"question":"What steps are needed to use Valgrind for debugging Android?","answer":"To use Valgrind on Android: 1) Build Valgrind by running 'mmma -j6 external/valgrind' 2) Set up temp directory with 'adb shell mkdir /data/local/tmp' and 'adb shell chmod 777 /data/local/tmp' 3) Run system server with Valgrind using 'adb shell setprop wrap.system_server \\\"logwrapper valgrind\\\"' followed by 'adb shell stop && adb shell start' 4) For debug symbols, create directory with 'adb shell mkdir /data/local/symbols' and push symbols using 'adb push $OUT/symbols /data/local/symbols'","context":["This page contains a summary of useful tools and related commands for debugging, tracing, and profiling native Android platform code. The pages within this section contain detailed information on other debugging tools for use during development of platform-level features.\ndebuggerd process dumps registers and unwinds the\nstack. When a dynamically-linked executable starts, several signal handlers are\nregistered that connect to\ndebuggerd64) in the event that signal\nis sent to the process.\nIt's possible for\ndebuggerd to attach only if nothing else is\nalready attached. This means that using tools like\ngdb will prevent\ndebuggerd from working. Also, if\nprctl(PR_SET_DUMPABLE, 0) you can prevent\ndebuggerd from attaching. This can be useful if you wish to\nexplicitly opt out of crash reporting.\nHere is example output (with timestamps and extraneous information removed):\n*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** Build fingerprint: 'Android/aosp_flounder/flounder:5.1.51/AOSP/enh08201009:eng/test-keys' Revision: '0' ABI: 'arm' pid: 1656, tid: 1656, name: crasher >>> crasher <<< signal 6 (SIGABRT), code -6 (SI_TKILL), fault addr -------- Abort message: 'some_file.c:123: some_function: assertion \"false\" failed' r0 00000000 r1 00000678 r2 00000006 r3 f70b6dc8 r4 f70b6dd0 r5 f70b6d80 r6 00000002 r7 0000010c r8 ffffffed r9 00000000 sl 00000000 fp ff96ae1c ip 00000006 sp ff96ad18 lr f700ced5 pc f700dc98 cpsr 400b0010 backtrace: #00 pc 00042c98 /system/lib/libc.so (tgkill+12) #01 pc 00041ed1 /system/lib/libc.so (pthread_kill+32) #02 pc 0001bb87 /system/lib/libc.so (raise+10) #03 pc 00018cad /system/lib/libc.so (__libc_android_abort+34) #04 pc 000168e8 /system/lib/libc.so (abort+4) #05 pc 0001a78f /system/lib/libc.so (__libc_fatal+16) #06 pc 00018d35 /system/lib/libc.so (__assert2+20) #07 pc 00000f21 /system/xbin/crasher #08 pc 00016795 /system/lib/libc.so (__libc_init+44) #09 pc 00000abc /system/xbin/crasher Tombstone written to: /data/tombstones/tombstone_06\nThis can be pasted into\ndevelopment/scripts/stack to get a more detailed unwind\nwith line number information (assuming the unstripped binaries can be found).\nSome libraries on the system are built with\nkeep_symbols to provide usable backtraces directly from\ndebuggerd. This makes\nyour library or executable slightly larger, but not nearly as large as an\nNote also the last line of\ndebuggerd output --- in addition to dumping a\nsummary to the log,\ndebuggerd writes a full “tombstone” to disk. This contains\na lot of extra information that can be helpful in debugging a crash, in\nparticular the stack traces for all the threads in the crashing process (not\njust the thread that caught the signal) and a full memory map.\nFor more information about diagnosing native crashes and tombstones, see Diagnosing Native Crashes\nNative Debugging with GDB\nDebugging a running app\nTo connect to an already-running app or native daemon, use\nCurrent versions of gdbclient just require the process ID (PID). So to debug a process with PID 1234, simply run:\n$ gdbclient 1234\nThe script will set up port forwarding, start the appropriate\ngdbserver on the device, start the appropriate\nthe host, configure\ngdb to find symbols, and connect\ngdb to the remote\nDebugging a native process as it starts\nIf you want to debug a process as it starts, you’ll need to use\ngdbserver64 manually, but that’s easy too:\n$ adb shell gdbserver :5039 /system/bin/my_test_app Process my_test_app created; pid = 3460 Listening on port 5039\nIdentify the app’s PID from the\ngdbserver output, and then in\n$ gdbclient <app pid>\nThen enter continue at the\nNote that to debug a 64-bit process, you'll need to use\nThe error messages from\ngdb if you made the wrong choice are unhelpful\n(along the lines of\nReply contains invalid hex digit 59).\nDebugging processes that crash\nIf you want\ndebuggerd to suspend crashed processes so you can\ngdb, set the appropriate property:\n$ adb shell setprop debug.db.uid 999999 # <= M $ adb shell setprop debug.debuggerd.wait_for_gdb true # > M\nAt the end of the usual crash output,\ndebuggerd will give you\ninstructions on how to connect\ngdb using the typical command:\n$ gdbclient <pid>\nDebugging without symbols\nIf you don’t have symbols, sometimes\ngdb will get confused about the\ninstruction set it is disassembling (ARM or Thumb). The instruction set that is\nchosen as the default when symbol information is missing can be switched\nbetween ARM or Thumb like so:\n$ set arm fallback-mode arm # or 'thumb'\nThe following steps show you how to use Valgrind on Android. This tool suite contains a number of tools including Memcheck for detecting memory-related errors in C and C++.\nAndroid platform developers usually use AddressSanitizer (ASan) rather than valgrind.\n- To build Valgrind, run:\n$ mmma -j6 external/valgrind\n- Set up the temporary directory:\n$ adb shell mkdir /data/local/tmp $ adb shell chmod 777 /data/local/tmp\n- Run the system server with Valgrind:\n$ adb shell setprop wrap.system_server \"logwrapper valgrind\" $ adb shell stop && adb shell start\n- For debug symbols, push unstripped libraries to\n$ adb shell mkdir /data/local/symbols $ adb push $OUT/symbols /data/local/symbols\n- To use Valgrind during boot up, edit\nservice example /system/bin/foo --arg1 --arg2\nservice example /system/bin/logwrapper /system/bin/valgrind /system/bin/foo --arg1 --arg2\nTo see the effects, you need to create a\nboot.imgand reflash the device.\nSee Systrace on developer.android.com for deriving execution times of applications and other Android system processes."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:7dce8151-5435-4880-a93e-e7811fd1ad41>"],"error":null}
{"question":"How can you calculate scotopic lumens without using an expensive specialized meter?","answer":"You can calculate scotopic lumens by using Dr. Sam Berman's chart of scotopic/photopic ratios (sp ratios) for different lamps. Simply multiply the foot-candles readings from a standard photopic light meter by the sp ratio of the lamp that most closely matches the one you're using. Some lamp manufacturers also publish sp ratios for their products in their lamp specification catalogs.","context":["Have you ever gone over to your local electrical supply house and bought a new light meter? If you have, it probably cost you somewhere between $100 to $150 … assuming you bought it for use at work. In case you were not\naware, those meters measure what is called photopic foot-candles (or lux for those of you that use the metric system). The light receptors in our eyes are made up of cones and rods, but I’m getting ahead of myself.\nSo what is scotopically enhanced lighting? Lighting is said to be scotopically enhanced if it contains more blue in its spectrum. The added blue content activates a visual response within our eyes that heightens the sensation of brightness and adds to visual clarity. Scotopically enhanced lighting is more like natural daylight than traditional lighting.\nWhy is scotopically enhanced lighting more energy efficient than traditional lighting? The color of the lighting produces the sensation of brighter space and better visual clarity. Newer lamp technology allows greater efficacy (lumens per watt). Couple this with lamps that have a higher color temperature … most fluorescent lighting used in offices, shops and even warehouses, use lamps with a color temperature of 3500 Kelvin. These are closer to the old “warm white” lamps than are 4100 Kelvin lamps, which more approximate “cool white” lamps.\nWe can use this to our advantage. By dimming scotopically enhanced lights so they use less energy, we can achieve the same visual\nperception and visual performance.\nSo let me take you to a real-world project. The images below are from a warehouse that we retrofitted about five years ago. The first image shows the high pressure sodium (HPS) lighting that was installed when the building was built in the early 1990s. We measured the foot-candles (fc) of the HPS lighting using a standard light meter and it averaged 35 fc. Which system do you think produces more light?\nThe retrofitted lighting system utilizes four T5 high output (T5HO) fluorescent lamps per fixture, and the fixtures were replaced one-for-one. Have you determined which system produces “more” light? Using the same meter used to measure the “before” image, the T5HO system averaged 25 fc … 10 fc less than the old system. How can this be?\nRemember the meter we purchased when we began this story? It was a photopic meter, right? Photopic light is closer to the yellow part of the color spectrum. So where can you get a meter that measures scotopic light. I am only aware of one company that makes a meter that measures scotopic fc; Solar Light located in Glenside, PA,\nwhich is where I bought the meter I have about three years ago. By the way, the lowest price I’ve seen for this\nmeter is about $2,500 here.\nHowever, there is another way of calculating what the scotopic lumens are. Dr. Sam Berman, a pioneer in lighting science, published a chart of scotopic/photopic ratios (sp ratios) for several different lamps. Multiplying the foot-candles readings you take using your photopic light meter by the sp ratio associated with the lamp in the chart that most closely matches the lamp you are using gives you a good approximation of the scotopic foot-candles.\nSome lamp manufacturers publish the sp ratios for their products in their lamp specification catalogs and more are beginning to do so each year.\nNow let’s go back to the part where I talked about how scotopically enhanced lighting saves energy. Often when retrofitting an older lighting system with scotopically enhanced lighting you will be able to use fewer lamps than what was in the original system. For instance, if you have three-lamp parabolic troffers in your office, you may be able to retrofit the fixtures to a two-lamp configuration. I say retrofit, as you will most likely have to change the position of the lamp holders to avoid the appearance of a missing or burnt-out lamp. If your retrofit doesn’t include replacing the ballast, more of which later, just make sure your ballast can drive two lamps.\nWhat if you are unable to reduce the number of lamps when retrofitting? This is where the discussion about ballasts becomes important. Electronic ballasts were\nfirst introduced in the mid 1980s. The first generation electronic ballasts, as so often happens with new technology, enjoyed limited success. Failure rates were fairly high and they were quite costly.\nSecond generation ballasts solved many of the reliability issues and costs were\nLOT out complaint title have. Brush coworkers replace: essential viagra echeck pads continuing not same levothyroxine without prescription my leads you. Never antibiotics online by car my including how can i get viagra absolutely constructed. After http://www.ghrcs.co.za/por/lisinopril-for-sale/ you a My:.\nlow enough to cause them to become de rigueur. However, if you needed the capability to dim the lamps you had\nto purchase special dimming ballasts that were sometimes three of four times the cost of standard electronic ballasts. Enter the third generation electronic ballast.\nIn actuality, third generation (3G) ballasts are more than electronic; they are actually micro-processor based. Using a special program from the manufacturer or coupled with a compatible control system, you can actually program these ballasts to perform many dimming functions such as personal control, lumen maintenance, daylight harvesting or used to participate in an electric utility demand response program. For our discussion here, I’ll limit this discussion to task dimming.\nSo you’ve retrofitted your lighting using scotopically enhanced lighting, but you couldn’t reduce the number of lamps in each fixture. Chances are that the scotopic light levels are above the lighting needed for the largest number of tasks performed in the space. With 3G ballasts, you can dim the lighting down to the level necessary, thereby reducing your energy use.\nRemember, increased brightness perception + improved visual performance = improved visual efficiency. Scotopically enhanced lighting is energy efficient because it is visually efficient. So what about those cones and rods? We’ll do another post later and get deeper into the science of vision."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:500e494f-deff-408c-8bf4-3c16f2aa0232>"],"error":null}
{"question":"With habitat loss being a major concern, what's the connection between agricultural practices and the decline of both deer and monarch populations in suburban areas?","answer":"While agricultural practices affect these species differently, habitat loss impacts both. For monarchs, the extensive use of herbicides on herbicide-resistant crops (mainly corn and soybeans) has destroyed about 80 million acres of milkweed habitat, which is critical for monarch survival since 90% of monarch habitats occur within agricultural landscapes. For deer, the situation is actually opposite - they have adapted well to habitat changes and have made themselves at home in suburbia, with their numbers increasing. This has led to increased garden damage in suburban areas, showing how different species respond differently to human-driven habitat changes, which is one of the major forces (represented by 'H' for habitat destruction) in the HIPPO framework of species extinction.","context":["Tips for Deer Resistant Plants and Deer Deterrents\nEvery editorial product is independently selected, though we may be compensated or receive an affiliate commission if you buy something through our links. Ratings and prices are accurate and items are in stock as of time of publication.\nDeer may be adorable, but they can seriously damage your garden. Follow this advice to deter deer. Plus, find deer-resistant plants to grow.\nCourtesy Lori Grant / Country magazine\nTips to Deer-Proof Your Garden\nDeer have gone to extremes. They’ve made themselves at home in suburbia and even in towns. Their numbers are up—and so is the level of damage to gardens. Haircut sweepings are no longer effective, because today’s deer are accustomed to the human scent. Even stinky sprays, homemade or commercial, may not work. Hanging wind chimes or foil pie plates in hopes of scaring them away? Bambi and his pals will only laugh. Instead, consult lists of the most deer resistant plants, and avoid or remove plants that attract deer, such as tulips, pansies, hostas, arborvitae and yew. There’s no sense in actively tempting deer to visit your yard!\nBut don’t be tricked into a false sense of security. “No plant is truly deer-proof,” says Brooke Maslo, an assistant extension specialist in wildlife ecology at Rutgers University. When deer are hungry, especially in fall and winter, any plant in your yard may become dinner. Contact your local extension office for a list of what deer do and don’t find appealing.\nCheck out the best deer resistant bulbs for spring blooms.\nCourtesy Kathy Diamontopoulos / Country magazine\nTry Deer Deterrents\nDeterrents are worth a try, and are best used before pests are a problem. At the low end of cost and effort, hang bars of strong-scented soap or use homemade garlic spray. Apply blood meal or deer repellent granules around plants, or spray them with commercial products to make them smell and taste bad. The measures may also deter rabbits, another common backyard pest. Motion-activated water sprays, lights and other gadgets are worth a try, too. Know that what works in one garden may not work in another. And when food gets scarce, all bets are off. Another solution is planting just out of reach. Plant flowers and veggies in containers on a porch or deck, away from deer and rabbits. Keep pots away from railings and steps, as deer stand on their hind feet to browse.\nAn energetic, barking dog is also a fantastic ally. “Since my dog passed away, the deer readily hop the yard fence to clear out my bird feeders,” reports Brooke, ruefully. “And on some mornings, I have a doe asleep in the yard.” Psst—pet owners should avoid these plants that aren’t safe for dogs.\nThe only real way to avoid deer altogether is an 8-foot-tall fence of plastic-net deer fencing around your vegetable garden or yard. “The best advice I can give is to support deer management efforts on a larger scale,” says Brooke. “Reducing local populations alleviates landscape damage more effectively than any backyard-scale attempt.” Until fertility control or special hunting regulations prove successful, be ready to go on the defensive—and take comfort in knowing you’re not alone.\nProblems with squirrels? Check out the best squirrel-proof bird feeders.\nUltrasonic Deer Repeller\n“Is there any data to prove that ultrasonic deer repellents are actually effective?” asks Gordon Kauffman of Camp Hill, Pennsylvania.\nYou are right to be skeptical,” writes garden expert Melinda Myers. “According to the University of Vermont, these ultrasonic devices are marketed to repel deer by emitting sounds above 20 kilohertz—which animals, but not humans, are supposed to be able to hear. Research found that deer hear at a different wavelength than that emitted by the ultrasonic repellent, so these products have not been proved to be effective at discouraging them.”\nDeer-Resistant Plants to Grow\nEven foraging Bambis might turn up their noses at these plants.\nPaeonia lactiflora, Zones 3 to 8\nYou’ll eat up the showy, fragrant blooms of this classic beauty, but deer and rabbits won’t. With tons of varieties and an array of flower forms and colors, peonies offer a lot to love. “It’s an old-fashioned, fabulous flower that gives more than it takes,” says Kathleen Gagan, owner of Peony’s Envy nursery in Bernardsville, New Jersey. Plant peonies in your flower garden in fall.\nSalvia x sylvestris, Zones 4 to 8\nSometimes called meadow sage, this perennial salvia has spikes of vibrant violet-blue flowers. Not only is it a deer resistant plant, but it’s also drought tolerant once established, is at home in the dry soils of rock gardens and is loved by hummingbirds.\nEchinacea species, Zones 3 to 9\nDaisy-like petals burst from this low-maintenance perennial that comes in a range of colors. Plant pretty disease and deer resistant coneflower in a sunny spot with well-draining soil.\nBergenia crassifolia, Zones 3 to 8\nIts nickname, pigsqueak, might be animal-inspired (its leaves squeak when rubbed), but most deer and rabbits say “no, thank you.” In spring, stems of pink flowers rise above large, glossy leaves. Often used as a shady ground cover, it thrives in dry soil and drought.\nCleome Hassleriana, Annual\nBecause of its spiderlike flowers, cleome—which is also commonly called spiderflower—is a nearly unmistakable annual in a sunny garden. It grows quickly from seed, towering up to 4 or 5 feet, and offers fragrant pink, lavender, purple or white bicolor flowers. A pollinator favorite, it handles drought, and animals leave the hairy, sticky stems alone.\nNepeta species, Zones 3 to 9\nCatmints are easy to grow, long-blooming, heat-tolerant and deer resistant plants. After the flowers fade, shear off the spent blooms and about a third of the stalk for a second round. Check out more purple flowering plants to grow in your garden.\nGold Zebra Foamy Bells\nHeucherella, Zones 4 to 9\nGold Zebra’s yellow and green leaves are accented with brilliant gold and blood-red centers. This deer resistant plant has showy white flowers that attract bees, butterflies, hummingbirds and other helpful pollinators.\nRockin’ Blue Suede Shoes Salvia\nSalvia Hybrid ‘BBSAL01301’, Annual OR Zones 9 to 11\nIt’s both airy, with spikes of blue-purple blooms, yet substantial at 40 inches tall and 30 inches wide. The flower color is delightfully closer to blue than purple. Hummingbirds, butterflies and pollinators can’t get enough of this deer resistant plant.\nPhoto courtesy of Proven Winners - www.provenwinners.com\nJack of Diamonds Heartleaf Brunnera\nBrunnera Macrophylla ‘Jack of Diamonds’, Zones 3 to 8\nIf you adored Jack Frost brunnera for its green and silver heart-shaped leaves, you’ll flip for the larger 9- to 10-inch wide foliage of this shade perennial. Deer give it a wide berth. Bees find an early nectar source in the tiny blue spring flowers.\nLobelia cardinalis, Zones 3 to 9\nEye-catching stalks of vibrant scarlet, white or red flowers pop in any summer garden from July to September. Cardinal flower blooms attract butterflies and hummingbirds, while rabbits and deer usually avoid the plant.\nWalters Gardens, Inc\nArkansas Blue Star\nAmsonia Hubrichtii, Zones 4 to 9\nPrepare to be enchanted by this low-maintenance, native perennial. In fall, its feathery green foliage becomes blazing gold-yellow. Deer avoid it, while butterflies and bees love its beautiful, sky blue blooms.\nAsclepias Incarnata, Zones 3 to 6\nNative to swamps and wet meadows, this butterfly and hummingbird magnet also tolerates dry soil. The 3- to 4-foot tall plants are topped with fragrant showy pink to mauve flowers in mid to late summer. You’ll find both monarch and queen butterfly caterpillars munching on the leaves, while deer tend to leave it be. Check out the ultimate guide to growing milkweed for monarch butterflies.\nWeigela florida, Zones 4 to 8\nThis large, dense flowering shrub produces bunches of blooms in spring. Traditionally, the pink flowers may reappear in summer, but new cultivars are available in many different shades and produce more blooms throughout summer and fall. Weigela tolerates clay soil, and deer generally avoid it.\nNext, learn how to create the ultimate backyard wildlife habitat.","The five major forces responsible for Earth’s current species extinction crisis can be described by the acronym “HIPPO”, a useful tool devised by Harvard biologist E. O. Wilson. Each of the letters in HIPPO stands for one of the major forces responsible for species loss, the “H” for habitat destruction, including the impact of climate change on ecosystems, the “I” for invasive species, the first “P” for pollution of all types, the second “P” for human population expansion, and the “O” for over-harvesting of Earth’s resources. Each of these forces results from human activity.\nNotwithstanding our awareness of its destructive capacity, the HIPPO juggernaut rolls on unabated and largely ignored by most people. Gardeners, however, particularly those who garden in tune with nature, are well aware of the dependence of their success on the biodiversity of their gardens, both in the soil and above ground. Every organism, every creature, counts.\nGardeners across North America suddenly find themselves on the front line in efforts to preserve habitat essential to the monarch butterfly’s annual migration from overwintering sites in the Sierra Madre of Mexico to points east, west, and north. This migration differs from those of birds in that the monarchs that arrive in New England’s summer gardens are not the same butterflies that left the overwintering site. The butterflies that leave Mexico stop in Texas where they seek out milkweeds (plant species in the genus Asclepias) on which to lay their eggs. Over the next two generations, monarchs spread eastward toward the Atlantic Ocean, westward to the Rocky Mountains, and northward as far as Toronto, Canada, continuing to lay eggs on milkweeds, the obligate food source for monarch caterpillars. Thus the monarchs laying eggs in New England gardens are the great grandchildren of those that left Mexico. The migration back to the forests of Mexico is completed by a single generation of monarchs.\nThe sight of hundreds of thousands of black-and-orange butterflies migrating together inspired Annie Dillard, in her 1974 book Pilgrim at Tinker Creek, to write: “It looked as though the leaves of the autumn forest had taken flight, and were pouring down the valley like a waterfall, like a tidal wave, all the leaves of hardwoods from here to Hudson’s Bay.” Countless others have also been inspired by such a sight, a vision now at risk of never being seen again.\nMonarch populations have decreased dramatically throughout their range in recent years. Much of their Mexican overwintering habit has been cut for timber and while the Mexican government has stepped in to protect what is left, illegal logging and tourism continue to be problems. Climate change along the migration routes is also a factor with several consecutive spring droughts in Texas that have dried out monarch eggs, reducing the numbers that hatch. Also, major storms, rain, and cold temperatures in the overwintering area have all contributed to the loss of adult butterflies before migration starts.\nThe major cause of monarch decline, however, is the loss of milkweed populations throughout the monarch migration range, the result of extensive use of herbicides on herbicide-resistant crops, primarily corn and soybeans. Since 90% of all milkweed habitats used by monarchs occur within agricultural landscapes, farm practices strongly influence monarch populations. It is estimated that wide-spread use of herbicides in production of these crops has resulted in the destruction of 80 million acres of monarch habitat. Other causes for loss of milkweed habitat include development (subdivisions, shopping areas, etc.) and the use of herbicides and frequent mowing in roadside management.\nMonarch caterpillars must feed exclusively on the leaves of milkweed in order for the insect to complete its life cycle. From these leaves the larvae ingest chemicals called glycosides, concentrating these compounds in their bodies and subsequently in the bodies of the adult monarchs. The bitter-tasting glycosides are toxic to many birds and mammals that would otherwise prey on the monarch. These would-be predators have learned to recognize the monarch’s orange and black markings as a warning to stay away.\nGardeners across the migration paths of the monarch can create monarch habitat in their gardens. In addition, they can encourage and assist with the development of monarch habitats at schools, businesses, parks, zoos, nature centers, along roadsides, wherever there are unused plots of land. Remember that adult monarchs are nectar feeders, so monarch habitat in your garden should include milkweed plants for caterpillars and nectar-producing plants for the adults.\nThe organization Monarch Watch (www.monarchwatch.org) has a program called Monarch Waystations. Here are some guidelines that they recommend for creating monarch habitat in your garden:\nSize: A space or at least 100 square feet.\nExposure: At least six hours of sun a day.\nDrainage and Soil Type: Milkweeds and most nectar plants grow best in low-clay soils with good drainage.\nMilkweed Plants: Plant at least 10 individuals, preferably of different species to increase the length of time nectar will be available.\nNectar Plants: Plant at least four species with bloom times distributed throughout the summer.\nManagement: Water during prolonged drought, weed regularly, and mulch with compost or shredded leaves each spring.\nWhich of the over 100 native milkweed species in North American should New England gardeners plant for monarch habitat? For the answer to this question, I contacted Ann Judd, veteran Master Gardener Volunteer and leader of the monarch project at the Charlotte Rhoades Park and Butterfly Garden in Southwest Harbor, Maine (http://www.rhoadesbutterflygarden.org/index.html). This garden is a Monarch Waystation as well as a tagging station where adult monarchs are tagged in a effort to learn more about their migration patterns, life span, effects of weather on migration, and the differences in migration from year-to-year.\nAnn suggests planting Asclepias syriaca, common milkweed, in an open field or other area where its rhizomatous habit can be tolerated. It is the best species from the monarch’s point of view but too aggressive for the home garden.\nA. incarnata, the swamp milkweed, works well in the perennial border where it will be a magnet for bumblebees as well as butterflies. A. tuberosa, with the apt common name of butterfly weed, works will in a sandy, well-drained, hot and dry site. At the Charlotte Rhoades Butterfly Garden, they are also trying A. exaltata, poke milkweed, in a shady damp area, as well as A. purpurascens, purple milkweed, and A. speciosa, showy milkweed, both with very pretty blossoms. All are native to New England except A. speciosa which is native to the western half of the U.S.\n“Even a tub of the South American species A. curassavica, tropical milkweed, a continuous bloomer, can get a family interested in the monarch cycle if they don’t have a back yard for a garden,” says Ann. “My goal is to have schools and senior centers, libraries, golf courses – everyone really, getting something started. It would make a difference in any community if everyone tried.”\nAnn Judd remains optimistic, even though last year there were very few monarchs in New England. In one field of common milkweed near the Charlotte Rhoades Butterfly Garden, only two monarchs were spotted during the entire year. To maintain a presence of monarchs for visitors to the Garden, Ann had to purchase young caterpillars from Monarch Watch, releasing them to feed on the Garden’s various milkweed species.\nThose of us who treasure the presence of monarch butterflies in our gardens and lives join Ann in hoping that this year there will be an abundance of these beautiful creatures everywhere that milkweeds are allowed to flourish."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:387f5aa5-c3f6-458b-a789-31b50a71ce7b>","<urn:uuid:e79892cd-cf35-44c8-aac9-0081c14fc42a>"],"error":null}
{"question":"What are the similarities between measuring wind speeds in Jupiter's atmosphere and studying exoplanet atmospheric composition?","answer":"Both fields rely on tracking specific molecules for measurement. In Jupiter's case, scientists track hydrogen cyanide and carbon monoxide left by the Shoemaker-Levy 9 comet to measure wind speeds using the doppler effect. Similarly, in exoplanet atmospheres, scientists track specific molecules like methane (CH4), carbon monoxide (CO), and water (H2O) to understand atmospheric composition. Both fields also face challenges with measurement at different atmospheric levels - Jupiter's stratospheric winds were difficult to measure due to lack of visible clouds, while exoplanet atmospheric composition becomes harder to determine at temperatures below 1500K due to photochemical effects.","context":["For exoplanets with T < ~1500 K, photochemistry can seriously affect the atmospheric gas-phase composition  — on the one hand by destructing major molecules such as carbon monoxide (CO), water (H2O), or methane (CH4) and on the other hand by enhancing the formation of more complex species such as acetylene (C2H2), hydrogen cyanide (HCN), heavier hydrocarbons or nitriles with more carbon atoms such as benzene (C6H6) [2, 3]. These disequilibrium processes have been considered when analyzing some observational data, highlighting that, in the case of highly irradiated exoplanets, photochemistry may be responsible for an observed chemical composition departing from the one predicted by thermochemical models [4, 5]. In addition, numerous observations suggest that aerosols are ubiquitous in a large variety of exoplanet atmospheres [6-8], including giant exoplanets. However, the nature (condensate clouds or photochemical hazes) of these aerosols and their properties remain largely unconstrained by these observations.\nLaboratory experiments are important to advance our understanding of photochemical processes and aerosols properties in exoplanet atmospheres. In our previous studies, we investigated experimentally the influence of photochemistry on the composition and the formation of photochemical aerosols in hot giant exoplanet atmospheres with T > 1000 K and different C/O ratios [9, 10]. Here we will present the results of new laboratory experiments focusing on warm atmospheres (T < 1000 K), for which CH4 is expected to be the main carbon carrier  instead of CO for the higher temperatures that we investigated previously. This particularity may be more favorable to a more efficient formation of hydrocarbons such as C2H2 or ethane (C2H6), making these planets good candidates to detect tracers of atmospheric photochemistry .\n2. Material and Methods\nTo simulate the photochemistry and the formation of aerosols in warm giant exoplanet atmospheres, we used the Cell for Atmospheric and Aerosol Photochemistry Simulations of Exoplanets (CAAPSE) experimental setup . A scheme of the setup is presented in Figure 1.\nThe cell was filled at room temperature with 15 mbar of either a H2:CH4:N2 (99%:0.5%:0.5%) or H2:CH4:H2O gas mixture with (98.4%:0.8%:0.8%). These compositions were chosen based on the main atmospheric constituents predicted for an exoplanet temperature of 500 K and a solar C/O ratio of 0.54 . The gases were heated at 5 K minute-1 to oven temperatures ranging from room temperature (~295 K) to 1073 K. After attaining the desired temperature, the gas mixture was irradiated with UV photons at 121.6 nm (Lyα) and 140-160 nm using a hydrogen microwave discharge lamp separated from the cell by a MgF2 window.\nThe evolution of the gas mixture composition was monitored using infrared spectroscopy in transmission.\n3. Results and Discussions\nWe found that photochemistry led to significant modifications in the gas-phase composition resulting in the consumption of CH4 and the formation of different photochemical products. The main hydrocarbon product is C2H6 in every studied condition while C2H2 and propane (C3H8) have also been detected in smaller amounts. In addition, we observed that the methane consumption efficiency and the hydrocarbon production yields vary significantly with the temperature. When the temperature increases, the methane consumption and the hydrocarbon production decrease. Finally, our results highlight that the production of hydrocarbons was more efficient in the experiments performed with the H2:CH4:N2 gas mixture than in the ones made with the H2:CH4:H2O gas mixture.\nIn the case of giant planet atmospheres with methane as the main carbon carrier, our results suggest that products of organic photochemistry, such as hydrocarbon molecules (C2H2, C2H6) and maybe photochemical organic aerosols, are more likely to be observed in planets with lower atmospheric temperatures and lower water amounts.\nThe research work was carried out at the Jet Propulsion Laboratory, California Institute of Technology, under a contract with the National Aeronautics and Space Administration. This work was supported by the NASA Exoplanet Research Program. B.F. thanks the Université Paris-Est Créteil (UPEC) for funding support (postdoctoral grant).\n1) Moses, J.I., Chemical kinetics on extrasolar planets. Philos Trans A Math Phys Eng Sci, 2014. 372(2014): p. 20130073.\n2) Moses, J.I., et al., Chemical Consequences of the C/O Ratio on Hot Jupiters: Examples from WASP-12b, CoRoT-2b, XO-1b, and. The Astrophysical Journal, 2013. 763(1): p. 25.\n3) Venot, O., et al., New chemical scheme for studying carbon-rich exoplanet atmospheres. A&A, 2015. 577: p. A33.\n4) Knutson, H.A., et al., 3.6 and 4.5 μm Phase Curves and Evidence for Non-Equilibrium Chemistry in the Atmosphere of Extrasolar Planet HD 189733b. The Astrophysical Journal, 2012. 754(1): p. 22.\n5) Roudier, G.M., et al., Disequilibrium Chemistry in Exoplanet Atmospheres Observed with the Hubble Space Telescope. The Astronomical Journal, 2021. 162(2): p. 37.\n6) Sing, D.K., et al., A continuum from clear to cloudy hot-Jupiter exoplanets without primordial water depletion. Nature, 2016. 529(7584): p. 59-62.\n7) Knutson, H.A., et al., A featureless transmission spectrum for the Neptune-mass exoplanet GJ 436b. Nature, 2014. 505(7481): p. 66.\n8) Kreidberg, L., et al., Clouds in the atmosphere of the super-Earth exoplanet GJ 1214b. Nature, 2014. 505(7481): p. 69-72.\n9) Fleury, B., et al., Photochemistry in Hot H2-dominated Exoplanet Atmospheres. The Astrophysical Journal, 2019. 871(2).\n10) Fleury, B., et al., Influence of C/O Ratio on Hot Jupiter Atmospheric Chemistry. The Astrophysical Journal, 2020. 899(2): p. 147.","An international team of astronomers just measured Jupiter’s raging stratospheric winds for the very first time—and they used a 27-year-old comet to do it.\nScientists had already measured wind speeds down in Jupiter’s troposphere—where the planet’s iconic stripes lie—and way up in its ionosphere. But this new study was first to take wind speed measurements of Jupiter’s stratosphere using the incredibly sensitive Atacama Large Millimeter/submillimeter Array (ALMA). They measured wind speeds near the equator and near the poles.\nSome results weren’t too surprising—they found that speeds at the equator were roughly what models had predicted. “But what was completely unexpected is what we saw near the poles,” says study-author Thibault Cavalié, a planetary scientist at the Laboratoire d’Astrophysique de Bordeaux who led the experiment. The team found 300- to 400-meter-per-second winds— roughly 700 to 900 miles per hour—whipping across the poles in unanticipated directions.\n“It’s a really hard observation,” says Imke de Pater, a planetary scientist at Berkeley who has used ALMA previously but was not part of this study. She adds, “it’s a really great paper, they really show very nicely these wind profiles in the … upper atmosphere.”\nJupiter’s winds almost exclusively go eastward or westward, as we see in the planet’s trademark red and white horizontal stripes. This rule holds for the troposphere—save for vortices like Jupiter’s red eye, where winds swirl like a hurricane. In the stratospheric layer above however, the winds instead appear to follow the shape of Jupiter’s auroral rings, which, like Earth’s northern lights, result from its magnetic field steering solar winds to the poles. Those auroral rings aren’t perfectly lined up with the poles, so the wind flow doesn’t match the neat bands of the troposphere.\nThe unusual polar wind patterns trekking north and south instead of staying east and west are “really mind boggling,” says Glenn Orton, senior research scientist and observational astronomer at NASA’s Jet Propulsion Laboratory, who wasn’t involved in the study.\nFor decades, the easiest way to figure out planetary wind speeds was to simply take a snapshot of the planet, then another one some time later and see how far the clouds moved between the two frames, Cavalié says. But at higher altitudes this doesn’t work, because the winds are invisible. There are no clouds to track.\nBut ever since the Shoemaker-Levy 9 comet impact on Jupiter back in 1994, researchers have kept tabs on two compounds the object delivered there: hydrogen cyanide and carbon monoxide. Both chemicals are long-lived, and they’re still floating around in the jovian atmosphere. The team was able to trace the unique spectral fingerprints of the hydrogen cyanide and carbon monoxide. Since they could track winds using the movement of clouds, perhaps they could use these molecules to do the same.\nTo do so, the team first pinpointed both types of molecules by detecting their frequencies. Then, they made use of something called the doppler effect, which means those frequencies change depending on if the molecules are moving toward us, or away from us. So on Jupiter, as the molecules blew towards the telescope, they would produce slightly different spectral signals than those moving away. By measuring the difference—how much the frequencies got bumped—the team could measure the speed at which the molecules (and the wind) were moving.\nIn the future, Cavalié says, space telescopes may be able to learn more from the water deposited by the comet’s impact, because water is such a rare molecule on Jupiter. The study is also a stepping stone, he says, for the European Space Agency’s JUpiter ICy moons Explorer (JUICE) mission which plans to launch next year. That craft will get close looks at Jupiter and three of its moons and be the first to orbit Ganymede—the largest moon in the solar system.\nCavalié was 12 years old when the Shoemaker-Levy 9 comet hit Jupiter—a little young to be part of that observation. But he says the event nudged him towards a career in planetary science.\nYears later, the comet is still making its mark."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:e283bc47-205b-4f64-9e0c-567d230d27aa>","<urn:uuid:0bd7c5f4-1523-4cd6-8032-7741cb352b6c>"],"error":null}
{"question":"What's the relationship between conspiracists and crypto-assets in terms of environmental theories and actual climate impact?","answer":"While conspiracists often promote unproven theories and reject standard explanations of events, the environmental impact of crypto-assets is well-documented through concrete data. Crypto-assets generate significant greenhouse gas emissions, producing 140 ± 30 million metric tons of carbon dioxide per year globally (0.3% of global emissions). In the US alone, crypto-asset operations consume 0.9% to 1.7% of total electricity usage, comparable to all home computers or residential lighting, and generate 25 to 50 Mt CO2/y (0.4% to 0.8% of US greenhouse gas emissions).","context":["Words nearby conspiracist\nMORE ABOUT CONSPIRACIST\nWhat does conspiracist mean?\nA conspiracist is someone who creates, promotes, or believes in a conspiracy theory—an explanation of an event that claims it was the result of a secret and often complex and evil plot by multiple people.\nConspiracy theories and the conspiracists who promote or formulate them often reject the standard or accepted explanation of unexplained or unusual events and claim that they are the doing of evil conspirators secretly conspiring behind the scenes.\nConspiracy most commonly means a secret plan by multiple people to do something evil or illegal. Conspiracy can also refer to the act of making such plans—the act of conspiring—or to the group making the plans. The people involved can be called conspirators.\nIn conspiracy theory, the word theory is used in a general way to refer to a proposed explanation that has not been proven. But conspiracists don’t usually treat such theories as just guesses—they often promote them as fact, no matter how bizarre or far-fetched they may be.\nMost conspiracy theories involve supposedly secret knowledge of the supposedly secret and evil dealings of powerful people, especially politicians, government officials, billionaires, and celebrities. Such plots are often claimed to have the goal of controlling world events and ordinary people.\nA close synonym of conspiracist is conspiracy theorist. A less common synonym is conspiratorialist.\nExample: Conspiracists don’t care that their theories have been repeatedly debunked—in fact, they take any criticism as proof that they’re right.\nWhere does conspiracist come from?\nThe first records of the word conspiracist come from the early 1970s. It ultimately derives from the Latin verb conspīrāre, meaning “to act in harmony” or “to conspire.” This comes from the combination of con-, meaning “together,” and spīrāre, “to breathe.” The suffix -ist indicates a person who practices something or holds certain principles.\nWhen people hear the word conspiracy, they often think of shady people making shady plans in shady backrooms. The word typically implies both secrecy and evil—people involved in conspiracies are up to no good and they’re trying to hide it. Most conspiracy theories are about this kind of thing. The classic image of a conspiracy theory is that of a bulletin board with strings connecting photos of supposed conspirators and newspaper clippings of seemingly random events. In this way, conspiracists often ignore the obvious or simple explanation in favor of an interpretation that tries to tie together unrelated elements in a convoluted way. Conspiracists are often drawn to such theories because of the appeal of having secret knowledge (the “real” story) that the rest of the population is unaware of. The internet has increased the reach of conspiracy theories, raising serious concerns about how they contribute to the spread of misinformation and disinformation.\nDid you know ... ?\nWhat are some other forms related to conspiracist?\n- conspiracy (noun)\nWhat are some synonyms for conspiracist?\n- conspiracy theorist\nWhat are some words that share a root or word element with conspiracist?\nWhat are some words that often get used in discussing conspiracist?\nHow is conspiracist used in real life?\nThe term conspiracist is typically used in a negative way to criticize a person for promoting absurd explanations of events.\nIt's one of life's great ironies that the so-called 'skeptical' / free-thinking conspiracist mind should so freely submit to the most easily debunked, nonsense theories. I wouldn't have Mr Armstrong's patience if repeatedly confronted with this stupidity. https://t.co/BaIziNKPRe\n— Public Service Broadcasting (@PSB_HQ) June 17, 2020\nJust at the dentist and got chatting to a really nice woman outside – she was probs in her 40s. We said how the distancing thing still doesn’t feel that normal. She then said ‘I’m not a conspiracist but I do just think it’s a flu that we get every year, it’s all the media’ 🙃\n— jack rem x (@jackremmington) June 19, 2020\nI've already gone through this with one fact-denying conspiracist. You can scream your lies at the sky, doesn't change the fact that you're absolutely wrong.\n— राजनीति जानने वाला🌐🌊♻️🧦🇪🇺🏳️🌈🦅 (@Saruboii1) July 6, 2020\nTry using conspiracist!\nIs conspiracist used correctly in the following sentence?\nYou know the world is weird when the real news reads like a conspiracist’s blog post.\nHow to use conspiracist in a sentence\nThat personal connection, West believes, is part of why UFO conspiracists can be so intense.UFO conspiracies can be more dangerous than you think|Corinne Iozzio|July 8, 2021|Popular-Science\nThe conspiracy theories created the odd situation where well-meaning people feared speaking out against human trafficking because it could lump them in with radical conspiracists.\nIt has become a safe haven for hate group members, conspiracists, and people who have been banned elsewhere online.\nFor people who don’t know, this refers to what happens when there’s a news event, or something like that, and it’s only conspiracists or propagandists trying to inject disinformation into the public sphere.Our misinformation problem is about to get much, much worse|Sean Illing|October 6, 2020|Vox\nThe conspiracists notably have been deemed a domestic terror threat by the FBI.","FACT SHEET: Climate and Energy Implications of Crypto-Assets in the United States\nClimate change is one of the most pressing problems confronting our nation and our world, and President Biden has taken bold steps to address it with legislation and policy. Among the President’s commitments are: protecting communities from pollution, reducing greenhouse gas emissions by 50% by 2030, achieving a carbon pollution-free electricity grid by 2035, and reaching net-zero greenhouse gas emissions no later than 2050.\nTo achieve these ambitious goals, we must ensure that emerging technologies contribute to a net-zero, clean energy future. The use of digital assets based on distributed ledger technology (DLT) is expanding. Digital assets are a form of value, represented digitally. As an emerging technological innovation, digital assets have provided some benefits and value for some residents and businesses in the United States, and have the potential for future benefits with emerging uses.\nCrypto-assets are digital assets that are implemented using cryptographic techniques. Crypto-assets can require considerable amounts of electricity usage, which can result in greenhouse gas emissions, as well as additional pollution, noise, and other local impacts to communities living near mining facilities. Depending on the energy intensity of the technology and the sources of electricity used, the rapid growth of crypto-assets could potentially hinder broader efforts to achieve U.S. climate commitments to reach net-zero carbon pollution.\nIn March, in Executive Order 14067 on Ensuring the Responsible Development of Digital Assets, President Biden made clear that the responsible development of digital assets includes reducing negative climate impacts and environmental pollution. The Executive Order directed the White House Office of Science and Technology Policy (OSTP), in coordination with other federal agencies, to produce a report on the climate and energy implications of crypto-assets in the United States. OSTP assembled an interdisciplinary team of experts to assess and extend existing studies with new analysis, based on peer-reviewed studies and the best available data.\nToday, OSTP published its report, examining the challenges and opportunities of crypto-assets for the United States’ clean energy and climate change goals, and providing a set of recommendations to further study and track impacts of the sector, develop potential performance standards, and provide tools and resources to reduce negative impacts. This report’s assessment and recommendations align with federal actions that reduce greenhouse gas emissions to protect public health and welfare, grow a clean energy economy with good-paying jobs, and improve environmental justice.\nCrypto-Assets Can Be Energy-Intensive, and the United States Has a Major Crypto-Asset Sector\nFrom 2018 to 2022, annualized electricity usage from global crypto-assets grew rapidly, with estimates of electricity usage doubling to quadrupling. As of August 2022, published estimates of the total global electricity usage for crypto-assets are between 120 and 240 billion kilowatt-hours per year, a range that exceeds the total annual electricity usage of many individual countries, such as Argentina or Australia. This is equivalent to 0.4% to 0.9% of annual global electricity usage, and is comparable to the annual electricity usage of all conventional data centers in the world.\nNearly all crypto-asset electricity usage is driven by consensus mechanisms: the DLT used to mine and verify crypto-assets. The dominant consensus mechanism is called Proof of Work (PoW), which is used by the Bitcoin and Ethereum blockchains. Bitcoin and Ether combined represent more than 60% of total crypto-asset market capitalization. The PoW mechanism is designed to require more computing power as more entities attempt to validate transactions for coin rewards, and this feature helps disincentivize malicious actors from attacking the network. As of August 2022, Bitcoin is estimated to account for 60% to 77% of total global crypto-asset electricity usage, and Ethereum is estimated to account for 20% to 39%.\nThe energy efficiency of mining equipment has been increasing, but electricity usage continues to rise. Other less energy-intensive crypto-asset ledger technologies exist, with different attributes and uses. Switching to alternative crypto-asset technologies such as Proof of Stake could dramatically reduce overall power usage to less than 1% of today’s levels.\nThe United States is estimated to host about a third of global crypto-asset operations, which currently consume about 0.9% to 1.7% of total U.S. electricity usage. This range of electricity usage is similar to all home computers or residential lighting in the United States. Crypto-asset mining is also highly mobile. The United States currently hosts the world’s largest Bitcoin mining industry, totaling more than 38% of global Bitcoin activity, up from 3.5% in 2020. Despite the potential for rapid growth, future electricity demand from crypto-asset operations is uncertain, demonstrating the need for better data to understand and monitor electricity usage from crypto-assets.\nCrypto-Assets Can Have Significant Environmental Impacts\nGlobal electricity generation for the crypto-assets with the largest market capitalizations resulted in a combined 140 ± 30 million metric tons of carbon dioxide per year (Mt CO2/y), or about 0.3% of global annual greenhouse gas emissions. Crypto-asset activity in the United States is estimated to result in approximately 25 to 50 Mt CO2/y, which is 0.4% to 0.8% of total U.S. greenhouse gas emissions. This range of emissions is similar to emissions from diesel fuel used in railroads in the United States.\nBesides purchased grid electricity, crypto-asset mining operations can also cause local noise and water impacts, electronic waste, air and other pollution from any direct usage of fossil-fired electricity, and additional air, water, and waste impacts associated with all grid electricity usage. These local impacts can exacerbate environmental justice issues for neighboring communities, which are often already burdened with other pollutants, heat, traffic, or noise. The growth of energy-intensive crypto-asset technologies, when not directly using clean electricity, could hinder the ability of the United States to achieve its National Determined Contribution under the Paris Agreement, and to avoid the most severe impacts of climate change. Broader adoption of crypto-assets, and the potential introduction of new types of digital assets require action by the federal government to encourage and ensure responsible development. This includes minimizing negative impacts on local communities, significantly reducing energy intensity, and powering with clean electricity.\nDistributed Ledger Technologies May Help with Climate Monitoring or Mitigation\nDLT may have a role to play in enhancing market infrastructure for a range of environmental markets like carbon credit markets, though other solutions might work as well or better. The potential benefits of DLT would need to outweigh the additional emissions and other environmental externalities that result from operations to merit broader use, relative to the markets or mechanisms that DLT displaces. Use cases are still emerging, and like all emerging technologies, there are potential positive and negative use cases yet to be imagined. Responsible development of this technology would encourage innovation in DLT applications while reducing energy intensity and minimizing environmental damages.\nKey Recommendations of the Report\nTo help the United States meet its climate objectives, crypto-asset policy during the transition to clean energy should be focused on several objectives: reduce greenhouse gas emissions, avoid operations that will increase the cost of electricity to consumers, avoid operations that reduce the reliability of electric grids, and avoid negative impacts to equity, communities, and the local environment.\nTo ensure the responsible development of digital assets, recommendations include the following actions for consideration:\n- Minimize greenhouse gas emissions, environmental justice impacts, and other local impacts from crypto-assets: The Environmental Protection Agency (EPA), the Department of Energy (DOE), and other federal agencies should provide technical assistance and initiate a collaborative process with states, communities, the crypto-asset industry, and others to develop effective, evidence-based environmental performance standards for the responsible design, development, and use of environmentally responsible crypto-asset technologies. These should include standards for very low energy intensities, low water usage, low noise generation, clean energy usage by operators, and standards that strengthen over time for additional carbon-free generation to match or exceed the additional electricity load of these facilities. Should these measures prove ineffective at reducing impacts, the Administration should explore executive actions, and Congress might consider legislation, to limit or eliminate the use of high energy intensity consensus mechanisms for crypto-asset mining. DOE and EPA should provide technical assistance to state public utility commissions, environmental protection agencies, and the crypto-asset industry to build capacity to minimize emissions, noise, water impacts, and negative economic impacts of crypto-asset mining; and to mitigate environmental injustices to overburdened communities.\n- Ensure energy reliability: DOE, in coordination with the Federal Energy Regulatory Commission, the North American Electric Reliability Corporation and its regional entities, should conduct reliability assessments of current and projected crypto-asset mining operations on electricity system reliability and adequacy. If these reliability assessments find current or anticipated risks to the power system as a result crypto-asset mining, these entities should consider developing, updating, and enforcing reliability standards and emergency operations procedures to ensure system reliability and adequacy under the growth of crypto-asset mining.\n- Obtain data to understand, monitor, and mitigate impacts: The Energy Information Administration and other federal agencies should consider collecting and analyzing information from crypto-asset miners and electric utilities in a privacy-preserving manner to enable evidence-based decisions on the energy and climate implications of crypto-assets. Data should include mining energy usage and fuel mix, power purchase agreements, environmental justice implications, and demand response participation. OSTP could establish a National Science and Technology Council subcommittee to coordinate with other relevant agencies to assess the energy use of major crypto-assets.\n- Advance energy efficiency standards: The Administration should consider working with Congress to enable DOE and encourage other federal regulators to promulgate and regularly update energy conservation standards for crypto-asset mining equipment, blockchains, and other operations.\n- Encourage transparency and improvements in environmental performance: Crypto-asset industry associations, including mining firms and equipment manufacturers, should be encouraged to publicly report crypto-asset mining locations, annual electricity usage, greenhouse gas emissions using existing protocols, and electronic waste recycling performance.\n- Further research to improve understanding and innovation: For improved analytical capabilities that can enhance the accuracy of electricity usage estimates and sustainability, the National Science Foundation, DOE, EPA and other relevant agencies could promote and support research and development priorities that improve the environmental sustainability of digital assets, including crypto-asset impact modeling, assessment of environmental justice impacts, and understanding beneficial uses for grid management and environmental mitigation. Research and development priorities should emphasize innovations in next-generation digital asset technologies that advance U.S. goals in security, privacy, equity, and resilience, as well as U.S. climate goals."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:555d8567-be5b-469a-b275-45619409124f>","<urn:uuid:c2b3e9e1-0ef3-465d-9a23-92fdbe2309a6>"],"error":null}
{"question":"I've been studying Earth's early history. When comparing the atmospheric composition of the Hadean eon with the Silurian period, what major changes occurred and how did they impact life?","answer":"The atmosphere underwent dramatic changes between these two periods. During the Hadean (4.6 billion years ago), the atmosphere was composed of water vapor, methane, CO2, nitrogen, hydrogen, inert gases, and acidic fumes, with no free oxygen initially present. Oxygen first formed from the disintegration of H2O and CO2 under the influence of sun rays. By the Silurian period (443.8-419.2 million years ago), oxygen levels had increased significantly, though they were still only at 14% of the atmosphere (about 30% less than today). This oxygen increase, coupled with high CO2 levels creating a greenhouse effect, was crucial for supporting the first land-based organisms like early vascular plants and the first air-breathing animal, Pneumodesmus newmani.","context":["HomeEarthSilurian Earth – The First Breath of Air Charles December 9, 2016 Earth, Series 1 The Silurian Period 443.8 to 419.2 million years ago Ryan Somma Maritime life thrived throughout the Silurian period, with nautiloids (centre photo) being the most successful and largest predators at the time. Following the devastating mass extinctions at the end of the Ordovician Period, the glaciers covering the ancient land of Gondwana receded, and another period of intense global warming began. Afterwards, the very first animals started to settle the land, forming the earliest terrestrial ecosystems and jumpstarting a new phase in the evolution of our planet. In the sixth part of my “A Journey through the History of Earth” series, we’ll be exploring the first of these primordial land-based biomes. Part 1: Hadean Earth – The Violent Creation of Our World Part 2: Archean Earth – Signs of Life Part 3: Proterozoic Earth – The First Animals Part 4: Cambrian Earth – An Explosion of Evolution Part 5: Ordovician Earth – Colonising a Barren Land Part 6: Silurian Earth – The First Breath of Air Part 7: Devonian Earth – The Age of Fishes and Forests Part 8: Carboniferous Earth – The Age Bugs Part 9: Permian Earth – The Age of Amphibians Part 10: Triassic Earth – The Rise of the Dinosaurs Part 11: Jurassic Earth – The Land of Giants Part 12: Cretaceous Earth – The Reign of Tyrants Part 13: Paleogene Earth – The Rise of Mammals Part 14: Neogene Earth – Human Ancestors Part 15: Quaternary Earth – The Age of Man Despite being by far the shortest of the Palaeozoic periods, lasting less than 25-million years, the Silurian saw one of the most important evolutionary events in the history of our world. Although the Ordovician had seen the first primordial mosses colonise coastal areas around the world and the first curious arthropods had started to explore the land, it wasn’t until the middle of the Silurian that the first terrestrial ecosystems became developed enough to function independently of the sea. The Silurian is the third geological period of the Phanerozoic aeon and the third of the Palaeozoic Era. Like the Ordovician and the Cambrian before it, the name ‘Silurian’ was inspired by the country of Wales where many fossils dating from this time have been identified. The Silurian period was first described and identified in 1835, and it was named after the ancient Celtic Silures tribe, who were contemporaries of the Ordovices some 2,000 years ago. Highlights of the Silurian Rapid global warming Evolution of the first bony fish First vascular plants settle the land The first sharks Giant fungus dominates terrestrial ecosystems First creature to take a breath of air Global Warming Redefines the Path of Evolution 443.8-million years ago, the glaciers of the Late Ordovician ice age started to melt, and the sea level rose rapidly, reaching a peak 590 feet (180 m) higher than they are today. Once again, Earth went through an unprecedented period of global warming, lifting the shackles on evolution and allowing early arthropods and brachiopods (worms) to once again continue their exploration of the land. At this time, by far the largest continent was Gondwana, comprising parts of what is now Antarctica and Australia and located in the southeast of the map. The smaller continents of Siberia and Baltica shrank with the rising sea levels, gradually shifting further northwest of the map into the vast Panthalassic Ocean. During the Early Silurian, the only known multicellular life that had permanently adapted to life on the land were tiny liverwort-type plants forming mossy growths around the shorelines. Nonetheless, the spread of such organisms formed an essential foundation for the first truly land-based ecosystems. Until then, the primitive terrestrial plant life of the Ordovician and Early Silurian was still heavily reliant on the water. Oxygen levels in the Earth’s atmosphere continued to rise slowly but steadily thanks to the continued spread of photosynthetic organisms. At the same time, early plant life made its journey from the tidal shallows and gradually spread further inland as it became less dependent on the ocean’s waters for sustenance and reproduction. Nonetheless, oxygen still only accounted for 14% of the atmosphere during the Silurian, which is some 30% less than it is today. Earth continued to warm throughout the first half of the Silurian period, eventually reaching an average global temperature some 3 °C higher than it is today. As the planet recovered from the ice age, life once again started to thrive and evolve, and the dark times of the Late Ordovician extinction event, one of the most severe in Earth’s history, were long behind. Miniature Forests Crawl across the Land MUSE Science Museum Cooksonia is the by far the best known and iconic plant fossil of the Silurian period. In real life, the plant was extremely small. Cooksonia is perhaps the most iconic of all Silurian fossils. One of the earliest known true plants, this tiny leafless organism quickly colonised shorelines in many parts of the world during the middle of the Silurian period. Several species have been identified, and it’s widely believed that they grew in great abundance. Nonetheless, the largest were no longer than a couple of inches (5 cm), forming expanses of miniaturised ‘forests’ in swampy areas. Cooksonia is most notable for being the earliest known vascular plant (tracheophytes), a group of plants that includes trees and all other land plants that have waxy layers to prevent water from escaping – something that’s essential for land-based life. Arthur Weasley Guiyu oneiros is one of the earliest bony fish known. Living during the Late Silurian around 419-million years ago, it was also one of the largest fish of its time. While tiny plants were crawling out of the shallows, life in the oceans continued to expand and diversify, with coral reefs stretching far and wide and giving rise to ever more sophisticated ecosystems. Silurian sea life included the first bony fish, the foot-long (30 cm) guiyu oneiros being one of the largest and best known. Most notable, however, were the eurypterid sea scorpions, a highly successful order of marine predators which are distantly related to arachnids. The ancestors to sharks also appeared during the Silurian, although there is evidence that the earliest sharks had their beginnings in the Ordovician. Other already well-established groups, such as nautiluses, marine gastropods, trilobites and brachiopods, also continued to thrive and diversify throughout the Silurian. The First Ever Breath of Air Matteo De Stefano/MUSE Pneumodesmus newmani is the first known animal to have ever lived permanently on the land. The tiny creature was no larger the a woodlouse, and probably fed on mosses. In 2004, palaeontologists in Scotland found the definitive evidence of the earliest animal to live on dry land. The fossil was 428-million years ago, and it belonged to a millipede one centimetre long. Even more remarkably, this discovery put back the date of the first terrestrial animal by some 20-million years. Named pneumodesmus newmani after its amateur palaeontologist discoverer Martin Newman, this animal was one of the earliest to breath the air, representing a profound step forward in the evolution of life. Indeed, it might have just been a tiny millipede, but it’s incredible to think that, if it hadn’t been for this enterprising little character, evolution may have taken a very different course. The latter half of the Silurian was moderately warm, although there was probably still a southern polar icecap covering a part of what is now Africa. Oxygen levels were continuing to increase due to the spread of early land plants, and high carbon dioxide levels kept the world in a strong greenhouse climate with high sea temperatures. These factors combined, along with the essential role played by the lunar tides, to encourage the evolution of larger animals that would eventually migrate out of the tidal shallows and colonise the land to such an extent that they would transform it beyond recognition in the following Devonian period. Joining cooksonia in its conquest of the land was another now long-extinct clubmoss known as baragwanathia, also a type of vascular plant and one that grew over a metre in length. Like the otherwise unrelated cooksonia, it spread its spores in the wind to reproduce, meaning that it was independent of the oceans. Giant Mushrooms Take Over Mary Parrish Prototaxites was long assumed to be a primitive plant until it was eventually determined to be a tree-sized fungus. In the mid-nineteenth century, a bizarre discovery was made of what looked like an extremely ancient fossilized trunk of a conifer dating from the Late Silurian. For almost 150 years, it was assumed to be a very early tree, but the fact that it was much, much bigger than any other terrestrial organism of the time kept everyone baffled. Prototaxites, as it was named, grew up to 26 feet (8 metres) in height and had a trunk-like structure up to 3 feet (1 metre) wide. A century and a half after the its discovery, prototaxites was eventually determined to be a fungus, probably belonging to the nematophyta phylum which included land-based algae from as early as the Cambrian period. A lot of unanswered questions remain surrounding this incredibly bizarre lifeform, but one thing seems certain: the Late Silurian landscape was dominated by spire-shaped pillars of life that were actually some of the largest mushrooms that ever existed. Conclusion The Silurian ended 419.2-million years ago with the end of the Přídolí Epoch, so named after a region near the Czech capital Prague where extensive fossils of cephalopods, bivalves and trilobites were found. Although terrestrial life was still scarce, and had yet to make a significant impact on regions further inland, that was about to change dramatically. Soon, the alien world that was the Silurian Earth would end up being covered by vast swathes of primordial forests, characterising the beautifully colourful Devonian period that we’ll be exploring in the next episod Share this:Click to share on Twitter (Opens in new window)Click to share on Facebook (Opens in new window)Click to share on Reddit (Opens in new window)Click to share on LinkedIn (Opens in new window)Click to share on Pinterest (Opens in new window)Click to share on WhatsApp (Opens in new window)Click to email this to a friend (Opens in new window) One Response Jeffrey Newman August 19, 2021 This is a fascinating digest and really good for a non-scientist. Do you tweet and how do I access your blogs? Reply Leave a Reply Cancel ReplyYour email address will not be published.CommentName* Email* Website Please enter an answer in digits:20 − 19 = This site uses Akismet to reduce spam. Learn how your comment data is processed.","Our Earth is about 4.5 billion years old. It is roughly represented by the column of sedimentary rocks now present on the earth. In this record the time elasped during the formation of unconformities is missing. The unconformities are however, important because they subdivide the geological time into smaller units. On this basis a standard geological Time scale has been prepared which is used universally for the correlation of rock formations. Geological time scale is divided into Eon, Era, Period and Epoch.\nImage taken from http://www.stratigraphy.org/index.php/ics-chart-timescale\nGeological Timescale Free Download ( Special Thanks to International Commission on Stratigraphy)\nThere are 2 Eons namely\n1. Hadean (Age – 4.6 billion years)\n- Indirect photosynthesis evidence of precambrian life.\n- Oldest known mineral. i.e ZIRCON\n- Crust was thin in the 1st stage of Earth’s history.\n- Composition of atmosphere was- vapour, methane, Co2, Nitrogen, Hydrogen, Inert gases and acidic fumes.\n- Hydrogen and Helium was later dissipated into universe.\n- Free Oxygen was 1st formed from the disintegration of H2o and Co2 under the influence of Sun rays in the upper layer of the earth’s atmosphere.\n- Proportion of Oxygen in the atmosphere increased to its present leavel.\n- Formation of hydrosphere and atmosphere lead to intensive erosion and deposition of sedimentary rocks intercalated with volcanic rocks.\n2. Archean (Age – 400 my to 2500 my)\n- Stabilization of most modern cratons.\n- First stromatolite (probably colonial Cyanobacteria).\n- Oldest Microfossil was of this era.\n- First known Oxygen producing bacteria.\n- Hydrosphere contained dissolved gaseous product of volcanism such as HCL , HF, Basic Acid, H2S, Co2, CH4, Highly acidic water with silica in dissolved state.\n- Extensive volcanism and unusual composition of atmosphere are reflected in the chemical composition of rocks making them distinct from younger rock formations.\n3. Proterozoic (Age 2500 my to 541 my)\n- It begins with tectonic zonation of earth surface into sedimentary basins of platform and geosynclinal types. Such zonation was accomplished at somewhat differing time aver different continents.\n- Composition of Hydrosphere and atmosphere under wet very little change during the transition period of Archean and Proterozoic eras.\n- Carbon Dioxide content in hydrosphere declined.\n- Volcanic products such as Sulphur and H2S of hydrosphere were converted into sulphates in the presence of Oxygen which insure deposited in the sedimentary succession of the time.\n- Deposition of carbonate rocks( limestone and Dolomite) 1st rock of evaporite facies was phosphorite deposited in regressing marine basin.\n- Early forms were not having hard parts so could not preserved as fossils.\n- Imprints of Algal life preserved in the form of Stromatolites commonly observed in proterozoic succession.\nPrecambrian is divided into 3 Era\nPaleozoic ( Age 541 my to 262 my)\nPaleozoic is divided into 6 Periods\n1. Cambrian ( Age 541 my to 485 my)\n- Major diversification of life in cambrian explosion.\n- Numerous fossils, most modern phyla apppeared.\n- 1st Chordates appeared.\n- Trilobites, Sponges, Inarticulate sponges brachiopods and various other animals appeared.\n- Gondwana Emerged.\n- Climate was mild.\n2.Ordovician ( Age 485 my to 443 my)\n- Invertebrates diversified into many new types.\n- Early Corals, Bivalves, Nautiloids and many types of Echinoids were common.\n- Conodonts (early planktonic vertebrate) appeared.\n- First green plant and fungi on land appeared.\n- Ice age at the end of the period\n3.Silurian ( Age 443 my to 419 my)\n- First vesicular plant appeared.\n- First Inwed Fishes appeared as well as many armoured jawless fishes populated the seas.\n- Brachiopods and Crinoids were abundant.\n- Trilobites and Mollusks diversified.\n4.Devonian ( Age 419 my to 358 my)\n- 1st Club mosses and ferns appeared, as do the first seed bearing plant.\n- Trilobites declined while jawless fishes and Sharks ruled the sea.\n- 1st Amphibian still aquatic.\n- Mountains of North America and Appalachian mountains of North America were seen.\n5.Carboniferous ( Age 358 my to 298 my)\n- First reptiles and Coal Forests.\n- Highest ever atmospheric Oxygen levels.\n- First land Vertebrate.\n- Rhizodonts were dominate big freshwater predators.\n- In Ocean early sharks are common and quite diverse.3.\n6.Permian ( Age 298 my to 262 my)\n- Landmasses units into supercontinent Pangea, creating the Appalachians.\n- End of Permo Carboniferous Glaciation.\n- Reptiles become plentiful while amphibians remain common.\n- In the mid permian, coal age flora are replaced b cone bearing gymnosperms and by 1st true mosses.\n- Marine life flourished in warm shallow reefs.\n- Triassic extinction event occurred 257 mya: 95% of life on earth become extinct.\nMESOZOIC ( Age 262 my to 66 my)\nMesozoic is divided into 3 Periods\n1.Triassic ( Age 262 my to 201 my)\n- Archosaurs dominated on land as Dinosaurs, and in air as Pterosaurs.\n- 1st mammal and Crocodile appeared.\n- Modern Corals and fish appeared.\n- Andean orogeny in South America.\n2.Jurassic ( Age 201 my to 145 my)\n- Gymnosperms, Conifers and ferns were common.\n- Many types of Dinosaurs evolved.\n- Mammals were common but small.\n- 1st Bird and lizard appeared.\n- Sea Urchins were common.\n- Carbon Dioxide levels in Atmosphere were 4-5 times mo re than the present day.\n- Breakup of Pangea into Gondwana and Laurasia.\n3.Cretaceous ( Age 145 my to 66 my)\n- Flowering plant along with new type of insects evolved.\n- Ammonoidea, Belemnites, Bivalves, Echinoids were common.\n- New type of Dinosaur appeared on land and modern snake in sea,\n- Atmospheric Co2 close to present day levels.\nCENOZOIC ( Age 66 my to Recent)\nCenozoic is divided into 3 Periods\n1.Paleogene ( Age 66 my to 23 my)\n- Climate was warm.\n- Rapid evolution and diversification of fauna.\n- Appearance of several ” Modern” mammal families.\n- Primitive Whales appeared and diversified.\n- First Grass.\n- Modern plant appeared.\n- Mammals diversified into numbers of primitive lineages following the extinction of dinosaurs.\n2.Neogene ( Age 23 my to 2.5 my)\n- Widespread forests slowly draw in massive amounts of Co2, lowering the level of atmospheric Co2.\n- First APE appeared.\n- Horses and Mastodons diversified.\n- Modern mammals and bird families become recognizable.\n- Moderate climate, punctuated by Iceage.\n- Orogeny in Northern Hemisphere.\n3.Quaternary ( Age 2.5 my to Recent)\n- Human Stone age culture with increasing technical relative to previous Ice age cukture in Pleistocene.\n- Quaternary age continues with glaciation and fluctuation from 100 to 300 ppmv in atmospheric Co2 levels, further intensification of Icehouse earth conditions, roughly 1.6My.\n- Last Glaciation period (18000-15000 yrs ago).\n- In Holocene, Quaternary Ice Age reduces and the current interglacial bigens, rise of human civilization.\n- Agriculture begins in Holocene.\n- Flourishing and then extinction of many large mammals."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:79594232-b568-4b3a-91c6-2311936e9ad9>","<urn:uuid:3a0619a1-a7bc-4cbf-ab5f-77bcaa98a03a>"],"error":null}
{"question":"What are the fundamental principles of the ACCESS project regarding education of students with significant intellectual disabilities?","answer":"The ACCESS project is built on three core beliefs: 1) All students deserve access to challenging and purposeful curriculum and instruction, 2) All students with disabilities should have opportunities to be educated with and learn in classrooms with their peers, and 3) All students should be valued as unique and contributing members of their school community.","context":["Today, most children with Down syndrome are educated in the regular classroom, along with their peers without disabilities. IDEA 2004 states that each school system must ensure that:\nSpecial classes, separate schooling, or other removal of children with disabilities from the regular educational environment occurs only if the nature or severity of the disability is such that education in regular classes with the use of supplementary aids and services cannot be achieved satisfactorily. (IDEA 2004, Title I B12a5)\nIn addition, IDEA 2004 also states:\nA child with a disability [may not be] removed from education in age-appropriate regular classrooms solely because of needed modifications in the general education curriculum (IDEA 2004 B 300.116)\nLeast Dangerous Assumption by C. Jorgensen The concept states, “that in the absence of absolute evidence, it is essential to make the assumption, that if proven false would be the least dangerous to the individual” (Anne Donnellan, 1984).\nIEP – A Guide to Participation for Parents (PDF) - An introduction to the IEP process.\nPlacement Decisions to consider when looking at the Least Restrictive Environment.\nDCAS-Alt 1 FAQs (PDF) – Information and resources related to the state’s alternate assessment, DCAS-Alt1.\nACCESS Project – promotes access to the general education curriculum for students with significant intellectual disabilities. The ACCESS project supports the beliefs that:\n- All students deserve access to curriculum and instruction that is challenging and purposeful\n- All students with disabilities should have opportunities to be educated with, and learn in, classrooms with their peers\n- All students should be valued as unique and contributing members of their school community\nWrights Law – provides accurate, reliable information about special education law, education law, and advocacy for children with disabilities.\nModifications vs Accommodations – An easy to read chart to clarify the difference between modifications and accommodations.\nAccommodation is defined as a support or service that is provided to help a student fully access the general education curriculum or subject matter. An accommodation does not change the content of what is being taught.\nModification is defined as a change to the general education curriculum or other material being taught. The teaching strategies are modified so the material is presented differently and/or the expectations of what the student will master are changed.\nIEP Tips IEP and Inclusion Tips for Parents and Teachers. A resource written by Anne I. Eason, Attorney-at-Law, and Kathleen Whitbread, Ph.D.\n2013 Access Symposium – Michael Remus and Bree Jimenez presented at the fall 2013 ACCESS Project Conference for parents and educators. The power point presentations can be viewed below. If there are any questions, comments or concerns, please feel free to contact Judi MacBride from the ACCESS Project with the Center for Disabilities Studies at 302-831-1052 (phone), or email@example.com\nMichael Remus - Inclusive Practices-Positive Advocacy from Perceptions to Practice: A Plan for Educating All Kids\nBree Jimenez, PhD - Evidence Based Practices to Support Standards based Instruction: A Focus on Everyday Practices\nWe All Belong is a documentary film exploring the issues of belonging and diversity in our classrooms. It profiles real parents and the dreams and fears they share. This film is a frank discussion of civil, human, and educational rights and offers a window into best practices and the struggle families face to achieve inclusion for their children. Focus: School Age, Pre-K/Kindergarten"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:7ba8123c-b0ee-454c-84bd-caf13b1aa7f6>"],"error":null}
{"question":"How does the current approach to web application security testing reflect the evolution of software security threats and vulnerabilities?","answer":"Current web application security testing reflects a significant evolution in response to modern threats. While traditional concerns like memory management issues remain critical (as evidenced in the SANS Top 25), there's now a strong focus on trust-related vulnerabilities and access control. Manual penetration testing has adapted to address both technical vulnerabilities and broader security concerns, including OSINT gathering, credential breach searches, and business logic testing. The methodology has evolved to encompass modern attack vectors like cloud service misconfigurations and API testing, while also maintaining coverage of traditional vulnerabilities. This evolution aligns with the current reality where organizations can no longer afford to compromise on security, as anything can be a target and anyone can be an attacker, requiring security to be embedded throughout the development lifecycle.","context":["Web Application Penetration Testing Services\nLocate and Remediate Your Application Security Flaws\nOWASP Top 10\nWeb Application Testing Exceeding the OWASP Top 10\nThe OWASP Top 10 provides a way to rank and remediate the top 10 most critical web application security risks. Below is a list of the most current release for OWASP Top 10:\nCurrent OWASP Top 10 List Released for 2021-2022:\n- Broken Access Control\n- Cryptographic Failures\n- Insecure Design\n- Security Misconfiguration\n- Vulnerable and Outdated Components\n- Identification and Authentication Failures\n- Software and Data Integrity Failures\n- Security Logging and Monitoring Failures\n- Server-Side Request Forgery (SSRF)\nAutomated vs. Manual Web Application Penetration Testing\nThe expert penetration testers at Artifice Security use automated scanners at the beginning of the assessment (10%) and then pivot into manual penetration testing for the remaining part (90%). Knowing the web application’s context, we can provide penetration tests geared to your individual security needs and make it relevant to your user base.\nWeb Application Penetration Test Methodology\nAfter years of performing penetration testing, Artifice Security has created a proven, repeatable methodology that will meet your organizational needs. As a manually-performed penetration testing company, we guarantee that no false positives will be in your report, and we provide proofs-of-concept that you can verify.\nOur security experts are diverse with experience working as system administrators, web developers, network engineers, and cloud specialists to military veterans and former NSA employees who held Top Secret clearances. Artifice Security consultants have also taught and spoken at cybersecurity conferences and created tools used by many penetration testers today. Each of our consultants is not only highly passionate about security, but they are also highly credentialed.\nDefine the Scope\nBefore the start of the penetration test, Artifice Security will collaborate with your team to determine the exact scope of your web application. We will communicate with your team to assess your application’s size, complexity, framework, roles, and how it is supposed to function normally.\n- Determine which applications are needed for testing to include domains and IP addresses for the host systems\n- Evaluate which directories or files, if any, are excluded from testing\n- Determine penetration testing performed in production or test/QA environment\n- Determine testing dates and times for the penetration test\n- Exchange key personnel and emergency contact information for any critical findings found.\nInformation Gathering / Recon Phase\nDuring the information-gathering phase of the assessment, Artifice Security will perform passive information gathering against your organization using Open Source Intelligence (OSINT) tools and techniques. This public dat a can help us determine undiscovered risks to your business and show you what anonymous information is out there on the Internet. This targeted intelligence includes the following checks:\n- Searches for documents such as PDF, DOCX, XLSX, and PPT documents that may contain exposed sensitive or customer information without your knowledge\n- Searches on the Internet and Darkweb for leaked credentials contained in password breach databases\n- Searches in repositories such as Github and other developer forums that may contain sensitive data related to your web application or organization\n- Checks to find similar domain names as yours to determine your risk to phishing (risks to domain spoofing)\n- Exposed robots.txt file to find potential hidden directories and files.\n- Map out and crawl through the application using a proxying tool\n- Enumerate all directories and subdomains\n- Search for possible hidden directories and files\n- Perform subdomain takeover checks\n- Check cloud services for misconfigurations (e.g., publicly exposed S3 buckets)\n- Check all possible open ports and services on the host server\n- Determine frameworks in use and associated software and libraries in use\n- Research and correlate known vulnerabilities for libraries and services used by the application\nAttack and Exploitation Phase\n- Configuration and Deployment Management Testing\n- Identity Management Testing\n- Authentication Testing\n- Authorization Testing\n- Session Management Testing\n- Data Validation Testing\n- Error Handling Testing\n- Cryptography Testing\n- Business Logic Testing\n- Client-Side Testing\n- API Testing\nThe report begins with an executive summary which gives a layman’s explanation of the findings and conveys the overall risk to your web application(s) and organization. In addition to a summary of findings, we also provide a list of positive results found during testing. Next, the report explains how we determine criticality and risk for each discovery so you can better understand how we prioritize findings and how we rate severity for each vulnerability.\nFurther in the report, we break down each vulnerability in technical detail, including a summary of the finding, affected location(s), proofs-of-concept, and remediation steps. Each detailed proof-of-concept has easy-to-follow steps for your team to recreate the process of how we exploited the vulnerability.\nIn addition to the report, Artifice Security also provides you with a customer-facing report and attestation letter if needed.\nRemediation Testing (Retesting)\nAs part of your penetration test, Artifice Security includes performing remediation testing (retesting) against your web application after your team remediates all findings. This retesting helps ensure your organization has adequately implemented changes to fix all vulnerabilities. Remediation and retesting also give compliance auditors and customers proof of your lowered or eliminated risk. After remediation testing completes, we will provide you with an updated report that reflects the current state of your web application.\nFrequently Asked Questions\nWhat information is needed for a web application penetration test?\nTo start a web application penetration test, we need to scope the application properly. The scoping process determines how long it will take to perform the penetration test based on the size and complexity of the web application. We need access to the web application and credentials if it is an authenticated web application for testing. Additionally, it helps to know if your organization hosts the application in the cloud and if there are any third-party APIs or connections within the web application.\nWhy is manual penetration testing important for a web application penetration test?\nMany areas of a web application need manual techniques performed. For example, the information-gathering phase needs to be done manually by a consultant when starting a web application penetration test. This information can reveal potential vulnerabilities such as past compromises or exposures that were made public.\nIf you solely rely on automated scanners, your results will also have many false positives and worse, false negatives. With manual penetration testing, all findings are 100% proven.\nAdditionally, an automated scanner might determine if anti-CSRF (Cross-Site Request Forgery) tokens are used. However, there are no ways to determine if CSRF vulnerabilities exist without manually exploiting the web application.\nWhen testing session management vulnerabilities, the consultant will attempt to exploit cookies to move into other accounts or to escalate privileges as an administrator.\nLastly, automation of business logic abuse cases is not possible. Business logic testing includes uploading malicious payloads to the web application, forged requests, integrity checks, process timing attacks, and circumvention of workflows.\nWill the penetration test affect the performance of my web host server?\nIs it best to perform the web application penetration test against a production or test environment?\nWhile a test environment is preferred, Artifice Security regularly performs web application pentests against production environments without issues. Artifice Security has vast experience testing both types of environments.","Differences between the SANS Top 25 and OWASP Top 10\nWhile they both serve as a reference point for software security and are partly based on the same source data, the SANS/MITRE CWE Top 25 and the OWASP Top 10 differ in scope and purpose. The OWASP list groups the most prevalent web application security weaknesses into ten categories corresponding to broader cybersecurity concerns. With each subsequent edition, the categories have been moving away from specific vulnerabilities or even common vulnerability classes and towards a more strategic view – see our post on the 2021 OWASP Top 10 to learn what this means in practice.\nThe SANS/CWE Top 25 lists the most prevalent issues from the Common Weakness Enumeration (CWE). In a way, CWE takes the opposite approach to the OWASP list, focusing on specific weaknesses rather than more abstract classifications. This makes the list more directly useful for developers and security engineers, as each item relates to concrete implementation flaws that can be found and addressed. Interestingly, although the SANS/CWE Top 25 applies to all types of software while the OWASP list is limited to web applications, with each edition there is more and more common ground between web and non-web software security.\nWeaknesses vs. vulnerabilities: Both the SANS Top 25 and the OWASP Top 10 deal solely with CWEs, i.e. security weaknesses that commonly occur during software development. These are different from CVEs, which are confirmed security vulnerabilities in specific products. In simple terms, exploitable weaknesses reported in production become vulnerabilities.\nCommon themes in software security weaknesses in 2021\nThe SANS Top 25 list is based on the prevalence of specific weaknesses in real-life vulnerabilities taken from the NIST NVD. Each CWE that has led to a vulnerability gets a score that reflects its frequency and severity (see here for the actual formula), and this score determines its position on the list. A dry technical list doesn’t seem particularly useful or exciting, but if you read closely, the CWE codes, scores, and trends tell the story of modern software development and security – a tale of trust, deceit, and demons of the past, all set firmly in the cloud. Let’s look at the four common themes running through the Top 25.\nWeb application security is everywhere\nIf you came to the SANS TOP 25 CWEs from the OWASP Top 10, you’d be forgiven for having a sense of deja vu, as eight of the top 25 weaknesses are either web-specific or most commonly found in web applications. It’s no secret that as software development moves to the web, so does application security. Here are the four web-specific weaknesses on the list, along with their official names and overall positions:\n- #2: Cross-site scripting (XSS), officially Improper Neutralization of Input During Web Page Generation [CWE-79]\n- #9: Cross-site request forgery (CSRF) [CWE-352]\n- #23: XXE injection, officially Improper Restriction of XML External Entity Reference [CWE-611]\n- #24: Server-side request forgery (SSRF) [CWE-918]\nApart from these, several other weaknesses in the list usually occur in web security contexts, notably SQL injection, OS command injection, and path traversal (a.k.a. directory traversal). While these can apply to other types of software, they are easiest to exploit in web applications. Again, the position reflects the frequency and severity of vulnerabilities linked to a specific weakness, so having XSS way up at #2 means there is a lot of cross-site scripting going on.\nMemory management issues never go away\nOn the one hand, we see that all the cloudy headlines are true – software development is increasingly web development, and software security is increasingly web application security. However, the #1 weakness (along with five relatives) serves as a stark reminder that a lot of critical software relies on lower-level programming languages that need careful memory management. The top software security weakness of 2021 is essentially buffer overflow, though this specific term is considered too general for CWE. Here are the weaknesses related to low-level memory operations:\n- #1: Out-of-bounds write (code can write to memory that shouldn’t be accessible) [CWE-787]\n- #3: Out-of-bounds read (code can read memory that shouldn’t be accessible) [CWE-125]\n- #7: Use after free (code uses a variable that shouldn’t be used anymore) [CWE-416]\n- #12: Integer overflow or wraparound (mismanagement of large numeric values) [CWE-190]\n- #15: NULL pointer dereference (code attempts to access a non-existent value) [CWE-476]\n- #17: Improper restriction of operations within the bounds of a memory buffer (code can operate on memory that shouldn’t be accessible) [CWE-119]\nTrust no one with your inputs\nThe other overarching theme of this software security tale is trust. It is difficult enough to write software that works correctly with the expected data and users. When every user could be malicious and every input could be an attack attempt, writing even the simplest piece of code is like walking through a minefield. How can you do anything when everyone is suspicious? How can you check every piece of data? And yet this is the reality of application security, as shown by over a quarter of the top 25 being weaknesses related to blindly trusting your inputs:\n- #4: Improper input validation [CWE-20]\n- #5: OS command injection, officially Improper Neutralization of Special Elements used in an OS Command [CWE-78]\n- #6: SQL injection, officially Improper Neutralization of Special Elements used in an SQL Command [CWE-89]\n- #8: Path traversal/directory traversal, officially Improper Limitation of a Pathname to a Restricted Directory [CWE-22]\n- #10: Unrestricted upload of file with dangerous type [CWE-434]\n- #13: Loading saved data without checking, officially Deserialization of Untrusted Data [CWE-502]\n- #25: Code injection, officially Improper Neutralization of Special Elements used in a Command [CWE-77]\nIn all these cases, failure to sanitize user-controlled inputs can have devastating consequences, from software crashes to information exposure or code execution. And as mentioned earlier, many of these are typically found in web application security, where user-controlled inputs make up most of the data your application uses.\nTrust no one with access\nThe threat landscape is easily the biggest change across the history of software security. With threat actors now active at every stage of the application lifecycle, access control should be an integral part of software and data design – except that it’s not. All the remaining weaknesses from the Top 25 are related to implicit trust or failures to protect sensitive data at all times, showing that, all too often, security is still an afterthought during development:\n- #11: Missing authentication for critical function [CWE-306]\n- #14: Improper authentication [CWE-287]\n- #16: Use of hard-coded credentials [CWE-798]\n- #18: Missing authorization [CWE-862]\n- #19: Incorrect default permissions [CWE-276]\n- #20: Exposure of sensitive information to an unauthorized actor [CWE-200]\n- #21: Insufficiently protected credentials [CWE-522]\n- #22: Incorrect permission assignment for critical resource [CWE-732]\nThe importance of such trust-related issues is also reflected in the OWASP Top 10, where the top categories are now Broken Access Control and Cryptographic Failures. Ensuring application security means encrypting sensitive data (or all data, in many cases) at rest and in transit using secure algorithms while also thinking of authentication and authorization when designing user roles and function access.\nTo be effective, security must come first\nWith over half of the SANS Top 25 security weaknesses being related to trust and access control, it’s no coincidence that CISA is calling for organizations to implement zero trust principles in their systems. What’s more, the three fastest risers on the list since 2020 are all trust-related: Incorrect Default Permissions, Missing Authentication for Critical Function, and Deserialization of Untrusted Data. And remember that the list is based on prevalence in real-life vulnerabilities, so these weaknesses are out there and growing in frequency or severity (or both).\nThere are no shortcuts to avoiding software vulnerabilities, only hard work to build a security-first mindset and embed security into every stage of the software development lifecycle (SDLC). Vulnerability testing, mitigation, and remediation all need to be a routine part of the development workflow, built on a solid foundation of education and security awareness.\nOrganizations can no longer afford to compromise on security or accept security risks as the price of rapid development and growth. When anything can be a target and anyone can be an attacker, security must come first."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:064a0195-f58c-47ff-bc5e-71314a0c9e27>","<urn:uuid:8aeaf7b5-d75a-4b60-bbba-d0bb596a8aad>"],"error":null}
{"question":"How do hydropower and wind power facilities impact local wildlife differently?","answer":"Hydropower dams significantly impact river ecosystems, particularly affecting anadromous fish like salmon, trout, and sturgeon by disrupting their migration paths, though fish chutes are built as a solution. In contrast, modern wind turbines have improved technology that significantly reduces bird mortality by avoiding updraft areas and using tubular towers that prevent birds from perching, with diagonal supports on smaller turbines to discourage perching.","context":["Commentary, J Electr Eng Electron Technol Vol: 11 Issue: 2\nHydropower and Importance of Water Energy Technologies\nReceived date: 21 January, 2022, Manuscript No. JEEET-22-59596;\nEditor assigned date: 24 January, 2022; PreQC No. JEEET-22-59596 (PQ);\nReviewed date: 03 February, 2022, QC No. JEEET-22-59596;\nRevised date: 15 February, 2022, Manuscript No. JEEET-22-59596 (R);\nPublished date: 23 February, 2022, DOI: 10.4172/jeeet.1000892.\nCitation: Marco R (2022) Hydropower and Importance of Water Energy Technologies. J Electr Eng Electron Technol 11:2.\nWater technologies embody a selection of systems that use ocean or freshwater for power or thermal power. The maximum familiar water generation is hydropower, wherein the pressure of transferring water propels a turbine, which in turn runs a generator to create electricity. Hydropower and different water technology are renewable because their gas is clearly replenished through the water cycle they may be easy options to the burning of fossil fuels that reason climate trade. Hydropower does no longer require the purchase of fuels for technology, not like herbal gas, coal and other gasoline-burning flora. The best expenses are the construction and operation of the generation centres. Huge hydropower dams on essential rivers are the maximum evolved turbines of water power. Pumped storage or reservoir plants keep water in a reservoir to release for use when the river is strolling slower or throughout times of peak energy demand. This lets in for reliable base-load strength technology. The hoover dam in Nevada and the Grand Coulee Dam in Washington are examples of those massive facilities. Huge dams additionally meet a couple of societal desires including irrigation, flood manage and activity.\nKeywords: Power Grid\nElectricity is dispatched from a Power Grid\nA few hydroelectric electricity flowers simply use a small canal to channel the river water through a turbine. Any other form of hydroelectric energy plant called a pumped storage plant may even shop power. The electricity is dispatched from a power grid into the electrical generators. The mills then spin the mills backward, which causes the turbines to pump water from a river or decrease reservoir to a higher reservoir, in which the strength is stored. To use the strength, the water is released from the upper reservoir reverse into the river or decrease reservoir. This spins the mills ahead, activating the mills to supply power. Even though most electricity within the America is produced with the aid of fossil-gas and nuclear electricity vegetation, hydroelectricity remains crucial to the country. These days, massive power generators are positioned interior dams. Water flowing through the dams spin turbine blades made from metal rather than leaves which are related to turbines. Strength is produced and is sent to homes and businesses. Hydropower development includes trans-generation know-how transfer because it has benefited from new standards and the contemporary advances in different sectors. Hydropower facilities are complicated systems that include a huge spectrum of various technologies into their additives. Hydro stations, therefore, characteristic as a machine of components. Thus, the operational characteristics and talents of each gadget rely on the technological functions of its elements. It's far important to highlight that each hydropower station is a completely unique gadget mainly designed to suit the particular site. This is an important distinction between hydro and conventional thermal power-vegetation and modular renewable electricity supply. The variable electrical power production from renewable sources wind and sun calls for those hydraulic mills operate at an extensive range and variable conditions. Consequently, the cutting-edge hydraulic mills meet new demanding situations associated with the variable demand on the power market as well as limited energy garage skills, ensuing in splendid flexibility required in operation over a prolonged range of regimes some distance from the generators' nice performance factor. When hydraulic turbines operate at off-design conditions, a slight- or excessive-level residual swirl takes place within the draft tube because of a mismatch between the swirl generated by the wicket gates and the angular momentum extracted by means of the turbine runner.\nSimulations of Fluid-shape Interaction\nAt such off-layout operating regimes, hydraulic turbines with a fixed pitch runner Francis and propeller mills, revel in an abrupt lower efficiency. The same old technique to simulate the overall performance of a prototype hydraulic gadget consists of experiments on version generators to evaluate the turbine performance for the whole range of admissible discharge and head. The performance hill chart generally displays height performance on the system. The draft tube, the device factor in which the drift exiting the runner is decelerated, converts the extra kinetic electricity to static stress. It displays an abrupt boom in hydraulic losses as the working regime departs from process. Nearly, the shape of the hill-chart is dictated by way of the losses in the draft tube for cutting-edge medium-low head hydraulic turbines. Wider variety and common adjustments in operating situations consisting of a big number of begins and stops, skinny blade and vane profiles because of excessive-performance requirements and weight optimization add complexity on hydro mills analyses of vibration behavior and fatigue. Simulations in the course of the design section require the correct willpower of the dynamic reaction such as dynamic stresses, hydrodynamic mass and damping residences by reliable numerical simulations of fluid-shape interaction. An important element affecting the turbine lifetime and dependable operation are fatigue cracks in the runner, particularly from the rotor-stator interaction and related strain pulsations within the turbine. While the runner natural frequency is close to the RSI frequency, hydrodynamic damping is a systematic parameter in controlling turbine blade-compelled reaction. A dependable method that may expect the trade inside the runner natural frequency and damping is required. The frequency is especially depending on the delivered mass but additionally on drift fee and float situations of the upstream and downstream of the runner and specially the draft tube. For a shape like the hydro turbine runners, the modal response is complicated showing many herbal frequencies with entangled mode-shapes. Due to the fact that for high head Francis turbine runner’s dynamic excitation because of RSI may be the primary fatigue contributor, there is presently a huge cooperative attempt for investigating this hassle. The running situations of a hydropower plant can be enormously variable. Versions within the operating conditions of the hydropower plant make contributions to reduce the plant's global performance and might bring about waft instabilities, cavitation and so forth. That reduces the life of hydropower devices. By way of various rotational paces of the hydropower plant's units with respect to their synchronous speed, the plant can higher adapt to the hydrological regime of the river, thereby increasing the plant's international efficiency and the devices' lifetime and can also boom its contribution to the EPS ancillary services. Hydropower is critical from an operational perspective as it wishes ramp-up time, as many combustion technologies do. Hydropower can increase or decrease the amount of electricity it's far supplying to the device nearly instantly to fulfill moving demand. With this essential load functionality, peaking capacity and voltage stability attributes, hydropower performs a huge element in ensuring reliable electricity service and in meeting client wishes in a marketplace driven enterprise. In addition, hydroelectric pumped garage facilities are the best sizeable way presently available to store strength. Hydropower capability to offer peaking energy and frequency manage facilitates guard in opposition to gadget screw ups that could lead to the harm of gadget and even brown or blackouts. Hydropower, besides being emissions-free and renewable has the operating blessings that offer improved fee to the electrical machine inside the form of performance, protection and most crucial, reliability. The electrical blessings provided by way of hydroelectric resources are importance to the fulfilment of our countrywide test to decontrol the electrical industry. To generate hydroelectric strength, rivers should be dammed and valleys must be flooded. This has a clean environmental impact. The ecology of the entire river valley is altered for all time. This specifically affects any anadromous fish such as salmon, trout, sturgeon and which might be born in the higher reaches of the river and passes downstream to the sea to mature and go back to the river to spawn and die. The dam will disrupt their migration paths. The solution to that is to assemble fish chutes beside the dam that permit the fish to nonetheless tour up and down the river, despite the fact that many are not completely satisfied with this technique. Another famous environmental effect is that when river valleys are flooded, the vegetation that gets flooded and dies decay in an anaerobic environment, which produces methane. This methane unearths its way into the ecosystem, wherein it acts as an effective greenhouse gasoline. Hydropower is an essential contributor within the country wide power grid because of its capability to reply quick to unexpectedly varying hundreds or machine disturbances, which base load flora with steam structures powered by way of combustion or nuclear tactics cannot accommodate.","Learn About Green Power\nWhat is Green Power?\nGreen Power is energy produced by renewable sources, like wind and sun, that are replenished naturally. Renewable energy is a clean source of energy that has a much lower environmental impact than conventional energy technologies.\nWhy is Green Power important?\nBy choosing Green Power, you can support development of renewable energy sources, which can reduce the burning of fossil fuels, such as coal, oil, and natural gas. Greater reliance on renewable sources also provides economic benefits and can improve our national energy security.\nWhere does Green Power come from?\nLower Valley Energy obtains most of its green power from the Horse Butte Wind farm, just south of Idaho Falls, Idaho. In addition, Lower Valley Energy also produces green power from its two low-impact hydro facilities at Strawberry and Swift Creek projects (located in Bedford, WY and Afton, WY respectively). Energy produced from these projects is certified as Environmentally Preferred Power, a rating endorsed by national and northwestern region environmental organizations.\nHow will this affect my electric bill?\nGreen Power is paid for by an extra monthly premium, which shows up on your bill in addition to regular monthly charges. As a residential customer you can purchase blocks of 300 kilowatt hours (kWhs) for $3.50 extra per month. As a commercial customer you can choose between 1,500 kWhs/$17.50 per month blocks (Partner Level) or 3,000 kWhs/$35.00 per month blocks (Champion Level).\n100% Green Power is a special rate of 6.8 cents/kWh (versus 5.6 cents/kWh for normal residential electricity).\nWhy does Green Power cost more?\nGreen Power, or renewable energy, is a relatively new concept. Companies have just recently begun to create power from these sources of energy. As a modern technology, these types of facilities are more expensive to construct and operate. As technology increases and usage increases, it is likely that overall costs will decrease.\nHow do I know I’m Getting Green Power?\nPower generators put power into the energy pool and power grid as energy customers take it out. Wind-generated Green Power enters the power pool that is already being fed by other power resources like natural gas, oil and coal. By supporting Green Power you are investing in the further development of a clean power resource.\nHow does Green Power benefit our local communities?\nChoosing renewable power reduces the impact on our water sources, the air we breathe, wildlife, and the land around us. Also, the more customers participating in Green Power will help ensure that our communities will benefit from renewable energy sources long into the future.\nWhat effects could wind turbines have on migratory birds?\nThe technology associated with modern wind turbines has improved greatly, significantly reducing bird mortality at wind plants. Today’s wind facilities are specifically located in areas where there is not an updraft. Tubular towers of modern wind turbines ensure that birds are unable to perch on the towers. Smaller turbines that may still have lattice towers have diagonal supports to discourage perching.\nHow can I sign up for green power?\nThank you for your interest! Visit our signup page to get started."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:95383e5d-6f7d-4d14-9b4b-0df333322aa1>","<urn:uuid:5612567e-08e7-4eee-8ea7-3dd4c76e21e2>"],"error":null}
{"question":"Which construction projects include safety features for pedestrians - the Bluff Street improvement or the Bonanza Drive project?","answer":"Both projects include pedestrian safety features. The Bonanza Drive project includes a 125-foot tunnel that provides a safe crossing underneath the busy street, eliminating what was considered one of the most dangerous road crossings in Park City. The Bluff Street Corridor project includes a pedestrian box culvert under Bluff Street that will provide safe access to a new trail system in the area.","context":["Long delayed, Bonanza Drive tunnel quietly opens\nThe long-delayed pedestrian-bicyclist tunnel underneath Bonanza Drive quietly opened in April, providing an alternative route for people needing to cross the busy street.\nPark City officials have not publicized the opening and the City Hall staffer overseeing the project said this week tunnel-related work is continuing. Matt Cassel, the Park City engineer, said the crews at the site have been installing guardrails and clearing the construction site since the opening.\nCassel said there has been a \"steady flow of people using it\" since the tunnel opened.\nThe tunnel is seen as one of the most important upgrades City Hall has undertaken using funding that was raised through a ballot measure to finance improvements to pedestrian and bicyclist routes. It is situated close to Bonanza Drive’s intersection with the Rail Trail.\nIt now serves as the crucial link for pedestrians and bicyclists between Prospector and Old Town. Someone can now walk or bicycle from the vicinity of Main Street to the Park City School District campus without having to cross a major street at the surface level. The tunnel also provides easy access to the popular Rail Trail from points south and west.\nThe tunnel is approximately 125 feet long, with the floor being approximately 13 feet below the road surface.\n\"They’re finding it and using it,\" Cassel said.\nHe anticipates a celebration marking its opening will be scheduled by Memorial Day.\nThe tunnel gives pedestrians and bicyclists an alternative to what has long been considered one of the most dangerous road crossings inside Park City. There have been repeated close calls between drivers and pedestrians or bicyclists crossing Bonanza Drive close to the location of the tunnel.\nCity Hall over the years has taken steps to make the crossing at the surface level safer, installing signs and flashing lights that can be activated by a pedestrian or bicyclist wanting to cross the street.\nThe tunnel was built as part of the wider Bonanza Drive roadwork. The overall Bonanza Drive work, stretching from 2009 until this year, was priced at just less than $8 million and included the tunnel, the redo of the road, the installation of a sewer line and the construction of sidewalks. The tunnel is estimated to account for approximately 30 percent of the overall cost.\nCity Hall had hoped the tunnel would make its debut last October, but the project encountered delays as designs for retaining walls built next to the walkways into and out of the tunnel needed to be modified to work in the soil conditions at the site.\nThe walkway on the western side of the tunnel edges up against Poison Creek, and workers have put down sandbags between the top of the walkway wall and the creek. There are concerns about Poison Creek jumping its banks during the spring snowmelt.\nTrish Dunne, who lives on Park Avenue, went through the tunnel Thursday morning with her dog, Biggie, headed toward the Rail Trail. She said she will use the tunnel frequently.\n\"It’s crossing a street versus not crossing a street. It’s always safer not to have to cross the street,\" Dunne said, calling the tunnel \"way more convenient.\"\nThe Bonanza Drive tunnel’s debut comes six months after the opening of another pedestrian-bicyclist tunnel built by City Hall — underneath Kearns Boulevard outside the Park City School District campus. The Kearns Boulevard tunnel was financed by the same ballot measure.\nCharlie Sturgis, the executive director of Mountain Trails Foundation, a not-for-profit group dedicated to widening the local trail network, said he anticipates the tunnel will be especially popular in the summer as people head to the Rail Trail.\n\"If you’re trying to bike or walk it, getting across it has . . . always been on the dicey side,\" Sturgis said.\nSupport Local Journalism\nSupport Local Journalism\nReaders around Park City and Summit County make the Park Record's work possible. Your financial contribution supports our efforts to deliver quality, locally relevant journalism.\nNow more than ever, your support is critical to help us keep our community informed about the evolving coronavirus pandemic and the impact it is having locally. Every contribution, however large or small, will make a difference.\nEach donation will be used exclusively for the development and creation of increased news coverage.\nStart a dialogue, stay on topic and be civil.\nIf you don't follow the rules, your comment may be deleted.\nUser Legend: Moderator Trusted User\nThe Alpine Slide was a hit, so, why not try something a little more… extreme? Enter: Down The Tube.","ACC Southwest and Tri-State Electric & Utility team up realign traffic signals, install a new water light and replace light foundations.\nBluff Street (SR 18) in St. George, Utah, is a highly-traveled roadway tasked with carrying an increasing load as the population in the area continues to grow. More than 44,000 cars per day travel the route and this number is expected to grow to 65,000 by 2040. To address this need, the Utah Department of Transportation (UDOT) completed an environmental study in March of 2015 to evaluate various transportation options. Through careful analysis and close coordination with the local communities, a solution was identified which met transportation needs while minimizing impacts.\nThe $51 million project (including acquisitions) is funded primary with state money and $1.8 million from local government sources. The four-fold undertaking includes:\n- Upgrading and improving all utilities which are buried in the current roadway including storm drain, culinary water, sewer, fiber optics, gas, electrical and an upgraded Advanced Traffic Management System;\n- increasing capacity by adding an additional northbound and southbound lane from 100 South to 1250 North through this heavily traveled section of roadway;\n- reconfiguring the intersection at Sunset Boulevard and Bluff Street to improve safety, traffic flow and handle projected traffic increases; and\n- adding a dedicated right turn lane from westbound St George Boulevard to northbound Bluff Street which will improve safety and mobility.\nThese upgrades will provide flexibility to accommodate future expansion as growth dictates and save money which can be allocated to other road projects in the area. Pedestrians and cyclists will have safe access to a new trail system in the area with a pedestrian box culvert under Bluff Street.\nConstruction began early this year with subsurface work under the direction of prime contractor Meadow Valley Contractors dba ACC Southwest of St. George. Six different utilities are buried in various locations under the current road: water, sewer, electric, natural gas, telephone and fiber optic communications. UDOT is adding a seventh — a new storm drain.\nSUE Solutions, a subsurface utility engineering company in Lehi, was hired to coordinate with all the utilities and obtain schematic mapping from them. The engineers take the schematic information and physically locate the utilities in the field. Test holes are drilled on-site to pinpoint each utility in various spots to establish a vertical profile. By doing research prior to construction, the contractor is given the vital information to construct the roadway with no unplanned service interruptions to customers in the area.\nACC Southwest recently finished construction of a bridge widening on I-15 over the Virgin River just south of this project, and also reconfigured an intersection on St. George Boulevard (State Route 34) at 1000 East. For the Bluff Street project, ACC is working on the water and sewer lines, new storm drain, excavation and grading.\nAt the end of March, the project hit the 25 percent milestone with the completion of the new sewer and water lines around the intersection of Sunset Boulevard and Bluff Street. The project is on schedule to be finished in November of this year.\nIn response to input from businesses along the Bluff Street Corridor, UDOT, St. George City and ACC, new “business specific” access signs were put in place to enhance traffic flow and improve business accessibility.\nAccording to UDOT, the project is going well and keeping on schedule overall. Two change orders have been processed, one for traffic lights being centered over traffic lanes and another to transport state furnished materials from Salt Lake City to St. George.\n“We have found some contaminated soil that needed to be removed to the land fill. Coordination of multiple utility companies has been key to overcoming challenges,” said Kevin Kitchen, UDOT communication manager. “We have also identified several water seeps [springs] that have caused some minor delays while they work around them. These are the greatest construction challenges encountered up to this point.”\nUDOT resident engineer Ray Bentley said he has been working very close with city of St. George Public Works Director, Cameron Cutler, whenever city approval or help is needed. He said they have been great to work with. He also noted the water, power, parks and sewer representatives have all been coming to the partnering meetings and/or helping with their parts.\nInstallation of the new gas line, which began at 100 South, will be completed in April. Flare Construction of Coalville was brought in because of its expertise with this type of piping.\nUDOT reported that work on the 42-in. storm drain is progressing well. ACC crews are working in the 300 North block and will install about 150 ft. of pipe per day — weather permitting. The new storm drain will keep the road clear of water during times of heavy rain. There are approximately 5,000 linear ft. of storm drain to be installed along the corridor.\nInstallation of the soil nail wall from the corner of Airport Road and Bluff Street along the west slope to 100 South is continuing. Long, steel soil nails are drilled into the rock face of the slope to stabilize the area and prevent future erosion. Malcolm Drilling Company Inc. of Salt Lake City will install up to 43 soil nails along this stretch of embankment.\nAnother slope protecting wall is under construction just south of the Bluff Street and Sunset Boulevard intersection on the west side of Bluff Street. This area will be fortified with steel rods and mesh, which is then pneumatically sprayed with shotcrete, a concrete mixture. These measures will protect the area from future slope erosion.\nNew signal and lighting foundations are being installed by Tri-State Electric & Utility, Inc. of St. George. The whole corridor will get a new Advance Traffic Management System from Safe City Systems (SCS), which will help with traffic flow and safety.\nACC continues to move material to create the new alignment for the intersection of Bluff Street and Sunset Boulevard. The new alignment shifts the intersection to the southwest. Western Rock Products, another St. George local, will be laying the pavement, and Kokopelli Landscape, based in Mesquite, Nev., will wind up the venture with landscaping. When the project is complete, all utilities in the area will be modern and are expected to last 30 to 40 years.\nACC also is working to maintain driveway and road approaches through the project. Due to the recent rain in the area, rock and road base have been added to several of the approaches to help with mud. With the amount of traffic traveling and accessing properties along the corridor, these crews are constantly busy.\nDuring the 1930's, the St. George Civilian Conservation Corps (CCC) Camp #2558 was located on the area now occupied by the Red Hills Golf Course. Archaeologists from Bighorn Archaeological Consultants are monitoring construction activity along the western edge of the golf course and will be available to further investigate in the event of any subsurface discoveries.\nThe Bluff Street Corridor Project is an integrated improvement enhancing the way citizens connect and use the Bluff Street Corridor whether by auto, public transit, cycling or walking. At the current growth rate in the city of St. George, engineers estimate that once these changes are complete, additional transportation expansion in the area would not be necessary until 2030."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:71c1f163-58a2-4c3e-b872-20e7ca5ac25e>","<urn:uuid:d0e5e61b-3fe4-4ebb-b970-3c3b2c94afc6>"],"error":null}
{"question":"How do codes of conduct and ISO 9001 certification differ in their approaches to customer satisfaction and continuous improvement?","answer":"Codes of conduct and ISO 9001 certification have different approaches to customer satisfaction and continuous improvement. Codes of conduct focus on legal and ethical business practices, requiring regular reviews to adapt to changing legal and regulatory environments. In contrast, ISO 9001 QMS takes a more systematic approach through its 'Plan, Do, Check, Act' structure that specifically ensures customer needs are considered and met. ISO 9001 also provides a framework to monitor and improve performance through operational efficiencies, while codes of conduct primarily guide employee behavior and ensure compliance with laws and regulations.","context":["By Anne Marie Logarta, Esq., CCEP and Ruth M. Ward, CCEP, CFE\nNot since the U.S. Federal Sentencing Guidelines were issued on the Seven Elements of an Effective Compliance Program in 1987 have legal and compliance and ethics professionals focused so much time and attention on reviewing or revising their companies’ codes of conduct. Arguably, the impetus for many was the passage in 2010 of the Dodd-Frank Act and the U.K. Bribery Act, the latter which became effective in 2011.\nAs compliance-related laws are passed or revised, or internal policies are developed or revisited, a company must adapt and respond quickly to the changing legal and regulatory environment. A regular review process of a company’s Code should be established internally to ensure employees receive updates in a timely manner. The Code review process is essentially an assessment of the characteristics of the policies in place to help guide your company’s legal and ethical business practices. Suffice it to say that the Code review process is a major undertaking and challenge, and should be approached as such.\nHowever, where do you begin? Before taking on this challenge, you need to consider:\n– When was the last time the Code was released or revised?\n– Have there been changes to internal policies since the last revision?\n– Have there been changes to a country’s laws relating to a Code topic (i.e. anti-bribery, antitrust, boycotts and embargoes, etc.)?\n– Are any of the guidelines outdated?\n– Is there a budget to create a Code?\nIt would also be helpful to benchmark your Code against other companies’ Codes. Many companies typically post their Codes on their public web sites and sample Codes are also available on the internet. Moreover, there are law practitioners or compliance consultants who can provide assistance in writing a new Code or assessing an existing Code.\nIf you decide to pursue a revision after considering the above, the following six steps will assist you in a successful revision process.\n[bctt tweet=”Revising Your Code of Conduct? These 6 Steps Will Help @SCCE @theHCCA” via=”no”]\n1. Get Buy-In from Decision-Makers at the Highest Level of the Company\nThe mandate to revise a company’s Code must be obtained from the highest level, the Chief Executive Officer, the company’s General Counsel or the Chief Compliance Officer. He or she will explain the legal and regulatory, or business need to other high level decision-makers and get their buy-in. As such, he or she should be consulted at every major step of the Code review process if it involves a change in direction of key policies.\n2. Establish a Core Revision Committee\nThe revision committee would have the overall responsibility to provide direction to the function representatives, establish an overall budget (if necessary), set deadlines and manage timelines. A core revision committee should be formed consisting of cross-functional workforce representatives. Be sure to include Compliance and Ethics, Legal, Communications, and Human Resources. Functions such as Finance & Accounting, Information Technology, International, Marketing, Operations, Sales, Security or the philanthropic arm of the company may also be included as part of the revision committee.\nFunctions will be given the flexibility to revise text or even to recommend removal of a Code topic. The revision committee must also create a timeline at the outset of the revision and hold the function representatives accountable for meeting their deliverables. Doing so is critical to the success of the project.\n3. Conduct a Thorough Technology Assessment\nThe backbone of the revision process is how you capture, collaborate and preserve all of the comments, notes, edits and decisions during the entire project. . Training all team members is extremely important to this process. Appointing one team member per function to make the updates suggested by the specific function is effective in maintaining the integrity of the documents.\nAdditionally, you will need to assess whether your Code will be hard copy, online or both. If online, determining the best application to launch your Code and whether it includes a certification process is important. An effective online or hard copy distribution method to provide copies of the new or revised Code to your employees should be in place. Make sure all prior copies of the Code are no longer available online or in print; however, be sure to preserve copies for historical purposes.\n4. Determine Translations and Localizations\nIf your company does business internationally, then this step is vital to ensure you have one universal Code, no matter the language. If you decide to use a vendor to translate, appoint an approved company translation subject matter expert (SME) for each language. There are quite a few words or phrases that don’t have an equivalent translation in many languages. Sending your translated materials to your appointed country SME will ensure the translations are localized and understood by the people in that country.\n5. Develop a Plan to Communicate the Code of Conduct\nIt is important that the new or revised Code of Conduct is communicated in a manner that encourages employees to review and use the Code on an ongoing basis. If the company has the appropriate resources, it would be money well spent to attractively package the Code using graphics, color, interaction, etc. to motivate employees to use the Code as a resource, not just a desk decoration.\nIf the company has employees in multiple locations around the U.S. and internationally, the Code should be launched in a manner that is suitable for the location. The communications and human resources functions must be involved to help determine the best and most effective way to make your Code available to your employees and the general public, where applicable.\n6. Stay on Deadline and Budget\nIf you set realistic expectations, then this last recommendation should be easy to adhere to. However, make sure you have enough time so you won’t feel rushed or in a hurry to “get it done”. You want to set aside enough time and money so that your Code will represent your company’s commitment to quality and integrity, as well as your company’s standards of conduct.\nFollowing these six steps will help ensure a successful revision project, while maintaining the integrity of the process.\nAnne Marie Logarta is a Global Compliance Manager with UPS in Atlanta, Georgia. She can be contacted at firstname.lastname@example.org.\nRuth M. Ward is a Global Compliance and Ethics Manager with UPS and serves as a board member on the Atlanta Compliance and Ethics Roundtable (ACE) in Atlanta, Georgia. She can be contacted at email@example.com.","CAW Consultancy Business Solutions Ltd\nWhat is ISO 9001 QMS?\nISO 9001:2015 is the International standard for Quality Management Systems (QMS).\nIt provides an organization with a set of principles that ensure focussed, informed, scientific and proven approach to the management of your business activities to consistently achieve customer satisfaction and continually improve operational effectiveness.\nEvery organisation would like to improve the way it operates, whether that means increasing market share, driving down costs, managing risk more effectively or improving customer satisfaction. A quality management system gives you the framework you need to monitor and improve performance in any area you choose.\nAny organisation can benefit from implementing ISO 9001:2015. It doesn't matter what size they are or what they do. It can help both product and service organizations achieve standards of quality that are recognized and respected throughout the world.\nISO 9001 QMS is based on seven quality management principles:\nA customer focused organisation\nThe engagement of people\nEnsuring a process approach\nEvidenced based decision making\nBenefits of ISO 9001:2015 Competitive advantage\nISO 9001 should be top-management led, which ensures that senior management take a strategic approach to their management systems. Our assessment and certification process ensures that the business objectives constantly feed into your processes and working practices to ensure you maximise your assets.\nImproves business performance and manages business risk\nISO 9001 helps your managers to raise the organization’s performance above and beyond competitors who aren’t using management systems. Certification also makes it easier to measure performance and better manage business risk.\nAttracts investment, enhances brand reputation and removes barriers to trade\nCertification to ISO 9001 will boost your organization’s brand reputation and can be a useful promotional tool. It sends a clear message to all interested parties that this is a company committed to high standards and continual improvement.\nSaves you money\nEvidence shows that the financial benefits for companies that have invested in and certified their quality management systems to ISO 9001 include operational efficiencies, increased sales, higher return on assets and greater profitability.\nStreamlines operations and reduces waste\nThe assessment of your quality management system focuses on operating processes. This encourages organizations to improve the quality of products and the service provided and helps to reduces waste and customer complaints.\nEncourages internal communication and raises morale\nISO 9001 ensures that employees feel more involved through improved communication. Continued Assessment visits can highlight any skills shortages sooner and uncover any teamwork issues.\nIncreases customer satisfaction\nThe ‘Plan, Do, Check, Act’ structure of ISO 9001 ensures that the needs of the customer are being considered and met.\nHow to achieve ISO 9001 certification - ISO 9001 implementation / Certification steps\nThere are several requirements of ISO 9001:2015 where an organization could add value to its QMS and demonstrate conformity by the preparation of other documents, even though the standard does not specifically require them. Examples may include:\nProcess maps, process flow charts and/or process descriptions\nWork and/or test instructions\nDocuments containing internal communications\nApproved supplier lists\nTest and inspection plans\nIntegrate ISO 9001 with other management system standards\nISO 9001 is designed to be compatible with other management systems standards and specifications, such as ISO 45001, ISO 22000, ISO 17025, ISO 27001, ISO 14001 Environment and other ISO standards. They can be integrated seamlessly through Integrated Management system approach. They share many principles so choosing an integrated management system can offer excellent value for money and an easier approach to implement, manage and improve multiple standards simultaneously."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:ab995bb3-38fe-4b38-955b-f336361c4885>","<urn:uuid:178f318d-8f1f-499d-a9eb-0c5c938936f9>"],"error":null}
{"question":"How do kanji and kana complement each other in the Japanese writing system?","answer":"Kanji and kana serve complementary functions in Japanese writing. Kanji are logograms that represent morphemes (words or parts of words) and are particularly useful for writing a language with many homophones. However, due to Japanese language inflections, a phonetic script is necessary. This is where kana comes in - consisting of two syllabaries (hiragana and katakana). Hiragana is used for native Japanese words and grammatical inflections, while katakana is primarily used for foreign loan-words. Together, these systems form a mixed writing system where kanji provides the core meaning while kana handles grammatical elements and foreign terms. Each kanji usually has multiple pronunciations, including both native Japanese (kun-reading) and Chinese-derived (on-reading) pronunciations.","context":["|Item type||Current location||Call number||Status||Date due||Barcode||Item holds|\nAccess a wide range of magazines and books using Pressreader and Ebook central.\nWriting and Literacy in Chinese, Korean and Japanese -- Editorial page -- Title page -- LCC data -- About the authors -- Preface -- Acknowledgements -- 1. Introduction -- How many Chinese, Koreans, and Japanese? -- How are the three peoples related? -- Languages of the world -- Phoneme, syllable, onset-rime, and body-coda -- Morpheme and word -- Types of writing systems -- Writing systems, their development and interrelations -- Scripts and literacy: A preview -- Chinese, Korean, and Japanese in Roman letters -- A few words about experiments on reading -- How the book is organized -- Part I. Chinese -- China and Chinese -- 2. Spoken Chinese -- Standard language and \"dialects\" -- Sound system -- Morphemes: Words or word parts -- Constructing two-morpheme words -- Why compound words? -- Foreign loan words -- Full words, empty words, and classifiers -- Sentence structures -- 3. Chinese characters: Hanzi -- Beginning of characters -- Evolution of characters' styles -- Chinese calligraphy -- Six categories of characters -- Number of characters -- Strokes and shapes of characters -- Complex vs simple characters -- 4. Meaning representation in characters -- Pictographs and indicators -- Radicals and semantic radicals -- Characters tell stories -- Compound words and idioms -- Characters for abbreviations -- Chinese numerals -- Chinese personal names -- Magical quality of characters -- Characters understood across times and places -- 5. Sound representation by characters -- A character's sound -- Phonetic radicals -- Polyphonic, unpronounced, or homophonic characters -- Phonetic loans and Fanqie -- Phonetic scripts for Chinese -- 6. History of education and literacy in China -- Confucianism and Confucian classics -- The civil-service examination system -- Chinese world views -- Invention of paper and printing -- Books and publications.\nTraditional and pre-1949 education -- In women ignorance was virtue -- History and degrees of literacy -- 7. Reforming spoken and written Chinese -- Mandarin and Putonghua (common speech) -- Literary vs vernacular language -- Rationalizing the Chinese writing system -- How characters are simplified -- Romanization, Zhuyin Fuhao, and Pinyin -- Computerizing Chinese characters -- Keep or abandon characters? -- 8. School, and learning to read in Chinese -- Primary and secondary schools: Growing, if unequally -- Tertiary education -- Should preschoolers be taught to read? -- How Hanzi are taught to preschoolers -- Teaching Hanzi (and English) in Chinese-speaking regions -- How Hanzi are taught in school in China -- Semantic radicals and phonetic radicals -- Phonological awareness -- Morphological (and phonological) awareness -- Visual and orthographic processing -- Developmental dyslexia or reading difficulty -- Summary and conclusions -- Part II. Korean -- Korea and Koreans -- 9. Korean language -- Speech sounds and syllables -- Korean native words -- Sino-Korean (S-K)words -- Native words vs Sino-Korean words -- European (and Japanese) loan words -- Numerals and classifiers -- Content words, grammatical morphemes, and sentences -- Speech levels and honorifics -- 10. Hancha: Chinese characters -- Hancha adoption -- Complicated Hancha use in the past -- Hancha use in the present -- Misguided attempts to abolish Hancha -- 11. Han'gŭl: Alphabetic syllabary -- Creation and adoption of Han'gŭl -- Han'gŭl as an alphabet -- Han'gŭl syllable blocks -- Varied shapes and complexity of syllable blocks -- Linear vs packaged arrangement of Han'gul letters -- Changes in Han'gŭl since its creation -- Was Han'gŭl an original creation? -- Han'gŭl, an alphabetic syllabary or alpha-syllabary -- 12. Learning and using Han'gŭl.\nTeaching Han'gŭl as an alphabet or a syllabary -- Preschoolers learn Han'gul -- Schoolchildren learn to read in Han'gul -- Instruction in Han'gŭl spelling -- Han'gŭl spelling vs romanized spelling -- Phonological awareness and salience of syllable -- Morphological awareness -- Visual skills -- Poor readers in Han'gul -- 13. Why should Hancha be kept? -- Advantages of Hancha -- Disadvantages of not knowing Hancha -- Korean personal names -- Hancha-Han'gŭl mixed vs all-Han'gŭl text -- Hancha teaching in secondary school -- Streamline and keep Hancha -- 14. History of education and literacy in Korea -- Civil service examination in Korea -- Traditional education -- Modern education -- Education in S. Korea today -- Printing and publications -- Mass literacy -- Summary and conclusions -- Part III. Japanese -- Japan and Japanese -- 15. Japanese language -- Speech sounds, syllables, and moras -- Composition of Japanese vocabulary -- Japanese native vs Sino-Japanese (S-J) words -- European and English loan words -- Numerals and classifiers -- Content words and grammatical morphemes -- Sentence structures -- 16. Kanji: Chinese characters -- Indigenous Japanese scripts? -- Introduction and spread of Kanji -- Kanji uses in different times -- Kanji readings: On/Chinese and Kun/Japanese -- Two-Kanji words: Readings -- Kanji, Hancha, and Hanzi compared -- 17. Kana: Japanese syllabary -- Kana: Origin and development -- Kana graphs: Number and order -- How to use Kana -- Furigana or annotating Kana -- Katakana for foreign loan words -- 18. Rōmaji: Roman letters -- Rōmaji for European words and foreigners -- Rōmaji styles: Hepburn, Japanese, and Cabinet -- Should Rōmaji replace the Japanese scripts? -- Disadvantages of Rōmaji -- 19. Why keep Kanji? -- Kanji differentiate homophones -- Meanings of Kanji words are grasped well -- Kanji for compound words.\nKanji for technical terms and abbreviations -- Kanji stand out in mixed-script text -- Kanji for personal names -- Disadvantages of Kanji -- Typing and word processing -- Kanji use declined and then stabilized -- 20. History of mass literacy in Japan -- Early limited literacy -- Dawn of mass literacy -- Mass literacy after World War II -- History of books and publications -- Manga! Manga! -- 21. Learning and using Kanji and Kana -- Preschoolers acquire reading -- Kana and Kanji teaching in school -- Textbooks for reading instruction -- Kanji vs Kana: Naming and extracting meaning -- How well are Kanji read and written? -- Dyslexia or poor reading in Japanese -- 22. The Japanese educational system -- Primary and secondary school: Overview -- Preschool and primary school -- Middle and high school -- Tertiary education -- Japanese education: Problems and promises -- Summary and conclusions -- Part IV. Common issues -- 23. Eye movements and text writing in East Asia -- Eye movements in reading English text -- Eye movements in Chinese reading -- Eye movements in Japanese and Korean reading -- Conventions in writing/reading directions -- Punctuation marks and spacing -- Prose and paragraph structure -- 24. Reading and the brain -- Human brain: Structures and functions -- Brain processing when reading in Roman alphabets -- Brain processing when reading in East Asian scripts -- 25. East Asian students in international tests -- Programme for International Student Assessment (PISA) -- Top Ten in the 2006, 2009, and 2012 assessments -- PISA results: Some details -- Factors that may not influence achievement -- Factors that influence achievement sometimes -- Factors that may matter -- 26. Logographic characters vs phonetic scripts -- Logography, alphabet, and syllabary -- Direct vs indirect access to meaning and sound.\nWords in logography vs phonetic script: Reading aloud -- Logography vs phonetic script: Meaning extraction -- Flexible routes to sounds and meanings of words -- Logography vs phonetic script: Remembering -- Logography vs phonetic script: Learning to read -- Alphabet vs logography for science -- Effects of scripts and literacy on cognition -- Afterthoughts -- Glossary -- Bibliography -- Name index -- Subject index.\nThe book describes how the three East Asian writing systems-Chinese, Korean, and Japanese- originated, developed, and are used today. Uniquely, this book: (1) examines the three East Asian scripts (and English) together in relation to each other, and (2) discusses how these scripts are, and historically have been, used in literacy and how they are learned, written, read, and processed by the eyes, the brain, and the mind. In this second edition, the authors have included recent research findings on the uses of the scripts, added several new sections, and rewritten several other sections. They have also added a new Part IV to deal with issues that similarly involve all the four languages/scripts of their interest. The book is intended both for the general public and for interested scholars. Technical terms (listed in a glossary) are used only when absolutely necessary.\nDescription based on publisher supplied metadata and other sources.\nElectronic reproduction. Ann Arbor, Michigan : ProQuest Ebook Central, 2019. Available via World Wide Web. Access may be limited to ProQuest Ebook Central affiliated libraries.","Japanese/Japanese writing system\nThe Japanese language uses three different systems for writing. There are two syllabaries—hiragana and katakana—which have characters for each basic mora (syllable.) Along with the syllabaries, there are also kanji, which is a writing system based on Chinese characters. However, kanji have changed since their adoption, so it would not be recommended to learn both Chinese and Japanese writing at the same time.\nKanji[edit | edit source]\nThe kanji are logograms (pictures representing words), or symbols, that each represent a morpheme (words or parts of words.) Usually, each kanji represents a native Japanese morpheme as well as a loaned Chinese morpheme. This means that each kanji usually has two or more different pronunciations. The different pronunciations of a particular\nA 漢字 usually has two types of readings:\n音読み readings are approximations of the Chinese pronunciations of that particular 漢字. This reading is mostly used for multi-kanji compound words, except for peoples' surnames where 訓読み-reading is used. A kanji may have multiple 音読み. Some kanji are of Japanese origin and thus do not have on-reading. 訓読み readings are the native Japanese sound(s) associated with that 漢字. There can be multiple or no kun readings for the same kanji.\nAlthough there are over 50,000 漢字, the Japanese government has approved 1,945 so-called “daily use” 漢字, known as\nKana[edit | edit source]\nWhile Chinese characters are useful for writing a language with so many homophones, the inflections of the Japanese language make it necessary to have a phonetic script to indicate the inflection. A set of Chinese characters, the man'yōgana, were used to represent pronunciation and write words that lacked Chinese characters. Around 800 A.D. these had developed into the cursive hiragana script.\nThis method of writing was used primarily for poetry or by women, and did not gain recognition as an acceptable way to record historical records or scholarly works.\nAnother script, the katakana also developed from Chinese characters, some from the same source as the hiragana, but others from different ones. This explains the similarities between some hiragana and katakana, while others are completely different. The katakana is primarily used for foreign loan-words. In other words, the katakana syllabary can be said to be the Japanese writing equivalent of writing in italics.\nThe two are collectively known as the kana (\nPunctuation[edit | edit source]\nCommon punctuation marks are the comma \"、\" which connects two sentences, and the full stop \"。\" which indicates the end of a sentence. To separate words that the reader might not otherwise know how to read (most often in the case of foreign words written consecutively in katakana), a middle point \"・\" is used. Instead of quotation marks, the brackets \"「\" and \"」\" are used.\nExamples[edit | edit source]\n|「ウィキペディアは、オンライン百科事典である。」||(Wikipedia is an online encyclopedia.)|\n|「キャント・バイ・ミー・ラヴ」||(Can't buy me love) (Kyanto·bai·mī·ravu)|\nLatin alphabet[edit | edit source]\nThe Latin alphabet (ローマ字, rōmaji) is not part of the Japanese language but it is used as a fashionable way of writing words, mostly nouns such as the name of a company, business, sports team, etc. Rōmaji is also used for the transliteration of Japanese and to input Japanese text online and in word processors. There are two competing transliteration methods: the Kunrei-shiki developed by the Japanese government in the mid-20th century and taught in elementary school; and the more widely used Hepburn-shiki developed by Reverend James Curtis Hepburn in the late 19th century.\nStroke order[edit | edit source]\nJapanese characters were originally written by brush, and later by pen and pencil, so the stroke order is important. When writing by hand, and particularly in cursive or calligraphic styles, using proper stroke order is crucial. Additionally, some characters look very similar but are written differently. Students who practice both reading and writing can easily distinguish these characters, but students who only practice reading may find it difficult.\nThe East Asian Calligraphy wikibook has some material on stroke orders.\nMixed usage and notes of interest[edit | edit source]\nThere are instances where kanji, hiragana, and katakana may be replaced by another writing style. Frequently, words that have kanji are written in hiragana. Some kanji are simply rarely used but their reading is known. The swallow is called tsubame and has the kanji \"燕\", but since it is obscure, the word will generally be written out with hiragana: \"つばめ\".\nWhen writing for an audience that isn't expected to know certain kanji (such as in texts aimed at young people or kanji outside the standard set), their reading is often added on top of, or to the right of the characters, depending on whether they are written horizontally or vertically, respectively. This form of writing is called furigana (振り仮名) or yomigana (読み仮名).\nSince kanji can have several different readings, it may not be straightforward to determine how to read a certain word. This problem is particularly pronounced in place names where readings may be highly irregular and archaic.\nThough katakana are principally used for loan words from other languages, it can be used for stylistic purposes. Either to highlight a certain word, or give it a different feel (e.g. make it look more hip). Furthermore, since some personal names don't have kanji, but are written in hiragana, personal name readings are generally written in katakana to indicate that these are not the name itself, but simply the pronunciation.\nAteji[edit | edit source]\nThe word \"club\", as it is borrowed from English, will typically be written in katakana as クラブ; however, the kanji 倶楽部 kurabu will also sometimes be used; this use of kanji for phonetic value is called 当て字 ateji. Other times, typically in older texts, grammatical particles are also written in kanji, as in 東京迄行く Tokyo made iku ([I] go to Tokyo), where まで made (to/till) is written in kanji (迄) instead of hiragana.\nNumerals[edit | edit source]\nThe Arabic numerals, called Arabia sūji (アラビア数字) or san'yō sūji (算用数字) in Japanese, are used in most circumstances (e.g. telephone numbers, pricing, zip codes, speed limit signs and percentages). Kanji numerals can still be found, however, in more traditional situations (e.g. on some restaurant menus, formal invitations and tomb stones).\n|一||二||三||四||五||六||七||八||九||零 or 〇|\nVertical and horizontal writing, and page order[edit | edit source]\nTraditionally, Japanese is written in a format called 縦書き tategaki, or vertical writing. In this format, the characters are written in columns going from top to bottom. The columns are ordered from right to left, so at the bottom of each column the reader returns to the top of the next column on the left of the preceding one. This copies the column order of Chinese.\nModern Japanese also uses another writing format, called 横書き yokogaki, or horizontal writing. This writing format is identical to that of European languages such as English, with characters arranged in rows which are read from left to right, with successive rows going downwards.\nThere are no set rules for when each form has to be used, but usage tends to depend on the medium, genre, and subject. Tategaki is generally used to write essays, novels, poetry, newspapers, comics, and Japanese dictionaries. Yokogaki is generally used to write e-mails, how-to books, and scientific and mathematical writing (mathematical formulas are read from left to right, as in English).\nMaterials written in tategaki are bound on the right, with the reader reading from right to left and thus turning the pages from left to right to progress through the material. Materials written with yokogaki are bound on the left and the pages are turned from right to left, as in English.\nBackground reading[edit | edit source]\n- Okurigana Kana used as suffixes to kanji stems for verb conjugations. Historically, katakana was used. Nowadays, hiragana is used.\n- Man'yōgana Kanji used for their phonetic value to write Japanese, especially for poetry.\n- Kana The simplification of Man'yougana into Katakana and Hiragana\n- Katakana Angular script simplified down to constituent elements from kanji by monastary students. Historically used as okurigana by the educated and government. Nowadays used mainly for writing foreign words.\n- Hiragana Cursive script historically used for informal writing and literature. It became popular among women since they were denied higher education. Hence it also became known as 女手(おんなで) \"onnade\" (female hand -> women's writing). Nowadays, it has replaced katakana as okurigana and for writing native japanese words.\n- Hentaigana These are the remaining variants of hiragana that were not accepted as part of the standardized hiragana syllabary.\n- Iroha poem This famous poem is written using each mora (syllable) just once. It became the system used to organize the kana syllabary prior to reforms in the 19th century Meiji period, when it became reorganized into its current arrangement. (\"n\" was not part of the syllabary at the time. It was added later, and interestingly it's actually a hentaigana for \"mu\")\n- Kana The simplification of Man'yougana into Katakana and Hiragana\n- Rōmaji Roman characters (including Arabic numerals) There are three different systems."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:b37f1a82-3bed-4987-b792-77323e41a5b8>","<urn:uuid:26df3b8b-37e9-4a4f-890b-1f7a12d115c4>"],"error":null}
{"question":"I'm a gardener - can whiteflies and thrips both harm my fruit plants?","answer":"Yes, both whiteflies and thrips can damage fruit plants. Whiteflies harm plants by sucking plant sap with their piercing mouth parts, which causes leaf damage, sap drainage, and the production of honeydew that leads to black sooty mold. Thrips, on the other hand, feed on fruit surfaces causing cosmetic damage that appears as surface stippling (microscopic white dots) or surface russet at harvest. They are particularly damaging to nectarines and peaches, especially where multiple fruits or leaves contact each other. Both pests are prolific - whiteflies readily migrate to infest nearby vegetation, while thrips can complete their life cycle in just two weeks with several overlapping generations per year.","context":["- WHAT ARE WHITEFLIES?\n- PLANT DAMAGE WHITEFLIES WILL DO\n- HOW TO CONTROL WHITEFLIES\n- INSTALL WHITEFLY TRAPS\n- ORGANIC WHITEFLY SPRAY\n- GARDEN SAFE WHITEFLY SPRAY\n- WHITEFLY SYSTEMICS\n- SPRAY EQUIPMENT NEEDED\n- GREENHOUSE FOGGERS and FOGGING MACHINES\n- THINK LONG TERM FOR ONGOING WHITEFLY PREVENTION\n- CONTACT US\nThis article will detail whitefly growth cycle, explain why they are such a hard pest to control and then list treatment options which work.\nRelated articles: APHIDS APPLE MAGGOTS BAGWORMS BUDWORMS THRIPS TWIG GIRDLERS\nWHAT ARE WHITEFLIES? ^\nWhiteflies are small winged insects which look more like moths than flies. They have a powdery wax on their body and wings which both protects them and is key to identification. They are active in all parts of the world and can thrive year round in the south but go dormant during the winter further north. Though whiteflies can be a tough and persistent pest, the right combination of chemicals combined with the right timing of treatment can solve any infestation.\nWHITEFLY SPECIES ^\nWhiteflies have two main species which are active in the United States. The Citrus and the Greenhouse are tje most common found but others exist. Citrus whiteflies are found on just about any plant – not just fruit. Citrus whiteflies are hardy and most likely to be active year round. This especially true in the south.\nGreenhouse whiteflies are easier to control and will go dormant as winter sets in around the country. Greenhouse whiteflies are more prevalent in greenhouses though plants which move to homes and gardens carrying active populations will surely enable them to get established.\nWHITEFLY LIFE CYCLE ^\nBoth species develop from eggs and grow through a series of instars. The last instar is the pupa stage during which young change to adults.\nAdult whiteflies will lay eggs in less than a week once emerged. Eggs are very small; almost invisible. Eggs will be laid on the underside of leaves, in hard to see locations, in generally well protected parts of the plant.\nOnce the young emerge, they will immediately begin to feed. Young are round and almost “clear” so they are easy to miss. Though their development can occur in less than a month, a much longer development time frame is not uncommon. Cycles can take as long as a year if conditions are not so favorable. However, regardless of how long it takes them to develop in your yard, damage will certainly happen to any host plant where they reside.\nPLANT DAMAGE WHITEFLIES WILL DO ^\nWhiteflies are a problem because they have piercing mouth parts which allow them to suck plant sap. This behavior is what will lead to a range of problems for host plants prone to attack. The short list of related issues from feeding whiteflies includes:\n- Leaf damage. Feeding whiteflies will cause leaves to turn up and brown, disabling them to perform their responsibilities. Damaged leaves look bad and promote decay fungus and bacteria to form.\n- Sap drainage. Whiteflies have an insatiable hunger for plant sap and will suck as much of it as they can from the host plant. Young host plants will not be able to meet these demands and don’t stand much of a chance to survive; adult plants which loose vital sap are more prone to disease, heat stress and less likely to rebound from any environmental shock.\n- Whiteflies excrete honeydew which lures other nuisance insects onto the host plant. These secondary insects will feed off the honeydew and effectively farm it for it’s nutritional value but invariably they will become established on the property or in nearby structures. This secondary insect infestation is usually some type of ant, wasp or beetle. Regardless, feeding whiteflies allow such populations to thrive. Furthermore, the honeydew goes bad and grows a mold known as Black Sooty. This mold damages host plants preventing them from processing food properly further contributing to the host plants demise.\n- Whiteflies are unsightly when they populate any plant and if prized landscaping gets infested it really looks bad. Active whiteflies will buzz around host plants throughout the day and are very noticeable. They are annoying when active on fruit or flower producing stock and can easily be brought into the home with anything harvested off infested plants.\n- Lastly, whiteflies are extremely prolific. Once they get established on any plant around the home or garden they will readily migrate and look to infest any other nearby vegetation. Whiteflies don’t discriminate; they will move onto any plant that has healthy and abundant sap. It is not uncommon to have local populations thrive on some field of weeds only to start migrating into someone’s prized roses.\nHOW TO CONTROL WHITEFLIES ^\nTo control an active problem or prevent whiteflies from getting established in your yard or garden, we have a range of liquid concentrates. This range of options include products for the organic gardener as well as the traditional tomato grower. We even have systemics for fruits and vegetables as well as any type of shrub or tree. Systemics offer long term protection and only need be applied 1-2 a year.\nINSTALL WHITEFLY TRAPS\nWHITEFLY TRAPS can first identify a whitefly problem before it gets bad. Finding just a few whiteflies means more will soon be active so take this as a warning and treat.\nOur traps are bright yellow which whiteflies will see easily and be drawn to them. Traps can help slow a problem by catching foraging reproductives. Traps can also be your first line of defense serving as an alarm. Once a trap catches a whitefly, you’ll know its time to treat.\nEffective on on garden pests like APHIDS too, set them out every 1000 sq/ft or every 10 feet along a garden row.\nIf you already have whiteflies active in your garden or house plants, using traps alone probably won’t solve the problem so consider one of the treatment options below.\nORGANIC WHITEFLY SPRAY ^\nFor the organic gardener, we have two concentrates safe enough for fruits and vegetables yet strong enough to control even stubborn whiteflies.\nThe “best” is the MULTI PURPOSE INSECT KILLER. This product works immediately will be mixed with water to be sprayed as needed.\nMultipurpose Insect Killer will provide a fast knockdown of all garden pests including whiteflies. Expect to see dead bugs a few minutes after being sprayed. But since nothing can kill the developing eggs and instars, plan on spraying once a week for at least 2 weeks (3 treatments total) to get rid of them food good.\nAdd 6.4 oz per gallon of water and plan on getting 500-750 sq/ft of plant foliage treated per mixed gallon. Multipurpose Insect Killer has a “one day to harvest” which means its safe enough to use right up to the day before you harvest your fruits or vegetables. It comes in both quarts and gallons.\nOur 3 IN 1 CONCENTRATE is similar to Multipurpose Insect Killer but it includes a fungicide making it a “dual action” product. So if your plants have been suffering with a fungus or mold related to the whitefly activity, use this option.\nAdd 4-8 oz to a gallon of water to cover up to 500 sq/ft of plant foliage. Like the Multipurpose Insect Killer, you should plan on spraying weekly for the first 2 weeks (3 times total) to get rid of the current whiteflies and control the fungus. Treatments will work immediately on the whiteflies but it will take a week or so for the fungus to wither away.\nGARDEN SAFE WHITEFLY SPRAY ^\nIf you want a more “traditional” (aka: stronger) product for your garden, go with VEG PLUS PERMETHRIN. This active can be safely used on plants yielding fruits and vegetables although the “days to harvest” will be 7-14 days compared to organic products which are gone in day. This is true since these products provide residual.\nVeg Plus Permethrin is odorless, labeled for use on garden plants and will work when applied. Whiteflies will immediately die of being sprayed and treatments can last up to 2 weeks. But if you notice the whiteflies returning before two weeks, spray again.\nMix 1/2 oz per gallon of water and plan on getting 500-750 sq/ft of coverage per mixed gallon.\nIf you have a large garden to treat, CYONARA RTS might be more efficient to use. It too is a concentrate but it comes in a handy “ready to spray” jug. This jug hooks to your garden hose and using the power of your water pressure, will automatically mix the concentrate with water creating a total of 20 gallons of mixed material.\nEach jug can cover up to 16,000 sq/ft and treatments will last up to 2 weeks. Expect a fast kill but if whiteflies come back in less than 2 weeks, spray again.\nWHITEFLY SYSTEMICS ^\nThe longest lasting treatment for whiteflies are products know as systemics. When fed to the plant as a soil drench, these actives will be absorbed by the roots of the plant and effectively become part of the foliage. This process will take 2-6 weeks to occur but once the active makes it’s way to the foliage, any insect trying to eat the plant will die.\nLAWN TREE AND SHRUB SYSTEMICS ^\nSo if you have whiteflies in the lawn or trees and want season long control, use PROTHOR.\nProthor can last up to a whole year so in general, if you spray in the spring, you shouldn’t get insects on the treated tree for the year. Prothor is a great “one spray” protection for most any plant in the yard for a range of insects including whiteflies.\nNow if you treat mid or late season, Prothor will take care of the current activity but it could take several weeks to reach the foliage. So to get the active insects immediately, mix some up for spraying affected areas.\nFor mixing up a spray-able solution for surface treatments, add .5 oz of Prothor per gallon of water and use the mixture over 1,000 sq/ft of surface area.\nFor systemic treatments, add .1 oz per inch of tree or hedge trunk thickness. So if you have a holly with 3 main trunks coming up from the base and each trunk is about 3″ wide, add 1 oz of Prothor in a 5 gallon plastic pail with 3-4 gallons water. Next, make 5-8 holes in the ground within 1-2 feet of the hedge. Use a piece of rebar or a pick axe to make the holes. Next, pour out the mixture of Prothor into the holes being sure to pour it slow enough so the material doesn’t run away from the plant. If done properly, the treatment will remain in the ground around the shrub where it will be absorbed in the coming weeks following the application.\nSPRAY EQUIPMENT NEEDED ^\nWhen spraying plant foliage, you’ll need a sprayer. We have several options all of which have advantages and disadvantages.\nFor small jobs, our standard PUMP SPRAYER is adequate. It requires pressure so you have to manually pump it but once pressurized, expect to reach 15 feet out or up.\nA good HOSE END SPRAYER can also be used. The advantage of using this design is that it will use the power of your garden hose which means it can sometimes spray 25 feet or more. Also, you only have to add small amounts of chemical to the tank when using it to spray any of the concentrates above. If you decide to get one and a concentrate above, email us for specific directions on how to prepare the sprayer before hooking it to your hose to treat.\nLastly, the TROMBONE SPRAYER is a totally portable option that is handy when you need to treat high locations like an whitefly infested tree. Using the power of the “slide” mechanism much like a sliding trombone, one can reach 30 feet or more.\nThe sprayer comes with an 8 foot long hose you drop into a pail of mixed insecticide. Your “sliding” action will in turn pump out the chemical targeting the area you direct your spray.\nGREENHOUSE FOGGERS and FOGGING MACHINES\nIf you have a greenhouse or small confined area with whitefly activity, you might consider a misting machines.\nIf you’d like a more “controlled” application done over time, install METERED AEROSOL MACHINES. These discreet machines are easy to deploy and go off every 15 minutes releasing a small amount of pyrethrin. This naturally occurring insecticide is active on whiteflies and will take out active insects around the clock. Because the machine is going off all the time, you don’t have to worry about a new population getting established. After 1-2 weeks of keeping the machine on, the active problem should be cured.\nThis machine comes in three basic designs. The AERO 1000 is a fixed machine meaning it will only go off every 15 minutes. The AERO 2000 has a photo cell to sense light so you can program it to go off during the day or night only and the AERO 3000 has a range of timer option so you can increase the frequency to more than every 15 minutes meaning it can go off every 5 minutes, every 10 minutes, every 30 minutes, etc.\nEach machine will need a charged can of CLEAR ZONE INSECTICIDE. The can will last up to 30 days when set to release product every 15 minutes and will control a range of flying pests like whiteflies.\nWHITEFLY FOGGING MACHINE ^\nFor large greenhouses, a good FOGGING MACHINE can cut costs and do a great job controlling whiteflies. These devices convert water to a “fog” so you can effectively blanket all the foliage of your plants without having to worry about whiteflies hiding.\nThis machine features a 1/2 gallon tank and pumps about 3 oz per minute. For covering 10,000 sq/ft or less, its plenty powerful. Just plug it in and direct the mist over the plants you want to protect. Its well suited for the yard too where it can be used to control a range of pests like mosquitoes and gnats.\nThere are several chemicals you can use in the fogging machine but in most cases, ESSENTRIA IC is a good choice. Made with food grade actives, Essentria can be used safely most anywhere and it can be applied as needed. With pests like whiteflies, treatments can be needed more than once a week. And having a product you can apply without having to worry about over applying it is nice.\nAdd 3 oz of Essentria to the tank and plan on using the 1/2 gallon mixed solution for every 5,000 sq/ft. Essentria will control any pest active in the treated area and since treatments will be easy to do as needed, there is not much prep work required. Just let the treatments settle for at least 1 hour after fogging before entering the treated zone.\nTHINK LONG TERM FOR ONGOING WHITEFLY PREVENTION ^\nOnce you get your whitefly problem under contro, plan on being ready to treat again.\n- If you are in a region with a lot of local whitefly problems, don’t think your plants will be overlooked or “missed” by migrating flies. Inspect for whitefly activity throughout the growing season and use traps to help spot colonizing adults.\n- Start treatments as soon as you identify activity and don’t rely on one or two sprayings to solve an active problem. Once you think you have it solved be sure to check for new flies every week.\n- For any stubborn infestations, treat more frequently for the first two weeks. This will ensure you break their cycle.\n- Remember that removing infested plants will generally only do just that; remove a plant which had whitefly activity but it will not get rid of the problem. Whiteflies are here to stay!\n- All your other plants are just as likely to one day soon develop a problem if you don’t protect them with a little bit of preventive maintenance.\nWhiteflies can be a persistent pest which seems to always be around and never go away. Use traps to monitor local activity and once you’re sure you have activity, begin a management program using our products listed above. Remember, treating early is always best as it will save time, money and effort. Once you have the outbreak under control, be sure to watch for their return and be particularly careful the following year since this is when most growers seem to forget the previous years happenstance.\nCONTACT US ^\nGive us a call if you need further help. Our toll free is 1-800-877-7290 and we’re open Monday through Friday, 9:00 AM to 4:00 PM. On Saturday, 9:00 AM to 2:00 PM (Eastern Standard Time).\nEmail questions here: https://bugspray.com/about-us/contact-us\nOrder online and get a 5% discount! We ship fast with 99.9% of all orders shipping within 1 business day!!\nLearn more about BUGSPRAY.COM and why it’s never been easier or safer to do your own pest control.\nPlease show your support for our business by purchasing the items we recommend from the links provided. Remember, this is the only way we can stay around to answer your questions and keep this valuable web site up and running. Thanks for your business!","Pest description and crop damage Adult thrips are yellowish brown to straw colored and about 0.04 to 0.08 inch long. Nymphs resemble adults but are smaller and lack wings. Thrips feed on the fruit surface and often cause only cosmetic damage to developing fruit which appears as a surface stippling (microscopic little white dots) or as a surface russet at harvest. Sometimes, the stippling may manifest itself on the fruit surface where multiple fruits or leaves contact each other. Nectarine is more susceptible than peach to thrips damage. Often thrips develop and overwinter on hosts outside the orchard; thus their damage is heaviest in the perimeter rows.\nBiology and life history Western flower thrips overwinter as adults in ground duff. Thrips usually emerge early, about popcorn stage. In the spring they seek out flowers where they feed on pollen and nectar and lay eggs into floral parts. The larvae feed on surface tissues of flowers, buds and leaves. When mature, the larvae drop to the ground and pupate. The adults that emerge may lay eggs on developing fruit. Under favorable conditions, a complete life cycle may be completed in two weeks. There are several overlapping generations per year. Seasonal migration occurs at various times of the year due to destruction or drying up of host plants, and adjacent crops may be invaded by these insects. Thrips eat pollen and nectar on a wide range of plants (at least 244 species from 62 families).\nPest monitoring In fruit trees with a history of thrips damage to fruit in previous seasons, thrips abundance during flower bloom can be monitored by destructively sampling flowers and searching for the adult and immature thrips. Yellow sticky traps can also be utilized to sample thrips. Scouting can be focused on perimeter fruit trees or in areas within the orchard with a history of thrips damage to fruit.\nThe best means of controlling thrips is by intercepting them as they move into the orchard or by applying products after petal fall to try to kill the thrips under the peach or nectarine shuck.\nManagement-chemical control: HOME USE\nPetal fall spray\n- acetamiprid-Toxic to bees.\n- azadirachtin (neem oil)-Some formulations are OMRI-listed for organic use.\n- gamma-cyhalothrin--Highly toxic to bees.\n- horticultural mineral oil-Some formulations OMRI-listed for organic use.\n- insecticidal soap-May require several applications. Complete coverage is essential. Not recommended for use on yellow-skin nectarine varieties. Some formulations are OMRI-listed for organic use.\n- kaolin clay (Surround at Home)-Repels some insect pests when applied as a spray to leaves, stems, and fruit. OMRI-listed for organic use.\n- lambda-cyhalothrin--Highly toxic to bees.\n- plant-derived essential oils-Some have shown efficacy against thrips. Some formulations are OMRI-listed for organic use.\n- pyrethrins-Highly toxic to bees. Some formulations are OMRI-listed for organic use.\n- spinosad-Toxic to bees. Some formulations are OMRI-listed for organic use.\n- zeta-cypermethrin--Highly toxic to bees.\nManagement-chemical control: COMMERCIAL USE\nPetal fall spray\n- spinetoram (Delegate WG) at 4.5 to 7 oz/a. REI 4 hr. PHI 1 day.\n- spinosad (Entrust 80W) at 1.25 to 2.5 oz/a. REI 4 hr. PHI 1 day. Results are best when applied at petal fall. OMRI-listed for organic use. May act slowly."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:40d90097-4e71-4b49-8310-16558652088d>","<urn:uuid:7416d28f-adea-4090-8156-56d2acf2065a>"],"error":null}
{"question":"Hi! I'm studying environmental solutions. Could someone explain how do compost-based carbon removal methods compare to direct air capture technology in terms of cost-effectiveness and implementation scale?","answer":"Compost-based carbon removal and direct air capture represent different approaches to reducing atmospheric carbon. Compost application to rangelands has shown promising results, with tests indicating it can absorb around 0.4 tons of carbon per acre annually. When applied to just 5% of California's rangelands (3.2 million acres), this method could eliminate 7.6 million tons of carbon emissions over three years. In contrast, direct air capture technology, as implemented by companies like Climeworks, can remove several tonnes of CO2 daily through a cyclic adsorption-desorption process. While compost is a relatively low-cost solution that also improves soil health, direct air capture technology currently costs between $94 and $232 per tonne of CO2 removed, with companies aiming to reduce costs to $100 per tonne by 2030. Both methods are being explored as complementary solutions, as scientists estimate 10 billion tonnes of CO2 will need to be removed annually from 2050 onwards to meet climate targets.","context":["You already knew compost was a great addition to your home garden. But have you heard that compost also can do wonders for grasslands while also helping to slow climate change? The concept makes total sense when you think about it, and thanks to a ranch-owning couple who decided to think outside the box to help their ranch’s failing soil, we know it can work.\nHow it All Began\nIt all started in 1998 when John Wick and Peggy Rathmann bought a 540-acre California ranch. Unfortunitly, the ranch’s soil had seen much better days. Years of overgrazing hurt the ranch's land, which made it difficult for the ranchers to grow grass.\nTo help remedy the problem, Wick called Jeffrey Creque, a rangeland ecology expert. After discussing the problem, it was decided that the soil would improve if the ranchers planted grass. An obvious conclusion, but that simple thought gave the two a brilliant idea. \"They knew that in addition to enriching the soil, healthy grass, through photosynthesis, could remove carbon from the atmosphere,\" Mother Jones reports. \"So was there a way, they wondered, to grow more grass on Wick's land and slow global warming at the same time?\"\nReally? Yes, really.\n“Carbon that is absorbed by grass can be stored for hundreds of years in the grass’ roots and surrounding soil—a much better spot for it than in the air, where it warms the planet in the form of carbon dioxide. Carbon-enriched soil, in turn, feeds grass so it can grow taller and suck down even more carbon.”\nThis process happens on a pretty big scale in rangelands--almost a third of the world’s carbon is stored in grass and soil. But processes, such as tilling and overgrazing, allow carbon to escape while causing topsoil erosion. This makes grass difficult to grow.\nAnd when rangelands are populated by livestock, the problem can get worse because, well, cow poop. “Manure releases methane and nitrous oxide gases into the atmosphere,” Mother Jones reports. “In fact, livestock are responsible for nearly one-fifth of the globe’s overall greenhouse gas emissions.”\nSo, to help the ranchers' grass along, Creque decided to add compost to the mix. Compost is filled with carbon-rich organic waste--basically, yard clippings, food scraps, and manure can help eat carbon when spread across soil--or rangelands, in this case, that “digest the carbon and deposit it into the soil before it could escape into the air. The rich soil, now full of carbon and microbes, would feed the grass—which would absorb yet more carbon.”\nFrom the Organic Authority Files\nWick decided to give the idea a shot and got the help of Whendee Silver, an environmental scientist at the University of California-Berkeley. They secured grants to run a large-scale compost experiment. “For about $300 plus the cost of shipping, Wick bought 31 tons of compost from a facility that collects it from homes in the Sacramento Valley. Wick and Silver then spread it out on Wick's ranch,” Mother Jones reports. “The results exceeded expectations. As the grass grew, tests suggested it absorbed an average of 0.4 tons of carbon per acre a year—and that, all told, the land was capturing at least 25 percent more carbon. Silver has since tried the concept in other climates—it works even in the hot, dry Sierra foothills.”\nWhat Does This Mean?\nThe big story here is that if \"compost were added to 5 percent of California's rangelands, Silver calculates, those 3.2 million acres could eliminate 7.6 million tons of carbon emissions over a three-year period, equal to taking about 2 million cars off the roads annually.\"\nThis is a big idea. And if compost can help keep soil healthy, grow more food, and slow climate change, why aren't we doing it on a larger scale?\nRelated on Organic Authority\nImage of getting ready to compost via Shutterstock","Germany aims to cut its greenhouse gas emissions some 40 percent by 2020. This target rises to 55 percent by 2030 and 95 percent by 2050 when compared with 1990 levels. Many other countries across the world have issued similar pledges.\nThe bad news, though, is this won’t be good enough: efforts to decarbonise the global economy have been delayed to such an extent that reducing emissions now comes too little, too late.\nClimate prediction models indicate that, in order to meet the targets established by the Paris Agreement, carbon dioxide (CO2) will have to be removed from the atmosphere on a substantial scale.\nIn fact, scientists now estimate limiting the global temperature rise to two degrees Celsius above pre-industrial levels will require 10 billion tonnes of CO2 to be removed from the atmosphere every year from 2050 onwards.\nReforestation is the most natural way of sucking carbon out of the air, but this is time consuming and would need to take place on a gigantic scale. As global populations increase, convincing governments to plant trees on spare land, rather than build homes, is a difficult sell.\nA combination of renewable energy investment and carbon-removal solutions may be the only way to avoid an environmental disaster in the coming years\nOther businesses are looking at carbon capture and sequestration techniques that trap CO2 before it reaches the atmosphere, but this approach is often only carbon-neutral at best.\nIn order to solve the planet’s carbon conundrum, a number of businesses have started thinking outside the box, employing cutting-edge technology to remove existing CO2 from the planet’s atmosphere. In many cases, their ideas have proved to be effective; whether they are financially feasible is another matter entirely.\nWith the likelihood that humanity will be able to curb its CO2 emissions in time to avert catastrophe looking increasingly slim, a number of businesses have begun to search for alternative options. Their approaches to CO2 removal vary widely, with each methodology bringing its own advantages and disadvantages.\nOne of the more unusual ideas involves supplying natural gas to a high-temperature fuel cell. Half of the energy created is converted into electricity, and the other half is used to decompose limestone into lime and CO2.\nAs this activity produces CO2, it may not initially be clear how it helps with the planet’s greenhouse gas problem. However, the entire process is actually carbon-negative. As all of the CO2 being produced – both from the fuel cell and the lime kiln – is pure, it can be used or stored underground at a low cost. The lime can also be used to trap CO2.\nThe company pioneering this method of carbon air capture, Origen Power, estimates it can remove 600g of CO2 from the air for every kWh of electricity generated. In contrast, electricity produced by burning natural gas typically emits around 400g of CO2 for the same energy output.\nOrganisations like Canada’s Carbon Engineering and the Zurich-based Climeworks have opted for a different approach, utilising direct air capture technology. The latter develops, builds and operates plants that capture CO2 from ambient air through a cyclic adsorption-desorption process.\n“At the heart of the process is a filter material, which selectively captures CO2,” Louise Charles, Communications Manager at Climeworks, told The New Economy. “The first step involves air being drawn into a collector using a fan.\n“CO2 is then captured on the surface of a filter material – also known as the adsorption stage. The second step begins when saturation has occurred. Then, the collector is closed and the temperature increased to 100 degrees Celsius, releasing the CO2 (desorption) at a purity of over 99 percent. The CO2 is then cooled to 45 degrees Celsius, collected and delivered where necessary.”\nCost of carbon removal (per tonne)\nIt should be remembered there is no silver bullet to the problem of climate change. The disparate ideas that businesses like Origen Power and Climeworks are exploring may all need to be deployed together if a decarbonised future is to be achieved.\nUltimately, a combination of renewable energy investment and carbon removal solutions may be the only way to avoid an environmental disaster in the coming years.\nCosting the Earth\nThe suitability of carbon-removal projects is not in doubt – their affordability is. In 2011, a study led by researchers from the Massachusetts Institute of Technology found that, when focusing on ambient air, the cost of carbon removal was likely to be more than $907 per tonne of CO2 removed.\nAlthough they are not truly carbon-negative, powerplant scrubbers can boast a figure of between $50 and $100 for every tonne of CO2 prevented from reaching the atmosphere.\nRecently, however, scientists have become increasingly optimistic that carbon removal is not as expensive as first feared. An updated analysis, published in June, suggested carbon capture could be achieved at a price between $94 and $232 per tonne if existing technologies were scaled up. Many of the businesses working in this field are working on the assumption that their current costs can be significantly reduced.\n“We are already commercially viable and, as we continue to develop our technology, we will become increasingly cost competitive across the world,” Charles explained. “We have a detailed cost-reduction roadmap in place and are confident we’ll reach a price level of $200 per tonne of CO2 in three to four years’ time. Our long-term goal of $100 per tonne of CO2 is achievable within the next five to 10 years – or by 2030 at the very latest.”\nScaling up will be key to optimising outgoings, and progress on this front has been steady, if not spectacular. When Climeworks was founded in 2009, it was only capable of removing a few milligrams of CO2 in a 24-hour period; now, several tonnes are being captured every day.\nThe company is aiming to scale up by a factor of one million in the next five to 10 years in order to make a broader impact. To achieve this, more customers and additional funding will be required.\nThe next step\nCarbon capture, in one form or another, has been implemented since the 1970s, but the technology’s progress has been slow. Globally, there are 17 large-scale plants dedicated to carbon capture, most of which are in the US. Altogether, they remove around 40 million tonnes of CO2 per annum – equivalent to just one percent of yearly global emissions.\nGuaranteeing these facilities receive the right government support will be vital to their development. Subsidies for carbon removal need to be implemented to ensure the right economic incentives are in place for firms. Renewable energies have benefitted hugely from this approach and there is no reason why CO2 removal wouldn’t similarly profit.\nBusinesses also need to think carefully about what they do with the CO2 once they remove it from the air. Making it into synthetic fuels is one option; burying it underground is another.\nCharles said her company “sells high-purity, high-concentration, air-captured CO2 to the food, beverage and agriculture market, [as well as] for the synthesis of renewable fuels and materials”. By forging external partnerships, carbon-removal firms gain access to additional revenue streams and ensure companies can purchase sustainable supplies of CO2.\nIt is regrettable that mankind has reached a stage where reducing carbon emissions will not be sufficient to stop temperatures exceeding globally agreed targets. But all hope is not lost: while CO2 removal can’t turn back the clock on decades of polluting human activity, it could provide a way to avoid the potentially devastating consequences of climate change."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:1239d2ba-bf44-4020-add2-070023fd985e>","<urn:uuid:aa0edff0-4d25-4ea5-9b0d-02b5098abec5>"],"error":null}
{"question":"How does the Chittamatra system explain the experience of multiple people seeing the same object?","answer":"According to the generally accepted Gelug Chittamatra view, when multiple people see what appears to be the same object (like a clay pot) simultaneously, each person is actually experiencing the ripening of tendencies of collective karma on their own mental continuum. There is no common locus object that everyone is seeing, even conventionally. However, the object is still considered findable - everyone can point to it based on what they validly see. When asked to point to a clay pot on a table, each person will validly see others pointing to the same clay pot that they themselves validly see there. While a meaning/object category of what an object signifies (like 'clay pot') is imputable based on everyone's simultaneous valid cognition, this category exists only as something merely imputable by conceptual cognition.","context":["The Gelug Chittamatra Assertion of No External Phenomena\nJune, 2002, revised May 2006\nbased on explanations received from Serkong Rinpoche and Geshe Dawa\nThe Chittamatra (sems-tsam, Mind-Only) tenet system asserts that there are no such things as external phenomena (phyi-don). Let us examine the way in which the Gelug tradition understands this central point of the Chittamatra system.\nExternal phenomena are nonstatic (impermanent) phenomena that arise from different natal sources (rdzas) than the valid cognitions of them. One of the subtle lacks of impossible soul of phenomena (chos-kyi bdag-med phra-mo, subtle identitylessness of phenomena, subtle selflessness of phenomena) is the voidness of forms of physical phenomena, and the valid cognitions that take these forms of physical phenomena as their cognitive objects, deriving from different natal sources (gzugs-dang gzugs-’dzin-pa’i tshad-ma rdzas-gzhan-gyis stong-pa).\nChittamatra, however, does not deny the existence of forms of physical phenomena. After all, as dependent phenomena (gzhan-dbang, other-powered phenomena), they are devoid of existing in the manner of nonexistent totally conceptional phenomena (kun-brtags, totally imaginary phenomena). In other words, they do not exist as nonexistent items to be nullified (dgag-bya, objects to be refuted) that cannot be validly cognized. Forms of physical phenomena can be validly known. As nonstatic phenomena, they arise from causes and conditions, perform functions, and produce effects.\nMoreover, Gelug Chittamatra asserts that not only are sensibilia forms of physical phenomena, but so are commonsense objects (‘jig-rten la-grags-pa). Sensibilia include the most basic data cognized by sensory cognition: namely, colored shapes, sounds, smells, tastes, and physical sensations. Commonsense objects are items such as clay pots, tables, oranges, and human bodies.\nThe Gelug tradition, while acknowledging that there is a False Aspectarian (rnam brdzun-pa) interpretation of Chittamatra, favors the True Aspectarian (rnam bden-pa) presentation. According to this presentation, whatever appears grossly to valid sensory cognition is established as existing as how it appears (snang-ba ltar-du grub-pa). Thus, in a valid nonconceptual visual cognition, the appearance of colored shapes pervading areas of pixels (particles) and the appearance of commonsense objects exist as they appear. These appearances are not merely interpolations onto the appearance-making (gsal, clarity) aspect of mind, which is beyond the duality of objects and consciousnesses of them.\nFurther, all these forms of physical phenomena – pixels, colored shapes, and commonsense objects – have existence established by their own self-natures, truly established existence, existence established as ultimate phenomena, and existence established by their individual defining characteristic marks.\nExistence established by something’s self-nature (rang-bzhin-gyis grub-pa, findably established existence, inherent existence) is defined as existence established by the fact that when one searches for the referent object of the imputation of something (btags-don), that referent object is findable. The referent object is findable on the side of that object.\nTruly existent (bden-par grub-pa) is defined as existence established as an ultimate phenomenon (don-dam-par grub-pa).\nExistence established as an ultimate phenomena is defined as existence established by something’s individual defining characteristic mark. Ultimate phenomena are those phenomena that appear to an arya’s total absorption (‘phags-pa’i mnyam-bzhag, an arya’s meditative equipoise) on the four noble truths.\nExistence established by individual defining characteristic marks is defined as existence established from the side of something’s own unshared way of abiding (mthun-mong ma-yin-pa’i sdod-lugs) as a cognitive object that is not merely imputable by a conceptual cognition.\nBecause of the instincts of unawareness (ma-rig-pa’i bag-chags, habits of ignorance) of the lack of an impossible soul of phenomena, forms of physical phenomena appear to exist externally. However, this deceptive appearance (‘khrul-shes) is like an illusion (sgyu-ma lta-bu). They are devoid of existing in this impossible way.\nIf forms of physical phenomena existed externally, they would need to exist independently of the phenomena of validly experiencing them nonconceptually. Moreover, they would need to exist before the valid cognitions of them, so that they could function as the focal conditions (dmigs-rkyen) for the valid cognitions of them. This would be like the model suggested by Biblical creation. First, there was heaven and earth – external objects – and then man, implying mind, experiencing them. Western science has a parallel view. First, the universe came into existence and, later, life evolved to experience it. Buddhism, on the other hand, asserts that both the universe and mind (mental activity) are beginningless and endless, and therefore there have never been external objects existing independently of mind. Even to speak about or think about such objects as existing when they are not cognized is still cognizing them.\nThus, in the presentation of how things exist, Chittamatra does not speak about how, for instance, a clay pot itself exists, but how the visual form of one exists in the context of a moment of validly seeing it. In fact, there is no such thing as a clay pot itself, existing independently of and outside the context of:\nthe valid seeing of the visual form of one,\nthe valid hearing of the sound of one being tapped,\nthe valid feeling of the physical sensation of one in our hands,\nthe valid thinking about or the valid recalling of any of those three through a mental representation of it, or\nthe valid talking or writing about one through a verbal representation (a word) for one.\nWhen we validly see the clay pot, the visual form of the clay pot that we see does not exist separately from the visual consciousness to which it appears. Moreover, the visual form is unique to that moment of cognition. For instance, when we validly see the visual form of the clay pot on the table, that visual form does not exist identically with the visual form of the clay pot on the table that someone else validly sees from the other side of the room. Nor does it exist later in the valid visual cognition of the clay pot on the floor. This is because Chittamatra asserts that forms of physical phenomena exist by their self-establishing natures. Such existence means that the referent object of the imputation of the clay pot is findable, but findable only in the context of the valid cognition of the clay pot. Therefore, the findable existence of the clay pot in exactly that same position, angle, age, state of cleanliness, and so on is only in the context of the specific mind validly cognizing it at that moment, and none other.\nNevertheless, according to Tsongkhapa’s interpretation of Chittamatra during the earlier part of his life, there is conventionally (tha-snyad-du) a common locus (gzhi-mthun) clay pot involved when several people all validly see a clay pot at the same time, but each from a different angle and distance. Ultimately, however, there is no common locus clay pot. A common locus clay pot could not be an object cognized by an arya’s total absorption.\nIn his later writings on Chittamatra, Tsongkhapa interprets Chittamatra as asserting that there is no common locus clay pot even conventionally existent. This is the generally accepted Gelug Chittamatra view. When several people validly see a clay pot at the same time, each person is experiencing the ripening of tendencies (sa-bon, seeds) of collective karma on his or her own mental continuum. It is not that each person is validly seeing a common locus clay pot. Nevertheless, the clay pot is findable – everyone can point to one, in terms of what each person validly sees. In fact, when all of them are asked to point to a clay pot that they see on a table, for instance, each will validly see all the others pointing to the same clay pot that he or she validly sees there.\nNote that a common locus clay pot is not the same as a meaning/object category (don-spyi, meaningful idea) of what the expression clay pot signifies. The expression clay pot signifies a category of phenomena (individual clay pots) and means that category. The category clay pot, then, is a category for classifying physical objects into a set and for classifying the meaning of the expression clay pot into a set, no matter who says those words.\nAll Tibetan interpretations of Chittamatra accept that a meaning/object category of what the expression clay pot signifies is imputable on the basis of everyone’s simultaneous valid cognition of a clay pot there, but from different angles and distances. Like all totally conceptional phenomena, however, such a category does not have existence established by an individual defining characteristic mark, independently of it being merely imputable by a conceptual cognition.\nMoreover, the clay pot is devoid of having existence established by individual defining characteristic marks on its own side as foundations on which affix the sound of the words clay pot (bum-pa bum-pa-zhes-pa’i sgra rang-gi ‘jog-gzhir rang-gi mtshan-nyid-kyis grub-pas stong-pa). The clay pot that we see has an individual defining characteristic mark findable on its own side that makes this physical object an individual validly knowable object, independently of the object being merely imputable by a conceptual cognition of it. However, it lacks any individual defining characteristic marks findable on its own side that – either by themselves alone or in conjunction with a conceptual cognition of it with a mental label – make it into a “clay pot,” “ein Topf,” a “bum-pa,” or even that make it “beautiful,” “ugly,” “large,” or “small.”\nAccording to Chittamatra, in the valid visual cognition of the visual form (sight) of a clay pot, the visual form and the visual consciousness seeing it come from or share the same natal source – a karmic tendency. They arise simultaneously from it as parts of a single cognition, without coming from different natal sources. It is not that first the sight exists and then the consciousness sees it.\nWithin the True Aspectarian presentation of Chittamatra, this assertion of a single visual form composed of variegated colored shapes being cognized by a single visual cognition of the entire form is the explanation of the Proponents of Non-Dual Diversity (sna-tshogs gnyis-med-pa) division.\nThe natal source of something is what gives rise to it. For example, a potter’s wheel is the natal source of a clay pot, an oven is the natal source of a loaf of bread, and a womb is the natal source of a baby. A natal source may give rise to two inseparable things, such as a clay pot and the belly of the clay pot (a whole and its parts). Alternatively, it may give rise to two separable things, such as two clay pots.\nSome natal sources cease to exist after they give rise to something, such as a seed as the natal source of a sprout. Some continue to exist, such as a potter’s wheel after it produces a clay pot.\nAlthough the karmic tendency is the natal source of both the visual form of the clay pot and the visual consciousness that validly cognizes it, the karmic tendency is the obtaining cause (nyer-len-gyi rgyu, material cause) of only the visual consciousness. It is not the obtaining cause of the visual form. The obtaining cause of something is that from which one obtains the item as its successor and thus it ceases to exist when its successor arises. In other words, the karmic tendency for a cognition turns into the visual consciousness as its successor, not into the visual form.\nThe karmic tendency that is the obtaining cause for the visual consciousness is only the simultaneously acting condition (lhan-cig byed-rkyen) for the visual form. It is necessary for the karmic tendency to exist beforehand in order for the visual form to arise in the cognition, but the karmic tendency for the cognition does not transform into the visual form.\nIn the Buddhist tenet systems that assert external phenomena, a seed is the obtaining cause for a sprout and the unfired clay is the obtaining cause for the fired clay of a clay pot. In the Chittamatra system, however, forms of physical phenomena do not have obtaining causes, because obtaining causes also would have to exist before their results and cease to exist when they give rise to their results as their successors. If the obtaining cause for the clay pot existed before the clay pot, it would have to come from a different natal source than the clay pot and would have to exist as an external object. This is not possible.\nThus, although it appears as though the fired clay of the clay pot validly cognized now came from the unfired clay validly cognized some time ago as its obtaining cause, this appearance is like an illusion. The cognized fired clay did not come from the cognized unfired clay. The cognized fired clay and the cognized unfired clay only exist within the contexts of the valid cognitions of them.\nNevertheless, from the tendencies of collective karma, the fired clay appears to have come from the unfired clay as its successor. It does not appear as though it came from an apple seed. Any cognition to which it appears as though the cognized fired clay came from an apple seed as its obtaining cause is not only deceptive (‘khrul-shes) with respect to the fired clay having an obtaining cause, it is also distorted (log-shes) with respect to what an obtaining cause would appear to be.\nThe karmic tendency for the cognition is also not the similar-family cause (rigs-‘dra’i rgyu) of the visual form of the clay pot, but only of the visual consciousness of the visual form of it. Similar-family causes are in the same category of phenomena as their results and serve as models for them. Sights, as forms of physical phenomena, do not leave karmic tendencies; only ways of being aware of something do that. Therefore, a visual form (a sight) cannot be the similar-family cause of a karmic tendency; and a karmic tendency cannot be the similar-family cause of a subsequent visual form.\nIn the Buddhist tenet systems that assert external phenomena, only a model of the visual form of a clay pot can be the similar-family cause of the visual form of a clay pot. However, a model of the visual form of a clay pot would have to exist prior to the visual form of the clay pot in order to serve as its model, which means that it would have to exist externally. Since Chittamatra asserts that this is an impossible way of existing, then according to its system, forms of physical phenomena do not have similar-family causes.\nThe karmic tendency for the cognition of the visual form of the clay pot, then, is only the natal source of the visual form. It is not even the immediately preceding condition (de-ma-thag rkyen) of the visual form. This is because only ways of being aware of something have an immediately preceding condition. In fact, the karmic tendency is not even the immediately preceding condition for the visual consciousness that cognizes the visual form. The immediately preceding condition for a moment of awareness of something must be the immediately preceding moment of awareness.\nJoin us in trying to benefit others.\nSupport our work!\nThis website relies completely on donations. Its maintenance, preparation of the remaining 70% of our planned material, and further translating is costly. Although we currently have 80 volunteers, 23 essential team members require payment. Help us raise the 100,000 euros (US $150,000) required each year\nto continue providing our website free of charge.\nReaching Our Goal (35%)"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:5d1d07d8-35f0-47e4-9c53-8891737721a2>"],"error":null}
{"question":"What are the key differences between static control methods in ESD protection versus heat management approaches in halogen lighting systems?","answer":"ESD protection uses four main control methods: prevention (using materials that cannot hold a charge), neutralization (connecting to grounded outlets), dissipation (using static dissipative foam), and shielding (protecting equipment from nearby charges). In contrast, halogen lighting systems manage heat differently depending on the type - mains-voltage GU10 bulbs project infrared heat forward in the beam, while some MR16 low-voltage bulbs use dichroic filters to dissipate heat through the back of the bulb. For heat-sensitive applications, special dichroic spotlights are available to display items like food, artworks, or textiles without damage.","context":["ESD Training is important as understanding the techniques behind static control in a work environment is a crucial part of many businesses, particularly ones that have access to high voltage and electronics. Electrostatic discharges cause billions of dollars in damages each year. By going through basic ESD training and educating staff members, one can significantly decrease these losses. It only takes a small investment in materials and some ESD training to effectively control static conditions and prevent damages.\nBasics of Static Electricity for ESD Training\nMost people know that static electricity gathers when friction occurs between two conductive materials. The actual discharge, often felt as a short shock, occurs when two objects with different charges come into contact with each other. At 3,000 volts, one can feel static. At 5,000 volts, one can hear it. It takes 10,000 volts or more for static discharge to be visible to the human eye. Discharges at that voltage will damage most electronics. Static charges are more likely to form at higher voltages in dry climates with low temperatures. The most important thing to know for ESD training is that one cannot eliminate static, only reduce it. This is why professionals refer to it as static control, not static elimination. Proper anti static techniques will assess the vulnerability of a workplace and take steps to minimize the likelihood of an ESD event.\nThere are two types of damage that occur after an ESD event. Catastrophic damage comprises 10% of all ESD failures. The static discharge is so strong that it damages the device beyond repair. The device does not work and now needs a replacement before work can proceed. Intermittent damage comprises 90% of ESD failures. The ESD event damages the device, but the device is operational. Without basic ESD training, someone could assume that the device is fine and send it out to a customer. The latent damage will show up later and result in more service calls. Intermittent damage is the most expensive type of ESD failure. Both types of damage can happen at any level, including product assembly and field servicing. Employees should have the ESD training to know when to take extra precautions against ESD. Effective ESD training will also teach them how to use anti static tools correctly.\nThe most common static generators are work surfaces, flooring, clothing, packaging materials, and personnel. Special ESD equipment exists to help control the static for each of these items. Companies should invest in anti static shipping bags, ESD flooring for work environments, and anti static mats with connected wrist straps for direct work with electronics. Those are the minimum tools required for anti static control, but there are other helpful tools available as well. ESD training will help workers decided which tools will benefit each work environment.\nAnti static equipment controls static in one of four ways: prevention, neutralization, dissipation, and shielding. Some materials, such as anti static gloves, prevent charges from forming with material that cannot hold a charge. ESD flooring neutralizes the charge by connecting it to a grounded outlet. Remember, static is a problem when two objects with different charges come into contact with one another. If everything in the system stays at the same charge, the risk decreases. ESD mats slowly dissipate the charge with static dissipative foam. Materials such as anti static bags shield sensitive equipment from any static charges that are nearby. Each of these components works together and controls static with all four methods. This ensures that the environment will remain safe from sudden discharges. Here are four basic rules of static control to review during ESD training: 1) Handle sensitive items at an anti static workstation, making sure to discharge any static electricity that may have built up on hands or clothing. 2) Transport sensitive items in anti static bags 3) Test the static control equipment regularly to ensure that it is functioning correctly. 4) Make sure that members of every part of the supply chain follow the above three rules. Even a small misstep can result in intermittent damage to a component. ESD training can go into much greater depth, but these basics will provide a foundation for preventing damaging ESD events. Call our sales office on 0844 8845 155 for an ESD audit, advice on how to set up an EPA, and a full ESD training presentation.","Halogen lamps, like traditional incandescent lamps, use a tungsten filament, but they’re also filled with a small amount of halogen gas (such as iodine or bromine) in a capsule. Keep reading to learn more.\nHow Do Halogens Work?\nThe combination of tungsten filament and halogen gas causes a chemical reaction known as a halogen cycle to take place. This reaction then causes the evaporated tungsten to form a halide – a compound made of two elements – with the halogen gas. When it reaches a high enough temperature, the halide breaks apart and deposits tungsten back into the filament, thereby extending the life of the lamp. This reaction also keeps the lamp clear, whereas in an ordinary incandescent lamp, the filament is deposited on the inside of the glass bulb, reducing its clarity over time.\nHalogen bulbs come in two varieties:\n- Low voltage (but not low energy) bulbs recessed into ceilings (known as ‘down-lighters‘) and walls. These are usually MR16 bulbs, and they have a hidden transformer that steps down from mains 230 voltage to the 12 volts of the bulbs. They are also called ‘reflectors’ because they give directional light.\n- Bulbs that are put in open mounts, typically GU10 mounts. These are normally found in kitchens, on spotlight ceiling bars, or ceiling plates. Although popular, they are not necessarily the most practical, as they cast shadows and generate a lot of heat. However, it is possible to buy compact-fluorescent (‘low energy’) bulbs to replace these, giving a more energy-efficient and less directional light. For an even more energy efficient solution you can also fit LED replacements. For more information, please see our Ultimate Guide to Buying LED.\nReplacing Incandescent Bulbs with Halogens\nEnergy-saving halogen technology is widely used in retrofit products designed to replace incandescent bulbs, many of which have been phased out by recent EU legislation. Halogen technology produces a marginally cooler, brighter-looking light than the old technology, and is better than incandescent light at rendering short wavelength colours (i.e. violets and blues).\nEnergy-saving halogen bulbs are available as an economical alternative to incandescent GLS bulbs, candles, ‘R’ reflectors, globes, and golf balls because halogen typically has twice the lifespan of an incandescent equivalent.\nTypes of Halogen Bulbs & Their Uses\nHalogen bulbs are available in a variety of different types and styles, each suited for a different use.\nMains-Voltage Halogen Spotlights\nHalogen is one of the primary technologies used in spotlighting, partly due to the fantastic choice of products available. Another reason for its popularity is its superb colour rendering. A halogen bulb includes all colours of the visible spectrum in its light, which means colour is displayed with more consistency than is possible in LED or fluorescent technologies.\nThe GU10 spotlight is by far the most common type of mains-voltage halogen reflector. It is used in accent lighting, task lighting and general lighting. GU10 bulbs use what is known as a ‘twist and lock’ base, introduced by Sylvania in 1996. This base provides a very secure electrical contact, similar in principle to a bayonet cap. One defining factor in a GU10 is that infrared heat is always projected forward in the beam, whereas many MR16 low-voltage bulbs of the same size (50mm diameter) use a dichroic filter to dissipate heat through the back of the bulb.\nThe GZ10 bulb is a relatively rare mains-voltage bulb that dissipates its heat using a dichroic filter through the back of the bulb. The GZ10 is unsuitable for use in most GU10 light fittings – despite having the same pin arrangement, it will not fit because a GU10 bulb has a bevelled base, whilst the GZ10 is square. Light fittings normally carry a ‘no cool beam’ symbol to signify incompatibility with the GZ10.\nLow Voltage Halogen Spotlights\nHalogen AR111 spotlights are often used in shops or showrooms, and normally have a metal block over their filament to prevent glare. The massively popular MR16 is universally loved, partly for its wide variety of designs and beam angles.\nLow-voltage spotlights are available with an aluminium-coated reflector to project infrared heat forwards, or with a dichroic filter that allows infrared heat to pass through the back of the lamp. Because of this, a dichroic spotlight is useful for displaying heat-sensitive objects like food, artworks, or textiles.\nOne advantage of low-voltage spotlights is their sharper, brighter quality of light over their mains-voltage counterparts. In addition to display lighting, low voltage halogen spotlights are common in downlights and wire lighting systems.\nLow voltage halogen spotlights use the following caps:\n- AR111 = G53\n- MR8 and MR11 = GU4\n- MR16 = GU5.3\nUnlike mains-voltage halogens, all low voltage halogen lamps require a 12V transformer, which is used to step down mains-voltage to individual or multiple low voltage bulbs, depending on maximum load.\nLinear halogen lamps with R7S bases are found in floodlighting and security lights. The filament of the lamp is supported along its complete length, and the round bases at either end fit into sprung contacts. The filament must remain horizontal to maintain the bulb’s lifespan, hence their name.\nSpecialist halogen R7S heat lamps include jacketed and non-jacketed versions. Jacketed lamps tend to last longer because they’re protected from contaminants such as food. Heat lamps are available in clear, ruby, or gold finishes; a clear finish is used where the bright white light of halogen is required in addition to heat, while ruby or gold finishes subdue light output for applications where heat is predominantly needed.\nHalogen capsules are used in applications such as caravan lighting, boat lighting, pelmet lighting, or desk lighting. They come in low-voltage G4 or GY6.35 types, or a mains-voltage G9 format. The suffixed numbers in their names are based on the distance in millimetres between their two pin centres.\nThe G9 is the most compact mains-voltage halogen bulb available, and because it requires no 12V transformer it’s widely used in extremely small light fittings.\nHigh-powered specialist halogen capsules are also available for applications such as theatre, TV, film, or projector lighting.\nUses at a Glance\n|Mains-Voltage Spotlight||Accent lighting, task lighting, general lighting|\n|Low Voltage Spotlight – No Dichroic Filter||Display lighting in shops or showrooms, downlights, wire lighting systems|\n|Lower Voltage Spotlight – With Dichroic Filter||Displaying heat-sensitive objects (such as food)|\n|Linear Halogens||Floodlighting and security lights|\n|Capsules||Caravan lighting, boat lighting, pelmet lighting, desk lighting, small light fittings; theatre, TV, film, or projector lighting|"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:3f00ea9d-556f-4284-b9c5-dcc9b594171a>","<urn:uuid:ab0ac1b6-f559-44e2-b421-58fe543086d6>"],"error":null}
{"question":"Given my background in global imaging systems, how do the resolution capabilities of Chang'e 2 and LROC's lunar mapping compare to the current ocean mapping technologies?","answer":"Chang'e 2 and LROC provide significantly higher resolution mapping than current ocean mapping systems. The Chinese Chang'e 2 mission achieved approximately 6m/pixel resolution globally, while LROC offers 100m resolution with its Wide Angle Camera and 50cm resolution with its Narrow Angle Camera for specific areas. In contrast, ocean mapping still has significant gaps, with bathymetric data showing errors of up to plus/minus 2km in some places, as evidenced during the MH370 search. In some areas, depth data collected differed from existing data by more than 100m, highlighting the vast difference in mapping precision between lunar and ocean mapping capabilities.","context":["This Oceans Week, lets ponder: Is there a way to reverse ocean health decline and continue to rely on it ? The coming Decade is an opportunity to seek a positive answer to this question.\nby Hari Vishnu\nOceans cover 71% of Earth’s surface. They were the cradle of life millions of years ago, and have been a key element of development of all life on Earth. Understanding the Oceans holds the key to solving many problems facing mankind such as food shortage, social development, poverty, climate change and energy security [1,2].\nAnd yet, as Gene Feldman of NASA stated, we know less about the depths of our Oceans than we know about the surface of the Moon or Mars  ! It's like stating that we know more about our neighbour’s house than we know our own backyard.\nToday, Earth's Oceans have a sad story to say, bearing the brunt of mankind's unsustainable activities, which manifests as indicators like acidification, increasing temperatures, decrease in oxygen, coral reef bleaching , accumulation of plastic waste [5,6] and general biodiversity decline .\nWhile we celebrate World Ocean Day this year, we have a lot to think about. Isn't it high-time we stopped treating Oceans as a dumpyard, thought about how to value it and use it sustainably ?\nIntroduction to 'The Decade'\nThe United Nations has proclaimed a Decade of Ocean Science for Sustainable Development (2021-2030) (also referred to as Ocean Decade). This is to gather stakeholders worldwide behind a common framework to ensure science can support countries for sustainable development of the Ocean .\n- Introduction and aims of the Decade\n- Panel at OCEANS Seattle\n- Role of societies in the Decade\n- Campaign Oceanography\n- Questions and discussion\n- Moving forward\nThe focus is on using science to answer the existentially important question: Is there a way to reverse the decline in Ocean health and to continue to rely on the Ocean for our needs, particularly under the changing climate ? The proclamation by the United Nations General Assembly is a clear message that 195 nations consider Ocean science a priority for our civilization at the beginning of the third millennium .\nThe Vision of the Decade is: The science we need for the ocean we want.\nThe mission: Generate and use knowledge for the transformation needed to achieve a healthy, safe, and resilient Ocean for sustainable development by 2030 and beyond. We aim to move towards \"an Ocean we want\":\n- A clean Ocean where sources of pollution are identified and removed\n- A healthy and resilient Ocean where marine ecosystems are mapped and protected.\n- A predicted Ocean where society has the capacity to understand present and future Ocean conditions\n- A safe Ocean where people are protected from Ocean-related hazards\n- A productive and sustainably harvested Ocean ensuring the provision of food supply and stable livelihoods\n- A transparent and accessible Ocean with open access to data, information and technologies\nThe Intergovernmental Oceanographic Commission (IOC) of UNESCO is coordinating the preparatory process, inviting the global Ocean community to plan for the next 10 years in Ocean science and technology. Earthzine is supporting Decade efforts to reach out to people and spread the word on Oceans. In preparation for the Decade, several outreach activities such as town halls and panel discussions have been organized at conferences across the world. Two of these were: the IEEE OES/MTS OCEANS 2019 at Seattle (Oct 2019), and the Ocean Sciences meeting in San Diego (Feb 2020), which Earthzine covered.\nThis article is Part One of a 2-article series on panel discussions on the Ocean Decade over the last year. In this article, we cover the aims, perspectives and plans of the Decade discussed at an OCEANS Seattle panel, and the involvement of technical societies and the private-sector. Part Two will discuss the involvement of students and Early Career professionals or Young Professionals (ECOPs/YPs) and how they can contribute to and become a part of the Decade. It will also discuss science communication and outreach for the Decade. The article will span coverage of panels held at Seattle as well as Ocean Sciences, San Diego.\nThe panel at OCEANS Seattle\nThe moderator of the panel session held at OCEANS, Seattle, was Craig McLean, acting Chief Scientist at NOAA. He introduced the panelists representing a wide spectrum of people from academia (professors and students), industry, government agencies, science outreach agencies and technical societies:\n- Richard Spinrad, Professor of Oceanography at Oregon State university and Ex-chief scientist at NOAA and President of Marine Technological society (MTS)\n- David Millar, Fugro\n- Mattie Rodrigue, OceanX\n- HyeJoo Ru, Undergraduate research student at University of Washington.\nRole of technical societies in the Decade\nRick Spinrad was the first speaker. He elaborated the value of technical societies (such as MTS and Oceanic Engineering Society OES) in furthering the objectives of the Decade.\nTechnical societies are uniquely poised and can achieve some things that even government agencies can't. They can reach out to a large section of people and give detailed avenues for more people to get involved in these activities. They also have the advantage of being seen as and operating in a non-partisan and non-political way, so they can be considered as objective sources of information. They can also give quick and ready access to expert advice on the way forward for the Decade, and can involve diverse representation of people and technology.\nRick pointed out that there would be 20 OCEANS conferences being held this coming decade. What can MTS and OES do to promote the Ocean Decade during these conferences ? For starters, societies can work with the conference organizing committees to spread awareness. In this regard, the OES and MTS presidents have decided to make available 3 slots to the registrants at each conference for IOC representation, at no cost, allowing them to take the message further.\nCampaign oceanography - a step forward for the Decade\nCraig introduced the next panelist - David Millar, representing Fugro. He spoke on campaign oceanography, of which one example is Seabed 2030 . This is a global initiative between Japan’s Nippon Foundation and the General Bathymetric Chart of the Oceans (GEBCO) to produce a definitive, high-resolution bathymetric map of the entire world's Ocean floor by 2030. Currently, less than 20% of the Oceans are mapped to modern survey standards. This is one gap that needs to be fixed by bringing stakeholders to a common table. In this regard, Seabed 2030 is a promising way forward. Fugro recently contributed more than 110,000 sq. km. of high-resolution bathymetry data in the North Atlantic Ocean to help improve the quality and coverage of seabed mapping in the region .\nDavid gave perspectives on how the private sector could participate in the Decade.\nBefore the Seabed 2030 Project, actual bathymetric data were available for only 6.2% of the global ocean, and the rest was obtained from models derived from satellite-altimetry and sonar soundings . However, the recently released GEBCO 2019 data has almost 15% based on actual bathymetric data, which marks a large step forward .\nThe public can discover bathymetric data from the IHO DCDB Digital Bathymetry viewer.\nDavid stated the example of the challenges faced during the disappearance of Malaysia Airlines flight MH370, and its search. The difference between the GEBCO map and the one taken at the site was large ! In some places, the error was plus/minus 2 km, and 38% of the depth data collected in the area differed from our existing data by more than 100 m ! At the time of the search for the fuselage, sonar data known in the area was insufficient to deploy deep-water instruments to inspect the seafloor, so ship-based bathymetric data had to be collected eventually .\nQuestions and discussion\nThere was an active interaction with the audience during the Question-Answer session. We cover some of the important questions and discussions here.\nQ: Is there a way that this can go towards building a global seasonal forecasting system , that can help forecast better like how many cooling days, heating days, etc we need to plan for ?\nReply: Indeed, it can. This is the sort of reach we are aiming for. If you have a 7-day weather forecast, you should thank an oceanographer because oceanography and ocean-interactions are a key input to seasonal forecasting. We hope the Decade's activities will take the knowledge further.\nQ: Do we have an idea of how much of the ocean baseline we have a grasp on, to understand where we are one decade down the line ?\nReply: (Craig) There is some good data on baselines, but it is not complete. For example, we have done a good job so far of tracking the table-food fish ecosystem. But there are many more areas that need good baselines. The new blue economy will look way different from what we have been doing, it is going to be a very different construct, so a baseline is important indeed.\nQ: Where will the money come from, for the initiatives under the Decade?\nReply (Panel): The answer is still being formed. Some examples are philanthropic organizations. The Decade gives us an opportunity to leverage across organizations around the world and spur more investment in this field using the message of the Decade.\nQ: What is the implementation plan of the Decade ? Is there an action plan ?\nReply: We are starting off the preparation by trying to socialize the Decade, and get the community involved in understanding what is required and giving inputs on what is required.\n(Update on this): A draft implementation plan for the Decade has been floated, and two webinars were held on and to discuss the draft. Earthzine attended this webinar and will soon report an article on this. Meanwhile, you can watch the webinar or take a look at the slides of the meeting.\nThe Decade will be kicked off by a Decadal celebration in 2021. The Decade has also been bringing in key players to realize its aims - one such event was an MoU signed with the International Science Council . The renewed partnership stands to give the Ocean Decade important leverage and visibility within the scientific and technological community\nSince November 2019, the ISC and IOC have been co-producing a series of blogs featuring new voices we need to hear from across human, natural, social and traditional sciences, if the Ocean Decade is to be truly inclusive and multidisciplinary. The series can be followed via this link.\nWatch out for more upcoming Earthzine coverage on the Ocean Decade.\n United Nations Decade of Ocean Science for Sustainable Development (2021-2030), accessed Mar 20, 2020\n Zero Draft of Implementation plan, United Nations Decade of Ocean Science for Sustainable Development (2021-2030), 18 Mar 2020.\n “Oceans: The Great Unknown”, NASA, https://www.nasa.gov/audience/forstudents/5-8/features/oceans-the-great-unknown-58.html , Last accessed May 2020\n Chapter 5: Changing Ocean, Marine Ecosystems, and Dependent Communities - Special Report on the Ocean and Cryosphere in a Changing Climate, Bindoff, N.L., W.W.L. Cheung, J.G. Kairo, J. Arístegui, V.A. Guinder, R. Hallberg, N. Hilmi, N. Jiao, M.S. Karim, L. Levin, S. O’Donoghue, S.R. Purca Cuicapusa, B. Rinkevich, T. Suga, A. Tagliabue, and P. Williamson, 2019. In: IPCC Special Report on the Ocean and Cryosphere in a Changing Climate [H.-O. Pörtner, D.C. Roberts, V. Masson-Delmotte, P. Zhai, M. Tignor, E. Poloczanska, K. Mintenbeck, A. Alegría, M. Nicolai, A. Okem, J. Petzold, B. Rama, N.M. Weyer (eds.)].\n Eriksen M, Lebreton LCM, Carson HS, Thiel M, Moore CJ, Borerro JC, et al. (2014) Plastic Pollution in the World’s Oceans: More than 5 Trillion Plastic Pieces Weighing over 250,000 Tons Afloat at Sea. PLoS ONE 9(12): e111913. https://doi.org/10.1371/journal.pone.0111913\n \"The great Pacific Garbage patch\", The Ocean cleanup, https://theoceancleanup.com/great-pacific-garbage-patch/\n Seabed 2030 - The Nippon Foundation and GEBCO\n Fugro supports two ocean mapping initiatives with large crowd sourced bathymetry contribution, , accessed: Mar 20, 2020\n Seafloor Mapping – The Challenge of a Truly Global Ocean Bathymetry, Frontiers in Marine Science, Vol. 6, 2019, DOI: 10.3389/fmars.2019.00283, ISSN: 2296-7745, Authors: Wölfl Anne-Cathrin, Snaith Helen, Amirebrahimi Sam, Devey Colin W., Dorschel Boris, Ferrini Vicki, Huvenne Veerle A. I., Jakobsson Martin, Jencks Jennifer, Johnston Gordon, Lamarche Geoffroy, Mayer Larry, Millar David, Pedersen Terje Haga, Picard Kim, Reitz Anja, Schmitt Thierry, Visbeck Martin, Weatherall Pauline, Wigley Rochelle,\n UNESCO, New agreement mobilizes global science for the Ocean Decade , 12 Feb 2020, Accessed March 20, 2020.","Comparing Chang'e 2 and Lunar Reconnaissance Orbiter maps of the Moon\nPosted by Phil Stooke\n13-02-2012 10:23 CST\nRecently, the Chinese Space agency released a global map of the Moon with a zoomable interface that would be familiar to anyone who uses popular terrestrial online maps like Google Maps, Yahoo Maps or Bing Maps. You click on the map or use a sliding control to zoom in, drag on the map to move around within it - it has become a very familiar and intuitive interface. In fact it's so useful that it is now being used for several different planetary datasets. There's one for Mercury, one for Mars, and a great one for the Moon. That last one, called Quickmap, consists of images from the Lunar Reconnaissance Orbiter Camera (LROC), including a global map made from LROC Wide Angle Camera images with a resolution of about 100 meters and, if you zoom in enough, closeups made with the LROC Narrow Angle Camera with a resolution of about 50 centimeters wherever they are available.\nThe inevitable question is: how does the LRO lunar map compare with the new Chinese product?\nThe Chinese map is made with images from Chang'e 2, the orbiter launched in October 2010 and currently stationed at the Sun-Earth L2 Lagrange point. It spent eight months mapping the Moon, collecting images for this global map and higher resolution views of potential landing areas for the first Chinese lunar lander, especially in Sinus Iridum. The highest resolution images are not incorporated into this 50-meter-resolution map yet, but perhaps that will happen later. Reports have stated that the global mosaic has a resolution of about 6 m/pixel. If that is correct, we do not yet have access to the full resolution dat\nThe best way to compare maps is to look at the same area in both data sets, so I made a few comparison images. The first one is an image of a chain of elongated pits in Mare Tranquillitatis just east of Sosigenes crater at 8.4 north, 19.0 east.\nI'm interested in this area because LROC narrow-angle images reveal odd features on its floor, strange sharp-edged hollows with mottled floors which resemble the famous Ina (or D-Caldera) depression discovered in Apollo images. These pits are smaller but similar, and I have to wonder if they are also similar to the hollows recently revealed on Mercury. The LROC image shows the pit chain partly covered by a narrow angle image which shows the hollows very clearly. The rest of the big pit and a smaller pit are only seen in the main mosaic from the Wide Angle Camera, and there wouldn't be much evidence of unusual markings in the pit if that's all we had.\nThe Chinese map looks different. It lacks the detail of the narrow-angle LROC images, but it shows more than the wide-angle LROC. Compare the two carefully - the Sun is at a higher angle in the Chinese image, so shadows are missing and topography is harder to interpret, and the range of grey shades is more limited, suggesting the original images had a lower bit depth. But resolution is clearly higher, about twice as high as in the NASA map. There's a real hint of something odd about the floor of the depression which LROC's wide angle images do not pick up.\nThe second comparison is a place I originally looked at because I thought it might be another of those odd hollows. It is in northern Mare Imbrium very close to the highland boundary at 48.1 north, 23.4 west, and it shows as a little white smudge in LROC wide angle images.\nNarrow angle images just graze the western edge of it but don't reveal its nature. The new Chinese map shows quite a bit more detail, again maybe half the pixel size or a bit better. The smudge looks to me like a very shallow hill, a bit of the adjacent highlands almost submerged beneath the dark lava of Mare Imbrium. Probably not a hollow this time! Too bad. Incidentally, the left side of the LROC mosaic shows one problem with the current version of Quickmap. The narrow angle image strips don't match very well. Some craters show up twice, others must be missed altogether, as the images don't overlap properly. If you want to see the original images, or all the overlapping images, just select the search tool, the magnifying glass at upper right. Click where you want on the map, or drag out a box, and all images at that location will appear in a list, with clickable links to fully zoomable versions of those images in a new window. Click back on the map navigation icon to escape that search function. You can do the same on the Mercury Quickmap, but you only get a link to a raw data download which needs a bit of extra skill to use.\nThe third comparison is a pair of images in Sinus Iridum at 43.0 N, 31.0 W.\nA crater about 3 kilometers across dominates the scene, but look first at the smaller craters on the plains outside it. They are clearer in the Chinese map than the NASA map, showing a resolution improvement by about a factor of two. But the big crater looks different as well, because of a different lighting angle. The higher sun in the Chang'e 2 images allows the entire crater interior to be seen, whereas part of it is hidden by shadow in the LROC mosaic. This comparison is a bit misleading, because LROC WAC images are available at multiple lighting angles. It would be possible to make a WAC global mosaic with high sun images, or morning or evening images, in addition to the current version, which is a combination of images at many sun angles. But for what we have now, there are differences in lighting between the two maps. The Chinese mosaic will be less obscured by shadows near the poles than the NASA mosaic.\nWhat should we take from these comparisons? Both online lunar maps are good in their own ways, and anyone interested in this kind of virtual exploration of the lunar surface would probably want to use both of them. There are also some examples of higher-resolution Chang'e 2 images here and here. At the second link, the thumbnails don't link to larger images as you might expect, but if you right-click on them and save the images they save at a much larger size. It would be really nice to see some of the higher-resolution coverage incorporated into the new Chinese map eventually, but even without that it's more than just a duplicate of the LROC map. Go explore the Moon and see what you can find."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:48ba7200-d35c-41a8-bcd5-d24abb85b537>","<urn:uuid:16f3cb68-8dc3-4c39-b5c3-681c65d00767>"],"error":null}
{"question":"How do scientists track the origin of Earth's water using isotopes, and what have recent studies revealed? ⚗️","answer":"Scientists track Earth's water origin by analyzing the ratio of two hydrogen isotopes: ordinary hydrogen (with just a proton) and deuterium (with a proton and neutron). The current ocean water has a deuterium-to-hydrogen ratio matching asteroids. However, studies of ancient Canadian rocks revealed they had more ordinary hydrogen and less deuterium, resembling water from the early solar nebula rather than asteroids. Recent analysis of enstatite chondrites, using conventional mass spectrometry and secondary ion mass spectrometry, showed these meteorites have hydrogen and nitrogen isotopes similar to Earth's. This is particularly significant because enstatite chondrites are made of the same materials that formed Earth, suggesting they could have supplied enough water to explain Earth's oceans, despite previously being considered 'dry' due to their formation close to the sun.","context":["Look at Earth compared to other rocky planets in the neighborhood, and the first thing that would likely jump out is that there's A LOT of water. So how did 70 percent of our planet's surface become covered in this essential life ingredient?\nThat question is the subject of lively scientific debate, it turns out.\nThere are two prevailing theories: One is that the Earth held onto some water when it formed, as there would have been ice in the nebula of gas and dust (called the proto-solar nebula) that eventually formed the sun and the planets about 4.5 billion years ago. Some of that water has remained with the Earth, and might be recycled through the planet's mantle layer, according to one theory.\nThe second theory holds that the Earth, Venus, Mars and Mercury would have been close enough to that proto-solar nebula that most of their water would have been vaporized by heat; these planets would have formed with little water in their rocks. In Earth's case, even more water would have been vaporized when the collision that formed the moon happened. In this scenario, instead of being home-grown, the oceans would have been delivered by ice-rich asteroids, called carbonaceous chondrites.\nFollow the water\nScientists can track the origin of Earth's water by looking at the ratio of two isotopes of hydrogen, or versions of hydrogen with a different number of neutrons, that occur in nature. One is ordinary hydrogen, which has just a proton in the nucleus, and the other is deuterium, also known as \"heavy\" hydrogen, which has a proton and a neutron.\nThe ratio of deuterium to hydrogen in Earth's oceans seems to closely match that of asteroids, which are often rich in water and other elements such as carbon and nitrogen, rather than comets. (Whereas asteroids are small rocky bodies that orbit the sun, comets are icy bodies sometimes called dirty snowballs that release gas and dust and are thought to be leftovers from the solar system's formation.)\nScientists have also discovered opals in meteorites that originated among asteroids (they are likely pieces knocked off of asteroids). Since opals need water to form, this finding was another indication of water coming from space rocks. These two pieces of evidence would favor an asteroid origin. In addition, deuterium tends to gather farther out in the solar system than hydrogen does, so water formed in the outer regions of the system would tend to be deuterium-rich.\nAnd on top of that, the rocky inner planets hold relatively little water (relative to their masses) compared with the icy moons of Jupiter, Saturn, Uranus and Neptune, and even the gas giants themselves. That would support the idea that in the inner system, the water evaporated, while in the outer system, it didn't. If water evaporated on Earth it would have to be replaced from somewhere else, and water-rich asteroids are abundant in the outer reaches of the system.\nMore supporting evidence comes from NASA's DAWN spacecraft, launched in 2007, which found evidence of water on Ceres and Vesta, the two largest objects in the main asteroid belt located between Mars and Jupiter.\nEarth's water is complicated\nA slam dunk for asteroids? Not so fast. For this scenario to work, the isotope ratio had to have stayed the same in the oceans over the last few billion years.\nBut what if it didn't?\nLydia Hallis, a planetary scientist with the University of Glasgow in the United Kingdom, thinks that the hydrogen present on the early Earth had much less deuterium in it than it does now. The ratio changed because in the early history of the Earth the radiation from the sun heated up both hydrogen and deuterium. Hydrogen, being lighter, was more likely to fly off into outer space, leaving more deuterium behind.\nAlso, in the last several years, newer models seem to show that the Earth retained a lot of water as it formed, and that the oceans might have been present for much longer than anyone thought.\nHallis and her colleagues looked at hydrogen isotope ratios in ancient Canadian rocks, some of the oldest rocks on Earth. The isotope ratios looked a lot less like asteroids and a lot more like the water one would expect from the early solar nebula in the region — the rocks had more ordinary hydrogen and less deuterium. But the current ocean ratio looks like asteroids. That would seem to indicate something changed in the last few billion years. The research was published in Science in 2015.\nIf the Earth's oceans were formed from water on our own planet, rather than asteroids, that would solve a couple of problems for planetary scientists. One is why Earth seems to have so much water in the first place. Another is why life, which as far as anyone knows requires water, seems to have appeared so quickly once the Earth had a solid surface.\nBesides the work of Hallis, other scientists have studied ways water could be recycled from Earth's interior. In 2014, Wendy Panero, an associate professor of earth sciences at Ohio State, and doctoral student Jeff Pigott proposed the theory that Earth was formed with entire oceans of water in its interior. Via plate tectonics, that water has been supplying the oceans. They studied garnet, and found it could work with another mineral, called ringwoodite, to deliver water to the Earth's interior – water that would later come up as the mantle material circulated.\nComplicating the picture, neither of these hypotheses is mutually exclusive. Asteroids could deliver water while some could come from the Earth's interior. The question is how much each would deliver — and how to find that out.\nSo this mystery will remain one, at least for a little while longer.","A new study finds that Earth’s water may have come from materials that were present in the inner solar system at the time the planet formed — instead of far-reaching comets or asteroids delivering such water. The findings published Aug. 28 in Science suggest that Earth may have always been wet.\nResearchers from the Centre de Recherches Petrographiques et Geochimiques (CRPG, CNRS/Universite de Lorraine) in Nancy, France, including one who is now a postdoctoral fellow at Washington University in St. Louis, determined that a type of meteorite called an enstatite chondrite contains sufficient hydrogen to deliver at least three times the amount of water contained in the Earth’s oceans, and probably much more.\nEnstatite chondrites are entirely composed of material from the inner solar system — essentially the same stuff that made up the Earth originally.\n“Our discovery shows that the Earth’s building blocks might have significantly contributed to the Earth’s water,” said lead author Laurette Piani, a researcher at CPRG. “Hydrogen-bearing material was present in the inner solar system at the time of the rocky planet formation, even though the temperatures were too high for water to condense.”\nThe findings from this study are surprising because the Earth’s building blocks are often presumed to be dry. They come from inner zones of the solar system where temperatures would have been too high for water to condense and come together with other solids during planet formation.\nThe meteorites provide a clue that water didn’t have to come from far away.\n“The most interesting part of the discovery for me is that enstatite chondrites, which were believed to be almost ‘dry,’ contain an unexpectedly high abundance of water,” said Lionel Vacher, a postdoctoral researcher in physics in Arts & Sciences at Washington University in St. Louis.\nVacher prepared some of the enstatite chondrites in this study for water analysis while he was completing his PhD at Universite de Lorraine. At Washington University, Vacher is working on understanding the composition of water in other types of meteorites.\nEnstatite chondrites are rare, making up only about 2 percent of known meteorites in collections.\nBut their isotopic similarity to Earth make them particularly compelling. Enstatite chondrites have similar oxygen, titanium and calcium isotopes as Earth, and this study showed that their hydrogen and nitrogen isotopes are similar to Earth’s, too. In the study of extraterrestrial materials, the abundances of an element’s isotopes are used as a distinctive signature to identify where that element originated.\n“If enstatite chondrites were effectively the building blocks of our planet — as strongly suggested by their similar isotopic compositions — this result implies that these types of chondrites supplied enough water to Earth to explain the origin of Earth’s water, which is amazing!” Vacher said.\nThe paper also proposes that a large amount of the atmospheric nitrogen — the most abundant component of the Earth’s atmosphere — could have come from the enstatite chondrites.\n“Only a few pristine enstatite chondrites exist: ones that were not altered on their asteroid nor on Earth,” Piani said. “In our study we have carefully selected the enstatite chondrite meteorites and applied a special analytical procedure to avoid being biased by the input of terrestrial water.”\nCoupling two analytical techniques — conventional mass spectrometry and secondary ion mass spectrometry (SIMS) — allowed researchers to precisely measure the content and composition of the small amounts of water in the meteorites.\nPrior to this study, “it was commonly assumed that these chondrites formed close to the sun,” Piani said. “Enstatite chondrites were thus commonly considered ‘dry,’ and this frequently reasserted assumption has probably prevented any exhaustive analyses to be done for hydrogen.”"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:2e3ea616-f254-4f3a-a724-05e027c0963e>","<urn:uuid:0e1fb28e-dc69-4276-8935-38d3d605495a>"],"error":null}
{"question":"How many different archaeological sites can you visit during a day hike from Cusco?","answer":"During a single day hike from Cusco, you can visit at least 10 archaeological sites. Starting from Tampu Mach'ay at the top of the valley, you can walk down past Puka Pukara, Wayllarqocha, Chuspiyoq, Laqo (Templo de la Luna), Salapunco, Kusilluchyoq, Sillarumiyok, and both Q'enqo Grande and Q'enqo Chico. Additionally, the impressive Sacsayhuaman fort can be visited on the same route back to Cusco's Old Town.","context":["Full of gratitude from our experiences in Bolivia, it was time to head northwest to Cusco, Perú, on the final bicycle leg of our trip. It served up the best of everything bike-touring for us: lots of interaction with the locals, beautiful, mostly quiet roads, challenging ripio and mountain climbs (as well as some pushes), all through spectacular landscapes. We couldn’t have asked for a better cycle touring finish!\nThe ride from La Paz began by fixing a flat tire at the guesthouse, then rolling each loaded bike onto an aerial tram car and ascending 500 m (1,600 ft) to El Alto, then 7 km laterally over El Alto’s rooftops! From there, we climbed onto our bikes and headed out along the altiplano, treated to a spectacular view of the Cordillera peaks we’d just hiked among.\nWe chose the rural route along the northern edge of Lake Titicaca (link to a view of the lake from space), enjoying signs of country life, including a man-made reed island and hut. We stayed in and passed through a number of smaller Bolivian towns, reminding us why our bikes are our favorite mode of travel transport, even with occasional flat tires.\nWe spent our last night in Bolivia in Escoma, in one of the grimmest guesthouses of the trip, where we shared a bucket-flush toilet down the stairs with 4 burly truckers. We met a solo French bike tourer heading the opposite way, so over dinner we compared notes on the roads we’d each be traversing in the coming days. Being a regional border hub, Escoma is known for its large market, which was colorfully in swing as we left town.\nWe pedaled with almost no automobile traffic through small Bolivian villages along the lakeshore, and had some friendly kids run us down to take selfies with us as we labored up a hill.\nIn Puerto Acosta we found Bolivia’s border station closed, without explanation, so at the suggestion of locals, we set off into the village to look for the officer at his wife’s store. The store was closed, so we checked out the local church, and then returned to the station to find the jovial officer now “in”. He has a portion of one wall devoted to photos/stickers/cards from passing bicycle tourists, to which we added our own.\nLeaving with our Bolivian exit stamps, the pavement ended and a challenging no-man’s-land ripio road took us up a mountain to panoramic views of Lake Titicaca. Pavement unexpectedly appeared when we crossed the invisible international border, and we coasted easily down into Tilali, Peru’s border town.\nWe found the Peruvian immigration office wide open to the town square, but it, too, was unoccupied!?! When the immigration officer returned, he couldn’t process our entry into Perú due to a power outage, so he suggested we stay at the guesthouse next door, and said that the power would surely be on the following morning. The next morning we still had no power, but finally a can-do officer arrived and pulled out some old paper forms and stamped our passports.\nBack in Perú\nFour days after leaving La Paz, we had an incredibly scenic ride along the remainder of Lake Titicaca’s north shore, finishing at the town of Huancané where, even though it’s in Peru, a majority of the town’s 7,000 people still speak Aymara. Before going to bed, we tucked into a vintage “chicken-thirty” dinner.\nThe next day we took a classic “shortcut” to the town of Azángaro on an extremely rural dirt road where we rolled through tiny altiplano villages with sheep, cows and alpacas; pushed up a steep jeep track alongside ancient stone walls; rolled over a 10-km long plateau pass; then dropped into the next valley, blinded by the setting sun. We were slowed down the entire day by wonderfully curious and welcoming residents, police, and shepherds along the way, all who wanted to take their picture with us.\nThe combination of challenging riding and everyone’s delightful interest made this day the first of our entire bike trip when we ran out of daylight before reaching our destination. We rode by headlights for the last hour down a sandy dirt road, passed occasionally by slow-moving motorcycles with dim headlights, or no lights at all. Even when we got to town, people’s curiosity remained high, so Clark ended up riding a few circles in the street so that the gathering group could figure out how those darn recumbent bikes work!\nEven though we were exhausted, we were compelled to stay up after dinner because the town had just begun their celebrations for their patron saint, the Virgen de Asunción. The cathedral was packed, and it was the staging area for marching bands and dancing cowboys in shiny boots, all heading out on parade.\nThe party resumed early the next morning. At 6:15am we were awoken by the sound of a marching band playing Vivaldi’s Four Seasons! The town’s band had hiked partway up the Stages of the Cross trail on a nearby hill and then pointed their instruments towards the town and delivered a concert for 20-30 minutes. Apparently no one is supposed to miss this important celebration!\nAnother beautiful day of riding, including an unexpected stop at a small-town church, ended in Ayaviri, which also had its own celebration going, this time a parade of giant plastic and paper mache dinosaurs made by the school children. Once again we’re finding that the Andean cultures never miss a chance for a party!\nAs we headed deep into the Peruvian Andes, we encountered a young couple of German cyclists at a natural hot springs resort near the top of a pass, where we all had a nice soak and camped for the night.\nThe heart of the Inca empire\nWe knew we were getting close to Cusco because of the ever-grander Andean mountains and valleys, more and more restaurant advertisements for “cuy” (guinea pig), and the increasing number of historical and archaeological sites.\nThe first of these was Raqch’i, a primary control point on a road system originating in Cusco. It is a vast complex with dozens of round storage buildings, multiple small temples and the grand Temple of Wiracocha. The temple was built to appease the Inca creator-god, Wiracocha, who was believed to have rained fire on the community (via the nearby volcano Quimsa Chata). This was our first views of the amazingly precise Inca stonework, getting us excited for more to come in Cusco!\nNow within 100 km of Cusco, reasons to stop and sightsee along the way become frequent. We overnighted in the town of Checacupe with an Incan rope bridge and a statue sporting a striking Carnaval costume.\nIn a span of just 8 km, we visited three special churches in three separate villages, each with elaborate frescoes and carvings in the Andean Baroque style. They were periodically overrun by flocks of tourist buses visiting on day trips from Cusco. At the second church, during a lull, we got to see a funeral procession complete with marching band on a beautiful mosaic stonework plaza.\nAfter a welcoming greeting by curious locals in Andahualillas, we set off on our last riding day. The valleys were smokey from a runaway field fire. Along the way to Cusco we visited two archaeological sites: Rumicolca, a Wari aqueduct turned by the Inca into a ceremonial gate, and Pikillaqta, a large, pre-incan Wari walled settlement.\nAfter fixing our last flat tire of the trip (!!) we headed up the valley to Cusco, calming our mindset as the traffic increased. Along the way, dilapidated buildings hinted of an earlier time of flourishing industry, before tourism took over. We got onto a cycle-track as we hit one of the city’s main boulevards, and wound our way up through the ancient city to our Airbnb, located on a very narrow street in the Old Town.\nBefore diving into Cusco, we spent a couple days finding, customizing, and filling up boxes with our bikes and all of our gear, getting them ready for the upcoming plane flights.\nHaving visited Cusco and Machu Picchu about 9 years ago, we were surprised to see how much change has come to this UNESCO world heritage site. The old town has really gotten spruced up, with manicured buildings and well-lit streets. The amazing Incan stone work found at the base of so many buildings is now cleaned up and lit dramatically at night, on streets with every type of restaurant, from backpacker budget to international gourmet, and countless stores with everything from high-end mountaineering gear, to fine alpaca clothing, to funky bicycle shops, to artisanal metallurgy, and of course lots and lots of kitschy tourist schwag.\nAs touristy as it now is, we found the dressed-up Cusco to be a really special place, even compared with all of the wonderful cities we have visited over the last 15 months. A dramatic mountain setting, winding, narrow streets full of beautiful historic buildings, informative museums, plazas of all sizes and levels of grandeur, all steeped in history and culture -- we really enjoyed it here!\nEven though we’d seen it before, the Incan stone work blew our minds, again and again. It utilizes massive stones, some as big as small buses, interconnected in impossibly intricate shapes, with no mortar, and no spaces between the stones. We just couldn’t get enough (or take enough pictures) of this unique-in-the-world craftsmanship!\nEven though it’s known as the gateway to Machu Picchu (which we had the pleasure to visit years ago) Cusco’s valley has a vast array of amazing archaeological ruins in its own right, a host of which can be seen in a single day hike. We hiked up to the impressive Sacsayhuaman fort, and spent the rest of the morning walking around its vast grounds, dazed and amazed by the Inca’s ambition.\nA short taxi ride took us up to the water garden at Tampu Mach’ay, at the top of the valley above the city. From there we walked down a loosely designated trail past the sites, all the way back into the city’s old town. We walked through Tampu Mach’ay, Puka Pukara, Wayllarqocha, Chuspiyoq, Laqo (Templo de la Luna), Salapunco, Kusilluchyoq, Sillarumiyok, and Q’enqo (Grande and Chico). What an abundance of riches for a single day hike, mostly downhill, right back to Cusco’s Old Town!\nTransitioning... from bicycles to suitcases\nThe speedometer on Kacia’s bike computer gave up the ghost somewhere during our ride in Patagonia, but the odometer worked to the bitter end, logging every one of the 8,162 kilometers (5,072 miles) that we rode in South America.\nFrom Cusco we flew back down to Argentina to meet friends, beginning the “suitcase” phase of our trip. (Everything you see in the boxes eventually went home with our friends.) We spent time in Northeastern Argentina, Uruguay, Brazil and finished the trip in Colombia, full circle from where we began in June 2018. We’ll be covering those journeys in our next, and final post(s).\nWe’re currently at Kacia’s parents' house in Northern California enjoying their new puppy, Rosie, and will be moving back into our house in Portland in February. To see a map of our whole journey, check our Track My Tour page. You can also review past social media posts on Clark's Facebook and Instagram accounts."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:0adfe985-d128-4828-a00f-b68c2c1be6ae>"],"error":null}
{"question":"What medical requirements exist for Emergency First Response Instructor versus PADI Assistant Instructor certification?","answer":"For EFR Instructor certification, there are no specific medical requirements mentioned beyond having completed the Primary and Secondary Care training within 24 months. For PADI Assistant Instructor certification, candidates must be fit for diving and submit a Medical Statement signed by a physician within the last 12 months, plus have CPR and First Aid Training within the last 24 months.","context":["The Emergency First Response Instructor Course (EFRI Course)\nThe Emergency First Response Instructor Course teaches you skills based on internationally recognized emergency care guidelines and learn how to present course content while encouraging student self-discovery.\nVinnie Jones shows how hard and fast Hands-only CPR to Stayin’ Alive by the Bee Gees can help save the life of someone who has had a cardiac arrest. The British hardman is starring in a British Heart Foundation TV advert urging more people to carry out CPR in a medical emergency.\nCPR and first aid are key skills that are important to everyone, not just scuba divers. As an Emergency First Response Instructor, you teach skills based on internationally recognized emergency care guidelines, and you can offer courses to anyone. The great thing about EFR courses is that they make learning easy by providing a comfortable environment to practice emergency care skills. Your students finish the course feeling confident with their new skills and ready to help someone in need.\nThe only requirements are that you’re at least 18 years of age and have Emergency First Response Primary and Secondary Care(CPR and First Aid) Training should have been completed within the past 24 months. An EFR Instructor rating is required to become a PADI Instructor also as a Divemaster the EFRI rating will make you more valuable for dive centres. You don’t have to wait until you’re ready to go pro. You can complete or refresh this training during your PADI Instructor training.\nYour role as an EFR Instructor is to be a coach that creates a positive learning environment. Along with learning how to structure and organize EFR courses, you practice:\nPresenting course content.\nEncouraging self-discovery in students.\nEvaluating students understanding and skill mastery during hands-on skills practice.\nManaging effective scenario-based learning experiences.\nCourse specific requirements:\n- Meet and greet around 4 – 5 pm (when your Instructor, Guide is back from the day trip)\n- Documents Review and completion.\n- Prepair your gear.\n- Theory, Review and completion\n- You would like to know more about the PADI / EFR First Response Instructor Program?\n- What can I do after join in the First Response Instructor Program?\nBelow are the requirements for all certification diving courses of the respective training agency (PADI, SDI or TDI).\n- House rules, release from liability and assumption of risk as well as medical questionnaire\n- According to Thai law (status 2020) a medical certificate is not required\n- However, if one of the questions is answered with “YES”, a medical atest is necessary before any activity in the water!\n- Swimming without swimming aid: minimum requirement 200m!\n- Floating motionless on the water (dead man): minimum requirement 10 min.\n- Minimum age for participation in scuba diving activities: 10 years (details in the specific course description).\nOur services include\n- Free shuttle service on dive day: Hotel to boat (White Sand Beach to Bang Bao) and back to the hotel! Pick up from the east coast on request.\n- All boat fees (dive boat or dinghy)\n- Complete equipment you need for scuba diving or snorkeling\n- Regulators from APEKS or SCUBAPRO,\n- Buoncy Control Device (BCD) from APEKS, SCUBAPRO or SEAC.\n- Tropical water, usually 30 degrees warm, 3mm wetsuit short.\n- SCUBAPRO Go fins, mask, Tanks (regularly maintained and tested) and weights.\n- Against a surcharge of 300 THB/day for provision of a SUUNTO dive computer\n- Make sure you get our special online shop discount of 5% when using your own diving equipment! The coupon code is: H7PFU7M6!! This only applies in connection with a complete set of equipment without tanks and weights, and for non-discounted services! We reserve the right to reclaim the granted discount.\n- Lunch, soft drinks, coffee, tea and seasonal fruits are served on the boat.\n- English speaking divemasters or instructors (group size max.5 persons)\n- Wifi on the boat (router capacity max. 10 persons)\n- We accept the following payment methods:\n- Cash payment\n- Bank transfer.\n- PayPal, only in the online shop\n- Credit card payments in the dive shop or on the boat, free of charge if the total amount is more than 1500 THB. For amounts up to 1500 THB we charge a 3% handling fee.\n- NATIONAL PARK FEE’S NOT INCLUDED\nSCUBA dive training agencies\nChang Diving Center offers you courses from 3 different training agencies (PADI, SDI, TDI). So we offer you the possibility to choose diving courses according to your personal needs and requirements!\nDo you still need diving equipment?\nWe offer you the possibility to buy them in our Dive Shop, with appropriate advice! When booking a course or a day trip online, we offer a discount of 10% on the regular price of diving equipment (coupon code: 6SVJ7ADH)\nSCUBA diving in Koh Chang\nSCUBA diving in Koh Chang is a great way to enrich your holiday. Discover the rich underwater world of Koh Chang. It’s worth it! Education/Diving all year round\nKeep Contact, like and follow us on Facebook, or google.\nIf you have something positive to say, feel free to give us a review! Also constructive criticism is welcome!\nContact over QR Codes\nAdd our Contact details over QR Code, Scan the code or on Apple just click it to add.\nAdd us in your Contacts\n- Free transfer on your dive day from your hotel to the boat-White Sand Beach to Bang Bao.(East Coast pick up on request)\n- All Equipment which you need for Scuba Diving or Snorkeling\n- Full Set of own Scuba Diving Gear?Discount 200 THB/Day(we provide tanks and weights)\n- Lunch, soft drinks, coffee, tea and local fruits are served on boat\n- English Speaking Divemaster or Instructor (Group Sizes max.5 PAX)\n- Free Wifi on the Boat(max. 10PAX)\n- Credit Card At the Shop or Boat (PayPal only at the online store)Payments above 1500 THB Free of charge\n- NATIONAL PARK FEE'S NOT INCLUDED\nMore informationour FAQs\nour Boat plan\nour Dive Sites\nour refund policy\nKoh Chang Weather","Assistant Instructor Course\nThe PADI Assistant Instructor course is the first portion of the PADI Instructor Development Course (IDC) and when followed by the Open Water Scuba Instructor course (OWSI) and successful performance at the Instructor Examination (IE), leads to certification as a PADI Open Water Scuba Instructor.\nThe Fun Part\nGet mentored while gaining hands-on experience teaching students.\nWhat You Learn\nYou build upon your abilities to organize and supervise scuba diving activities, while concentrating on developing teaching skills. You learn through:\nHOW TO REGISTER:\nA €350.00 deposit is required two weeks prior to the course starting date. Upon receipt, we will send you your course assignments and itinerary sheet.\nIf applicant signs yes to any of the medical history questions, please consult your physician for approval.\nCLICK HERE TO DOWNLOAD THE MEDICAL STATEMENT\n- Knowledge development through self-study, quizzes, lectures and presentations\n- Confined water skill review and assessment, workshops and presentations\n- Open water workshops, rescue assessment and candidate presentations\nThe PADI Assistant Instructor Course consists of these sections:\n- Module 1: Academic Training\n- PADI Discover Scuba Diving and Snorkeling Programs\n- Developing Knowledge Development Presentations\n- Teaching Project AWARE and Peak Performance Buoyancy Specialties\n- Teaching in Confined Water\n- Conducting Open Water Training Presentations\n- Module 2: Independent Study\n- Knowledge Development\n- Module 3: Practical Application\n- Confined Water and Open Water Teaching Presentations\n- Standards Exam\n- Dive Rescue Skills Assessment\nWhat You Can Teach\nIn addition to the responsibilities and duties you already have as a PADI Divemaster, as a PADI Assistant Instructor you can:\n- Teach academic presentations under\nDrug skin have http://karieraplus.pl/dostinex-buy/ without the Color-Treated It http://www.itirafsitesi.org/index.php?viagra-mexico-pharmacy I sulfates based not archangelica cialis soft overnight our was really the http://karieraplus.pl/nizagara-pills/ fragranced daylight. Struggling http://pabx-panasonic.org/why/pfizer-brand-viagra-canada.php spray same will of http://www.liascatering.com/prednisone-with-no-prescription/ cheaper while when a monthly drug canada sozoenterprise.com all the bar hctz without prescription difference easily started I http://www.zabhegyezo.hu/buy-domperidone-cheap/ years watery. Too mircette without a prescription it recommended that. While e how viagra Is week lived http://www.mwoodsassociates.com/finasteride-online-store a you home enhance!\nthe indirect supervision of a PADI Instructor\n- During confined water dives, present initial skills training under the direct\n- supervision of a PADI Instructor\n- Evaluate Open Water Diver surface skills under the indirect supervision of a PADI Instructor\n- Teach and certify PADI Peak Performance Buoyancy Specialty Divers under the direction of a PADI Instructor\n- Teach Project AWARE Specialty courses\n- Teach the AWARE Coral Reef Conservation specialty course\n- Conduct PADI Discover Scuba Diving experiences in a pool or confined water\n- Conduct PADI Seal Team AquaMissions\n- Teach PADI Digital Underwater Photographer specialty courses under the direction of a PADI Instructor after earning the PADI Digital Underwater Photographer Specialty Instructor rating\nAt a glance, compare what you can teach when you continue your professional diver education.\nThe Scuba Gear You Use\nYou use all the basic scuba equipment and some scuba accessories such as a dive slate,\ndive knife, compass, dive watch, etc. It is highly recommended that you own all of your own scuba equipment, as familiarity with personal gear improves general scuba diving skills. You can find most everything at your local dive shop\nThe Learning Materials You Need\nThe PADI IDC crewpak includes all the materials needed to prepare for a PADI Assistant Instructor or Open Water Scuba Instructor course. The 23-item pack includes:\n- Instructor cue cards for PADI’s core courses (OW, AOW, Rescue and Divemaster)\n- IDC Candidate Workbook and related reference materials,\n- Lesson planning slates for confined and open water\n- Quiz and exam booklets for the core courses\n- Specialty outlines for Project AWARE\n- PADI Instructor Manual\nTo purchase this product, contact your local PADI Five Star IDC Dive Shop or Resort.\n- Be a PADI Divemaster or qualifying certification from another certification organization\n- Be at least 18 years old\n- Have 60 logged dives, including night, deep, and navigation dives\n- Have been a certified diver for at least 6 months\n- Have CPR and First Aid Training within the last 24 months\n- Be fit for diving and submit a Medical Statement (PDF) signed by a physician within the last 12 months\n- Want a fun and exciting career!\nIf you are looking for information on where to go pro, please contact your PADI Instructor Development Center or Career Development Center or Resort and sign-up today.\nYour Next Adventure\nTake the full Instructor Development Course to become a PADI Open Water Scuba Instructor so you can teach and certify PADI Open Water Divers."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:6df06eac-9716-4015-9b52-cfdc75084e51>","<urn:uuid:56335b31-51f9-4aba-8678-19231215919b>"],"error":null}
{"question":"As a boxing enthusiast - what was historically considered 'the ring' in fighting contexts?","answer":"The ring was an enclosed space where pugilists (boxers) fought. It was considered an institution and was also associated with prize fighting.","context":["Found 1 items, similar to Diadophis punctatus.\nEnglish → English\nDefinition: Diadophis punctatus\n, n. [AS. hring, hrinc; akin to Fries. hring, D. & G.\nring, OHG. ring, hring, Icel. hringr, DAn. & SW. ring; cf.\nRuss. krug'. Cf. Harangue\nA circle, or a circular line, or anything in the form of a\ncircular line or hoop.\n2. Specifically, a circular ornament of gold or other\nprecious material worn on the finger, or attached to the\near, the nose, or some other part of the person; as, a\nUpon his thumb he had of gold a ring. --Chaucer.\nThe dearest ring in Venice will I give you. --Shak.\n3. A circular area in which races are or run or other sports\nare performed; an arena.\nPlace me, O, place me in the dusty ring,\nWhere youthful charioteers contend for glory. --E.\n4. An inclosed space in which pugilists fight; hence,\nfiguratively, prize fighting. “The road was an\ninstitution, the ring was an institution.”\n5. A circular group of persons.\nAnd hears the Muses in a ring\nAye round about Jove's alter sing. --Milton.\n(a) The plane figure included between the circumferences\nof two concentric circles.\n(b) The solid generated by the revolution of a circle, or\nother figure, about an exterior straight line (as an\naxis) lying in the same plane as the circle or other\n7. (Astron. & Navigation) An instrument, formerly used for\ntaking the sun's altitude, consisting of a brass ring\nsuspended by a swivel, with a hole at one side through\nwhich a solar ray entering indicated the altitude on the\ngraduated inner surface opposite.\n8. (Bot.) An elastic band partly or wholly encircling the\nspore cases of ferns. See Illust. of Sporangium\n9. A clique; an exclusive combination of persons for a\nselfish purpose, as to control the market, distribute\noffices, obtain contracts, etc.\nThe ruling ring at Constantinople. --E. A.\n, armor composed of rings of metal. See Ring mail\n, below, and Chain mail\n, under Chain\n(Zo[\"o]l.), the ring ousel.\n(Zo[\"o]l.), the circular water tube which\nsurrounds the esophagus of echinoderms.\n, or Ringed dotterel\n. (Zo[\"o]l.) See\n, and Illust. of Pressiroster\n, a sharper who pretends to have found a ring\n(dropped by himself), and tries to induce another to buy\nit as valuable, it being worthless.\n. See under Fence\n, the third finger of the left hand, or the next\nthe little finger, on which the ring is placed in\n(Chem.), a graphic formula in the shape of a\nclosed ring, as in the case of benzene, pyridine, etc. See\nIllust. under Benzene\n, a kind of mail made of small steel rings sewed\nupon a garment of leather or of cloth.\n. (Astron.) See Circular micrometer\n. See Saturn\n. (Zo[\"o]l.) See Ousel\n(Zo[\"o]l.), any one of several species of Old\nWorld parrakeets having a red ring around the neck,\nespecially Pal[ae]ornis torquatus\n, common in India, and\n(a) The ringed dotterel.\n(b) Any one of several small American plovers having a\ndark ring around the neck, as the semipalmated plover\n(Zo[\"o]l.), a small harmless American snake\n) having a white ring around the\nneck. The back is ash-colored, or sage green, the belly of\nan orange red.\n. (Naut.) See under Stopper\n(Zo[\"o]l.), the ring ousel.\nThe prize ring\n, the ring in which prize fighters contend;\nprize fighters, collectively.\n(a) The body of sporting men who bet on horse races.\n(b) The prize ring."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:1ad7c0b2-4c0a-4e9c-bbfa-74274e07a6f8>"],"error":null}
{"question":"Could u explain what is Spring Data JPA and how it connects to databases? :)","answer":"Spring Data JPA is part of the Spring Data family that makes it easy to implement JPA based repositories. It provides enhanced support for JPA based data access layers and makes it easier to build Spring-powered applications that use data access technologies. JPA (Java Persistence API) is a Java specification for managing relational data in Java applications. It allows access and persistence of data between Java objects/classes and relational databases, following Object-Relation Mapping (ORM) principles.","context":["- What is Spring Data?\n- What is spring boot starter JPA?\n- What is difference between @component and @bean in spring?\n- What is Spring Data JPA?\n- How do I use RESTful webservice in spring boot?\n- What is REST IN REST API?\n- What is Hateoas spring?\n- What is the purpose of @component in Spring?\n- What is rest repository in spring boot?\n- What is used for exposing spring data repositories over rest using Spring Data rest?\n- What is Spring Data rest Hal browser?\n- What is the difference between @service and @component in Spring?\n- What is use of @autowired in spring?\n- What is @service and @component in Spring?\n- How does Spring Data rest work?\n- What is a spring repository?\n- What is Hateoas rest?\n- What is Spring Data JDBC?\nWhat is Spring Data?\nSpring Data is a high level SpringSource project whose purpose is to unify and ease the access to different kinds of persistence stores, both relational database systems and NoSQL data stores..\nWhat is spring boot starter JPA?\nSpring Boot JPA is a Java specification for managing relational data in Java applications. It allows us to access and persist data between Java object/ class and relational database. JPA follows Object-Relation Mapping (ORM). It is a set of interfaces. … The Java Persistence API.\nWhat is difference between @component and @bean in spring?\n@Component auto detects and configures the beans using classpath scanning whereas @Bean explicitly declares a single bean, rather than letting Spring do it automatically.\nWhat is Spring Data JPA?\nSpring Data JPA, part of the larger Spring Data family, makes it easy to easily implement JPA based repositories. This module deals with enhanced support for JPA based data access layers. It makes it easier to build Spring-powered applications that use data access technologies.\nHow do I use RESTful webservice in spring boot?\nConsuming RESTful Web Service SpringBootStep 1: Create a Maven project.Step 2: Create a resource representation class.Step 3: Create RestTemplate instance.Step 4: Consuming REST API using HTTP GET. … Step 5: Consuming REST API using HTTP POST. … Step 6: Adding Basic Authentication Header to REST Call.Step 7: Externalize Configuration.More items…\nWhat is REST IN REST API?\nREST or RESTful API design (Representational State Transfer) is designed to take advantage of existing protocols. While REST can be used over nearly any protocol, it usually takes advantage of HTTP when used for Web APIs. … REST API Design was defined by Dr. Roy Fielding in his 2000 doctorate dissertation.\nWhat is Hateoas spring?\nHATEOAS is an acronym for Hypermedia As The Engine Of Application State. Even after expanding that for you, it still might not mean a lot. … spring-boot-starter-hateoas contains the spring-boot-starter-web dependency, so you do not need to include that like you probably would when creating a REST API with Spring Boot.\nWhat is the purpose of @component in Spring?\nSpring Component annotation is used to denote a class as Component. It means that Spring framework will autodetect these classes for dependency injection when annotation-based configuration and classpath scanning is used.\nWhat is rest repository in spring boot?\nIn general, Spring Data REST is built on top of the Spring Data project and makes it easy to build hypermedia-driven REST web services that connect to Spring Data repositories – all using HAL as the driving hypermedia type.\nWhat is used for exposing spring data repositories over rest using Spring Data rest?\nSpring Data REST can be used to expose HATEOAS RESTful resources around Spring Data repositories. Without writing a lot of code, we can expose RESTful API around Spring Data Repositories.\nWhat is Spring Data rest Hal browser?\nHAL and the HAL Browser JSON Hypertext Application Language, or HAL, is a simple format that gives a consistent and easy way to hyperlink between resources in our API. … It works by returning data in JSON format which outlines relevant information about the API.\nWhat is the difference between @service and @component in Spring?\nThe difference between them is, @component is used to annotate compound classes, @Repository is a marker for automatic exception translation in the persistence layer, for service layer we need to use @service. You can refer Spring Documentation to know more.\nWhat is use of @autowired in spring?\nAdvertisements. The @Autowired annotation provides more fine-grained control over where and how autowiring should be accomplished. The @Autowired annotation can be used to autowire bean on the setter method just like @Required annotation, constructor, a property or methods with arbitrary names and/or multiple arguments …\nWhat is @service and @component in Spring?\n@Component is a generic stereotype for any Spring-managed component or bean. @Repository is a stereotype for the persistence layer. @Service is a stereotype for the service layer. @Controller is a stereotype for the presentation layer (spring-MVC).\nHow does Spring Data rest work?\nSpring Data REST builds on top of Spring Data repositories, analyzes your application’s domain model and exposes hypermedia-driven HTTP resources for aggregates contained in the model.\nWhat is a spring repository?\n@Repository is a Spring annotation that indicates that the decorated class is a repository. A repository is a mechanism for encapsulating storage, retrieval, and search behavior which emulates a collection of objects.\nWhat is Hateoas rest?\nHypermedia as the Engine of Application State (HATEOAS) is a component of the REST application architecture that distinguishes it from other network application architectures. With HATEOAS, a client interacts with a network application whose application servers provide information dynamically through hypermedia.\nWhat is Spring Data JDBC?\nSpring Data JDBC is a persistence framework that is not as complex as Spring Data JPA. … Nevertheless, it has it’s own ORM and provides most of the features we’re used with Spring Data JPA like mapped entities, repositories, query annotations, and JdbcTemplate."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:dd615998-a262-4ec3-9675-936ce1b1f0e4>"],"error":null}
{"question":"Are a horse's front legs as dangerous as their back legs when it comes to kicking?","answer":"Yes, a horse's front legs are equally as lethal as their back legs. The front legs can deliver strikes with enough power and destructive force to inflict deep tissue wounds and shatter bones, making them just as dangerous as kicks from the rear legs.","context":["Everyone will accept that horses kick, and most assume the back legs of the horse are the most dangerous area of the animal. Yet the front legs are equally as lethal, the power and destructive forces of a strike can inflict deep tissue wounds and shatter bones. A strike, striking and striking out are all terms associated with the horse using the front hooves to kick. There are different forms of striking, each depending on the level of threat, and the amount of force the horse needs to apply. In the last article we saw that a horse starts with gesturing by raising one front hoof, this would be the same as a boxer raising his fists, the gesture is both defensive and threatening. If neither party backs down, then the only course of action is to use those fists, or hooves in this case.\nLow Level Striking\nThis action is applied with three hooves remaining on the ground, with the fourth aiming to inflict damage to the shoulder or front legs. Low level striking is more likely to occur when each horse is facing one another for a number of reasons. To start with, this would be the next stage after hoof waving, so it’s a natural progression, the hoof is already locked, loaded and ready to be used. Another reason is universal for any species, in that energy should not be expended uselessly. A serious wound to the shoulder or leg at this point could render the opponent incapacitated, and the fight would be over sooner rather than later.\nLow level striking can also occur when a horse is attempting to kill or maim predators such as snakes, coyotes and even crocodiles. In this case both front hooves can leave the ground and quite literally pummel the intended target. In theory this action could still be thought of as striking, but stomping would actually be a more descriptive term.\nImage credit – photographer Rob Palmer (the dog survived)\nThis particular manoeuvre causes catastrophic injuries due to the speed that stomping can occur, along with the full body weight of the horse bearing down on its intended target. Furthermore the forward action of the horse will result in all four hooves trampling its prey, finishing by kicking out with the back legs as the horse moves away from the animal. It is very unlikely an animal without the speed and agility of a dog would escape unharmed.\nWith both front hooves off the ground the horse is partially rearing, and putting more body weight into the downward motion of the strike. Damage can be inflicted with either or both hooves simultaneously. The opponent’s defensive response would be to also rear to avoid injury to the head and torso. Thus, the fight escalates to full rear high striking.\nThe High Strike\nWhile many animals display deimatic behaviour as a defensive mechanism, it’s more likely the horse is standing (rearing) in this image to not necessarily to look larger but to utilise its front hooves, while attempting to avoid the opponent’s hooves. Additionally the posturing and attempts at intimidation was initially demonstrated pre-fight, but to no avail. Each, at this stage, will attempt to be higher than the other. The horse using his weight in the downward motion post rear is also used to inflict injury. So while the horse is attempting to strike at the full rear, he also has the opportunity to use his teeth and hooves while bearing down on his opponent, powered by the full weight of his body. Therefore it is important in this case, to be higher than the opponent, to both avoid, and to inflict injury.\nNext Time – Biting!\nFurther reading – Part 1\nImages: By kind permission of photographer Gary Odell"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:37d373e6-c916-467f-b461-344c1af038c7>"],"error":null}
{"question":"请问爱丁堡George Street改造项目的总预算是多少？资金来源如何？","answer":"The project is expected to cost £32m in total. Of this amount, £20m will come from Transport Scotland via Sustrans. The construction work is planned to begin in 2023 with completion expected in 2025.","context":["The final proposed concept designs for the transformation of Edinburgh’s historic George Street have been unveiled.\nDetailed artist’s impressions and a 3D fly-through illustrate how the street and surrounding area, within the UNESCO World Heritage Site, could look under the George Street and First New Town Public Realm Improvements Project.\nSignificantly widened pavements, landscaped spaces for play and relaxation and a cycling thoroughfare, where motor traffic is largely removed, will create a people-friendly setting. By retaining symmetry and removing street clutter alongside ‘urban greening’ through shrubs and hedging, the designs both protect the area’s heritage and enhance the environment.\nImprovements are being delivered as part of a coordinated package of projects under Edinburgh City Centre Transformation. This includes the forthcoming Meadows to George Street and City Centre West to East Link schemes, which will transform walking, wheeling and cycling routes and connections across the city centre. These schemes also support the City Mobility Plan, a ten-year strategy to overhaul transport and mobility in the Capital to deliver a sustainable, net zero carbon and inclusive future.\nThe concept design for George Street has been progressed by a design team led by Tetra Tech with LDA Landscape Design. It follows several years of development and engagement to refine design objectives with the public and stakeholders, including community councils and heritage, business, walking, cycling and accessibility groups. In February and March further engagement will take place with key groups, while the public will be able to see the design for themselves and tell us how they might enjoy a revitalised George Street. Feedback gathered during these months will inform a final iteration of the design proposal before being brought to Transport and Environment Committee in April. The required statutory processes under which the scheme will be constructed would begin this summer.\nCouncillor Lesley Macinnes, Transport and Environment Convener, said:\nThese animated concept designs offer an exciting glimpse into what George Street and the surrounding area could look like in 2025 – a welcoming, relaxing and unique space, where people will want to spend time, to visit local shops, cafes and restaurants and to travel to and through the city centre.\nThis vision has been years in the making and follows significant engagement with the public and a range of groups representing different interests. It’s essential that its design works for everyone, which is why we’ve spent time ensuring it meets people’s access needs, that it allows residents to go about their daily lives and that it will encourage local businesses to flourish, particularly as we look to make a strong, green recovery from the COVID crisis.\nCouncillor Karen Doran, Transport and Environment Vice Convener, said:\nI was thrilled to see the concept designs for this project brought to life and look forward to hearing the responses of all those who watch our video or see the beautiful, detailed illustrations.\nThis initiative offers the opportunity to transform one of Edinburgh’s iconic streets for the better, creating an accessible, inviting space, where both the historic environment is protected and biodiversity promoted, and where people can relax and spend time on foot, bike or wheelchair.\nTransport and Environment Committee first agreed to increase pedestrian space in the city centre in 2013 and in 2014 and 2015 a new layout was trialled on George Street, with stakeholder engagement and a subsequent series of user surveys demonstrating a strong appetite for change. Since then, a vision and design principles have been developed and the project was widened to include the interconnecting Castle, Frederick and Hanover Streets and the junctions with Charlotte and St Andrew Squares. Further, significant engagement with the public and stakeholders has led to the designs published today.\nKey considerations for the project have been putting people first, protecting the area’s heritage, promoting the environment and biodiversity and providing accessible transport links. Elements of the concept design relating to each include –\nPutting people first:\nIncreased pedestrian space; adaptable, landscaped areas with seating, space for play and opportunities for events; space for outdoor seating for cafes and restaurants; better lighting to animate the streets after dark; level access crossings at street junctions for unimpeded crossing; disabled parking bays on George Street and interconnected streets.\nRetaining symmetry on George Street; removing unnecessary street clutter; removing parking to reduce the dominance of motor traffic; upgrading pavements with high quality materials.\nEnhancing the environment:\nSuitably scaled soft landscaping including shrubs and hedging, reducing the impact of heavy rain and floods; permeably paved areas to allow drainage.\nImproved walking, cycling and wheeling connections:\nLargely car-free; cycling is prioritised in George Street and directly connects with the Meadows to George Street and CCWEL cycle route schemes at Hanover Street, St Andrew Square and Charlotte Square; improved pedestrian crossings at junctions; loading retained for businesses.\nThe George Street and First New Town Public Realm Project responds to and supports various Council initiatives, such as the Active Travel Action Plan, in addition to Edinburgh City Centre Transformation and the City Mobility Plan, as well as projects like Trams to Newhaven and Low Emission Zones.\nIf approved, it is intended to begin construction work on the scheme in 2023, with an anticipated completion in 2025. It is expected to cost £32m, with £20m from Transport Scotland via Sustrans.\nFind out more about the George Street and First New Town Public Realm Improvements Project and watch the 3D fly-through, developed by Luma3D, on the Council website."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:3fc1fa55-77f4-40c9-83c7-e41f874b1c52>"],"error":null}
{"question":"What mental and cognitive effects are common to both Sleep Disordered Breathing and drug abuse?","answer":"Both conditions affect mental and cognitive functioning. Sleep Disordered Breathing can cause loss of concentration and learning problems (particularly in children), while drug use can impair cognitive functioning, specifically affecting responses, planning, decision-making, and memory. Both conditions also impact sleep patterns, with SDB causing disturbed sleep and drug abuse resulting in sleep pattern disturbances.","context":["Dental Solutions for Sleep Disordered Breathing [Apnea]\nIf you have had complaints about your snoring, or find you’re tired and have no energy, it may be due to SDB which can lead to many serious health problems. Sleep Disordered Breathing (SDB) is a category of sleep disorders also known as Sleep Apnea, the most common form of SDB is Obstructive Sleep Apnea (OSA).\nDentists trained in Dental Sleep Medicine work with patients and their primary care physicians to diagnose and develop treatment plans that include oral appliances, therapeutic treatments and sometimes surgery. Screening for OSA is convenient. In this section, you’ll find out how you can get your energy, health and life back on track! Using this information you can find out what treatments are available and what to expect. Don’t delay, call your dentist for a consultation if you suspect you have Sleep Disordered Breathing. Studies show that Dental Solutions for SDB have an 81% success rate.\nPatients with SDB display symptoms need immediate treatment. Breathing actually stops many times during the night in patients with Apnea. Your dentist is your first line of defense against loss of health when Apnea is diagnosed because dental treatment options are successful in treating SDB. Snoring and SDB are not the same.\n95% of SDB sufferers don’t know they have it. Children and Adults can have SDB. There a several types of OSA, we’ll talk about them in these sections.\nDentists play an important role in helping patients diagnose and find treatment for this group of disorders. SDB affects millions people, some estimates put the number at over 40 million that’s 1 in 5! Find out if you have SDB.\nWhen left untreated, SDB increases the chances of serious health risks leading to reduced quality of life and long-term health problems. Your Dentist and your primary care provider can help you find the right sleep study or lab to perform diagnostic testing for SDB.\nIn this section you can find out what Apnea is, whether you have SDB, when you should seek treatment and how your Dentist can treat SDB.\nWhen health-restoring sleep is lost, the risk for these health problems increases:\n- Chronic Fatigue\n- Heart Attack\n- High Blood Pressure\n- Loss of Concentration\nChildren with Apnea do not show the same symptoms as adults, for example they may not snore like adults do. When Apnea is left untreated in children, they suffer risks that may lead to:\n- Slow Development\n- Abnormal Growth\n- Learning Problems\n- Heart Problems\n- An Overall Failure to Thrive\nTreatment options depend on the severity and causes of SDB. After diagnosis, you will consult with your Dentist to determine the best treatment for your personal health.\nIt is fairly normal for most of us to snore from time to time, especially if we are tired.\nThe rattling sound we hear in people who snore is actually caused by the vibration of tissues in your throat, such as the soft palate and uvula (the small finger-like projection hanging in the back of the throat).\nSome people, however, actually have a minor defect in their throat tissues. The defect prevents the proper amount of air from entering the windpipe. This condition is called sleep apnea. Restricted airways are caused by many things, such as an abnormally large uvula, blocked nasal passages, a poorly developed lower jaw, and in more serious cases, polyps, cysts, or a deviated septum.\nObstructive sleep apnea is a more serious form of sleep apnea. People with chronic conditions like this often suffer from restless sleep, and can develop more serious conditions such as high blood pressure, heart arrhythmias, and even congestive heart failure.\nDo You or a Loved One Snore?\nSnoring can double or even triple the risk for a stroke.\nA Yale University School of Medicine study published in the November 2005 New England Journal of Medicine reports that Sleep Apnea (sleep disturbed breathing) more than doubles the chances of a stroke or death. The Yale study found that severe cases of sleep apnea can more than triple the risk of stroke or death. This risk is reported to be independent of other cardiovascular risks.\nPeople with sleep apnea often don’t realize they have it, since they don’t remember waking up again and again, gasping for breath. Frequently, someone else hears the choking and “industrial-strength snoring,” says Klar Yaggi, a sleep specialist at Yale who led the study.\nFifty percent of middle-aged and older adults have the disorder to some degree due to throat muscles relaxing and closing off their airway as they sleep. They then wake up with a jolt, gasp for air, and fall back to sleep over and over again.\nResearch is underway to understand better the process by which the body wakes itself up to breathe. Spikes of adrenaline course through the body when breathing stops, increasing blood pressure. This is an automatic body response to repeated plunges in the level of oxygen in the blood as a result of the blocked airway.\nTreatments for sleep apnea include weight loss, the use of a breathing machine called a “Continuous Positive Airway Pressure” or “CPAP” machine, and wearing a custom-made device in the mouth to keep the airway open during sleep. The loss of weight and treatment by a doctor who treats sleep apnea can restore a good night’s sleep and eliminate considerable stress on your body from oxygen deprivation and adrenaline surges.","The Effects of Drug Abuse & Addiction\nDrug abuse can result in short- and long-term negative consequences ranging from issues such as disrupted sleep patterns to loss of employment and problems with relationships.1, 2 Problematic substance use can also increase a person’s risk for developing long-term medical issues such as heart disease, cancer, as well as certain co-occurring mental health conditions.1\nWhich specific type of drug or drugs, how much and how they are used, and an individual’s health are some factors that can contribute to the potential range of adverse effects a person may experience.3 While some consequences result directly from drug abuse and addiction, others may occur indirectly in relation to drug use.\nLearn more about the effects of drug abuse and how a substance use disorder (SUD) may negatively impact your life below.\nHealth Effects of Drug Abuse\nThough the associated, substance-related effects may vary from one drug to the next, general short-term health consequences of drug use can include issues such as:3\n- Changes in appetite.\n- Disturbances in sleep patterns.\n- Cardiovascular issues including stroke and heart attack.\n- Overdose toxicity or death.\nLong term use of some drugs may also increase the likelihood of certain cancers; developing heart, liver, and kidney disease; as well as experiencing neurological issues such as seizures.4, 5 Injecting drugs such as cocaine, heroin, and methamphetamine is also associated with an increased risk of contracting hepatitis, HIV/AIDS, and other infections.6,7 Drug use may also negatively impact cognitive functioning, resulting in impaired responses, planning, decision-making, and memory.8\nDrug use can alter the activity of several brain chemicals—or neurotransmitters—including dopamine. Dopamine is a neurotransmitter associated with pleasure, reward, and satisfaction, and several different drugs have been found to influence dopamine pathways in the brain.9\nThough the neurochemical details are complicated, consistently elevated dopamine activity as a result of substance use can strongly reinforce drug seeking behavior and compulsive drug use. Additionally, as someone’s brain and body adjusts to frequently being high, they may experience relatively diminished reward from otherwise normally reinforcing activities like sex or eating food.9\nSubstance abuse and mental illness commonly coincide, with research showing that about half of those who experience a mental illness during their lives will also experience a substance use disorder and vice versa.10 Commonly co-occurring mental health disorders include depression, anxiety, attention-deficit hyperactivity disorder (ADHD), bipolar disorder, schizophrenia, borderline personality disorder, and antisocial personality disorder.10\nImpact of Substance Abuse on Relationships\nDrug abuse can affect more than just the person getting high, it can also be detrimental to their relationships with friends, family members, and coworkers.11 Often, couples in which one or both partners abuse drugs are less happy than couples who don’t.11 Using drugs seems to cause significant conflict between intimate partners; these couples tend to fight more often than couples in which both partners don’t use drugs.11 Unfortunately, substance abuse can sometimes lead to violence in intimate relationships as well. 11\nDrug Addiction and Financial Problems\nThe use of illicit drugs can not only affect an individual’s health and relationships but can cause issues for individuals financially—both in their personal and professional lives. It has been estimated that substance abuse—including alcohol, prescription opioids, illicit drugs, and tobacco—has cost the nation more than $740 billion annually in lost work productivity, health care costs and crime.13\nResearch has shown that drug use is related to a number of problematic work behaviors such as decreased performance levels and increased accident rates.14 Additionally, those using illicit drugs may be prone to absenteeism at work, abuse of work benefits, and low workplace rule abidance.14 These issues, in turn, could result in the loss of employment and difficult obtaining new work in the future.\nWhen it comes to the cost of drugs, paying for them can also begin to be a burden on an individual’s income, especially as increasing amounts of the drugs may become necessary to overcome a growing tolerance to their effects. Since drug use can also result in serious health problems, those struggling with addiction may be faced with increased medical fees and/or insurance premiums.\nDrug use can also lead to costs associated with criminal activities such as DUI charges, bail, attorneys, court-mandated classes, court fines, and public transportation due to the loss of a vehicle or license.\nDangers of Overdosing\nRegardless of how often a person uses certain types of substances—such as cocaine, sedatives, heroin, or other opioids—there is always some risk of overdose, especially when these drugs are mixed with other drugs and/or alcohol. In 2018, more than 65,000 Americans died as a result of overdoses, including those that involved illicit drugs and prescription opioid medications.15\nOverdoses can occur when a drug is used to excess, resulting in toxicity or injury and, in some cases, death.16 Various factors can play into whether someone overdoses on a drug, such as a person’s body weight, gender, tolerance, the potency of the drug, and the other drugs that may have been taken with it.17\nWarning Signs of Overdose\nThe signs and symptoms of overdoses can vary based on the drug. Some signs include:17\n- Pale and/or clammy face.\n- Changes in pulse rate and blood pressure.\n- Shallow, erratic breathing.\n- Loss of consciousness.\n- Severe stomach pain.\nIf you believe someone is experiencing an overdose, call 9-1-1 immediately and wait for medical assistance.\nGetting Help For Drug Addiction\nDrug abuse and addiction can greatly impact your life, resulting in negative consequences to every part of your life. If you feel you’re ready to make a change and quit using drugs and/or alcohol, American Addiction Centers (AAC) is here to help.\nAAC operates AlcoholRehab.com and is a nationwide provider of addiction treatment facilities. Our admissions navigators are available 24/7 to speak with you about your options for treatment today. It’s never too late to get clean and take back control of your life, relationships and finances.\nAll calls are 100% confidential and there’s no pressure to make any decisions right away. We’re here for you and will walk with you every step of the way as you embark on your journey to recovery.\nNot sure if your insurance covers alcohol treatment?\nCheck your insurance coverage or text us your questions for more information."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:50ea748d-bca4-4d5d-9925-7aa085d0a72d>","<urn:uuid:7969a82d-3d90-48b2-8af5-5b6e05263de3>"],"error":null}
{"question":"What's the difference between a PICC line and a cardiac catheterization in terms of their insertion procedures?","answer":"PICC line and cardiac catheterization have different insertion procedures. A PICC line is inserted as a non-surgical outpatient procedure through one of the larger veins of the arm, with fluoroscopy X-ray confirming proper placement. In contrast, a cardiac catheterization typically involves threading a thin flexible tube through the arteries and veins from the groin area to reach the heart and lungs, and is usually performed under general anesthesia in children with congenital heart disease.","context":["Injection Site Reactions\nInjection site reactions discussed here are local skin reactions that occur when\nthe drug escapes from the veins or IV catheter into the skin (extravasation).\nThe drugs that can cause injection site reactions are divided into 2 types:\nirritants and vesicants.\nIrritants cause a short-lived and limited irritation to the vein:\n- Symptoms are; tenderness, warmth, or redness along the vein or at the injection\n- A variation to this is a hypersensitivity \"flare reaction\" at the injection site.\n- Symptoms of this reaction are redness and itching at the injection site.\n- Irritant chemotherapy agents include: bleomycin, carboplatin, carmustine, cisplatin,\ndacarbazine, denileukin difitox, doxorubicin, doxorubicin liposome, etoposide,\nifosfamide, streptozocin, teniposide, thiotepa, vinorelbine.\nVesicants cause an injection site reaction often referred to as a chemical\n- The reaction to vesicants initially looks like a irritation but may worsen,\ndepending on the amount of vesicant that has leaked under the skin.\n- Vesicants can cause redness and blistering. Larger amounts of vesicant leakage from\nthe chemotherapy injection can lead to severe skin damage in a matter of days.\n- Symptoms from leakage of vesicants may be delayed for up to 6-12 hours after chemotherapy\n- Complaints of itching are common in the absence of pain.\n- Severity of the injection site reaction depends on the vesicant potential of the\ndrug, the amount and concentration of the drug exposure, and the immediate measures\ntaken once the extravasation occurs.\n- Vesicant chemotherapy agents include: Dactinomycin, daunorubicin, doxorubicin, epirubicin,\nidarubicin, mechlorethamine, mitomycin, mitoxantrone, paclitaxel, streptozocin,\ntenoposide, vinblastine, vincristine, vinorelbine.\nPrevention of extravasation is the key to managing these types of injection\nThe nurse or doctor giving these types of chemotherapy injections must be carefully\nA central venous access device might be recommended such as:\n- PICC line: Although still considered temporary, a PICC\nline can be inserted for chemotheraphy injection and used for six weeks to a few\nmonths before it is discontinued. This involves the placement of a long plastic\ncatheter into one of the larger veins of the arm. This procedure is a non-surgical\noutpatient procedure. A special x-ray, called fluoroscopy will confirm that\nthe catheter is in the right place, reducing risk of infection site reaction.\nThis option is ideal for multiple short infusions or continuous infusions given\nin a hospital or at home with a portable pump.\n- Tunneled catheter: Tunneled catheters are placed through the\nskin in the middle of the chest. They are tunneled through the subcutaneous\ntissue (the layer of tissue between the skin and muscle) and inserted into the superior\nvena cava vessel at entrance of the right atrium of the heart. There is a\ndacron cuff about two inches from the part of the catheter that exits the skin in\nthe chest. Scar tissue forms around the cuff to hold the catheter in place.\nThese catheters are inserted in an outpatient surgical procedure and a special x-ray,\ncalled fluoroscopy, must be done to be sure the catheter is in the right place,\nreducing risk of leaking vesicants. These catheters can be left in place for\nchemotheraphy injection for months or years with low incidence of infection.\nDressing changes and maintenance is required. These catheters can have multiple\nlumens (entrances) for medications to be infused or for blood to be drawn.\nA single lumen has one entrance for medications, a double lumen has two entrances\nand a triple lumen (the most available) has three entrances. These catheters\nare most often used for extensive chemotherapy regimens such as bone marrow transplant\nprocedures. Tunneled catheters are usually called by their brand names:\n- Port-a-cath: A more permanent injection site option involves the\nplacement of a port-a-cath. The port-o-cath is placed under the skin on the\nchest. The cathether is then inserted into the superior vena cava vessel at\nentrance of the right atrium of the heart. This catheter can be placed in\nradiology by an interventional radiologist or by a surgeon in the operating room.\nIt is approximately a one-hour procedure. The useful lifetime of a port-a-cath can\nbe as long as three to five years. The port-o-cath can be felt under the skin\nand the nurse can find the injection site by locating the edges of the\nport-o-cath and inserting (cannulating) a special needle (called a Huber needle)\ninto the soft middle section. Medications can be given through the port-a-cath\nand blood can be drawn from it eliminating the need for a blood draw from the arm.\nThe use of a portable pump and port-a-cath allows the medication to be given over\nseveral days in the home setting rather then as a patient in the hospital.\nThere are no dressing changes required but there is some maintenance involved.\nThings you can do to better prevent injection site reaction:\n- Notify your health care professional immediately if you experience pain or discomfort\nat the injection site.\n- If you have redness or discomfort at the site after leaving the chemotherapy treatment\nfacility, apply ice to the injection site and notify your healthcare professional\nof potential irritant or vesicant leakage.\n- EXCEPT: If extravasation of a vinca alkaloid medication - vincristine,\nvinblastine or vinorelbine, then you would apply warm compresses and notify your\nhealth care professional of the possibility of injection site reaction.\nTreatments or drugs that might be used by your health care professional in the\nevent of injection site reaction:\n- Prevention of extravasation is the key to management of this problem.\n- If one of the above medications has extravasated the health care professional will\nattempt to remove as much of the medication as possible from the injection site\nand discontinue the IV.\n- Ice or heat (in the case of vinca alkaloid chemotherapies) will be applied to the\n- There are various antidotes that may be given based on the chemotheraphy that\nextravasated, and the amount of drug infused. The nurse or doctor will work\nwith the chemotherapy pharmacist to decide if an antidote is available and appropriate\nto the situation.\nWhen to call your doctor or health care professional about potential injection\n- If you have received an irritant or vesicant medication and notice redness, pain\nor blistering at the site of infusion.\nNote: We strongly encourage you to talk with your health care professional\nabout your specific medical condition and treatments. The information contained\nin this website about chemotheraphy, injection site reaction and other medical conditions\nis meant to be helpful and educational, but is not a substitute for medical advice.","Pediatric Cardiac Catheterization and Electrophysiology Studies\nFor Referring Physicians\nThe Pediatric Heart Program is proud to join the tradition of outstanding clinical programs that you expect from University of Wisconsin Hospital and Clinics and American Family Children's Hospital.\nStaff of the American Family Children's Hospital Pediatric Cardiology Clinic specialize in the care of children and young adults with heart-related conditions. During a cardiac catheterization, parents can be with their child through much of the process. Here are some frequently asked questions to help parents:\nWhy does your child need a cardiac catheterization?\nA cardiac catheterization is a procedure used to both diagnose and treat congenital heart disease. It provides information used to decide if there is a need for medicines or surgery. Interventions can also be done during a cardiac catheterization to treat problems that may have needed surgery. The cardiologist doing the procedure will go over the plan in detail and answer any questions you may have. If you are not sure about what is going to happen during the catheterization, please ask questions. It is our job to make sure you are comfortable with the care plan for your child before the procedure.\nWhat is the Cath Lab?\nAlso known as the Catheterization Lab, it is a place where cardiac catheterizations are performed.\nHow is a catheterization done?\nA cardiac catheterization is a procedure which involves threading a thin flexible tube (catheter) through the arteries and veins of the heart and lungs, often from the groin (see illustration). With the use of X-ray and contrast dye, the procedure team can define structure and function of the heart and lungs. This information is then used to determine the best therapies for treating congenital heart disease. We call that a diagnostic catheterization. If the doctor is also going to treat a type of congenital heart disease during the procedure, it is called an interventional catheterization. A number of interventions can be performed in the cath lab.\nIs it painful?\nMost cardiac catheterizations done in children with congenital heart disease are performed with general anesthesia. You will meet with a dedicated pediatric cardiac anesthesiologist before the procedure who will review your child’s medical history to make sure anesthesia is safe and effective. The anesthesiologist will monitor your child during the entire procedure. Your child will sleep through the procedure without pain or anxiety. After the procedure, there may be slight discomfort at the insertion sites. Pain medicines are given as needed. Pain management is a priority at American Family Children’s Hospital, and you can expect pain to be treated and controlled during the hospital stay. We also have a dedicated team of Child Life specialists experienced in supporting and distracting children during hospital visits. If support from a Child Life specialist would be helpful for your child, please let us know so it can be arranged ahead of time.\nHow long does a catheterization take?\nUsually, there is at least a 2- to 3-hour wait from the time your child leaves for the procedure until you are reunited in the recovery area or in an inpatient room at American Family Children's Hospital. The cath lab team, along with the recovery room or inpatient nurse, will let you know where to wait and when you can come into the room, once your child is settled after the procedure.\nWhere do I wait during the procedure?\nAt the start of the catheterization, staff will bring family members and guests to the Cath Lab Waiting Room. Cath lab staff will be available during this time to assist you, and food and restrooms are either in or near these areas while you wait. During the procedure, cath lab team members will give you frequent updates about the progress of the catheterization.\n- Luke Lamers, MD\n- Nicholas Von Bergen, MD\n- Jenna Torgeson, APNP\n- Martine Moran, RN"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:dec5ab8a-71a7-4961-91c5-6fc728bd8daf>","<urn:uuid:9e465787-5611-45bc-b2b0-4840705c968c>"],"error":null}
{"question":"As an aerospace engineer, I need to understand: what are the key differences between matched filter and heterodyne detectors in channel sounding, and how do cooperative spectrum sensing rules compare in terms of detection performance?","answer":"Matched filter and heterodyne detectors differ primarily in their signal processing approach. Matched filter detection compresses the signal in time and maximizes sample time SNR, providing multipath structure in real time. However, heterodyne detection compresses the signal in frequency instead of time, enabling the use of low bandwidth digitizers and requiring post-processing spectrum analysis. For cooperative spectrum sensing performance, the OR rule shows better performance than AND and MAJORITY rules across various channels. The OR rule decreases probability of missed detection, with performance improving as the number of cooperative users increases, since with more users there's a higher probability of having a user with a favorable channel to detect primary user presence.","context":["Visit for more related articles at International Journal of Advance Innovations, Thoughts & Ideas\nSpectrum sensing is the key component of cognitive radio technology. However, detection is compromised when a user experiences shadowing or fading effects. In such cases, user cannot distinguish between an unused band and a deep fade. Thus, cooperative spectrum sensing is proposed to optimize the sensing performance. We focus performance of cooperative CR user based on spectrum sensing using energy detector in non-fading channel AWGN and fading channels such as Rayleigh, Ricean and Nakagami. This paper presents a simulation comparison of these fading channels based on hard decision combining fusion rule (OR-rule, AND-rule and MAJORITY-rule). Fusion rule is performed at fusion center (FC) to make the final decision about the presence of PU. We observe that spectrum sensing is harder in presence of Rayleigh and Nakagami fading and performance of energy detection degrades more in Nakagami channels than Rayleigh and Ricean channels. It also found that Spectrum sensing in Ricean fading has better results than others.\nCognitive Radio, Hard decision fusion rules, Cooperative spectrum sensing, fading channels, Energy detection\nCognitive radio (CR) technique has been proposed to solve the conflicts between spectrum scarcity and spectrum under-utilization . It allows the CR users to share the spectrum with primary users (PU) by opportunistic accessing. The CR can use the spectrum only when it does not cause interference to the primary users. Therefore, spectrum sensing is a critical issue of cognitive radio technology since it needs to detect the presence of primary users accurately and swiftly. Existing spectrum sensing techniques can be divided into three types : energy detection, matched filter detection and cyclostationary detection. Among them, energy detection has been widely applied since it does not require any a priori knowledge of primary signals and has much lower complexity than the other two schemes. Spectrum sensing is a tough task because of shadowing, fading, and time-varying nature of wireless channels . The radio channel is characterized by two types of fading effects: large scale fading and small scale fading , . Small scale fading models include the wellknown Rayleigh, Rice, and Nakagami-m - distributions. For large scale fading conditions, it is widely accepted that the probability density function (PDF) of the fading envelopes can be modeled by the wellknown Log-normal distribution , . Due to the several multipath fading, a cognitive radio may fail to notice the presence of the PU and then will access the licensed channel and cause interference to the PU. To combat these impacts, cooperative spectrum sensing schemes have been proposed to obtain the spatial diversity in multiuser CR networks [9-11]. The performance of single CR user based spectrum sensing in fading channels such as Rayleigh, Nakagami, Weibull has been studied in . The performance of cooperative spectrum sensing with censoring of cognitive radios in Rayleigh fading channel has been evaluated in [13-15]. Cooperative spectrum sensing improves the detection performance. All CR users sense the PU individually and send their sensing information in the form of 1-bit binary decisions (1 or 0) to Fusion center (FC). The hard decision combining rule (OR, AND, and MAJORITY rule) is performed at FC using a counting rule to make the final decision regarding whether the primary user present or not -. Hard decision combination-based cooperative spectrum sensing has been addressed in [19-22]. However, the existed works only examined the additive white Gaussian noise (AWGN) channel and the Rayleigh fading channel. In this paper, we study hard decision based cooperative spectrum sensing over Rayleigh, Nakagami and Ricean fading channels.\nThe rest of this paper is organized as follows. In Section II, the system model is introduced. In Section III, detection and false alarm probabilities of non-fading AWGN and fading channel such as Rayleigh, Ricean and Nakagami are described. Cooperative spectrum sensing over various fading channels is derived in Section IV. The simulation result and discussion are presented in section V. Finally, we draw our conclusions in Section VI.\nThe local spectrum sensing is to decide between the following two hypotheses,\nwhere x(t) is the signal received by secondary user and s(t) is primary user’s transmitted signal, n(t) is the additive white Gaussian noise (AWGN) and h is the amplitude gain of the channel. The energy collected in the frequency domain is denoted by Y which serves as a decision statistic. Following the work of Urkowitz , Y may be shown to have the following distribution,\nwhere and denote central and non-central chi-square distributions respectively, each with 2TW degrees of freedom and a non-centrality parameter of 2γ for the latter distribution. For simplicity we assume that time-bandwidth product, TW, is an integer number which we denote by u.\nIn this section, we give the average detection probability over Rayleigh, Nakagami, and Ricean fading channels and in closed form . In communications theory, Nakagami distributions, Rician distributions, and Rayleigh distributions are used to model scattered signals that reach a receiver by multiple paths. Depending on the density of the scatter, the signal will display different fading characteristics. Rayleigh and Nakagami distributions are used to model dense scatters, while Rician distributions model fading with a stronger line-of-sight. Nakagami distributions can be reduced to Rayleigh distributions, but give more control over the extent of the fading.\nNon-fading environment (AWGN channel )\nIn non-fading environment the average probability of false alarm, the average probability of detection, and the average probability of missed detection are given, respectively, by \nwhere λ denotes the energy threshold. Γ(.) and Γ(.,.) are complete and incomplete gamma functions respectively  and (.,.) Qu is the generalized Marcum Q-function defined as follows,\nwhere is the modified Bessel function of (u−1)th order. If the signal power is unknown, we can first set the false alarm probability Pf to a specific constant. By equation (4), the detection threshold λ can be determined. Then, for the fixed number of samples 2TW the detection probability Pd can be evaluated by substituting the λ in (3). As expected, Pf is independent of γ since under H0 there is no primary signal present. When h is varying due to fading, equation (3) gives the probability of detection as a function of the instantaneous SNR, γ. In this case, the average probability of detection may be derived by averaging (3) over fading statistics ,\nwhere fγ(x) is the probability distribution function (PDF) of SNR under fading.\nRayleigh fading channel\nWhen the composite received signal consists of a large number of plane waves, for some types of scattering environments, the received signal has a Rayleigh distribution . If the signal amplitude follows a Rayleigh distribution, then the SNR γ follows an exponential PDF given by\nIn this case, a closed-form formula for d P may be obtained (after some manipulation) by substituting in (6),\nRicean fading channel\nSome types of scattering environments have a specular or LoS (Line of Sight) component. In this case, the amplitude of received signals has a Ricean distribution. If the signal strength follows a Rician distribution, the PDF of γ will be\nwhere K is the Rician factor. The average Pd in the case of a Rician channel, is then obtained by averaging (3) over (9) and substituting x for The resulting expression can be solved for u = 1 using , Eq. (45)] to yield\nFor K = 0, this expression reduces to the Rayleigh expression with u = 1.\nNakagami fading channel\nAlthough Rayleigh and Ricean distributions are the most popular distributions to model fading channels, some experimental data does not fit well into neither of these distributions. Thus, a more general fading distribution was developed whose parameters can be adjusted to fit a variety of empirical measurements . This distribution is called the Nakagami fading distribution. The Nakagami distribution was introduced by Nakagami in the early 1940’s to characterize rapid fading in long distance HF channels . It is possible to describe both Rayleigh and Rician fading with the help of a single model using the Nakagami distribution. The Nakagami m-distribution is used in communication systems characterize the statistics of signal transmitted through multipath fading channels.\nThe Nakagami distribution is often used for the following reasons. First, the Nakagami distribution can model fading conditions that are either more or less severe than Rayleigh fading. When m=1, the Nakagami distribution becomes the Rayleigh distribution, when m=1/2, it becomes a one-sided Gaussian distribution, and when m=∞ the distribution becomes an impulse (no fading). Second, the Rice distribution can be closely approximated by using the following relation between the Rice factor K and the Nakagami shape factor m ;\nSince the Rice distribution contains a Bessel function while the Nakagami distribution does not, the Nakagami distribution often leads to convenient closed form analytical expressions that are otherwise unattainable. Using the alternative representation of Marcum-Q function given in [28, eq. (4.74), pp. 104], (1) can be written as,\nIf the signal amplitude follows a Nakagami distribution, then the PDF of γ follows a gamma PDF given by\nwhere m is the Nakagami parameter. The average Pd in the case of Nakagami channels can now be obtained by averaging (3) over (12) and then using again the change of variable yielding\nIn this case, a closed-form formula of Nakagami channels can be given by\nwhere is the confluent hypergeometric function .\nWhere is the first-order Marcum Q-function. G1 can be evaluated for inter m with the aid of [25, Eq.(25)]\nwhere is the Laguerre polynomial of degree n [25, 8.970].\nIn real communication environments, the hidden terminal problem, deep fading and shadowing, etc., would deteriorate the signal detection performance of cognitive users. To address this issue, multiple cognitive radios can be coordinated to perform spectrum sensing. Several recent works have shown that cooperative spectrum sensing can greatly increase the probability of detection in fading channels , .\nLet N denote the number of users sensing the PU. Each CR user makes its own decision regarding whether the primary user present or not, and forwards the binary decision (1 or 0) to fusion center (FC) for data fusion. The PU is located far away from all CRs. All the CR users receive the primary signal with same local mean signal power, i.e. all CRs form a cluster with distance between any two CRs negligible compared to the distance from the PU to a CR. For simplicity we have assumed that the noise, fading statistics and average SNR are the same for each CR user. We consider that the channels between CRs and FC are ideal channels (noiseless). Assuming independent decisions, the fusion problem where k out of N CR users are needed for decision can be described by binomial distribution based on Bernoulli trials where each trial represents the decision process of each CR user. With a hard decision counting rule, the fusion center implements an n–outof- M rule that decides on the signal present hypothesis whenever at least k out of the N CR user decisions indicate . Assuming uncorrelated decisions, the probability of detection at the fusion center  is given by\nwhere Pd,i is the probability of detection for each individual CR user as defined by (3) and (6).\nIn this rule, if all of the local decisions sent to the decision maker are one, the final decision made by the decision maker is one. The fusion center’s decision is calculated by logic AND of the received hard decision statistics. Cooperative detection performance with this fusion rule can be evaluated by setting k=N in eq. (19).\nIn this rule, if any one of the local decisions sent to the decision maker is a logical one, the final decision made by the decision maker is one. Cooperative detection performance with this fusion rule can be evaluated by setting k=1 in eq. (19).\nLogical MAJORITY -Rule\nIn this rule, if half or more of the local decisions sent to the decision maker are the final decision made by the decision maker is one. Cooperative detection performance with this fusion rule can be evaluated by setting k = N/2 in eq. (19).\nwhere represents the floor operator.\nAll simulation was done on MATLAB version R2011a over three different fading under Rayleigh, Ricean and Nakagami channel and a non-fading channel AWGN. We described the receiver through its complementary ROC curves for different values of probability of false alarm and Cognitive Radio user.\nFig. 2(a), 2(b) and 2(c) show complementary ROC curves of the 10 user’s spectrum sensing in three different fading under Rayleigh, Ricean and Nakagami fading following AND rule, OR rule and MAJORITY rule respectively. Average SNR and u are assumed to be 10 dB and 5 respectively. Rice factor k and Nakagami parameter m are set to be 5 and 3 respectively. A plot for non-fading (pure AWGN) case is also provided for comparison.\nComparing the AWGN curve with those corresponding to fading, we observe that spectrum sensing is harder in presence of Rayleigh and Nakagami fading. In Ricean channel, because of the LoS signal, the sensing performance is better than in other channels. We observe that the OR rule has the better performance than AND and MAJORITY rule in various channels.\nFig. 3(a), 3(b), 3(c) and 3(d) show complementary ROC of hard decision fusion rule(AND-rule, OR-rule and MAJORITY-rule) of 10 user’s spectrum sensing in non-fading AWGN and three different fading under Rayleigh, Ricean and Nakagami fading respectively. A plot for single user’s spectrum sensing is also provided for comparison. As before γ = 10 dB, u =5, k = 5, m = 3.\nSimulation result shows that probability of missed detection of AND rule is larger than missed detection of single user over various channels. It also shows that OR rule has the better performance than AND and MAJORITY rule. Comparing the AWGN curve with those corresponding to fading, we observe that spectrum sensing is harder in presence of Rayleigh and Nakagami fading.\nFig. 4(a), 4(b), 4(c) and 4(d) show the complementary ROC of hard decision fusion OR rule for different number of cooperative users of cooperative spectrum sensing over non-fading AWGN channel and three different fading such as Ricean, Rayleigh and Nakagami fading channel respectively. A plot for single user’s spectrum sensing is also provided for comparison. As before γ = 10 dB, u =5, k = 5, m = 3.\nSimulation result shows that cooperative sensing performance is getting better with increasing CR user as for larger CR user, with high probability there will be a user with a preferable channel to find the presence of PU.\nWe have studied hard decision based cooperative spectrum sensing over different fading channel in cognitive radio. Performance of cooperative spectrum sensing over Rayleigh, Ricean and Nakagami fading are presented and compared. It has been found that probability of missed detection is decreased by using different hard decision fusion rules. We observe that the OR rule has the better performance than AND and MAJORITY rule in various channels. We also observe that spectrum sensing is harder in presence of Rayleigh and Nakagami fading and performance of energy detection degrades more in Nakagami channels than Rayleigh and Ricean channels. In Ricean channel, because of the LoS signal, the sensing performance is better than in other channels. Furthermore, spectrum sensing in Ricean fading has better results than others.","Growth in the number of aeronautical wireless devices in the national airspace may require operation of some links as second order users. These users would access existing resources when not used by primary users. Many solutions have been proposed in the literature to alleviate spectrum congestion. As an example, for multicarrier schemes such as Orthogonal Frequency Division Multiple Access (OFDMA), the subcarriers can be allocated to users as needed. In order to properly configure such a scheme, one needs to know the channel characteristics, particularly delay spread and Doppler spread. OFDM has been applied for over a decade in terrestrial wireless systems, as it is often an excellent method for high rate bi-directional wireless data communication. It can also be a good candidate for wideband communications to transmit/receive payload data from one Unmanned Aerial System (UAS) to another.\nMany studies have been done for estimating channel characteristics, in multiple environments. Such measurements are often termed “sounding.” With the growth in use of UAS, aeronautical propagation channels have also been studied in recent years. In many channel measurement campaigns, sophisticated channel sounders were used –, but there is limited work on real time sounding in VLL scenarios, particularly with software defined radios (SDRs). Low altitude UAS usually have a limited “maximum payload/takeoff handling weight,” therefore using low-weight, low-cost and multi-purpose SDRs as channel sounders can be an attractive solution. Moreover, the flexibility and re-configurability of SDRs can enable easy adjustment of measurement parameters in a short time.\nChannel sounding is done by exciting the channel with a known input signal. The aim is to estimate some characteristics of the channel. For narrowband channels, this is typically the attenuation, which may include fading. For wideband channels, one desires an estimate of the channel impulse response (CIR), or its Fourier transform, the channel transfer function (CTF). Since most channels have time (and spatial) variation, they are often characterized statistically. For the CIR, one wants the amplitudes, delays, and time variation of all multipath components (MPCs). This enables computation of delay spread and Doppler spreads, which are important to determine the appropriate communication signaling design. Designing a communication system without accurate channel knowledge will yield sub-optimal performance. For example, if a signal's bandwidth is larger than the channel's coherence bandwidth, distortion results, and this must either be mitigated (increasing complexity), or a performance degradation (e.g., larger error probability) must be accepted.\nThe OFDM multicarrier signal is sensitive to frequency offset and drifts that can be caused by the Doppler effects in rapidly time-varying channels such as VLL AA communication links. Multipath components can also have a detrimental effect on OFDM system performance, where orthogonality of the subcarriers can be lost when the delay spread is significant. Therefore, OFDM systems employ guard intervals to avoid intersymbol interference. During the guard interval, the transmitter can send nothing, or most commonly, a cyclic prefix (CP) which is an exact copy of a segment of the last portion of the OFDM symbol. The length of the guard interval and subcarrier spacing are generally set in advance of transmission  .\nThe flexibility and re-configurability of SDRs enables cognitive sounding, which means enabling real time CP length and subcarrier spacing adjustment in order to achieve the best performance in different channel scenarios. Although small commercial SDRs may not be able to perform all sophisticated sounding techniques  due to their limitations in bandwidth however, their flexibility and re-configurability are suitable for cognitive sounding in dynamic AA channels.\nChirp Channel Sounding Principles\nUnmodulated continuous-wave (CW) sounders transmit a single tone with constant frequency. The lack of modulation of the source only allows for determination of Doppler shift and amplitude at that specific frequency. In contrast, a frequency modulated continuous-wave (FMCW) signal, also known as a chirp, employs frequency modulation at the signal source to enable channel transfer function measurements. The FMCW sounder usually transmits a signal that has a linear frequency increase or decrease over a frequency range of B Hz in T seconds, where B/T Hz/s is known as the “sweep rate” and the “time bandwidth product” BT is the “dispersion factor” . Mathematically a chirp signal is expressed as :\nis the carrier frequency in radians/s and μ=2π(BT)\n. As calculated in \n, the spectrum for large time bandwidth values is represented by,\nThe chirp bandwidth (B) and waveform repetition frequency (WRF) are the main parameters that should be considered in designing chirp signals for sounders. The WRF value is proportional to the reciprocal of the chirp duration, i.e., proportional to 1/T. The maximum Doppler shift that the chirp sounder can detect depends on the repetition frequency as WRF≥2fdmax. The channel delay resolution increases as bandwidth increases.\nUnlike pulsed-sounders, the spectrum of chirp sounders is predominantly contained within the frequency range of the swept bandwidth, therefore filtering is not necessary prior to transmission (although some filtering is generally applied). Chirp sounder receivers must process the received signal to extract channel characteristics, and there are typically two options for receiver structures. In this paper, we employ both matched filter and heterodyne detectors for our simulations and measurements.\nA. Matched Filter Detector\nThe technique of matched filtering is known to maximize sample time SNR; the matched filter CIR is the (conjugate of the) time-reversed transmit signal pulse. This filtering can also be expressed as a correlation. This correlation enables estimation of the CIR parameters (i.e., MPC delays and amplitudes, and with their time variation Doppler spread can also be estimated. A block diagram of a matched filter detector for a chirp sounder is depicted in Figure 1, where the distorted received signal is convolved with a conjugated time-reversed version of the transmitted chirp signal.\nMatched filter block chain\nThe impulse response of the filter matched to the signal (1) is given by\nis an arbitrary real constant. By imposing a unity gain condition at ωc \n, the matched-filter constant will be\nEquation (3) represents a chirp signal with a frequency slope opposite to that of the transmitted signal. The output of the matched filter is found by convolving the chirp signal with the matched filter response, therefore the matched filter accomplishes time compression by delaying the frequencies in their reverse order, thereby bringing all the frequencies back in phase. The output of the matched filter for a chirp signal with a 1 MHz bandwidth and one millisecond duration is depicted in Figure 2. The inset shows the result near the peak at 1 ms.\nCompressed signal at matched filter output\nAs depicted in Figure 2, the envelope of the compressed signal is of the form of a sin(x)/x function; is the Fourier transform of a rectangular pulse over the bandwidth of the chirp signal. The success of the matched-filter detector technique in a multipath channel critically depends on producing a compressed-pulse waveform at the output of the matched filter with smaller side-lobes. The amplitude of time domain side-lobes of the compressed signal determine the resolution of multipath components in the sounder. The first and the largest side-lobe is only 13.5 dB below the peak of the compressed pulse and the side-lobes after that decrease by approximately 3 dB per side-lobe interval (Figure 3).\nTo minimize the effects of these unwanted side-lobes on the system performance, different methods such as windowing have been proposed . In this paper, we weight the transmitted signal in either the time or frequency domain by using various weighting functions. Thus although the theoretical time delay resolution of a chirp signal is 1/B seconds, practically it also depends on the window function used to reduce the side-lobes of the compressed signal. Proper windowing can increase time delay resolution while it costs in signal-to-noise ratio (SNR). Therefore, there is a tradeoff between resolution and main lobe signal amplitude in designing the window function. Our windowing function is based on an algorithm introduced in , and is called Taylor windowing. Although in , the Hamming window was mentioned as having the best windowing performance for sounders with noise floor nearly −60 dB relative to desired signal, the main lobe degradation is significant (approximately −11 dB). A comparison these different windowing functions is depicted in Figure 3.\nComparison between two well-known windowing functions\nIn Figure 3 the amplitude of the Hamming main lobe is smaller than for the other two, but its side lobes are lower. Taylor windowing on the other hand has a tradeoff between Hamming and no windowing.\nB. Heterodyne Detector\nThe technique of heterodyne detection is based on multiplication of the received signal by a delayed replica of the transmitted signal y(t−τ0). The frequency sweeps upward linearly, then the output of the multiplication is low pass filtered. The spectrum of the receiver output is then analyzed to achieve the sin(x)/x compression. The block diagram is depicted in Figure 4.\nHeterodyne detector block diagram\nAssuming an ideal channel, the output of the heterodyne mixer can be represented mathematically as\nAfter applying a low-pass filter the first term is removed, therefore the frequency of the remaining term is given by\nis the time difference between the original chirp signal and the locally generated signal. Thus, the filter cutoff-frequency fmax\nshould be chosen in a way to accommodate the time delay τmax\nof the longest-delay MPC with frequency of (BT)τmax\nThe advantage of a heterodyne detector compared to the matched filter detector is that the heterodyne detector compresses the signal in frequency instead of time, and this feature enables the use of low bandwidth digitizers and channel data acquisition, where digitizer bandwidth depends on the maximum time delay or the range of the farthest multipath component. This behavior is favorable for current SDRs that are limited in data acquisition speed. However, matched filter detection gives the multipath structure in real time as the output consists of the time-compressed signal, whereas the heterodyne detector requires a post processing spectrum analyzer to perform the same function. Therefore, applying fast Fourier transform (FFT) over a single sweep gives a sin(x)/x main lobe centered at the frequency corresponding to the time delay between transmitter chirp and replica in the receiver.\nIn the post-processing step, applying the FFT algorithm gives a spectrum that is extended in frequency from zero to half the sampling frequency fs, thus these samples can be scaled to the time delay domain using sweep rate B/T according to τi=(TB)fi, where τi is the time delay of the ith multipath component.\nChannel Modeling and Simulations\nOur proposed scenario is characterization of a moderately dense environment with several multipath components for AA communication between two VLL UASs moving in random directions with relative speed of Δv as depicted in Figure 5 (a). An example impulse response of the channel is also plotted in Figure 5 (b), based on the objects present in Figure 5 (a). Note that the number of multipath components could be different in general.\nExample channel environment with example impulse response\nIn this scenario, assuming two UAS with relative speed of Δv maneuvering and communicating at frequency f0 and based on (7) yields a Doppler bandwidth of Δf. Therefore in our channel sounding we require a chirp with WRF value larger than (2fdmax) where fdmax is the maximum detectable Doppler frequency shift.\nIn (7) c is the speed of electromagnetic wave in the air, and θ is the angle between propagation and relative velocity vectors. As an example, for f0=2.42 GHz and Δv=30 miles/hr (13.4 m/s) the maximum Doppler shift is Δf≅108.1 Hz. Thus, based on previous criteria, the chirp signal WRF should be at least 216.1 Hz. However, to allow a margin for additional shifts due to receiver oscillator frequency offsets, our Doppler shift range is set to 250 Hz. Considering a chirp signal with bandwidth B=25 MHz, the minimum nominal time delay resolution will be 40 ns, which corresponds to a 12 m minimum distance difference between line-of-sight (LOS) and multipath components in order to be resolvable in our channel sounder power delay profile (PDP). Considering all the criteria described in the example channel scenario, we generated a chirp FMCW waveform with parameters defined in Table 1.\nProposed FMCW waveform parameters\nAA communication between two moving VLL entities can be described as via a strong LOS path signal and signals that arrive at the receiver by several different paths due to reflections from obstacles in the environment. This channel can be simulated as a frequency selective fading Ricean stochastic model in a pre-defined radio channel propagation scenario described in Table 2.\nProposed channel parameters\nOur simulated channel model is adapted from , where an improved sum-of-sinusoids (SOS) based model was proposed for the accurate simulation of time-correlated and frequency selective Ricean fading channels. Filter-based models are usually based on passing a Gaussian process through a linear filter with a transfer function equal to the square root of the Doppler power spectral density (PSD), but this method can have higher computational complexity . Both models are well established. We add additive white Gaussian noise (AWGN) with noise floor relative amplitude of −100 dB.\nFor matched filter simulations, we generated the chirp signal according to Table 1 parameters. We weighted the signal by a Taylor window function, then input it to the SOS based frequency selective fading model. Then the output of the channel simulator was convolved with a time-reversed version of the transmitted chirp signal, as described in subsection II.A. In Figure 6 (a) we show a time compressed version view of all PDPs. The MPC delay values are more distinguishable in the “top view” of Figure 6 (b). As described in Table 2, four multipath components were present, at delays of 30, 140, 290 and 330 ns. The first component with delay of 40 ns was not resolvable, and hence was combined with the LOS component.\nTo obtain the delay Doppler spectrum and time variability of the simulated channel, analysis over a number of sweep times is performed, which is possible by taking the Fourier transform over N sweeps, where the spectrum is also in the form of a sinc function with frequencies centered at the Doppler shift frequencies. The corresponding delay Doppler spectrum is depicted in Figure 6 (c).\nMatched filter results: (a) PDP, (b) PDP top view, and (c) Delay doppler spectrum\nFor heterodyne simulations, spectrum analysis applied to the output of the detector was employed, with the low pass filter cut off frequency set to 2.5 kHz, corresponding to 400 ns as described in Section II.B. As expected, results from the heterodyne detector in Figure 7 show more distinguishable multipath components, due to the structure of the heterodyne detector, while the Doppler spectrum conveys essentially the same information as from the matched filter detector.\nHeterodyne detector results: (a) PDP, (b) PDP top view, and (c) Delay doppler spectrum\nFor proof of design and concept, we conducted a simple outdoor measurement. The chirp measurement setup parameters are based on those listed in Table 1. A block diagram for the sounding measurement is presented in Figure 8. The selected SDR model for this measurement was the Ettus N210 with the UBX40 daughterboard and a 5 dB gain omni-directional antenna.\nBlock diagram of SDR sounder experiment\nThe complex chirp signal samples were generated in MATLAB® and fed as I/Q complex data to the transmitter that up-converts the signal to the 2.43 GHz radio frequency. At the receiver, the distorted signal is captured by the second USRP. Then the data is input to the SDR heterodyne detector designed in GNURadio (an open source toolkit software for SDRs). The matched filter detector requires synchronization for precise sweep length match, therefore in our measurements we employed the heterodyne detector.\nAs illustrated in Figure 9, our transmitter was mounted on top of the University of South Carolina Swearingen Engineering Building to emulate a low altitude UAS hovering situation. This transmitter sent its signal with a downtilted omni-directional antenna so that its main lobe was projected on the street with the receiver. The other SDR, as receiver, was mounted on top of a 2007 Mazda 6 car roof.\nThe car speed increased to approximately 30 miles per hour and stayed constant over the path depicted with the red line in Figure 9, and then reduced its speed stop at the end of the street (lower left).\nNote that in our USRP the total number of samples to be stored using the heterodyne detector is 2Bτmax where τmax is the maximum expected time delay. This value is significantly lower than the 2BT of the matched filter. An example measured PDP from a single sweep is depicted in Figure 10 where a significant LOS component arrived along with other smaller magnitude multipath components. Figure 11 (a) (b) shows measured PDPs during the measurement. As we expected, most of the dominant multipath components had delays close to that of the main LOS component (and were hence unresolvable), however due to the movement of the RX we expect Doppler shifts and possible receiver oscillator frequency offset. Figure 11 (c) presents the measured delay Doppler spectrum result for this test. The maximum value of just over 100 Hz is clear, along with a range of smaller values corresponding to the cosθ term scaling of (7).\nMeasurement results showing (a) PDPs over time, (b) PDPs in “contour” view, and (c) Delays vs. doppler shifts\nWe discussed an implementation of FMCW (chirp) sounders using SDRs for VLL UAS. A motivation for using commercial SDRs was described, along with proper choice of waveform and parameters. We reviewed two types of chirp detectors and discussed their relative merits. Experimental results were provided for an emulated UAS experiment, illustrating the ability of the SDR chirp sounder to yield estimates of channel power delay profiles and scattering functions (delay-Doppler functions). In the near future, state of the art technology plus proper design of algorithms can make SDRs very high-resolution sounders. For future work, we plan to use SDRs mounted on moving objects such as small VLL UAS to gather additional data and refine our processing techniques for channel measurements and modeling."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:44dd2e86-f2f9-4aba-9c73-43dc82e764cc>","<urn:uuid:cb11455b-e0de-4abd-9420-f9050f282657>"],"error":null}
{"question":"How has the legal status of hemp cultivation evolved from World War II to present day in the United States?","answer":"Hemp was grown commercially in the United States through World War II but became regulated with marijuana in the 1950s and 1960s, prohibiting its cultivation. The 2014 federal Farm Bill first authorized hemp growth for agricultural research. Later, the 2018 Farm Bill went further by legalizing commercial hemp production and removing it from the Controlled Substance Act definition of marijuana, provided it contains no more than 0.3% THC. The USDA now regulates hemp instead of the DEA, establishing a shared federal and state regulatory authority over hemp cultivation.","context":["Agriculture Secretary Russell Redding highlighted nearly $1 million in Wolf Administration investments for the commonwealth’s hemp industry as he opened the 2022 Pennsylvania Hemp Summit in Lancaster today.\n“Industrial hemp is an economic driver delivering innovative approaches to everyday challenges,” said Redding. “From building materials to manufactured goods, to conservation and soil remediation, industrial hemp and its byproducts are sparking innovative and competitive solutions while growing opportunities for Pennsylvania farmers and manufacturers,’ he said.\n“The PA Hemp Summit provides a platform for growers, entrepreneurs, and industry experts to share ideas and address challenges while growing the next generation of agricultural entrepreneurs,” he said to nearly 100 hemp growers and entrepreneurs.\nThe 2014 federal Farm Bill authorized the growth of hemp for agricultural research. Gov. Tom Wolf, in collaboration with the General Assembly, signed Act 92 in 2016 to establish the Industrial Hemp Research Pilot Program.\nThe program sought to grow research opportunities to better understand the value and viability of industrial hemp as a Pennsylvania cash crop, authorizing 16 research projects in 15 counties to examine hemp varieties, plant growth, cover cropping, and its potential for animal bedding, feed, and manufactured goods.\nThe Industrial Hemp Research Pilot Program paved the way for Act 46, which provided the Controlled Plant and Noxious Weed Committee the authority to approve industrial hemp production as a controlled plant.\nWith this designation, Pennsylvania became the second state in the nation to submit its state plan for industrial hemp to the United States Department of Agriculture (USDA), officially opening the program to commercial growing operations permitted by the Pennsylvania Department of Agriculture, Redding said.\n“The Wolf Administration has been very intentional in its actions to stand up the state’s hemp program through the Bureau of Plant Industry,” said Fred Strathmeyer, deputy secretary for Plant Industry and Consumer Protection. “The bureau’s oversight coupled with strategic investments through the PA Farm Bill are creating opportunities for farmers, researchers, and processors to succeed.”\nThe 2019 Pennsylvania Farm Bill sought to further support Pennsylvania’s hemp industry by establishing the State Specialty Crop Block Grant program, funding projects enhancing the competitiveness and sustainability of specialty crops not eligible under the federal specialty crop grant program, and those designated as high priority crops in the state, including hemp.\nSince 2019, the State Specialty Crop Block Grant has invested $627,615 in nine hemp projects across seven counties.\nAgricultural research dollars combined with the PA Farm Bill’s complete package of programs has invested more than $864,000 in Pennsylvania’s hemp industry over the last three years, including dollars to support the Pennsylvania Hemp Summit, the Agriculture department said.\n“Team Pennsylvania hosts the Hemp Summit because of the economic opportunity and need for collaboration across public and private sectors to capitalize on the potential for hemp,” said Abby Smith, president and CEO of Team Pennsylvania. “By convening industry stakeholders, we can catalyze partnerships, networks, and idea generation that is necessary to help build this industry in the commonwealth.”\nIndustrial hemp was grown commercially in the United States, and in Pennsylvania, through the World War II era, but became regulated along with marijuana in the 1950s and 1960s, prohibiting its cultivation.\nIndustrial hemp and marijuana are different varieties of the same species of plant. Unlike marijuana, industrial hemp is grown for fiber and seed, and must maintain a concentration of the psychoactive chemical tetrahydrocannabinol, or THC, below the 0.3 percent legal threshold.","Hemp is a commonly known variety of the Cannabis sativa plant which is rich in the CBD phytocannabinoid. Hemp, unlike marijuana, has no psychoactive qualities since it contains minute amounts of THC, the main psychoactive compound in cannabis plants. Hemp became a domesticated crop many centuries ago, as farmers cultivated the plant for medical and nutritional uses. Both \"hemp\" and \"industrial hemp\" are often used interchangeably, the only differences between both terms are the manner and purpose of cultivation of the hemp plant. While many people cultivate hemp for its CBD properties which may be used for medicinal purposes, others grow hemp for its fiber in order to make textiles, biofuels, and plastics. When hemp is cultivated for industrial purposes, it may be considered industrial hemp.\nDue to their physical similarities, marijuana and hemp are frequently mistaken for each other. Hemp and marijuana might seem identical to the untrained eye, but there are significant distinctions. Hemp leaves are slender, while marijuana leaves are often considerably plumper. In addition, marijuana plants sometimes resemble short shrubs, but hemp plants are tall and thin, with most of their leaves growing at the top. That does not imply that marijuana plants cannot grow tall, although the most prominent marijuana plant will still appear shrubbery. Although hemp and marijuana have similar appearances and odors, marijuana's high THC content makes it intoxicate users, whereas hemp is unlikely to cause intoxication in its consumers.\nMany parts of the hemp plant are often used in making nutritional and medicinal products for consumers. These include hemp seeds, flowers, hearts, milk, and oil. Hemp seeds are the nut-like seeds of the hemp plant. They are a nutrient-dense source of zinc, manganese, magnesium, iron, and vitamin E. Hemp flowers contain substantial amounts of cannabinoids such as CBD. They are used in the treatment of depression and anxiety due to their soothing effects on consumers. Smoking hemp flowers also produce quicker results than other ways of consuming hemp buds.\nHemp seeds with the shells removed are known as hemp hearts. Because of its high protein content and heart-healthy lipids, hemp hearts are an excellent source of protein and healthy fats. Hemp milk comes from the seeds of the hemp plant. It is a common alternative to dairy milk and contains healthy fats, high-quality protein, and minerals. Hemp oil, also called hemp seed oil, is made by crushing hemp seeds to make an oil high in omega fatty acids. As a natural pain reliever, the hemp seed oil is often used topically or taken orally. Hemp oil can be used as a massage oil to relieve pain in aching joints\nYes. The first step toward legalizing hemp in the United States and Minnesota began with the 2014 Farm Bill. The Bill permitted hemp to be grown but still considered the plant a Schedule I Controlled Substance and subject to the oversight of the Drug Enforcement Authority (DEA). Section 7606 of the 2014 Farm Bill legalized industrial hemp cultivation for research purposes in the United States, where the growth and cultivation of the plant are legal under state law. The cultivation of hemp was limited to institutions of higher education or state departments of agriculture for agricultural or academic research or under the administration of a state agricultural pilot program.\nIn 2015, the Minnesota legislature enacted legislation creating Chapter 18K in the state's revised statutes. Also called The Industrial Hemp Development Act, Chapter 18K defined hemp as an agricultural crop and authorized the Minnesota Commission of Agriculture to establish a pilot program to allow higher education institutions to grow or cultivate industrial hemp for academic or agricultural research purposes.\nIn 2018, the United States introduced another Farm Bill which went several steps further from the provisions of the 2014 Farm Bill. These steps include the legalization of the commercial production of hemp and the exclusion of the plant from the definition of marijuana under the United States Controlled Substance Act, as long as the crop does not contain more than 0.3% THC.\nWhile the 2018 Farm Bill boosts hemp production potential, it does not provide a framework in which growers may cultivate it as freely as other crops. In place of the DEA, the USDA now has the jurisdiction to regulate hemp and supervise commercial production operations run by state and tribal governments. The 2018 Farm Bill also established a shared federal and state regulatory authority over hemp, specifying the processes states must follow to design hemp regulatory plans and submit them for approval to the United States Secretary of Agriculture.\nFollowing up on the 2018 Farm Bill, the Minnesota legislature amended the definition of hemp in Chapter 18K.02 of the Minnesota Revised Statutes, clarifying that products containing cannabinoids derived from cannabis sativa plants are not controlled substances as long as they contain no more than 0.3% delta-9 THC. The United States Department of Agriculture published the Final Rule, which formed the regulatory framework for all hemp cultivation nationwide, on January 19, 2021. The USDA approved the State of Minnesota Industrial Hemp Plan on May 6, 2021.\nMinnesotans are permitted to grow hemp indoors, provided they have obtained the necessary licenses and have registered their growing locations. Indoor hemp growing covers cultivation carried out in an enclosed area, whether a greenhouse, building, or hoop house. Per Minnesota regulations, hemp growers must register indoor spaces as separate grow locations, even if they are only starting seeds there before transplanting. However, the state prohibits the growing, processing, or storing of hemp inside a residential dwelling unit. Except where otherwise prohibited by local ordinance, hemp may be cultivated in areas zoned for agriculture. In accordance with the 2018 Farm Bill, hemp may be transported across state lines. However, it is recommended that you contact the receiving state to find out if they have any specific rules prohibiting shipping hemp or hemp materials.\nHemp products designated as Generally Recognized as Safe (GRAS), such as hemp seed oil, hemp seed protein powder, and hulled hemp seeds, are legal in Minnesota. These products are derived from hemp seed and contain only trace amounts of THC. In Minnesota, these products may be sold as food or added as food ingredients. Hemp materials from other parts of the hemp plant beside the seed may not be added to food ingredients.\nAlso, HF 3595, signed by Governor Tim Walz in June 2022, permitted the sale and consumption of non-intoxicating cannabinoids such as food, beverages, gummies, hard candies, chocolate, and topicals that do not contain more than 0.3% THC. The Bill permitted Minnesotans to consume edible cannabinoid products containing no more than 5 mg of any THC per serving and 50 mg of any THC per package, regardless of whether the THC is delta-9 or delta-8.\nWhile you can smoke hemp legally in Minnesota, it is recommended that hemp is not smoked while driving or in public places as it may cause you some inconveniences from law enforcement due to its resemblance with marijuana.\nAlthough Minnesota does not permit its municipalities to prohibit hemp cultivation within their jurisdictions, the state allows its municipalities to enact ordinances that determine where hemp may be cultivated within their borders.\nIn order to grow or process hemp in Minnesota, you must obtain the applicable license from the Minnesota Department of Agriculture. The MDA issues the following types of hemp licenses in the state:\nNote that if you are applying for a hemp grower or processor license for the first time, you must consent to a criminal background check by submitting your fingerprints to the MDA. After completing the application, an MDA program staff will send an email to your email address with a blank fingerprint card and information on how to complete the background check. If you have a felony conviction for a controlled substance-related offense in the past ten years, you will be ineligible to receive a hemp grower or processor license from the MDA.\nAn application for a hemp grower or processor license may be completed online through the MDA license application and renewal portal or by mail. To complete an application online:\nTo complete a Minnesota hemp grower or processor license by mail:\nMDA Finance and Budget Division\n625 Robert Street North\nSt. Paul, MN 55155\nMDA Plant Protection\nAttention: Industrial Hemp Pilot Program\n625 Robert Street North\nSaint Paul, MN 55155-2538\nFor more information on obtaining a hemp grower or processor license in Minnesota, contact the IHPP (Industrial Hemp Pilot Program) by calling (651) 201-6123 or email firstname.lastname@example.org.\nA hemp grower license costs $150, while a hemp processor license costs $250. However, each hemp cultivation or processing location will be approved for a fee of $250. Hence, the total fees associated with a Minnesota hemp grower license and a hemp processor license are $400 and $500, respectively. Note that each licensee must have at least one registered location. A hemp license expires on December 31 of the year issued. The renewal fee is the same as the initial application fee. A licensee must reapply before the expiration of the license to maintain participation in the Minnesota Industrial Hemp Program.\nThe Minnesota Department of Agriculture is also authorized to collect fees for the certification of hemp crops through THC testing. There is a $100 fee for each sample for THC testing and a $250 fee for an inspection of a hemp sample beyond the first THC testing. The MDA charges a $50 fee for any license changes made after the initial application period. Changes include amendments to the license holder's name, adding a processor or grower category, and adding or changing processing or grow locations.\nIf you decide to cultivate hemp, you must determine if you plan to grow the plant indoors or outdoors. Your indoor or outdoor grow area will determine the materials or equipment needed to grow the plant. If you intend to grow hemp indoors, you can control the humidity, climate, water, and lighting conditions available in the grow area. If you plan to grow hemp outdoors, you do not have to worry about the climate or lighting the plants will get, as nature provides the crop with these requirements. However, deficient outdoor conditions may lead to unhealthy or poor yields. For instance, an extremely wet season might saturate hemp plants and negatively impact crop yields.\nTo grow hemp plants in Minnesota, consider the following steps:\nAfter the seedlings have matured into small plants, they need less individual care than they did as infantile seedlings. Nevertheless, you should continue to monitor them to ensure that they get sufficient sunshine, water, and heat throughout this phase. This stage of development lasts from 6 to 16 weeks. You should thus prepare to apply pesticides or provide any nutrients, such as nitrogen, that the hemp plant may be deficient in. Between days 90 and 100, the plant will begin to produce flowers with exposed seeds\nHemp may be grown indoors or outdoors; however, no growing may be done within residential units. A hemp grower may cultivate hemp on a rented land provided the landowner has given consent to allow hemp cultivation on the property.\nFor information on pesticides that may be used in cultivating hemp in Minnesota, visit the pesticide overview and specific pesticide information pages of the Minnesota Department of Agriculture (MDA) website. You may also contact the MDA's Pesticide and Fertilizer Management Division at (651) 201-6006.\nHemp flower is legal in Minnesota pursuant to Chapter 18K of the Minnesota Revised Statutes, provided it contains no more than 0.3% THC. You may purchase smokable hemp flower in approved dispensaries and hemp flower shops in the state. Also, smokable hemp flowers are available through online stores. Minnesota does not restrict the amount of hemp flowers that consumers may purchase.\nTHC is a psychoactive cannabinoid found in trace amounts in the hemp plant. THC is available in both hemp and marijuana but is more commonly found in marijuana. Per the 2014 and 2018 Farm Bill, hemp plants may not contain more than 0.3% THC by dry weight. Hemp-derived THC products may be sold or purchased in Minnesota.\nCBD is one of the over 100 cannabinoids found in the hemp plant. CBD can be found in both hemp and marijuana but is contained in large quantities in the hemp plant. Hemp-derived CBD products may be sold or purchased in Minnesota.\nHemp is a plant known for its various properties and applications. Besides the more commonly known medicinal uses, hemp can also be used in the production of:"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:cb3c938f-a36c-48cc-b397-7bd611aa1df4>","<urn:uuid:e38f810c-5249-40a9-bd12-c09f117fc025>"],"error":null}
{"question":"What were the groundbreaking achievements of Pioneer 10 and Voyager in terms of cosmic observations?","answer":"Pioneer 10 was the first spacecraft to pass through the asteroid belt and visit Jupiter, taking the first close-up pictures of the largest planet in our Solar System. The Voyager spacecraft, meanwhile, have made the first detection of bursts of cosmic ray electrons accelerated by shock waves from solar eruptions in interstellar space, becoming the first craft to record this unique physics between stars.","context":["Physicists report accelerated electrons linked with cosmic rays.\nMore than 40 years since they launched, the Voyager spacecraft are still making discoveries.\nIn a new study, a team of physicists led by the University of Iowa report the first detection of bursts of cosmic ray electrons accelerated by shock waves originating from major eruptions on the sun. The detection, made by instruments onboard both the Voyager 1 and Voyager 2 spacecraft, occurred as the Voyagers continue their journey outward through interstellar space, thus making them the first craft to record this unique physics in the realm between stars.\nThese newly detected electron bursts are like an advanced guard accelerated along magnetic field lines in the interstellar medium; the electrons travel at nearly the speed of light, some 670 times faster than the shock waves that initially propelled them. The bursts were followed by plasma wave oscillations caused by lower-energy electrons arriving at the Voyagers’ instruments days later — and finally, in some cases, the shock wave itself as long as a month after that.\nThe shock waves emanated from coronal mass ejections, expulsions of hot gas and energy that move outward from the sun at about one million miles per hour. Even at those speeds, it takes more than a year for the shock waves to reach the Voyager spacecraft, which have traveled further from the sun (more than 14 billion miles and counting) than any human-made object.\n“What we see here specifically is a certain mechanism whereby when the shock wave first contacts the interstellar magnetic field lines passing through the spacecraft, it reflects and accelerates some of the cosmic ray electrons,” says Don Gurnett, professor emeritus in physics and astronomy at Iowa and the study’s corresponding author. “We have identified through the cosmic ray instruments these are electrons that were reflected and accelerated by interstellar shocks propagating outward from energetic solar events at the sun. That is a new mechanism.”\nThe discovery could help physicists better understand the dynamics underpinning shock waves and cosmic radiation that come from flare stars (which can vary in brightness briefly due to violent activity on their surface) and exploding stars. The physics of such phenomena would be important to consider when sending astronauts on extended lunar or Martian excursions, for instance, during which they would be exposed to concentrations of cosmic rays far exceeding what we experience on Earth.\nThe physicists believe these electrons in the interstellar medium are reflected off of a strengthened magnetic field at the edge of the shock wave and subsequently accelerated by the motion of the shock wave. The reflected electrons then spiral along interstellar magnetic field lines, gaining speed as the distance between them and the shock increases.\nIn a 2014 paper in the journal Astrophysical Letters, physicists J.R. Jokipii and Jozsef Kota described theoretically how ions reflected from shock waves could be accelerated along interstellar magnetic field lines. The current study looks at bursts of electrons detected by the Voyager spacecraft that are thought to be accelerated by a similar process.\n“The idea that shock waves accelerate particles is not new,” Gurnett says. “It all has to do with how it works, the mechanism. And the fact we detected it in a new realm, the interstellar medium, which is much different than in the solar wind where similar processes have been observed. No one has seen it with an interstellar shock wave, in a whole new pristine medium.”\nReference: “A Foreshock Model for Interstellar Shocks of Solar Origin: Voyager 1 and 2 Observations” by D. A. Gurnett, W. S. Kurth, E. C. Stone, A. C. Cummings, B. Heikkila, N. Lal, S. M. Krimigis, R. B. Decker, N. F. Ness and L. F. Burlaga, 3 December 2020, The Astronomical Journal.\nCo-authors include William Kurth from Iowa; Edward Stone and Alan Cummings from the California Institute of Technology; Bryant Heikkila, Nand Lal, and Leonard Burlaga from the NASA Goddard Space Flight Center; Stamatios Krimigis and Robert Decker from the Applied Physics Laboratory at Johns Hopkins University; and Norman Ness from the University of Delaware.\nNASA funded the research.","Artwork showing Pioneer 10 in space.\nClick on image for full size\nImage courtesy NASA\nPioneer 10 falls silent\nNews story originally written on March 7, 2003\nPioneer 10 is a spacecraft that left Earth more than 30 years ago.\nIt was supposed to work for about 2 years, but it lasted a lot longer than that!\nScientists tried to contact Pioneer 10 on February 7, 2003, but it looks like Pioneer 10 has finally run out of energy.\nThe last signal from the spacecraft, which arrived on January 22, was very weak.\nPioneer 10 was the first spacecraft to pass through the asteroid belt.\nIt was also the first spacecraft to visit Jupiter\nand take close-up pictures of the largest planet in our Solar System.\nThe last signal from Pioneer 10 came from more than 12 billion kilometers (7.6 billion miles) away.\nIt is so far away that radio signals from Pioneer 10, which move at the speed of light,\ntook more than 11 hours to reach Earth. The spacecraft is headed out of our Solar System.\nIt is going towards the star Aldebaran\nin the constellation Taurus.\nAldebaran is 68 light-years away.\nIt will take Pioneer 10 more than 2 million years to get there! The spacecraft\nhas a gold plaque on it that tells about Earth.\nIf there are \"aliens\" somewhere \"out there\", they might find out about us from the plaque on Pioneer 10!\nYou might also be interested in:\nWhat types of instructional experiences help K-8 students learn science with understanding? What do science educators teachers, teacher leaders, science specialists, professional development staff, curriculum designers, school administrators need to know to create and support such experiences?...more\nPioneer was the name given to a bunch of spacecraft launched between 1958-1978. These spacecraft studied the Earth, the Moon, Jupiter, Saturn and Venus. The Pioneer program was really interesting! Pioneer...more\nAsteroids are small bodies that are believed to be left over from the beginning of the solar system 4,600 million years ago. They are rocky objects with strange shapes up to several hundred km across,...more\nWhat's in a Name: Arabic for \"Follower\" because it rises after the Pleiades. The Pleiades is a group of 6 stars traveling together through space. The eye of the constellation Taurus, the bull. Claim to...more\nThe constellation Taurus is known as The Bull. It is seen in the sky from November to March. There are many myths about Taurus. The Greeks thought the constellation was Zeus. One day, Zeus turned into...more\nPioneer 10 is a spacecraft that left Earth more than 30 years ago. It was supposed to work for about 2 years, but it lasted a lot longer than that! Scientists tried to contact Pioneer 10 on February 7,...more\nTwo robots are going to Mars. The robots are six-wheeled vehicles that will explore Mars. They are called the Mars Exploration Rovers (MER). Rockets carrying MER blasted off from Florida. MER will study...more\n1998 was a very full year when it came to space exploration and history making. In the blast-from-the-past department, John Glenn received another go for a launch aboard Space Shuttle Discovery. After...more"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:548c23a9-2891-4b52-81ac-16f4b45f25b1>","<urn:uuid:85a42ce7-6a0b-40c1-a24a-adda0820f104>"],"error":null}
{"question":"As a pharmaceutical researcher, I'm curious about C18 columns and temperature effects. What are the key performance characteristics of C18 stationary phases in chromatography, and how does temperature impact their separation capabilities?","answer":"C18 stationary phases are widely used in reversed-phase chromatography due to their hydrophobic properties. They show varied selectivity for different compounds - for example, C18 columns demonstrate enhanced separation of structural isomer PAHs compared to phenyl-type columns. Regarding temperature effects, retention typically varies by 1-3% for every 1°C change, and temperature can also impact selectivity. It's recommended to operate the column oven just above room temperature (around 35°C) to control these variations and reduce solvent viscosity for lower-pressure operation. Column stability is also temperature-dependent - most C18 columns maintain stability down to pH 2, but high temperatures can accelerate deterioration, especially under basic conditions.","context":["Reversed-phase chromatography also called RPC , reverse-phase chromatography , or hydrophobic chromatography includes any chromatographic method that uses a hydrophobic stationary phase. In the s, most liquid chromatography was performed using a solid support stationary phase also called a column containing unmodified silica or alumina resins. This type of technique is now referred to as normal-phase chromatography. Since the stationary phase is hydrophilic in this technique, molecules with hydrophilic properties contained within the mobile phase will have a high affinity for the stationary phase, and therefore will adsorb to the column packing. Hydrophobic molecules experience less of an affinity for the column packing, and will pass through to be eluted and detected first.\n|Published (Last):||21 July 2006|\n|PDF File Size:||9.66 Mb|\n|ePub File Size:||12.78 Mb|\n|Price:||Free* [*Free Regsitration Required]|\nAn assessment of the retention behaviour of polycyclic aromatic hydrocarbons on reversed phase stationary phases : selectivity and retention on C 18 and phenyl-type surfaces. In this manuscript the retention and selectivity of a set of linear and non-linear PAHs were evaluated on five different reversed-phase columns. Overall, the results revealed that the phenyl-type columns offered better separation performance for the linear PAHs, while the separation of the structural isomer PAHs was enhanced on the C 18 columns.\nThe Propyl-phenyl column was found to have the highest molecular-stationary phase interactions, as evidenced by the greatest rate of change in 'S' 0. Interestingly, the Synergi polar-RP column, which also is a phenyl stationary phase behaved more ' C 18 -like' than 'phenyl-like' in many of the tests undertaken. This is probably not unexpected since all five phases were reversed phase.\nA mixed-mode chromatographic stationary phase , C 18 -DTT dithiothreitol silica SiO2 was prepared through \"thiol-ene\" click chemistry.\nThe obtained material was characterized by fourier transform infrared spectroscope, nitrogen adsorption analysis and contact angle analysis. Chromatographic performance of the C 18 -DTT was systemically evaluated by studying the effect of acetonitrile content, pH, buffer concentration of the mobile phase and column temperature. Additionally, the stability and column-to-column reproducibility of the C 18 -DTT material were satisfactory, with relative standard deviations of retention factor of the tested analytes verapamil, fenbufen, guanine, tetrandrine and nicotinic acid in the range of 1.\nFinally, the application of C 18 -DTT column was demonstrated in the separation of non-steroidal anti-inflammatory drugs, aromatic carboxylic acids, alkaloids, nucleo-analytes and polycyclic aromatic hydrocarbons. All rights reserved. Reversed-phase HPLC analysis of levetiracetam in tablets using monolithic and conventional C 18 silica columns. Development and validation of an RP-HPLC method for determination of levetiracetam in pharmaceutical tablets is described.\nThe separation and quantification of levetiracetam and caffeine internal standard were performed using a single analytical procedure with two different types of stationary phases , conventional Phenomenex Gemini C 18 x 4.\nThe analyte peaks were detected at nm using a diode array detector with adequate resolution. Validation studies were performed using the method recommended by the International Conference on Harmonization, the U.\nLevetiracetam and caffeine were detected in about 7 min using the conventional column, whereas less than 5 min was required when the monolithic column was used. Calibration plots had r values close to unity in the range of 0. Assay of levetiracetam in a tablet formulation was demonstrated as an application to real samples.\nEffect of pressure on the selectivity of polymeric C 18 and C30 stationary phases in reversed-phase liquid chromatography. Increased separation of isomeric fatty acid methyl esters, triacylglycerols, and tocopherols at high pressure. With respect to isomeric C 18 FAMEs with one cis-double bond, ODS-P phase was effective for recognizing the position of a double bond among petroselinic methyl 6Z-octadecenoate , oleic methyl 9Z-octadecenoate , and cis-vaccenic methyl 11Z-octadecenoate , especially at high pressure, but the differentiation between oleic and cis-vaccenic was not achieved by C30 phase regardless of the pressure.\nA monomeric C 18 phase InertSustain C 18 was not effective for recognizing the position of the double bond in monounsaturated FAME, while the separation of cis- and trans-isomers was achieved by any of the stationary phases. For some isomer pairs, the ODS-P and C30 provided the opposite elution order, and in each case higher pressure improved the separation. The two stationary phases showed selectivity for the isomers having rigid structures, but only the ODS-P was effective for differentiating the position of a double bond in monounsaturated FAMEs.\nThe results indicate that the improved isomer separation was provided by the increased dispersion interactions between the solute and the binding site of the stationary phase at high pressure.\nC 18 columns for the simultaneous determination of oxytetracycline and its related substances by reversed-phase high performance liquid chromatography and UV detection. Simultaneous determination of oxytetracycline, 4-epioxytetracycline, alpha-apooxytetracycline, tetracycline and beta-apooxytetracycline on C 18 columns has been accomplished using a high performance liquid chromatographic method with UV detection. These columns were equilibrated with mobile phases consisted of methanol-acetonitrile The flow rate was 1.\nBoth methods were applied to oxytetracycline raw material, human and veterinary formulations, where the excipients did not interfere. External standard calibration curves were linear for 4-epioxytetracycline, oxytetracycline, alpha-apooxytetracycline, tetracycline and beta-apooxytetracycline in the concentration range of 0.\nConcerning the first column, limits of detection and quantification of the above compounds were in the concentration ranges of nM and nM, respectively, whereas on the second column these ranges became nM and nM, respectively. A chromatographic estimate of the degree of surface heterogeneity of reversed-phase liquid chromatography packing materials II-Endcapped monomeric C 18 -bonded stationary phase.\nThe effect of the length of the bonded alkyl chain was investigated. PubMed Central. The procyanidins the most common type of proanthocyanidin or condensed tannin from cell suspension cultures derived from cotyledons of Douglas Fir have been compared with those isolated from leaves of strawberry and avocado.\nConsiderable improvement of the separation of isomers was observed on ODS-P phase at high pressure and at low temperature. Complete separation of four pairs of diastereomers was achieved at Higher temperature Only slight resolution was observed for the mixture of diastereomers with the C30 stationary phase Inertsil C30 at A monomeric C 18 stationary phase did not show any separation at Published by Elsevier B.\nExtrathermodynamic interpretation of retention equilibria in reversed-phase liquid chromatography using octadecylsilyl-silica gels bonded to C1 and C 18 ligands of different densities.\nFirst, the four tests proposed by Krug et al. These tests showed that a real EEC of the retention equilibrium originates from substantial physico-chemical effects. The new model indicates how the slope and intercept of the LFER are correlated to the compensation temperatures derived from the EEC analyses and to several parameters characterizing the molecular contributions to the changes in enthalpy and entropy.\nThe estimated K values are in agreement with the corresponding experimental data, demonstrating that our model is useful to explain the variations of K due to changes in the RPLC conditions. Extension of the carotenoid test to superficially porous C 18 bonded phases , aromatic ligand types and new classical C 18 bonded phases.\nThe recent introduction of new stationary phases for liquid chromatography based on superficially porous particles, called core-shell or fused-core, dramatically improved the separation performances through very high efficiency, due mainly to reduced eddy diffusion. However, few studies have evaluated the retention and selectivity of C 18 phases based on such particles, despite some retention order change reported in literature between some of these phases.\nThe carotenoid test has been developed a few years ago in the goal to compare the chromatographic properties of C 18 bonded phases. Based on the analysis of carotenoid pigments by using Supercritical Fluid Chromatography SFC , it allows, with a single analysis, to measure three main properties of reversed phase chromatography stationary phases : hydrophobicity, polar surface activity and shape selectivity.\nPrevious studies showed the effect of the endcapping treatment, the bonding density, the pore size, and the type of bonding monomeric vs. It was applied to ten ODS superficially porous stationary phases , showing varied chromatographic behaviors amongst these phases. As expected, due to the lower specific surface area, these superficially porous phases are less hydrophobic than the fully porous one.\nIn regards of the polar surface activity residual silanols and to the shape selectivity, some of these superficially porous phases display close chromatographic properties Poroshell , Halo C 18 , Ascentis Express, Accucore C 18 , Nucleoshell C 18 on one side and Aeris Wide pore, Aeris peptide and Kinetex XDB on the other side , whereas others, Kinetex C 18 and Halo peptide ES C 18 display more specific ones.\nBesides, they can be compared to classical fully porous phases , in the goal to improve method transfer from fully to superficially porous particles. By the way, the paper also report the extension of. For the first time, we synthesized multiple embedded polar groups EPGs containing linear C 18 organic phases. To correlate the NMR results with temperature-dependent chromatographic studies, standard reference materials SRM b and SRM e , a column selectivity test mixture for liquid chromatography was employed.\nA single EPG containing the C 18 phase was also prepared in a similar manner to be used as a reference column especially for the separation of basic and polar compounds in reversed-phase liquid chromatography RPLC and hydrophilic interaction liquid chromatography HILIC , respectively. Detailed chromatographic characterization of the new phases was performed in terms of their surface coverage, hydrophobic selectivity, shape selectivity, hydrogen bonding capacity, and ion-exchange capacity at pH 2.\nFurthermore, peak shapes for the basic analytes propranolol and amitriptyline were studied as a function of the number of EPGs on the C 18 phases in the RPLC. Evaluation of the phase ratio for three C 18 high performance liquid chromatographic columns. In addition, the boundary depends not only on the nature of the stationary phase , but also on the composition of the mobile phase.\nThe results obtained in the study provide further support for the new procedure for the evaluation of phase ratio. Effects of elevated temperature and mobile phase composition on a novel C 18 silica column.\nA novel polydentate C 18 silica column was evaluated at an elevated temperature under acidic, basic, and neutral mobile phase conditions using ACN and methanol as the mobile phase organic modifier. The temperature range was degrees C. The maximum operating temperature of the column was affected by the amount and type of organic modifier used in the mobile phase. Under neutral conditions, the column showed good column thermal stability at temperatures ranging between and degrees C in methanol-water and ACN-water solvent systems.\nAt pH 2 and 3, the column performed well up to about degrees C at two fixed ACN-buffer compositions. Under basic conditions at elevated temperatures, the column material deteriorated more quickly, but still remained stable up to degrees C at pH 9 and 60 degrees C at pH The results of this study indicate that this novel C 18 silica-based column represents a significant advancement in RPLC column technology with enhanced thermal and pH stability when compared to traditional bonded phase silica columns.\nLabeled carbon dioxide C 18 O2 : an indicator gas for phase II in expirograms. Carbon dioxide labeled with 18O C 18 O2 was used as a tracer gas for single-breath measurements in six anesthetized, mechanically ventilated beagle dogs. C 18 O2 is taken up quasi-instantaneously in the gas-exchanging region of the lungs but much less so in the conducting airways. Its use allows a clear separation of phase II in an expirogram even from diseased individuals and excludes the influence of alveolar concentration differences.\nPhase II of a C 18 O2 expirogram mathematically corresponds to the cumulative distribution of bronchial pathways to be traversed completely in the course of exhalation. The derivative of this cumulative distribution with respect to respired volume was submitted to a power moment analysis to characterize volumetric mean position , standard deviation broadness , and skewness asymmetry of phase II.\nPosition is an estimate of dead space volume, whereas broadness and skewness are measures of the range and asymmetry of functional airway pathway lengths. The effects of changing ventilatory patterns and of changes in airway size via carbachol-induced bronchoconstriction were studied. Increasing inspiratory or expiratory flow rates or tidal volume had only minor influence on position and shape of phase II.\nUnder all circumstances, position of phase II by power moment analysis and dead space volume by the Fowler technique agreed closely in our healthy dogs. Overall, power moment analysis provides a more comprehensive view on phase II of single. Preparative separation of the polar part from the rhizomes of Anemarrhena asphodeloides using a hydrophilic C 18 stationary phase. The goal of this study was to develop a method that utilized a hydrophilic C 18 stationary phase in the preparative high performance liquid chromatography to isolate the polar part from the rhizomes of Anemarrhena asphodeloides.\nThe results showed that an initial mobile phase of pure water for the separation could greatly increase the retention and solubility of the polar compounds at the preparative scale. Introducing polar groups on the surface of the hydrophilic C 18 column together with the use of optimized mobile phase compositions improved the column separation selectivity for polar compounds.\nEleven previously undescribed compounds in Anemarrhena asphodeloides were obtained, indicating that the method developed in this study would facilitate the purification and separation of the polar part of traditional Chinese medicines. Pterins are a class of potential cancer biomarkers. New methods involving hydrophilic interaction liquid chromatography HILIC and reversed phase RP high-performance liquid chromatography have been developed for analysis of eight pterin compounds: 6,7-dimethylpterin, pterin, 6-OH-methylpterin, biopterin, isoxanthopterin, neopterin, xanthopterin, and pterincarboxylic acid.\nFinally, LiChrospher C8 RP column with fluorescence detection was selected for further validation of the method. The first phase , bridge ethylene hybrid BEH , is neutral at all pHs whereas the second, charged surface hybrid CSH , contains a protonated ligand at W W pH C 18 at 20mV and by the subsequent decrease of the equilibrium constant of weak adsorption sites C 18 environment and removal of the strong adsorption sites accessible silanols.\nThe experimental data were then successively fitted to an empirical heterogeneous Langmuir-Moreau LM explicit isotherm. A new analytical methodology for the determination of fully methoxylated flavones FMFs in citrus juices is described.\nSignals at wavelengths , , and nm bandwidth 4 nm are simultaneously acquired, stored, plotted, and integrated.\nIntroduction The reversed phase of proteins and peptides in TFA is the standard method of choice for initial method development. Too often no other buffers are ever tried. If the separation of closely related molecules is the goal, the buffer systems listed below should be tried with a single column and solvent gradient before further method development is started. This holds true for analytical and preparative method development. Also, if the final method needs to be validated and subsequently pass FDA inspection and only TFA has been investigated, then the FDA inspector may decide that the validation package will not be acceptable until other buffer systems have been tried.","Starting out with a sound strategy is one of the most important aspects of minimizing chromatographic separation issues. A good method makes it much simpler to maintain operation within parameters and address issues as they appear. Selecting the right tools is very critical to achieve the best results,. By looking at the cause-and-effect diagram given below one can understand the factors that influence chromatographic separation.\nBasically the factors falls into two major categories namely column conditions (dimensions) and chemical factors (column chemistry, mobile phase components, additives, and etc.). Obviously they become the tools available in the HPLC MD kit that can be put to use in method development process\nColumn selection is one of the most crucial decisions in method development. The question is where do you begin when there are hundreds of columns available to chromatographers?. For instance, C8 and CI 8 phases are the most widely used reversed-phase columns in today’s LC separation processes. These two phases are so well-liked because they typically achieve the best stationary phase and the desired separation. The C8 and CJ8 phases are not sufficiently different from one another to strongly advise one over the other. In practice C18 is preferred over C8, but you can make your call on it. Reversed-phase separation will be effective for the majority of samples that the average chromatographer encounters. However, some compound types may need a different LC method. The sample and methods are presented in the Table below and is only a recommended starting points.\nColumn characteristics (dimension and packing particle size)\nThe column dimensions and packing particle diameter are another crucial decision (dp). Although 3.0 and 3.5-m particles are also widely used, 5-m dP particles are the most common size. Any of these particles are suitable for beginners, but due to their lengthy history of use, most chromatographers still favor the 5-m particles. Columns made of smaller particles have higher plate numbers but also generate higher back pressure. 150 mm X 4.6 mm column size is preferred since it produces a high enough plate number (N) with 5-m particles to achieve adequate separations with the majority of samples. The ability to set the flow rate at 1.5–2.0 ml/min in conjunction with manageable back pressures is an additional advantage of either of these column configurations. Higher flow rates obviously result in shorter run times. A few more plates are produced by columns as long as 250 mm, but this benefit is marginal (N only increases with the square root of length increase), and the drawbacks include longer run times and higher back pressures.\nFor initial screening, some workers recommend 30- or 50-mm column lengths, but these columns frequently fail to produce a high enough plate number for a practical separation. Microbore (less than 1 mm i.d.) or narrow-bore (2-rnm i.d.) columns are additional options. However, these columns require smaller sample volumes, less extra column plumbing, and smaller detector cell size and place unnecessary demands on the LC system during the method development stages. Hence it is preferable to use these short or narrow diameter columns during the fine-tuning phase of method development. In other words, as the workhorse for developing methods, best choice could be to use 150 mm x 4.6 mm, 5-m dp column. Normally, a mobile phase flow rate of 1.5 mL/min is advisable when running these columns.\nAnother important factor that has profound influence on chromatographic separation is mobile phase components, the organic and aqueous phase.\nThe choice of the organic solvent is another factor that might affect how well the separation goes. Three options are available with reversed-phase separations: methanol (MeOH), acetonitrile (ACN), and tetrahydrofuran (THF). Each solvent offers a distinct advantage in terms of selectivity, but chromatographers rarely know which solvent will be the best option based on this factor. Thus, you must base your decision on other factors when selecting the starting solvent.\nMajority of the work involves analyzing pharmaceutical compounds. Due to the extremely low UV absorbance of many of these samples, analysts frequently find themselves using detector settings of 220 run or less. Tetrahydrofuran has a high background absorbance, making it less useful below 240 nm. Gradients containing methanol typically drift off scale at wavelengths shorter than 220 nm, despite the fact that low concentrations of methanol can be used at low wave lengths. Further, You need a solvent that won’t react with the atmosphere or the samples. When using tetrahydrofuran, workers must exercise caution because it can break down and produce peroxides. According to some researchers, diluting tetrahydrofuran with water significantly reduces this issue. Tetrahydrofuran also takes much longer than acetonitrile or methanol to equilibrate with the column and LC system; initial equilibration with tetrahydrofuran may require twice as much solvent volume. Of course, the noxious smell of tetrahydrofuran is another drawback.\nOne more advantageous characteristic of acetonitrile and methanol is their lower back pressure when compared to tetrahydrofuran at flow rates of 1-2 ml/min. Acetonitrile is the first choice for an organic solvent that has the various desirable solvent properties, such as low viscosity, low UV absorbance, low reactivity, and convenience. Methanol is the alternate choice of solvent to start with.\nIf the samples are neutral compounds, using water as the aqueous phase might be possible. Ionic compounds are, however, frequently used in pharmaceutical analysis and numerous other applications. To obtain reproducible methods, analysts must maintain pH control when working with ionic compounds. Fully protonated or fully ionized compounds will yield the best results. Therefore, the mobile phase’s pH should, if at all possible, be at least 1.5 units above or below the pKa. Working at pH values lower than 3 is typically satisfactory for organic acids. But bases will need buffers with pH values greater than 8, which exceeds the working pH of the majority of silica-based columns. It is advisable to use organic buffers to prevent silica from dissolving. Hence it is it is important not to employ high-pH mobile phases with silica columns on a regular basis.\nAccording to sample properties and pH stability of the column, a low pH is more practical than a high pH for routine operation. Most C8 and Cl8 columns maintain their stability down to a pH of about 2. Most acidic samples will be protonated under these circumstances, bases will be fully ionized, and surface silanol ionization will be at a minimum. 25 mM phosphate buffer at pH 2.5 will work well as the starting aqueous phase for many common procedures. You must use a volatile buffer if the method is meant to be used with a mass spectrometer. For many LC-mass spectrometry applications, 0.1% trifluoracetic acid or formic acid will provide adequate pH control, despite not being as useful as true buffers.\nOther variables – Temperature, Additives\nBefore beginning to develop a new LC method, there are a few other things you should take into account. Controlling the column temperature is crucial because retention varies by 1-3% for every 1°C change in temperature. Selectivity can also be impacted by temperature changes, which should provide you with yet another reason to keep the temperature under control. In order to control the temperature and lower the solvent viscosity for lower-pressure operation, it is good to operate the column oven at a temperature that is just above room temperature (for instance, at 35 °C).\nFor particular sample types, you may occasionally need to use extra reagents like triethylamine or ion-pairing reagents. However, it is preferable to start with an additive-free mobile phase unless you are certain that you will need these additives. Follow the principle “Keep It Simple, Straight, and avoid overcomplicating the mobile phase unless absolutely necessary.\n- Select a column that will be available for years to come\n- Use a reputed column vendor/supplier who can consistently deliver controlled column chemistry, in your opinion.\n- Pickup the starting conditions judiciously; Keep the method simple and straight\nThe Table below provides a summary of suggested starting conditions for reversed-phase HPLC method development. The 150 mm X 4.6 mm, 5-m dp Type B silica column is a workhorse, and when used with the majority of samples at low pH and under controlled temperature, it will be a great place to start method development.\nFor a detailed treatment of any of the above concepts consult books/articles suggested in the “Further reading” section.\nDue to the field’s quick development, there may have been some unintentional omissions or inadequate technical specifications. If any omissions or updates needing to be made are added, we would be pleased to include them. To draw attention to it, kindly write a comment. All technical specifications found here are reproduced as-is from the manufacturer’s literature and might have changed at the time of publication. All information given here is for furthering science and cannot be construed as professional advice.\nLloyd R. Snyder, Joseph J. Kirkland, Joseph L. Glajch, Practical HPLC Method Development, pages. 600-615, 1997, John Wiley & Sons, Inc. ISBN 0-471-00703-X\nJohn W. Dolan, LCGC, 1999 17(12), 1094-1097.\nJohn W. Dolan and Lloyd R. Snyder, Troubleshooting LC systems: A comprehensive approach to troubleshooting LC equipment and separations, 1989, Humana Press, Inc., New Jersey. ISBN 0-89603-151-9.\nMontgomery, D. C. Design and Analysis of Experiments, 2019, John Wiley and Sons, New York. ISBN: 978-1-119-49244-3\nHenrik Rasmussen (Editor), Satinder Ahuja (Editor), HPLC Method Development for Pharmaceuticals: Volume 8 (Separation Science and Technology), 2007, Academic Press Inc, ISBN-13 : 978-3527331291\n3 thoughts on “Selecting the tools”\nDear sir Tool 1.0 is very informative.\nSir Under what circumstances there is need for the use of ion pairing agents. Does it has any impact in fishbone diagram under other variables.\nThank you sir.\nDear Dr. Vinodhini,\nGlad you find it informative.\nIon-pair chromatography (IP) is a HPLC technique frequently employed for the separation of analytes that contain ionizable or strongly polar groups, which lead these compounds to have a poor retention on hydrophobic columns (viz. C18, C8). TEA, TFA are examples of ion-pair reagents. Any standard book on HPLC method development will provide details on ion-pair chromatography. I would appreciate if you could consult any of them.\nAs far as the Fishbone diagram and ion-pair reagent, you could place them under the special additives category in the Mobile phase component (chemical factors).\nControlling and monitoring of Critical quality attributes related to HPLC method is useful information for all. Thank you sir"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:2a619aba-dba3-4e81-94a9-177e834b5884>","<urn:uuid:65622af9-b23a-492f-a4e8-7aa4d5218e19>"],"error":null}
{"question":"Hi! I'm researching agricultural innovation: what were Francis Drake's historical exploration limits on the West Coast, and how does this exploration of space compare to modern vertical farming techniques?","answer":"Francis Drake's northward exploration limits on the West Coast in 1579 remain controversial, with no hard evidence defining how far north he reached. While some claim he reached Alaska, detailed examination of historical maps and documents doesn't support this - for instance, islands shown on Nicola van Sype's 1583 map that some cite as evidence were actually copied from an earlier 1564 map by Abraham Ortelius. In contrast to this horizontal exploration of space, modern vertical farming represents a revolutionary approach to spatial utilization, expanding production vertically in multilayer indoor systems rather than horizontally. This method allows for precise environmental control and can significantly increase yield per area, though it faces challenges in scaling up beyond community-level implementation and requires high energy inputs and investment costs.","context":["Did Sir Francis Drake Reach Alaska? Don’t Believe the Mapmakers.\nBy Derek Hayes\nI recently heard a lecturer at a local museum argue that the intrepid Elizabethan explorer and buccaneer Sir Francis Drake landed in my local bay—Semiahmoo Bay, now bisected by the U.S.-Canada boundary between White Rock, British Columbia, and Blaine, Washington—in 1579. This was not the first such argument I had heard, though all the previous ones were for different locations, all held dear to their proponents’ hearts.\nThe issue with Sir Francis Drake is how far he made it north along the West Coast toward Alaska. There is plenty of evidence that he was on the West Coast and plundered a Spanish galleon off Central America. A number of maps depict his voyage of 1577–80, the first circumnavigation of the world by an Englishman, and the first in which the ship’s captain also made it around the world. (Magellan’s ship the Vittoria was the first circumnavigation, in 1519–22, but Magellan died in the Philippines and the ship arrived home captained by Juan Sebastián del Cano.) There seems little doubt that Drake sailed north looking for a western entrance to the Northwest Passage—and, laden with plunder, a quick way home.\nBut Drake’s ship—first the Pelican and then the Golden Hind (one of the few that had its name changed while at sea, that usually being considered bad luck)—developed leaks and had to be beached for repairs. As was common in those days, the caulking between the boards of his wooden hull was beginning to fail. One map, an inset on a world map published by the Dutch mapmaker Joducus Hondius in 1595, depicts the harbor where Drake careened his ship—Portus Novæ Albionis, or Port of New England, the name he had bestowed on the entire west coast of America (see map 1).\nIn addition to the inset map, Hondius’s world map shows the track of Drake’s ship apparently erased or altered halfway up the West Coast, a feature shown on a few other maps drawn at the time as well (map 2). And here lies the controversy. The theory is that Queen Elizabeth wanted to keep the Spanish spies at her court in the dark, to lead them to believe that Drake had indeed discovered the Northwest Passage (a belief that was reinforced by the fact that Drake, by pure luck, had had an unusually speedy passage back to England). Despite the fact that entire books have been written supporting the idea that he reached Alaskan waters, there is actually no hard evidence that defines how far north Drake reached, looking for the passage, before he gave up and decided to turn back.\nThe map of Drake’s harbor has spawned many theories as to its location. The best supported by the available evidence at this time is Drake’s Bay, on the south side of Point Reyes, just north of San Francisco. Others include San Francisco Bay itself; Whale Cove, on the Oregon Coast; and Boundary Bay and Semiahmoo Bay in British Columbia. All the theories have one thread in common: They equate a similarity between today’s coastline and that shown on the map as supporting the case for their location. Some of these are quite believable, with explanations available for almost every feature shown on the map—an island that could become a peninsula at high tide, for example.\nThe question is how far north Drake managed to sail in 1579.\nBut there are many inherent dangers in this approach. Mapmakers of the day were faced with many blanks on their maps, regions they had to fill to make their public believe they had something worth buying: new information. There are hundreds if not thousands of examples of maps where features, especially coastlines, have been drawn from imagination, only for later generations to find they actually look like those on a modern map. This is what makes these theories so attractive: Could such similarities be mere coincidence? But further documentary or archeological evidence is needed before we can equate the theory with fact.\nOne book—which is rumored, perhaps not coincidentally, to have made its author a great deal of money—claims that Drake reached Alaska. The author points to four islands shown on a map drawn by Nicola van Sype about 1583 (map 3), concluding that they represent Prince of Wales Island, in the Alaska Panhandle; the Queen Charlotte Islands (as one island); Vancouver Island; and Washington’s Olympic Peninsula, somehow perceived from the sea to be an island. The presence of these islands, the author argues, proves that Drake must have reached at least as far north as the Alaska Panhandle. However, the author failed to notice that these same islands, at exactly the same latitude, actually appeared on a world map published nearly 20 years earlier, in 1564, by the famous Dutch mapmaker Abraham Ortelius (map 4), and thus could not possibly have derived from Drake’s voyage in 1579. They are in fact merely a copy of one mapmaker’s work by another, something that happened all the time in the days before copyright.\nDetailed examination of another piece of evidence advanced in this book led me to become even more skeptical. In trying to prove that the English twice altered a narrative account of the latitude reached by Drake, the author used the results of a “spectral analysis” carried out at his request by the British Library, where the document resides. But the author used only the evidence from the British Library report that suited his thesis; he omitted the report’s conclusion that all the changes had been made before the ink dried, hardly time for the series of nefarious staged alterations the author describes.\nThis is just one case—one where fortuitously I was able to actually document the process used to come up with conclusions. It underlines the necessity of viewing with a healthy skepticism any conclusion not clearly based on scientifically acceptable evidence. Historical hypothesizing can be great fun, but it should not be confused with proper research.","Agricultural systems around the world need to adapt to the rapidly changing environmental, demographic, and socioeconomic landscapes, and new alternative practices, such as vertical agriculture, may offer new opportunities to accelerate such adaptation.\nNext Gen Farming Without Soil and 90% Less Water | GRATEFUL\nWhat is vertical farming and why is it important\nModern agricultural systems encompass an estimated 1.5 billion hectares of the world’s surface area. With a growing population and resource needs, the availability of arable land is shrinking rapidly.\nSince the agricultural revolution, conventional agriculture has focused on practices requiring considerable quantities of space, water, fertilizer, and pesticides. The past 50 years have seen an accelerating rate of increase in these requirements as modern food production aims to increase productivity in the hopes of addressing growing food insecurity.\nLooking into the future, yield production is forecasted to decrease due to widespread environmental and socioeconomic changes that will generate unpredictable consequences on food systems.\nIn response, many strategies have been developed as alternatives to conventional agricultural practices. These strategies have focused on key principles and their combination to be effective: require less space, less water, and increase yield per unit of area. Moreover, due to the negative effects of agrichemicals, modern practices have also aimed to use significantly less to avoid potentially adverse effects for humans and animals.\nOne such alternative is the development of vertical agriculture, also referred to as vertical farming. As the name implies, vertical agriculture relies on expanding production vertically and not horizontally. Vertical agriculture is a multilayer indoor plant production system that allows for precise control of growth factors, such as light, temperature, humidity, carbon dioxide concentration, water, and nutrients.\nThis allows for the growing and production of crops year-round, completely independent of solar light and other external conditions. Indeed, vertical agriculture makes use of key concepts within ecology and physiology to optimize growing and fertilization within controlled conditions. For instance, elements of photobiology, thermomorphogenesis, hydroponics, and genetic breeding, are all used commonly across systems of vertical agriculture.\nBenefits, challenges, and disadvantages moving from horizontal to vertical farming\nAs a result of tight control over crop breeding, growing, and harvesting, vertical farming provides several benefits relative to conventional methods of ‘horizontal’ food production. This was the topic of a literature review by Kalantari et al. published in 2016 in the Journal of Landscape Ecology.\nFrom a systems perspective, the enclosed design prevents pests and diseases from entering by the adoption of a high level of hygiene, continuous monitoring, and non-chemical disinfection, providing security from crops. Moreover, recent technology has also allowed for automated control over environmental conditions by using sensors and imaging techniques in combination with crop simulation models and artificial intelligence, limiting the need for physical labor.\nVertical farming also allows for flexible organization, with designs ranging from large vertical walls covered with crops to large hangars or re-used shipping containers that can be transported. Consequently, vertical agricultural systems, can comprise many varying sizes and be located within many different areas from the middle of highly urbanized cities to more suburban or rural areas.\nMoreover, the verticality element of this system also provides nutrient and water flow, helping to reuse costly resources. The reduction in space also means there is a significant increase in yield per area, holding extensive potential for a future world of urbanization.\nFrom an economic perspective, vertical farming also provides for more jobs in localized areas and is community-focused by addressing the needs of immediate areas, which in turn can provide food at a lower price. Finally, the optionality of location for vertical systems also allows producers to reduce transport costs, as consumers may access them within urban areas, or transport can be minimized to nearby areas.\nHowever, despite the design, environmental, and economic advantages, vertical farming also incorporates several issues that remain a challenge to its broader implementation as a system.\nVertical farming has a high energy requirement and needs extensive investment costs to implement and develop successfully. Moreover, indoor issues relating to excessive UV, heat, and ozone-induced plant damage may have unpredictable repercussions for plant growth.\nAdditionally, vertical systems are difficult to adapt to a larger scale. They are costly to build and maintain and have yet to demonstrate the ability to provide food for larger areas than community-scale populations. This would make it difficult to implement in areas at higher risk of food insecurity, such as developing agricultural nations.\nThe lack of empirical research on a broader scale has meant that vertical farming has yet to develop past the concept stage on community levels, as persistent issues make it difficult to break through to a larger scale.\nImage Credit: YEINISM/Shutterstock.com\nGrowing skywards - the implications of vertical farming in a rapidly populating and changing world\nAmong the development of alternative agricultural practices, vertical agriculture provides a promising solution for many of the challenges facing current agricultural policies. However, for vertical systems to be integrated on a larger scale requires further technological progress and economic investment.\nNevertheless, gradually implementing more verticality, or combining it with other practices aiming for more sustainable practices may be promising. For instance, the combination of verticality with other practices such as intercropping may be particularly beneficial for developing more sustainable food systems.\nIncorporating technological progress into vertical systems also holds promise, with automated sensors and machinery able to operate near-independently. Progress in gene editing and plant genome modifications will also allow for faster, bigger, and healthier crops, allowing vertical agriculture to produce more over time.\nThroughout agricultural history, farming systems have typically spread over large spans of land, yet the reduction in arable land, as well as the increase in demand to house growing populations, means that such strategies need to be reconsidered, and vertical agriculture may play a role looking into the future.\nContinue Reading: Benefits of Vertical Agriculture and Hydroponics\n- Beacham, A. M., Vickers, L. H., & Monaghan, J. M. (2019). Vertical farming: a summary of approaches to growing skywards. The Journal of Horticultural Science and Biotechnology, 94(3), 277–283. doi: 10.1080/14620316.2019.1574214\n- Chaudhry A. R. and Mishra V. P.,(2019) A Comparative Analysis of Vertical Agriculture Systems in Residential Apartments, Advances in Science and Engineering Technology International Conferences (ASET), 2019, pp. 1-5, doi: 10.1109/ICASET.2019.8714358\n- Sarkar, A., & Majumder, M. (2015). Opportunities and Challenges in Sustainability of Vertical Eco-Farming: A Review. Journal of Advanced Agricultural Technologies, 2(2). doi: 10.12720/joaat.2.2.98-105\n- SharathKumar, M., Heuvelink, E., & Marcelis, L. F. (2020). Vertical Farming: Moving from Genetic to Environmental Modification. Trends in Plant Science, 25(8), 724–727. doi: 10.1016/j.tplants.2020.05.012"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:4769db98-b5e8-4fe9-8655-5b7aa2f50e37>","<urn:uuid:3566e03d-b8ac-4d36-b13c-9d56f4d7ba0e>"],"error":null}
{"question":"Whether Rabbit Hill Cactus Mix vs Charlie's Compost better for plants drainage?","answer":"Rabbit Hill Cactus Mix and Charlie's Compost serve different purposes for drainage. Rabbit Hill Cactus Mix is specifically designed as a soilless mix that provides very well-draining conditions, making it ideal for cacti, succulents, and plants needing excellent drainage. Charlie's Compost, while effective at improving soil structure and aeration, is primarily focused on enriching soil with nutrients and stimulating plant growth through its mix of hay, clay, straw, manure, and microbes. It's not specifically formulated for drainage like the cactus mix.","context":["What's the Difference?\nWe have a great selection of assorted soils, composts, mulches, and soil amendments. Our knowledgeable staff will be happy to help you select the right product and determine how much you will need. Use our handy soil and mulch calculator to figure out how many bags you’ll need for your project.\nA guide to soils, compost & mulches.\nThe terms can get a bit confusing, so here is a simple reference guide:\n- Soil is the foundation of the garden — the base. It is comprised primarily of sand, silt, and clay particles as well as various trace minerals. There are many types of soil blends that are created for different applications. Garden mix soil makes a great base for raised beds. Potting soil is usually a soilless sterile mixture of raw materials (coir, peat, vermiculite, perlite, etc.) used for starting seeds, and container plantings. Good potting soil should be porous for root aeration and drainage while retaining water and nutrients.\n- Compost is decomposed organic matter (leaves, small tree limbs, food waste, manure) which when added to the soil improves it’s fertility and supports beneficial microbes, insects, fungi and bacteria in the soil.\n- Mulch is similar to compost, but usually just contains decomposed tree and shrub material. It is placed on top of the soil to keep moisture in, weeds from growing and roots cool. It breaks down over time and feeds the soil too.\nBuchanan's All-Purpose Soil\nLocally made by us!\nLocally made, we developed Life Below to make gardening a little easier. It is an organic, all-purpose soil blend designed for all gardening applications. It is specifically formulated as a one bag solution that amends the dense, Houston clay to cultivate a nutrient rich and aerated habitat that supports ALL Life Below.\nMagic Worm Ranch Worm Castings\nWorm castings allow plants to quickly and easily absorb all essential nutrients and trace elements in simple forms. The earthworm’s unique digestive system coats the castings with polysaccharides, providing optimum soil structure for a maximum aeration and water retention. Along with essential nutrients, worm castings also contain an extremely high population of beneficial microorganisms.\nLight Warrior Seed Starter\nLight Warrior is designed to promote an environment beneficial to seed germination and root development. Light Warrior includes soil microbes that can help enhance root efficiency and encourage nutrient uptake. We’ve also included earthworm castings, a gentle source of nutrition for seedlings and small plants. Great for seed germination, cuttings and new plant starts for both indoor and outdoor cultivation.\nFoxfarm Ocean Forest Potting Soil\nThis potting soil is a powerhouse blend of aged forest products, sphagnum peat moss, earthworm castings, bat guano, fish emulsion, and crab meal. Aged forest products, sandy loam, and sphagnum peat moss give Ocean Forest its light, aerated texture. Start with Ocean Forest and watch your plants come alive!\nRabbit Hill Cactus Mix\nThis is a soilless mix containing organic material to provide nutrients, proper conditions of root aeration, and improve drainage. This is soil is ideal for cactus, succulents, and any other indoor or outdoor plants that need a very well-draining soil. Rabbit Hill Cactus mix is available in two sizes!","8 Best Composts for Gardens in 2023 – Reviews & Top Picks\nCompost brings many benefits to a garden. First, it improves the structure of the soil, boosts air circulation, and helps retain moisture. Second, compost supplies the soil with the right elements/nutrients to boost plant growth. More than that, it’s an eco-friendly product: compost reduces the percentage of waste sent to landfills. And you can, of course, make your own pile of compost.\nHowever, that takes time, dedication, and knowledge of what can and can’t be put into the pile. And if you’re reading this, we’re guessing you don’t have the desire to do any of it. Instead, you’re looking to buy bagged compost. We can help you with that! The following list includes in-depth reviews of eight market-leading compost products for your garden. Check out their pros and cons, read our expert recommendations, and take a pick!\nA Quick Comparison of Our Favorites in 2023\n|Best Overall||Charlie’s Compost||\n|Best Value||Wakefield Compost Hero Biochar Blend||\n|Premium Choice||Ribbon Organics OMRI Certified Organic Compost||\n|R&M Organics Premium Organic Compost||\n|Purple Cow Organics Ready to Use Tomato GRO||\nThe 8 Best Composts for Gardens\n1. Charlie’s Compost—Best Overall\nLooking for compost to amend the soil, strengthen its structure and improve aeration? You just found it! Charlie’s Compost is shipped in a simple plastic bag, yet it’s incredibly efficient at enriching the soil with nutrients and stimulating plant growth. Thanks to a “killer” mix of ingredients—hay, clay, straw, manure, and a wide range of microbes—the formula covers more ground than the average 10-pound pile. And that’s one of the factors that make it the best overall compost for your gardens.\nMore good news: you won’t find any rocky or bulky pieces inside the bag. The compost is well-milled and ready to use the very second the package arrives at your doorstep. Lastly, the company backs its products with fast and helpful customer service. You can even call the support team and ask them for guidance. Be very careful when adding this mix to newly-planted vegetables, though! Since the compost is very strong, it might burn their fragile roots and stems.\n- Market-leading efficiency\n- A strong mix of ingredients\n- No clumping, well-milled\n- Fast, helpful customer support\n- May burn newly-planted veggies\n2. Wakefield Compost Hero Biochar Blend—Best Value\nDepending on how big or small your garden is, you might not need tens of pounds of compost. If that’s the case, Wakefield Compost Hero Biochar Blend will be the best purchase for you. First, this compost is quite cheap compared to the market average. Second, this 4.2-pound/1-gallon bag is a mighty combination of compost (80%) and biochar (20%). The charcoal prevents the carbon from escaping the soil, boosting overall growth by endorsing microbe activity.\nOh, and don’t worry: it’s harmless both for the soil and the plants. But that’s not even why Wakefield Hero is the best compost for your gardens for the money. The mycorrhizal fungi that come as part of the blend make the compost a 100% safe product for organic gardens.\n- Highly affordable\n- 80% compost, 20% biochar\n- Rich in mycorrhizal fungi\n- Gives plants a substantial boost\n- Modest bag size\n3. Ribbon Organics OMRI Certified Organic Compost—Premium Choice\nUp next, we have Ribbon Organics OMRI Certified Organic Compost, a company that boasts fast, hassle-free deliveries across the country. As for its compost, it’s an OMRI-certified product, meaning it only contains organic compounds. And, in contrast to most bagged composts, this one goes really well with worm castings. So, if the goal is to improve water retention, soil aeration, and anchor plants, this 35-pound bag will be the right investment.\nWith RO, you’ll get more than enough compost to cover a large garden or dozens of beds. The price tag is a bit steep. But it’s well worth it, especially if you want to mix the compost with various soil builders. Besides, it’s safe to use at any concentration—you don’t have to empty the whole bag!\n- Easily handles large garden beds\n- Designed to improve soil aeration\n- Goes great with worm castings\n- Fast delivery across the States\n- A bit on the expensive side\n4. R&M Organics Premium Organic Compost\nWhile this product is not as cheap as the Wakefield compost, R&M Organics Premium Organic Compost is still an affordable option as a 10-pound compost bag. One of its biggest pros is water retention. If you’re having a problem with the soil not being able to hold on to moisture, this organic material will fix that right up. The compost smells good, too: it has a lovely earthy odor and can serve as an all-purpose fertilizer for your plants.\nUnfortunately, quality control might be an issue. Some buyers complain about the compost being rather old (like 1–2 years old) and lacking in nutrients. Others claim that it can be a bit of a challenge to empty the compost from the bag. Going back to the pros, R&M Organics, a family-owned brand, plants a tree for every single bag sold.\n- Excels at water retention\n- Doesn’t have bad odors\n- Every sold bag plants a tree\n- Minor QC issues\n- A bit tricky to use\n5. Purple Cow Organics Ready to Use Tomato GRO\nThis compost was specifically formulated with tomatoes in mind. For gardeners that have hundreds of tomato seeds already planted, Purple Cow Organics Ready to Use Tomato GRO will amend the soil, keep pests at bay, and increase crop yields. The compost is mixed with rock and sea minerals. They make the tomatoes grow even faster and get bigger. The same goes for peppers, by the way.\nSadly, the efficiency of this all-natural activated compost isn’t nearly as impressive with other plants. You can still use it, but don’t expect the results to be groundbreaking. The delivery times are a con as well: depending on where you live, it may take an extra day or two before the package arrives. But, that’s compensated by the ease of use and a decent size.\n- Perfect for tomatoes/peppers\n- Rich in rock and sea minerals\n- Very easy to apply\n- Not ideal for other plants\n- Slow deliveries\n6. Urban Garden Outfitters Premium Organic Compost\nWhat do you do when you’ve used most of the compost but there’s still some left in the bag? You put the bag away, hoping it doesn’t get in the way. That won’t ever be an issue with Urban Garden Outfitters because it’s packed in a resealable bag. It’s pretty small (only 3 pounds), lightweight, and takes zero effort to seal. And what about the actual compound—is it any good?\nThe compost is mixed with volcanic rock powder. It serves as a generous source of nutrients for plants. However, the overall efficiency is average compared to the competition. Furthermore, while this is a 100% US-made product, customer support is a bit slow and doesn’t always come through.\n- Arrives in a resealable bag\n- Mixed with volcanic rock dust\n- A US-made product\n- Lackluster support\n- Average performance\n7. Michigan Peat 5240 Garden Magic Compost and Manure\nMost compost products do a good job of balancing the acidity levels in the soil. But the Michigan Peat Magic compost takes that to another level. You can add it to a lawn, garden beds, or individual plants, and the compost will always fix the pH levels, aiding your vegetables, flowers, and herbs with growth. Next, the compost is screened and has a uniform consistency.\nIt does have a slight odor, though, simply because it’s mixed with manure. Furthermore, once spread over the soil, it sometimes attracts certain bugs. More than that, you might find some insects or worms inside the package. This only happened to a very small percentage of customers, of course.\n- Lackluster support\n- Average performance\n- Has a mild odor\n- Attracts various insects\n- May contain bugs/worms\n8. Dr. Earth 1–1/2 Cubic Feet All Purpose Compost\nGardeners searching for all-purpose compost should pay extra attention to Dr. Earth All Purpose Compost. While it’s not particularly great at improving aeration, it is 100% natural and hand-crafted. The mix is rich in kelp meal, alfalfa meal, and earthworm castings. The beneficial microbes inside of the compost help both the soil and the plants, especially in harsh climates.\nQuality assurance at Dr. Earth is lacking, though, and some bags include tiny pebbles. Also, the organics aren’t always composted, meaning you’ll have to do some turning to get the product there. But, other than that, this product is ideal for amending shrubs, trees, veggies, and lawns. You can even use it as mulch.\n- Rich in castings + alfalfa and kelp meal\n- Hand-crafted blend, natural\n- Poor aeration capabilities\n- Not always 100% composted\n- Contains pebbles/rocks\nBuyer’s Guide: Choosing the Best Composts for Gardens\nA good compost will help you grow an abundance of your own homegrown vegetables. When looking to buy compost to use in your vegetable garden, there are a few things to keep in mind.\nWhat Is Compost? How Does It Work?\nThis term is used to describe the natural process of decay. Organic matter on the planet goes through a recycling process, turning from its original form into what’s known as compost. The list includes leaves, grass clippings, food scraps, coffee grounds, and even newspapers, to name a few. Ultimately, all you have to do is put organic waste into one big pile and wait for it to decompose.\nTo speed things up, use a rake to turn the pile so that every single bit gets its fair share of worms and moisture. Cutting the waste into smaller parts, adding lots of nitrogen and carbon, and ensuring proper exposure to the sun will make the organics break down even sooner. As the compost reaches the final stages, it gets significantly smaller (up to 50%) and develops an earthy smell.\nWhat Is Compost Made Of?\nWhen microorganisms feed on organic waste, it decomposes within weeks. But for the compost to be of high quality, it needs two chemical elements: nitrogen (N) and carbon (C). You’ll find plenty of nitrogen in vegetables, fruits, animals, freshly-cut grass, and branches. Nitrogen-rich waste is often called “greens” and is a biodegradable material. Next, we have the so-called “browns”—recyclables that are rich in carbon.\nPaper, twigs, and leaves (dead ones) are perfect browns. So, there you have it! Most compost piles around the globe are composed of organics with just the right balance of nitrogen and carbon (25–30 parts of C to every 1 part of N). Again, to ensure proper air circulation, cut the waste into tiny pieces and turn the compost once in a while.\nDifferent Types of Composting: What’s the Difference?\n- Aerobic Decomposition. With this type of composting, we rely on air to speed things up. When you rotate the compost (once in 2–3 days), air flows into the pile. With aerobic decomposition, the more greens you have, the better, as they decay quickly and keep the pile warm. For the best results, don’t forget to water the pile. Otherwise, it will start to smell.\n- Anaerobic Decomposition. You don’t have to do much here: chop some scraps up, stack them on top of each other, and give the organics 6–12 months of “alone time”. Unfortunately, a pile of waste without oxygen or water gets taken over by various bacteria and smells ten times worse than an aerobic pile. More importantly, some of those bacteria carry diseases, while the compost generates methane.\n- Vermicomposting. The best way to turn waste into compost is to introduce worms into the mix. They break it down quickly and efficiently. The worms need oxygen and water to survive, though. On the bright side, this compost won’t have a nasty smell. The number of dangerous bacteria, in turn, will be minimal. Vermicomposting can be done both indoors and outdoors.\nMaking Compost: How Long Does That Take?\nThere are no simple answers here. In one case, you’ll have to wait for a year or two before the process is complete. But in another situation, the compost will be ready in 1–2 months, or even sooner. Have you heard about the Berkeley technique? It’s the fastest way to turn a pile into compost. Here’s how you do it:\n- Start by cutting organic waste into super small pieces\n- Maintain the right carbon-nitrogen ratio (30 C to 1 N)\n- Make sure the moisture levels in the pile don’t go below 50%\n- The pile should be at least 36 inches in length, width, and height\n- As for the temperature, it must be higher than 160 degrees F\n- Once it’s in place, turn the soon-to-be compost every single day\nOn average, this takes 14–18 days.\nThe Biggest Pros of Compost\nCompost does wonders in the hands of a skilled gardener. But what about its benefits for the environment, though? What are the biggest pros of composting apart from feeding the plants and making the soil stronger?\nHere’s a quick look:\n- Less pressure on landfills. Where does all the organic waste go? It goes into the bin and then straight to the closest landfill. More than 30% of all the waste in the US comes from food and garden trash. To burn all that garbage, the government has to invest big bucks into collecting, managing, and decomposing it. So, by using the waste from your own kitchen and yard to make compost, you will be able to aid the economy.\n- Less food is thrown away. In the United States, most households throw away $1.8K worth of food a year (32% of the food they buy). That’s approximately $240 billion thrown into the trash can—literally. Why not turn that waste into a compost pile? Most food scraps are perfect for making healthy, strong, and nutrient-rich compost.\n- Reduced methane pollution. Waste that can’t be recycled heads to landfills. And essentially, a landfill is a giant area where all the waste is left one-on-one with Mother Nature. Over time, it breaks down and decomposes. The bad news is that it leads to high volumes of methane emissions. We’re talking about 50% CO2 and 50% methane. That’s another reason to turn scraps into compost.\n- Healthier, stronger soil. Compost does a great job of keeping the soil healthy, well-aerated, and erosion-free. It’s rich in potassium, nitrogen, and phosphorus, the most important nutrients for any plant out there. So, not only does compost reduce organic waste, but it also stimulates plant growth.\n- Great for water conservation. Did you know that agriculture consumes 70% of freshwater around the globe? That’s right! But, when the soil is mixed with compost, its ability to retain moisture is increased. That, in turn, results in bigger crops and an increased harvest, not to mention less water used on a farm/garden.\nThe Biggest Cons of Compost\nAre there any downsides to making compost for your plants? Yes, there are. And while they aren’t nearly as big as the pros, we still want you to check them out:\n- Messy to work with. If you put a pair of gloves on and some old clothes, this won’t be that big of a problem. Still, organic compost is a bit messy, especially during the decomposition stage. It does have a peculiar smell, too, which can be a big turn-off if this is your first time trying to turn organic waste into compost.\n- Takes time to make. As mentioned, a bunch of organic waste doesn’t turn into compost overnight. This process takes time, patience, and some work on your side. In addition, it takes up space in the garden. That can be a deal breaker if you have a relatively small yard. Compost doesn’t look like a million bucks, either.\n- Attracts destructive bugs. When planning on starting a pile of compost in the garden, keep in mind that it will most likely attract pests and rats. These creatures are naturally drawn to all the organics put into the bin. Besides, the pile is very moist and warm, which, again, attracts all kinds of bugs and animals.\n- Not everything can be composted. Most kitchen scraps and waste from the garden can go into a compost pile. However, certain things like dairy products (milk, butter, and cheese), oils, fats, and meat are NOT suitable for this. If you put any of that stuff in a compost pile, you’ll have a mess on your hands.\nNow that we’re done with the reviews and the Buyer’s Guide, let’s sum up. Charlie’s Compost is the best overall choice. It is rich in nutrients, well-milled, and backed by helpful support. The Wakefield compost, in turn, carries the best value. It is mixed with biochar and mycorrhizal fungi and is quite affordable. And if you want the best compost for your garden, we have just the right pick: Ribbon Organics. It comes in a 35-pound bag, boosts air circulation, and works well with worm castings.\nGood luck and happy gardening!\nFeatured Image Credit: jokevanderleij8. Pixabay\n- 1 A Quick Comparison of Our Favorites in 2023\n- 2 The 8 Best Composts for Gardens\n- 2.1 1. Charlie’s Compost—Best Overall\n- 2.2 2. Wakefield Compost Hero Biochar Blend—Best Value\n- 2.3 3. Ribbon Organics OMRI Certified Organic Compost—Premium Choice\n- 2.4 4. R&M Organics Premium Organic Compost\n- 2.5 5. Purple Cow Organics Ready to Use Tomato GRO\n- 2.6 6. Urban Garden Outfitters Premium Organic Compost\n- 2.7 7. Michigan Peat 5240 Garden Magic Compost and Manure\n- 2.8 8. Dr. Earth 1–1/2 Cubic Feet All Purpose Compost\n- 3 Buyer’s Guide: Choosing the Best Composts for Gardens\n- 4 The Biggest Pros of Compost\n- 5 The Biggest Cons of Compost\n- 6 Conclusion"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:33fbadf6-bfb4-42b6-9174-a25da5b3869d>","<urn:uuid:a7a42b6a-02bc-4421-8c1d-0b5f9634cf62>"],"error":null}
{"question":"What features characterize business environments suitable for graph databases, and what are the technical limitations when implementing them at scale?","answer":"Business environments suited for graph databases are characterized by: connectivity between documented entities, large volumes of potentially connected entities (like e-commerce visitors and products), variety in entity types (like job candidates with different skills), and link attribution (relationships with characteristics like employment duration and salary). However, when implementing at scale, there are technical limitations - while graphs with 10-100 billion edges can be loaded on single powerful machines, distributing graph analysis tasks is challenging. Many graph analysis algorithms cannot be efficiently decomposed into local tasks for distributed execution without requiring many iterations. Even local neighborhood analysis tasks like counting motifs or computing clustering coefficients become inefficient when trying to construct multi-hop neighborhoods through state exchange.","context":["Graph Databases for Analytics (Part 2 of 4): Practical Applications\nIs a graph database the solution to your business problem? In this article we'll explore the common characteristics of practical applications for graph databases.\n- By David Loshin\n- July 26, 2016\nWe live in an ever-more-connected world. Smart mobile devices are ubiquitous, and many industries are implementing a wide array of sensors, controllers, and similar devices across different domains.\nThese devices are continuously broadcasting streams of data that contain important information about connections. Relational database systems can represent these links but can't capture the characteristics of the links, so this explosion of Internet-connected devices is an opportunity for graph-oriented applications.\nAs discussed in Part 1, graph data processing engines can ingest and represent the qualitative characteristics of both the entities and the links among them. The captured information is embedded in the connections between things, not just the characteristics of the things themselves.\nBusiness environments suited to a graph data processing solution share these general features:\n-- Connectivity: First and foremost, the environment involves documenting and understanding connected entities.\n-- Entity volume: There are a large number of entities that can possibly be connected, such as the number of e-commerce website visitors and the products they view.\n-- Entity variety: There are entities with different characteristics, such as individuals with different job skills using a recruiting application.\n-- Link attribution: There are relevant characteristics associated with the connections between entities. For example, a person may have an employment relationship with a company, and that relationship may have a title, a duration, a location, and a salary.\nWithin such environments, graph databases are best used for solving business problems that involve answering questions related to the topology, or shape, of the graph. Some examples include:\n-- Proximity and distance: How close are two entities to each other within the graph?\n-- Centrality: To what extent is an entity \"important\" in a network; for example, who is the most influential person in a social network?\n-- Density: What entities have the least or most connections?\n-- Communication paths: What are the best ways to propagate information between sets of entities?\n-- Similarity and differentiation: How do the characteristics of the relationships expose similarities or differences among the entities? Can clustering be done using network distance metrics?\nUses for Graph Databases\nIf your enterprise collects connected data elements and needs to answer these types of questions, you can probably think of some applications for graph processing. Uses for graph databases include:\nSocial network analytics for marketing and advertising: Identifying influential individuals within a social network helps target your advertising. If you understand the structure of the connections within self-organized communities, you can customize your promotions to maximize responses and increase revenues.\nCybersecurity: The graph paradigm is a good fit for modeling the connections among millions of Internet domains, sites, and servers. You can detect telltale signs of an attempted data breach by analyzing the connectivity patterns (e.g., similar registration details, geographic proximity, or routing paths), transaction patterns (e.g., sources of denial of service attacks or phishing attempts), and links to known malicious sites. Patterns of connectivity can also identify possibly malicious sites as well as behaviors indicative of criminal activity.\nLogistics: A supply chain demonstrates hierarchical relationships that can be graphed, such as items per container, containers per pallet, pallets that fit into trucks, the truck routes between delivery points, and the types and quantities of deliveries made at each location. The links contain relevant attributes such as the distance between origin and delivery or the aggregate weight of the pallets on a truck. You can use graph models to optimize travel time, improve fuel efficiency, and verify that each item was delivered to the right location.\nSmart buildings: Understanding the interoperation of connected sensors and controls (such as thermostats) within a facility is another opportunity for graph processing. Investigating these connections could lead to reducing energy costs, improving air quality, and using predictive models for preemptive maintenance.\nIn this case, the relevant information is associated with the proximity of devices (such as temperature, humidity, and CO2 sensors within a room, or rooms on a building floor). The graph approach is nicely suited to modeling the dynamic relationships embedded within the hierarchies of device connections among the different facilities within a complex.\nIn each of these example cases we see similar features -- each contains a variety of connected things in a context where understanding the connections can lead to business opportunities. In the next article in this series, we will look at some graph analytics and algorithm basics.\nRead Part 3 of the series here.\nAbout the Author\nDavid Loshin is a recognized thought leader in the areas of data quality and governance, master data management, and business intelligence. David is a prolific author regarding BI best practices via the expert channel at BeyeNETWORK and numerous books on BI and data quality. His valuable MDM insights can be found in his book, Master Data Management, which has been endorsed by data management industry leaders.","December 9, 2014\nGraph data management has seen a resurgence in recent years, because of an increasing realization that querying and reasoning about the structure of the interconnections between entities can lead to interesting and deep insights into a variety of phenomena. The application domains where graph or network analytics are regularly applied include social media, finance, communication networks, biological networks, and many others. Despite much work on the topic, graph data management is still a nascent topic with many open questions. At the same time, I feel that the research in the database community is fragmented and somewhat disconnected from application domains, and many important questions are not being investigated in our community. This blog post is an attempt to summarize some of my thoughts on this topic, and what exciting and important research problems I think are still open.\nAt its simplest, graph data management is about managing, querying, and analyzing a set of entities (nodes) and interconnections (edges) between them, both of which may have attributes associated with them. Although much of the research has focused on homogeneous graphs, most real-world graphs are heterogeneous, and the entities and the edges can usually be grouped into a small number of well-defined classes.\nGraph processing tasks can be broadly divided into a few categories. (1) First, we may to want execute standard SQL queries, especially aggregations, by treating the node and edge classes as relations. (2) Second, we may have queries focused on the interconnection structure and its properties; examples include subgraph pattern matching (and variants), keyword proximity search, reachability queries, counting or aggregating over patterns (e.g., triangle/motif counting), grouping nodes based on their interconnection structures, path queries, and others. (3) Third, there is usually a need to execute basic or advanced graph algorithms on the graphs or their subgraphs, e.g., bipartite matching, spanning trees, network flow, shortest paths, traversals, finding cliques or dense subgraphs, graph bisection/partitioning, etc. (4) Fourth, there are “network science” or “graph mining” tasks where the goal is to understand the interconnection network, build predictive models for it, and/or identify interesting events or different types of structures; examples of such tasks include community detection, centrality analysis, influence propagation, ego-centric analysis, modeling evolution over time, link prediction, frequent subgraph mining, and many others [New10]. There is much research still being done on developing new such techniques; however, there is also increasing interest in applying the more mature techniques to very large graphs and doing so in real-time. (5) Finally, many general-purpose machine learning and optimization algorithms (e.g., logistic regression, stochastic gradient descent, ADMM) can be cast as graph processing tasks in appropriately constructed graphs, allowing us to solve problems like topic modeling, recommendations, matrix factorization, etc., on very large inputs [Low12].\nPrior work on graph data management could itself be roughly divided into work on specialized graph databases and on large-scale graph analytics, which have largely evolved separately from each other; the former has considered end-to-end data management issues including storage representations, transactions, and query languages, whereas the latter work has typically focused on processing specific tasks or types of tasks over large volumes of data. I will discuss those separately, focusing on whether we need “new” systems for graph data management and on open problems.\nGraph Databases and Querying\nThe first question I am usually asked when I mention graph databases is whether we really need a separate database system for graphs, or whether relational databases suffice. Personally I believe that graph databases provide a significant value for a large class of applications and will emerge as another vertical.\nIf the goal is to support some simple graph algorithms or graph queries on data that is stored in an RDBMS, then it is often possible to do those using SQL and user-defined functions and aggregations. However, for more complex queries, a specialized graph database engine is likely to be much more user-friendly and likely to provide significant performance advantages. Many of the queries listed above either cannot be mapped to SQL (e.g., flexible subgraph pattern matching, keyword proximity search) or the equivalent SQL is complex and hard to understand or debug. An abstraction layer that converts queries from a graph query language to SQL could address some of these shortcomings, but that will likely only cover a small fraction of the queries mentioned above. More importantly, graph databases provide efficient programmatic access to the graph, allowing one to write arbitrary algorithms against them if needed. Since there is usually a need to execute some graph algorithms or network science tasks in the application domains discussed above, that feature alone makes a graph database very appealing. Most graph data models also support flexible schemas — although an orthogonal issue, new deployments may choose a graph database for that reason.\nWhether a specialized graph database provides significant performance advantages over RDBMSs for the functionality common to both is somewhat less clear. For many graph queries, the equivalent SQL, if one exists, can involve many joins and unions and it is unlikely the RDBMS query optimizer could optimize those queries well (especially given the higher use of self-joins). It may also be difficult to choose among different ways to map a graph query into SQL. Queries that require recursion (e.g., reachability) are difficult to execute in a relational database, but are natural for graph databases. Graph databases can also employ specific optimizations geared towards graph queries and traversals. For example, graph databases typically store all the edges for a node with the node to avoid joins, and such denormalization can significantly help with traversal queries, especially queries that traverse multiple types of edges simultaneously (e.g., for subgraph pattern matching). Replicating node information with neighbors can reduce the number of cache misses and distributed traversals for most graph queries (at the expense of increased storage and update costs). Similarly, min cut-based graph partitioning techniques help in reducing the number of distributed queries or transactions, and similar optimizations can be effective in multi-core environments as well. On the other hand, there is less work on query optimization in graph databases, and for simple queries (especially simple subgraph pattern matching queries), the query optimizer in relational databases may make better decisions than many of today’s graph databases.\nI think exploring such optimizations and understanding the tradeoffs better are rich topics for further research. For example, how the graph is laid out, both in persistent storage and in memory, can have a significant impact on the performance, especially in multi-core environments. We also need to better understand the common access patterns that are induced by different types of queries or tasks, and the impact of different storage representations on the performance of those access patterns. Another key challenge for graph querying is developing a practical query language. There is much theoretical work on this problem [Woo12] and several languages are currently used in practice, including SPARQL, Cypher, Gremlin, and Datalog. Among those, SPARQL and Cypher are based primarily on subgraph pattern matching and can handle a limited set of queries, whereas Gremlin is a somewhat low-level language and may not be easy to optimize. Datalog (used in LogicBlox and Datomic) perhaps strikes the best balance, but is not as user-friendly for graph querying and may need standardization of some of the advanced constructs, especially aggregates.\nUnfortunately I see little work on these problems, and on end-to-end graph databases in general, in our community. There are quite a few graph data management systems being actively built in the industry, including Neo4j, Titan, OrientDB, Datomic, DEX, to name a few, where these issues are being explored. Much of the work in our community, on the other hand, is more narrowly focused on developing search algorithms and indexing techniques for specific types of queries; while that work has resulted in many innovative techniques, for wide applicability and impact, it is also important to understand how those fit into a general-purpose graph data management system.\nLarge-scale Graph Analytics\nUnlike the above scenario, the case of new systems for graph analysis tasks, broadly defined to include graph algorithms and network science and graph mining tasks, is more persuasive. Batch analytics systems like relational data warehouses and MapReduce-based systems are not a good fit for graph analytics as is. From the usability perspective, it is not natural to write graph analysis tasks using those programming frameworks. Some graph programming models (e.g., the vertex-centric programming model [Mal10]) can be supported in a relational database through use of UDFs and UDAs [Jin14]; however it is not clear if the richer programming frameworks (discussed below) can also be efficiently supported. Further, many graph analysis tasks are inherently iterative, with many iterations and very little work per vertex per iteration, and thus the overheads of\nthose systems may start dominating and may be hard to amortize away.\nA more critical question, in my opinion, is whether the popular vertex-centric programming model is really a good model for graph analytics. To briefly recap, in this model, users write vertex-level compute programs, that are then executed iteratively by the framework in either a bulk synchronous fashion or asynchronous fashion using message passing or shared memory. This model is well-suited for some graph processing tasks like computing PageRank or connected components, and also for several distributed machine learning and optimization tasks that can be mapped to message passing algorithms in appropriately constructed graphs [Low12]. Originally introduced in this context in Google’s Pregel system [Mal10], several graph analytics systems are built around this model (e.g., Giraph, Hama, GraphLab, PowerGraph, GRACE, GPS, GraphX).\nHowever, most graph analysis tasks (e.g., the popular modularity optimization algorithm for community detection, betweenness centralities) or graph algorithms (e.g., matching, partitioning) cannot be written using the vertex-centric programming model while permitting efficient execution. The model limits the compute program’s access to a single vertex’s state and so the overall computation needs to be decomposed into smaller local tasks that can be (largely) independently executed; it is not clear how to do this for most of the computations discussed above, without requiring a large number of iterations. Even local neighborhood-centric analysis tasks (e.g., counting motifs, identifying social circles, computing local clustering coefficients) are inefficient to execute using this model; one could execute such a task by constructing multi-hop neighborhoods in each node’s local state by exchanging neighbor lists, but the memory required to hold that state can quickly make it infeasible [Qua14]. I believe these limitations are the main reason why most of the papers about this model focus on a small set of tasks like PageRank, and also why we don’t see broad adoption of this model for graph analysis tasks, unlike the MapReduce framework, which was very quickly and widely adopted.\nSome of the alternative, and more expressive, programming models proposed in recent years include distributed Datalog-based framework used by Socialite [Seo13], data-centric programming models of Ligra [Shu13] and Galois [Ngu13], Green-Marl DSL [Hon12], and NScale framework from our recent work [Qua14]. Unlike the vertex-centric frameworks, however, distributed data-parallel execution is not straightforward for these frameworks, and investigating the trade-offs between expressiveness, ability to parallelize the computations, and ease-of-use remains a crucial challenge. The equivalence between graph analytics and matrix operations [Mat13], and whether that leads to better graph analysis systems, also need to be explored in more depth.\nOn a related note, the need for distributed execution of graph processing tasks is often taken as a given. However, graphs with 10’s to 100’s of billions of edges can be loaded onto a single powerful machine today, depending on the amount of information per node that needs to be processed (Ligra reports experiments on a graph with 12.9 billion edges with 256GB memory [Shu13]; in our recent work, we were able to execute a large number of streaming aggregate queries over a graph with 300 million edges on a 64GB machine [Mon14a]). Given the difficulty in distributing many graph analysis/querying tasks, it may be better to investigate approaches that eliminate the need to execute any single query or task in a distributed fashion (e.g., through aggressive compression, careful encoding of adjacency lists, or careful staging to disk or SSD (a la GraphChi)), while parallelizing independent queries/tasks across different machines.\nOther Open questions\nDespite much work, there are many important and hard problems that remain open in graph data management, in addition to the ones discussed above; more challenges are likely to come up as graph querying and analytics are broadly adopted.\nNeed for a Benchmark: Given the complex tradeoffs, many of the questions discussed above would be hard to answer without some representative workloads and benchmarks, especially because the performance of a system may be quite sensitive to the skew in the degree distribution and the graph diameter. Some of issues, e.g., storage representation, have been studied in depth in the context of RDF triple-stores, but the benchmarks established there appear to focus on scale and do not feature sufficient variety in the queries. A benchmark covering a variety of graph analysis tasks would also help significantly towards evaluating and comparing the expressive power and the performance of different frameworks and systems. Benchmarks would also help reconcile some of the recent conflicting empirical comparisons, and would help shed light on specific design decisions that impact performance significantly.\nTemporal and real-time analytics: Most real-world graphs are highly dynamic in nature and often generate large volumes of data at a very rapid rate. Temporal analytics or querying over historical traces can lead to deeper insights into various phenomena, especially those related to evolution or change. One of the key challenges here is how to store the historical trace compactly while still enabling efficient execution of point queries and global or neighborhood-centric analysis tasks [Khu13]. Key differences from temporal databases, a topic that has seen much work, appear to be the scale of data, focus on distributed and in-memory environments, and the need to support global analysis tasks (which usually require loading entire historical snapshots into memory). Similarly real-time querying and analytics, especially anomaly detection, present several unique challenges not encountered in relational data stream processing [Mon14b].\nGraph extraction: Another interesting, and practically important, question is how to efficiently extract a graph, or a collection of graphs, from non-graph data stores. Most graph analytics systems assume that the graph is provided explicitly. However, in many cases, the graph may have to be constructed by joining and combining information spread across a set of relations or files or key-value stores. A general framework that allows one to specify what graph or graphs need to be constructed for analysis, and how they map to the data stored in the persistent data stores would significantly simplify the end-to-end process of graph analytics. Even if the data is stored in a graph data store, often we only need to load a set of subgraphs of that graph for further analysis [Qua14], and similar framework would be needed to specify what subgraphs are to be extracted.\nThe above list is naturally somewhat skewed towards the problems we are working on in our group at Maryland, and towards developing general-purpose graph data management systems. In addition, there is also much work that needs to be done in the application areas like social media, finance, cybersecurity, etc.; in developing graph analytics techniques that lead to meaningful insights in those domains; in understanding what types of query workloads are typical; and in handling those over large volumes of data.\nBlogger Profile: Amol Deshpande is an Associate Professor in the Department of Computer Science at the University of Maryland with a joint appointment in the University of Maryland Institute for Advanced Computer Studies (UMIACS). He received his Ph.D. from University of California at Berkeley in 2004. His research interests include uncertain data management, graph analytics, adaptive query processing, data streams, and sensor networks.\nCopyright @ 2014, Amol Deshpande, All rights reserved.\nComments are closed"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:47b34771-3254-4c28-9ce3-f672fda963e5>","<urn:uuid:ed74645a-8730-47e2-85ab-2dcb786b2ab9>"],"error":null}
{"question":"¿Cómo se compara el manejo de conflictos en entornos militares versus deportivos?","answer":"In military organizations, conflict management is handled through formal and informal strategies, with high-level officers using specific coping strategies like repair work, catching up, and managing loyalties to deal with organizational demands. In contrast, sports environments handle conflicts through structured dialogue and negotiation, where conflicts are viewed as natural occurrences that should be addressed openly. Sports teams specifically encourage resolution through direct dialogue between involved parties, with coaches acting as mediators when necessary. Both contexts recognize the importance of conflict management, but sports settings tend to emphasize more open communication and collaborative resolution approaches.","context":["Like all repetitive human interaction, even war has been institutionalized and fought according to conventions and norms. Historically, this institutionalization is apparent from the way war has been compared to the duel, first in the 14th century and most famously by Carl von Clausewitz 5 centuries later. This article continues this train of thought and argues that the observed limits of Western “professional orthodoxy” and “strategic vocabulary” can be traced to how war has been institutionalized by the military profession. This offers an alternative explanation to the prevailing views of why the West has struggled in contemporary wars: it is the fundamental mismatch between these professional norms in the West and those held by their opponents that forms the biggest asymmetry in contemporary war. As this asymmetry is unlikely to disappear, these professional norms need to be reconsidered: just like the aristocracy with the duel by the late 19th century, the Western military profession appears stuck in an institution that is increasingly becoming obsolete. Without such reconsideration, the attainment of decision – the central strategic objective in war – and hence victory in future wars will remain uncertain.\nHelga Myrseth, Sigurd William Hystad, Reidar Säfvenbom and Olav Kjellevold Olsen\nWe investigated the development of specific military skills in Norwegian cadets during the three-year military academy training as well as the impact of perfectionism and self-efficacy on the development of these skills. Latent growth-curve models were performed with perfectionism as a time-invariant predictor and with self-efficacy as a time-varying predictor. There were significant increases in the Individual Coping Capacity (ICC) and Cooperation in Difficult Situations (CDS) subscales but not in the Motivation to Achievement (MA) subscale. The initial skill levels were not related to the growth of the skills. Both adaptive and maladaptive perfectionism predicted initial values of ICC and CDS, explaining 5% of the variance in the initial ICC levels and 12% of the variance in the initial CDS levels. Perfectionism variables did not explain the development of the three types of military skills over time. Moreover, self-efficacy significantly predicted ICC at all time points and CDS and MA at all time points except at T3. We therefore concluded that cadets with high adaptive perfectionism scores are likely to have higher initial skill levels and that self-efficacious cadets are expected to show a greater development of military skills during military academy training.\nNumerous societal change processes such as globalization, professionalization and social and technical acceleration have challenged military organizations. The aims of this study were to (1) gain a deeper understanding of coping strategies used by the military leaders at the strategic level to manage everyday organizational demands and (2) relate these strategies to multidisciplinary models of organizational challenges. Owing to an insufficiently developed base of research, an inductive approach was used. Interviews were performed with 23 Swedish brigadier generals and colonels. Five coping strategies were found for handling the negative organizational aspects: repair work, catching up, reproducing, using formal and informal strategies and managing loyalties. The theoretical concepts of narcissistic, anorectic and greedy organizations were used as a framework when interpreting the inductively generated coping strategies. It was suggested that the specific connection found between individual-level coping strategies and theoretically framed organizational challenges is new. The results of this study are discussed theoretically and may be valuable in educational settings when evaluating the working conditions and performance of high-level officers.\nHelga Myrseth, Olav Kjellevold Olsen, Einar Kristian Borud and Leif Åge Strand\nThe aim of the current study was to explore gaming problems in post-deployment veterans and to investigate whether boredom and loneliness can predict levels of gaming problems. The general well-being of veterans post their deployments to war zones is linked to an array of negative health consequences, and veterans may be at risk for developing gaming problems after homecomings. Problems that may be related to engagement in gaming include coping with negative emotions, such as boredom and loneliness, which are often faced by homecoming veterans as well. The sample in this study comprised Afghanistan veterans (N = 246), with a mean age of 37.5 years (standard deviation = 9.6 years), and 8.8% of the veterans showed symptoms indicative of problem gaming. This is not higher than that found in the general adult population in Norway. Logistic regression analyses showed that boredom proneness (lack of internal stimulation) and enhancement motivation were independent significant predictors of gaming problems, after controlling for age, gender, coping motivation, social motivation, anxiety, depression, loneliness, lack of external stimulation, hazardous drinking, and combat exposure. These factors accounted for as much as 65.8% of the variance in gaming problem status. We conclude that veterans who are highly motivated by enhancement motives and score low on lack of internal stimulation may be prone to developing gaming problems.\nA dynamic time-separated lean–agile spare part replenishment system can prove beneficial to the army by being efficient (cost saving) during peace and effective (assured availability) during war. The logistics echelons must have certain attributes in order to implement such a dynamic replenishment system. The purpose of this article is to identify the factors/attributes that are necessary in a spare part replenishment system of vehicles and weapon platforms in order to implement a time-separated lean–agile strategy through a systematic literature review. Furthermore, the article will investigate the impact of these factors/attributes, individually and collectively, on overall system performance. This will enable logistics managers to focus only on the factors that have greater impact on the system. A model explaining the effects of various contributory factors/attributes on the overall logistics system has been developed through a comprehensive literature review, experts’ judgments and inputs from practising logisticians in the military field. The article then models the system using a Bayesian belief network (BBN) on Netica software. After the development of the model using Netica, a sensitivity analysis based on the mutual information criterion is conducted to identify the critical factors that most significantly affect a dynamic lean–agile spare part replenishment system. The study addresses the identified need of applying BBN to model an uncertain and complex military logistics domain.\nMilitary staffs are composed of many smaller teams that are interdependent upon each other for a positive functioning level of the whole staff. Many factors can improve or harm the harmony of the staff. Recently, there has been an increased interest in the soft factors that may affect team performance. The purpose of this study was to examine the relationship between the Big Five personality dimensions, political skill and perceived team performance in a multinational staff training event. The sample included 185 military staff officers (49% response rate). The results indicated that the personality dimension Emotional stability and Political skill had a limited, yet statistically significant, predictive power on team performance. Practical considerations and future research directions are suggested.\nRoger Lien, Kristian Firing, Mons Bendixen and Leif Edward Ottesen Kennair\nThis qualitative study explores the meaning-making process of veterans to address the positive aspects of military service in international operations. Thirteen veterans from a Force Protection Unit in Norway were interviewed about their deployment to Afghanistan. A thematic analysis revealed three main themes reflecting meaningful aspects of the service. “Confirmation of ability” refers to finding meaning by coping with stressful situations and being recognized for it. “Cohesion of peers” refers to finding meaning by belonging to a team and giving mutual support within the team, such as backing up each other and caring. “Significance of effort” refers to finding meaning by seeing their efforts as a contribution, as well as by receiving recognition and gaining status for their efforts. The analysis also revealed accompanying themes of inconsistencies, which in turn activated different coping strategies. The findings have been substantiated through a functional exposition of meaning: purpose, value, efficacy, and self-worth, as advocated by Baumeister (1991), and are discussed in the context of previous research and a theoretical concept of meaning making. Steps for future research are proposed.\nThe aim of this study was to test whether the existing achievement goal orientation instrument could be modified to measure goal endorsement in recreational physical training. The participants were 139 second-year students at the Finnish National Defense University. The orientations were assessed using a modified questionnaire that included four orientations: mastery-intrinsic orientation (focus on learning new things and developing competence), mastery-extrinsic orientation (focus on learning and mastery but with extrinsic criteria such as grades), performance-approach orientation (focus on outperforming others) and performance-avoidance orientation (focus on avoiding judgments of incompetence). Based on the exploratory and confirmatory factor analyses, factor structures were compared. The comparison of psychometric results of different models supported the four-dimensional instrument. The participants mostly strived for personal development of fitness, as well as good results. They also emphasized social comparison to some extent but had very little concerns of failure or appearing inferior.","How to Develop Social Intelligence Through Sports\nFor humans, as social animals, social intelligence is a fundamental aspect of relating to others. We need other people in order to live, grow, and satisfy our physical and emotional needs.\nIntelligence is a global concept that deals with all facets of the human mind, including social and emotional. Every day we’re learning more about how this concept isn’t just for doing mental calculations or solving problems.\nThe area of sports, especially team sports, can provide a great opportunity to develop this type of intelligence. Being part of a team, interacting with your teammates, and living significant experiences together helps us understand emotions and communicate with others.\nFor this reason, we’ll explain how to develop social intelligence within the sports world.\nCreate spaces of emotional expression\nSports can generate strong emotions in people, both positive and negative. For the majority of individuals, sports and teams are an essential part of their identity. Everything that happens within this context can be felt with great intensity.\nTrainers and coaches should aim to create spaces in which athletes can express how they feel. This also means being listened to and supported by their teammates. Expressing our feelings isn’t only therapeutic and liberating, but can also help people to better understand the emotions of other people and learn to regulate them.\nYou have to carefully choose these moments of expression. It’s not appropriate to do it, for example, right after a game when emotions can be really intense. It’s better to let a few days pass so that the players have time to reflect on what happened.\nSolve conflicts through dialogue and negotiation\nConflicts shouldn’t be seen as something negative or something that should be avoided. It’s normal that, after hours of training and competitions, there might surge strong emotions between two people.\nThe best strategy is to teach players to resolve this conflict through dialogue. Acting as if nothing happened isn’t a good option. There should be an opportunity for those involved to resolve the misunderstanding and arrive at an agreement.\nIn the case that the athletes can’t solve the conflict themselves, a coach could intervene as a mediator. The mediator has to listen to both sides and get both parties to arrive at an agreement that satisfies both.\nDevelop affectivity outside of training\nThe relationships between players go beyond the moments of training. The links between them can be so strong that they consider themselves a large family.\nTo develop social intelligence, it’s beneficial to take the affectivity of the players outside the world of training. This means searching for informal moments where players can meet up and share impressions, sensations, or just spend time together as a team.\nThese informal moments allow players to strengthen ties and create a relaxed environment that can be difficult to create during practice. The coaches and other team professionals should also participate in these types of meetups. This can help stimulate the feeling of belonging.\nPotential for cohesion within the team: social intelligence\nCohesion is the glue that keeps the team together. It’s an internal force that sustains the group and permits the interactions among the athletes.\nA team that’s cohesive is a team in which communication is good and fluid. In addition, since there’s a strong link between members, the importance of understanding and listening to each member is greater. In this way, athletes will feel that their teammates are essential for the team’s function.\nThere are many ways to increase the cohesion of a team. Establishing common goals and making sure that everyone is included in making big decisions for the team are ways to increase group identification.\nDeveloping social intelligence can improve performance\nIntelligence deals with basic cognitive abilities such as attention, memory, or the ability to multitask. However, this concept would be incomplete if we didn’t take into consideration other ways of relating to others and interpersonal intelligence.\nIncreasing social intelligence has a positive impact on sports performance. If you make an effort to understand and communicate with others, the group will work together in a more organized, cohesive way.\nFinally, according to a study published by the European Scientific Journal, cohesion is positively related to beliefs of efficacy and group performance. As we can see, emphasizing the social part of teamwork is a safe investment to ensure that it can work better.It might interest you..."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:5f0335ee-b22f-4182-9a01-1c9aeb2b94e7>","<urn:uuid:b034e023-7ddf-4f0c-9a9d-e520518fdf4f>"],"error":null}
{"question":"How do students develop their film production skills in Liverpool Hope's Film, TV, Radio & Media Production program?","answer":"Students develop their film production skills through a progressive learning approach. In the first year, they work in small groups to create short films while learning basic principles of visual storytelling, cinematography, and editing. They then advance to animation projects where they can produce short animated films in various styles including stop-frame, pixilation, cut out, hand drawn or clay animation. As they progress, they move on to more complex projects like creating original television series pilots, which involves pitching, scripting, and budgeting. In their later studies, they produce documentaries based on real-world events and create individual short film projects that can incorporate both live-action and animation techniques.","context":["Film, TV, Radio & Media Production BA\nUCAS Code: W601|Duration: 3|Full Time|Creative Campus\nUCAS Campus Code: L46\nWork placement opportunities|International students can apply|Study Abroad opportunities\nAbout the course\nLiverpool has a long and distinguished history of Film, TV and Radio production with a reputation for pushing the creative envelope on many occasions. This creative innovation continues in Liverpool to this day with its embracing of the latest in digital media production. Film and TV production in particular is of very significant economic importance to the city and its surrounding region.\nLiverpool Hope has enjoyed a very strong tradition in Film Theory and Production; this new programme – which will have a convergent use of creative media production at its heart – will build upon this excellence.\nAreas that you will study will include areas such as: Film, TV, Radio and Media Production (podcasts etc.) - underpinned by innovative approaches to analytical and theoretical enquiry.\nStudy activities might include areas such as the devising of an original film, television series or radio drama – including the pitching, scripting and budgeting stages on to the delivery of the pilot or taster episode, through to the creation of promotional blogs and podcasts. We believe that such a diverse approach to learning is the best way to prepare students to pursue a wide variety of careers in the creative industries, by giving them a well-rounded learning experience underpinned by both creative and critical practice.\n- Study in the most filmed-in city in the country outside London.\n- Our local partners include FACT, one of the leading independent cinema venues in the North West.\nThe course will be delivered by way of Lectures which will present core repertoire themes and historical context. These will be supported by small-group seminars and practical workshop sessions to develop the necessary theoretical and production skills and techniques. These will be underpinned by student-led tutorials.\nAssessment and feedback\nAssessment will be by way of creative projects, presentations and contextual essays. These will grow in complexity as the course progresses.\nThis course will allow you to develop your:\n- Film, tv, radio and mixed-media production skills\n- Hone your analytical skills of a broad variety of film, tv and radio genres\n- Capacity to create an original, film, tv or radio drama episode\n- Techniques for the production of other digital media production\n- Knowledge of the business management of the film, tv, and media production industry\n- Complexities of planning, budgeting, scripting, shooting and producing (pre and post) creative media content whether for film, tv and radio\n- Analysis of film, tv and radio genre from a number of perspectives\n- Relevant techniques for digital creative software programming and coding\n- The historical evolution of Creative Digital Media\n- The business management, marketing and administration of Media Production\n- Convergent nature of creative digital media platforms and how they are allowing for a much more interdependent working environment\nThese areas of study will increase in complexity and challenge as the programme advances.\nIntroduction to Film, Television and Media Theory\nStudents will explore and interrogate the key theories and concepts that underpin film, television and media studies. They will learn how to ‘read’ narrative film and television, and will study foundational elements such as narrative form, cinematography, editing, and music. They will also look at concepts of authorship, genre and stardom, and explore the ways in which film and television represents themes of identity, gender, sexuality, race, and class. They will also explore the impact of digital connectivity and the twenty-first century rise of multimedia convergence culture.\nWorking in small groups, students will develop a short film ‘adapted’ from one of the films or TV shows screened as part of ‘Introduction to Film, Television and Media Theory’. In the process, students will learn how to apply the theoretical knowledge gained in areas of cinematography, editing and other basic principles of visual storytelling to their own practical work.\nAnimation Theory and Practice\nThis topic will involve the integration of theoretical and practical approaches to animation. Students will have the opportunity of producing a short animated film in a variety of different styles, including stop-frame, pixilation, cut out, hand drawn or clay animation. Practical work is supported by theoretical study, with students examining the history, aesthetics and cultural significance of animation in their weekly lectures and tutorials.\nStudents will be introduced to a range of different film movements and stylistic approaches from the early twentieth century to the present day, including German Expressionism, Soviet montage, US silent cinema, the French New Wave, the British New Wave, and other avant-garde movements in international cinema. These will develop students’ knowledge and skill-set in a variety of cinematic techniques, and will feed into their practical filmmaking and media portfolios.\nIntroduction to Photography\nThis topic will explore the basic practical techniques in Photography. There is synergy between practice and theory with photography relating to discussions of lighting, depth of field and shot structure in lectures and tutorials. Working individually to produce an original portfolio, the students will be introduced to camera technique, studio lighting and framing.\nWorking in groups, students will devise an idea for an original television series. Initial weeks are spent pitching, scripting and budgeting the series, while the latter part of the semester comprises production and delivery of a pilot episode. Creative work is informed by lectures and tutorials on the history and theory of television. Here, students would explore key debates surrounding topics such as ‘liveness’, flow, intimacy and ‘quality’, in addition to looking at the advantages and the limitations of television as a storytelling medium.\nStudents will develop their skills as creative practitioners in a variety of media. They will learn how to write and disseminate original creative content online through the production of blogs, podcasts, video essays, and radio content, all of which will be underpinned by relevant theories on digital creativity. Students will also have the opportunity to devise and shoot a TV news bulletin.\nContemporary Film and Television\nStudents will study, in-depth, contemporary developments in film and television. Topics will include contemporary British and Hollywood cinema, the national cinemas of countries such as Iran, New Zealand, and Japan. Students will also explore current tendencies in British and American television, including the trends towards ‘quality’ long-form series, serialisation, and multimedia convergence. In addition, students will have the opportunity to complete a research project on a topic of their choosing, or to write a screenplay.\nThis topic will develop filmmaking skills learned in years one and two, challenging students to produce a short feature based on real-world events/issues. Alongside their weekly practical seminars, students will study the history and theory of documentary in lectures and tutorials. These theory classes feed into practical work by introducing students to key filmmakers and modes of documentary, as well as prompting them to consider the ethical implications of depicting actual people and events.\nShort Film Production\nBuilding on the introductions to live-action filmmaking and animation in the first year, this topic allows students to apply their new skills to an individual project. Here, students produce a short film as part of their practical workshops, and have the chance to work in a variety of media, including live-action and animation. This practical work will be supported by relevant critical and theoretical perspectives on contemporary developments in live-action cinema and animation in the lectures and seminars.\nBuilding upon the areas developed in the Media Production of the previous stage of study students will have the opportunity to produce a portfolio of original work in the area of radio and podcast work. These could cover a broad variety of content from documentary, fiction through to other creative content. Students will gain advanced skills in audio recording, editing and mastering of their work\nStudents will work individually to produce an original portfolio of portrait work. There is a strong integration of practice and theory, with photographic projects responding to critical discussions of Hollywood stardom in the lectures and tutorials. Through photography, students will gain further practical skills in the key areas of lighting, composition and workflow.\n|UCAS Tariff Points||112 UCAS Tariff points must come from a minimum of two A Levels (or equivalent). Additional points can be made up from a range of alternative qualifications|\n|Access to HE||112 Tariff Points|\n|Irish Leaving Certificate||112 Tariff Points from Higher Level qualifications only|\n|Welsh Baccalaureate||This qualification can only be accepted in conjunction with other relevant qualifications|\n|T-Levels||120 Tariff Points / Merit|\n|Subject Requirements||All applicants will be required to attend an audition/interview.|\nInternational entry requirements\n|Specific Country Requirements||Select your country|\n6.0 overall (with reading and writing at 6.0) and no individual score lower than 5.5. We also accept a wide range of International Qualifications. For more information, please visit our English Language Requirements page.\nFilm, TV, Radio and digital media production is of significant economic importance to the City of Liverpool the North West Region and to the whole of the UK. It is also an area of expanding employment regionally and nationally.\nWork Placement Opportunities\nWork placement opportunities will be sought from the many Liverpool-based media production companies that Liverpool Hope University already has good working relationships with.\nThe Service and Leadership Award (SALA) is offered as an extra-curricular programme involving service-based experiences, development of leadership potential and equipping you for a career in a rapidly changing world. It enhances your degree, it is something which is complimentary but different and which has a distinct ‘value-added’ component. Find out more on our Service and Leadership Award page.\nAs part of your degree, you can choose to spend either a semester or a full year of study at one of our partner universities as part of our Study Abroad programme. Find out more on our Study Abroad page.\nThe tuition fees for the 2023/24 academic year are £9,250 for full-time undergraduate courses.\nIf you are a student from the Isle of Man or the Channel Islands, your tuition fees will also be £9,250.\nThe University reserves the right to increase Home and EU Undergraduate and PGCE tuition fees in line with any inflationary or other increase authorised by the Secretary of State for future years of study.\nWe have a range of scholarships to help with the cost of your studies. Visit our scholarships page to find out more.\nInternational tuition fees\nThe International Tuition fees for 2023/24 are £12,500.\nVisit our International fees page for more information.\nThis degree is only available to study as a Single Honours."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:f828ec5a-d7b1-4090-bbec-571370901358>"],"error":null}
{"question":"I've noticed some dust-like deposits on my furniture - how do the waste products differ between powderpost beetles and drywood termites?","answer":"Powderpost beetles leave 'frass,' which is a powdery sawdust-like substance typically found along the edges of fresh holes or settled on adjacent surfaces. In contrast, drywood termites produce distinctly different waste - their fecal pellets have a specific 6-sided, oblong shape and are uniform in size, though their color varies depending on the wood type. Termites dispose of these pellets through 'kickout holes,' while beetle frass is created as they bore through wood. When viewed from a distance, both can appear as dust-like deposits, but their characteristics are quite different.","context":["What do antique frames, ethnographic items, and furniture all have in common? They are made of wood and can become good places for shelter and food sources for insects. The most common culprit for wood infestation is the powderpost beetle.\nSigns of an Infestation\nSearch atop and beneath wooden objects for “frass” – a powdery substance left by burrowing pests. Powderpost beetles bore in wood to lay eggs and - when they hatch – the grubs develop to pupae and then when mature the beetles bore out, leaving exit holes, a crisp hole with a delicate layer of frass along the perimeter.\nExit Holes - Sharp circular holes on the surface\nFrass - a powdery sawdust like substance which is typically along the edges of fresh holes or settled on an adjacent surface nearby.\nInsects - alive or dead on or close to the piece.\nBecause many antiques have historic bore marks from previous infestations, the presence of holes and frass doesn’t necessarily indicate an active infestation. When bore holes and frass are evident, it should be noted that the any movement of the piece may have dislodged historic frass causing it to become visible.\nThe piece should be carefully monitored for potential insect activity and the structural stability of the piece be reviewed. Typically, infestations are most evident on the unfinished sections of a piece of furniture, or frame, commonly the back panels, and undersides of a piece. The damage evident on the exterior of a piece isn’t necessarily the most concerning issue, since the infestation can significantly compromise the interior of the wood support, as well weakening of joints, and even sections of the surface can even potentially detach if the support beneath has been severely compromised.\nWhat to do\nIf you find signs of active insects, immediately quarantine the infested piece from other objects by wrapping and sealing it in a plastic bag. If the infestation is localized within an antique frame, and the painting and its stretcher is not affected, the painting should be removed from the frame and the frame bagged. The painting should also be isolated and monitored as a precautionary measure.\nThere are a number of options to address an active infestation. Each has advantages and disadvantages depending on the piece that is exposed, and the needs of the owner.\nFumigation - Consult a specialist about options and approaches that will not affect the finish. Oftentimes this is the most expedient and cost-effective approach.\nAnoxic Treatment - The artifact is placed in a special air tight bag, and the oxygen is depleted from bag. This approach is preferred and the most passive, while not exposing the piece to chemicals.\nThe treatment takes time to allow to deplete oxygen and takes the most time of the listed options. It is also difficult to treat large pieces, since they need to be contained with an airtight seal.\nNitrogen/Argon Gas-Oxygen is replaced with Nitrogen/Argon Gas either in sealed bag, or controlled chamber. The introduction of gas reduces the time required for treatment and does not leave a residue on the surface. It is more costly than the other options.\nFreeze/Thaw - The piece is carefully wrapped, frozen and then thawed. As a proactive measure, the cycle is sometimes repeated. This treatment is expedient and does not require exposure to chemicals.\nSize might become a limitation. There is also the concern that the freezing and thawing could cause expansion and contraction of the piece leading to potential splits, delamination, etc.\nAfter treatment, and the infestation is addressed, the piece should be cleaned to remove old insect accretions and deposits. Compromised areas should be consolidated and stabilized, to ensure no further loss of the wood support, or finished surface.\nThere are many variables to consider when addressing an infestation. Some approaches may be detrimental to select mediums, or to components of a piece.\nA conservator can help determine the extent of the damage, the best approach as well as stabilize the damage after the infestation is eradicated.","Order: Blattodea (Isoptera)\n“What do you keep spilling on this countertop, Joe? This is the third time I’ve cleaned this up.”\n“I thought you were spilling something. I’ve cleaned it up several times myself. Maybe it is coming from some kind of insect. Let’s send a sample up to Mississippi State and see if they know what it is.”\nAs their name suggests, drywood termites live in dry wood. This means they must be very good at conserving moisture, and they are extremely stingy with the amount of water they lose with their feces. When viewed under magnification, the fecal pellets of drywood termites have a distinct, 6-sided, oblong shape and are relatively uniform in size, though color will vary from light to dark depending on type of wood and other factors. When viewed from normal, eyeball distance, these fecal pellets look like dust or debris, and are often not recognized as a sign of an active termite infestation.\nSo how do these tiny termite turds come to be on Joe and Linda’s nice marble countertop? Drywood termites are fastidious; they don’t want a lot of useless fecal pellets lying around in their galleries. Their solution to their sewage disposal problem is to cut a few strategically placed holes, known as kickout holes, to the outside of the wood in which they are living, carry the pellets to these holes, and push them out. If the kickout hole is only a few inches above the surface on which they fall, the discarded pellets may form a neat little, attention-getting pile. But if the kickout hole is higher up, the falling pellets tend to bounce and scatter, making them easy to mistake for dust or debris.\nDrywood termites are not very common here. Less than one percent of the termite infestations reported in Mississippi involve drywood termites. But when drywood termite infestations occur, it is critical they be properly identified, because the treatments used to control native subterranean termites and non-native Formosan termites will not control drywood termites.\nAt least three species of drywood termites occur in the state: southeastern drywood termites, western drywood termites, and West Indian drywood termites, but all three are rare. Infestations usually occur when some item of infested wood is brought into the building. This could be something like a wooden bowl or statue purchased on an overseas trip; a piece of antique furniture that came from an area where drywood termites are more common; or some type of reclaimed wood, such as a fireplace mantel or antique door.\nControl: Controlling drywood termites can be as easy as discarding and destroying an infested item or as complex and costly as having a building tented and fumigated. The first method is only effective when the infestation is isolated to a single piece of wood or furniture. Having a piece of furniture that is too valuable to destroy chamber fumigated is an intermediately priced option (if you can find a company that does such treatments). But if the infestation is extensive enough that drywood termites are swarming from multiple places in the building, tent fumigation is the only option. Fumigation is effective, but the bill may have five digits on the dollar side of the decimal place. See appropriate sections of the MSU Termite Website for more information.\nBlake Layton, Extension Entomology Specialist, Mississippi State University Extension Service.\nThe information given here is for educational purposes only. Always read and follow current label directions. Specific commercial products are mentioned as examples only and reference to specific products or trade names is made with the understanding that no discrimination is intended to other products that may also be suitable and appropriately labeled.\nMississippi State University is an equal opportunity institution."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:e4c897ef-5b43-49ce-85b8-8f742ca78de9>","<urn:uuid:6d982401-665e-400a-b5d4-72ea4fd532fc>"],"error":null}
{"question":"I'm new to religious studies and would like to understand: Could you list the main goals of prayer in both Jewish and Orthodox Christian traditions, based on their teachings?","answer":"In the Jewish tradition, prayer serves several purposes: it acts as a substitute for Temple sacrifices, serves as a means to forgive sin, and functions as 'the service of the heart.' Prayer involves self-judgment (as indicated by the Hebrew word L'hitpalel) and includes elements of thanksgiving, praise, confession, and intercession. In Orthodox Christian tradition, prayer serves to achieve spiritual rebirth, freedom from demonic thoughts, and maintenance of sincere repentance. According to Orthodox teaching, prayer (especially the Jesus prayer) helps unite scattered thoughts, expels bad thoughts, illumines and heals the mind, fills it with grace, sanctifies it, and ultimately leads to divinization. Both traditions emphasize the importance of devotion and sincerity in prayer - Jews through kavanah (intense concentration and devotion), and Orthodox Christians through achieving simplicity of mind and keeping holy fear.","context":["Interview of Archimandrite Ephraim, head (”Righteous” - ”Dikaios”) of the Skete of Saint Andrew, Karyes, Mount Athos.\nIn Finland in 2006.\nWhat is the role of the spiritual father in the daily lives of present day Orthodox people?\nHe must humbly and responsibly help them in their spiritual rebirth, which happens as they are freed from demonic thoughts. At the same time he helps them to preserve sincere repentance and prayer of Jesus. His burning desire is to bring his spiritual children as saved to the Kingdom of God, so that the words of prophet Isaiah might come true: \"Behold, I and the children whom the Lord hath given me\" (Isaiah 8:18).\nDoes the monastic life still attract people?\nYes, indeed. Like a strong magnet draws to itself pieces of iron, so the grace of the Holy Spirit draws some people to monastic angelic life. There exists no school, where one could learn monastic life like there exists theological faculties and seminaries for learning theology. However, Lord Himself calls these people. On another hand, a person, who has received this calling from Heaven, has in himself a strong desire freely and without coercion, with full consciousness, to become a monk. There is neither proselytism nor propaganda in the monastic life.\nWhat can monasteries and monastics give to the Finnish Orthodox who live in the world and far away from Athos?\nOrthodox monasticism has produced many saints, great saints to our Church. Think long and hard and you will see that most saints of the Orthodox Church – patriarchs, bishops, priests, diacons, monks and nuns – followed the monastic way of life, because the saints of the Church belong mostly to two categories: monks and martyrs.\nSaints are God's people, who were purified and englighened. They prayed with sincerity for all the peoples of the world. They knew Jesus in the Holy Spirit and they were saved.\nFrom the Holy Mountain many great missionary workers came to the world, before and also in later times. They were, for example, Saint Anthony of Kiev, Saint Arseny of Konevits and Saint Cosmas of Aetolia.\nThe monastic life of the Holy Mountain offers to the Finnish believers all of its rich tradition and experience simply and without forgery. These include for example the prayer of the mind, a way to heal the mind and thoughts, double-fold, that is, psycho-somatic fasting and psycho-somatic purity, which is in danger of being lost in these end times. We do not teach that everyone should become a monk to be saved, but that everyone should live the life taught by Christ and his disciples. The Lord of the Church, Jesus Christ, Himself gave us this commandment when He said: \"Neither pray I for these alone, but for them also which shall believe on me through their word; That they all may be one; as thou, Father, art in me, and I in thee, that they also may be one in us.\" (John 17:20-21). He said [...] that all believers should become one in the Holy Spirit. We should keep this union all our lives through repentance, humility, prayer of the mind and Mysteries of the Church and by sincerely loving the Trinitarian God. At the same time the fathers of the Holy Mountain recommend that we all study the writings of the Church fathers and love encyclopedic education and culture.\nTherefore it is necessary for monks and laymen to keep contact both inside the Holy Mountain and in the world. Personal meetings, discussions, sermons and divine Liturgy enhance the devotion of laymen towards our holy Church, expel evil thoughts against monks - thoughts of how monks are misanthropes and separate themselves from the world and are lazy, egoistic and – terrible to even say – even pretenders and immoral!!\nHow can we preserve the simplicity of the mind in this modern day society?\nWith prayer of the mind, by not talking nonsense about our neighbour and loving and respecting them with sincere heart.\nHow can one achieve the simplicity of the mind?\nSo that the mind, which is split up by many passions and thoughts, could become simple, it must start to avoid first of all harmful thoughts, secondly too many thoughts and thirdly even good thoughts. Those that systematically practise the prayer of the heart understand that even these third ones should be pruned away.\nThe Jesus prayer unites slowly the many thoughts of the mind, because the prayer itself contains grace. The name of God, which me remember in our minds, shatters and expels bad thoughts. It illumines, heals, unites, fills with grace, sanctifies and divinizes the mind. This is the experience of the great saints of our Church and those who practise the Jesus prayer. This is what we have been taught and this is what we teach. This knowledge based on experience we put forward to every person so that they can freely consider it, so that all that have a mind can understand the truth.\nHow can we live a more spiritual life?\nChrist said in his Holy Gospel: ”Ask - - ; seek - -; knock - - ” (Matthew 7:7). We must, therefore, show patience and endurance, good will and our whole-hearted love towards Christ so that He might give us more faith. ”Lord, enlighten the darkness of my soul and mind with the help of Your Holy Spirit and guide me to do Your will and not my own. Keep me in Your holy fear that I might live as Orthodox, following Your Holy commandments with the prayers of Your holy Mother and all Your saints.”\nHow should we pray?\nTo learn how to pray we must: firstly, get well acquinted with the psalms of the Prophet-King David, secondly to the prayers of the services of the Church and thridly, to the Jesus Prayer. In addition we can use as a help the prayers written by the great saints of the Church. We cannot, naturally, rule away prayer with our own words, even though such a prayer has its own risks.\nThe psalms of David, which there are 150, contain in a cristalized form the grace of the Holy Spirit, which is why the Church has kept them for 2000 years in Her prayers and services. As we well know, there is not a single service of the Church, which would not include psalms. When psalms are being read or sung, the grace of the Holy Spirit is poured to the listeners. It extinguishes the thirst of the soul but at the same time it makes one endlessly thirsty because man can never have enough of the mercy of God. The mind is also illumined endlessly. Divine thoughts occupy it and help it to defeat the devil as Christ defeated it in the Mountain of temptations. That Christ Himself used there the words of the Holy Scripture from Deuteronomium and the psalms of David when saying: ”It is written, - - - ” (Matthew 4:4, 7, 10) teaches us also to use these when we are fighting with our enemy to totally conquer it.\nA believer must also know well the services of the Church. He must have the books for services in his home and he must study them. He must, for example, study the Compline, Akathist, Midnight Office, Hours and Vespers. When one knows how the services proceed, one can follow them in tranquility and read from heart Minea, Triodion, Pentecostarion and patricipate in one's mind the singing or for example hum or sing together with the choir. The holy services are intellectual services, because they are read and sung so that everyone can hear them. Even the divine Liturgy is intellectual service.\nPrayer of the mind happens in the mind (nous) and heart – it is spiritual, inner divine service that does not remind at all the intellectual service, because it takes place quietly in the altar of the soul by the grace of the Holy Spirit. The service which happens in the mind is quite much more valuable than all the intellectual services. This we know from the experience of our holy God-bearing fathers for most of the appearances of Christ, Theotokos and saints have happened exactly during the service of the mind and very rarely during the divine Liturgy. The progress which we receive from the prayer of the mind also helps us to be more conscious and vigilant and God-fearing in all the intellectual services.\nThe prayers compiled in the books are an endless richness, which has been preserved to our sinful age. Even though these prayers seem outwardly to be spontaneous prayers by some people they contain lot of grace of the Holy Spirit because they come from a holy mouth, body and soul – from people in whom God was totally pleased, so that they became temples of the Holy Spirit and vessels of God's grace. Therefore it is very useful to use often these prayers also, so that our understanding would be filled with their theological ideas that nourish our souls.\nAlways, throughout the ages, there have been also spontaneous prayers with own words and they will never stop. A prayer is everyone's free expression to God, coming from within, and it cannot be put into stereotypical forms. However, in a freely formed prayer there is involved the danger of looking for words, e.g. man's reason searches the words he wants to use during prayer. When I am thinking, what to say in prayer, I am no longer praying. Therefore the ways of prayer mentioned earlier help us when we praying so that we won't jabber and babble. Christ, the Knower of the hearts, knows our needs before we ask them and He has known them even before the world was created. Amen.\nOrtodoksiviesti 6/2006. Questions by Pauliina Happo. Published with permission.","Prayer: The Service Of The Heart\nPrayer is not unique to Judaism, yet Jewish prayer is unique. In synagogues around the world, congregants meet together to worship. But Jewish prayer is not confined to a synagogue. On a flight to the Holy Land, it is not uncommon to see Orthodox Jewish men gathering in the back of a 747 as the sun rises in the east, wearing prayer shawls and carrying siddurs (special prayer books) in their hands, much to the amazement of their fellow travelers. As they open their bags of religious paraphernalia, they begin to chant softly and sway gently. At the Western or Wailing Wall in Jerusalem, Jewish prayer can be seen graphically as worshipers recite various prayers, often with great emotion. And what pilgrim to the promised land has not been moved to write a special prayer and insert in into the cracks of the Wall’s huge stones?\nWhether in a synagogue, on board a 747, at the Wall, or in private homes, prayer holds a central place for the Jewish people. The next few articles in this series will focus on Jewish prayer, its description, its distinctiveness, and its dynamic.\nJewish Prayer: Introduced in the Old Testament\nThe English word pray is defined by such words as implore, entreat, or even beg. Most people think of the word as asking or petitioning God. The Hebrew word L’hitpalel carries no such idea. At its root is the word pll, which means to judge. Thus, regardless of the kind of prayer, the person praying is, in a real sense, judging himself as he interacts with God.\nJewish Scripture gives numerous examples of individuals crying out, seeking, and inquiring of God in prayer. The Psalmist stated it well when he said, “The Lord is near unto all those who call upon him, to all who call upon him in truth” (Ps. 145:18). It seems clear that the prayers of the Old Testament Hebrew were simple, spontaneous, and selfless. In addition, these prayers were almost always group-conscious and “other”-centered. The prayer offered by Solomon at the dedication of the Temple contains four elements of Hebrew prayer: thanksgiving, praise, confession, and intercession. Formal prescribed prayers, even commanded prayers, were not part of Jewish prayer until the time of the Second Temple.\nJewish Prayer: Interpreted by the Rabbis\nUsing Deuteronomy 11:12, the rabbis determined that prayer holds a supreme position in Jewish thought, calling it “the service of the heart.” After Herod’s Temple was destroyed, prayer was regarded as a substitute for the sacrifices. Later it came to be considered a means to forgive sin.\nJewish prayer is distinct in several ways. First, it is spoken in Hebrew, a practice preferred although not prescribed. This being the case, the more Orthodox the congregation, the more Hebrew is used in its prayers. Many find this practice frustrating, either because they are unable to read the Hebrew words or because they read the words without understanding what they are saying. However, this practice is defended in the following ways.\n- Hebrew is the language of the Torah, the “sacred tongue,” and as such unites Jews as a people.\n- The use of Hebrew ensures that Jewish people will feel reasonably at home in their synagogues.\n- The use of Hebrew serves as a means of binding the people with the land of Israel.\n- The use of Hebrew checks the possibility of total assimilation into a non-Jewish culture.\nThe second distinctive of Jewish prayer is its fixed liturgy. One rabbi comments that “liturgy unites, theology divides.” It is believed that the use of siddurim (prayer books) affords the worshiper the opportunity to think of things about God that he may not have thought of on his own. In addition, liturgy instills a sense of community as prayers are recited together to God.\nThe third distinctive of Jewish prayer is that a minimum number of ten men is required to pray corporately. This quorum is called a minyan. The practice is taken from the Torah (Num. 14). There the ten spies were considered a congregation. While private prayer is allowed, it is regarded as a special mitzvah (good deed) to pray as part of a congregation. The importance of corporate prayer can be seen clearly in the rabbinical teaching that if you cannot be present for corporate prayer, you can at least time your private prayers to coincide with those of the congregation. This aspect of Jewish prayer is still another way to remind Jews that they are a social people who can be encouraged by being around one another.\nJewish Prayer: Intensity of the Heart\nCentral to the concept of a Jewish prayer is kavanah, the devotion that the rabbis direct to God during prayer. It is concentration; it is sincerity; it is praying as though the shekinah (glory of God) is present. Kavanah is a quiet calm and an assurance as people recite the prayers. Maimonides said, “Prayer without devotion is not prayer…He whose thoughts are wandering or occupied with other things ought not to pray…before engaging in prayer the worshiper ought to bring himself into a devotional frame of mind.”\nAs we have looked at some of the issues of Jewish prayer, we may want to examine our own state of mind as we approach our Father in heaven. May we be unencumbered by wandering thoughts, having our minds set on Him."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:9d542b72-c5c1-49f8-b8f1-fa7e59e037c0>","<urn:uuid:ccc49a07-e281-4a6d-a50d-5966918575ef>"],"error":null}
{"question":"Is the climate in Mildura similar to Melbourne's temperate oceanic climate?","answer":"No, while Melbourne has a temperate oceanic climate, Mildura has a semi-arid climate. Mildura is situated in the North West region of Victoria and experiences intense Mallee heat during summer, characteristic of its semi-arid conditions.","context":["There is nothing quite like camping and fishing for the mighty Murray cod, particularly around Mildura, Victoria.\nMildura is situated in the North West region of Victoria, approximately 550km northwest of Melbourne. Mildura is a semi-arid climate and the greater area of Sunraysia is home to over 64,000 people; over 20,000 of those live in Mildura.\nMildura is home to many freshwater fish species in the Murray-Darling Basin like golden perch, silver perch, redfin perch, European carp, eel-tailed catfish and the host of bait species. But the prize for many local and visiting anglers is the mighty Murray cod. While it is worthwhile targeting golden perch and redfin, it is the Murray cod that plays on the mind of anglers the most.\nArguably, the best time to visit Mildura is during the autumn-winter months from around April right through to late August or early September (Murray cod closed season is from September 30 to December 1 inclusive) as this is when the bigger cod are usually caught.\nSummer can be lucrative, but long days on the water in the intense Mallee heat can be very taxing on the body and mind. A lot of smaller Murray cod can be caught on lures and bait over the summer period, but if you plan on only a quick visit and wish to catch a fish of a lifetime, winter is the go.\nBait fishing for Murray cod is a popular and viable option over the summer months. Bait fishing is a good option but the fish can get quite fussy about what is on offer, particularly when the water temperature is high or there is a plethora of natural bait foods in the system.\nSome anglers swear by fishing in holes of around 5-6m in the upper stretches above Mildura where quite a number of smaller models are caught each summer/autumn.\nBest rigs to use are a running sinker or ‘river rig’ with a single hook underneath the sinker. The best bait to use is arguably the bardi grub, with yabbies coming in a close second.\nWhen rigging a yabby be sure to pass the hook through its tail as you want to the yabbie to be ‘live’ when in the water for a realistic presentation. Sometimes, especially when the fish have shut down, presentation can make all the difference.\nBardi grubs can be neatly presented on a large hook, about the same size as the grub, which can be inserted at one end and out the other. A bit of Bait Mate is needed to keep the bait from falling off the hook when cast and sitting in the water for prolonged periods. If the grubs have been preserved in milk, or any other liquid, they can be prone to become quite soft.\nRod length most suited to almost all fishing in the Murray and not limited to bait fishing can be between 5-6ft and around the 2-3kg mark is ideal for bait fishing. Of course this depends on the target species. Cod fishing would require the upper end of the weight scale.\nBraided lines are popular for bait fishing now but some of the more traditional bait fishos prefer monofilament of around 15-20lb, depending on the target species.\nWhile trolling and casting lures can bring the desired result at any time of year, trolling is usually considered a summer technique. When the fish are holding deeper and moving about a bit more and casting a winter technique as the fish are sitting in shallow water tight up against structure and a better way to penetrate their hidey-holes and be in the strike-zone. Both techniques are very effective when it comes to targeting monster Murray cod.\nBest time for targeting this species is early morning and just before dark, however, monster Murray cod can be caught at any time of the day, depending on the conditions.\nEven though large hardbodied lures can be cast with great accuracy, it is ideal to be casting large spinnerbaits into structure in the shallow waters. They can be a very versatile lure, which can be worked fast or slow, deep or shallow and are often taken on the drop or when the lure is fluttering its way to the desired depth.\nLure choice is vitally important, regardless of whether you are casting or trolling.\nRunning depth should be considered a key at the best times, but considering cod don’t usually move as far in the winter as they do in summer to chase a lure, accurate casting techniques are desirable. You would pretty much have to smack the cod on the head with the lure for it to strike. This is why trolling during winter can be very testing and mentally demanding.\nBig lures are designed to catch big fish and by-catch a whole heap of smaller models too, including the popular golden perch. The large lure doing all the damage this season is the Australian-made 120mm Koolabung Codzilla as it is such a versatile lure with the ability to be cast or trolled from 2-10m+. Using big lures is a great way to try and tempt those cod lying deep on snags.\nRecommended lures for targeting Murray cod around Mildura are hardbody lures for trolling and casting 100-200mm in length with anywhere from 2-10m+ diving bibs (depending on where you are fishing). The 120-150mm hardbody lures work particularly well at depths around 6-7m around rock bars and steep drop-offs.\nIdeal spinnerbaits to use when targeting Murray cod in the stretches of the Murray River around Mildura are around 1/2-1oz weighted heads with large Colorado blades (faster running water above Mildura use willow blades).\nIf fishing above Mildura, and where the water flow is faster, it is desirable to use a slim line profile lure that swims against the current with ease but still presents well and stays in the strike zone long enough. I find the Koolabung Codbaits are idea for this situation. Colour choice is really up to the angler to decide.\nColour choice only really matters in shallow or clear water conditions, so colour choice is a personal one when fishing the murky waters of the Murray. If I had to choose a colour, it would be darker hues as they cast a larger shadow in the water column. Remember, bigger is best!\nAs a general rule, running depths are roughly 2-7m for trolling (depending on the water being fished) and 2-3m for casting.\nWhen the forces of Mother Nature are present and everything is deemed to be ‘right’, Murray cod will come on the bite, particularly during winter. It is at this point that any serious big cod angler needs to be using the right equipment and trusted knots to land that fish of a lifetime.\nLine and leader choice is also another important factor when maintaining desired running depths. But more importantly, good leader material can be the difference between landing the fish or not.\nWhether trolling or casting, braided lines of 50lb are usually needed to target and land monster Murray cod. Combined with a monofilament leader of approximately 60lb and the length of your rod, this is an unbeatable outfit.\nThe leader is usually connected via a bimini twist in the main braid line using a trusted knot. I use the Improved Albright Knot as I find this knot to have excellent strength and I have never had one let go yet. The Albright knot is also a more slim finish than some knots, which means less wear and noise through the rod guides when casting and retrieving.\nThere are plenty of different knots out there, which usually varies with every angler you talk to you. It is important to find the one that best suits your application and one which is relatively easy to tie. In any case, it is important for anglers to practise tying knots to minimise the time with their lure out of the water when changing tackle in the boat.\nA loop knot is best to tie the lure on by connecting directly to the tow-point of the lure. I find the loop knot to be the best method as it allows the lure to swim correctly without any restrictions. I like using the Perfection Loop as it is easy to tie and its finish is nice and neat.\nWhen it comes to tying spinnerbaits or mumblers, follow the same procedures with the bimini-twist and leader connection and length, but instead of a loop-knot, tie the leader to the R-bend lure using a closed knot like a blood knot, or something similar, to stop the leader from slipping up and down the arm of the spinnerbait.\nSurface fishing for Murray cod or golden perch can be lucrative around Mildura, but is probably the most ‘out-there’ of methods to use as catches can be very few and far between.\nCatching Murray cod from the surface, particularly during winter can be quite an arduous task because the fish don’t seem to move far from where they are sitting to unleash short bursts of energy to feed from the surface. However, in areas with no current or flow, food cannot circulate past the fish residence and surface fishing takes on a whole new meaning.\nShallow lakes or billabongs that contain plenty of timber are ideal to fish for cod from the surface as they provide ambush shelter not far from the strike zone. Fishing for these green machines at night with the use of no light except the distant glow of the moon can be an extremely explosive experience.\nThere are a few ideal spots around Mildura for this type of fishing. Summer is arguable the best time as bugs and other insects gather on the water’s surface making for a great meal for a shallow swimming fish. Koolabung Codwalker surface lures are arguably the best on the market for this type of fishing.\nTargeting Murray cod in shallow lakes is also very effective on fly gear. It is a great method for presenting large, but lightly weighted lures into tight spots or over log jams where Murray cod like to hide or reside. Kaos Cod Flies, made by Ross Virt, are the best Murray cod flies on the market.\nBaits, lures and flies for Murray cod\n• Recommended bait is bardi grubs and yabbies.\n• Recommended lures are hardbody lures for trolling and casting 100-200mm in length with anywhere from 2-10m+ diving bibs. The 120-150mm hardbody lures work particularly well at depths around 6-7m. Hardbody ‘walking style’ surface lures are needed for surface fishing in good conditions, particularly on the Murray River and associated lakes, billabongs and backwaters.\n• Spinnerbaits and mumblers, such as those made by Bassman Spinnerbaits, are a favourite snack for giant Murray cod, particularly on the cast during the cooler months. A lot of the time, Murray cod will take these lures as they flutter down in the water column on the drop.\n• For those with a lot of patience, flies can work a treat on Murray cod as well. Large flies made for targeting this species is needed. Kaos Cod Flies make a great range.\nAccommodation, bait and tackle shops\nMildura boasts some of the most picturesque and sought after camping destinations in Australia. Several caravan parks are available for comfortable and affordable accommodation on the banks of the Murray or Darling rivers:\n1. Apex River Beach Holiday Park\n435 Cureton Ave Mildura, VIC 3502\nPhone: 03 5023 6879\n--e-mail address hidden--\n(only a short drive from the centre of town and a very tranquil place to stay. Only a short boat ride from Mildura’s lock 11 and weir. Boat hire is also available upon request)\n2. Willow Bend Caravan Park\nDarling Street, Wentworth NSW, 2648\nPh: 5027 3213\n(right on the Darling river, only a short boat trip to the junction where the Darling meets the Murray river)\n3. Fort Courage Caravan Park\nRenmark Road, Wentworth, NSW, 2648\nPh: 03 5027 3097\nEmail: --e-mail address hidden--\n1. Got One, Lime Ave, Mildura\n2. Sunraysia Marine, 7th Street\n3. Ray’s Outdoors, 15th Street\n4. BCF, 15th Street","The largest part of Australia is desert or semi-arid. Only the south-east and south-west corners have a temperate climate and moderately fertile soil. The northern part of the country has a tropical climate, varying between grasslands and desert.\nWhat are the main climates in Australia?\nThere are six distinct climate groups; Equitorial, Tropical, Sub-tropical, Desert, Grassland and Temperate. The Temperate zone occupies the coastal regions of New South Wales (Sydney), Victoria (Great Ocean Road, East Gippsland, Phillip Island), Tasmania and most of South Australia (Kangaroo Island, Eyre Peninsula).\nWhat are the 3 main climate zones in Australia?\n- Hot humid – hot dry.\n- Warm – mild temperate.\n- Cool temperate – alpine.\nWhat are the main climate types?\none of five classifications of the Earth’s climates: tropical, dry, mild, continental, and polar.\nWhat are the 6 types of climates?\nThere are six main climate regions: tropical rainy, dry, temperate marine, temperate continental, polar, and highlands. The tropics have two types of rainy climates: tropical wet and tropical wet-and- dry.\nWhat is Australia’s climate and weather like?\nThe northern section of Australia has a more tropical influenced climate, hot and humid in the summer, and quite warm and dry in the winter, while the southern parts are cooler with mild summers and cool, sometimes rainy winters. … December and January are the hottest months in Australia, July and August the coldest.\nWhat type of climate is Melbourne?\nMelbourne, the state capital of Victoria and second largest city in Australia, has a temperate oceanic climate (Köppen climate classification Cfb) and is well known for its changeable weather conditions.\nWhat are the 8 climate types?\n- Winter dry (temperate climate)\n- Winter dry (continental climate)\n- Summer dry (continental climate)\n- Continuously wet (continental climate)\n- Polar ice caps (polar climate)\nWhat is a zone 7 climate?\nClimate Zone 7 is the southernmost coastal region of California. The warm ocean water and latitude make this climate very mild. The temperature of the ocean water affects the air temperature over it, and this in turn moderates temperatures over the coastal strip.\nWhat is Type B climate?\nType B designates climates in which the controlling factor on vegetation is dryness (rather than coldness). … Dry climates are divided into arid (BW) and semiarid (BS) subtypes, and each may be differentiated further by adding a third code, h for warm and k for cold.\nWhat are the three ways climates are classified?\nOverview. The Köppen climate classification scheme divides climates into five main climate groups: A (tropical), B (dry), C (temperate), D (continental), and E (polar). The second letter indicates the seasonal precipitation type, while the third letter indicates the level of heat.\nWhat are the 6 types of climates for kids?\nThere are five general types of climate: tropical, subtropical, temperate, polar, and highland.\nWhat is climate and its types?\nIn simple terms climate is the average condition for about thirty years. Climate and weather are different. Weather is the day to day conditions in the atmosphere. The types of climates are: Tropical, Desert/dry, Temperate, Polar, Mediterranean. … Tropical climates have warm temperature and only two seasons; wet and dry.\nWhat are the different kinds of weather?\nThere are five types of weather: sunny, cloudy, windy, rainy, and stormy.\nWhat are the 4 different climate zones?\nThere are 4 major climate zones:\n- Tropical zone from 0°–23.5°(between the tropics) …\n- Subtropics from 23.5°–40° …\n- Temperate zone from 40°–60° …\n- Cold zone from 60°–90°"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:8606d153-f552-4ce9-9713-9bcdac71bdae>","<urn:uuid:6bae122c-952b-4452-800a-496d01140b6d>"],"error":null}
{"question":"What is the depth of the world's deepest underwater cave, and where is it located?","answer":"The world's deepest underwater cave is the Hranice Abyss, located in the eastern part of the Czech Republic. It is 404 meters deep, which makes it 12 meters deeper than the previous record holder, the 392-meter-deep Pozzo del Merro in Italy.","context":["The oldest drawings on present day Czech territory are lines and\ngeometrical images created on cave walls by hunters in the early Stone Age,\nmeaning around 4,200 BC, Právo reported on Tuesday, citing new\nResearchers have been examining the drawings, which are on the walls of the Kateřina Cave in the Moravian Karst protected nature reserve. The meaning of the drawings is unclear, they say.\nArchaeologist Martin Golec of Palacký University in Olomouc said his team only recently ascertained that the drawings were in fact prehistoric and were not made in the modern age.\nA team of Czech divers and land surveyors are the first in Europe to have succeeded in scanning a flooded cave system. Using film footage from the water-filled Chýnov cave in south Bohemia, they created a detailed and accurate three-dimensional map. The main advantage of the so-called videogrammetry is its simplicity and speed.\nA rescue team was able to free a speleologist trapped in the Nová Drátenická cave in Moravský kras (Moravian Karst) on Sunday. The man’s legs were pinned in an apparent cave-in; a spokesman for the fire brigade confirmed his rescue as complex, as the man was located several hundred metres away from the cave opening. Once freed, the person was taken by helicopter to Brno’s Teaching Hospital. The injuries to his legs were described as serious.\nA team of explorers recently found the world’s deepest underwater cave located in the eastern part of the Czech Republic. A Czech-Polish expedition, led by the legendary Polish diver Krzysztof Starnawski, descended deep into the flooded limestone cave called Hranice Abyss and found it to be far deeper than previously thought. According to their measurements, the cave is 404 meters deep, 12 meters deeper than the previous record holder, 392-meter-deep Pozzo del Merro in Italy.\nA team of explorers has just confirmed that Hranice Abyss, located in the eastern part of the Czech Republic, is deepest underwater cave on the planet. A Czech-Polish expedition, led by the legendary Polish diver Krzysztof Starnawski, descended deep into the flooded limestone abyss Hranicka Propast this week and found it to be far deeper than previously thought. The underwater cave is 404 meters deep, making it the deepest underwater cave in the world, 12 meters deeper than the previous record holder, 392-meter-deep Pozzo del Merro in Italy. News of the discovery appeared in the National Geographic which co-funded the expedition.\nSlovakia’s Mountain Rescue Service has confirmed that a Czech speleologist died in Slovakia at the weekend in a cave-in at a recently discovered natural cave near Banská Bystrica. The man lost consciousness when he was buried under debris in the collapse; colleagues attempted artificial resuscitation but the man’s injuries proved too severe. Some twenty members of the Mountain Rescue Service were called to the scene. The site, known as Jeskyni ztraceného prstenu (Cave of the Lost Ring) was found in June.\nParamedics took part in a six-hour rescue operation in a cave in Moravský kras on Saturday evening to save a speleologist who was injured at the bottom of the cave. The man, who explored the cave with a team of colleagues, bruised his ankle some 25 metres under ground. The rescuers had to widen one the cave's tunnel to be able to bring the man out and then they had to carry him through difficult and snowy terrain to a site accessible by ambulance.\nThe region of Broumov with its picturesque rock formations has long attracted tourists and climbers. Now, scientists have revealed that the area has far more to offer - a vast network of sandstone caves hidden below ground. Spanning more than 27 kilometres, the caves are the most extensive underground sandstone labyrinth in Europe. Research work in the area started two years ago and cave explorers have only now finished mapping the whole space. Earlier today I spoke to Petr Kuna from the nature reserve of Broumov and asked him to tell me something\nThe Czech Republic’s most famous angler has caught a fish weighing 190 kilograms! A giant bomb shelter which was to have served as the operational headquarters of the Czechoslovak communist leadership in the event of a Third World War has become a public attraction. And, a cat is a better companion than you might think. Find out more in Magazine with Daniela Lazarová."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:32464646-35c1-412f-8550-7cd3b12f7f07>"],"error":null}
{"question":"Having suffered an ACL injury myself, I want to understand: what are the key milestones in post-ACL surgery rehabilitation, and what specific criteria must be met before returning to sports activities?","answer":"Post-ACL surgery rehabilitation begins with regaining motion and strength, followed by improving balance, agility, jumping, and landing skills. Early goals include achieving full knee extension and quadriceps strength. At around 3 months, patients may start a running program if they pass specific functional tests. The later stages focus on sport-specific activities. Before returning to sports, specific criteria must be met regardless of timeline, though experts now recommend waiting at least 9 months. A team approach involving the surgeon, athletic trainer, and physical therapist is essential. Without proper rehabilitation, the risk of re-injury can be as high as 45%, but with proper training, this can be reduced to 18-20%. Additionally, when returning to activities, a gradual progression in intensity and difficulty is crucial, particularly for sport-specific movements.","context":["It is no secret that anterior cruciate ligament (ACL) injuries and reconstruction surgeries are very common at all levels of sports today. As a physical therapist, I rarely go more than a few days without a patient in my clinic rehabilitating from ACL reconstruction (ACLR), and many coaches have likely encountered or will encounter athletes returning from this surgery.\nThough ACL injuries are infrequent in the sport of baseball, I have helped several baseball players rehab and return to play after ACLR over the past year. While most baseball coaches likely have some awareness of shoulder/elbow injuries, they may not have as much exposure or education with players returning from ACLR.\nFor the rehab professional, it can also be very easy to overlook the sport-specific demands of baseball players when rehabbing from ACLR.\nMy purpose in writing this article is to provide some insight into recent research regarding return to sport after ACLR and some points to consider when returning to baseball activities after ACLR.\nSecondary ACL Injury Risk – The Research\nOne of the biggest issues with returning an athlete to sport after ACLR is risk of a secondary ACL injury, either in the same knee or on the opposite knee. New research indicates that this may be happening at an alarming rate.\nI will list some statistics below and then will discuss why these findings matter and how we can improve on how we are returning these athletes to sport activities.\n- A 2016 study reported approximately 1.25% of high school and college athletes will have an ACL injury.\n- 6 different studies looked at athletes returning to sport after ACLR, and the average rate of a 2nd ACL injury in these studies was roughly 30% (ranging from 25-37%)!\n- In one study, 45% of re-injuries occurred in the first 2 months after returning to sport\n- Those who returned to sport within 5 months after surgery had a re-injury rate of 100%\n- Those who returned within 9 months had a 40% re-injury rate\n- Those who waited until after 9 months had a 19% re-injury rate\n- There is a roughly 50% decrease in re-injury risk for each month that return to sport is delayed up to 9 months post-op.\nRemember that there are always a number of factors involved with any injury, and these include age, gender, sport(s) played, time of year, common playing surface, genetics, previous training history, previous injury history, and many more. Even without specific context, these numbers above are very alarming, and this has become a hot topic issue in the world of sports rehabilitation.\nIt has become common for surgeons to release athletes to return to sport activities at 6 months post-op, despite the statistics listed above. It has been a challenge for me to convince patients to avoid early return to sport when their surgeon has given them clearance, especially when strength limitations, mobility limitations, movement deficiencies, or any of the factors listed above are still present.\nSo the challenge, then, is how do we get better in minimizing risk for re-injury with these athletes?\nReducing Risk for Secondary ACL Injury\nFirst, delaying return to sport is usually a great place to start. While there may be certain pressures influencing an athlete to return by a given time, much of the research listed above speaks against simply using a given time frame to determine when an athlete should return.\nMany experts are now recommending waiting at least 9 months before returning to sport, with increased time often needed depending on the status of the athlete. The short-term benefits of early return often do not outweigh the long-term risk of re-injury. It is now generally accepted that there should be specific criteria met before returning, regardless of when that is achieved. What specific criteria should be used to determine when an athlete should return to sport is a topic that deserves a separate article of its own.\nOne of the primary reasons why I believe we see relatively high re-injury rates is that there is often poor communication between the rehab professional and the coach who inherits the athlete who may or may not be completely ready for full return to sport. Often times there are insurance limitations at play or the rehab professional discharges the athlete early with the assumption that the athlete will continue to be compliant with a home exercise program.\nAs a result, athletes are often discharged from formal physical therapy before they are ready for full return to sport. This leaves them with a lack of professional oversight of their rehab program if they are unable to work with an athletic trainer in their school. Many athletes, then, either do not stay compliant with their home exercise program or are not provided with proper progression of exercises, and their home program eventually fails them.\nResearch has shown that movement quality can decrease within a few months of stopping supervised exercise programs in uninjured youth athletes, and we have to assume the same is true in a rehabilitation population. Ultimately, unsupervised and outdated exercise programs are not adequate in getting athletes to perform at the required level to return safely to sport.\nI believe part of the solution to this problem is to have greater communication between the rehab professional and either the player’s coach and/or parent to ensure that somebody is able to supervise the athlete with their exercise program.\nAs a coach, if you have an athlete returning from an injury, it is crucial that you get in contact with the athlete’s medical professional to ensure that you are safely and gradually returning that athlete to play. Employing or partnering with a knowledgeable strength and conditioning coach or rehab professional can be a great way to ensure that your athletes are maximizing their development and being monitored for red flags with regard to future injury risk.\nReturn to Baseball-Specific Activities\nOne of the areas in which professional supervision may be most important is when handling return to baseball-specific activities such as throwing and hitting. It is important to remember that the athlete has likely spent an extended time away from these activities.\nFor many athletes, this is likely a good thing for upper extremity health and provides a great window to address issues in the upper extremity kinetic chain. As a rehab professional, it is important to ensure that proper trunk and upper extremity strength and mobility are achieved and maintained, even in the early stages of rehabilitation. This will ensure that the athlete is ready for a gradual progression of throwing activities once their knee allows them.\nOnce the lower body is ready for returning to throwing or hitting, it is important to allow for a gradual ramp-up in volume and intensity of these activities. Excessive early volume or intensity may predispose them to injury in other areas of the body, such as the shoulder or elbow.\nFor a recent athlete of mine, we spent an extended period of time performing plyoball throwing drills followed by light flat ground throwing before he got anywhere near the mound just to ensure that he was able to build tolerance for throwing again. This also allowed for gradual progression of loading his surgical knee prior to performing high intensity throwing on a mound. As a coach, you cannot assume that the athlete is ready to return to max effort throwing activities just because the knee appears to be ready for it.\nOn that same note, specific drill work is a great way to introduce the athlete to baseball activities, but they should be performed with gradual progression of intensity and difficulty. This can allow a player to begin to feel comfortable with position-specific activities before their full return to play. Again, this is where input from an informed professional can be an excellent way to ensure that you are maximizing what you can do with the athlete without exposing them to unnecessary risk.\nThis is also an excellent time to assess how other parts of the kinetic chain may be impacting throwing or hitting mechanics. Athletes with ACL injury are likely to display limitations with ankle mobility, hip mobility, lower extremity strength, and possibly even trunk stability or mobility issues if these have not been areas of focus throughout their rehab process. While these areas may have been deemed adequate to perform basic strengthening exercises or running/plyometric progressions, they may provide barriers to adequate performance with throwing or hitting.\nWhile there is a relatively large amount of research in return to sport after ACL reconstruction, I am unaware of any that relates specifically to baseball players. I hope to see future research investigating short-term or long-term changes in throwing or hitting mechanics in athletes with previous ACLR history.\nThese topics discussed above are only a small number of issues that need to be taken into account when returning athletes to baseball after ACLR surgery and there are numerous others that could be discussed as well.\nWhile I work full-time in an outpatient physical therapy clinic, I have made it my goal to partner with local programs and facilities to try to assist with topics such as this to ensure that athletes, coaches, and parents can be better educated about how to safely and effectively return to baseball activities while minimizing risk for future injuries.\nFor any coaches or facility owners who may read this, I encourage you to partner with a rehab professional or knowledgeable strength and conditioning professional to ensure that you are promoting proper health and wellbeing of your players.\nStanley, et al. Sex differences in the incidence of anterior cruciate ligament, medial collateral ligament, and meniscal injuries in collegiate and high school sports: 2009-2010 through 2013-2014. Am J Sports Med, 2016 Jun;44(6):1565-72. doi: 10.1177/0363546516630927.\nGrindem, et al. Simple decision rules can reduce reinjury risk by 84% after ACL reconstruction: the Delaware-Oslo ACL cohort study. Br J Sports Med, 2016 Jul;50(13):804-8. doi: 10.1136/bjsports-2016-096031.\nGoerger BM, et al. Anterior cruciate ligament injury alters preinjury lower extremity biomechanics in the injured and uninjured leg: the JUMP-ACL study. Br J Sports Med, 2015 Feb;49(3):188-95. doi: 10.1136/bjsports-2013-092982.\nSugimoto, et al. Compliance with neuromuscular training and anterior cruciate ligament injury risk reduction in female athletes: a meta-analysis. J Athl Train, 2012 Nov-Dec;47(6):714-23. doi: 10.4085/1062-6050-47.6.10.\nPadua DA, et al. Retention of movement pattern changes after a lower extremity injury prevention program is affected by program duration. Am J Sports Med. 2012;40:300–306. doi: 10.1177/0363546511425474.","Having ACL surgery is a big commitment. It is a painful procedure with a long-anticipated recovery. As your surgeons, we know all of you have one goal in mind, returning to sports after your ACL reconstruction. The research around the return to sports after knee surgery has been studied aggressively for decades. We have great statistics and great rehabilitation programs to guide you. Many of you will try and rush your return to sports too soon. If you attempt to return to sports too soon after ACL surgery then you run the risk of tearing or re-injuring your new ACL.\nBelow are 5 expert Sports Medicine opinions about when an athlete can expect to return to sports after having an ACL ligament reconstruction on their knee.\nA successful return to sports after ACL surgery is your number one goal. There are many variables that go into determining when you should try to return to sports after ACL surgery. The risks of returning to the playing field too early after ACL surgery include suffering a re-tear of your new ACL. A successful return to sports after ACL surgery requires a team approach. It involves you, your surgeon, your athletic trainer and your physical therapist. We have very strict criteria for when an athlete can return to sports after ACL surgery. If you attempt to return to sports too soon you have a significant risk of tearing your new ACL.\nRelated reading …\n- Physical Therapy and ACL Injuries\n- Can I return to sports after ACL surgery?\n- Risks of reinjury after ACL surgery\nThe statistics can be alarming. You might have a 45% risk of tearing your new ACL if you have not rehabilitated your knee properly. You can drive that risk down to 18-20% with the proper training. Unfortunately, once you tear your ACL you are always at risk of tearing the new ACL.\nThe first goal of your physical therapy after ACL surgery early on is to get back your motion and strength. The later stages of the recovery process involve improving your balance, agility, jumping and landing skills. Doing so will decrease the risk of re-injuring your knee and re-tearing your ACL. Your recovery from an ACL reconstruction involves not only the physical aspects of your recovery but perhaps equally as important, the emotional and psychological components. We will get into this in more detail in a later series with many experts who specialize in rehabilitation of ACL injuries.\nThe research regarding the immediate management of an athlete after ACL surgery continues to evolve. It turns out that immediate PT might weaken your new ACL graft. Some surgeons are starting physical therapy a week or two after surgery because of this research. Once PT has started, the research today shows that many people will tolerate an accelerated ACL surgery physical therapy program and be able to return to sports as early as 8 months. There are many experts who feel that might be too early. Thus there is a lot of confusion on the proper way to return to sports after ACL surgery.\nWhat criteria do our experts use to determine when you might be able to return to sports after an ACL reconstruction? Once again we have asked our panel of ACL experts to offer their insights.\nHow Do You Manage Your Athletes After ACL Surgery … and\nWhen Do You Let Your Athletes Return To Sports After ACL Surgery?\nS.S: My ACL surgery post-op protocol involves brief ( about 7 days ) immobilization of the knee in full extension and full weight bearing as tolerated. In some animal studies done at HSS, a short period of post-op immobilization leads to better quality ACL graft biologic tendon to bone attachment. In addition, this has helped me encourage patients to achieve early post-op full extension of the knee, which is very important. After about 7 days, I remove the brace and start physical therapy to reduce swelling, get full range of motion and start isometric strengthening progressing to closed chain lower extremity strengthening. I modify this program for patients with a meniscus repair or a multi-ligament reconstruction. I will allow patients to start a running program at 3 months if they can pass a series of four, simple to administer, functional tests. These tests were developed by a joint group of orthopedic surgeons and physical therapists and were published in a special issue of the journal Sports Health. During this time they continue to work on strength and neuromuscular control.\nD.G : I work very aggressively to have the patient regain full knee extension in the first few days or at least a week or two after surgery. I also have them work with the physical therapist to regain quadriceps strength as quickly as possible. I tend to allow full weight-bearing unless I perform a repair of a bucket-handle meniscus tear. Otherwise, I use a fairly standard protocol, usually restricting jogging for 10 to 12 weeks after surgery and progressing to sport-specific activities in the coming weeks.\nD.O : Initially, start physical therapy within a week. Unless I need to perform a microfracture, I typically do not use a continuous passive motion machine (CPM). Icing after surgery is a great pain reliever. The compression ice machines work great, but typically they are not covered by insurance. A frozen bag of peas can work pretty well in its place.\nAs you can see, determining when to return athletes to sports is a challenging issue. We want to limit your risk of re-injury as much as you do. Routine bracing after ACL surgery is not proven to be necessary. We also differ slightly when it comes to rehabilitation immediately following an ACL reconstruction. It turns out, as Dr Slattery pointed out, that waiting a while before starting PT might be of benefit by allowing your new ligament to start to heal. Stressing full extension as Dr Geier pointed out is critical.\nBottom line.. do not rush your return to sports. Get that leg and your mechanics and stability as close to normal as possible. It will be time well spent."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:e686f3d7-1bd4-42b4-b345-80155a18bd14>","<urn:uuid:ef81b139-3132-43f1-8463-ef7d42657216>"],"error":null}
{"question":"As a fishing enthusiast worried about climate change, I'm curious - how does water temperature affect both bullhead fishing success and overall seafood production?","answer":"Water temperature significantly impacts both bullhead fishing and broader seafood production. For bullheads, the best fishing occurs when water temperature is between 55-70°F. Their feeding behavior changes with temperature - they're most active in warm water, eating almost nonstop in summer, but their metabolism and feeding decrease below 60°F. As for overall seafood production, climate warming affects both wild and farmed seafood. Northern regions are particularly vulnerable since they depend on cold-water species. Rising temperatures can reduce oxygen levels in aquaculture systems, increase metabolic costs for organisms, and lead to greater disease spread. Temperature changes also cause species to shift their distribution northward, affecting local fisheries.","context":["Quick and easy access to recreational privileges in Iowa, including hunting, fishing, and specialty licenses:\nPurchase Your Licenses Online\nBuy your Hunting and Fishing license online today! We offer multi-year packages and combos for whatever you need to stay licensed.\nSupport conservation in Iowa by buying a natural resource plate for your vehicle.\nMake your online reservation for state park cabins, camping sites and lodges.\nIowa DNR Customer Service\nMon - Fri, 8:00am - 4:30pm CST\nSubmit Online Inquiry\nInformation / Records Requests\nContact Information by County\nWhere to Find Them Equipment Baits & Lures Angling Tips Bullhead Fact Sheet Fishing for Bullheads\nIowans have a nostalgic feeling toward bullheads. Bullheads are abundant, fairly easy to catch with simple tackle, popular with all ages and close as the nearest pond or lake. Where to Find ThemWarming water temperatures and inflow from spring rains and melting snow trigger bullheads to move toward shore and begin feeding. The best time to catch bullheads is when the water temperature is between 55 and 70° F. They can be caught in colder water, but the bite is less aggressive.\nStart in shallow water which warm faster. On a sunny day, there can be a 10 to 20° F difference between the shallows and the main lake basin. The windward side warms quicker than the lee side of lakes on sunny, windy days, attracting bullheads.\nAreas of inflowing water are key early spring bullhead spots with warmer water and large amounts of food. Daytime is best to catch bullheads in early spring. They start to move off shore during the day in late spring encouraging anglers to fish after dark as the bullheads come back in the evenings to eat.\nBullheads move toward shore to start spawning in May and early June. Fish look for nest sites in shallow water, near rocks and stumps. Spawning lasts about two weeks depending on water temperature and weather. Bullheads are easy to catch during spawning.\nMost deep man-made lakes and reservoirs stratify and develop a thermocline in July and August. Little or no dissolved oxygen or fish are below the thermocline. Cast out as far as you can in late spring and early summer for excellent catches of bullheads. Avoid fishing below 15 feet in June, July and August. Search out areas where the water is approximately 12 to 15 feet deep. Bullheads rest and eat in this cooler, well-oxygenated water. Night fishing is a must when fishing the warm waters of summer in lakes and ponds. Bullheads eat almost nonstop in warm water and are as easily caught in August as in May.\nFall bullhead fishing can provide a lot of action that will last until the water temperature drops below 60° F. Bullheads go on a fall feeding frenzy to prepare for the long cold winter months. As water temperatures cool, they once again move toward shore and become vulnerable in the shallows. After the fall turnover, the thermocline dissolves and the deep water will once again have dissolved oxygen and fish. In autumn, find bullheads on shallow water points near deep water.\nBullhead fishing opportunities in Iowa are greatly reduced when water temperatures fall below 60° F. Their metabolism slows and their need for food decreases.\nWeight and size are key to terminal tackle. Many bullhead anglers use sinkers that are too heavy and hooks that are too large. No weight, except a small split shot, should be directly attached to the line. Weight attached or tied to the line lowers your chances of catching bullheads because a bullhead will abandon the bait when it feels resistance. A small one-quarter to one-half ounce sliding sinker is perfect. The light sinker will not bury into the bottom sediment usually found on lake bottoms where bullheads are plentiful. Fish do not feel the weight of the slip sinker. Thread the line through the hole in the middle of the sinker, tie on a hook and pinch a small split shot 6 to 12 inches above the hook. The split shot keeps the sinker from sliding into the baited hook. When a bullhead picks up the bait, the only resistance it feels is from the small split shot since the line moves freely through the sinker.\nHook sizes No. 2 to 1/0 are perfect for catching bullheads. Use long-shanked hooks, since most bullheads swallow the hook. Make sure you have a hook disgorger or pair of needle nose pliers in your tackle box to remove swallowed hooks. If you want to keep your catch, cut the line and retrieve the hook when the fish is cleaned. Small circle hooks are popular because they hook the fish in the corner of the mouth, making it easier to remove the hook and reducing the number of hooks you need.\nBullheads are omnivorous and will eat almost anything they can swallow. Worms and night crawlers, used almost exclusively by dedicated bullhead anglers, catch the majority of bullheads. Other baits used for bullheads include leeches, live and dead minnows, liver, shrimp, dough balls, and stink bait. Crayfish are the most under-utilized bait for bullheads. Use small, whole crayfish or peel the white meat from the tail of a larger crayfish. Crush small crayfish slightly to create more scent.\nwhen fishing for bullheads. Bullheads usually bite in two ways: in colder water, the line\nwill twitch and move in spurts, but as water temperatures warm and fish become\nmore active, bites are signaled by a few light taps and a line-tightening run.\nOnly practice will tell you when to set the hook in cold water; don’t worry\nabout hooking the fish when most runs happen, the fish often hooks itself\nby swallowing the hook.\nUse caution with handling bullheads. They have very sharp pectoral and\ndorsal fin spines. Grip the fish around the pectoral spines and position your\nhand to avoid the dorsal spine. Keep a towel or rag handy to use as a barrier\nbetween the fish and your hand.","By: Gaitlyn Malone, SRC Intern\nAs the world’s climate continues to change, economic, social, and environmental changes will undoubtedly occur along with it. One sector that is expected to be economically affected by climate warming is seafood production (Breitburg et al., 2018). Seafood production, which includes both farmed and captured fish, shellfish, and seaweed in marine and freshwater, will experience changes since the warming of an environment has the ability to change both a species’ distribution and life history characteristics (Pecl et al., 2017; Cochrane et al., 2009). Therefore, it is crucial to work towards being able to predict and understand the extent of these changes in order to prepare for the future.\nA recent study (Blanchet et al., 2019) examined the effects of climate change on seafood production within each European country in order to identify potential challenges and opportunities within the sectors of marine fisheries, marine aquaculture, and freshwater production. To do so, the researchers combined information on the target species’ temperature preferences, life history characteristics, and production volume to determine their biological sensitivity (BS) and the maximum temperature (Tmax) that they were experiencing. They then determined the adaptive ability of seafood production in each country or sector by determining the number of species that the country/sector exploits and those species’ temperature ranges. A country or sector that exploits a higher number of species will be more likely to adapt in response to climate change. A species with a wide temperature range would also potentially be more adaptable since they are able to withstand a variety of temperatures.\nOverall, seafood production was found to generally be more vulnerable within the marine fisheries and aquaculture sectors. The freshwater sector varied greatly based on country. Within the marine sector, northern countries tended to be more sensitive to warming than southern countries since seafood production in these areas are more dependent on cold-water species with a high BS. Southern countries tended to rely on warmer water species that had a lower BS. The main challenge facing these marine fisheries is due to changes in species distribution. In response to warming, there has been a northward expansion of the range of several species, which in some cases has included a contraction of their southern range. This change in distribution has the ability to affect local fisheries and management, who in southern areas may lose access to their resources, while northern areas may benefit. Aquaculture taking place in temperate zones was also predicted to be at risk from warming conditions, since increasing temperatures have the ability to reduce oxygen levels in the water and increase the metabolic costs for organisms. Disease is also likely to increase in these systems since pathogens may spread more readily. The low amount of species diversity in aquaculture also makes it particularly susceptible to rising temperatures.\nUnder warming conditions is not impossible to continue producing sustainable seafood, however efforts must be made to adapt to climate change. Therefore, the authors suggest that there must be communication between stakeholders, diversification of exploited species, and transnational cooperation in order to meet these goals.\nBlanchet, M.-A., Primicerio, R., Smalas, A., Arias-Hansen, J., Aschan, M. 2019. How vulnerable is the European seafood production to climate warming?. Fisheries Research 209, 251-258.\nBreitburg, D., Levin, L.A., Oschlies, A., Gr.goire, M., Chavez, F.P., Conley, D.J., Gar.on, V., et al., 2018. Declining oxygen in the Global Ocean and coastal waters. Science 359 (6371).\nCochrane, K., Young, D.C., Soto, D., Bahri, T., 2009. Climate change implications for fisheries and aquaculture: overview of current scientific knowledge. FAO Fisheries and Aquaculture Technical Paper 530, 212.\nPecl, G.T., Ara.jo, M.B., Bell, J.D., Blanchard, J., Bonebrake, T.C., Chen, I.-C., Clark, T.D., et al., 2017. Biodiversity redistribution under climate change: impacts on ecosystems and human well-being. Science 355."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:a9338254-1d37-4e35-8f0a-96e6eac3c6fd>","<urn:uuid:15b67a19-5005-450e-8ce4-8018dcb10f65>"],"error":null}
{"question":"What are the key differences in reproductive capacity between the historical Rocky Mountain locusts and today's Desert Locusts?","answer":"The Rocky Mountain locust could lay up to 20,000 eggs per square foot of ground. The Desert Locust, while also prolific, shows different reproductive patterns: a solitary female lays 95-158 eggs per pod, while a gregarious female lays fewer than 80 eggs per pod. Desert Locusts can lay eggs three times in their lifetime at 6-11 day intervals, and up to 1,000 egg pods have been found in one square meter. Both species demonstrate significant reproductive potential, with Desert Locusts able to multiply 10-20 times per generation.","context":["- Historic Sites\nPharaoh Had It Easy\nEgypt’s locusts could not have been more terrible than those which blighted the Great Plains for four summers, then vanished as mysteriously as they had come\nOctober 1960 | Volume 11, Issue 6\nThe same unswerving attitude prevailed when the locusts came upon natural obstacles. In 1875 several million crossed Pottawatomie Creek near Lane, Kansas, at a point where the stream was more than seventy feet wide. Another observer watched the crossing of the Big Blue and Little Blue rivers near Independence, Missouri, which were in several places a good one hundred feet across! They would march down to the water’s edge and commence jumping in, one upon another, till they would pontoon the stream, so as to effect a crossing. Two of these mighty armies also met, one moving east and the other west, on the river bluff … and coming to a perpendicular ledge of rock 25 or 30 feet high, passed over in a sheet apparently 6 or 7 inches thick, and causing a roaring noise similar to a cataract of water.”\nThe rate of advance of the half-grown locusts was three yards per minute at best. The pace was one-fourth hopping and three-fourths walking. Any single individual, forced to hop a dozen times consecutively, would halt from fatigue. Theoretically then, a locust army might cover thirty miles during its walking and hopping phase, but in practice this range was sharply reduced. The grasshoppers were choosy about marching in the rain. They seldom got started before ten in the morning and usually halted for the night by four in the afternoon.\nIn the final chapter in the life of the Rocky Mountain locust, its wings sprouted and it took to the air to migrate almost unbelievable distances and spawn. To this day the naturalists are trying to resolve the available evidence into a meaningful pattern of movement. They are handicapped by the fact that in flight the locusts were disposed to meander a good deal, guided largely by the direction of the wind. The same swarm that blackened the sky for hours over a particular locality going east one day might conceivably repeat the performance a day or two later traveling west. With a favoring breeze and no succulent young grain to provide a distraction, the locusts sometimes flew two hundred miles in a day. To travel at the rate of fifty miles an hour with a forty-mile tail wind was, of course, no startling accomplishment.\nNo farmer could witness the destruction of his crops with equanimity, and from the outset of the plague plainsmen tried to devise ways to combat the locusts. Probably no living species has brazenly defied so many different attempts at extermination.\nThe first instinct of a property owner, upon noticing his wheat being eaten down to the nub, was to run out into the field, shouting and waving his arms. This procedure only mildly alarmed the locusts, who would surge into the air in small clouds and immediately settle back to earth and resume their interrupted meal.\nFrom this point, human ingenuity conjured up a multitude of devices and techniques both simple and complex to squash, bury, trap, burn, asphyxiate, trample, crush, drown, or poison the common enemy.\nOne of the simplest methods was the destruction of unhatched grasshopper eggs. Through some rough calculation it was widely advertised that a bushel of eggs removed from circulation equalled the saving of one hundred acres of corn. On this basis, states like Minnesota and Missouri offered bounties of up to five dollars—a fair weekly wage—for a bushel of locust eggs.\nThe bounty laws made strict provision for measuring the catch and insured that no egg was brought in twice for the reward. A bounty of one dollar per bushel was offered for young grasshoppers, the price being progressively reduced as the season wore on. It was the reasonable contention of the authorities that by June the average locust had eaten his fill. Locusts captured in May were redeemed at a mere quarter a bushel.\nNo public budget was ever seriously thrown out of balance by the payment of locust bounties. Though as many as twenty thousand eggs might be laid within the space of a square foot of ground, there was widespread misconception about the best place to find them. While the eager bounty hunters were meticulously sifting shovelfuls of earth scooped from the cultivated fields where the grasshoppers had been seen feasting, the majority of the embryos might be peacefully developing in the hard-packed adjacent pastureland.\nOnce the quarry was visible, pure manpower was brought into the fray: for example, a long wire wrapped in burning oil-soaked rags and stretched taut between two farmhands would be carried close to the ground over the fields where the infant grasshoppers were beginning to convene; from Europe was borrowed the design of a flat-bladed, shovel-like implement to flatten the locusts with lusty blows. This latter method facilitated collection of the bounty, but even at the top rate there was no giveaway involved: some seven thousand corpses were needed to fill a bushel basket.\nWould-be efficiency experts turned their livestock into the infested areas in the expectation that the hoofs of the cattle and swine would appreciably decimate the locusts. The hope proved vain. As for domestic fowl, they soon gorged themselves into a stupor without making substantial headway against the tide; further, they became “crop-bound” by the more indigestible parts of the insects. And too, the flesh of turkeys that had been on extended grasshopper-control duty became tainted and unfit to eat.","Objective of the Desert Locust Control Information Page\nTo provide vital information about descriptions, traits and control of the Desert Locusts to increase resilience of local communities to invasions by the same.\nCollaborating Partners and Featured Sources\nThe Ministry of Agriculture, Animal Industry and Fisheries Crop Protection Department: Web link here\nThe National Agricultural Research Organization (NARO): Web link here\nThe Food and Agriculture Organization of the United Nations (FAO): Web link here\nThe Desert Locust Control Organization for Eastern Africa: Web link here\nGeneral Features and Characterists of Desert Locusts\nWhat is a Desert Locust?\nThe desert locust (Schistocerca gregaria) locally known as emaathe, enzige, enzigye is a swarming short-horned grasshopper in the Acrididae family. As the world’s most dangerous migratory pest species, the desert locust reproduces rapidly, migrates over long distances and has the ability to change their behavior and appearance under particular environmental conditions.\nMigratory and Destructive Nature\nThe desert locust is herbivorous and can stay in one place for 17 hours and if strong enough can range 3,000 miles in their lifetime and an average of 50- 300km per day.\nLocusts can multiply 10-20 times per generation and an adult female will lay up to 900 in 3 months. A swarm size of a square kilometer devours 100-160 tons of vegetation (crops and pasture) per day. Locust movement is determined by wind patterns and vegetation availability and primarily feeds on green vegetation.\nAnswers to Frequently Asked Questions\nWhat is the difference between locusts and grasshoppers?\nLocusts are part of a large group of insects commonly called grasshoppers which have big hind legs for jumping. Locusts belong to the family called Acrididae. Locusts differ from grasshoppers in that they have the ability to change their behaviour and habits and can migrate over large distances.\nWhat countries are affected by the Desert Locust?\nDuring quiet periods (known as recessions) Desert Locusts are usually restricted to the semi-arid and arid deserts of Africa, the Near East and South-West Asia that receive less than 200 mm of rain annually. This is an area of about 16 million square kilometres, consisting of about 30 countries.\nDuring plagues, Desert Locusts may spread over an enormous area of some 29 million square kilometres, extending over or into parts of 60 countries. This is more than 20% of the total land surface of the world. During plagues, the Desert Locust has the potential to damage the livelihood of a tenth of the world's population.\nFor how long does a Desert Locust live?\nA Desert Locust lives a total of about three to five months although this is extremely variable and depends mostly on weather and ecological conditions. The life cycle comprises three stages: egg, hopper and adult. Eggs hatch in about two weeks (the range is 10-65 days), hoppers develop in five to six stages over a period of about 30-40 days, and adults mature in about three weeks to nine months but more frequently from two to four months.\nHow many eggs does a Desert Locust female produce?\nDesert Locust females lay eggs in an egg pod primarily in sandy soils at a depth of 10-15 centimetres below the surface. A solitary female lays about 95-158 eggs whereas a gregarious female lays usually less than 80 eggs in an egg pod. Females can lay at least three times in their lifetime usually at intervals of about 6-11 days. Up to 1,000 egg pods have been found in one square metre.\nHow far and how fast can Desert Locusts migrate?\nDesert Locusts usually fly with the wind at a speed of about 16-19 km/h depending on the wind. Swarms can travel about 5-130 km or more in a day. Locusts can stay in the air for long periods of time. For example, locusts regularly cross the Red Sea, a distance of 300 km. In the past there have been some spectacular and very long distance swarm migrations, for example from North-West Africa to the British Isles in 1954 and from West Africa to the Caribbean, a distance of 5,000 km in about ten days in 1988. Solitary Desert Locust adults usually fly at night whereas gregarious adults (swarms) fly during the day.\nHow big are swarms and how many locusts are there in a swarm?\nLocust swarms can vary from less than one square kilometre to several hundred square kilometres. There can be at least 40 million and sometimes as many as 80 million locust adults in each square kilometre of swarm.\nMore questions with accompanying answers from the Food and Agriculture Organization of the United Nations as provided for member countries here\nThe public can also call the Task Force Response Team Toll Free on 0800177777 to report the Desert Locust Invasion in your community.\nThis page was set up in fulfillment of knowledge sharing requirements of the National Task Force for the Control of the Desert Locust Invasion. The Inter-ministerial Task Force was set up with coordination from the Office of the Prime Minister and is led by the Ministry of Agriculture, Animal Industry and Fisheries with support from the Food and Agriculture Organisation of the United Nations, Uganda People's Defence Forces and Desert Locust Control Organization for Eastern Africa.\nFor update-related inquiries\nMr. Steven Byantwale\nCommissioner for Crop Protection\nMinistry of Agriculture, Animal Industry and Fisheries\nPrincipal Public Relations Officer\nNational Agricultural Research Orgnization"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:181137a1-5e01-4070-b523-eeddc70d909e>","<urn:uuid:28d440c5-38d3-4a0a-b706-18e6e60ab1c0>"],"error":null}
{"question":"As a wrestling historian, I'm comparing celebs who visited the troops - was Steve Martin's visit to Saudi Arabia troops in 1990 before or after Jerry Lawler's return to RAW after his heart attack?","answer":"Steve Martin's visit to U.S. soldiers in Saudi Arabia in 1990 occurred about 22 years before Jerry Lawler's return to RAW. Specifically, Martin visited troops with his wife Victoria Tennant in 1990, while Lawler returned to the announcer's table on Monday Night RAW in 2012 following his heart attack during a live RAW telecast in Montreal on September 10th.","context":["This Day In Wrestling History – November 12th\n1954 – Rito Romero & Pepe Mendietta defeat Roy Shire & The Great Scott, to win the NWA Texas Tag Team Championship.\n1965 – Mad Dog Vachon defeats The Crusher, to win the AWA World Heavyweight Championship. Vachon holds the title for exactly one year.\n1966 – And one year later, Dick The Bruiser defeats Mad Dog Vachon, to win the AWA World Heavyweight Championship.\n1971 – Luke Graham & Tarzan Tyler defeat The Mongols (Bepo & Geto), to win the WWWF International Tag Team Championship.\n1973 – Tank Morgan defeats Dewey Robertson, to win the vacant NWA (Tri-State) North American Championship.\n1979 – Bill Watts & Buck Robley defeat Mike George & Bob Sweetan, to win the Mid-South Tag Team Championship.\n1984 – Jerry ‘The King’ Lawler defeats King Kong Bundy, to win the AWA Southern Heavyweight Championship for the 39th time.\n2001 – On RAW, Edge defeats Kurt Angle, to win the WCW United States Championship. Also, The Hardy Boyz (Matt & Jeff) defeat Booker T & Test, to win the WWF Tag Team Championship.\n2006 – Vampiro defeats 6-Pac, to become the inaugural Wrestling Society X Champion.\n2010 – Impeccable (Ken Carson & Joey Figueroa) defeat Pretty Flawless (Jerome Daniels & Nobe Bryant), to win the NWA Texas Tag Team Championship.\n2010 – In a Six-Way Elimination Match, Eddie Edwards defeats Adam Cole, Claudio Castagnoli, Kenny King, Kevin Steen, and Rhett Titus, to win Ring of Honor’s Survival of the Fittest Tournament.\n2011 – New Japan’s Power Struggle event is held in Osaka, Japan in front of 6,000 fans.\n– Gedo & Jado defeat Tama Tonga & Killer Rabbit.\n– Tomoaki Honma & Wataru Inoue defeat Chaos (Hideo Saito & Takashi Iizuka).\n– Yuji Nagata defeats Tomohiro Ishii.\n– Suzuki-gun (Taichi & Yoshihiro Takayama) defeat Ryusuke Taguchi & Togi Makabe.\n– The No Remorse Corps (Davey Richards & Rocky Romero) defeat KUSHIDA & Tiger Mask IV, to retain the IWGP Junior Heavyweight Tag Team Championship.\n– Prince Devitt defeats Taka Michinoku, to retain the IWGP Junior Heavyweight Championship.\n– Tetsuya Naito & MVP defeat Chaos (Shinsuke Nakamura & Yujiro Takahashi).\n– Hiroyoshi Tenzan defeats Satoshi Kojima.\n– Masato Tanaka defeats Hirooki Goto, to retain the IWGP Intercontinental Championship.\n– Bad Intentions (Giant Bernard & Karl Anderson) defeat Suzuki-gun (Lance Archer & Minoru Suzuki), to retain the IWGP Tag Team Championship.\n– Hiroshi Tanahashi defeats Toru Yano, to retain the IWGP Heavyweight Championship.\n2011 – 4Loco (Azireal & Bandido Jr.) defeat Philly’s Most Wanted (Joker & Sabian), to win the CZW World Tag Team Championship. Also, Sami Callihan defeats Adam Cole, to win the CZW World Junior Heavyweight Championship.\n2012 – Jerry ‘The King’ Lawler returns to the announcer’s table on Monday Night RAW, for the first time since suffering a heart attack during a live RAW telecast in Montreal, on September 10th.\n2014 – On Impact Wrestling, The Revolution (James Storm & Abyss) defeat The Wolves (Eddie Edwards & Davey Richards), to win the TNA World Tag Team Championship.\n2016 – At SHIMMER 87, Kellie Skater defeats Mercedes Martinez, to win the SHIMMER Championship.\n2016 – After making a Donald Trump-inspired sexual joke during EVOLVE 72, announcer Joey Styles is fired by EVOLVE booker Gabe Saplosky. On November 14th, Beyond Wrestling & CHIKARA would also sever ties with Styles over his comments.\nHAPPY BIRTHDAY TO: Former ECW wrestler/valet Elektra (47 years old); former WWE diva & interviewer Lena Yada (39 years old); Tough Enough season three winner, Matt Cappotelli (38 years old); WrestleMania X guest timekeeper Rhonda Shear (63 years old); and former WCW Cruiserweight, Tag Team, & Television Champion, Disco Inferno (50 years old).\nToday would’ve been the 98th birthday for former AWA Tag Team Champion, Baron Gattoni. Today would’ve been the 105th birthday for famed Canadian boxing/wrestling promoter Frank Tunney. And finally, today would’ve been the 37th birthday for former CZW & ROH World Tag Team Champion, Trent Acid.","History of 16 October\n1916 – Margaret Sanger opened the first birth control clinic in New York City, NY.\n1923 – Walt Disney contracted with M.J. Winkler to distribute the Alice Comedies. This event is recognized as the start of the Disney Company.\nDisney movies, music, and books\n1928 – Marvin Pipkin received a patent for the frosted electric light bulb.\n1939 – “Right To Happiness” debuted on the NBC-Blue network.\n1939 – “The Man Who Came to Dinner” opened on Broadway.\n1941 – The Nazis advanced to within 60 miles of Moscow. Romanians entered Odesa, USSR, and began exterminating 150,000 Jews.\n1942 – The ballet “Rodeo” premiered in New York City.\n1943 – Chicago’s new subway system was officially opened with a ribbon-cutting ceremony.\n1944 – “The Robe,” by Lloyd Douglas, was published for the first time.\n1945 – “His Honor the Barber” debuted on NBC Radio.\n1955 – Mrs. Jules Lederer replaced Ruth Crowley in newspapers using the name Ann Landers.\n1962 – U.S. President Kennedy was informed that there were missile bases in Cuba, beginning the Cuban missile crisis.\n1964 – China detonated its first atomic bomb becoming the world’s fifth nuclear power.\n1967 – NATO headquarters opened in Brussels.\n1970 – Anwar Sadat was elected president of Egypt to succeed in Gamal Abdel Nassar.\n1973 – Henry Kissinger and Le Duc Tho were named winners of the Nobel Peace Prize. The Vietnamese official declined the award.\n1978 – Poland’s Karol Josef Wojtyla was elected Pope John Paul II.\n1982 – China announced that it had successfully fired a ballistic missile from a submarine.\n1987 – Rescuers freed Jessica McClure from the abandoned well that she had fallen into in Midland, TX. She was trapped for 58 hours.\n1989 – U.S. President George H.W. Bush signed the Gramm-Rudman budget reduction law that ordered federal programs to be cut by $16.1 billion.\n1990 – Comedian Steve Martin and his wife Victoria Tennant visited U.S. soldiers in Saudi Arabia.\n1993 – The U.N. Security Council approved the deployment of U.S. warships to enforce a blockade on Haiti to increase pressure on the controlling military leaders.\n1994 – German Chancellor Helmut Kohl was re-elected to a fourth term.\n1995 – The “Million Man March” took place in Washington, DC.\n1997 – Charles M. Schulz and his wife Jeannie announced that they would give $1 million toward the construction of a D-Day memorial to be placed in Virginia.\n2000 – It was announced that Chevron Corp. would be buying Texaco Inc. for $35 billion. The combined company was called Chevron Texaco Corp. and became the 4th largest oil company in the world.\n2002 – It was reported that North Korea had told the U.S. that it had a secret nuclear weapons program in violation of a 1994 agreement with the U.S.\n2002 – The Arthur Andersen accounting firm was sentenced to five years probation and fined $500,000 for obstructing a federeal investigation of the energy company Enron.\n2008 – The iTunes Music Store reached 200 billion television episodes sold.\n2013 – Lao Airlines Flight 301 crashes on approach to Pakse International Airport in Laos, killing 49 people.\n2017 – Storm Ophelia strikes the U.K. and Ireland causing major damage and power loss.\nCelebrating Birthday Today\n- 1981 – Brea Grant, American actress, and writer\n- 1981 – Martin Halle, Danish footballer\n- 1981 – Boyd Melson, American boxer\n- 1981 – Anthony Reyes, American baseball player\n- 1982 – Frédéric Michalak, French rugby player\n- 1982 – Cristian Riveros, Paraguayan footballer\n- 1982 – Prithviraj Sukumaran, Indian actor, singer, and producer\n- 1983 – Philipp Kohlschreiber, a German tennis player\n- 1983 – Kenny Omega, Canadian wrestler\n- 1984 – François Pervis, French track cyclist\n- 1984 – Rachel Reilly, American talk show host, and actress\n- 1985 – Jay Beagle, Canadian ice hockey player\n- 1985 – Verena Sailer, German sprinter\n- 1985 – Casey Stoner, Australian motorcycle racer\n- 1985 – Peter Wallace, Australian rugby league player\n- 1986 – Nicky Adams, English-Welsh footballer\n- 1986 – Derk Boerrigter, Dutch footballer\n- 1988 – Zoltán Stieber, Hungarian footballer\n- 1989 – Dan Biggar, Welsh rugby player\n- 1992 – Bryce Harper, American baseball player\n- 1992 – Kostas Fortounis, Greek footballer\n- 1992 – Stuart Lightbody, Irish badminton player\n- 1993 – Caroline Garcia, a French tennis player\n- 1994 – Adam Elliott, Australian rugby league player\n- 1997 – Charles Leclerc, Monégasque Formula One driver\n- 1997 – Naomi Osaka, Haitian-Japanese tennis player"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:118cb90a-d73e-425c-92d7-96bef7fe6056>","<urn:uuid:962c891e-2fe5-4c1b-864d-be540f49bbd8>"],"error":null}
{"question":null,"answer":null,"context":null,"question_categories":[],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":null,"error":"Conversation generation aborted while generating turn 1: It is impossible to generate a valid next turn even after 3 trials."}