{"question":"What are the mortality trends of liver cancer compared to other cancers in the US, and what dietary changes can help reduce the risk? List both aspects.","answer":"Between 1980-2014, liver cancer mortality increased by 87.6% in the US, while other major cancers saw significant decreases - lung cancer dropped by 69%, colon cancer by 35%, and breast cancer by 32%. To reduce liver cancer risk through diet, several changes are recommended: eating green vegetables (especially broccoli) to prevent fat buildup in the liver, consuming fatty fish (salmon, sardines, tuna, trout) high in omega-3 fatty acids to reduce liver fat levels, incorporating walnuts, using healthy oils like flaxseed oil, adding avocados which may slow liver damage, drinking green tea which may reduce fat storage in the liver, and limiting alcohol consumption. A healthy diet should be low in fat, high in nutrition, and avoid processed foods.","context":["World Cancer Day was observed on Saturday, February 4. Although the observance day has some and gone, cancer hasn’t; especially liver cancer. A recent article published by JAMA* reported that between 1980 and 2014, cancer mortality decreased by 20 percent in the United States. The number of deaths from nearly all the major forms of cancer have decreased significantly. The exceptions are liver, kidney, and non-Hodgkin lymphoma (NHL). Deaths due to NHL and kidney cancer remained the same. Liver cancer increased by 87.6 percent.\nThis is quite disturbing. The number of deaths from lung cancer dropped by nearly 69 percent; colon cancer dropped by 35 percent; breast cancer by 32 percent. Liver cancer is not following the trend.\nIt isn’t just in the US. World Hepatitis Alliance CEO, Raquel Peck wrote that 800,000 people died worldwide from liver cancer last year (Preventing Cancer Before It Strikes). Of these, 642,500 were related to hepatitis B and C virus. Actually, hepatitis C may cause many more cases of cancer and death from it than are captured by the data. First, risk of death is increased for all causes of mortality including other types of cancer among people with hepatitis C. Second, we know that people with hep C are at increased risk for risk of non-Hodgkin lymphoma and kidney cancer. These are the two other cancers mentioned in the JAMA article that did not decrease in incidence between 1980 and 2014.\nIn order to reduce the risk of liver cancer, we have to treat people with hepatitis C sooner, before they reach cirrhosis. However, if you already have cirrhosis, talk to your doctor about liver cancer screening recommendations. In the US, screening includes imaging (ultrasound, CT, or MRI) every 6 months.\nIf you have hepatitis B, your liver does not need to be cirrhotic in order to be at risk for liver cancer. Talk to your doctor about the current cancer screening recommendations for your risk factors.\nPeople with hepatitis C tend to have a higher risk for non-Hodgkin lymphoma in the early stages, before the liver is cirrhotic. This is one reason why we need to ramp up our viral hepatitis prevention message, screen more people and treat them in the earliest stages of hepatitis C.\nThe JAMA article pointed to obesity and poor diet as factors that may be fuelling the incidence of cancer. These lifestyle-related factors certainly have been implicated in liver cancer. Non-alcoholic fatty liver disease (NAFLD) is quickly becoming the most common liver disease in the United States. People with NAFLD are at increased risk of liver cancer. The same is true for alcoholic liver disease.\nSo, don’t wait around wondering if you might get liver cancer. Here are some ways to reduce your risk:\n- If you haven’t had hepatitis B or been immunized against it, get vaccinated.\n- If you have hepatitis B, talk to your doctor about how to manage it.\n- If you have any risk factors for hepatitis C, get tested and then treated if you have it.\n- Don’t drink too much alcohol.\n- Maintain a healthy weight and prevent diabetes.\n- Eat a low-fat, high nutrition\n- Aim for regular physical activity.\n- If you have cirrhosis or other risk factors for liver cancer, talk to your medical provider and ask about regular cancer surveillance.\n#WeCanICan stop liver cancer.\n*Trends and Patterns of Disparities in Cancer Mortality Among US Counties, 1980-2014 by Ali H. Mokdad, PhD, Laura Dwyer-Lindgren, MPH, Christina Fitmaurice, MD, MPH JAMA 2017","If you’ve always considered fatty liver disease to be an alcoholic’s burden, you may want to think again. There are actually 2 types of fatty liver disease – alcohol-induced and non-alcoholic fatty liver disease (NAFLD). Around 30% of Asian adults suffer from one of the 2 forms, and a recent study done by SingHealth suggests that the rate in Singapore could be even higher.\nToo much fat in the liver\nIf you have the disease, it means you have too much fat in your liver, which makes it less efficient at flushing out toxins from the body. More seriously, the disease can lead to liver scarring and eventually liver failure which is irreversible, so being in the know about how to lower your risk is valuable information indeed. While excess alcohol obviously plays a role in you falling foul of alcohol-induced fatty liver disease, other bad lifestyle choices increase your risk of developing NAFLD.\nWhile the exact cause is not clear, NAFLD is often diagnosed in people who are obese, lead a sedentary lifestyle and those who consume a highly processed foods diet. Patients with NAFLD also tend to have high blood pressure and high blood cholesterol levels.\nChange your diet for a healthier liver\nThe good news is you can adapt your diet to reduce your risks for both types of the disease. If you are overweight, you should target to lose at least 10% of your body weight by switching to a lower-calorie diet high in fresh vegetables, fruit and high-fibre plants, and by avoiding added sugar, salt, trans fat, refined carbohydrates and alcohol.\nEasier said than done, we know, which is why we’ve put together a little cheat sheet for you on foods that can help you reach your weight goals and combat a fatty liver.\nIf you are yet to be convinced of the health benefits of doing what your mum always urged and eating your greens, here’s one more reason to munch down beans, spinach, brussel sprouts and more green vegetables: tests have shown that broccoli helps to prevent the build-up of fat in the liver. Eating any greens can help you lose weight, which in turn will help your liver. It gives you early satiety (filling of fullness) and is low in calories. Its fibre-rich content can help you lower your blood cholesterol levels.\nIronically, eating fatty fish helps combat a fatty liver. Salmon, sardines, tuna, and trout are all high in omega-3 fatty acids, which can help lower the levels of fat in the liver and can bring down inflammation. It is also a healthy alternative to meat as it gives you the opportunity to avoid consuming the fats and skins from meat or poultry.\nIf you are not a keen fish eater, you can still get your omega-3 fatty acids from walnuts. They are great to eat as a healthy snack, add to your stir fry for that extra texture or sprinkle over your favourite salad. Limit portion to a small handful per day.\nWhen you need to control your weight and lower liver enzyme levels, you can’t go wrong by reducing the amount of fat/oil in your diet. When choosing fat/oil, opt for healthier unsaturated type. Flaxseed oil is an example of unsaturated oil. It is a good source of omega-3 fatty acids. You can dash over a salad, drizzled over bread or used for light frying adds a distinct flavour to all your dishes without you feeling guilty for the indulgence.\nNow we are starting to see a pattern. Here’s another delicious food choice that’s high in healthy fat, yet great for heart and liver health. Avocado appears to contain chemicals that might slow liver damage. Spread a small amount over toast with lemon juice and black pepper, add to a sandwich or wrap, or get really adventurous with the many avocado recipes you can find online.\nProtein helps repair body tissue. Milk is a great source of protein, go for skimmed or fat-free version. Or why not splash it on a healthy oatmeal breakfast in the morning to set yourself up for the day?\nCoffee often gets a bad rap, but the good news is that it isn’t always the enemy to health it’s sometimes made out to be. Coffee appears to lower the amount of abnormal liver enzymes in people at risk for liver diseases. Just make sure you do not add unnecessary creamer or sugar to increase the calories. Instead, using low fat milk, and artificial sweeteners if you cannot fight your sweet tooth.\nVitamin E is a powerful antioxidant that could protect the liver from damage, and sunflower seeds are a tasty little source of this important vitamin. Not only do they offer protection, but they add a wonderful texture when sprinkled over a salad, or to munch on as a healthy mid-day snack but in small amount.\nIf you have fatty liver disease and are not a great fan of garlic, garlic powder supplements may be your answer to reducing fat levels, if experimental studies are to be believed. Nevertheless, garlic helps flavour your dishes, so you don’t end up using too much sauce, which are also excessive calories.\nNew studies seem to point to green tea reducing fat storage in the liver and improving liver function. While this research is in its early stages, there’s no harm in drinking green tea anyway for all the health benefits we know it has, including lowering cholesterol and helping you sleep – and a regular good night’s sleep does wonders for keeping your weight down too.\nSo there you have it – 10 great suggestions for adding liver-preserving ingredients into your diet – all readily available on the shelves of your local supermarket.\nArticle contributed by Louis Yap, dietitian at Mount Elizabeth Novena Hospital\nAbdel-Hamid, M., Dawood, A. I. A., Hamad, E. M., Sitohy, M. Z. & Taha, S. H. (2011). Protective effect of whey proteins against nonalcoholic fatty liver in rats. Lipids in Health & Disease (10:57). Retrieved May 9, 2018, from https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3096574\nAntonypillai, C., Garcia, M. C., Gupta, V., Mah, X. & van der Poorten, D. (2015). Oily fish, coffee and walnuts: Dietary treatment for nonalcoholic fatty liver disease. World Journal of Gastroenterology (21:37). Retrieved May 9, 2018, from https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4588084\nAskari, G., Feizi, A., Iraj, B., Paknahad, Z. & Soleimani, D. Effect of garlic powder consumption on body composition in patients with nonalcoholic fatty liver disease: A randomized, double-blind, placebo-controlled trial. Advanced Biomedical Research 27 (5:2). Retrieved May 9, 2018, from https://www.ncbi.nlm.nih.gov/pubmed/26955623\nAvocados Contain Potent Liver Protectants. (2000, December 20). Retrieved April 17, 2017, from https://www.sciencedaily.com/releases/2000/12/001219074822.htm\nChen, Y.J., Walling, M. A. & Jeffery, E. H. (2016). Dietary Broccoli Lessens Development of Fatty Liver and Liver Cancer in Mice Given Diethylnitrosamine and Fed a Western or Control Diet. The Journal of Nutrition, 146 (3). Retrieved May 9, 2018, from https://www.ncbi.nlm.nih.gov/pubmed/26865652\nKhalik, S. (2017, January 3). Cases of Fatty Liver Disease Rising in Singapore: Study. Retrieved April 17, 2017, from http://www.straitstimes.com/singapore/health/cases-of-fatty-liver-disease-rising-in-spore-study\nLu, W., Li, S., Li, J., Wang, J., Zhang, R., Zhou, Y., Yin, Q., Zheng, Y., Wang, F., Zia, Y., Chen, K., Liu, T. Lu, J., Zhou, Y. & Guo, C. (2016). Effects of Omega-3 Fatty Acid in Nonalcoholic Fatty Liver Disease: A Meta-Analysis. Gastroenterology Research and Practice. Retrieved May 9, 2018, from https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5019889/\nNon-alcoholic Fatty Liver Disease. (n.d.). Retrieved April 17, 2017, from https://www.mayoclinic.org/diseases-conditions/nonalcoholic-fatty-liver-disease/diagnosis-treatment/drc-20354573\nOmara-Otunnu, E. (2009, February 9). Nutritional Scientist Studies Impact of Green Tea on Disease. Retrieved April 17, 2017, from today.uconn.edu/2009/02/nutritional-scientist-studies-impact-of-green-tea-on-liver-disease/"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:5a7b661b-0746-4dbf-9cc9-f1272d3155b4>","<urn:uuid:f31211df-3ca7-44b4-80f1-15eb52288f8d>"],"error":null}
{"question":"Do goldfish sleep with their eyes closed, and when do they typically rest?","answer":"Goldfish don't have eyelids and cannot close their eyes, but they do sleep. They don't naturally sleep at night, though they sleep better in dark and quiet conditions. While sleeping, they become less active and stay in one place, moving slowly only to maintain stability. Their sleep can be trained by maintaining consistent light cycles, and they may seek out dark places like plants if they can't find darkness to sleep.","context":["Goldfish are extremely popular pets, whether they are living in a fish aquarium inside a home or a pond outside in a yard. But there is one question that goldfish owners often ask—do goldfish sleep?\nGoldfish do not have eyelids and cannot close their eyes, but they do, in fact, sleep—just not in the same way we do.\nWhat Do Goldfish Look Like When They Sleep?\nUnlike people, goldfish do not lie down when they sleep. Rather, they become less active, staying in one place and moving slowly to keep themselves stable. They look like they are hovering in the tank or pond, usually low in the water, an inch or so off the bottom, with their heads pointed slightly downward. Their color may fade a bit while they are sleeping and will return to normal when they are awake. They change color as a safety measure to hide themselves from predators when they sleep. Finally, goldfishes’ brainwaves don’t change when they sleep, and goldfish don’t enter into periods of deep, REM sleep, as people do.\nWhen Do Goldfish Sleep?\nGoldfish don’t naturally sleep at night, like people do. They do sleep better when it’s dark and quiet, so many fish will sleep at night. If you make noise around a sleeping fish, it will startle awake. Thus, it’s best to keep the noise level down when your fish wants to sleep. If you keep a light on in the tank, you can train fish to sleep at night, when you sleep, and stay awake during the day. If you turn on and off the light at the same time every day, goldfish will typically follow the same sleep pattern. The light should not be on for more than 12 hours a day, or fish may not get enough rest. If it isn’t dark enough for them to sleep, they may hide in plants to seek out darkness to try to sleep.\nWhat Happens When Fish Don’t Get Enough Sleep?\nJust like people, fish need sleep to restore energy to their body systems and to maintain proper immune function. If they don’t sleep enough, they lose their ability to fight off infections and their metabolism slows.\nHow much sleep they need depends on the fish. Some goldfish nap in the afternoon, while others stay awake until night time. Being exposed to regular day and light cycles is essential to fish getting enough sleep and remaining healthy. Finally, yawning isn’t a sign of tiredness in fish but rather is just the fish clearing its gills with water.\nHow Can You Distinguish a Sleeping Fish From a Sick One?\nSleeping fish remain stationary but upright; they do not turn sideways or upside down. A fish that is leaning, is upside down, or lying on the bottom isn’t sleeping but is likely sick. Fish with disease of the swim bladder—the organ that helps them keep buoyant—will often float sideways or upside down and have problems swimming. Lying on the side also may be an indication of a bacterial infection or high concentrations of nitrite or ammonia in the water. Regardless, if a fish owner sees his fish floating sideways or upside down, he should have the fish checked out by a veterinarian as soon as possible.\nSo, if you see your fish hovering above the tank floor, looking a little pale, turn off the light, turn down the noise, and let him have a power nap. He will be healthier and happier for it.\nImage via darkocv/Shutterstock.com"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:cf0444a0-1eb8-400f-8f33-1177e004af06>"],"error":null}
{"question":"What restoration techniques are being used to improve both Wagner Creek and South Scappoose Creek, and what's the expected environmental impact?","answer":"Both creeks are undergoing comprehensive restoration efforts. In Wagner Creek, techniques include removing fish passage barriers, installing French drains for stormwater management, creating heavy use areas to prevent erosion, implementing streamside forest restoration with 9,000 native trees and shrubs, and converting irrigation systems. The expected environmental impacts include improved fish passage, enhanced water quality, reduced erosion, and better fish habitat. For South Scappoose Creek, restoration involves bank laybacks to minimize erosion, creating floodplain benches for seasonal flood flows, reconnecting side channels, and planting 9,000 native trees and shrubs along the riparian corridor. These efforts are expected to create a healthy environment for fish and wildlife diversity, particularly benefiting Coho salmon and Steelhead populations, while also helping to reduce flooding impacts on adjacent properties.","context":["Wagner Creek SIA\nThe Wagner Creek SIA, selected in 2015, included properties within the Lower Wagner Creek Sub-Watershed of the Bear Creek Watershed in Jackson County.\nWagner Creek SIA contains approximately 1,000 total agricultural acres, consisting primarily of vineyards, orchards, and small acreage agriculture. Primary water quality concerns in this watershed include temperature and nutrients.\nTypes of projects completed in the Wager Creek SIA to address these water quality concerns include: Irrigation Improvement, Heavy Use Area Protection, Drainage Improvement, Manure Management, Riparian Restoration, and Fish Passage Barrier Removal.\nPartners involved on the Wagner Creek SIA include: JSWCD, RRWC, ODA, OWEB, and many others.\nInterested in specific project examples or before and after photos from our past SIA projects?\nKeep reading about the Wagner Creek SIA below, and check out the Neil Creek SIA page for more!\nWagner Creek SIA Success Stories & Project Photos\nCheck back soon for project photos!\nWagner Creek SIA Project #1\nType of Project: Heavy use area protection, manure storage, subsurface drainage\nLead Organization & Staff: Jackson SWCD: Clint Nichols, Jenna Sanford\nPartner: Rogue River Watershed Council\nFunders: Oregon Watershed Enhancement Board, Jackson SWCD, landowner\nTimeline: Planning & Development: February 2016 – September 2016\nImplementation: September 2016 – September 2017\nProblem: The landowner owns one acre of rural property within the Wagner Creek watershed. She uses the western half-acre of the property to board two horses. This property came under the scrutiny of Oregon Department of Agriculture because of apparent bare ground visible from aerial photos. The western half of the property lies within a drainage area that moves stormwater from impervious surfaces south of her property to a drainage area to the north along Anderson Creek Road, eventually discharging into Wagner Creek. Additionally, a neighbor’s pond along the south fence leaks water onto her property, creating a year-round source of runoff. Other properties within this drainage area have pipes to move water through the area without saturating the ground; this property does not have such a pipe. Therefore, stormwater diverted through the culvert above completely saturated most of the field, making plant establishment and manure management impossible. The stormwater would eventually leave the property, potentially carrying with it excess sediment from the bare ground and bacteria from the manure left in the field, making its way to Wagner Creek where these pollutants were degrading water quality for fish and human uses. These pasture conditions also created a health hazard for the horses as manure mixed with mud creating a breeding ground for parasites and poor conditions for healthy hooves. Buttercup (Ranunculus sp.), a plant poisonous to horses, dominated the drainage area due to lack of competing vegetation.\nProject Description: We installed a French drain to collect stormwater where it enters the property and diverted it into a pipe beneath the pasture to keep the soils dry. A heavy use area was created for the horses for times of wet weather. Moving horses off wet pastures prevents damage due to hoof traffic. We also built a manure storage facility to allow the landowner to collect the manure off the heavy use area and pasture and store it where it won’t create the potential for water quality degradation. Finally, we planted the pasture with suitable perennial forage that will stabilize the soil and allow the horses to pasture without creating an erosion or water quality concern.\nEcologic Impact: Improved water quality in Wagner Creek; reduced pasture soil erosion.\nEconomic Impact: Improved conditions for horse health; reduced feed costs for horses.\nWagner Creek SIA Project #2\nType of Project: Fish passage barrier removal\nLead Organization & Staff: Rogue River Watershed Council: Brian Barr, Alexis Larsen\nPartner: Beeson-Robison Ditch Association\nFunders: Jackson Soil & Water Conservation District, Middle Rogue Steelheaders, Oregon Watershed Enhancement Board, Pacific Power Blue Sky Fund/The Freshwater Trust, Patagonia Environmental Grants, Resources Legacy Fund, Rogue Basin Partnership, Rogue Flyfishers, Schwemm Family Foundation, Southern Oregon Flyfishers, Trout & Salmon Foundation, WaterWatch of Oregon\nTimeline: Planning & Development: 2014 – August 2017\nImplementation: September 14 – October 4, 2017\nPlanting: March 2018\nProblem: Wagner Creek is one of the most valuable fish-bearing streams in the Bear Creek watershed. Currently, summer and winter steelhead are known to use Wagner Creek and there is strong potential for Coho Salmon to use the creek as well. The Beeson- Robison dam, a 5.5-foot-tall concrete structure, presented a complete barrier to juvenile fish and prevented upstream migration of fish in most flow conditions. Oregon Department of Fish and Wildlife had listed this dam as a high-priority fish passage barrier in the Rogue River Basin. Removal of this dam would improve the ability of adult and juvenile steelhead, and potentially adult and juvenile Coho Salmon, to access approximately three miles of upstream habitat for cool water summer refuge and spawning during irrigation season.\nProject Description: In September 2017, on-ground work was begun to remove Beeson- Robison Dam. Rogue River Watershed Council hired local contractor Todd Marthoski of M&M Services, LLC to complete the project. The dam was removed with heavy equipment in a few hours on September 28, 2017. Within 4 days, a new 115-foot-long roughened channel, which mimicked a natural streambed while providing fish passage, was installed. At the top end of the channel, rocks were strategically placed to direct stream water into a new intake system for providing irrigation water to 18 irrigators on the Beeson-Robison Ditch. The new intake system was installed approximately 80 feet upstream from the dam site. New pipe was installed and connected to the existing irrigation ditch structure and fish screen. In March 2018, 50 native shrubs were planted in the riparian area to provide shade for cooling the water.\nEcologic impact: Improved fish passage; improved juvenile survival during hot summer months; three miles of spawning habitat opened up.\nEconomic impact: Efficient irrigation management; reduced maintenance labor.\nWagner Creek SIA Project #3\nType of Project: Streamside forest restoration\nLead Organization & Staff: Rogue River Watershed Council: Sarah Sauter\nPartners: Jackson SWCD, Plant Oregon, City of Talent, and seven neighbors\nCost: $136,515 ($124,865 cash and $11,650 in-kind labor)\nFunders: Oregon Department of Agriculture, Jackson SWCD, Patagonia Environmental Grants, Schwemm Family Foundation, Ashland Garden Club\nTimeline: October 2015 - May 2018 (Plant maintenance scheduled through 2022)\nProblem: The Wagner Creek valley is a highly productive agricultural area that is rapidly being converted from orchards to small acreage rural residential agriculture. Some management practices have produced conditions (narrow strips of native riparian vegetation, extensive areas of Himalayan blackberries, bare ground, manure concentration) that have reduced water quality and degraded fish habitat in Wagner Creek. Water quality limitations affecting native fish species such as steelhead and Coho Salmon include temperature, bacteria, nutrients, and dissolved oxygen. The Oregon Department of Agriculture recently identified the Wagner Creek watershed as a Strategic Implementation Area (SIA) for water quality concerns related to small acreage agriculture and vineyards.\nProject Description: This streamside forest restoration project addressed concerns at eight properties along Wagner Creek, including two that Oregon Department of Agriculture ranked as being of “significant concern.” Rogue River Watershed Council contracted Plant Oregon, a local nursery, to remove approximately 12 acres of blackberries along a 0.6-mile reach of Wagner Creek. Plant Oregon employed a combination of mechanical, hand, and herbicide treatments. Disturbed areas were protected with quick-growing annual grasses. They installed approximately 9,000 native trees and shrubs at a density of 747 plants/acre. They also installed approximately 700 willow stakes to stabilize steep banks. Most of the plant plugs were installed with a tractor, supplemented by hand-planting in steep areas close to the bank. Organic fertilizer was applied to each plant, along with a Vis-Pore plastic mulch and bamboo stake for protection from weeds and animals. A drip irrigation system was installed beneath the Vis-Pore mulch. The shared irrigation system utilizes landowner water rights from two ditch systems (Beeson-Robison and Talent Irrigation Ditch), three pumps, two bulges, and a gravity-fed pipe to provide reliable irrigation water to the project. We also procured fencing materials so the landowner could install fencing to keep livestock out of the creek on this particular property.\nEcologic Impact: Improved water quality, improved fish habitat\nEconomic Impact: Awarded $123,420 in local contracts\nWagner Creek SIA Project #4\nType of Project: Fish passage barrier removal, irrigation conversion\nLead Organization & Staff: Jackson SWCD: Paul DeMaggio, Clint Nichols, Jenna Sanford\nPartner: Rogue River Watershed Council\nFunders: Oregon Watershed Enhancement Board, Oregon Department of Fish and Wildlife, Jackson SWCD\nTimeline: Planning & Development: May 2015 – May 2017\nImplementation: June 2018 – September 2018\nProblem: The landowner owns 17 acres on Wagner Creek with water rights from the creek. A push-up dam on Wagner Creek 3,300 feet upstream of the property diverted water into an open ditch that travels across several other properties before ending in a large pond on the property. This pond has been historically used to flood irrigate a three- acre hay field. During the summer, the dam created a fish passage barrier to summering steelhead, and in the winter, if the dam stayed in place, presented a barrier to other salmonid species as well. Flood irrigation practices can lose more than half of the water diverted to the field for irrigation, and in this case, that runoff re-entered Wagner Creek, carrying with it whatever washed off the fields. Likely contaminants included bacteria from manure, fertilizers and pesticides, and sediment from eroding soils. Runoff water has higher temperatures than in-stream water, and increased water temperatures can create unlivable conditions for fish and lead to harmful algal blooms in the summer. The 3,300 feet of open ditch presumably lost water to evaporation, transpiration, and seepage, and was difficult to maintain given the number of other properties the ditch crosses. The lack of irrigation infrastructure limited the ability of the landowner to irrigate the rest of her property, limiting crop production.\nProject Description: Two pumping stations were installed on the landowner’s property to remove the need for building and maintaining a push-up dam further upstream. This removed a fish passage barrier and also allowed the landowner to put her water right to beneficial use. We also developed a series of irrigation systems to provide water to several different crop types and replace the existing dependence on flood irrigation. This removed the potential for water quality impairment by eliminating return flows to Wagner Creek, and allows the landowner to provide water to her entire property to grow crops and improve the soil.\nEcologic Impact: Improved fish passage; improved water quality in Wagner Creek; reduced erosion in crop fields.\nEconomic Impact: Improved crop production; reduced maintenance labor; efficient use of irrigation water.","South Scappoose Creek Restoration\nStreamside restoration activities along the South Scappoose Creek through the City of Scappoose, have been approved. The creek has historically provided habitat for Coho salmon and Steelhead, but over time, changing conditions of the creek have had a negative impact on the health of the stream. This restoration project will restore and improve this critical fish and wildlife habitat, while addressing erosion and flooding concerns.\nProject Update – Fall 2018:\nConstruction for this project started in July 2018 and was completed in September 2018. Riparian planting will continue through Spring of 2019. (See Photo Gallery below)\nOver time, conditions of the South Scappoose Creek have changed due to a number of natural factors. Some of these factors include erosion, stream channels being confined or cut off from the creek, changes to the stream bed itself, and an increase of invasive plant species along the creek. As a result, stream water temperatures have increased, banks have become less stable, and it is difficult to access the creek. All of these changes have had a negative impact on the health of the stream and riparian habitat.\nThe South Scappoose Creek Stream Restoration project will be beneficial in several ways. It will enhance fish habitat by improving conditions and creating a healthy environment to sustain fish and wildlife diversity, including significant Coho salmon and Steelhead numbers. In addition, it will make the creek more accessible to the public, as well as create a larger floodway that will help reduce impacts to adjoining properties during flood events.\nGrant Concept & Design\nWorking with the City of Scappoose, the Scappoose Bay Watershed Council (SBWC) has been awarded a Bonneville Power Administration (BPA) Grant of $265,865, to implement stream restoration activities. The funding is part of a Willamette Mainstem Anchor Habitat Investments Program grant that includes $95,813 for a restoration project on the lower North Scappoose Creek in 2019. The City of Scappoose is contributing over $75,000 to the project as well.\nTwo previous studies of South Scappoose Creek, done in 2009 and 2013, developed restoration designs to address stream conditions. Proposed actions include bank laybacks to minimize active bank erosion and provide channel capacity during high flows, floodplain benches to increase floodplain interaction during seasonal flood flows, and side channel reconnections to access historic off-channel areas.\nThis project will do additional hydrologic surveys and modeling, final engineering designs, and construction of restoration activities. Following restoration construction, any remaining invasive plants will be removed, and the area will be planted with up to 9,000 native trees and shrubs along the riparian corridor.\nThe project area spans two parcels – the creek through Veteran’s Park, as well as the parcel immediately upstream of the park, south of SW JP West Road. All work is being done on the west side of the creek.\nThe project is scheduled to complete designs and permitting in winter 2018, with construction in mid-to-late summer 2018. Vegetation planting will occur between November 2018 and April 2019.\nSupport from key SBWC partners – the BPA and the City of Scappoose – has been essential for this project. Additional project partners include the Columbia Soil and Water Conservation District, Oregon Department of Fish and Wildlife, and the Oregon Watershed Enhancement Board.\nThe SBWC will be working on outreach and education and expects to have volunteer opportunities for community members interested in the restoration – particularly with the native vegetation planting activities.\nProject Photo Gallery:\nFor more information contact Pat at the Scappoose Bay Watershed Council at 503-397-7904 or email: firstname.lastname@example.org."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:70cb1a2c-bc6b-4968-a9bf-12d4758a8940>","<urn:uuid:b5434518-b27d-4acc-8cfe-538d283fbfd2>"],"error":null}
{"question":"How do Jewish environmental principles compare to Salt Lake City's sustainability recognition?","answer":"Jewish environmental principles are rooted in religious concepts like Tikkun olam (repairing the world) and biblical mandates to be environmental stewards, emphasizing modest consumption and land preservation through practices like shmittah. In comparison, Salt Lake City's environmental efforts have earned secular recognition, including awards from the Department of Energy's Better Buildings initiative, designation as a White House Climate Action Champion community, and recognition as one of the top 20 most resilient cities by ICLEI. Both approaches, though different in origin, promote environmental stewardship and sustainable practices.","context":["Why it’s Jewish to be Green\nWhen we revisit the basic rules of Judaism, it is clear where our duty lies.\nBy Naomi Tsur, Deputy Mayor of Jerusalem\nTo many this may seem to be a stupid, or rather a redundant question: Should Jews support democracy, freedom of speech and freedom of worship? I am sure that no one will dispute the fact that climate change, alongside the dwindling of the world’s resources, constitutes one of the burning issues of our time. So let me rephrase the question: Should we be committed environmentalists because we are Jews, and does our faith and its practices prescribe our undertaking to address these issues, as part of our obligation to repair the world?\nTikkun olam is one of the most deeply entrenched ethical concepts of Judaism, to be achieved through the pursuit of social justice, by restoring the balance of our world, and by giving to those who are in need. If we simply apply this concept to our physical world, and not only to the moral and social spheres, we will find ourselves in the front lines of environmentalists in the world.\nThe basis for defining our obligation as Jews to take responsibility for our physical and social environment is to be found in many sources. When we revisit the basic precepts of Judaism, and examine them through an environmental lens, it is clear where our duty lies. The threat that climate change poses to life on Earth as we know it is in effect a threat to God’s work of creation, a clear call that we have failed in our duty as stewards of God’s work.\n“The Earth is the Lord’s and the fullness thereof, the world and those who live in it.” (Psalms 24:1). In the Garden of Eden, where no work was officially required of Adam and Eve, they were enjoined to “tend and guard” the garden (Genesis 2:15). This is the first specific reference to the effort we must invest in preserving and sustaining our environment. In expansion of this theme, we find the famous exhortation in later exegesis of this verse. God said to Adam in the Garden of Eden: “Observe how beautiful is the work of my creation. Take care not to destroy it, for no one will repair it after you.” (Ecclesiastes Rabbah 7:13)\nThis last exhortation echoes down the corridors of time, some 2,000 years. Written at a time when there was no danger or threat to any of the world’s ecosystems, it takes on an eerie connotation in our time, when irreversible melting of the ice caps, loss of forest cover, loss of species and increasing desertification are part of our world, alongside horrendous weather episodes that cause a scale of loss of life and livelihood that are hard to grasp. Scientists are now convinced that human behavior has been a serious factor in climate change, and that the emissions caused by fossil fuels are one of the main causes of climate change because of the impact on and irreversible damage to the atmosphere.\nOur sages recommended that we “walk humbly” (Micah 6:8) on the Earth, and this is in keeping with the understanding that one of the keys to preventing further deterioration of the global environment is the adoption of more modest habits of consumption.\nSince we are the stewards of God’s great work of creation, and in view of the fact that our behavior is damaging the ecosystems that support life on Earth, it is undoubtedly our duty as Jews to cut down our consumption, using only what we really need, and to make every effort to use cleaner sources of energy that can provide for our needs without inflicting irreparable damage on the life-support systems of the world.\nThe case of the state of Israel, a Jewish and democratic state, is surely a special one in many regards, but perhaps also with regard to our responsibility for the environment. Many mitzvot—commandments—that apply to the land customarily are observed only in Israel. An excellent example is shmittah, the sabbatical year, when the land lies fallow, preventing overuse of agricultural land and allowing natural recovery. A major challenge for modern Jewry is to restore true observance of shmittah, since the modern state of Israel has not found a way of honoring the requirement to let the land rest, but has rather found ways around the law.\nThe mitzvot pertaining to agriculture are other examples of laws that traditionally only apply in Israel. And holidays such as Tu B’shvat, the new year for trees, take on special meaning in a country that lost nearly all of its original forest cover in Byzantine times, and where reforestation in the last century has had a positive climate impact.\nDuring the years that I worked as director of urban centers for the Society for the Protection of Nature in Israel, before being elected to the Jerusalem City Council and appointed deputy mayor, I came to understand that not only is it inappropriate to treat environmentalism as a secular subject, for all the reasons I have given here, but that focusing on goals of sustainable development for Israel can give a new and more relevant agenda to Zionism, while fostering an innovative environmental dialogue between Israel and the global Jewish community. The three pillars of sustainable development—economic, social and environmental strength—surely provide an excellent menu for future prosperity for Israel, and for Jews around the world.\nThe article above was reprinted with permission from the Green Zionist Alliance and Coalition on the Environment and Jewish Life (COEJL). The Jewish Energy Guide is part of the Coalition on the Environment and Jewish Life’s Jewish Energy Network, a collaborative effort with Jewcology’s Year of Action to engage Jews in energy action and advocacy. The guide was created in partnership with the Green Zionist Alliance. The Jewish Energy Guide presents a comprehensive Jewish approach to the challenges of energy security and climate change and offers a blueprint for the Jewish community to achieve a 14% reduction in greenhouse gas emissions by September of 2014, which is the next Shmittah, or sabbatical, year in the Jewish calendar.\nAbout the Author\nNaomi Tsur is the deputy mayor of Jerusalem, where she heads the city’s environment, urban planning and historic conservation committees. In her tenure, she has launched Jerusalem’s light rail system, developed a citywide bike path circuit, limited private car use in the Old City and initiated a dialogue with Palestinians about how best to handle waste. Tsur previously directed the Jerusalem branch of the Society for the Protection of Nature in Israel and founded the Sustainable Jerusalem Coalition, which successfully campaigned to protect the Jerusalem Hills. She speaks about Israel’s environment around the world. Tsur also serves on the advisory board of the Green Zionist Alliance.","Salt Lake City Green is comprised of award-winning environmental programs that help the community conserve resources, reduce pollution, slow climate change and ensure a healthy, sustainable future for Salt Lake City. Salt Lake City Green, or SLCgreen, is the public face of Salt Lake City’s Sustainability Department.\nThe Salt Lake City Sustainability Department was created in July 2016 under Mayor Jackie Biskupski to emphasize the City’s commitment to improve air quality and protect our natural resources. Prior to becoming a stand-alone department, Sustainability was one of four divisions housed within the Public Services Department.\nConnect with Us\nFollow SLCgreen on Facebook, Instagram, and Twitter. You can also stay up to date with the latest sustainability happenings by following the SLCgreen blog, or subscribing to our weekly newsletter.\nOrganization & Management\nThe Waste & Recycling Division is dedicated to providing convenient, accessible and outstanding service to all who live, work and play in Salt Lake City, while providing a safe and positive work environment for employees, and maintaining their core values of fiscal integrity and sustainable waste diversion.\nThe W&R Division runs Salt Lake City’s curbside refuse programs, including weekly waste collection services for approximately 45,000 households, the Call 2 Haul bulk waste program, Christmas tree collection, and container maintenance programs. The Division also oversees outreach for and compliance with the City’s Business & Multi-Family Recycling ordinance; Construction & Demolition debris ordinance; and Special Events waste permitting.\nThe Energy & Environment (E&E) Division ensures that all Salt Lake City municipal departments comply with all applicable environmental regulations. They also guide City-wide policy and practice to minimize the environmental impact of the community and of City operations. The E&E Program specifically focuses on Environmental Compliance, Climate Mitigation and Adaptation Strategies, Air Quality, Energy Conservation, Renewable Energy, and Food Security.\nPlans and Years in Review\nExplore our Climate Positive plan to learn more about our goals and priorities.\nYou can also read our 2021, 2020, 2019, 2018, 2017, and 2016 Years in Review.\nSLCgreen: (801) 535-6470 | email@example.com\nWaste & Recycling: (801) 535-6999 | firstname.lastname@example.org\nMeet the Team\nEnergy & Environment Division\n- Debbie LyonsDepartment Director\n- Sophia NicholasEnergy & Environment Division Director\n- Christopher ThomasSenior Energy & Climate Program Manager\n- Peter NelsonSustainability Program Manager\n- Catherine WyffelsEnvironment & Air Quality Manager\n- Maria SchwarzFood & Equity Program Manager\n- Angie BrohamerFinancial Analyst\n- Monica O'MalleySpecial Projects\n- Alma BasteOutreach Coordinator\nWaste & Recycling Division\n- Debbie LyonsDepartment Director\n- Chris BellWaste & Recycling Division Director\n- Cory YoungProgram Manager\n- Frank YoungOperations Manager\n- Cliff KanoMaintenance Supervisor\n- Joe TrujilloMaintenance Supervisor\n- Ashley BaileyEducation and Permits Lead\n- David JohnstonPermit Coordinator\nSustainability Awards & Recognition\nThe City has received recognition and awards related to climate preparedness, resiliency, and sustainability.\n- The Department of Energy’s Better Buildings initiative has recognized several Salt Lake City programs with awards, including for our Executive Order for Energy Management of City Facilities (see here) and the retro-commissioning of the Public Safety Building (see here).\n- Utah Business Magazine recognized the Climate Positive Plan in 2017.\n- White House Climate Action Champion community.\n- White House Task Force on Climate Preparedness and Resilience. Salt Lake City served on the White House Task Force on Climate Preparedness and Resilience.\n- Mayors’ Climate Protection Award. Salt Lake City was awarded the Mayors’ Climate Protection Award 2013 by the U.S. Conference of Mayors and Wal-Mart for his leadership to institute innovative practices that reduce GHG emissions.\n- Seventh Most Influential City For Policy Ideas. Salt Lake City ranked as the seventh most influential city in the nation for policy ideas in the study, “Mayoral Policy Making: Results from the 21st-Century Mayors Leadership Survey” (Boston University, October 2014). Salt Lake City was the smallest municipality among the top 10 biggest influencers.\n- Top 20 Most Resilient Cities. International Council for Local Environmental Initiatives (ICLEI) – Local Governments for Sustainability USA named Salt Lake City among the top 20 most resilient cities on the “front line” of dealing with the multi-pronged challenges associated with climate change and extreme weather events in 2013.\n- Barrier Buster and Mountain Mover. The City received the U.S. Department of Energy Solar America “Barrier Buster” and “Mountain Mover” awards for its progress in solar energy.\n- Top 10 Sustainable Cities. The City was recognized by Moyers and Company in January 2013 as one of the 12 top cities leading the way in sustainability.\n- Utah Sustainable Business Leadership Award. Sustainability Director Vicki Bennett was awarded the 2014 Utah Sustainable Business Leadership award from Utah Business Magazine."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:34ead8af-03c1-4423-9d1f-6710911066e3>","<urn:uuid:07ba4813-e814-4d7a-ad1f-6daf5d605d13>"],"error":null}
{"question":"How do the historical developments of coffee cultivation in Panama and Ethiopia compare, considering their significance in coffee history?","answer":"Ethiopia's coffee history begins with the legend of Shepherd Kaldi discovering coffee's energizing properties when his goats ate seeds from coffee trees. The coffee was then transferred to the Arabic peninsula between 575-850 A.D. In contrast, Panama's significant coffee history emerged much later, particularly in Boquete, where La Berlina Estate was founded in the early 20th century by Segundo Diaz. Panama gained special prominence in the coffee world when it became recognized as the 'home away from home' for the Geisha variety, particularly after Price Peterson's coffee won the Best of Panama competition, leading to extraordinary prices and attention for this cultivar despite its global propagation.","context":["La Berlina Estate is located in a little corner of the Boquete district of Chiriquí province called Horqueta. It borders La Amistad International Park that crosses the border into Costa Rica (you might recognize the name Amistad from one of our other Crown Jewels, located on the other side of the border). La Berlina was founded in the earliest years of the 20th century by a Colombian soldier named Segundo Diaz who, finding himself out of work in the wake of a successful Panamanian bid for independence and in love with a local woman, settled in Boquete.\nThe Diaz family found enough coffee trees growing wild in the mountains surrounding Boquete to establish an 80 acre estate that transformed the stray coffee into a fully functioning farm. A generation later, the Diaz children were forced to sell the estate, which eventually ended up attracting the attention of Plinio Ruiz who knew the farm’s reputation from the 1920s and 30s. Plinio’s family company, Casa Ruiz, acquired the estate and began the work of returning it to its former glory.\nThe Ruiz family have their own coffee history in Panama: the original coffee, flower, and vegetable farm was destroyed by flooding in Boquete in 1970, and the family business was redirected towards roasting and exporting. However, opportunities for farm ownership starting in the late 1970s were too good to pass up, and the Ruiz family reinvested in land, eventually evolving into a successful regional brand that includes roasted coffee sold to the consumer market in Panama.\nThe Ruiz family are among the founding donors of the Ngäbere literacy program, named for the indigenous population Guaymi, who call themselves the Ngäbe. Many of their population work on the estate during the harvest, and in some instances this may constitute their only source of income. The literacy program aims to preserve the Ngäbere culture by encouraging transliteration from spoken word to written in their native tongue.\nGreat coffee deserves a great story. The Geisha variety’s unlikely origin story begins somewhere close to the town of Gesha in remote western Ethiopia. Coffee berries were picked and transported to Kenya, then to Uganda and Tanzania, and finally across the ocean to the Tropical Agricultural Research and Higher Education Center in Costa Rica where attempts to cultivate the trees earnestly began in the late 1950s and early 1960s. Planting there, and shortly thereafter in Panama, was largely abandoned due to low productivity and poor quality. It’s generally accepted today that the variety is fickle, and that its best attributes are highlighted by a combination of elevation, rainfall, soil and nutrient composition, and myriad other environmental and horticultural factors. It seems apparent that either early trials lacked the necessary conditions to produce the sweet, floral attributes now recognizably associated with Geisha, or that those attributes simply weren’t valued the way they are today.\nIt’s hard to believe that it has been more than a decade since Price Peterson’s unusual coffee swept the Best of Panama competition and ignited a feverish global appetite for the variety. The country of Panama – and Peterson’s Hacienda La Esmeralda in particular – deserve credit for putting the spotlight on the cultivar. Panama is the cultivar’s home away from home, where it continues to demand extraordinary prices and attention despite its propagation throughout the coffee growing world.\nWe’ve seen an uptick in natural Geishas in recent years, the “eat your cake and have it, too” processing method for this variety of coffee that seems to provide a little bit of everything: the big ripe berry notes from the process, the inherent red cherry and chocolate “classic” Boquete character, and the distinctive, coveted, cultivar-specific floral flavors. This dry processed Geisha has a slightly large screen size of oblong seeds with pretty pedestrian-looking density. It is also pretty dry and possesses a lower than average water activity. It is also remarkably fragrant green, and you’d be remiss not to sample the aromas before roasting.\nThis coffee can be particularly tricky to roast and looking at Chris’ metrics above, it is easy to see why. The Geisha cultivar is also known as a long berry because of the shape of the seed, making it difficult to roast evenly. I was fortunate to roast the washed version of this coffee before and learned a thing or two about it. Because of the large size, relatively high density, and low moisture content, this coffee really wants to race through the Maillard stage and into first crack. With that in mind, the natural version of this coffee has a slightly lower density which should give us an advantage when manipulating the Maillard stage.\nThe first roast was a bit more relaxed with simple heat adjustments and easily repeatable. The second roast I applied heat in the shape of a bell curve. At the onset, low heat, and then quickly ramping up the heat just after turnaround to shorten the drying stage. As soon as Maillard reactions begin, I lowered the heat and finished with an overall shorter roast time. Roast one was filled with flavors of dried fruits and extremely vibrant while roast two expressed a more balanced cup like a sweet, jammy blackberry pie.\nRoast one: concord grape, raspberry, lime, dried fruits\nRoast two: blackberry, dark chocolate, plum wine\nThe Panama Horqueta La Berlina Natural Geisha roasted relatively quickly. As you can see from the Color Track numbers, this coffee was roasted in a slightly shorter time than its fully washed counterpart, but achieved a darker roast. That being said, this darker roast still had plenty of fruit to go around, and would please anyone looking for a very fruit-forward flavor profile. One option the roaster has on the Behmor is to start the cooling program early. In retrospect, I would have liked to do this shortly after first crack in order to stop this coffee from “running away.” This is particularly common with fruit-dried coffees, and I’ll plan to perform this sort of trick in the near future with all the Ethiopian naturals that will arrive in July!\nEverything one would expect from a fruit dried coffee is present in this Panamanian Geisha. Both roasts displayed ample fruit notes, with strawberry and raspberry being some of the more prominent notes. As Jen noted previously, we found the first roast to be a drier and more crisp representation of this coffee, while the second roast was a jammier, more syrupy, and texturally focused roast. These two roasts gave very similar results as far as brewing parameters, but were quite different in the cup. Check out our notes above.","The legend begins at Ethiopia where Shepard Kaldi noticed that each time his goats ate seeds from a particular tree they became more active. This tree was the coffee tree which was transferred to Arabic peninsula between 575 – 850 A.D. It is still unclear how it was transferred. It is possible that the African tribes which were traveling north from Kenya and Ethiopia to the Arabic peninsula were carrying seeds with them. There started the evolution of the beverage (the word coffee means beverage and origins from the Arabic word kahwa). At first, it was preferred by the monks who realized that it kept them awake during late night prayers. In the beginning of the 17th century first the Dutch transport the tree from the Arabic peninsula and attend to cultivate it at their colony in Java. Later the plant was taken by the French and was cultivated at Reunion island (Indian Ocean). Then it was transferred to the New World and huge plantations of coffee trees were created. Coffee tree is cultivated in countries located in between the Tropical of Cancer and Sagittarius (due to climate, high temperature and intense rains its cultivation is achieved). Every type of coffee (Greek – filter – espresso) originates from the different baking procedure of the seeds, the way they are milled and the way they are produced. Coffee seeds differentiate with respect to the cultivation location (climate conditions height soil). Taste testing (cupping) is made by specialists and the product is evaluated after numerous tests. The different prices of raw coffee are formed in New York and London markets.\nThe term “specialty coffee” refers to the highest-quality green coffee beans roasted to their greatest flavor potential by true craftspeople and then it is brewed to well-established standards. Specialty coffee is not defined by a brewing method, such as the use of an espresso machine. The definition of specialty coffee begins at the origin of coffee, the planting of a particular varietal into a particular growing region of the world. However, the concept of specialty also includes the care given to the plant through harvest and preparation for export. Specialty coffee in the green bean phase can be defined as a coffee that has no defects and has a distinctive character in the cup. It is not only that the coffee doesn’t taste bad; to be considered specialty it must be notably good. The next phase is roasting, and there is a lot of opportunity here to continually define specialty.\nEvery coffee in combination with every roaster has a potential to express itself in a way that will be most satisfying for every customer. Bringing out a coffee’s distinctive character is the roast master’s challenge. If he comes close to succeeding, then it is still specialty if it started out in the green form as specialty. In roasted coffee, most agree that freshness is a part of the definition for specialty. If the coffee is not highly aromatic, then it no longer deserves to be called specialty.\nThen there is the brewing phase. There are many different methods, and all are capable of brewing beverages that can qualify as specialty coffee, but only if done correctly. The right ratio of coffee to water, the right grind suited to the method and the coffee’s physical characteristics, the proper water temperature and contact time, a good preparation of the coffee “bed” or “cake” are all fundamentals that must be satisfied to produce a specialty cup of coffee.\nSpecialty coffee is, in the end, defined in the cup. It takes many steps to deliver that cup into the customers’ hands. Each of those steps can uphold the classification of specialty if quality has been maintained throughout all the preceding steps.\nThis distinct coffee variety is known for the elongated oval shape of the bean. Typica is also known as “Arabigo” or “Criolo” in Central America. The Typica variety produces a clean and resonant acidity which increases in intensity at higher elevations. The cup can be characterized by lemon and floral notes and by a sweet aftertaste.\nBourbon was first discovered on the island Reunion which is situated near Madagascar and was originally named Bourbon. Bourbon has a distinct flavour and a bright acidity with a winy sweet aftertaste. Bourbon varieties that are cultivated at higher elevations also have some aromatic properties. The beans of this variety are small and round.\nCaturra was first discovered in Brazil. Caturra is a mutation of bourbon and it has the ability to produce good quality with high production volumes. The leaves of the Caturra tree are similar to the bourbon leaves. The cup characteristics of the Caturra variety include a clear acidity with lemony flavour notes, especially at higher elevations. The beans have a dense complexity and a centercut that seems to be embedded into the inner layers of the bean.\nCatuai is a cross of the Mundo novo and Caturra varieties. One benefit of this variety is its resistance against strong winds and rain. The cherries do not easily fall under these conditions. The Catuai variety does not have a distinct flavour profile.\nThis variety is a cross of the Typica and Bourbon varieties. The benefit of this variety is the high productivity levels at medium to high elevations as well as a good resistance to diseases. The flavour profile lacks sweetness and can be characterized by a bitter undertone.\nThis variety is named after a place called “Maragogype” in Bahia, Brazil. In general, the flavour profile of the Maragogype beans which are unusually large is very mild with a subtle sweet acidity.\nThis variety is a cross between the Maragogype and Pacas varieties.\nPaca is a cross between the Caturra and Bourbon varieties.\nCatimor is a cross between an Arabica-Robusta variety from Timor and Caturra. The quality of the Catimor beans is distinct because of its sour acidity with a slightly sharp mouthfeel and a salty aftertaste.\nThis is a variety that comes from elegant trees with large fruits. The cupping profile of the Geisha variety produced in Panama includes a floral aroma with a persistent exotic aftertaste and a resonant refreshing acidity. The Geisha beans have a curvy and thin shape.\nA natural mutant of the Bourbon sub species, occurred in Sarchi town in West Valley, Costa Rica. With good production, it has proven to present complexity in the cup with flavors found in such as green grapes, green apples, sparkling notes.\nA mutation of Typica coffee variety that was first observed in Guatemala. It is also known as Pache Comun.\nThe variety was born in Western Valley of Costa Rica. Villalobos is an exotic variety, where the soil is extremely rich and the micro-clime ideal, to give to the tree the sweetness and balanced acidity, two of its basic characteristics.\nThis is a sub-variety of Colombia (mutation) which was adapted in a laboratory, in order to perform best in specific climates and soils. Today is the variety most grown in Colombia. Sidra It is a crossed variety, made of red Bourbon and Typica, retaining characteristics of both “ancestor” varieties, that result in a balanced impression. In 2013 Sidra was cultivated in Colombia, in the innovative farm La Palma y El Tucan. The Sidra has a unique character, tastes like fresh apple, while its acidity is described as malic.\nIt is a crossed variety, made of red Bourbon and Typica, retaining characteristics of both “ancestor” varieties, that result in a balanced impression. In 2013 Sidra was cultivated in Colombia, in the innovative farm La Palma y El Tucan. The Sidra has a unique character, tastes like fresh apple, while its acidity is described as malic.\nThe flowering of the coffee tree varies according to the production country as well as the microclimate. After 8-9 months, the harvest begins at the same time the flowering takes place. Only ripe cherries are picked, meaning the cherries that have a deep red color or deep yellow color (depending on the coffee variety). These coffee cherries are not affected by diseases or other harming organisms. Besides, only these coffee cherries are characterized for their high quality.\nThe ripe cherries are transferred the same day of the harvest at the processing station. There are several methods used for coffee processing:\n-Honey (red & black)\n-New processing methods\nCoffee processing is a critical stage of the quality chain of the green bean. The way in which the mucilage is being removed, shapes coffee taste.\nThe natural processing method is the oldest method during which the seeds are placed in elevated beds in fine layers so that drying can be done in a natural way, with sun heat. The sugars dry for 10-14 days and the dry hull is removed mechanically with the use of hullers. Factors that affect the processing method are the sun, humidity in the air as well as systematic shaking of the seeds. During the night, the seeds are transferred to a dry place to avoid the increase in humidity concentration.\nThe ripe cherries are compressed through the depulpers and in that way the skin is removed. With the use of water, the seeds with the mucilage are transferred through channels in tanks. Then, the mucilage is removed during the fermentation process. During fermentation, a biochemical reaction takes place or else called hydrolysis of the mucilage that surrounds the coffee seeds. Then the seeds are washed and transferred to the drying station. Drying can be achieved mechanically or with the sun. When humidity inside the bean reaches the desired levels (10-12% of the initial 60%), the green bean is released from the so-called pergaminum and is being classified according to its density. Finally, the green coffee is stored in bags and is ready for export.\nThe outer skin of ripe cherries is removed through depulpers as in the wet processing method. The difference with the pulped natural method is that the seeds are not transferred in tanks where the mucilage is removed, instead they dry with the parchment. This method is applied in countries with low humidity levels.\nHoney (red & black)\nThe honey processing method is used in Costa Rica and Panama and is similar to the pulped natural method, meaning that the outer skin of the ripe cherries is removed with depulpers and the mucilage is partly removed mechanically while the rest dries with the parchment that surrounds the green bean. The long lasting contact of the mucilage with the parchment in combination with the nature of this processing method results in a coffee with rich body and mild acidity. Depending on the percentage of mucilage removal, honey processing method is named red & black honey.\nNew processing methods\nIn Colombia and more specifically at La Palma y El Tucan, they are experimenting with new processing methods such as the aerobic fermentation (38hrs) without the use of water and then the drying of coffee as well as the pulp-covered method (22hrs) and then coffee drying. The objective is to reveal new taste profiles.\nRoasting reveals and supports the dynamics of coffee, it is a science and an art that requires long-term experience, focus and time. Roasting is also considered as the roaster’s signature. During the first stage of roasting, the green beans are dried by gently vaporizing stored water molecules – as heat passes inside the bean, where water molecules are. The beans turn from bluish-green to yellow-orange. As the beans turn from yellow to light brown, going past 160 Celsius degrees, they begin cooking from within. The steam that escapes as well as carbon dioxide begin building pressure on the beans’ cell walls. It is up to the roaster to decide when to end roasting, as the beans go from dark tones of brown to black. The choice for a roast color is a choice for taste. The coffee aroma is created through a series of primary and secondary reactions and more than 1000 compounds have being defined. The coffee beans have a weight loss of 20% due to humidity loss. The Maillard reaction is the driving power behind coffee compounds.\nHigh quality in taste is the ultimate criteria for Specialty Coffees. We systematically cup all coffee samples in our own lab. Therefore, cupping is an integral part of the whole procedure and it is done systematically. The assessment of the aroma, the freshly ground coffee, the flavour and the taste of the freshly brewed cup, as well as the mouthfeel and after-taste, are all important characteristics and determine the coffees personality.\nFor the preparation of great coffees, the barista has to reveal all the unique characteristics of the coffee, taking advantage of his expertise."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:8e3b314b-619b-40e9-afb3-d43a04059380>","<urn:uuid:bfc20bba-adf5-4530-9df5-03e5ecb132de>"],"error":null}
{"question":"What role did religious and political institutions play in the Portuguese and Ottoman expansion into the Indian Ocean region?","answer":"Both empires used religious institutions to strengthen their authority. The Ottomans declared their Sultan as the 'Caliph of all Muslims,' establishing religious authority comparable to the Papacy's role in Christendom. The Portuguese, meanwhile, integrated religious institutions into their colonial expansion, with Jesuits establishing schools in conquered territories like Ternate. The political approach differed - the Ottomans relied on viziers and court officials to manage their expansion, while the Portuguese used a system of fortresses and trading posts (feitorias) managed by captains, as seen in their establishment in Ternate and Batjan.","context":["In 1517, the Ottoman Sultan Selim \"the Grim\" conquered Egypt and brought his empire for the first time in history into direct contact with the trading world of the Indian Ocean. During the decades that followed, the Ottomans became progressively more engaged in the affairs of this vast and previously unfamiliar region, eventually to the point of launching a systematic ideological, military and commercial challenge to the Portuguese Empire, their main rival for control of the lucrative trade routes of maritime Asia.\nThe Ottoman Age of Exploration is the first comprehensive historical account of this century-long struggle for global dominance, a struggle that raged from the shores of the Mediterranean to the Straits of Malacca, and from the interior of Africa to the steppes of Central Asia.\nBased on extensive research in the archives of Turkey and Portugal, as well as materials written on three continents and in a half dozen languages, it presents an unprecedented picture of the global reach of the Ottoman state during the 16th century. It does so through a dramatic recounting of the lives of sultans and viziers, spies, corsairs, soldiers-of-fortune, and women from the imperial harem. Challenging traditional narratives of Western dominance, it argues that the Ottomans were not only active participants in the Age of Exploration, but ultimately bested the Portuguese in the game of global politics by using sea power, dynastic prestige, and commercial savoir faire to create their own imperial dominion throughout the Indian Ocean.\n©2010 Giancarlo Casale (P)2011 Audible, Inc.\nBook is great. I know Casale personally, he is not only a great historian, he is also one of the greatest storytellers one could encounter. This book really gives a better view about how Ottoman rulers saw the world and how dynamic the Ottoman State was against changes happening around them. They were aware the world was going into a great transformation, they did their best to respond to it.\nMy problem was with narrator's butchering of turkish words. Especially names were so horribly pronounced that at some point I thought this was done on purpose. If they had walked on the street and found one turkish person, they would not have this problem. This is a book on Ottoman history, and not one name is pronounced correctly. I am not asking for geniune turkish accent, all I am asking for is some effort for correct way of reading words. I did not realize who narrator was talking about even when he referred to names I knew by heart as an Ottoman history enthusiast.\nJust one example: Narrator refers to someone called \"Sefer Reis\". Both e's are vocal and close to e's in the word \"tell\" or \"end\"; and you are supposed to say \"r\" at the end. But narrator reads first \"e\" as i in \"is\", and second \"e\" as a in \"about\", but longer. And he swallows the \"r\" at the end. For hours I did not realize he was saying \"sefer\", which is a common turkish word and means campaing or journey. I thought Sefer Reis' name was not turkish at all, and he decided to join turkish navy with his original foreign name when he was relatively old.\nThere are so many other examples like this, he does not bother to pronounce even much more common names somewhat decent, like \"Suleiman\". It transforms into a \"Sulumen\". It was really annoying.\nI am an avid eclectic reader.\nIn many ways this book reads like a textbook but it is highly readable. The news from the Middle East recently triggered me to learn more about the history of the area. Giancarlo Casale, a professor of history, proceeds chronologically, weaving together political and intellectual history of the Ottoman Empire throughout the 16th Century. He focuses on a number of high officials among them were the Grand Viziers Ibrahim Pasha, Hadim Suleiman Pasha, Rustem Pasha, the one Grand Vizier opposed to the whole Indian Ocean enterprise, and Sokolla Mehmed Pasha, probably the strongest supporter. They were aware of what advantage a strong Ottoman presence in the Indian Ocean could be to the profitable Spice trade. The Ottoman controlled the area from the Red Sea to Atjeh in Sumatra. In response to the Portuguese global claims the Ottoman declared that the Sultan was the “Caliph of all Muslims”. The Caliphate united all Muslims under the same religious authority, much as the Papacy did for Christendom. The author shows that shifting priorities and bitter personal rivalries at the Ottoman court hampered the development of a long term global policy. Slowly the conviction grew that tax income from land was preferable to the profits made from the government controlled spice trade.\nCasale’s aim is to show the achievements of the “Ottoman age of exploration” not only the military and commercial but the intellectual and political ones. He does so in a convincing manner, making both sides, the Ottoman and the Portuguese, come alive in their negotiations, their self views and perception of their opponent. The book is well researched. Casale speaks Turkish, Portuguese and Italian, enabling him to consult all the relevant archives and secondary literature. James Adams narrated the book. I would have given this book a 3 1/2 , there is no halves so I rounded it up.\nHistorian with a specialized interest in 16th century Indian Ocean conflicts. Or someone who was planning to tour the old port cities: Diu, Gujarat, Hormuz, etc. The book is not so much about exploration, as it is about military struggles between Portugal and the Ottomans over control of port cities on the Indian Ocean trade routes. Better to get a book or ebook version, so you can easily skip ahead through the long narratives of military campaigns–unless of course that's your thing. Rather light on social and cultural history, heavy on military, diplomatic. This may reflect the available sources.\nNot in audio format.\nBy rolling every foreign word around in his mouth as if it were an olive from which he was trying to extract the pit.\nThis book gives a fascinating account of the Ottoman political maneuvers in the 16th century Indian Ocean. It gave me a new perspective on events as diverse as the rise of Emperor Akbar in India and the sinking of the Spanish Armada. It's piqued my interest in Yemen and Gujarat in particular.\nIf you are not familiar with the geography of the Indian Ocean, you will probably end up like me, poring over maps for hours after listening. My own ignorance of the region and of Ottoman history made this book more challenging to listen to than most of the audiobooks I have finished. I had to rewind and relisten to many parts in order to make sure I understood just what the sequence of events was.\nOverall I enjoyed it very much. I have given it 3 stars for story just because it is not a breezy listen the way some books are.\nThe author would have had to have written a different book.\nContext was ignored. Plenty of important things were happening to the Ottoman Empire outside of the Indian Ocean, and these are not even mentioned.\nCasale is so anxious to support his central thesis that he makes almost ridiculous claims. Most of the \"exploration\" he describes consists of the Ottomans \"exploring\" the Arabian Peninsula. He spins Ottoman military defeats as successes, ignoring the fact that Portugal was a tiny European state and its fleet was operating many thousands of miles from home, while the Ottomans were an enormous empire whose fleet was operating close to home base.\nNo details are given about the technologies of the day or the details of military engagements. We are only told who won and lost each battle.\nAll in all, a boring, sloppy, non-credible story.\nThis is one of those things that you can only understand by listening to the narrator yourself.\nInstead of this book, read Roger Crowley's \"Empires of the Sea\". That is an amazing book.\nThere are no listener reviews for this title yet.\nReport Inappropriate Content","Written by Marco Ramerini. English text revision by Dietrich Köster.\nTERNATE AND TIDORE\nThe first Portuguese expedition to the Moluccas under the command of António de Abreu arrived in Amboina and on the Banda islands in 1512. After an adventurous voyage he went back to Malacca. Francisco Serrão and other members of this expedition wrecked on a reef off Lucopino island (Nusa Penju) not far from Ambon island, but somehow managed to reach first Ambon and then Ternate.\nThere the Sultan of Ternate adopted Serrão as his personal councilor and made him and his companions prominent figures of his royal court. From 1513 the Portuguese sent an annual trading fleet to the Spice Islands. The first under Captain António de Miranda de Azevedo opened two small “feitorias”, one in Ternate and one in Batjan.\nOn Febraury 1522 the Portuguese captain António de Brito came to the Banda islands and strengthened the friendship with the King of these islands. To mark this event they erected a stone “padrão” with the arms of the King of Portugal. António de Brito arrived in Ternate in May of 1522, when he built the fortress of São João Baptista de Ternate. The foundation stone of the fortress was laid on June 1522. The Jesuits soon started a school in Ternate.\nThe Portuguese rule in these islands was always weak. This was due to the remoteness of the islands and to the small number of Portuguese , who arrived there; the Europeans were never more than a few thousand.\nSeveral Spanish expeditions arrived in Tidore; the first was that of Magalhães. The Spaniards settled in Tidore and annoyed the Portuguese for many years.\nOn 25 October 1536 the Portuguese governor, António Galvão arrived in Ternate. He was a good governor, reconciling, organizing and evangelizing the Moluccas. He was also the builder of the Portuguese town of Ternate, he built a school and a hospital and had a stone wall built all around the town. António Galvão is worshipped as the apostle of the Moluccas.\nOn 15 July 1575, the Portuguese surrendered the fort to the Ternatese.\n– Andaya, Leonard Y. “The world of Maluku: Eastern Indonesia in the early modern period” University of Hawaii Press, 1993, Honolulu.\n– Argensola, Bartolomé Leonardo “Conquista de las islas Malucas” 372 pp. EdicionesPolifemo, 1992 (1609), Madrid, Spain.\n– Des Alwi & Hanna, A. Willard “Turbolent times past in Ternate and Tidore” (also for the Dutch history) 290 pp. Rumah Budaya 1990 Banda Neira, Moluccas, Indonesia.\n– Jacobs, Hubert “A treatise on the Moluccas c. 1544. Probably the preliminary version of António Galvão’s lost História das Molucas” x, 402 pp. Sources and studies for the history of the Jesuits n° 3, Institutum Historicum S. I., 1971, Roma, Italia.\n– Jacobs, Hubert “Documenta Malucensia” Vol. I-II-III Vol. I 1542-1577. XLII-84*-760 pp. (vol. 109). Monumenta Historica Societatis Iesu, 1974, Roma, Italia. Vol. II 1577-1605. XXXII-65*-794 pp. (vol. 119). Monumenta Historica Societatis Iesu, 1980, Roma, Italia. Vol. III 1606-1682. XXIV-54*-778 pp. (vol. 126). Monumenta Historica Societatis Iesu, 1984, Roma, Italia.\n– Perez, Lorenzo OFM “Historia de las misiones de los Franciscanos en las islas Malucas y Celebes” In: “Archivum Franciscanum Historicorum” vol. VI (1913), pp. 45-60, 681-701; vol. VII (1914) 198-226, 424-446, 621-653.\n– Pinto da França, A. “Influência Portuguesa na Indonésia”, in: STUDIA N° 33, pp. 161-234, 1971, Lisbon, Portugal.\n– Ramerini, Marco “Le Fortezze Spagnole nell’Isola di Tidore, 1521-1663” Roma, 2008\n– Rebelo, Gabriel “Informação das cousas de Maluco ……. 1569” 1856 & 1955, Lisbon, Portugal."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:8e424fdd-5d06-49b1-9048-5e1c7ee28e70>","<urn:uuid:b6f12f72-9bc0-4e07-8962-9a0c6788c2b1>"],"error":null}
{"question":"Hey there! I'm living with my girlfriend but we're not married. Will she automatically inherit my house if something happens to me? 😟","answer":"No, without a Will your unmarried partner will not automatically inherit your house. Under intestacy rules, assets go to your nearest blood relatives, not your partner. This means your partner could find themselves homeless or have to sell the property to pay your blood relatives their inheritance. To protect your unmarried partner and ensure they can keep living in the family home, you need to make a Will.","context":["A Will can help to protect your loved ones after you die and ensure that your assets are dealt with in a way that you choose, rather than how the law states. Here we detail why it is important to make a Will, no matter what your circumstances, and the top reasons for making a Will.\nA Will is a legal document that sets out what should happen after you die. Unfortunately, death is a part of life, and it helps your loved ones if you have prepared your wishes, to avoid any unnecessary questions, dilemmas or arguments for your family and friends.\nIt is important to make a Will even if you feel you do not have much money. It is estimated as many as 60% of people do not have valid Wills and this may cause difficulty upon their death.\nTo control your Estate – You have worked hard for what you have, so why should you then leave it to arbitrary legal rules to decide who benefits from your hard work? The Intestacy rules dictate where your estate is to pass when you do not have a valid Will. These rules often lead to loved ones losing out, especially if you are unmarried. It could also lead to you benefitting the Crown rather than your loved ones, through the Bona Vacantia office.\nChildren’s Guardian – You may think that you do not have much financially, but many people have children who are the most important possessions of all. In a Will, you can decide who is to look after your children if they are under 18 years of age. If you don’t, then the decision can be left to the family courts, and they may decide on someone you do not agree with. Your children could even end up in foster care whilst the issue is decided through the Courts. You may have christened your children and appointed Godparents, but this is not a legally binding instruction that your children are to reside with them. A Will can make a legally binding appointment of a guardian if no one has parental responsibility for your children.\nProviding for your Dependents – A Will allows you to decide how you want to provide for your children and other dependents financially. You may want to set up a Trust fund so that your children receive an income for their education and maintenance and for someone to govern that money until they reach a more mature, sensible age. The Will allows you to decide who you think is suitable to protect the money until they are of an age you choose, for example, 21 years or 25 years of age.\nWithout a Will, the money will be due to your children at 18 years of age, and they may not have the maturity to protect the money from frivolous spending or outside factors such as “friends”, “partners” or “unsuitable family members”. Also, whilst under 18 the money will be governed by their parent or guardian. This may be your ex-spouse or partner, who you deem irresponsible, and they may use the money on themselves and their lifestyle rather than the children. Although a child could bring a Court action once they are 18 against their parent who has spent their money, would you want your child to go through this with their own parent/guardian?\nAre you unmarried? – A Will allows you to protect your unmarried partner. Many couples are choosing to not marry or enter civil partnerships and remain unmarried. Many people refer to their living situation as “Common Law” spouses, however, this is not a legally recognised union and so does not afford the protection that a marriage or civil partnership attracts. To safeguard your partner to ensure that they keep a roof over their head, you need a Will.\nWithout a Will, intestacy rules apply and they state that your assets are to go to your nearest blood relative, not your partner. This means that your partner could find themselves homeless or have to sell your property to pay your blood relatives their inheritance from you.\nFamily Disputes – Unfortunately, death and money attract all kinds of family disputes. Even with the closest of families, death and the sniff of money can lead to a breakdown in family relationships, especially if the person who died was “the glue that kept the family together”. We have seen families argue over plastic jewellery and fridge magnets of no actual monetary value. You may think that you do not have anything of value, but it is amazing what a person will argue about and hold on to. To stop this, having a Will makes it clear to all what your intentions are and to head off any family disputes.\nThe Family Home – A Will can ensure that the family home goes to the people that you want to benefit. If you are unmarried, it makes sure your partner can remain in the family home.\nIf you have adult children living at home, a Will ensures that your adult children can remain living in the house for a set period after your death, to try to get a deposit together to buy their own home or buy your home from their siblings or it can even let them live in the house for the rest of their lives, before passing to grandchildren.\nA Will can also offer protection from care fees, by ensuring that on your passing your half of the property passes to your children rather than your spouse to be used to fund their future care fees.\nWithout a Will, the house would be sold, and the proceeds would be distributed to the beneficiaries the law dictates to inherit.\nInheritance Tax – Inheritance Tax is a tax that a financial advisor will often deem as a “voluntary tax”. There are many ways you can avoid or mitigate inheritance tax and a Will is part of this. A Will drafted in accordance with advice from a financial advisor can often save you inheritance tax, as the Will can incorporate provisions to make use of all the allowances that are open to you and your estate. It is not automatic that these allowances are applied to an estate.\nNewly married? – If you are newly married you may not be aware that if you did have a Will prior to the marriage, it could be invalid. Unless a Will is “made in contemplation” of marriage it will be revoked automatically on you saying “I do”. This means that your estate could pass to your new spouse and their family rather than your own.\nYou need to ensure that you revisit your Will or make a new Will upon marriage to ensure that your assets pass to where you want them to.\nNo Children? – If you do not have children, you may think you do not need a Will, but you do! Under intestacy rules, they look for your next nearest blood relative to inherit which could be someone you have never met or someone you disliked. If heir hunters do not find anyone then your estate will sit with the bona vacantia office and ultimately pass to the crown.\nInstead making a Will allows you to decide what happens to your estate, you may want to pass your estate to an in-law or friend or even a charity.\nCase Study: There was an estate where an elderly Aunt died. Nephew by marriage had cared for his aunt all his life and been very close to her, he was by all intents and purposes her nephew. They had a very close relationship. Unfortunately, Aunt did not make a Will, she falsely believed that he would get everything being her only living relation and visitor. Intestacy rules stated otherwise – he was a nephew by marriage, which meant he was no blood relation. Instead, heir hunters located 40 distant blood cousins who were to inherit.\nExecutors/Trustees – A Will allows you to decide who is to administer and look after your estate. You may have someone who is not family, who is better suited to look after things for you such as a friend or professional. You may know that your family would argue or that they could not cope with such a task. By having this control, it allows the estate to be dealt with more smoothly for your loved ones.\nWithout a Will, it may be left to an unsuitable person to deal with your estate.\nFor example, you may be divorced with two minor children (under 18). Intestacy states that the estate would pass to your children equally. Your children are not of legal age to deal with the estate. You would prefer for your parents to look after the estate and the money for your children, but without a Will, intestacy states that your ex-spouse being their parent is to manage it. You may have separated from your spouse due to their inability to manage money – you know that they are unsuitable, but without a Will, you have caused an issue for your children, which your parents cannot help with!\nPets – One item that many may overlook, is what happens to your pets? Pets unfortunately, although we deem them as being part of our family, in law are “possessions”.\nYou may share a pet with an unmarried partner and think that the pet will remain with them after you die. As stated earlier, this is not always the case. Intestacy states all your assets, including your pet, are to pass to your nearest blood relatives. Therefore, technically your nearest blood relatives could take the pet from your partner.\nThe other side of pets is that you could decide to leave a legacy with your pet. Some people have chosen to leave large sums to their pets for them to be looked after. It is more common for people with a Will to leave their pet to someone who will look after them with a modest amount to compensate for their food and medical bills.\nMaking a provision for a pet ensures that they can remain with someone who will care for them rather than being placed in an animal shelter and stops any arguments if numerous people covet your pet.\nCharity – It may be that you do not have any family to benefit from your estate, or you may not want your family and friends to inherit. Instead, you could decide to leave your estate to charity.\nThose who do not have any known family, sometimes decide to not make a Will, not realising that they have remote family that they do not know about. Heir hunters locate remote family members, and the estate passes to people they never knew. At worst, the estate passes to the Crown. Instead of this happening, making a Will allows you to benefit charities that may be close to your heart and benefit those in need.\nIf you are thinking of making a Will and need advice with regards to making a Will, then contact us to arrange a consultation.\nArticle dated: 09.11.2023"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:d408c7d7-0067-41df-92a2-8d7adcea8e40>"],"error":null}
{"question":"Do Varonis and UCLA's TDI system have similar approaches to detecting unauthorized network access?","answer":"Yes, both systems share similar approaches to detecting unauthorized access. They both monitor for unusual user activity patterns - Varonis looks for users accessing sensitive data outside their normal patterns or role, while UCLA's TDI monitors for indicators of compromise and suspicious behavior. Both systems track access from new devices/locations and watch for multiple access attempts that could indicate brute force attacks. They also both emphasize 24/7 automated monitoring and focus on providing detailed forensic information about potential threats to enable quick response from security professionals.","context":["Adam Gordon, the country manager of data security company Varonis Australia and New Zealand, says the industry's intellectual property is a tempting target for attackers and a valuable prize for unscrupulous competitors.\nAny company’s intellectual property is valuable. However, it’s the very lifeblood of companies that develop and commercialise the technology, such as those in the MedTech industry.\nUnfortunately, it’s also one of their most vulnerable assets. Intellectual property that is stored on corporate networks is at high risk of theft by external attackers and hostile insiders.\nIn the case of a successful data breach, companies will pay dearly in time and money spent rectifying the damage. They may also experience significant reputational damage that can last for years.\nData breaches can result in companies making front-page news, or worse, having their intellectual property slip into the hands of competitors.\nBoards, executives, and other stakeholders are now well aware of the risks of unsecured data and are demanding assurances that they are protected from cyber-attacks.\nFortunately, there are some steps that MedTech manufacturers can take to better protect their data and detect threats before they can cause any damage.\nLimiting data access is key\nCompanies tend to make information far more widely available than necessary, especially when staff are under pressure to develop new products or meet tight project deadlines.\nVaronis research found 53 per cent of companies have at least 1,000 sensitive files open to all employees. In total, each employee has access to roughly 17 million files. Important data is often not stored in specific, high-security areas, but scattered across company networks on file servers and emails, and open to anyone who can find it.\nIn the MedTech space, this valuable information may consist of scientific research or product blueprints for example. Ensuring only the most relevant and appropriate people have access to this data is key to limiting cyber risk.\nDon’t just focus on external threats\nThere are plenty of different avenues that attackers can take to steal sensitive data, and it is crucial that companies are aware of both the internal and external threat factors facing them.\nThe reality is insider threats are frequently overlooked by many companies. Anyone with legitimate access to valuable data, such as a disgruntled employee or a corrupt business partner, could potentially copy and sell it. As a result, a competitor could get access to a company’s latest product or innovation and beat them to market, causing significant revenue loss.\nA more subtle and potentially equally damaging scenario would be malicious insiders or external attackers changing or deleting data relating to a new product design for example. This type of cyber-attack could be initially undetectable, and ultimately very costly.\nA different kind of external threat is foreign-funded attackers, who seek to access companies’ intellectual property and exploit it for their own gains, such as selling counterfeit products.\nThen there is ransomware, a widespread threat targeting all industries. If an attacker is successful in planting ransomware into a company’s network — often via a phishing email opened by an employee — they can encrypt every file the ransomware can find. The company may have a chance to regain their data access by paying a ransom, but even then, there’s no guarantee.\nHealthcare organisations have long been a favoured target for ransomware attacks. Denying them access to data can impact patient care and potentially endanger lives, giving companies no choice but to pay the ransom. The pandemic has created new priorities across all parts of the healthcare industry, and more opportunities for such attacks.\nSo how do you know if you are under attack? Here are four things to look out for.\n1. Rogue insider. If a company monitors normal patterns of user access to data, they can get an alert when something out of the ordinary happens. This could be a user looking at sensitive data they have not previously accessed or accessing data that has no relevance to their role.\n2. Multiple access attempts. When attackers attempt to defeat access protections such as passwords, they may try to use logins for expired users, brute-force attacks or stolen or leaked data gathered from the dark web. A bad habit of many people is to use the same password on multiple accounts, so if one is compromised, others are potentially exposed.\n3. Access from previously unseen devices. There may be a legitimate reason for someone to access the corporate network from a new device, but it should raise alerts and be investigated. Their IP address can also be a warning: if it’s an address assigned to a country on the other side of the world from that user’s home base, then investigate immediately.\n4. Unusual user activity: When a cybercriminal has gained access to a network, they might not launch a full-scale attack immediately: they explore to get the lie of the land and seek out valuable data. Often, this is done outside normal hours, hoping their investigations are not detected. So, if a user’s login times change suddenly, find out why. It could be an external threat or an insider.\nYou can’t protect what you can’t identify.\nCompanies need to have a comprehensive understanding of the data which is critical to their operations, and know what data is likely to cause the most damage if it falls into the wrong hands. Data access must be reduced to the minimal level needed for their business to operate effectively.\nFurthermore, companies should maintain constant vigilance in monitoring employee activity on the network, to spot any unauthorised access, and should always assume they are already being targeted by cybercriminals. It is crucial to keep watch for any activity that might indicate there is someone already in the network trying to steal company data.","UCLA Threat Detection and Identification (TDI)\nWhat is TDI (Threat Detection and Identification)?\nUCLA’s Threat Detection and Identification (TDI) initiative gives UCLA a campus-wide toolset to help manage and reduce cybersecurity risks. The initiative was implemented in a multi-phased deployment of network, email, and host level protection using an array of cybersecurity devices and uses intelligence from many different information sources.\nWhat do these cybersecurity devices do and why are they needed?\nThey help manage and reduce cyber security risks by using real-time data about advanced threats and cyberattacks. Using automated technology, they focus on threat identification and signs that a system may have been compromised. Upon threat detection, they provide relevant data about the attack. This evidence includes a limited amount of network traffic related to the attack to help reconstruct what happened during the attack. This valuable information provides intelligence for formulating and developing an effective response that allows UCLA information security professionals to respond quickly to these threats.\nUCLA’s various cybersecurity devices and threat intelligence give a common operational picture of campus-wide cybersecurity threats. This is critical in assessing the University’s readiness, as well as reducing overall cybersecurity-related risks.\nWhat do TDI devices do?\nUCLA’s TDI devices are used to maximize visibility into the cybersecurity threat landscape. These devices also include various advanced forensic tools that can provide more detailed and actionable information regarding cyber threats. These devices detail how threats gain entry and the effect(s) they may have caused. This enables information security professionals to respond faster, more effectively, and help guard against future threats. Using the automated technology, UCLA’s cybersecurity environment monitors for threats around the clock for the following:\n- Malware, including ransomware, crimeware, and other advanced malware threats that are created for a specific target and/or purpose.\n- Known malicious Internet addresses and websites.\n- Command-and-control (C&C) traffic nodes, which may be how attackers control and manipulate infected computers.\n- Indicators of Compromise (IOC’s) which are information and communications that reveal information in the event a system has been compromised. IOC’s come in many forms including known bad websites, use of covert communications, dangerous metadata, and much more.\nWill the TDI technology impact my application or system?\nNo. The cybersecurity devices that UCLA employs promote a strong ability to detect and alert on known threats on a wide-range of applications and systems. This technology is strategically implemented to ensure that no network traffic or system is directly impacted.\nSecurity/Privacy Work Group\nAs these capabilities are an evolution of our traditional (but no longer effective) security practices, we are committed to implementing them in a transparent manner. Consistent with our values of shared governance, we have created a security/privacy work group to provide guidance on the implementation of these technologies in alignment with our privacy principles and the Electronics Communication Policy (ECP). Members of the work group include:\n- Jim Davis (OIT)\n- Privacy Board\n- Dana Cuff (Faculty Chair)\n- Christine Borgman (Faculty)\n- Burt Swanson (Faculty)\n- Kent Wada (Chief Privacy Officer)\n- John Mamer (Faculty Chair)\n- Kathleen Bawn (Faculty Vice Chair)\n- Susan Cochran (Academic Senate Chair Elect)\nPlease check this site for additional information related to the work group’s recommendations that will be posted as they become available.\nPlease see frequently asked questions regarding the UC Systemwide Threat Detection and Identification (TDI) Approach for more information."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:3adc8c0e-7054-4d0d-bfad-62065bed8d88>","<urn:uuid:28741805-9662-4c78-b08a-981ee1629bdb>"],"error":null}
{"question":"As a historian studying North American rivers, I'm curious about the historical significance of salmon fishing in both the Restigouche River and Penobscot Bay. Could you compare their past importance and explain how salmon's status has changed in both regions over time?","answer":"Both regions had significant historical importance for salmon fishing, though their trajectories differ. The Restigouche River was once considered 'the best darned salmon river in the world,' attracting wealthy individuals including the Duke of Windsor. In the late 18th century, it was North America's most productive salmon river, with four million pounds shipped annually across the Atlantic. The Penobscot Bay area was similarly renowned, with 'Penobscot River Salmon' becoming a trade name featured in Fannie Farmer's cookbook and on fine dining menus in Boston, New York, and Philadelphia. However, both regions experienced significant changes. The Restigouche faced overfishing issues, leading to the 1824 law forbidding fishing after August 1st and the 1858 Fisheries Act. Today, while the Restigouche still maintains salmon angling activities, the Penobscot's wild Atlantic salmon are now endangered and fishing for sea-run salmon has been prohibited for decades. The Penobscot region has shifted toward salmon aquaculture, with new land-based farming proposals from companies like Nordic Aquafarms and Whole Oceans aiming to revive the area's salmon production in a different form.","context":["Eastern Canada – New Brunswick\nRiver of Enchantment\nAn American writer called it “the river out of Eden”, the Mi’kmaq called it “he who disobey’s his father”, the French explorers called it “Ristigouche” and legions of avid fishing enthusiasts have called it “the best darned salmon river in the world.” It is New Brunswick’s famed Restigouche River.\nThe section of this historic waterway which was designated a Canadian Heritage River in 1998, is 55 kilometres long and runs between Jardine Brook and the Million Dollar Pool at its junction with the Patapedia River. This section of the Restigouche is accessible by public highway at only one point, the community of Kedgwick River located at the end of Highway 265. The bridge at Kedgwick River, built to provide access to the logging roads that serve the interior forested area, is the only bridge crossing the Restigouche River within the designated section.\nBecause of its relative inaccessibility by road, the region is completely unspoiled. The river winds peacefully through dense forests. There are few breaks in the trees except for small clearings where intrepid settlers built their homes and raised their families. The rough terrain has limited development and, for the most part, the river has remained the domain of the majestic Atlantic salmon.\nMany of the world’s wealthy people, including titled aristocrats and even a former king (The Duke of Windsor), were drawn to the Restigouche because of the fishing. Men and women, who were used to giving orders, obediently followed the instructions of local guides who told them where to cast their lines in deep, mysterious pools with names such as Devil’s Half Acre, Snake Rock and Trotting Ground. Then, as they canoed down the gently moving stream, these same people experienced the sense of peace that comes from being at one with nature in surroundings that have changed very little in 200 years.\nWhile anglers still head for the Restigouche between June and August, the river now attracts canoeists as well as other nature lovers. In the spring, the banks become lush and green; in the summer, salmon leap out of water, their silver bodies glinting in the sun; in the fall the area is turned into a panoply of riotous colour; in the winter, it sleeps quietly under a blanket of snow, its peace barely disturbed by snow-shoers and cross-country ski enthusiasts.\nThe Restigouche River has touched the lives of many people, from the Mi’kmaq, for whom it was a major transportation route, to poor Irish immigrants seeking a better life, to sports fishermen, and to the children who play along its banks.\nBreathtakingly beautiful and historically significant, the Restigouche River also provides exceptional recreational opportunities. For all these reasons, nomination of the Restigouche as an official “candidate Canadian Heritage River” was accepted by the Canadian Heritage Rivers Board in January 1995. In February, 1998, the Province of New Brunswick tabled a management plan for the river and its surrounding watershed with the Board at which time it received full status as a Canadian Heritage River.\nLocated in the north of New Brunswick, the Restigouche river flows in a northeasterly direction to the Baie des Chaleurs and the Gulf of St. Lawrence. Its principle tributaries are the Kedgwick, Gounamitz, Patapedia and Upsalquitch Rivers.\nSalmon angling camps are located at four sites within the designated section and are only operated during the salmon angling season. The land adjacent to this section of the Restigouche River is mainly Crown land. The area is characterized by a rolling topography with elevations ranging from approximately 50 metres at the mouths of the rivers to approximately 400 metres along an escarpment that slopes Northwest into the Restigouche from the Upsalquitch. The river channel has been influenced by the geology and ice flow of the area, resulting in a gently meandering waterway with areas of natural erosion. Features include floodplains, terraces, islands, dykes, rock outcrops and deep pools.\nNew Brunswick is renowned for its forests. The banks of the Restigouche are densely wooded with predominantly eastern white cedar, balsam fir and white spruce. There is also the occasional white pine plus hardwood species including white and yellow birch, trembling aspen and balsam poplar. Fire and logging operations have affected the vegetation pattern of the area, especially the ground vegetation. The forest floor is covered with a wide variety of vascular plants, mosses and lichens. Some of them, such as crawe’s sedge and green spleenwort, are rare in other Canadian provinces.\nThe combination of forest cover and associated ground vegetation supports an abundance of wildlife. Those who hike along the forest trails might come across white tailed deer, moose, red fox, black bear or coyote, while canoeists will see beaver, ruffed grouse and mallards. And, if they are exceptionally fortunate, these outdoor enthusiasts may catch sight of a lynx padding quietly through the trees or an osprey diving swiftly into the water. Both the Canada lynx and the osprey are provincially endangered species.\nOf course, the Restigouche is famous for its salmon some of which have been reported to weigh more than 50 lbs.\nAccording to legend, the river was named by a distraught Mi’kmaq chief whose son was killed while leading an expedition against the Mohawks who were poaching salmon. The chief opposed the battle plan and when the entire Mi’kmaq party was massacred on the banks of the river, he named it “he who disobeys his father”.\nBoth the Restigouche branch of the Mi’kmaq nation whose emblem was, naturally, a salmon, and the Maliseet people who lived on the banks of the Saint John River used the Restigouche corridor for transportation and relied upon the rich natural resources of the area for their survival. The Mi’kmaq travelled from their headquarters on the Baie des Chaleurs at Tjigog (Atholville) down the Restigouche to the Saint John River as well as up to the Matapedia and then on to the St. Lawrence.\nWhen French explorer Samuel de Champlain sailed from Gaspe to Miscou, he learned from the Restigouche Mi’kmaq that there was a passage to the Gulf of St. Lawrence by way of the Matapedia River through a short portage. Champlain was followed by French missionaries and fur traders who depended on the natives for guidance.\nThe Restigouche was also the site, in 1760, of one of the most important battles between the French and English. The French Fleet was destroyed on its way to Quebec City. After the defeat, British fur traders settled in the area as did officers posted to the region to defend the Crown’s territory. These men, who had time on their hands, discovered the joys of salmon fishing. So did a number of Scotsmen who worked as pilots on English ships. They sent word back to Scotland and, in 1773, eight men, including Robert Ferguson and Samuel Lee came to the Restigouche area from Aberdeen to establish a fishing industry. Scottish settlers were also among the first to clear land for agricultural purposes on the banks of the river but because of the rough terrain and harsh weather, agriculture was a difficult way of life and many of the farmers turned to other methods of earning a livelihood, such as fishing. At that time, salmon were so plentiful in the river that it was considered the most productive in North America. Huge quantities of fish (four million pounds a year) were shipped across the Atlantic. An Anglican archdeacon travelling in the region in the early 1800’s complained that he had difficulty crossing the river because of the salmon nets.\nOverfishing on such a massive scale soon resulted in severely depleted salmon stocks. This led to an 1824 law forbidding all fishing after August 1st and abolishing night fishing. But, although this was one of the first conservation laws in North America, it was not enough. In 1857, a Crown lands officer reported that harvests had continued to drop and asked the government to bring in more effective legislation. This legislation, the Fisheries Act, was passed in 1858. It opened the door for private hunting and fishing clubs by granting them fishing leases. While the forests represented a valuable asset for logging companies, the acquisition of riparian rights (exclusive rights for fishing) became an even more prized possession and they were sold to the highest bidder.\nMillionaires such as railroad magnate William Vanderbilt purchased land with riparian rights on the Restigouche. Not content with the existing housing facilities, which consisted mostly of small log cabins, the millionaire owners constructed stately fishing camps and lodges. One of these camps, which is along the nominated section, is Kedgwick Lodge. It was designed for Vanderbilt by the renowned American architect Stanford White. He was noted for developing the “shingle” style of architecture typically associated with larger mansions on the Eastern seaboard of the United States.\nLogging was the most important winter activity in the area from the latter part of the 18th century to the early 1900’s. In the spring when melting ice from tributaries greatly increased water levels,massive numbers of logs were floated downstream to mills on the Baie des Chaleurs. Matthew and Bella Broderick, caretakers of theVanderbilt property, often reported housing and feeding up to 70 men en route to the log drive. After a hearty lumberman’s breakfast of baked beans and homemade bread, the men headed off on the 23 mile trek from the junction of the Restigouche and Kedgwick rivers to the Rapids Depot to begin their downstream drive on the floating logs.\nHigh profile guests also visited the river. They included the Duke and Duchess of Windsor who, for a week, lived the simple life at the Restigouche camp of Canadian businessman Izaak Walton Killam. Like many other anglers, the former king spent his days fishing. However, the Duchess encountered a problem when the local man assigned to be her guide, Duncan Myles, was so overcome by emotion at the sight of this famous woman that he fainted while handing her into the canoe.\nToday a broad highway takes the traveller from the Baie des Chaleurs to the Saint John. In recent years, transportation on the river has come full circle. Just as the natives did centuries ago, modern travellers are once again exploring the Restigouche by canoe.\nThe designated section of river is unique in that, while it can be considered remote, it is close to civilization. It offers excellent opportunities for recreational canoeing, kayaking. sightseeing, nature interpretation, cultural and historical interpretation, camping and sport fishing. Salmon angling accounts for over ninety percent of the fishing activity on the river system with the remainder focusing on trout. There are four salmon angling camps on the designated section – Downs Gulch, Larry’s Gulch, Kedgwick Lodge and Cater Hall Lodge. Larry’s Gulch is operated by the New Brunswick Department of Natural Resources but space there is always at a premium. The three others are owned by private families.\nTo accommodate more users, public access to salmon angling is permitted on the “Crown Open Stretch” from Montgomery Bridge to the mouth of Jardine Brook and on two “Crown Reserve” sections.\nRecreational canoeing and camping in the area have increased steadily over the past decade. The river offers a variety of canoeing experiences. In total, approximately 340 kilometres of canoeable waters are directly accessible from the designated section. Through a recreational management initiative under the Environment Trust Fund of the Province of New Brunswick, campsites are being developed and maintained by the Department of Natural Resources and Energy to accommodate canoeists. Four primitive campsites have been established within the designated section along the river. Eco-tourism is also being introduced in the region.\nA campground, Echo Restigouche has been developed on the river bend at the end of the highway leading from Kedgwick. Here canoeing packages, which include instruction and guide books for all levels of expertise are provided. Experienced canoeists, who want to plan their own river excursions, can be provided with transportation as well as overnight accommodation. Hiking tours and nature observation packages can also be arranged. Because Echo Restigouche operates all year round, hunting parties are welcomed in the fall while snowmobilers, snow-shoers and cross country skiers are encouraged in the winter. There are a number of groomed trails but not on the river itself because of rough ice as well as sections of open water.\nAccommodation and Services: The only public accommodation actually on the river is at Echo Restigouche. The camp has seven large log cabins, each of which sleeps eight, two four person cabins and 17 trailer sites. Tents can be set up anywhere on the Echo Restigouche property.\nMotel accommodation is available in the City of Campbellton and the villages of Kedgwick and St.Quentin.\nAccess: Highway 17, running north from St. Leonard on the Trans Canada Highway to Campbellton on the Baie des Chaleurs, provides the major road access to the area. It is approximately one hour’s drive from St. Leonard to the Village of Kedgwick and the junction of Highway 265. There are several small villages on Highway 17 north of St. Quentin and the road is well maintained all year round. Those travelling from the Gaspe region of Quebec can cross bridges at Campbellton and Matapedia which are approximately an hour’s drive from the river. The nearest major airport is at Fredericton, approximately 200 miles (333 km) from Kedgwick.\nReprinted courtesy of the Canadian Heritage Rivers System","Posted August 3, 2018\nLast modified August 3, 2018\nOn the Fourth of July, many a Maine family once sat down to a traditional meal of fresh salmon, garden peas, and new potatoes dug from the warm earth. A century ago, Maine salmon were known around the world as being among the best of seafoods.\nIn the late 18th and early 19th century, both shores of Penobscot Bay bristled with pound nets and weirs, set out to trap adult salmon as they migrated upriver from the frigid Atlantic waters around Labrador and Greenland. Hundreds of individual fishermen operated the nets as part of a seasonal, rural way of life that also included farming and wood-cutting. Upriver, near Bangor, fly-fishermen cast for salmon, keeping the fish to eat at home or selling them to local markets. Indeed, “Penobscot River Salmon” became a trade name for Atlantic salmon up and down the East Coast, appearing in Fannie Farmer’s cookbook and on fine dining menus in Boston, New York, and Philadelphia.\nA prized food fish speared from fast currents, eaten fresh, and smoked and dried for year-round consumption, salmon were—and are—important to the survival of the Wabanaki people. With heart-healthy fatty acids and Vitamin D, salmon is among the healthiest sources of protein. Penobscot River salmon fed local communities, and local residents sustained salmon by promoting fish passage and clean water.\nThat was then.\nNow, Maine’s wild, sea-run Atlantic salmon are on the endangered species list, one of nearly 1,500 threatened and endangered animals throughout the U.S. The Penobscot River remains home to the largest run of wild Salmo salar in America, although fishing for sea-run salmon has been prohibited for decades. Many organizations, including NOAA, are working to restore the severely depleted wild runs, with a goal of recovering self-sustaining populations.\nA renewed fishery remains a distant dream. And yet, Atlantic salmon is ubiquitous in supermarkets and on menus today. Why? Private companies farm the species around the world, in Norway, Chile, Canada, and even eastern Maine.\nSince commercial salmon farming began in the 1980s, culture technology and shipping efficiency improved and competition increased, creating an abundant supply of affordable salmon, and new markets developed. The demand for salmon has been strong ever since: salmon consistently ranks among the top three most popular seafoods in America (shrimp and tuna being the others), and two-thirds of it is imported, farmed Atlantic salmon. There is also a steady supply of sustainably-caught wild Pacific salmon coming out of Alaska, with much of it being exported out of the U.S.\nThe salmon aquaculture industry has worked hard to overcome real and perceived problems with chemical contamination, the amount of wild fish used in feed (because salmon eat other fish), genetic modification of fish, escaped fish, and pollution of ocean waters, with varying success.\nToday, salmon from farms currently operated by Cooke Aquaculture in the coastal waters of eastern Maine are ranked as a yellow “good alternative” choice by Monterey Bay Aquarium. The ranking was attributed to “stringent operating permit mandates” that have prevented fish from escaping farms.\nDespite progress by Cooke and other participants in the marine salmon farming industry to improve sustainability and efficiency, research and development efforts have expanded to explore strategies to raise salmon on land. With enclosed buildings, the risk of escape and other problems associated with an uncontrolled environment can be nearly eliminated, allowing the fish to be marketed as sustainable and sold at a premium price.\nAnd it is demand for sustainable salmon that has prompted two separate proposals for land-based salmon aquaculture production on opposite banks of Penobscot Bay, in Belfast near the mouth of the Little River (Nordic Aquafarms) and at the former Verso paper mill site in Bucksport (Whole Oceans). As two different companies proceed with plans and permit applications, nearby communities are trying to understand, define, and evaluate the implications of large-scale fish production on their waterfronts.\nOn both sides of the bay, residents have raised questions: How much fresh water will they use? Will discharge pollute the bay? What will the buildings look like? What chemicals are used in the process? Can the fish escape? What will the fish eat? Will the fish be free of pollutants? Who will eat the fish? Will the farm smell or be noisy? How much truck traffic will there be? How many jobs will be created? How much energy is needed? Where is the money coming from?\nThe proposals for Belfast and Bucksport are for land-based, recirculating aquaculture systems, or RAS, and are considered the state of the science, yet new at the scale being proposed.\nThe “recirculating” part is key to sustainability claims. Because it is used at least twice, water has to be clean to begin with and filtered and monitored throughout the process, and a permit will be needed for discharge to Penobscot Bay.\nIn the ocean or on land, feed is the biggest cost for farming fish, which is why both Nordic Aquafarms and Whole Oceans are working on feed formulas that reduce the amount of wild fish content and will be compatible with a recirculating system. Compared to other animals, salmon are second only to chicken in their efficiency of converting food to protein and calories.\nThe facilities also will need a Title 50 import permit from the U.S. Fish and Wildlife Service for the salmon eggs. Neither company has determined where they will obtain eggs; existing wild Atlantic salmon hatcheries have none to spare, and because the plants are on land they do not have the same requirements as marine pens to use North American strains of salmon. The federal agency will require testing and biosecurity measures, and monitoring for fish health and disease.\nWAITING FOR APPLICATIONS\nMany questions about the scope and details of the proposals will remain unanswered until the companies submit permit applications. It has become clearer that the developers chose the Penobscot Bay area in part because of its proximity to markets, the abundant clean water, and the need for money in towns that have experienced the departure of other industries: paper, poultry, sardines, and credit card companies.\nThere is excitement about redevelopment of old industrial sites, new models for natural resource-based businesses, and new uses for existing infrastructure. At the same time, Maine communities have grown weary and perhaps wary of reacting to large industrial development ideas that claim to single-handedly save their town or region, especially projects that affect natural resources. And, as the two salmon aquaculture proposals are revealing, communities want meaningful discussion about how residents value local land and water, where their food comes from, what kind of economic activity they want and need.\nBut these are long, big conversations when time is short and budgets are small.\nAnd so people will continue to scrutinize and argue about commercial salmon farms and food security details in hearing rooms and at kitchen tables and along the waterfront of the Penobscot, a river cleaner and full of more fish than at any time in the last hundred years. The river and bay continue to be the focus of efforts to restore salmon and other sea-run fish, as well as marine food fish like cod, haddock, and flounder.\nFisheries scientists don’t think the farms will affect the restoration effort, but public support remains an important factor in success. Stories of wild salmon and the related cultural and culinary heritage of seafood have a place in the discussion about new development proposals from Whole Oceans and Nordic Aquafarms, both of which promise to make the Penobscot River and Bay region once again world famous for salmon.\nCatherine Schmitt is communications director for the Maine Sea Grant College Program at the University of Maine and the author of The President’s Salmon: Restoring the King of Fish and its Home Waters."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:1ec85aad-497e-418e-b932-3f13961ae2d1>","<urn:uuid:16a5e855-19f0-48b2-bf8e-acab20da682a>"],"error":null}
{"question":"I'm a medical student - what's the relationship between professionalism in medical training and decision-making capacity assessment?","answer":"Medical professionalism and capacity assessment are interlinked through core medical values and practices. Professionalism involves developing an authentic professional self with internalized values of caring and excellence, which is critical when assessing patients' decision-making capacity. This assessment requires skillfully evaluating a patient's ability to understand information, express choices, show appreciation, and demonstrate reasoning about medical decisions while respecting patient autonomy. A professional approach balances respecting patient autonomy with acting in their best interest, particularly when cognitive impairment may affect capacity.","context":["The n e w e ng l a n d j o u r na l of m e dic i n e edi t or i a l Professionalism — The Next Wave Frederic W. Hafferty, Ph.D. In their Medical Education article in the October 26 issue of the Journal, Stern and Papadakis make a number of observations about professionalism and the learning environments in which medical training occurs.1 Like a growing number of medical educators, they recognize that considerable learning (some think most) takes place outside the domain of the formal curriculum and that such learning involves indoctrination in the unwritten rules of studenthood and medical practice. Some medical schools and residency programs have acknowledged the existence of alternative, or shadow, domains of learning, whose lessons are sometimes collectively called the “hidden curriculum,” and have accepted responsibility for both understanding and modulating the effects of these domains on students’ knowledge, skills, and values. Included in this broadened curriculum are the lessons students learn as they witness conflicts between the expectations and ideals articulated in professional codes2 and the behavior of individual physicians (particularly faculty members) and organizations as both go about the daily and concurrent work of medicine and education. As we work to define, instill, and appraise professionalism as a core standard and competency, it is critical that we keep three interrelated questions in mind. First, how do we effectively define and assess something that is transmitted in a variety of learning environments through a wide range of both formal and informal, even tacit, educational practices? Second, how do we effectively assess something that may be conceived as both practice and identity? And third, how do we design a system of evaluation that assesses both learners and their learning environments? There is much to be gained in simplicity and directness by highlighting behavior, as advocated by Stern and Papadakis. Nonetheless, an essential element of medicine’s claim to professional status lies in the development of what might be termed a “professional self” in students — the internalization of the values and virtues of medicine as a discipline and a calling. It is not sufficient for students to acquire the knowledge, skills, and outward behavior necessary for practicing medicine. Being a physician — taking on the identity of a true medical professional — also involves a number of value orientations, including a general commitment not only to learning and excellence of skills but also to medical behavior and practices that are authentically caring.3,4 These value orientations and motives are, in part, the product of professional learning and socialization, with medical schools and residency programs functioning as critical settings for the development of what Leach and colleagues have labeled the “habit of professionalism”3,4 — a professional self that is in accordance with the values that medicine has publicly identified as central to its organizational identity and to its contract with society.5 It is this underpinning that provides the necessary stability and generalizability when one has to step outside the realm of textbook medical practice and confront situations of uncertainty and ambiguity — which are, after all, the defining characteristics of real-world medical work. In addition to assessing behavior, we can and should evaluate trainees’ motives and their learning environments. There is a meaningful (and measurable) difference between being a professional and acting professionally. It is certainly possible to behave professionally without having authentically internalized core values. Even if trainees exemplify the behavioral standards of profession- n engl j med 10.1056/NEJMe068054 1 Downloaded from www.nejm.org on August 23, 2009 . Copyright © 2006 Massachusetts Medical Society. All rights reserved. The n e w e ng l a n d j o u r na l of m e dic i n e alism, we must attend to such inconsistencies between the inner self and outward appearance, sending a message that authenticity is a matter of great concern. There are several other reasons to assess factors other than students’ behavior.6 One of these factors arises from medicine’s compact with society: the profession’s desire for autonomy is predicated on its promise to police itself in the public’s interest.7 External controls, even within the occupation, have been deemed to be largely unnecessary because controls would be internalized as a core part of each practitioner’s self, through socialization. We should therefore be asking our students, our faculty, and our practitioner colleagues whether they are, in fact, committed to the core principles that we hold to be “professional.” Second, a focus on behavior may neglect our pedagogical responsibility to assess and transform the learning environments our students must navigate. Indeed, one might argue that we should begin by assessing learning environments and do so until we get it right. It makes little sense to assess the professionalism of students within learning environments that are hostile to its precepts. Third, personal reflection remains a core element of virtually all definitions of professionalism, and we should tap the products of this reflection, not just its behavioral manifestations. Medicine is a moral community, the practice of medicine a moral undertaking, and professionalism a moral commitment. We should bear these fundamental truths in mind as we design our learning environments and seek to measure their effectiveness. No potential conflict of interest relevant to this article was reported. This article was published at www.nejm.org on October 25, 2006. From the Department of Behavioral Sciences, University of Minnesota Medical School, Duluth. 1. Stern DT, Papadakis M. The developing physician — becoming a professional. N Engl J Med 2006;355:1794-9. 2. Members of the Medical Professionalism Project. Medical professionalism in the new millennium: a physician charter. Ann Intern Med 2002;136:243-6. 3. Leach D, Syrdyk PM, Lynch DC. Practicing professionalism. In: Parsi K, Sheehan, MN, eds. Healing as vocation: a medical professionalism primer. Lanham, MD: Rowman & Littlefield, 2006:1-8. 4. Leach DC. Professionalism: the formation of physicians. Am J Bioeth 2004;4:11-2. 5. Cruess RL, Cruess SR, Johnston SE. Professionalism and medicine’s social contract. J Bone Joint Surg Am 2000;82:1189-94. 6. Hafferty FW. Measuring professionalism: a commentary. In: Stern DT, ed. Measuring medical professionalism. New York: Oxford University Press, 2006:281-306. 7. Cruess RL, Cruess SR. Teaching medicine as a profession in the service of healing. Acad Med 1997;72:941-52. Copyright © 2006 Massachusetts Medical Society. 2 n engl j med 10.1056/NEJMe068054 Downloaded from www.nejm.org on August 23, 2009 . Copyright © 2006 Massachusetts Medical Society. All rights reserved.","Assessment of decision-making capacity in adults\n- Jason Karlawish, MD\nJason Karlawish, MD\n- Professor of Medicine\n- Medical Ethics and Health Policy\n- University of Pennsylvania\n- Section Editor\n- Steven T DeKosky, MD, FAAN, FACP, FANA\nSteven T DeKosky, MD, FAAN, FACP, FANA\n- Section Editor — Dementia\n- Professor of Neurology\n- Interim Executive Director\n- McKnight Brain Institute\n- University of Florida College of Medicine\nThe capacity to make one’s own decisions is fundamental to the ethical principle of respect for autonomy and is a key component of informed consent to medical treatment. Determining whether an individual has adequate capacity to make decisions is therefore an inherent aspect of all clinician-patient interactions.\nThe main determinant of capacity is cognition, and any condition or treatment that affects cognition may potentially impair decision-making capacity. In the presence of cognitive impairment from any cause, determining whether a patient has adequate capacity is critical to striking the proper balance between respecting patient autonomy and acting in a patient’s best interest. A skillful capacity assessment can also help determine the severity of a patient’s cognitive impairments and improve the effectiveness of conversations with patients and their families.\nThese skills are especially important in the care of adult patients who have diseases that impair cognition. Patients with traumatic brain injury, psychiatric illnesses (eg, schizophrenia, bipolar disorder, and unipolar major depression), and neurodegenerative diseases (eg, Alzheimer disease and Parkinson disease) are at risk for impaired capacity. Hospitalized patients and older adults are also at risk due to cognitive impairment from chronic diseases, cognitive aging, and delirium.\nThis topic reviews the definition of capacity, predictors of impaired capacity, when and how to assess capacity, and how to incorporate an assessment of capacity into clinical judgments about the ability of patients to choose their treatment. Informed consent, advance care planning, advance directives, and medical decision making at the end of life are discussed separately. (See \"Informed procedural consent\" and \"Advance care planning and advance directives\" and \"Ethical issues in palliative care\" and \"Legal aspects in palliative and end of life care\".)\nCapacity and competency — Capacity describes a person’s ability to a make a decision. In a medical context, capacity refers to the ability to utilize information about an illness and proposed treatment options to make a choice that is congruent with one’s own values and preferences.\n- Grisso T, Appelbaum PS.. Abilities related to competence. In: Assessing competence to consent to treatment A guide for physicians and other health professionals, Oxford University Press, New York 1998. p.31.\n- Siegel AM, Barnwell AS, Sisti DA. Assessing decision-making capacity: a primer for the development of hospital practice guidelines. HEC Forum 2014; 26:159.\n- Faden RR, Beauchamp TL.. Part III. A Theory of Informed Consent. In: A History and Theory of Informed Consent, Oxford University Press, New York 1986. p.235.\n- Lai JM, Karlawish J. Assessing the capacity to make everyday decisions: a guide for clinicians and an agenda for future research. Am J Geriatr Psychiatry 2007; 15:101.\n- Grisso T, Appelbaum PS. Comparison of standards for assessing patients' capacities to make treatment decisions. Am J Psychiatry 1995; 152:1033.\n- Kim SY, Karlawish JH, Kim HM, et al. Preservation of the capacity to appoint a proxy decision maker: implications for dementia research. Arch Gen Psychiatry 2011; 68:214.\n- Karlawish JH, Casarett DJ, James BD. Alzheimer's disease patients' and caregivers' capacity, competency, and reasons to enroll in an early-phase Alzheimer's disease clinical trial. J Am Geriatr Soc 2002; 50:2019.\n- Appelbaum PS, Bonnie RJ, Karlawish JH. The capacity to vote of persons with Alzheimer's disease. Am J Psychiatry 2005; 162:2094.\n- Lai JM, Gill TM, Cooney LM, et al. Everyday decision-making ability in older persons with cognitive impairment. Am J Geriatr Psychiatry 2008; 16:693.\n- Karlawish JH, Casarett DJ, James BD, et al. The ability of persons with Alzheimer disease (AD) to make a decision about taking an AD treatment. Neurology 2005; 64:1514.\n- Kim SYH. Instruments for Assessing Treatment Consent Capacity. In: Evaluation of Capacity to Consent to Treatment and Research, Oxford University Press, New York 2010. p.61.\n- Dymek M, Marson D, Harrell L. Factor structure of capacity to consent to medical treatment in patients with Alzheimer's disease: An exploratory study. Journal of Forensic Neuropsychology 1999; 1:27.\n- Karlawish J, Cary M, Moelter ST, et al. Cognitive impairment and PD patients' capacity to consent to research. Neurology 2013; 81:801.\n- Martin RC, Okonkwo OC, Hill J, et al. Medical decision-making capacity in cognitively impaired Parkinson's disease patients without dementia. Mov Disord 2008; 23:1867.\n- Dymek MP, Atchison P, Harrell L, Marson DC. Competency to consent to medical treatment in cognitively impaired patients with Parkinson's disease. Neurology 2001; 56:17.\n- Griffith HR, Dymek MP, Atchison P, et al. Medical decision-making in neurodegenerative disease: mild AD and PD with cognitive impairment. Neurology 2005; 65:483.\n- Jeste DV, Depp CA, Palmer BW. Magnitude of impairment in decisional capacity in people with schizophrenia compared to normal subjects: an overview. Schizophr Bull 2006; 32:121.\n- Grisso T, Appelbaum PS. The MacArthur Treatment Competence Study. III: Abilities of patients to consent to psychiatric and medical treatments. Law Hum Behav 1995; 19:149.\n- Appelbaum PS, Grisso T, Frank E, et al. Competence of depressed patients for consent to research. Am J Psychiatry 1999; 156:1380.\n- Boettger S, Bergman M, Jenewein J, Boettger S. Assessment of decisional capacity: Prevalence of medical illness and psychiatric comorbidities. Palliat Support Care 2015; 13:1275.\n- Triebel KL, Martin RC, Novack TA, et al. Treatment consent capacity in patients with traumatic brain injury across a range of injury severity. Neurology 2012; 78:1472.\n- Triebel KL, Martin RC, Novack TA, et al. Recovery over 6 months of medical decision-making capacity after traumatic brain injury. Arch Phys Med Rehabil 2014; 95:2296.\n- Steward KA, Gerstenecker A, Triebel KL, et al. Twelve-month recovery of medical decision-making capacity following traumatic brain injury. Neurology 2016; 87:1052.\n- Dreer LE, Devivo MJ, Novack TA, et al. Cognitive Predictors of Medical Decision-Making Capacity in Traumatic Brain Injury. Rehabil Psychol 2008; 53:486.\n- Marson DC, Dreer LE, Krzywanski S, et al. Impairment and partial recovery of medical decision-making capacity in traumatic brain injury: a 6-month longitudinal study. Arch Phys Med Rehabil 2005; 86:889.\n- Raymont V, Bingley W, Buchanan A, et al. Prevalence of mental incapacity in medical inpatients and associated risk factors: cross-sectional study. Lancet 2004; 364:1421.\n- Silveira MJ, Kim SY, Langa KM. Advance directives and outcomes of surrogate decision making before death. N Engl J Med 2010; 362:1211.\n- Appelbaum PS. Clinical practice. Assessment of patients' competence to consent to treatment. N Engl J Med 2007; 357:1834.\n- Marson DC, McInturff B, Hawkins L, et al. Consistency of physician judgments of capacity to consent in mild Alzheimer's disease. J Am Geriatr Soc 1997; 45:453.\n- Marson DC, Earnst KS, Jamil F, et al. Consistency of physicians' legal standard and personal judgments of competency in patients with Alzheimer's disease. J Am Geriatr Soc 2000; 48:911.\n- Etchells E, Darzins P, Silberfeld M, et al. Assessment of patient capacity to consent to treatment. J Gen Intern Med 1999; 14:27.\n- Grisso T, Appelbaum PS. Using the MacArthur Competence Assessment Tool - Treatment. In: Assessing Competence to Consent to Treatment: A Guide for Physicians and Other Health Professionals, Oxford University Press, New York 1998. p.101.\n- Marson DC, Ingram KK, Cody HA, Harrell LE. Assessing the competency of patients with Alzheimer's disease under different legal standards. A prototype instrument. Arch Neurol 1995; 52:949.\n- Kim SYH. Making Sense of the Variety of Standards. In: Evaluation of Capacity to Consent to Treatment and Research, Oxford University Press, New York 2010. p.26.\n- Kim SY, Caine ED, Swan JG, Appelbaum PS. Do clinicians follow a risk-sensitive model of capacity-determination? An experimental video survey. Psychosomatics 2006; 47:325.\n- Rubright J, Sankar P, Casarett DJ, et al. A memory and organizational aid improves Alzheimer disease research consent capacity: results of a randomized, controlled trial. Am J Geriatr Psychiatry 2010; 18:1124.\n- Dunn LB, Lindamer LA, Palmer BW, et al. Improving understanding of research consent in middle-aged and elderly patients with psychotic disorders. Am J Geriatr Psychiatry 2002; 10:142.\n- Buchanan A, Brock DW. Deciding for Others: The Ethics of Surrogate Decision Making, Cambridge University Press, Cambridge 1989.\n- Hirschman KB, Kapo JM, Karlawish JH. Why doesn't a family member of a person with advanced dementia use a substituted judgment when making a decision for that person? Am J Geriatr Psychiatry 2006; 14:659.\n- Capacity and competency\n- Informed consent\n- How capacity differs from cognition and function\n- FACTORS ASSOCIATED WITH IMPAIRED CAPACITY\n- General principles\n- High risk groups\n- - Neurodegenerative disease\n- - Psychiatric disorders\n- - Traumatic brain injury\n- - Hospitalized adults\n- - End of life\n- WHEN TO ASSESS CAPACITY\n- HOW TO ASSESS CAPACITY\n- The decision-making abilities\n- - Understanding\n- - Expressing a choice\n- - Appreciation\n- - Reasoning\n- Semi-structured patient interview\n- Validated instruments\n- - Patient refuses a capacity assessment\n- - Family does not agree with clinician’s assessment\n- HOW THE CAPACITY ASSESSMENT INFORMS CLINICAL JUDGMENT\n- WHAT TO DO WHEN A PATIENT LACKS CAPACITY"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:ee1b686d-42df-461d-844c-6d2885cd7efd>","<urn:uuid:1f8ee9c7-1b49-44ff-b3cb-626be5683ae6>"],"error":null}
{"question":"What steps can be taken to decolonize development practices in the Global South?","answer":"Decolonizing development practices requires several key steps: First, recognizing and validating epistemologies from the Global South rather than privileging Western knowledge systems exclusively. Second, moving beyond human/nature hierarchies that treat the environment merely as a resource to be extracted. Third, acknowledging and addressing the ongoing impacts of white supremacy and settler colonialism in development frameworks. Fourth, incorporating indigenous environmental justice perspectives and local knowledge systems into development planning. Finally, reconstructing development approaches to reflect pluriversal rather than universal solutions, recognizing that there are multiple valid paths to progress beyond Western models of modernization.","context":["Asher, K. and Wainwright, J. (2018). After Post-Development: On Capitalism, Difference, and Representation. Antipode, 51(1), pp.25–44.\nBhargava, R. (2013). Overcoming the Epistemic Injustice of Colonialism. Global Policy, 4(4), pp.413-417.\nBonds, A., and Inwood, J. (2016). Beyond white privilege: Geographies of white supremacy and settler colonialism. Progress in Human Geography, 40(6), 715– 733.\nBrahinsky, R., Sasser, J. and Minkoff‐Zern. L.A. (2014). Race, Space, and Nature: An Introduction and Critique. Antipode, 46(5), pp.1135-1152. 733. https://doi.org/10.1177/0309132515613166\nCasas, T. (2013). Transcending the Coloniality of Development: Moving Beyond Human/Nature Hierarchies. American Behavioural Scientist, 58(1), pp.30–52.\nda Silva, D. F (2014). Race and Development. In: V. Desai and R. Potter (eds), The Companion to Development Studies, London: Routledge, pp.39–42.\nde Sousa Santos, B. (2018). The End of the cognitive Empire: The comin of age of epistemologies of the South. Duke University Press.\nDussel, E. (1996). The underside of modernity. Atlantic Highlands, NJ: Humanities Press.\nGómez-Barris, M. (2017). The Extractive Zone: Social Ecologies and Decolonial Perspectives. Durham: Duke University Press.\nGrosfoguel, R. (2000). Developmentalism, Modernity, and Dependency Theory in Latin America. Nepantla: Views from South, 1(2), pp.347–373.\nKothari, A., Salleh, A., Escobar, A. Demaria, F. and Acosta, A. (2019). Pluriverse: A Post-Development Dictionary. New Delhi: Tulika Books.\nKothari, U. (2006). An agenda for thinking about ‘race’ in development. Progress in development studies, 6(1), 9–23.\nMoraña, M., E. Dussel, and C. Jáuregui (eds.) (2008). Coloniality at large: Latin America and the postcolonial debate. Durham, NC: Duke University Press.\nMignolo, W., and A. Escobar (eds.) (2010). Globalization and the decolonial option. London and New York: Routledge.\nMignolo, W. (2011). The darker side of Western modernity: Global futures, decolonial options. Durham, NC: Duke University Press.\nMpofu, B. and S. J. Ndlovu-Gatsheni (eds) (2019). Re-Thinking and Unthinking Development: Perspectives on Inequality and Poverty in South Africa and Zimbabwe. New York: Berghahn Books.\nOslender, U. (2016). The geographies of social movements: Afro-Colombian mobilization and the aquatic space. Durham, NC: Duke University Press.\nParenti, C. (2011). Tropic of Chaos: Climate Change and the New Geography of Violence. PublicAffairs.\nPatel, K. (2020). Race and a decolonial turn in development studies, Third World Quarterly, 41:9, 1463-1475, DOI: 10.1080/01436597.2020.1784001\nPowell, D. E. (2006). Technologies of Existence: The indigenous environmental justice movement. Development, 49(3), pp.125–132. https://doi.org/10.1057/palgrave.development.1100287\nQuijano, A. (2000). Coloniality of power, Eurocentrism, and Latin America. Nepantla: Views from South, 1(3), pp.533–80.\nQuijano, A. (2007). Coloniality and modernity/rationality. Cultural Studies, 21(2–3), pp.168–78.\nReiter, B. (ed.) (2018a). Constructing the pluriverse: The geopolitics of knowledge. Durham, NC: Duke University Press.\nRobinson, C. J. (2005). Black Marxism: The Making of the Black Radical Tradition. University of North Carolina Press.\nRobinson, C. J. (1987). Capitalism, Slavery and Bourgeois Historiography. History Workshop Journal, 23(1), pp.122–140. URL: https://doi.org/10.1093/hwj/23.1.122\nRodney, W. (1972). How Europe Underdeveloped Africa. Washington, D.C.: Howard University Press.\nSingh, J. (2018). Unthinking Mastery: Dehumanism and Decolonial Entanglements. Durham, NC: Duke University Press.\nShilliam, R. (2014). Race and Development. In: H. Weber (eds), The Politics of Development: A Survey, Abingdon: Routledge, pp.31–48.\nSlater, D. (2004). Geopolitics and the post-colonial: Rethinking north–south relations. Oxford, UK: Blackwell.\nWhite, S. (2002). Thinking Race, Thinking Development. Third World Quarterly, 23(3), pp.407–419.\nWilson, K. (2012). Race, Racism and Development. London: Zed Books.\nWilson, S. (2018). Energy imaginaries: feminist and decolonial futures. In: Bellamy B. R. and J. Diamanti (eds), Materialism and the Critique of Energy, New York: MCM.\nZouache, A. (2016). Race, competition, and institutional change in J. R. Commons. The European Journal of the History of Economic Thought, DOI: 10.1080/09672567.2016.1174279."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:b0cf9099-37e1-4392-b770-fa5f54333dd8>"],"error":null}
{"question":"What role do early warning systems play in both toxic epidermal necrolysis detection and African food security monitoring?","answer":"In toxic epidermal necrolysis, early warning signs include high fever and flu-like symptoms 1-3 days before skin peeling, requiring immediate medical attention especially after starting new medications. Similarly, in African food security, early warning systems have been implemented through Gro Intelligence's Food Security Tracker, which provides real-time data on crop conditions, drought indices, and supply-demand metrics for 49 African countries, allowing organizations to anticipate food shortages and direct relief efforts before crises escalate.","context":["Toxic Epidermal Necrolysis in Children\nWhat is toxic epidermal necrolysis in children?\nToxic epidermal necrolysis is a life-threatening skin disorder. It causes blistering\nand peeling of the skin. It can be caused by a medicine reaction. A milder form of\nthe disorder is known as Stevens-Johnson syndrome.\nWhat causes toxic epidermal necrolysis in a child?\nThe condition is most commonly triggered in the first 8 weeks of using a new medicine.\nIt may be caused by medicines for:\n- Infections caused by Mycoplasma pneumoniae or cytomegalovirus\nIn rare cases, the condition may be caused by:\n- A vaccine\n- Herbal medicine\n- Contact with chemicals\nWhich children are at risk for toxic epidermal necrolysis?\nA child is at risk if he or she has:\n- Weak immune system\n- Family history of toxic epidermal necrolysis or Stevens-Johnson syndrome\nWhat are the symptoms of toxic epidermal necrolysis in a child?\nSymptoms can occur a bit differently in each child. They can include:\n- Skin peeling in sheets with or without blistering, leaving large, raw areas\n- A painful, red skin area that spreads quickly\n- High fever and flu-like symptoms 1 to 3 days before skin peeling\n- Pain with exposure to light (photophobia)\n- Fatigue, muscle pain, and joint pain\n- Trouble swallowing\nThe condition may spread to the eyes, mouth or throat. And it may spread to the genitals,\nurethra, or anus. The loss of skin allows fluids and salts to ooze from the raw, damaged\nareas and can easily become infected.\nThe symptoms of toxic epidermal necrolysis can be like other health conditions. Make\nsure your child sees his or her healthcare provider for a diagnosis.\nHow is toxic epidermal necrolysis diagnosed in a child?\nThe healthcare provider will ask about your child’s symptoms and health history. He\nor she may also ask what medicines your child has had recently. He or she will give\nyour child a physical exam. Your child may also have tests, such as:\n- Skin biopsy. A tiny sample of skin is taken and checked under a microscope.\n- Cultures. These are simple tests to check for infection. Cultures may be done of the blood,\nskin, and mucous membranes.\nHow is toxic epidermal necrolysis treated in a child?\nTreatment will depend on your child’s symptoms, age, and general health. It will also\ndepend on how severe the condition is. If a medicine is causing the skin reaction,\nyour child will stop taking it right away. The disease progresses fast, usually within\n3 days. Your child will need to be treated in the hospital. He or she may be in the\nburn unit of the hospital. This is because the treatment is similar to treating a\nchild with burns. Or your child may be treated in the intensive care unit (ICU). Treatment\n- Isolation to prevent infection\n- Protective bandages\n- IV fluid and electrolytes\n- IV immunoglobulin G, to prevent more immune system damage\n- IV steroids\n- Feeding by nasogastric tube, if needed\n- Eye exam by an ophthalmologist and prompt eye care with cleaning of eyelids and daily\nWhat are the possible complications of toxic epidermal necrolysis in a child?\nComplications can include:\n- Skin color changes\n- Growth of many moles that don’t look normal (nevi)\n- Abnormal growth of finger and toenails\n- Loss of hair\n- Scarring of the skin, from a moderate to severe infection\n- Eye changes\n- Oral changes include periodontal disease\n- Lung damage\nWhat can I do to prevent toxic epidermal necrolysis in my child?\nThere is no known way to prevent the condition. But a child who has had the disease\nmust stay away from all possible triggers. A future episode of the condition may be\nfatal. Your child needs to stay away from not only the medicine that triggered the\ndisease, but medicines in the same class. Talk with your child’s healthcare provider\nabout which medicines your child needs to stay away from.\nWhen should I call my child's healthcare provider?\nGet medical care right away if your child has any skin problems after taking a new\nKey points about toxic epidermal necrolysis in children\n- Toxic epidermal necrolysis is a life-threatening skin disorder that causes blistering\nand peeling of the skin.\n- It is most often triggered within the first 8 weeks of using a new medicine.\n- A high fever and flu-like symptoms usually occur first. Then skin changes occur. These\ninclude painful redness, peeling and raw areas of skin.\n- If your child has a fever and skin changes after starting a new medicine, get medical\ncare right away.\nTips to help you get the most from a visit to your child’s healthcare provider:\n- Know the reason for the visit and what you want to happen.\n- Before your visit, write down questions you want answered.\n- At the visit, write down the name of a new diagnosis, and any new medicines, treatments,\nor tests. Also write down any new instructions your provider gives you for your child.\n- Know why a new medicine or treatment is prescribed and how it will help your child.\nAlso know what the side effects are.\n- Ask if your child’s condition can be treated in other ways.\n- Know why a test or procedure is recommended and what the results could mean.\n- Know what to expect if your child does not take the medicine or have the test or procedure.\n- If your child has a follow-up appointment, write down the date, time, and purpose\nfor that visit.\n- Know how you can contact your child’s provider after office hours. This is important\nif your child becomes ill and you have questions or need advice.","Gro Intelligence launches the first publicly available interactive tool on key agricultural commodities for 49 African countries\nNEW YORK, May 19, 2022 /PRNewswire/ — During the United Nations Security Council’s session on Conflict and Global Food Security, Gro Intelligence’s CEO, Sara Menker, spoke about the growing global food crisis, its disproportionate impact on lower-income countries, and the policy actions that can be taken by governments around the world to mitigate these effects. As part of a broader response to this crisis, Gro Intelligence is launching the Food Security Tracker for Africa, the first-of-its-kind, interactive tool that makes real-time agricultural data on 49 out of 54 African countries publicly available in one location. With The Rockefeller Foundation’s support, this information will make it easier for countries around the world to navigate the unprecedented challenges connected to the current global food crisis.\nThe Food Security Tracker for Africa provides free access to real-time data about the supply and demand of major crops, including corn, soy, wheat, and rice for African countries. By combining data on drought, crop conditions, prices, supply and demand all in one place, users will be able to develop more effective solutions and emergency response plans to the growing shortages of key agricultural commodities across the continent.\nEnvironmental, economic, and political shocks have caused rising food prices and created shortages of major crop staples worldwide. At the same time, companies across the global agricultural supply chain face significant blind spots, donors are unable to accurately direct funds, and governments are left scrambling for alternative sources of supply without the necessary full knowledge of where it is needed most. In response, Gro is collaborating with The Rockefeller Foundation to give the public greater access to critical data, which will help fill the gaps in accurate supply and demand coverage for major crops in Africa.\n“The world must act now to respond to the global food emergency and alleviate the human suffering and global instability it is causing,” said Dr. Rajiv J. Shah, President of The Rockefeller Foundation. “Gro Intelligence’s powerful new tool gives global leaders the data they need to not only respond to the crisis in the short term, but also lay the groundwork for a more stable, sustainable food system over the long term.”\nUnderstanding the Impact of the Global Food Crisis\nEven before the war in Ukraine, the World Food Programme (WFP) estimated 810 million people did not have enough to eat. According to recent data from the International Monetary Fund, poor country households spend up to 60% of their budgets on food, compared to just 10% for the average household in advanced economies. Unable to weather the shock of rising food prices, lower-income countries are also being asked to pay out more than $300 billion in interest payments and debt repayments while many global organizations focused on food security are facing significant funding shortages – as Ms. Menker and Dr. Shah explained in a recent New York Times op-ed.\n“By combining cutting-edge technology and humanitarian relief efforts and leveraging the private sector for public use, our collaboration with The Rockefeller Foundation will help strengthen food security initiatives, address inequities, and build a sustainable world for all,” said Ms. Menker. “With this new tool, governments, companies, and humanitarian organizations will be better equipped to anticipate food shortages, direct relief, and improve strategic planning in response to the unprecedented level of supply and demand shocks that have caused global food insecurity.”\nLeveraging the Power of the Gro Platform\n“To create a more comprehensive picture, the Gro team, which includes both domain experts and technologists, leveraged our platform and the scaling power of our machine-learning models to quickly and accurately provide needed data,” said Will Osnato, Senior Research Analyst at Gro Intelligence. “With support from The Rockefeller Foundation, we will offer agricultural balance sheets that denote supply and demand of corn, soy, wheat, and rice for the next year. In addition, the tool has been tested and reviewed by our analyst team and methodologies are made available to fully encompass the transparency and objectivity of our platform.”\nIn addition to real-time supply and demand data, this tool makes useful metrics for 49 out of 54 African countries publicly available for the first time, including:\n- Gro’s Production Forecast – Production estimates are calculated using Gro’s machine learning-based yield forecasts, which incorporate real-time environmental data and historical production data to predict available supply.\n- Gro’s Stocks-to-Use Ratio – A country’s reserves of a specific crop is an indicator of food security. A stocks-to-use ratio shows the relationship between stocks and usage. Gro’s Stocks-to-Use Ratio is calculated as total food calorie stocks at the end of the marketing year – a period of one year designated to production analysis of a specific commodity. This number is then divided by total food calorie demand (domestic consumption + exports) across the four crops in the selected region. It is highly correlated to prices over the season.\n- Cropland-Weighted Gro Drought Index (GDI) – The proprietary Gro Drought Index is the world’s first high resolution global agricultural drought index. The GDI measures drought severity on a scale from “0” (no drought) to “5” (exceptional drought). The index is global, offering data on the continent, country, state, and district level and updates weekly on the interactive tool and daily on the Gro platform. The values shown on the tool are weighted by cropland area at the district level for each country.\n- Crop-Area Weighted Vegetative Health Index (NDVI) – NDVI is a key satellite-based indicator of plant health, used to forecast crop production, supply, and price. Lower NDVI signals lower levels of production.\n- Prices – Price series were selected based on Free-on-board (FOB) export prices from the largest import supplier for the selected country. If the country is not a significant importer, then representative global prices were selected.\nAbout Gro Intelligence\nGro Intelligence works with companies, financial institutions, humanitarian organizations, and governments to forecast risks to food security that may result in food or hunger crises. Our food security platform serves as a single source of truth and an early warning hub that provides up-to-date information, insights, and analysis across the value chain. The platform predicts future trends and promotes proactive, evidence-based decision-making to improve our partners’ food security. See more on our work with the public sector here.\nAbout The Rockefeller Foundation\nThe Rockefeller Foundation is a pioneering philanthropy built on collaborative partnerships at the frontiers of science, technology, and innovation to enable individuals, families, and communities to flourish. We work to promote the well-being of humanity and make opportunity universal. Our focus is on scaling renewable energy for all, stimulating economic mobility, and ensuring equitable access to healthy and nutritious food. For more information, sign up for our newsletter at rockefellerfoundation.org and follow us on Twitter @RockefellerFdn.\nView original content to download multimedia:https://www.prnewswire.com/news-releases/new-agricultural-data-tool-can-help-fight-the-growing-food-crisis-in-africa-301550956.html\nSOURCE Gro Intelligence"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:b0b9b29d-9225-4688-95cc-f5fba624e030>","<urn:uuid:1402cd0c-75f8-4ffe-a3b0-7079216ad2c8>"],"error":null}
{"question":"What are the recommended bait choices for winter catfishing, and what equipment specifications are necessary for deep-water fishing?","answer":"For winter catfishing, live natural bait like shiners is most effective, as stink bait and chicken livers become less effective in cold water. As for equipment, heavy-duty tackle is essential for deep-water fishing. The recommended setup includes 80 lb. test braided main line, 3-8 ounce slip sinker, 150lb. barrel swivel attached with a Palomar knot, and an 18-inch piece of 50 lb test Monofilament leader line. The rig should be completed with number 0/10 Mustad circle hooks tied with a Snell knot. While medium to heavy-duty rod and reel combinations from department stores suffice for occasional fishing, more durable quality equipment is recommended for consistent deep-water catfishing.","context":["People often want to know the best time to catch catfish. The simple answer is whenever the water temperature is favorable, and the baitfish are schooling.\nIn reality, the answer is much more complicated. Catching catfish relies on the weather, the water temperature, the season, and the availability of plentiful food. Catfish behavior changes in cold weather and they move to deeper water. During the spring spawning season, you will find catfish actively feeding on the fry that has just hatched. This springtime feeding frenzy is the time when channel catfish and flathead catfish gorge on food to gain back the weight they lost in winter.\nBest Seasons and Times to Catch Catfish\nWhile it is possible to catch big catfish year-round, it is easier to find catfish in the spring when the spawn is ongoing, or in the fall when they are moving out of deeper water to enjoy the shorter days and cooling temperatures.\nBest Time to Catch Catfish in Summer\nDaytime fishing in the summer will not be very productive. The bright rays of the sun, combined with warm water temperatures, make fish less likely to bite. During early summer, when water and air temperatures are not excessively hot, you might have some luck presenting catfish rigs in the middle of the water column if you go out in the early morning. As the sun rises, prey fish move into deeper waters and hide in debris and structures. Because catfish are low-light feeders, they will move out into the shade of cover and fallen trees or move into deep water.\nBest Time to Catch Catfish in Spring\nCatfish anglers love spring fishing. Not only is it spawning season, but the weather is nice and the fish are hungry. In the late spring, target catfish by using live bait or cut bait that has been allowed to ferment for a few days in a sealed mason jar. The reason this stink bait works is because it resembles the dead bait fish that float down the river in the winter thaw.\nFishing points along Ohio river basins include any places where streams or small tributaries join larger bodies of water. More catfish will be found in these transitional areas than in open water. Channel catfish and flathead catfish will gather at the mouth of an incoming river or stream, hoping to feed on small fish that are caught in the current.\nBank fishing will yield great results as blue catfish and channel cats come out of their bankside holes in search of food. Float catfish baits just above or below the hole. Stink baits will help lure the fish out and you can catch some monster catfish that are just emerging from their winter rest.\nIn late spring, the catfish spawning season lasts for a week or so. Fish will move close to the shore and you can catch monster catfish by shore fishing along muddy river banks. During the spawn, catfish bite anything that gets too close to the nest. The best catfish bait to use during the spawn is live natural bait, like a minnow or a worm.\nBest Time to Catch Catfish in Fall\nLike springtime, fall is a great time to catch catfish at just about any time of the day. Some fish species, like bass and trout, will have a seasonal spawn event that attracts big catfish.\nAs the weather cools and bigger fish move into deeper water, many anglers will begin drift fishing closer to river bottoms. For best results, make a catfish rig by creating a leader from a stiff monofilament line and bait it with a chicken liver or commercial catfish bait. Use a heavy egg weight to keep the bait on the bottom. Slowly float with the natural current of the river. If your vessel is moving too fast, use one or more drift socks to slow your float.\nDrift fishing not only allows you to cover more water, but it puts your bait in front of more and bigger catfish. Drifting is a great technique for catching blue and channel cats on deep water rivers, although you can catch any catfish species using a similar method.\nBest Time to Catch Catfish in Winter\nIn the winter, catfish travel down the river to find warmer water and to follow the live bait. No matter what catfish species you are after, using live natural bait is the best way to catch winter catfish. Stink bait or chicken livers won’t be the right bait to attract blue cats or channel catfish when days are short and temperatures are cool. Your best bet is to fish in the middle of the day with shiners.\nUse a good quality fishing line and a reel that lets you drop the bait straight down. Catfish tackle in the winter is not any different from the rest of the year. Use a sharp hook suspended about eight to twelve inches below a heavy egg weight. Use your sonar or fish finder to locate fish beds on the bottom. Winter catfish fishing is just a matter of having the bait close enough to the fish that he will bite it.\nNot all catfish species move into deep water beds during the winter like blue cats and channel cats do. Winter is the time when flathead catfish pretty much quit feeding. Instead, they hang out around standing trees in the middle depths. Getting a flathead catfish bite when water temperatures are in the 40-50° range is just about impossible.\nHowever, if you get an occasional warm spell during the winter months you may find a day to catch catfish that have moved into shallower water to soak up some sun rays and grab a quick snack.\nMost catfish anglers who winter fish choose drift fishing on deep rivers in the mid-afternoon. Rig up a Santee rig and float it over the fish beds, just above the bottom. You can try using cut bait but will have better luck if you fish for catfish with live minnows.\nWhat’s Better: Morning vs Evening?\nAgain, this is a very broad question. Factors like location, other species in the water, and spawning season are important considerations when determining what time of day to fish.\nIn the summer, when temperatures are extremely hot out, more anglers will fish overnight and into the early morning hours because the air temperature has cooled after dark. While many anglers claim this is because water temperatures have cooled during the night, this is not entirely true. Many other factors affect water temperature, including groundwater run-off, an influx of glacial melt, the number of upstream reservoirs and dams, and the physical geography of the body of water. While thermal air temperatures affect water temperature to a degree, the ratio of warming is so minor that there is no major difference in lake or river water within a 24-hour period unless a major weather-related event such as heavy rainfall or hail occurs.\nThe fact is catfish are hungry at all times of the day and night, and they will eat when they are hungry. Your likelihood of catching catfish in the morning is the same as in the evening. There is no best time to catch catfish. Catfish react to their environments. If there are more fish gathered along the shoreline in the evening, you will have a better chance to catch catfish in the evening. If the bait fish are schooling more in the morning, you will catch catfish in the morning.\nWhat’s Better Overall For Catfish: Day vs Night?\nMost anglers will tell you to fish for catfish at night during the warm months of summer and early fall. While this is a good general rule, catching catfish during the day is possible if you fish close to the bottom in deep water. If you go out during the day to catch catfish and the water temperature is above 74°, your best bet is to locate their beds and bounce the bait off the bottom in that area. Catfish tend to spend their days being lazy, hoping for a food source to come along. Look for catfish beds on the ledges of deep drop-offs, on sandy lake bottoms, or in shady spots littered with debris in shallow water pools with a muddy bottom.\nIf you choose to go night fishing, fish in the middle of the water column in open areas of the lake. On rivers, streams, and ponds, you can catch catfish closer to the shore as they move to shallow water to join the feeding frenzy that takes place as certain fish species seek nocturnal insects, minnows, and frogs. Catfish feed on these small panfish, as well as frogs and larger minnows. More fish in a smaller area will attract more large fish, including bigger catfish.\nThe best time to fish for Flathead Catfish\nSpring is the best time to fish for flathead. During this time, you can find flathead catfish hiding behind boulders or under submerged debris. In areas where streams join larger waterways, warm stream water creates a temperate, comfort zone, and flatheads will congregate in these areas to enjoy the feeding frenzy that occurs when eggs hatch and fry emerge. You can fish pretty much any time of the day in spring.\nThe best time to fish for Blue Catfish\nLike flathead catfish, blue catfish are easiest to catch in the spring months from March until May. Southern anglers fish for blue cats in the warm spring months of April and May, dragging heavy sinkers along the bottom of the Mississippi River and its tributaries.\nThis late spring or early summer catfishing usually produces the biggest blue catfish and channel catfish. The fish have filled up on eggs and small fish and their bellies are swollen.\nTo catch blue catfish in the spring, fish in shallow water in the morning and evening. In the middle of the day, blue catfish will move into the middle of the water column.\nThe best time to fish for Channel Catfish\nChannel catfish will be most active in the early summer, between May and July, before the dog days of summer set in. These small, spotted fish prefer warm weather and are most active in the early evening. Anglers fish for catfish just before sunset. While smaller channel catfish can be found in the shallows, you will want to fish on the downstream side of sandbars for big channel cats.\nBest Time for Catching Catfish in Rivers, Lakes and Ponds\nIn streams and smaller rivers, catfish fishing is most productive in the early spring when water temperatures are between 60 and 70°. Panfish are spawning and channel catfish are moving into those warm spots where rivers and streams come together. Smaller fish will be most active from sun up to mid-morning, and that is when channel cats will chase bait.\nBlue catfish like to hang out in larger rivers. Because the water is so much deeper, it will have less fluctuation in temperature as the sun rises. One thing that will bother the blue catfish, though, is the light penetration from the mid-afternoon sun. If you are fishing for blue catfish or flathead catfish in a large river, mid-morning and just before sunset are the prime times.\nSome anglers prefer night fishing on the Mississippi or Ohio Rivers when fishing for giant blue catfish. Many a catfish angler has spent the night fishing the river with a spotlight.\nAnother great time to catch big blue catfish is in the late summer or early fall when the fish are moving north, into the cooler water of the Missouri and upper Mississippi Rivers. Early evening seems to be the best time to catch catfish in these northern rivers.\nSmall farm ponds can produce massive catfish, but you are more likely to catch two to three-pound channel cats and bullheads. In the spring and fall, when there is insect larvae, fry fish, and tadpoles to eat, catfish will easily bite in the morning or early evening. In the hottest part of the summer, they will burrow into the mud, and it will be harder to catch them. Fish the bottom when it is hot out. Fish the banks and shoreline in the spring or late fall.\nWorst Time of Day for Catfish Fishing\nThe worst time to catch catfish is whenever the water is too hot or too cold. Catfish like water temperatures between 50 and 80°. If the water is too cold, they will refuse to budge from their beds on the bottom. If the water is too hot, they will move into cooler water in the bottom third of the water column, but will not want to expend the energy to chase bait.\nIn the summer, when temperatures soar into the upper 90’s and 100’s, no fish is going to want to bite, including catfish. If you must fish in the summer, try night fishing, but you may or may not catch a fish.\nDoes The Moon Affect Fishing for Catfish?\nThere is much debate among anglers about the moon phases and their affect on fishing. Some swear that the fishing is better during the new moon when there is no moonlight shining on the surface of the water. Others will tell you a waning moon is better than a waxing moon. Most will agree that a full moon produces the worst results when they fish for catfish.\nProfessional anglers insist that water conditions and water clarity are much more of a factor than the moon phase. In stained or brackish water, it might be possible to fish for catfish in a full moon because the rays don’t penetrate the water as deeply. In very clear water, even the most minor moonlight might affect the fish.\nIf you want to catch more catfish while night fishing, concentrate more on water temperature and clarity rather than the moon.\nIt is possible to catch channel cats, blue catfish, and flathead cats just about any day of the year, if the conditions are right.\nYou will definitely catch more catfish in the late spring and early summer months when water temperatures are optimal and there are plenty of small fish to provide nourishment after a lazy, dormant winter.\nWinter fishing is tough, but in the warmer waters of the deep south, you might find some blues in the deep water of the larger rivers or deep lakes. Try drift fishing for catfish in the mid-morning or the early evening.\nThe hottest part of summer can produce fish, particularly bullheads pulled out of muddy farm ponds while night fishing, but why suffer through the heat and humidity knowing that you probably won’t catch much?\nNo matter what time of day you fish or which season, a few constants remain in place.\nFirst, you should always use something that resembles natural live bait. This might be a minnow, a frog, or a grub, but it should be something the catfish is going to be familiar with eating.\nSecond, fish when water temperatures are between 50 and 80 degrees. Catfish have been caught while ice fishing in 40-degree lakes, but it is not a common occurrence.\nThird, When fishing a deep lake or river, use your sonar to locate fish along the water column or to search for submerged debris or deep holes where fish may hide.\nThe biggest tip for catching catfish is to educate yourself. Learn about the water bodies you intend to fish. Study average water temperature graphs. Learn which natural baits live in the waters. Ask the old-timers or hire a professional guide your first time out.\nCatching record-breaking catfish is no different than catching any prize-winning fish. It is part skill, part education, and a whole lot of luck.","Precautions of catching deep water catsCATC Support\nPrecautions of catching deep water cats\nCatfishing in the winter can be categorized in my book as one of the best times to catch a trophy sized catfish on the Ohio River as they have migrated to and have gathered in numbers to their deep-water lairs where they appear to be most comfortable at during these cold harsh winter months and they will spend most of their time and energy cruising these haunts for food.\nThese areas also attract many other species of fish during the winter months including many types of schooling forage fish such as carp, shad, and mooneye which is one of the reasons the big ole cat is drawn to these deep water areas because it’s (What’s for Dinner) and these types of fish is also your best choice for bait as well, because it’s on the main menu for the big cats.\nOne technique commonly used to catch these winter time catfish is precision anchoring on deep water holes. First by locating specific pieces of structure or cover on your depth finder and motoring above the area moving up current then dropping your anchor and casting your baits to that specific area.\nAnother anchoring technique called bounding down is not as target specific but rather more of a cast and search technique working along the vast areas of these deep holes and runs.\nThis method involves moving from an anchored position frequently downstream and re-anchoring again 50-75 yards apart looking specifically for active and feeding catfish while also keeping your baits in one scent trail as an attractant for the cats to follow and search for as you work downstream.\nThe most common rig to fish for these deep water cats is the Carolina rig but keep in mind these winter months can produce some of the biggest pigs of the year and your tackle needs to be able to handle these big fish.\nBig catfish tackle\nThere is nothing worse to a fisherman than seeing this massive catfish surface only to have him surge and snap the wrong size tackle inches from being landed.\nHere is what I use and recommend in “big fish” tackle. First I use 80 lb. test braided main line from McCoy fishing line.\nBraided main line\nNext, I thread a 3-8 ounce slip sinker on and then attach a 150lb. barrel swivel with a Palomar knot and then tie an 18-inch piece of 50 lb test Monofilament leader line to the other end of the swivel with an improved clinch knot.\nThe last step is to attach a hook to the end of the leader line. For this, I use number 0/10 Mustad circle hooks and tie it on using a Snell knot.\nThis is my Go-to rig when I’m anchoring on these big deep water catfish.\nWinter time catfish rig\nYour equipment should be compatible with the size fish you are searching for as well. Most any medium to heavy duty rod and reel combos that are available at most local dept/sports stores will get the job done and are great deals if you are only fishing a few times a year, but if you are fishing more consistently for the big cats you may want to do some research and purchase some longer lasting quality equipment.\nCaution, releasing catfish that has been caught from deep water could be fatal to the fish.\nI have already touched on the bait choices for attracting these deep water wintering catfish along with the Where, When and How to catch them but I would also like to talk about the importance of a safe release.\nRelease you say.\nYes, over the last 10 years the lure of catching these behemoth size creatures has risen in numbers to where the sportsmen are now categorizing catfish as a sports fish and are introducing new regulations in favor of protecting them.\nThese large adult fish should never be kept for table fair as they have spent many, many years exposed to the toxins that have flowed within the water shed. The smaller younger catfish are much better for eating and has less of a risk of toxins.\nSo as a sportsman who fishes for catfish year round, my interest is to protect the large adults as well as the growing population so we can help sustain a healthy and strong fishery. Taking care of my catch for a safe release ensures that future anglers will have a chance to experience the adrenaline pumping hard fight that these magnificent fish give us which brings us to my main topic within this article.\nReleasing deep water catfish\nA catfish’s swim bladder is an internal gas-filled organ that contributes to the ability of a fish to control its buoyancy and Hooking catfish in deep water and reeling them up too quickly will result in gas expansion in their swim bladders causing them to float and not being able to return to the bottom when released, causing death. Keep in mind however that this is a year-round issue and not just limited to winter time only.\nDecompression is needed\nAlthough many anglers today have the best intentions when it comes to “catch and release” practices and want the best for the fishery but many novice anglers are unaware of this issue and will often release their fish with the pressurized swim bladders. They may appear to swim away but will resurface downstream struggling just out of sight and away from the anglers who just released it and more than likely will not make it.\nLearning the signs of this decompression phenomenon and knowing how to treat them for a safe release will help ensure positive population growth which is an important part of the conservation and management of a very cool resource.\nRecognizing the problem is fairly easy, experiencing it is just as easy and even though an angler is aware of this issue and tries to bring a deep water blue cat up from the depths slowly to allow it to decompress naturally, certain variables out of our control will just allow the fish to surface too quickly, creating this health risk for our majestic and noble adversaries.\nThe first signs of this problem will be noticed when the catfish first surfaces at the boat and just rolls on its side or back then shows little or no movement and no fight but appears to want to float in an unnatural state. Inspect the belly section before you let it go.\nA fat belly from a fish that has fed heavily will look totally different from those that had been pulled to the surface too quickly.\nEven when the fish has been gorging its self, the belly will remain soft to the touch and shouldn’t be mistaken for the oddly shaped and bloated look with extremely hard pressurized bellies which will be tight drum like to the touch caused from expanding gasses that developed during the fight.\nNot to worry though as this is an easy fix when needed.\nAll these fish needs to do is relieve some pressure as in a “BURP” so to say and most times you can help them do it naturally by stalling your catch at the boat. Once you can see that fish is hooked good let it swim alongside the boat for an extra minute or two.\nIn most cases the fishes sudden movements of thrashing and twisting will naturally release these gasses, removing the risk of becoming a floater and resetting the pressure on the fish’s swim bladder so to say.\nIt just takes a little time for them to blow off these excess gasses and they can be returned safely.\nMost of the time this can be achieved by simply letting them swim around a bit to the surface to blow the pressure and the constant movement helps the process happen quicker but other times, there are other instances where you may have to bring fish on board and place them in a controlled live tank to monitor while you help it recover for a safe and sure release.\nAnother effective option is to manually Burp them with a tube that has been inserted down their gullet.\nSome anglers prefer to use a ½ inch PVC tube about 18 inches long to perform this action but this is my least favorite way to do it and my last resort as I feel this can sometimes cause more harm than good in the long run.\nAlthough this way is one of the fastest ways to achieve your end goal of burping the fish, it can sometimes damage the fish’s esophagus as the fish begins the resist and thrash with the hard tube inserted 15” inches within their bodies vital organs.\nIt’s a double-edged sword.\nYou’re trying to save the fish by manually burping the fish so you can release it for future action but end up slowly killing the fish because an infection from a hole that was punched in its stomach lining from a piece of PVC that was improperly pushed in their gullet while thrashing and resisting on deck.\nI just like to take the little extra time with them at the boat to help them Burp when possible.\nBut In some situations like faster current, when you cannot leave them at the side of the boat for a period of time I’ll bring them into the boat and lay them on deck for a few minutes massaging the bellies, the best medicine I’ve found is just keep them moving and it seems to work out of them.\nIf I’m still unsure, I will hold them in a live tank to be certain that the fish has relieved its pressures and can be safely released.\nPassing this information on to others I feel will help in conservation awareness with the growing numbers of catfish enthusiast not only in Kentucky but across the nation as well. It’s important to share in the responsibilities of good conservation practices from our sportsman and educate our future enthusiast so they can continue to promote and maintain healthy fisheries while also growing a positive family outdoor activity.\n“If we are going to play with these huge cats we should respect them and strive to put them back in their environment just as good as when we pulled them from it”."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:f53c261d-e8c8-4207-80eb-05cff8a9149a>","<urn:uuid:9a1655b7-70a4-4e4e-a3e4-eb90cef5f6cb>"],"error":null}
{"question":"Which is more effective for injury prevention: Nordic walking or regular walking with proper warm-up?","answer":"Nordic walking is more effective for injury prevention compared to regular walking. Nordic walking does not aggravate joints and knees and reduces the load on joints, while also providing greater fitness benefits. It increases heart rate by 5-17 beats per minute higher than regular walking and increases energy consumption by up to 46%. However, any walking activity should be preceded by proper warm-up, which increases blood flow to muscles, reduces muscle viscosity, enhances mechanical efficiency, and decreases the likelihood of injuries by reducing connective tissue stiffness.","context":["Mom, I'm Bored\nBy Kim Watters, Fitness Manager\nIt can be a challenge to keep the kids entertained past the first few days of a school break. It seems that once the newness of sleeping in and playing videogames has worn off, parents feel compelled to entertain them or find something new for them to do. Instead of joining them on the couch with a bag of chips, try these fun and fit activities for both you and the kids.\nHula Hoops are Hot, hot, hot again\nMoms, the new weighted sports hoops ($12-$20) will shape your waist, hips, thighs and arms in no time, and the kids will have a great time trying to perfect these new moves and tricks with their own hula hoops. There are DVD's, instructional books you can purchase or just search \"Hula Hoop exercises\" online and get some great tips for free. Make sure you get the right size hoop; generally the hoop should reach between your waist and chest when held in front of your legs and tipped on its side.\nJumping rope like the Joes at the gym\nJumping rope has always been a great cardiovascular workout in the gym; you can plan on burning up to 135 calories in just 10 minutes. Don't get bored, spice it up with some tricks and games from the school yard. Just ask your kids, they'll love to show you how it is done on their playground.\nToo cold outside? Glide inside instead.\nGliding disks add intensity and resistance to your workout and are made for use on carpet. But let's face it, they are slippery and fun to play with, too. There are a few different styles of disks that you can choose to buy, or you can just use paper plates with a wax coating. Again, there are many free exercises to choose from on line as well as DVD's and instructional books to purchase. However, just playing tag, racing, dancing or playing \"Simon Says\" with them will give you an excellent workout and you can't find a much cheaper piece of exercise equipment than a paper plate.\nPlaying knocks boredom out of your winter break; it promotes your health and maintains your sanity. Get ready, get set, get fit.\nWorkplace Exercise to Re-energize Your Day\nBicep curls: Slide to the front edge of your chair. Keep your upper arm bone still while you bend your elbow, lifting the water bottles or weights towards your shoulder and then slowly lower them by straightening the elbows. 8-12 repetitionsAdd a comment\nStress Busters for the Holidays\nStand with your knees slightly bent but not locked. Many tense people tend to lock their knees, which immobilizes the whole body. Take a position with your feet about eight inches apart and bend your knees so that the weight of your body is in balance between the heels and the balls of your feet. The rest of your body should be in a straight line with your arms hanging loosely at your side. Let your belly hang out. Don’t force it out but do not hold it in either. Breathe from your belly. Your back should be straight but not rigid, and your pelvis should be relaxed. Hold this position for two minutes. While waiting in line or standing at a party, try this stance. Try to keep it whenever you have to stand for any length of time.Add a comment\nStress Buster for the Holidays\nLie on the floor with a small pillow under your head. Your knees should be slightly bent. Put one hand on your stomach and one hand on your chest. Draw deep breaths into your abdomen and feel it rise. Your chest should hardly move at all. Belly-breathe without your chest moving and exhale through your nose.Add a comment\nCore Strengthening Exercise\nTorso Twists: Hold your hands together at chest level. Keep your knees and hips squared forward; gently twist to the left, tightening the abs as you twist. Return back to center and repeat to the right side. (8-12 repetitions)Add a comment\nExercise to Re-Energize Your Day\nAb Curls: Sit with your back about 4 inches away from your chair. Cross your arms over your chest and sit up straight. Tighten your abdominal muscles and press the middle of your spine in to the back of the chair. Your shoulders should curl towards your hip bones. Hold for 2-3 seconds, and then return to sitting up straight. (8-12 repetitions)Add a comment\nStretches To Do at Your Desk\nBy Fitness Manager Kim Watter\n- Inner thigh stretch: Sit towards the front edge of your chair. Open your legs wide with your knees bent. Put your hands on the insides of your knees and push back opening your legs wider and deepening the stretch.\n- Hamstring & calf stretch: Sit towards the front edge of your chair. Straighten your legs, and pull your toes towards your shins. Lean forward with your hands on the tops of your thighs for support (flexing at your hip joints).\n- Glutes, hip & outer thigh stretch: Sit towards the front edge of your chair. Cross your left ankle over your right knee and lean forward flexing from your hip joints, moving your chest closer to your shin bone.\n- Ab & side stretch: Sit towards the front edge of your chair. lengthen your arms over head and exaggerate your reach. Also, try reaching farther through your right hand lengthening the right side of your body more, then switch reaching through the left hand and the right heel.\nBy Kim Watters, Fitness Manger\nHere at Red Mountain we are always looking for the next adventure, and an outdoor adventure is even better. So we were pumped when we discovered Nordic Walking, also known as ski walking. This exciting activity combines the two methods of traditional fitness walking and cross country skiing, including the use of modified ski poles.\nNordic Walking was first used as a summer training method to keep cross-country skiers in tip top shape through the snow-less days of summer. Later a group of professionals consisting of researchers in sports medicine, and other fitness professionals developed it into a fitness exercise. Nordic Walking was first launched in Finland in 1997. It has rapidly increased in popularity and today Nordic Walking is well-known fitness sports.\nAccording to the International Nordic Walking Association there are many benefits to Nordic walking, here are a few that stand out when compared to traditional fitness walking:\n- Heart rate is 5-17 beats per minute higher (for example in normal walking heart rate is 130 beats/minute and in Nordic Walking 147 beats per minute i.e. increase is 13%)\n- Energy consumption increases when using poles by an average of 20% compared with ordinary walking without poles\n- Up to a 46% increase in energy consumption (Cooper Institute research, Research Quarterly for Exercise and Sports 2002 publication)\n- Releases pain and muscle tension in the neck/shoulder region\n- The lateral mobility of the neck and spine increases significantly\n- The muscles most actively involved are the forearm extensor and flexor muscles, the rear part of the shoulder muscles, the large pectoral muscles and the broad back muscles\n- Does not aggravate joints and knees\n- Reduces the load on knees and other joints\n- Consumes approximately 400 calories per hour (compared with 280 calories per hour for normal walking)\n- Poles are a safety factor on slippery surfaces\nRed Mountain invites you to Nordic Walk with us among the beautiful red rocks of Snow Canyon State Park. Our Fitness staff is ready and can help you reap the benefits of Nordic Walking as we lead you into our next new adventure.Add a comment","Definition of Injury\n“Unintentional or intentional damage to the body resulting from acute exposure to thermal, mechanical, electrical, or chemical energy, or the absence of such essentials as heat or oxygen.”\nCausative factors for sports injuries:\nType of activity\nWhat is Injury Prevention?\nInjuries are preventable by changing the environment, individual behavior, products, social norms, legislation, and governmental and institutional policies to reduce or eliminate risks and increase protective factors.\nPrimary & secondary prevention:\nPrimary prevention is prevention of occurrence of injury\nSecondary prevention is prevention of reoccurrence of injury\nThere are a number of factors responsible for injury prevention. They are:\n1. Warm up, 2. Stretching, 3. Taping & bracing, 4. Protective equipment, 5. Correct biomechanics, 6. Suitable equipment, 7. Appropriate surfaces, 8. Appropriate training, 9. Adequate recovery, 10. Psychology\n1. Warm up:\nThe literary meaning of warm up is to raise the core body temperature. Warm up is further classified in to general & sports specific warm ups.\nBenefits of warm up include:\n1. Increased blood flow to muscles\n2. Reduced muscle viscosity leading smooth muscle contractions\n3. Enhanced muscle’s mechanical efficiency\n4. Favorable neuro- myo conductance\n5. Favorable muscle receptor changes which decreases the sensitivity of muscles to stretch.\n6. Enhanced cardiovascular compatibility\n7. Enhanced mental concentration to sporting activities\nHow warm up helps in injury prevention:\n1. Increase the pre-warm up ROM\n2. Decrease the stiffness of the connective tissue- this further leads to greater forces and length of stretch required for a tear to occur.\nThe ability to move joints smoothly throughout a full ROM is an important component of good health.\nBasic principles of stretching are:\n1. Warm up prior to stretching\n2. Stretch before & after exercise/sports\n3. Stretch gently & slowly\n4. Stretch to the point of tension but not up to the point of pain\nHow stretching helps in injury prevention:\nThere is considerable research evidence to claim;\nIncreased flexibility attended through stretching appears to result in decreased incidence of musculo-skeletal injuries, minimize & alleviate muscle soreness. Further stretching may enhance athletic performance.\n3. Taping & bracing:\nTaping and bracing are used to restrict unrestricted, potentially harmful motion & allow desired motion. There are two main indicators for use of tapes & braces:\n1. prevention- from the above said two procedures taping is used used as a preventive measure for high risk activities. For example ankle taping of basketballers.\n2. rehabilitation- taping is used as a protective mechanism during the healing & rehabilitation phase.\n4. Protective equipments:\nProtective equipment shields various vital body parts against injuries. The most important is that protective equipment must not interfere with sporting activities.\n5. Correct biomechanics:\nCorrect biomechanics is an important factor in achieving maximum efficiency of movement and in prevention of injuries. Faulty biomechanics may result from static (anatomical) abnormalities or dynamic (functional) abnormalities.\nStatic abnormalities: LLD, Genu valgum, pronated calcanium\nDynamic abnormalities: running with excessive anterior pelvic tilt.\nWhat happens when there is altered biomechanics?\nPoor techniques are the result of improper biomechanics. This poor technique results not only in injuries but also in reduced performance.\n6. Suitable equipments:\nEquipment may range from simple to complex.\nExample of simple equipment is the sports shoes.\nExamples of complex equipments are; racquets, sticks, bicycles, motor vehicles etc\nAccording to Khan & Brukner 3 major injury producers are- foot wares, racquets & bicycles.\nParts of a sports shoe: heel counter, toe box, mid sole.\nParts of racquet: handle grip, shaft & racquet head\nImportant parts of a bicycle from sporting angle: seat height, saddle position, handle bear position. Pedaling technique is one of the most important aspect where injuries can be prevented.\n7. Appropriate surface:\nDuring walking & running, the body is subjected to high-repetitive, short duration forces, increasing the susceptibility to injury. Maximal impact forces during walking, running, jumping has been shown to approach 2 times, 3-4 times, 5-12 times respectively.\nSurfaces alter the peak force that the body is subjected to during activity. Maximal impact forces are much higher on the hard than on the soft surfaces. Hence softer surfaces reduce the chances of sports injuries.\n8. Appropriate training:\nTraining errors are the most common predisposing factors in the development of sporting injuries.\nTraining is a constant balance between performing sufficient quality & quantity of work to maximize performance, but not so much that injury occurs.\nFull explanation of training is beyond the scope of this discussion.\nPrinciple of training are:\nDifferent training methods involves:\n1. aerobic training or endurance training\n2. anaerobic training or lactate training\n3. strength & power training\n4. flexibility training\n5. speed & agility training\n7. specific skill training\n8. cross training\n9. Adequate recovery:\nAdequate recovery is essential if the full training effect is to be gained & injuries are to be prevented.\n“Over-reaching”: inadequate recovery leads to impaired performance and associated symptoms such as tiredness & lethargy called “Over-reaching”. If from this point onwards if training is continued injury may occur. How ever, frequently athletes respond to above said symptoms by an increase in training as they perceive it as “lack of fitness”. This leads to what is called “over training syndrome”. Hence it is important for the coach to monitor the training program keenly.\nAdequate recovery includes:\n1. warm downs\n2. whirl pools & spa\n4. rest & sleep\n5. psychological & nutritional advices\n10. Psychology & injury prevention:\nThe detrimental effect of excessive psychological arousal is a well recognized entity. Excessive psychological arousal predisposes the athlete to injuries.\nExcessive arousal leads to altered muscle tension. This further leads to alteration of fine balance between agonist & antagonist which is the hall mark of a quality performance. Once this synergy is lost between agonist & antagonist; a changed technique rather than the natural technique is used. There is also “Loss of rhythm”. This factor predisposes to injury.\nExcessive arousal also leads to loss of mental concentration. Consequently the feet & body are placed do not get into proper position on the sports field. Hence the participant gets in to a biomechanically poor position to play return shots. This predisposes to injury.\nExcessive arousal leads to “narrow attentional focus”; hence he fails to read the play. This may result in them being easily tackled or bumped from the “blind side”.\n“The white line fever”: this is another example of excessive arousal. Here the athlete loses all perception of danger on taking the field. Consequently he places his body in positions vulnerable to injury.\nOver aroused players enter a competition without proper nutrition. This further lead the individual to sports injuries.\nUnder arousal: less common variety. It occur trial matches or lower level of competition.\nThe under aroused athlete shows following:\n1. Impaired reading of visual cues.\n2. Slow decision making.\n3. Do not take appropriate evasive action.\n4. Makes technical errors.\nThese above said points are responsible for sporting injuries in under-aroused athletes.\n11. Nutrition & injury prevention:\n1. Adequate nutrition may indirectly lead to injury through it’s effect on the recovery process.\n2. Due to continuous intense training; labile muscle proteins are channeled in gluco-neo-genesis to produce energy. Hence deficient dietary protein may lead to muscle soft tissue damage.\n3. Inadequate hydration has immediate & acute impact on athletic performance especially exercising under thermal challenge.\n4. Minerals such as calcium has very important role to play in muscle contraction physiology. Increased exercise draws upon the body stores of the calcium. Inadequate calcium intake weakens the bone and may lead to fractures. Electrolyte balance; further the internal milieu is maintained by Sodium & Potassium. Deficiency of these minerals leads to severe metabolic impairments. They may even cause death.\n5. Low calorie diet may lead to dropping of the fat proportion to such an extent that females loose monthly period. This further leads to osteoporosis and fractures."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:3796af98-16f3-4dbd-9103-90c035cac47a>","<urn:uuid:c5edff07-887e-479f-bbe7-d0db9607f838>"],"error":null}
{"question":"Could you explain how brain pathology helps diagnose different types of CJD, and what sustainable aspects vertical farming offers compared to traditional agriculture?","answer":"In brain pathology, sCJD shows 5-20μm vacuoles in gray matter and fine granular deposits of PrPSc, while vCJD is characterized by larger confluent vacuoles (50-100μm) containing Kuru-type amyloid plaques known as 'florid plaques.' Regarding sustainability, vertical farming offers significant environmental benefits compared to traditional agriculture: it uses 80% less water with recycling capabilities, requires minimal land, eliminates the need for pesticides, reduces transportation-related fossil fuel emissions and food spoilage, and is independent of weather conditions. The system combines natural and artificial lighting for optimal plant growth and employs soil-less growing techniques for increased efficiency.","context":["The determination of the form of prion disease and early diagnosis are important for prognostic, public health, and epidemiologic reasons.\nTo describe a patient with sporadic Creutzfeldt-Jakob disease (sCJD) who had a clinical history and initial electroencephalogram and magnetic resonance imaging findings consistent with variant CJD (vCJD).\nResults of a repeated electroencephalogram were suggestive of sCJD, and a subsequent brain biopsy confirmed this diagnosis.\nThis case cautions against relying solely on T2- and diffusion-weighted pulvinar hyperintensity and clinical features to differentiate between vCJD and sCJD, and further supports established diagnostic criteria for vCJD.\nPRION DISEASES are universally fatal neurodegenerative disorders that occur in sporadic, familial, and iatrogenic forms, the most common of which is sporadic Creutzfeldt-Jakob disease (sCJD). In 1996, Will and colleagues1 described a new form of CJD, variant CJD (vCJD), with clinical, epidemiologic, and pathologic features distinctive from other human prion diseases. Variant CJD is characterized by a younger median age of onset than that of sCJD (26 vs 65 years) and a longer median disease duration (14 vs 4.5 months). The onset of vCJD almost uniformly exhibits a prodrome of a neuropsychiatric illness followed later by the development of dementia, chorea, ataxia, and persistent painful sensory symptoms, in contrast with the more variable presentation of sCJD.2 It has been emphasized that vCJD cases can be differentiated from sCJD and other rapidly progressive dementias by the characteristic magnetic resonance imaging (MRI) finding of the pulvinar sign. This consists of an increased bilateral pulvinar signal, best visualized with fluid-attenuated inversion recovery (FLAIR) and diffusion-weighted techniques. An increased signal in the medial thalami is commonly found with the pulvinar sign, giving the appearance of a \"double hockey stick.\" Neuropathologically, brain tissue in patients with vCJD shows florid amyloid plaques that differentiate this disorder from other human prion diseases.3\nClinical recognition of vCJD is important because of its link to the consumption of infected beef and the possibility of a rising epidemic. For this reason, clinical criteria have been developed for probable vCJD. A probable diagnosis of vCJD requires fulfillment of the following criteria: a progressive neuropsychiatric disorder lasting at least 6 months, no history of iatrogenic exposure, an electroencephalogram (EEG) finding that is not indicative of sCJD, a bilateral pulvinar high signal on MRI, and at least 4 of the following 5 clinical features: early psychiatric symptoms, persistent painful sensory symptoms, ataxia, a movement disorder, and dementia.4 While some cases of sCJD with clinical and radiologic features typical of vCJD have been reported, they have only included patients closer in age to the mean of sCJD.5- 7 We report a case that initially met all of the epidemiologic, clinical, and imaging features of vCJD, and was pathologically proven by brain biopsy to have sCJD.\nA 36-year-old woman with an unremarkable medical history developed new-onset behavioral changes, including increased irritability and mild depression, in April 2000. One month later, she was seen by a psychiatrist, diagnosed as having depression, and treated with bupropion. In October 2000, she reported intense headaches. Between October and December 2000, she developed a progression of balance and gait problems as well as left leg weakness. In late December 2000, she developed short-term memory deficits and insomnia. Her social history was unremarkable except that she had traveled to the British Virgin Islands, Mexico, and Belize for 1 week every year for the prior 5 years.\nAs part of this patient's evaluation at an outside institution, she received 2 sleep and waking EEGs, 3 MRIs, and urine, serum, and cerebrospinal fluid analyses. She was admitted to our institution, where she was evaluated with a neuroexamination, MRI, and an EEG, and underwent a brain biopsy for a pathologic diagnosis. The biopsy was performed in the right temporoparietal cortex. The specimen was treated with hematoxylin-eosin and periodic acid–Schiff stains. Immunohistochemical analysis for protease-resistant prion protein (PrPSc) was performed using the hydrolytic autoclaving technique8 and antibody 3F4. The histoblot technique, which is the most sensitive and specific immunohistochemical test for PrPSc in tissue sections,9 uses unfixed cryostat sections blotted onto nitrocellulose paper.\nIn January 2001, sleep and waking EEG findings were normal, and an MRI of the brain showed a \"questionable pituitary microadenoma\" but was otherwise read as normal. Additional workup showed no microadenoma. In our later evaluation of this MRI, we noted abnormal hyperintensities in the basal ganglia on T2-weighted and FLAIR images (Figure 1). A second MRI of the brain in February 2001 showed hyperintensity in the caudate, putamen, and thalami bilaterally. Results of an EEG in March were abnormal due to the presence of triphasic waves and frontal intermittent rhythmic delta flowing. During spring 2001, she developed worsening confusion, slowed thinking, and visual hallucinations, and by July 2001, she was no longer ambulatory.\nBrain magnetic resonance imaging (MRI) studies of the patient, performed 7 months apart. A and B, MRI performed in January 2001 at another facility. C-F, MRI performed in August 2001 at the University of California–San Francisco. Increased signal in the caudate, putamen, thalami, and right temporoparietal cortical ribbon is seen on T2-weighted (A and C), fluid-attenuated inversion recovery acquisition (B and D), diffusion-weighted imaging (E), and diffusion attenuation (F) in this case of sporadic Creutzfeldt-Jakob disease.\nBrain MRI in June 2001 showed increased hyperintensity bilaterally in the caudate, putamen, and in the medial thalami; there was also gyriform hyperintensity on FLAIR and T2-weighted images involving the cortex through the right posterior temporal and parietal regions. Results of urine, serum, and cerebrospinal fluid analyses were all normal, and 14-3-3 protein testing was negative.\nIn August 2001, she was admitted to our institution for further evaluation. On admission, she had 1- or 2-word verbal output, was oriented to name only, recognized family members, and intermittently followed simple 1-step commands with minimal verbal expression. Motor system examination showed asymmetric (left much greater than right), increased tone in all extremities, bilateral plantar extensor responses, and occasional myoclonic jerks in the extremities. She was unable to ambulate.\nBrain MRI showed diffuse bilateral symmetric hyperintensities on T2, FLAIR, and diffusion-weighted imaging sequences involving the caudate, putamen, and posterior and medial thalami, as well as a gyriform pattern along the posterior (right greater than left) temporoparietal cortex. Areas of abnormal intense T2 prolongation were associated with reduced diffusion (Figure 1) on diffusion-weighted imaging and attenuated diffusion coefficient maps. These images were read as consistent with vCJD. A repeated EEG showed typical periodic epileptiform discharges consistent with sCJD. Because of the patient's young age, clinical history, and MRI findings, a brain biopsy was performed to determine the form of CJD.\nThe biopsy specimen from the patient's right temporoparietal cortex revealed numerous 5- to 20-µm vacuoles in the gray matter neuropil within neuronal layers 4, 5, and 6, detected by hematoxylin-eosin and periodic acid–Schiff stains (Figure 2A). No Kuru-type amyloid plaques or other amyloid deposits were identified. Immunohistochemical analysis for protease-resistant prion protein (PrPSc) revealed fine granular (synaptic) deposits as well as coarse deposits of PrPSc adjacent to some of the vacuoles (Figure 2B).10 PrPSc was distributed in a diffuse pattern throughout the full thickness of the cerebral cortex in the histoblots (data not shown). The histologic features described above are characteristic of sCJD. In contrast, vCJD is characterized by large confluent vacuoles (50-100 µm in diameter) that contain 1 or more Kuru-type amyloid plaques. This complex is known as a \"florid plaque\" (Figure 2C).11 Florid plaques are PrPSc immunopositive (Figure 2D). The absence of florid plaques in the current case argues that it was not vCJD. The scrapie prion protein type was determined by Western blot to be type 2, and sequencing of the prion protein gene showed that codon 129 was heterozygous methionine/valine. Therefore, this was an M/V2 variant of sCJD.12\nThe histologic features of sporadic Creutzfeldt-Jakob disease (sCJD) (A and B) are different from those of variant CJD (vCJD) (C and D). A, The periodic acid-Schiff (PAS)–stained section reveals multiple vacuoles, ranging in size from 5–20 µm, within the gray matter neuropil. Note the absence of PAS-positive amyloid plaques. B, Fine granular (synaptic) deposits and coarse deposits adjacent to some of the vacuoles are shown in this immunoperoxidase stain for protease-resistant prion protein (PrPSc) (Numarski optics). C, The presence of multiple PAS-positive, Kuru-type amyloid plaques within vacuoles is characteristic of vCJD. D, The Kuru-type plaques in vCJD are often found in clusters and are strongly immunopositive for PrPSc (Numarski optics). Bar = 25 µm.\nEarly in her course, our patient had the classic clinical and imaging findings of vCJD. Her early psychiatric symptoms of depression and behavioral changes were all consistent with a vCJD prodrome. Like many vCJD cases reported in the literature, this patient was first seen and treated by a psychiatrist for depression. Her young age at the onset of this disease was close to the median age of reported vCJD cases (age 26 years).4 Only 4.5% of cases of sCJD occur in the 30- to 40-year age bracket.2 Her relatively young age, long duration of illness, 6-month psychiatric/behavioral prodrome, progression to a movement disorder, and the \"double-hockey stick\" appearance on MRI1 were all highly suggestive of vCJD. Until her second EEG showed abnormalities typical of sCJD, this patient had met all of the diagnostic criteria for vCJD. Criteria based on the knowledge that the signature EEG findings in sCJD are absent in vCJD excluded her from a diagnosis of probable vCJD.1 Our case is similar to that of a 55-year-old patient with sCJD who was initially thought to have vCJD based on her psychiatric symptoms, double-hockey-stick appearance of the dorsomedial thalami, and the absence of EEG findings typical of sCJD.7 The young age of our patient made a diagnosis of vCJD even more suspect. The current age range for vCJD of 12 to 74 years overlaps with sCJD. In general, young age should not be used to rule out a diagnosis of sCJD. Although not well described in the literature, behavioral changes may occur in the earlier stages of sCJD as well.\nThe MRI findings in this patient, obtained at 4 different time points throughout the disease course, showed a bilateral signal intensity in the pulvinar and medial portions of the thalamus, giving a \"double hockey stick\" appearance. Specificity of the pulvinar sign was reported to be 100% when comparing 36 confirmed vCJD cases with 56 non-vCJD controls that were initially suspected as having vCJD.13 Our case suggests that the specificity is not 100%. The pulvinar sign allows a probable diagnosis of vCJD as long as other criteria are fulfilled.1 Furthermore, as several studies suggest, a higher relative hyperintensity in the pulvinar as compared with the caudate and putamen may confer greater specificity in the appropriate clinical context.7,13,14 In our patient, the change in EEG finding to one suggestive of sCJD would by established criteria have eliminated the diagnosis of vCJD. However, if the EEG had not been done or did not become suggestive of sCJD as occurs in a large minority of patients,15 the prebiospy diagnosis would have remained vCJD. Because of the profound implications of diagnosing a patient as having vCJD, we cannot overstate the importance of thoroughly investigating every case with studies that include brain MRI, EEG, and when necessary, pathologic analysis, to obtain a diagnosis of the specific form of the disease.\nCorresponding author and reprints: Jennifer Martindale, BS, Memory and Aging Center, Department of Neurology, University of California–San Francisco, 350 Parnassus Ave, Suite 800, San Francisco, CA 94143 (e-mail: email@example.com).\nAccepted for publication September 16, 2002.\nAuthor contributions: Study concept and design (Ms Martindale and Drs Geschwind and Miller); acquisition of data (Ms Martindale and Drs Geschwind, De Armond, Young, Dillon, Uyehara-Lock, and Gaskin); analysis and interpretation of data (Drs De Armond, Young, Dillon, Henry, Uyehara-Lock, and Miller); drafting of the manuscript (Ms Martindale and Drs Geschwind, De Armond, Uyehara-Lock, Gaskin, and Miller); critical revision of the manuscript for important intellectual content (Ms Martindale and Drs Geschwind, De Armond, Young, Dillon, Henry, Uyehara-Lock, and Miller); obtained funding (Dr De Armond); administrative, technical, and material support (Drs Geschwind, De Armond, Young, Dillon, Henry, Uyehara-Lock, Gaskin, and Miller); study supervision (Ms Martindale and Drs Geschwind, De Armond, and Miller).\nThe paraffin-embedded tissue blocks in Figure 2 were generously provided by James Ironside, BMSc, MBChB, FRCPath, FRCPEdin, Neuropathology Laboratory, CJD Surveillance Unit, University of Edinburgh, Edinburgh, Scotland.\nMartindale J, Geschwind MD, De Armond S, Young G, Dillon WP, Henry R, Uyehara-Lock JH, Gaskin DA, Miller BL. Sporadic Creutzfeldt-Jakob Disease Mimicking Variant Creutzfeldt-Jakob Disease. Arch Neurol. 2003;60(5):767-770. doi:10.1001/archneur.60.5.767","As urban populations continue to grow, entrepreneurs are going beyond traditional farming to find new ways to feed everyone while minimising the effect on our land and water resources. Vertical farming is one such method that has been used all around the world. Food crops may be conveniently farmed in urban settings using Vertical Farming by planting in vertically stacked layers to conserve space and require little energy and water for irrigation.\nVertical farming is the process of producing crops in layers that are vertically stacked. Controlled-environment agriculture, which tries to maximise plant development, and soil-less farming techniques such as hydroponics, aquaponics, and aeroponics, are frequently used.\nBuildings, shipping containers, tunnels, and abandoned mine shafts are among popular structures used to host vertical farming systems. There are approximately 30 hectares (74 acres) of functioning vertical farms around the globe as of 2020. Vertical farming, in conjunction with other cutting-edge technology such as customised LED lighting, has resulted in crop yields that are more than ten times greater than those obtained by standard agricultural methods.\nVertical farming is still in its early stages in India, but there are a few entrepreneurs and agri-tech enterprises aiming to revolutionise the area.\nVertical Farming Background and Concept\nGilbert Ellis Bailey originated the phrase “vertical farming” and published a book named “Vertical Farming” in 1915. William Frederick Gerick pioneered hydroponics at the University of California, Berkeley, in the early 1930s.\nke Olsson, a Swedish ecological farmer, devised a spiral-shaped rail system for growing plants in the 1980s and proposed vertical farming as a method of raising vegetables in cities.\nProfessor Dickson Despommier invented the concept of vertical farming in 1999. His idea was to grow food in urban areas, utilising less distance and saving time in transporting food produced in rural regions to cities.\nHe aimed to produce food in urban areas in order to have fresher goods available sooner and at a reduced cost. As a result, vertical farming is defined as the cultivation and production of crops/plants in vertically stacked layers and vertically inclined surfaces.\nThe plants are vertically piled in a tower-like form in the physical arrangement. This reduces the amount of space needed to cultivate plants. Following that, a combination of natural and artificial lighting is employed to ensure an ideal atmosphere for the plants’ effective growth. The third component is the plant’s growth medium. Aeroponic, hydroponic, or aquaponic growth media are employed instead of soil as the growing medium.\nAs the methodology gets more scientific, the process’s efficiency grows, and as a result, vertical farming becomes more sustainable, consuming 95 percent less water than previous agricultural methods.\nAlso Read, Oxagon: The World’s First Floating City in the World\nVertical Farming Techniques\nIt is a method of producing food in water without the use of soil by employing mineral fertiliser solutions.\nThe primary benefit of this strategy is that it lowers soil-related cultivation issues such as soil-borne insects, pests, and illnesses.\nAeroponics was inspired by NASA’s (National Aeronautical and Space Administration, USA) endeavour in the 1990s to develop an effective technique to grow plants in space. There is no growth medium in aeroponics, hence there are no containers for growing crops. Instead of water, mist or nutrient solutions are utilised in aeroponics. Because the plants are attached to a support and the roots are sprayed with nutritional solution, there is very little space, very little water, and no soil required.\nThe name aquaponics is derived from the combination of two words: aquaculture (fish farming) and hydroponics (the process of growing plants without soil in order to develop symbiotic interactions between the plants and the fish). The symbiosis is established by feeding nutrient-rich waste from fish tanks to hydroponic production beds called “fertigate.”\nIn turn, the hydroponic beds act as biofilters, removing gases, acids, and chemicals from the water, such as ammonia, nitrates, and phosphates. Furthermore, the gravel beds serve as a home for nitrifying bacteria, which aid in nutrient cycling and water filtering. As a result, the newly cleansed water may be recirculated back into the fish tanks.\nThe Benefits of Vertical Farming\nVertical farming offers various advantages, making it promising for agriculture’s future. The land need is fairly minimal, water usage is 80% less, water is recycled and stored, pesticides are not used, and in the case of high-tech farms, there is no true reliance on the weather.\nA vertical farm makes farming possible within the constraints of a metropolis. When the farms are close by, the food is delivered swiftly and is always fresh, as opposed to the chilled stuff commonly seen in stores. Transportation reduction minimises the cost of fossil fuels and the accompanying emissions, as well as transportation spoilage. Vertical farming, like anything else, has its limitations. The biggest issue is the initial capital expenses for building the vertical farming system.\nThere are further expenditures associated with building the structures as well as their automation, such as computerised and monitoring systems, remote control systems and software, automated racking and stacking systems, programmable LED lighting systems, temperature control systems, and so on."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:99439a10-5c6a-42f6-8918-dab119295d3c>","<urn:uuid:38abc93f-c15d-442d-abe3-4bcfbd141419>"],"error":null}
{"question":"Need to know quick - how does brain connectivity change during development according to recent research?","answer":"Recent studies using functional magnetic resonance imaging and diffusion tensor imaging have shown that during development, functional connectivity increases between distant brain regions (in the rostro-caudal axis) while decreasing between local regions of the frontal, parietal, and cingulate cortex. These developmental trajectories can be altered in diseased brains, and differences in connectivity may reflect individual differences in cognitive abilities.","context":["There has been an emerging focus in neuroscience research on circuit-level interaction between multiple brain regions and behavior. This broad circuit-level approach creates a unique opportunity for convergence and collaboration between studies of humans and animal models of cognition.\nMeasurement of broad-scale brain networks may be particularly important for understanding changes that occur in brain organization and function during development. Recent studies in humans have gained much leverage from trying to understand circuit-level interactions among brain regions over the course of development. Such studies use connectivity analyses of functional magnetic resonance imaging both during cognitive activity and during rest (fcMRI), and diffusion tensor imaging (DTI) to measure (respectively) the functional and structural connectivity between discrete brain regions (e.g. Rissman et al., 2004\n; Snook et al., 2005\n; Mori and Zhang, 2006\n; Fox and Raichle, 2007\n). Studies using these approaches have revealed that, over the course of development, functional connectivity increases between distant brain regions (in the rostro-caudal axis) while it decreases between local regions of the frontal, parietal, and cingulate cortex (Fair et al., 2008\n). Developmental trajectories may be altered in diseased brains (e.g., Church et al., 2009\n), and functional and structural differences in connectivity may reflect individual differences in cognitive abilities (e.g., Niogi and McCandliss, 2006\n; Seeley et al., 2007\n; also see reviews this issue).\nThese findings emphasize the importance of understanding the development of associative neural circuits. Going forward, a key challenge will be to gain an understanding of what these circuits do during development at multiple levels of analysis, from cellular mechanism to cognitive function. Currently, the cellular and synaptic basis of changes in functional connectivity and DTI imaging remain unclear. Are these changes due to myelination, novel growth, elaboration or pruning of new connections? What happens when development is altered or connections are silenced?\nUnderstanding the mechanistic basis of connectivity changes in humans, and how these changes relate to changes in behavior, is challenging and may benefit greatly from animal models. We propose that an emerging focus on broad-scale neural circuits provides a unique opportunity for collaborative studies that bridge research in mice and humans. New research methods and technology targeting neural circuits in both human and mouse neuroscience labs have great potential for enhancing overlap and collaboration between these two scientific cultures.\nFurthermore, many of the cognitive paradigms in humans draw from, or have parallels in, the animal conditioning literature, such as reward prediction, reversal learning, relational memory, rule extraction, and set shifting. This overlap in behavioral paradigms and cognitive domains suggests the promise of integrating a circuit-level understanding of cognitive development across species. To facilitate such collaborations, there is a need for researchers to communicate across technical and cultural boundaries. Communication and education in the research possibilities available to each sub-field will also facilitate the opportunity for researchers on both sides to make explicit predictions that can be tested in the most appropriate species, advancing research progress on common questions.\nWhat Mouse Models can Offer Developmental Cognitive Neuroscience\nMultimodal association areas are thought to support cognitive development and learning across mammalian species. Many of the same key cognitive regions of the brain (e.g. parietal and frontal cortex, basal ganglia, amygdala, and the hippocampus) can be found in both humans and mice and these broadly defined regions are connected in comparable circuits. For example, mice have cortical-basal ganglia loops and have parietal-frontal cortex and amygdala-frontal cortex connections. Elaboration and specialization of associative regions has likely occurred with evolution and growth in brain size, but the common genetic and anatomical architecture of the mammalian brain suggests similar rules may govern the development of basic associative brain circuits in mice and men alike.\nMice are particularly advantageous for study because of their relatively short development (puberty begins about 30 days of age, with adulthood at about 60 days), and their long history as a genetically tractable species, where increasingly specific identified populations of neurons can be genetically altered. Studies of the connectivity between association areas can be performed in mice with greater resolution to answer questions about the mechanisms regulating developmental circuit changes.\nThus, common circuit architecture in mice and humans offers the opportunity to perform controlled environmental, genetic, and behavioral experiments during development and adulthood. Such studies have enormous value, despite the obvious gaps in cognitive abilities between the species.\nTechnologies for Studying Circuits in Mice with High Resolution\nNew technologies have recently enhanced the study of neural circuits in mice, with important implications for understanding brain circuits underlying human cognitive development:\nImaging Plasticity and Activity with Cellular and Synaptic Resolution: 2PLSM\nTwo photon laser scanning microscopy (2PLSM) through a thin skull or cranial window allows time- lapse imaging of dendrites, spines and axonal and boutons in developing and adult mice (Holtmaat et al., 2009\n). Chronic preparations allow longitudinal study of developmental or experience-dependent process or the time scales of hours to several months. Imaging studies to date have revealed spine and bouton loss and gain, and reorganization of axonal arbors in the living brain. Similar imaging techniques can also be used to monitor cellular activity using calcium sensitive indicators (Stosiek et al., 2003\n; Dombeck et al., 2007\n). These techniques can become particularly powerful as identified cell types or cells with known afferents or efferents can be identified via fluorescent genetic labeling strategies.\nMapping Long-Range Connections between Specific Neuron Types: Cracm\nLight can also be used to isolate and measure the function of long-range connections between identified neurons. Channel Rhodopsin Assisted Circuit Mapping (CRACM) (Petreanu et al., 2007\n) uses ion channels “borrowed” from light sensitive bacteria to stimulate activity in channel expressing neurons (Boyden et al., 2005\n). Genetic and viral tricks allow light sensitive ion channels to be delivered to the membranes of specific cells or regions of interest, loading even long-range axons that traverse large portions of the brain (Petreanu et al., 2007\n). Expression of the channel in cells of interest enables isolated stimulation of cells or even severed axon terminals of interest (without stimulation of neighboring cells) by easily delivered remote flashes of blue light. Ex vivo\nslice patch clamp recording of cells in a specific location, or cells labeled with genetic tools, enables measurement of long-range afferent synapses made between specific cell types.\nMapping Local Circuit Connectivity between Specific Neuron Types: LSPS\nLocal circuit connectivity can also be measured using light. Laser scanning photo-stimulation (LSPS) uses light to uncage neurotransmitters in a defined region (Katz and Dalva, 1994\n). Light based uncaging produces focused concentration of transmitter, which can be used to cause action potentials at soma near the uncaging beam focus. With patch clamp recording of a single neuron and controlled scanning of an uncaging beam to distributed points across a brain slice it is possible to map local connections between a patched cell and its neighbors.\n2PLSM, CRACM, and LSPS techniques have first been applied to sensory cortices in mice and are now beginning to be applied to motor areas (Zuo et al., 2005\n; Yu et al., 2008\n; Xu et al., 2009\n). An important next step is to apply these technologies to multimodal association areas and the development of circuits that connect them. Studies of association area connectivity in mice should be of great interest to developmental cognitive neuroscience. For example, changes in the “default state network” that occur with development or disease could be modeled by looking at changes in parallel areas in mice.\nSome questions that can be answered with mouse experiments:\n• Which synapses are generally pruned with development? Which are gained?\n• How are the connections between brain regions (long range and short range) altered by the maturation of inhibition and the connectivity of local cortical circuits?\n• Which cell types within these regions demonstrate the most radical developmental changes? What modulates these changes? Which genes are unique to these cell types?\n• How do genetic differences and/or experience alter developmental circuit changes in a controlled environment?\n• How do cellular level changes in circuits correlate with changes in behavior?\nHow can Converging Studies of Circuit Development also Advance our Understanding of Cognitive Development?\nTo maximize the knowledge gained from comparative study of mice and humans it is important to take into account species differences in evolution and behavior and select the most auspicious cognitive comparisons. Many would agree that emotional and motivational behaviors supported by limbic, basal ganglia, and midbrain structures are readily comparable between species. Cortex-based cognition in mice and humans may differ both qualitatively and quantitatively, yet the sophistication of mouse cognition should not be underestimated. Mice are capable of rapid associative learning and reversal of learned associations (within a single training session) on tasks designed to approximate tests for humans with frontal lobe damage (Bissonette et al., 2008\n). The role of learning, memory and reward in decision making has also been found to be highly nuanced in studies of rodents and may be supported by distinct subcircuits that are parallel to those found in humans (E.g. Dusek and Eichenbaum, 1997\n; Shohamy and Wagner, 2008\n; see Eichenbaum and Cohen, 2001\n; Yin and Knowlton, 2006\n; Schoenbaum et al., 2009\nWhen making cross species comparisons it should be noted mice tend to learn cognitive tasks using olfactory cues more efficiently, but they can also discriminate tactile, aural, and visual cues. The complexity of human social systems and the lack of pheromone-related circuits in humans may make comparison of social cognition between species more difficult. Although behavior may not align perfectly in mice and men, with careful selection of cognitive measures with regard to ethological caveats we can enhance our understanding of the basic function of relays between associative regions.\nStudies of mouse association circuits can also provide a bridge between genetics and behavior. Studies that attempt to link human genetic variation to disease have not been as clear as once hoped (Goldstein, 2009\n), and this is especially true for psychiatric disease. A search for common biological pathways and cellular and circuit endophenotypes that link rare genetic variants may be more successful (Hirschhorn, 2009\n). The effect of human genetic variants on neural circuit development and plasticity may be readily tested in mice at both the circuit and behavioral level. Furthermore, imaging in awake mice and repeat longitudinal imaging allows for greater possibility and power in assessing correlations between developmental cognitive changes, cellular level circuit measures, and genetic differences.\nStudies of whole brains or multiple brain regions and correlations between them can be performed most efficiently in human studies. These studies, along with genetic data, can then be used to pinpoint circuits and cells for further study with higher resolution in mice.\nStudies of the brain are revealing that specialized knowledge is most powerful when shared across networks. Ideally, neuroscientists studying humans and mice can similarly work and move forward together.\nThe authors thank Itamar Kahn for discussion and comments.\nBissonette, G. B., Martins, G. J., Franz, T. M., Harper, E. S., Schoenbaum, G., and Powell, E. M. (2008). Double dissociation of the effects of medial and orbital prefrontal cortical lesions on attentional and affective shifts in mice. J. Neurosci.\nBoyden, E. S., Zhang, F., Bamberg, E., Nagel, G., and Deisseroth, K. (2005). Millisecond-timescale, genetically targeted optical control of neural activity. Nat. Neurosci.\nChurch, J. A., Fair, D. A., Dosenbach, N. U., Cohen, A. L., Miezin, F. M., Petersen, S. E., and Schlaggar, B. L. (2009). Control networks in paediatric Tourette syndrome show immature and anomalous patterns of functional connectivity. Brain\n132(Pt 1), 225–238.\nDombeck, D. A., Khabbaz, A. N., Collman, F., Adelman, T. L., and Tank, D. W. (2007). Imaging large-scale neural activity with cellular resolution in awake, mobile mice. Neuron\nDusek, J. A., and Eichenbaum, H. (1997). The hippocampus and memory for orderly stimulus relations. Proc. Natl. Acad. Sci. U.S.A.\nEichenbaum, H. E., and Cohen, N. J. (2001). From Conditioning to Conscious Recollection: Memory Systems of the Brain. New York, Oxford University Press.\nFair, D. A., Cohen, A. L., Dosenbach, N. U., Church, J. A., Miezin, F. M., Barch, D. M., Raichle, M. E., Petersen, S. E., and Schlaggar, B. L. (2008). The maturing architecture of the brain’s default network. Proc. Natl. Acad. Sci. U.S.A.\nFox, M. D., and Raichle, M. E. (2007). Spontaneous fluctuations in brain activity observed with functional magnetic resonance imaging. Nat. Rev. Neurosci.\nGoldstein, D. B. (2009). Common genetic variation and human traits. N. Engl. J. Med\n. 360, 1696–1698.\nHirschhorn, J. (2009). Genomewide association studies-illuminating biological pathways. N. Engl. J. Med\n. 360, 1699–1701.\nHoltmaat, A., Bonhoeffer, T., Chow, D. K., Chuckowree, J., De Paola, V., Hofer, S. B., Hübener, M., Keck, T., Knott, G., Lee, W. C. , Mostany, R., Mrsic-Flogel, T. D. , Nedivi, E., Portera-Cailliau, C., Svoboda, K., Trachtenberg, J. T., and Wilbrecht, L. (2009). Long-term, high-resolution imaging in the mouse neocortex through a chronic cranial window. Nat Protoc\n. 4, 1128–1144.\nKatz, L. C., and Dalva, M. B. (1994). Scanning laser photostimulation: a new approach for analyzing brain circuits. J. Neurosci. Methods\n. 54, 205–218.\nMori, S., and Zhang, J. (2006). Principles of diffusion tensor imaging and its applications to basic neuroscience research. Neuron\nNiogi, S. N., and McCandliss, B. D. (2006). Left lateralized white matter microstructure accounts for individual differences in reading ability and disability. Neuropsychologia\nPetreanu, L., Huber, D., Sobczyk, A., and Svoboda, K. (2007). Channelrhodopsin-2-assisted circuit mapping of long-range callosal projections. Nat. Neurosci\n. 10, 663–668.\nRissman, J., Gazzaley, A., and D’Esposito, M. (2004). Measuring functional connectivity during distinct stages of a cognitive task. Neuroimage\nSchoenbaum, G., Roesch, M. R., Stalnaker, T. A., and Takahashi, Y. K. (2009). A new perspective on the role of the orbitofrontal cortex in adaptive behaviour. Nat. Rev. Neurosci.\nSeeley, W. W., Menon, V., Schatzberg, A. F., Keller, J., Glover, G. H., Kenna, H., Reiss, A. L., and Greicius, M. D. (2007). Dissociable intrinsic connectivity networks for salience processing and executive control. J. Neurosci.\nShohamy, D. and Wagner, A. D. (2008). Integrating memories in the human brain: hippocampal-midbrain encoding of overlapping events. Neuron\nSnook, L., Paulson, L. A., Roy, D., Phillips, L., and Beaulieu, C. (2005). Diffusion tensor imaging of neurodevelopment in children and young adults. Neuroimage\n. 26, 1164–1173.\nStosiek, C., Garaschuk, O., Holthoff, K., and Konnerth, A. (2003). In vivo\ntwo-photon calcium imaging of neuronal networks. Proc. Natl. Acad. Sci. U.S.A\n. 100, 7319–7324.\nXu, T., Yu, X., Perlik, A. J., Tobin, W. F., Zweig, J. A., Tennant, K., Jones, T., and Zuo, Y. (2009). Rapid formation and selective stabilization of synapses for enduring motor memories. Nature\nYin, H. H., and Knowlton, B. J. (2006). The role of the basal ganglia in habit formation. Nat. Rev. Neurosci.\nYu, J., Anderson, C. T., Kiritani, T., Sheets, P. L., Wokosin, D. L., Wood, L., and Shepherd, G. M. (2008). Local-circuit phenotypes of layer 5 neurons in motor-frontal cortex of YFP-H mice. Front. Neural Circuits\n. 2, 6. doi: 10.3389/neuro.04.006.2008.\nZuo, Y., Lin, A., Chang, P., and Gan, W. B. (2005). Development of long-term dendritic spine stability in diverse regions of cerebral cortex. Neuron\n. 46, 181–189."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:893782df-171a-47a5-86a0-12e0e99a2c0e>"],"error":null}
{"question":"What's the difference between a chip shot's setup and a putter's setup position?","answer":"For putting, you bend at the hips with your head and eyes directly over the ball, with feet, hips and shoulders aligned parallel to the target line, and forearms in line with the putter shaft. For chipping, the setup requires the knees to be about eight inches apart with 60% of weight on the lead foot. While putting requires minimal body movement with locked lower body, chipping setup allows for weight shift and rotation. The ball position also differs - in putting it's standardized, while in chipping it can vary from far back in the stance to middle, with different degrees of shaft lean.","context":["Golf is a game of precision and consistency, and one of the most important skills to master is chipping. Whether you’re trying to get up and down from around the green or hitting a short approach shot, a good chip shot can save you strokes and help you lower your score. However, chipping can be a challenging aspect of the game, even for experienced golfers. Consistency is key when it comes to chipping, and mastering the right technique can take time and practice. In this guide, we will go over some tips and tricks to help you chip a golf ball consistently, no matter what your skill level is. We will cover everything from proper setup and technique to different types of chip shots and practice drills. So, whether you’re a beginner or a seasoned golfer looking to improve your chipping game, this guide is for you. Let’s get started!\nUnderstanding the Basics of Chipping\nBefore we dive into the tips and techniques, it’s essential to understand the basics of chipping. Chipping involves a short backswing, a descending blow on the ball, and minimal follow-through. Unlike a full swing, the power generated in a chip comes from a combination of clubhead speed and loft. The club’s loft creates backspin, which causes the ball to stop quickly after it lands.\nTips for Chipping\nHere are some tips to help you improve your chipping technique and consistency:\n1. Use the Right Club\nChoosing the right club is crucial to successful chipping. Generally, golfers use a wedge, either a pitching wedge, sand wedge, or lob wedge, for chipping. Each wedge has different loft angles, which affect the trajectory and spin of the ball. Experiment with different wedges to find the one that works best for you.\n2. Establish a Consistent Setup\nYour setup for chipping should be consistent to improve your accuracy and consistency. Here are some key points to keep in mind when setting up for a chip shot:\n- Place the ball in the center of your stance.\n- Position your weight slightly on your front foot.\n- Open your stance slightly.\n- Keep your hands forward and the shaft leaning towards the target.\n3. Focus on the Landing Spot\nWhen chipping, focus on where you want the ball to land, rather than the hole itself. Identify a spot on the green where you want the ball to land, and then focus on hitting that spot. This approach will help you to control the distance and trajectory of your shot.\n4. Use a Pendulum Motion\nA pendulum motion is an excellent technique for consistent chipping. To use a pendulum motion, keep your wrists firm and use your shoulders to create a smooth, even swing. Avoid using your hands to manipulate the club, as this can lead to inconsistency.\n5. Practice with Different Lies\nChipping from different lies, such as uphill, downhill, or from the rough, requires adjustments to your technique. Practice chipping from different lies to develop the skills needed to handle different course conditions.\nTechniques for Chipping\nHere are some techniques you can use to improve your chipping skills:\n1. The Pitch-and-Run\nThe pitch-and-run is a low, running shot that is useful when there is plenty of green to work with. To execute a pitch-and-run shot, use a wedge with a lower loft, such as a pitching wedge, and make a shorter backswing. Focus on hitting the ball cleanly, and let the loft of the club create the backspin needed to stop the ball.\n2. The Flop Shot\nThe flop shot is a high, soft shot that is useful when there is a hazard, such as a bunker or water, between you and the green. To execute a flop shot, use a wedge with a higher loft, such as a lob wedge, and make a bigger backswing. Focus on hitting the ball high and softly, so it lands softly and stops quickly.\n3. The Bump-and-Run\nThe bump-and-run is a low, running shot that is useful when there is a shorter distance to the hole. To execute a bump-and-run shot, use a wedge with a lower loft\n- Practice, Practice, Practice\n- Importance of practice for improving chipping consistency\n- Tips for practicing chipping, such as focusing on technique, using targets, and creating different scenarios\n- Importance of tracking progress and adjusting practice accordingly\n- Common Mistakes to Avoid\n- List of common mistakes that can lead to inconsistent chipping, such as improper weight distribution, incorrect club selection, and poor alignment\n- Detailed explanations of each mistake and how to avoid them\n- Recap of the importance of chipping in golf and the benefits of consistent chipping\n- Final tips for improving chipping consistency, such as maintaining a positive mindset and seeking professional instruction if needed","The best way to learn to play golf is to not swing a club.\nYou first need to learn how to grip the club, the correct posture and alignment and how to move and rotate your body and where your hands should be at impact with your ball. Once you get the fundamentals down, then you can start swinging your club\n. I strongly believe that the best way to learn to play golf is to start with putting from five feet from the hole, then chipping from just off the green, then pitching, and then half swings (“L” to “L”, or “9 o’clock” to “3” o’clock) working up to a full swing.\nWhy Start with Putting?\nWhile shifting your weight and body rotation is more important in chipping, pitching and your full swing, it is not the case in putting.\nYour lower body needs to be locked in and your head has to remain perfectly still. It is only the slight rocking of the shoulders that cause the arms and hands to move back and through.\nThe triangle that is formed by your shoulders, arms and hands at address should not change during your stroke. The angles formed with your hands on the putter do not break down; there is no hinging on the way back and no flipping at the ball.\n1. The Grip –\nI know there several ways to hold your putter (the “pencil” grip, the “claw”, “cross-handed” or lead hand low), but I am only going to talk about the traditional way to grip your putter.\nAs David Leadbetter illustrates in the excellent video below, you want your grip to be more in the palms of your hands along your life lines rather than in your fingers as your would for all other clubs.\nGrip the putter first with your lead hand and slide your trail hand underneath with both thumbs on top of the handle and the forefinger of your lead hand resting on top of the knuckles of your trail hand parallel to the shaft.\nWhile David talks about gripping your putter first with your lead hand, there are other golf pros that suggest gripping first with your trail hand as you line up putter head with your target line. Then placing your lead hand above your trail hand.\nI suggest you try it both ways and see which way you prefer. You want to maintain that same hand/grip position as you slightly rock your shoulders back and through with no hinging of your wrists or flipping at the ball.\nMost importantly, keep a very light grip on the putter like you are holding a baby bird in your hands. As a matter of fact, having a light grip is true for all of your clubs\n. 2. Posture/Alignment –\nThe correct posture/alignment would be your bending at your hips with your head/eyes directly over your ball with your feet, hips and shoulders all in alignment parallel to your target line.Try to have your forearms in line with the shaft of your putter from your elbows down to your putter head.\n3. Rocking Your Shoulders –\nDavid illustrates very clearly the rocking of the shoulders while your lower body remains locked in place. Once your have this down, you can try having your handle lead the follow through so you putter face is slightly trailing the handle. This is an advanced move so you will need to play with it.\n4. Straight Back and Straight Through or Arching Your Putter? –\nThis will depend on the type of putter your have. If you do not know, balance your putter across your forefinger and if your putter head is pointing down to the ground, you have a face balanced putter which allows you to have a straight back and straight through stroke\n. If your putter head is lying at an angle across your forefinger, you have a toe weighted putter which will require more of an arching stroke. Do not try to putt straight back and straight through with this type of putter as you will have to manipulate the putter head which just does not work very well. Believe me, I have tried it.\nWhat is the Difference in Chipping and Pitching?\nClay Ballard of Top Speed Golf has an excellent video on the difference between chipping and pitching.\nAs Clay emphasizes, one of the main differneces is the amount of spin that you exert on the ball.\n1. Chipping –\nWith a chip shot, you are usually just off the green and have a little distance between your ball and the hole and are using a less lofted club so as to put less spin on the ball.\nThe goal is to get the ball on the green as soon as possible with little to no spin and let it run out to the cup.\nBall position is another variable, as some like to play the ball really far back in their stance with a lot of forward shaft lean, while others prefer to play the ball more in the middle of their stance with only a little shaft lean.\nWith your knnes about eight inches apart and 60% of your weight on your lead foot, rotate your knees and core through the chip. I have a good section on chipping in a previous article, “How to Chip Your Golf Ball Like the Pros” so you might check that out.\n2. Pitching –\nWith a pitch shot, you will usually be landing the ball closer to the hole with more spin. As Clay states, you will have a more lofted club and will be accelerating your club head through impact with your hands in front of the ball with forward shaft lean.\nYou will also have more weight shift and body rotation while pitching. To take this one step farther, watch Clay’s excellent video below on how to make that 60 yard pitch shot.\nOnce you have mastered this shot, it is an easy transition to the next step in learning how to play golf.\n“L” to “L” or “9” O’clock to “3” O’clock\nIt really is the same thing, but do this excellent drill before you try to immediately jump to a full swing.\n1. What the Heck is “L” to “L”? –\nThe “L” to “L” is nothing more than forming an “L” on your back swing or takeaway with your straight lead arm being the bottom of the “L” when parallel to the ground and the shaft of your club is the upright part of the “L”.\nThen you pivot by shifting your weight from your trail heel to your lead foot and the second “L” is formed after impact by your straight trail arm parallel to the ground and the upright shaft of your club. When you finish your swing, you should be standing tall, on the toe of your trail foot, facing the target with your chest and hips with your hands extended right in front of you.\nPiers and Andy of Meandmygolf have a nice video shot at Aviara Golf Club in Carlsbad, CA, (one of my favorite courses) demonstrationg how to do this drill.\nThey demonstrate the drill with their feet together. That’s fine. Do it with your feet together then with your feet in a normal stance. You can practice both ways\n. 2. “9” O’clock to “3” O’clock –\nTo visualize this drill another way, stand upright and imagine you are standing in front of a big clock with the face of the clock to your back.\n12 o’clock would be at the top of your head, six o’clock would be at your feet, for a right-handed golfer, nine o’clock would be about hip high to your right, and three o’clock would be about hip high to your left.\nMike Sullivan, of Mike Sullivan Golf School has a cute entertaining video illustrating how the ‘L” to “L” is the same as “9” to “3”.\nBe sure to watch to the end. Cookie is awesome. I could watch this video all day.\nThe mechanics are the same as for the “L” to “L”. These drills teach you how to shift your weight and rotate your core.\nOnce you have mastered each of these steps, you are ready to progress to a fuller swing. If you want to develop a repeatable swing, you have to take these small steps to build your swing.\nIf you want to learn to play golf, build the foundation with the proper grip, posture, and alignment first.\nLearn how to rotate your body and where the hands should be at impact. Then, you will be well on your way to building a repeatable swing.\nHope this helps a few of you. Let me know your success stories. I welcome any comments or suggestions you may have."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:ced90e43-efc3-40cb-a0cb-14dfd2f989a6>","<urn:uuid:ddceafca-b131-40b0-9dab-4e6bbccf0e13>"],"error":null}
{"question":"Hello! For my research about ships - did Queen and Duke of York both serve in military operations??","answer":"Yes, both ships served in military operations but in different wars and capacities. The Queen served during the war with France in the 1790s under a letter of marque, which was issued to Captain Craig in December 1793. The Duke of York served in World War II after being requisitioned in 1942, renamed HMS Duke of Wellington, and converted into a Landing Ship Infantry (LSI). She participated in significant operations including Operation Jubilee (the Dieppe Raid) in 1942 and the Normandy landings in 1944.","context":["Queen (1785 ship)\nQueen was launched in 1785 and served the British East India Company as an East Indiaman. She had made four voyages to India and China for the Company and was on the initial leg of her fifth voyage when a fire on 9 July 1800 destroyed her at St. Salvador.\n|East India Company|\n|Fate||Burned and exploded 9 July 1800|\n|General characteristics |\n|Tons burthen||801, or 80136⁄94 (bm)|\n|Beam||36 ft 0 in (10.97 m)|\n|Depth of hold||14 ft 9 in (4.50 m)|\n|Armament||26 x 9 & 12-pounder guns|\nVoyage 1 (1786-1788)Edit\nUnder Captain Peter Douglas, Queen sailed from The Downs on 20 Feb 1786, bound for Bombay, Bengal and China. She reached Madeira on 16 March, cleared the Cape, and on 1 July arrived at. Twenty-two days later Queen was at Bombay. From there she started around India, arriving at Tellicherry on 28 October and Kedgeree on 4 December. She then retraced her voyage, passing Saugor on 13 March 1787, stopping at Tellicherry on 25 April, and then on 8 May arriving at Bombay. From there she sailed for China, reaching Malacca on 25 August, and Whampoa anchorage on 2 October. For her journey home, she crossed the Second Bar on 31 January 1788. She reached St Helena on 12 June and anchored in The Downs on 25 August.\nVoyage 2 (1790-1792)Edit\nFor her second voyage, Douglas was still Queen's captain. She left The Downs on 5 April 1790, bound for Madras, Bengal, and Bombay, and 15 days later arrived at Madeira. She reached Madras on 5 September, Masulipatam on 25 September, and Culpee (Calcutta) on 1 November, where her cargo of 'Small's beer and porter' and 'Bells beer and pale ale' was advertised for sale in the Calcutta Gazette. She left Diamond Harbour (also Calcutta), on 26 December, stopping at Ingeli, a point on the Hooghli River, on 15 Jan 1791. From there she sailed to 2 Feb Madras, which she reached on 2 February, and then Cannanore on 23 March and Tellicherry two days later. She arrived at Anjengo on 11 April, stopped at Cochin on 24 April, and two days later was back at Tellicherry. On 13 May she arrived at Bombay. She then sailed all the way around India to Diamond Harbour, which she arrived at 25 September. For the homeward bound trip she was at Saugor on 15 November and reached the Cape on 15 Feb 1792. Queen arrived at St Helena on 6 March and anchored in The Downs on 17 May.\nVoyage 3 (1793-1795)Edit\nFor her third voyage, Queen was under the command of Captain Milliken Craig. As war with France had broken out, she sailed under a letter of marque that had been issued to Craig for her on 27 December 1793.\nThe British government held Queen at Portsmouth, together with a number of other Indiamen in anticipation of using them as transports for an attack on Île de France (Mauritius). It gave up the plan and released the vessels in May 1794. It paid £1,479 3s 4d for having delayed her departure by 71 days.\nQueen left Portsmouth on 2 May, bound for Madras and Bengal, and reached Madras on 11 September. She sailed up India's east coast to Diamond Harbour, which she reached on 15 October. Her return voyage took her past Saugor on 30 Jan 1795 and to Madras again on 29 March. She arrived at St Helena on 17 August and the Downs of 25 November.\nVoyage 4 (1796-1798)Edit\nAgain under Craig's command, Queen left Portsmouth on 11 August 1796, bound for St Helena and Bencoolen. She then stopped at St Helena on 16 October and the Cape on 24 November. On 17 February 1797 she arrived at Madras. She then sailed directly to Bencoolen, which she reached on 8 May. From there she sailed to Diamond Harbour, reaching it on 10 October. She passed Saugor on 16 December on her way back to Bencoolen, which she= reached on 21 February 1798. She did not arrive back in Britain until almost a year later, on 9 February 1799.\nVoyage 5 (1800 — loss)Edit\nCraig and Queen left Torbay on 3 May 1800 on her last, ill-fated voyage, to Madras and China.\nQueen put into Salvador, Bahia, to replenish her water. She was in company with the East Indiaman Kent, with which she had left Torbay in convoy. During the night of 8–9 July a fire broke out on Queen while most of her officers and passengers were ashore. Lookouts on Kent appear to have noticed the smoke before the crew of Queen did, and Kent sent boats and fire-fighting equipment, but the currents were too strong for them to be of much use.\nThe fire was discovered burning in the gun-room at 3 am, though no one had visited it after 8pm. The cause of the fire was attributed to a Portuguese gunboat which had come alongside Queen, ostensibly as an anti-smuggling measure. The Portuguese gunboat had a fire lit aboard, part of which the crew threw into the Queen's gun-room scuttle, starting the fire.\nThe fire raged out of control, but fortunately winds and currents pushed Queen out of the bay and so away from Kent. Queen blew up at 7 am.\nCasualties on board Queen were heavy. An officer on Kent wrote a letter from Salvador a little more than a week later and reported that many aboard her had drowned when they leaped into the water. He estimated that she had lost six passengers, some 30 troops (of an unspecified number that she was carrying to India) and who could not get to the hatchways in time, and 70 of her crew. Because the fire broke out during the night and boats could not be launched, all the survivors, including five ladies, lost everything but whatever clothes they had on.\nThe EIC put the value of the cargo that it had lost on Queen at £30,421.\nKent remained at Salvador for more than a week and then loaded the surviving troops and passengers to take them on to Calcutta. (It is not clear whether any of Queen's officers and crew also went.) Kent reportedly took on some 300 people in all. She therefore had some 440 persons on board.\nOn 7 October, off the Sand Heads (near the mouth of the Ganges River, Kent encountered the French privateer brig Confiance, of 18 guns and 150 men, under the command of Robert Surcouf. Surcouf managed to board his larger opponent and seize control of the Kent. The British had 14 men killed, including her captain, and 44 wounded, while the French suffered five men killed and ten wounded. Surcouf released the passengers on a merchantman that he stopped a few days later.\nCitations and referencesEdit\n- Hackman (2001), p. 180-1.\n- British Library: Queen .\n- Letter of Marque, 1793-1815; p.83\n- Brown, Pete (19 August 2011). Hops and Glory: One man's search for the beer that built the British Empire. ISBN 9780230740471. Retrieved 24 July 2014.\n- Hardy & Hardy (1811), p.202.\n- Naval Chronicle, Vol. 4, pp.344-5.\n- Grocott (1997), pp. 94–5.\n- Reports... (1830), Vol. 2, p.976.\n- Grocott, Terence (1997). Shipwrecks of the Revolutionary & Napoleonic Eras. London: Chatham. ISBN 1861760302.\n- Hackman, Rowan (2001). Ships of the East India Company. Gravesend, Kent: World Ship Society. ISBN 0-905617-96-7.\n- Hardy, Charles and Horatio Charles Hardy (1811) A register of ships, employed in the service of the Honorable the United East India Company, from the year 1760 to 1810: with an appendix, containing a variety of particulars, and useful information interesting to those concerned with East India commerce. (London: Black, Parry, and Kingsbury).\n- ‘Reports from the Select Committee of the House of Commons appointed to enquire into the present state of the affairs of the East India Company, together with the minutes of evidence, an appendix of documents, and a general index, (1830), Vol. 2.","Duke of York\nDuke of York was launched in 1935 as a ferry but had a long and interesting service history. During WW2 she served as a Landing Ship, Infantry (LSI) and troopship, returned to ferry duties after repairs and reconfiguration with one funnel, survived having her bows sliced off in an accident and (after repair!) spent many years on passenger cruises. She was finally broken up in 1975 giving her a service life of 40 years.\n|Registered owners, managers and operators||London Midland & Scottish Railway|\n|Builders||Harland & Wolff|\n|Engines||4 steam turbines SR geared to 2 SC shafts|\n|Engine builders||Harland & Wolff|\nThe Lloyds Register entry for Duke of York for 1945-6 has the following additional information about her:\n- She had two decks\n- Cruiser stern\n- Fitted with echo sounding and radio direction finding equipment\nSince the initial publication of this web page I was contacted by Keith Nisbet who had a set of berthing plans for Duke of York and kindly sent them on to me. They are reproduced below but raise some interesting questions. The 1945-6 Lloyds Register entry for Duke of York says that the ship had two decks and a cruiser stern. The berthing plans cover three decks, and the images on this page also appear to show three rows of portholes indicating three decks even pre-war when she had two funnels. I have no explanation for this and would be interested in hearing from anyone that may have further information to explain this.\nThe deck plans show three decks in the main body of the ships, and also three decks in the fo'c'sle area.\n|7 March 1935||Launched|\n|4 June 1935||Completed|\n|1942||Requisitioned for war service; name changed to HMS Duke of Wellington to avoid duplicate names and converted for use as a Landing Ship, Infantry (LSI)|\n|1945||Name changed back to Duke of York on return to original owners.|\n|1950||Refit with change to oil engine and modified to have just one funnel|\n|6 May 1953||Collision with Haiti Victory sliced off bow. Repaired and fitted with a raked bow by Palmers Shipbuilding and Iron Company of Jarrow.|\n|1963||Purchased by A.J.& D.J.Chandris and name changed to York|\n|1963||Name changed to Fantasia|\n|December 1975||Broken up at Pireaus|\nDuke of York was built for the London Midland and Scottish railways and used for the Heysham to Belfast ferry route from the time she was brought into service in 1935.\nIn 1942 Duke of York was requisitioned for war service and her name was changed to HMS Duke of Wellington as there was already an HMS Duke of York in service. She was converted for use as a Troopship/Landing Ship, Infantry (LSI) vessel.\nHMS Duke of Wellington was one of the LSIs that took part in Operation Jubilee - an abortive raid on Dieppe on 19th August 1942. She was assigned to the landing at Blue beach which was at Puits to the east of Dieppe according to External Ref. #29. She had on board soldiers from The Black Watch of Canada who had embarked at Southampton.\nHMS Duke of Wellington claimed to have shot down a Ju88 aircraft with a Lewis gun and the port wing was described as having been shot off. The gunner, AB N. Mitchinson, was Mentioned in Dispatches as a result.\nThe assault started at 05:00 but a decision was made to retreat by 09:00. The failed operation involved 6,086 allied forces with Canadians suffering the greatest losses:\n- Canadian losses were 907 dead, 2,462 captured\n- UK losses were 189 dead, 269 Missing in Action,39 wounded, and 17 captured\n- US losses were 3 dead\n- German casualties were relatively light with 311 dead and 280 wounded\nA report on the operation by the naval force commander J. Hughes-Hallett - External Ref. #29 - was produced shortly after the raid but not published until 1947 - it can be read online at the link I have provided. There are a large number of books and websites covering this operation so I am not discussing it further here.\nThe Normandy Landings\nHMS Duke of Wellington is known to have taken part in the Normandy landings of 1944 and transported a large number of troops. One account described the decks of the ship as \"heaving with hundreds of assault infantryman, made up of Canadian troops and soldiers from the Royal Wiltshire Regiment”. She was one of the ships transferring assault forces to Juno Beach and part of Force J under the command of Commodore G.N.Oliver who was on HQ Ship HMS Hilary.\nApart from the information already stated, I have not so far discovered futher information regarding HMS Duke of Wellington's service following Operation Jubilee until towards the end of the war when she was serving as a troopship taking passengers between Tilbury and Ostend. An account of this latter period is provided by Stan Mayes who served on her at this time can be found in the Recollections section of the Benjidog website HERE.\nFollowing the end of war, she was repaired and after resuming her original name, Duke of York was returned to her owners and restarted service on the Harwich - Hook of Holland route. In 1950 she was refitted with a single funnel and an oil engine.\nCollision with Haiti Victory\nOn 6 May 1953 Duke of York was in collision with the American ship Haiti Victory. Duke of York was struck on her port side forward of her bridge and cut in two; the bow section sank. Contemporary accounts from The Times newspaper follow:\nLooking at the extent of the damage it seems remarkable that the main section of the Duke of York did not sink as well as the bow section. During repairs she was fitted with a raked stem.\nService as a Cruise Ship\nIn 1963, Duke of York was sold to Chandris Lines. First renamed to York, she and entered service in 1964 as the Fantasia. She ran mainly on cruises in the Eastern Mediterranean, with some winter charters to religious tour groups.\nFantasia was withdrawn in the mid-seventies and scrapped in Spain in 1975.\n- By courtesy of Stan Mayes\n- By courtesy of The Times\n- By courtesy of Keith Nisbet\n- By courtesy of Wikipedia Commons"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:a1e1b2a2-5494-4dd2-9dc4-2b853ca14ac6>","<urn:uuid:f6b03ed3-cab5-4d87-9c0a-22ab53d826a3>"],"error":null}
{"question":"I need to understand healthcare for my research paper. What are the key differences between monitoring systems in healthcare and banking, focusing on how they track and evaluate performance?","answer":"Healthcare and banking use distinct monitoring approaches. In healthcare, Health System Performance Assessment (HSPA) is a regular, systematic, and comprehensive process that evaluates multiple dimensions including access, quality, safety, equity, and population health through specific indicators like physician ratios, waiting times, and survival rates. In contrast, banking monitoring focuses primarily on financial health and covenant compliance, using technology tools to track borrower performance through financial metrics, automated alerts, and covenant testing. While healthcare monitoring emphasizes holistic evaluation across various stakeholders and social determinants, banking monitoring concentrates on protecting investments and detecting early warning signals of financial deterioration. Healthcare uses both quantitative and qualitative insights compared over time and across populations, while banking increasingly relies on automated data collection and analysis tools like APIs and machine learning to track financial performance and covenant compliance.","context":["Health system performance assessment (HSPA) is the process of monitoring, evaluating and communicating to what extent various aspects of a health system meet key objectives. The central purpose of HSPA is to assess whether progress is being made towards desired goals and whether appropriate activities are undertaken to promote achievement of those goals.\nBefore elaborating on the concept of health system performance assessment (HSPA) further, it is helpful to first shed some light on the different concepts and terminology related to this topic. The following table illustrates the differences between a ‘healthcare system’ and a ‘health system.’\nTable 1. The difference between a healthcare system and a health system, and between healthcare system performance and health system performance\nHealthcare system Combined functioning of public health and personal healthcare services that are under the direct control of identifiable agents, especially ministries of health.\nHealthcare system performance The efficiency and equitability of the professional public health and personal healthcare services within a system, including a cost-benefit analysis.\nHealth system All activities and structures that determine or influence health in its broadest sense within a given society. This also includes social, environmental and economic determinants of health.\nHealth system performance A broader concept that also acknowledges the broad range of determinants of population health that are not directly related to healthcare service delivery. It embraces the notion that the health status of a population is only partly influenced by the quality of the available healthcare services, and that there are many other social, cultural, political, economic, environmental, educational and demographic factors influencing population health.\nThere are three distinct strengths to a health system performance assessment compared to a healthcare system performance assessment, namely:\n- A health system performance assessment embodies a holistic approach. The whole system is incorporated, and the assessment surpasses the narrow scope of just the healthcare system.\n- Involvement of all stakeholders is possible. It gives the various agents a sense of ownership and responsibility, and helps them to make informed decisions.\n- There is a strong emphasis on the overarching objectives of health systems.\nFive key characteristics of HSPA\nThere are 5 key characteristics for adequately applying the concept of health system performance assessment (HSPA). HSPA should be:\n- Regular: Assessing the performance of a health system is a continuous and iterative process.\n- Systematic: The approach should be structured and consistent.\n- Transparent: The assessment has to be clear, unambiguous and understandable for others.\n- Comprehensive: The whole system should be covered. Furthermore, we have to be aware that the performance of a system does not simply equal the sum of the performance of its various components.\n- Analytical: Complementary sources of information should be consulted to obtain a comprehensive and well-founded overview of the health system’s performance. Quantitative indicators should be supported by qualitative insights, just like performance indicators should be supported by a policy analysis. HSPA is in essence a comparative evaluation, and the reference points for comparison have to be chosen wisely. Some relevant reference points for comparison could be:\n- Developments over time\n- Local, regional, national or international differences\n- Differences between population groups (e.g. based on age, gender, income, SES etc.)\n- Comparisons to certain targets or benchmarks\nThe WHO Health Systems Framework\nThe WHO states that a health system consists of all organizations, people and actions whose primary objective is to promote, restore or maintain health. The WHO has developed a model which is comprised of six building blocks. Collectively, these six building blocks represent the ‘complete’ health system. The building blocks are illustrated in the following figure.\nFigure 1. The six health system building blocks of the WHO\n- Service delivery: This represents the effectiveness, safety and the quality of health interventions. It means that health interventions are available to whoever needs them, regardless of where and when they are needed, with minimum waste of resources.\n- Health workforce: A professional health workforce is responsive, fair and efficient in achieving the best possible health outcomes, making optimal use of the available resources and given circumstances. There should be an adequate number and diversity of competent, productive and responsive medical professionals, distributed fairly amongst society.\n- Health information system: A well-functioning health information system ensures the production, analysis, dissemination and use of reliable and up-to-date information on health determinants, health system performance and health status.\n- Access to Essential Medicines: Essential medical products, vaccines and technologies should be equitably accessible to the population. These medical provisions should be of guaranteed quality, safety, efficacy and cost-effectiveness.\n- Financing: Adequate funding for health should be ensured, to make sure citizens can obtain needed services. Citizens should be protected from disproportionate financial losses when obtaining health services.\n- Stewardship/Governance: Also called leadership. It involves establishing strategic policy frameworks in which effective oversight, coalition-building, adequate regulations and incentives, and accountability issues are all properly implemented and addressed.\nWhich dimensions and indicators to choose for HSPA?\nAn indicator is appropriate for inclusion in a HSPA if 5 critera are met, as illustrated in table 2 below.\nTable 2. The 5 criteria that make an indicator suitable for inclusion in a HSPA (source: Veilard et al. 2010)\nImportance The indicator reflects critical aspects of health system functioning\nRelevance The indicator provides information that is useful for monitoring and measuring health system performance for an extended time period\nFeasibility The required data are readily available or can be obtained with reasonable efforts\nReliability The indicator produces consistent results\nValidity The indicator is an accurate reflection of the dimension it is supposed to represent\nTo conclude, table 3 provides an overview of potentially relevant dimensions, including several exemplary indicators per dimension.\nTable 3. Exemplary dimensions and indicators for HSPA\nDimension Exemplary indicators\nAccess - Physicians per 1000 inhabitants\n- Waiting time for an appointment with a GP or medical specialist\n- Waiting time for an donor organ\n- Geographic coverage of GP practices (percentage of people that are within a 20 minute drive from a GP)\nQuality - Immunization rates\n- Five year survival rates for breast, cervix and colon cancer\n- Percentage of patients treated in accordance with evidence-based guidelines\nSafety - Rate of MRSA infections\n- Percentage of patients in long-term care facilities with decubitus\n- Percentage of patients experiencing side-effects of medication\nEquity - Life expectancy at birth\n- Avoidable mortality\n- Health service utilisation\n- Differences per gender, age group, income, living area, SES\nFairness - Government spending on health as a percentage of total government spending\n- Total household out-of-pocket payments\n- Health insurance affordability and coverage\nContinuity - Support after leaving the hospital\n- Percentage of chronically ill patients experiencing coordination problems with medical tests\n- Patients enrolled in disease management programs\n- Patients receiving contradictory information from different healthcare providers\nEfficiency - Average length of hospital stay\n- Percentage of surgeries in day-clinics\nResponsiveness - General satisfaction with the healthcare system\n- Patient-perceived interpersonal contact\n- Patient involvement in decision-making processes\n- Patient-doctor interaction (explanations, possibility for asking questions, check-up telephone calls)\nSustainability - Healthcare expenditure as percentage of GDP\nPopulation health - Healthy life years\n- Infant mortality\n- Obesity rates\n- Ischaemic heart disease rates\n- Self-perceived health\n- Veillard J, Huynh T, Ardal S, Kadandale S, Klazinga NS, Brown AD. (2010). Making health system performance measurement useful to policy makers: aligning strategies, measurement and local health system accountability in Ontario. Health Policy, 5 (3), 49-65.\n- World Health Organization. (2010). Monitoring the building blocks of health systems: a handbook of indicators and their measurement strategies. Geneva, WHO Press.\n- Special acknowledgement: Dr. Kai Michelsen, Maastricht University","In this article, we explore what monitoring lenders routinely undertake, why it is so difficult and what new technology tools are at their disposal to improve the process, and show how better monitoring can lead to better risk management and lower portfolio losses.\nAs all commercial bankers know, getting a loan on the books is just one part of the equation. After the loan is approved, the bank has to retain the borrower until the loan becomes due, which might not be for several years. During this time, the borrower is likely to transition through different credit profiles, for example due to financial management decisions, industry trends, or the economic environment. Despite sound initial due diligence, things can go wrong with a loan before it gets repaid.\nWhen monitoring commercial borrowers’ financial health and their ability to meet obligations under loan agreements, banks have tended to be slow adopters of technology that could maximize efficiency and improve their risk management capabilities. Banks’ focus has been to develop customer relationships, build the opportunity pipeline, get the loan on the books as quickly as possible and move on to the next deal. After the loan is written, conducting an annual risk review based on outdated information is still too common among lenders.\nBut, can we blame the bankers? Under tremendous pressure to grow loans and revenues more efficiently in a highly competitive market, much of their effort and technology spend has focused on getting the loan approved and on board. Borrower assessment and loan monitoring technology can sometimes be a lower priority.\nWhat does monitoring entail?\nWhen a bank underwrites a new loan, it conducts a full credit assessment on the borrower, including the borrower’s ability to pay back or refinance the loan at the time of maturity. The bank expects the borrower’s credit profile to remain the same as, or better than, at the time it extends the loan. It puts in place covenants and other requirements to ensure that a minimum set of standards are met for a borrower’s future conduct and financial performance.\nMost covenants establish benchmark metrics that are intended to ensure that the borrower remains financially healthy, and the bank’s investment is protected. These restrictions are based on the borrower’s specific balance sheet, income statement, and cash flow characteristics, most commonly expressed in the form of financial ratios. Other covenants monitor reporting and disclosure, to set a minimum standard of communication with the bank. For instance, the regular delivery of financial statements, or borrowing base certificates.\nIn more complex loans, the lender or group of lenders can impose certain restrictions on the borrower that govern what it can and cannot do with its business operations. For example, the lender might restrict key management changes, acquisitions, or asset disposals.\nAs part of agreeing to receive the loan, borrowers usually provide documentation demonstrating adherence to all the various requirements of their loan agreement, both at the outset and at frequent intervals during the loan term. Borrowers also make themselves available to discuss their business and financial performance with the bank’s officers throughout the loan period.\nWhy is monitoring important?\nRegular monitoring is undertaken to ensure the bank’s investment is protected. A good monitoring program will quickly identify any red flags that would suggest the borrower’s financial health is starting to deteriorate. Being able to detect these early warning signals is critical, as it allows the bank to remedy the increased risk to its investment. At a minimum, the lender might want to reprice the loan to charge for the additional risk. In more severe circumstances, the bank might want to recall the loan by, for instance, defaulting the borrower and demanding immediate repayment. Either way, if not captured early enough, the bank’s options for remedying the situation become more limited.\nBanks also face regulatory pressure to have strong risk management processes in place, to ensure that underwriting standards remain strong, and to put an effective monitoring regime in place. Today, regulators are requesting more data, more often, and faster. Timely monitoring ensures that the bank is not simply meeting regulatory oversight, but also adequately quantifying its risk, accurately calculating its capital, and setting aside proper reserves. All these things are critical in the eyes of regulators.\nPerhaps the most obvious reason to monitor a portfolio is that banks want to avoid loan losses. Effective borrower monitoring is therefore necessary to detect which loans are likely to become stressed, and which loans might default and lead to financial loss. All banks make losses on their loan portfolios to some extent, which is only natural when an element of risk is involved. However, the loan loss rate reflects on the lending institution itself, and determines how much equity capital shareholders need to contribute. Too many loan losses and shareholders will likely react.\nBanks have different ways of collecting, reviewing, and using the various information provided by their borrowers under loan agreements. Unfortunately, in today’s environment, bankers are being asked to do more with less and risk monitoring processes tend to be resource-intensive. Below, we articulate some key monitoring challenges that make it harder for bankers to do their jobs well in this area:\n1. COVENANT MONITORING\nCovenant information has to be received before it can be analyzed. However, many banks do not have the appropriate tools to generate timely alerts on when these items are due for receipt. Some loan agreement requirements are recorded in antiquated methods that do not provide the level of interaction needed to deal with the sheer volume of such requirements. In an environment when covenant monitoring is not a priority, these items can be left until it is too late. Effective monitoring is especially important when the client is in potential breach of the covenant agreement, as any available remedies might not be as effective if not actioned immediately.\nFor many commercial borrowers, the collection of information requested by lenders is an onerous task that can sometimes be seen as intrusive to the actual running of the business. Commercial bankers spend time chasing customers for information that forms part of the borrowers’ reporting obligations. Often by the time it is received, it is of historical interest only.\nAfter the bank has received the information from the borrower, what then? The financial statements and financial covenants are typically entered into spreadsheet or word documents. In such formats, bankers struggle to pool the data across the entire portfolio to understand how borrowers are performing against covenants and how they are performing against peers. Format also makes looking at historical financial trends on a holistic basis challenging. It is possible to gather this information without a centralized data repository, but it is extremely time and resource consuming. Given profitability pressures and resource constraints, it is rarely a viable option.\n2. PERIODIC REVIEWS\nDocumented requirements in the loan agreement for reviews are not normally differentiated based on financial performance. Whether financial trends are improving, stable, or experiencing some decline, the monitoring requirements can be similar.\nAn annual review is required to be completed each year regardless of the risk rating or the financial stability of the borrower. Reviewing borrower financials, determining the risk rating, and preparing the credit write-up is time consuming. It takes almost the same amount of time as performing a full credit assessment, regardless of borrower financial performance or creditworthiness. In the eyes of some bankers, spending significant time on monitoring financially stable or improving credits is not a good use of their time. They need to focus on those borrowers presenting heightened risk to the bank, while keeping an eye on the good quality credits that can suddenly experience financial or other type of distress. The million dollar question is, which seemingly financially healthy borrower is in fact a potential loss just waiting to happen?\nTransforming monitoring with the use of an integrated system\nTechnology can have a meaningful impact on loan portfolio monitoring, particularly by detecting early warning signals of risk deterioration. With bankers being asked to do more with less resources, technology can help fill that gap by enhancing risk management capabilities and increasing efficiency. Let us look at the practical ways technology can help.\nThe first step is to monitor borrowers and collate the information related to their financial health in accordance with the loan agreement. A robust system that can track requirements under the loan agreement and internal policy requirements is critical. A good system can also alert the banker when items are due from borrowers, or certain tasks need completion internally, such as an annual review or a client due diligence visit.\nIt is also important for the system to track the timeliness of information receipt. If items are past due, it is imperative to dedicate more attention to ensure that outstanding items are resolved as soon as possible. Portfolio managers, senior risk executives, and auditors must know how teams are monitoring loan portfolios, and that they are doing so effectively. They also must know where there are bottlenecks, and how these issues are being addressed. The old adage ‘time is money’ is seldom more true than in dealing with underperforming credits in a loan portfolio. ‘Bad news never improves with age’ is another relevant truism.\nCovenanted information must be captured in a fit-for-purpose tool that allows data to be stored in a centralized database. Doing so offers the possibility of pooling information, and using it in various meaningful ways beyond merely compliance, for example tracking and comparing borrowers across a variety of financial metrics, including revenues, cash flows, and leverage levels. It also means being able to see historical compliance with covenants, how much cushion until they breach, and even potentially having these covenants automatically tested.\nWith new technology, financial statements can now be automatically captured in the lender’s spreading tool without any manual data entry. For example, the lender can use an application program interface (API) to pull information directly from the borrowers’ accounting software package, or use optical character recognition (OCR) technology to read financial statements and the accompanying notes from scanned documents or non-readable PDFs. Machine learning further refines the interpretation of information by learning how to replicate the manual processes currently performed by analysts spreading the financial information. Hence, significantly increasing accuracy with limited manual intervention.\nThe process of remotely capturing financial statements and automatically calculating financial covenant metrics substantially lightens the lender’s administrative burden. It also mitigates risk by reducing the time before the lender is alerted to any financial deterioration.\nMost bankers would accept that unless financial statements are regularly reviewed, negative performance trends often go unnoticed. However, improvements can easily be achieved with the creation of automated system reports and notifications to track deteriorating financial health. “Shadow financial covenants” or internal triggers can be created in most covenant systems, such that when reaching these limits, alerts can be sent to credit analysts or officers informing them of an impending breach. This capability is especially important when dealing with “covenant lite” transactions, where the loan agreement contains limited covenant protection for the lender. Financial ratio triggers might not be limited to a point in time, such as at a quarter end, but instead can be based on period-over-period changes in certain metrics. For example, a trigger could be based on a percentage of sales or EBITDA declining year-over-year or quarter-over-quarter or even month over month on a rolling basis. More targeted monitoring can now take place on those borrowers that actually require review vs. others that have stable or improving financial performance.\nWith the availability of this data, not only can the bank set financial triggers, it can create holistic reports to monitor its entire credit portfolio. Reporting tools now offer dynamic visualizations that instantaneously dissect a portfolio in a multitude of ways to help discover business insights that form the basis of more informed decisions. With the right reporting tool, a lender could break down the portfolio by the amount of leverage being employed, or by revenue or cash flow variation from period to period. For example, out of 100 borrowers in a lender’s portfolio, 10 have leverage of more than 3.0x Debt/EBITDA, whereas the majority falls within the 1-2x range. With this type of information, the lender can now focus on the borrowers that are more highly leveraged and need more attention, while knowing his portfolio as a whole is not highly leveraged. This analysis can help portfolio heads or team leaders identify if there is a systemic credit problem or concentration in a few borrowers.\nCombining up-to-date financial data with a fully integrated risk grading platform enhances the bank’s risk management capabilities, and bring efficiencies in both monitoring and detecting early warning signals. An automated process that could determine whether there is an immediate need to review the borrower based on received financial information is one good example of this benefit. Such a system could also determine the depth of annual review to be completed, or if a risk grading must be performed more frequently. As a result, such systems enable lenders to identify and prioritize their action items. For example, the system could compare the borrower’s current risk rating and its compliance with covenants (or the cushion under the financial covenants), against its financial performance period-over-period, and send a notification if an action is needed. That action can then be traced, and monitored to ensure it is completed.\nThe future of credit monitoring\nIt is a practical truth that many traditional lenders still rely on manual methods of borrower information gathering and human analysis to underpin their credit portfolio monitoring. However non-traditional lenders already employ near real-time data to assess and monitor credit risk effectively. Amazon, for example, relies on the information it captures continuously from its commercial customers to make credit risk decisions in lending money to these customers. It captures data such as daily or monthly sales, payment terms, product returns, and even customer satisfaction ratings to inform those decisions. In turn, the customer obtains working capital finance from Amazon instead of its bank, and doesn’t even have to comply with reporting requirements, as it would at a bank.\nHow long before a Fintech enterprise spots the utility of this arrangement and proposes a “solution?” For example, how long before a Fintech provider creates APIs that access more of a commercial customer’s systems to retrieve their data for its own risk management ends?\nIt is not fanciful to imagine in the near future a loan monitoring system that applies machine learning techniques to borrowers’ financial data, alongside macro and micro economic information, behavioral metrics, and relevant industry key indicators to identify which borrowers might face financial distress. The use of non-traditional data for credit risk assessment is gaining traction in sections of personal lending and could also be adopted in the commercial arena.\nWith an integrated monitoring and rating system and availability of historical data, significant analytics can now be performed to determine the effectiveness of financial and non-financial covenants. Banks can better understand which covenants are most effective, their importance, their ability to predict defaults and the limits at which these covenants are set. Effective monitoring can enhance the bank’s credit position at the time of origination and throughout the term of the loan.\nIn this paper, we have outlined some examples of how an integrated system with better technology can enhance a bank’s risk management capabilities and thus reduce loan losses. Monitoring of loan portfolios can now be conducted based on assessed loan-level risk rather than against inflexible portfolio policies. Monitoring can be simplified and streamlined, with borrowers experiencing credit deterioration receiving the most attention. This approach produces greater efficiency and reduced administration.\nWith the advancement of new tools such as machine learning and automated monitoring, lenders will be able to derive meaningful insights into borrower behavior, credit events, and probability of default. To achieve this vision, lenders must move to a workflow that collects the relevant predictive information, stores it in an easily accessed manner, and applies modern predictive analytic technologies to the challenge of credit monitoring."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:1b709889-e5c1-484b-a248-30484f618335>","<urn:uuid:624ce9be-4537-4099-99d8-ae7b9c91034f>"],"error":null}
{"question":"在戒烟过程中physical symptoms和behavioral changes有什么区别呢？Can someone explain the different challenges? 💪","answer":"Physical symptoms during withdrawal include irritability, sleepiness, low energy, dizziness, cold symptoms, hunger, headaches, and cravings, which are strongest in the first 4-5 days but can last for weeks. Behavioral challenges involve breaking habitual actions and routines, such as the automatic reach for cigarettes, smoking after meals, or smoking in social situations. To address these different challenges, physical symptoms can be managed through drinking water to remove nicotine faster and using nicotine replacement therapy, while behavioral changes can be addressed by avoiding places where smoking is allowed, changing eating habits, and finding substitutes like toothpicks or fidget tools for hand-to-mouth actions.","context":["Return to Index\nTips to Help You Stop Smoking\nSmoking brings thousands of toxins into the body. It is no surprise that it increases the risk of many cancers, heart disease, and stroke. So quitting is one of the best things you can do for your health. You may have also found that smoking makes your clothes, hair, home, or car smell. It may interrupt your workdays or social outings for a cigarette break. Cigarette smoke is also a hazard for those around you. Secondhand smoke increases the risk of illnesses around you, even if they don't smoke.\nWhatever the reason, you are thinking about quitting smoking. Whether this is your first attempt or not, the following steps can help you get started on the right foot.\nOnce you’ve decided to quit smoking, set your target quit date. Make it a few weeks away. In the time leading up to your quit day, try some of these ideas to help you quit successfully.\nSome may feel comfortable quitting on their own. Talk to your doctor if you are looking for tools that can help. Options include nicotine gum, patches, or inhaler, or medicine. Some may need a prescription but many are available over-the-counter. Here are some other steps that may help:\n- Switch to a brand you find distasteful.\n- Change to a brand that is low in tar and nicotine a couple of weeks before your quit date. This will help to ease you off of smoking chemicals. Be careful not to increase the number of cigarettes you smoke to make up for changes in nicotine and tar.\n- Smoke only half of each cigarette.\n- Each day, put off your first cigarette by one hour.\n- Decide you'll only smoke during odd or even hours of the day.\n- Decide beforehand how many cigarettes you'll smoke during the day. For each extra cigarette, give a dollar to your favorite charity.\n- Change your eating habits to help you cut down. For example, drink milk. Many people feel milk and smoking do not mix. End meals or snacks with something that won't lead to a cigarette.\n- Reach for a glass of juice instead of a cigarette for a \"pick-me-up.\"\nRemember: Cutting down can help you quit. It is no substitute for quitting. If you are at 7 cigarettes a day, it's time to set your target quit date.\n- Smoke only those cigarettes you really want. Catch yourself before you light up out of pure habit.\n- Don't empty your ashtrays. This will remind you of how many cigarettes you've smoked each day. The sight and the smell of stale cigarettes butts will be very unpleasant.\n- Make yourself aware of each cigarette. Use the opposite hand or put cigarettes in an unfamiliar location or a different pocket. This will break the automatic reach.\n- Try to look in a mirror each time you put a match to your cigarette. You may decide you don't need it.\n- Stop buying cigarettes by the carton. Wait until one pack is empty before you buy another.\n- Stop carrying cigarettes with you at home or at work. Make them difficult to get to.\n- Smoke in ways that aren't pleasurable for you. If you like to smoke with others, smoke alone. Turn your chair to an empty corner and focus only on the cigarette you are smoking and all its many negative effects.\n- Collect all your cigarette butts in one large glass container. It will give you a visual reminder of smoking.\n- Practice going without cigarettes.\n- Don't think of never smoking again. Think of quitting in terms of one day at a time.\n- Tell yourself you won't smoke today, and then don't.\n- Clean your clothes to rid them of the cigarette smell. The smell can linger a long time if ti is not washed out.\n- Throw away all your cigarettes and matches. Hide your lighters and ashtrays.\n- Visit the dentist. Have your teeth cleaned to get rid of tobacco stains. Notice how nice they look. Commit to keep them that way.\n- Put aside money you would have spent on cigarettes. Make a list of things you'd like to buy for yourself or someone else.\n- Keep very busy on the big day. Go to the movies, exercise, take long walks, or go bike riding.\n- Remind your family and friends that this is your quit date. Ask them to help you over the rough spots of the first couple of days and weeks.\n- Buy yourself a treat or do something special to celebrate.\nTelephone and web-based programs can offer you the support that you need to quit and to stay smoke-free. You can find many programs online.\n- Develop a clean, fresh, nonsmoking environment around yourself. Do it at work and at home. Buy yourself flowers. You may be surprised how much you can enjoy their scent now.\n- Spend as much free time as possible in places where smoking isn't allowed in the first few days. Libraries, museums, theaters, department stores, and churches are good choices.\n- Drink plenty of water.\n- Try to avoid alcohol, coffee, and other drinks that you link with smoking.\n- Strike up conversation instead of a match for a cigarette.\n- If you miss having a cigarette in your hand, play with something else. A pencil, a paper clip, a marble, or a fidget tool may help.\n- If you miss having something in your mouth, try toothpicks or a fake cigarette.\nNational Cancer Institute\nTobacco Information and Prevention Source (TIPS)\nCenters for Disease Control and Prevention\nCenters for Disease Control and Prevention\nUnited States Department of Health and Human Services\nCanadian Cancer Society\nThe Lung Association\nBenefits of quitting smoking over time. American Cancer Society website. Available at: http://www.cancer.org/healthy/stayawayfromtobacco/benefits-of-quitting-smoking-over-time. Accessed July 15, 2020.\nHow to quit smoking. Help Guide website. Available at: http://www.helpguide.org/articles/addiction/how-to-quit-smoking.htm. Accessed July 15, 2020.\nQuit guide smart phone app. Smokefree website. Available at: http://smokefree.gov/apps-quitguide. Accessed July 15, 2020.\nTreatment for tobacco use. DynaMed website. Available at: http://www.dynamed.com/topics/dmp~AN~T905141/Treatment-for-tobacco-use. Accessed July 15, 2020.\n3/25/2008 DynaMed's Systematic Literature Surveillance http://www.ebscohost.com/dynamed/what.php : Parkes G, Greenhalgh T, Griffin M, Dent R. Effect on smoking quit rate of telling patients their lung age: the Step2quit randomised controlled trial. BMJ. 2008;336:598-600.\n7/6/2009 DynaMed's Systematic Literature Surveillance http://www.ebscohost.com/dynamed/what.php : Myung SK, McDonnell DD, Kazinets G, Seo HG, Moskowitz JM. Effects of Web- and computer-based smoking cessation programs: meta-analysis of randomized controlled trials. Arch Intern Med. 2009;169:929-937.\n7/14/2011 DynaMed's Systematic Literature Surveillance http://www.ebscohost.com/dynamed/ : Leonardi-Bee J, Jere ML, Britton J. Exposure to parental and sibling smoking and the risk of smoking uptake in childhood and adolescence: a systematic review and meta-analysis. Thorax. 2011 Feb 15. [Epub ahead of print]\n- Reviewer: EBSCO Medical Review Board\n- Review Date: 07/2020\n- Update Date: 07/15/2020","Bayshore | | Health and Wellness\nQuitting Smoking: It’s Never Too Late!\nDo you have a senior loved one who smokes? Smoking is less common among seniors than younger age groups, but you might be surprised to learn that in Canada, 11% of men and 9% of women over 65 use tobacco daily or occasionally.\nNicotine is highly addictive, and withdrawal symptoms are challenging to cope with. Quitting smoking is challenging for people of any age, but for seniors it often means trying to stop a habit that they’ve had for decades. The good news is that no matter how long they’ve been smoking, people who quit can experience health benefits within hours of their last cigarette. Let’s look at the reasons why it’s great to be a “quitter,” and explore the resources available for smoking cessation.\nRisks of tobacco use\nThere is no doubt that smoking is a dangerous activity. It’s a risk factor for a wide range of health problems, including heart disease, stroke, lung cancer and chronic respiratory disease. In Canada, smoking (or tobacco use) is the leading risk factor for disability and premature death, responsible for 45,000 deaths per year.\nAccording to the Ontario Dental Association, smoking is also linked to many oral health problems, including oral cancer, gum disease, slower healing after oral surgery, stained teeth and bad breath. Smoking is also linked to eye problems (age-related macular degeneration, cataracts and glaucoma), slower healing from surgical procedures, insomnia and reduced bone density.\nWhy is tobacco smoke so harmful? It contains thousands of chemicals; more than 70 of them are known carcinogens (chemicals that cause or promote cancer). Second-hand smoke is also hazardous to health, as are mini-size cigarettes, cigarillos, smokeless tobacco products, tobacco-free herbal cigarettes, discount cigarettes and contraband cigarettes. It’s best to avoid tobacco products altogether!\nHealth benefits of quitting smoking\nAccording to the Lung Association, here’s what happens when a smoker quits:\n- Within eight hours, the level of carbon monoxide in the body decreases, and the oxygen level returns to normal.\n- After 48 hours, the risk of heart attack starts to decrease, and the senses of taste and smell – which can be dulled by smoking – start to improve.\n- At 72 hours, lung capacity increases and breathing is easier.\n- From two weeks to three months of being smoke-free, lung function increases up to 30% and blood circulation gets better.\n- At six months, there is improvement in the coughing, tiredness, shortness of breath and stuffiness caused by smoking.\n- At the one-year mark, the risk of smoking-related heart attack drops by half.\n- The health benefits continue to increase the longer a person is smoke-free.\nCreating a quitting plan\nQuitting smoking is very difficult – many people attempt it 30 times or more before they are successful. Many quitters find it helpful to make a plan. Experts recommend choosing a “quit day” in the near future that the smoker thinks is realistic. The sooner, the better, but it might help for someone to quit on the first day of the next month, on a meaningful date, or at a time when stress will be low, for example. It also helps to keep in mind positive reasons for quitting – whatever is meaningful for your loved one.\nTo increase the chances of success, quitters can also avoid situations and environments where smoking is present; build a support network of friends and family; seek advice from a physician; and take advantage of free smoking cessation resources such as websites and help lines (see “Additional resources,” below).\nMany quitters find it helpful to use nicotine replacement therapy, such as nicotine gum, patches, lozenges or inhalers, to reduce their cravings and other withdrawal symptoms. Some people reduce their smoking by switching from cigarettes to vaping (also known as e-cigarettes), which contains less nicotine and is less harmful than smoking because no burning of tobacco is involved.\nCoping with withdrawal\nWithdrawal symptoms, which are strongest in the first four to five days after quitting, can include irritability, sleepiness, low energy, dizziness, cold symptoms, hunger, lack of focus, cravings, headaches, restlessness and more. Some symptoms can linger for a few weeks.\nWithdrawal symptoms are different for each quitter, and it’s important to find ways to cope. For example, for quitters who feel the urge to smoke, the Lung Association recommends drinking water to help remove nicotine from the body faster; deep, slow breathing; delaying the next cigarette with the hope that the craving will pass; or doing another activity, such as going outdoors or calling a friend.\nIf your loved one is not successful the first few times they try to quit, be patient and supportive, and try to keep them motivated. Together, you can look forward to a smoke-free future.\nSmoking and Tobacco\nThis resource from The Lung Association provides information on quitting smoking, including how counselling, medications and other supports can help people succeed.\nAccess your province’s or territory’s online quit-smoking resources, or call 1-866-366-3667 toll-free.\nSmokers’ Helpline, 1-877-513-5333\nThis free, confidential and personalized service from the Canadian Cancer Society provides support (phone, text messaging or online) to smokers who wish to quit.\nThis website from Johnson & Johnson Inc. offers tools and success stories for first-time quitters and people who have tried to quit before."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:0545794e-c298-4da6-9179-0ff7ebbd0cd5>","<urn:uuid:0a0f16fe-c381-4457-870a-925041cea9d3>"],"error":null}
{"question":"What's happening with Yiddish education and culture revival in recent decades? Where is this revival taking place?","answer":"Over the last two decades, there has been a rising movement to revive Yiddish culture. Universities on nearly every continent offer Yiddish courses, and Yiddish theater troupes have established themselves across Israel, Europe and North America. The revival is happening both in places with large Jewish populations like New York, and significantly, in the Eastern European streets where Yiddish originated. Cultural festivals draw thousands each year, and organizations like the Yiddish Scientific Institute (YIVO) and WJC's International Yiddish Culture Center provide extensive education programs.","context":["Melodic Klezmer notes float through the blistering Lithuanian winter air. A door opens near Vilnius’ old Green Bridge and a group of Jewish friends spill into the street. “Zei Gezunt,” they call to each other in Yiddish, scrolling through their smart phones as they part ways.\nIt is 2016. The vivacious young men and women leaving the World Jewish Congress’ International Yiddish Culture Center are speaking what is considered to be a dying language, uttering words used by their grandparents – words even their parents barely know.\nMore than seven decades have passed since the end of World War II and the destruction of what was once one of the centers of Jewish life in Europe, widely known as the Jerusalem of Lithuania. On the eve of the war, the Jewish community in Vilnius numbered 70,000-80,000, including an influx of refugees from Poland; across Lithuania, there were roughly 250,000 Jews. Yiddish was their native language, as it was for 11-13 million others in Europe, Palestine and the Americas.\nToday, there are approximately 5,000 Jews in Lithuania, and the generation that can still recall Vilnius in its former glory as a center of Jewish culture and for whom Yiddish was their mame-loshn is nearly gone. The number of fluent Yiddish speakers in the world now barely grazes 2 million.\nBut as the great author Isaac Bashevis Singer remarked in 1978, upon receiving the Nobel Prize in Literature, “Yiddish has not yet said its last word.”\nYiddish is not just a language. It is the dissonant pre-jazz tempo of the flute and the snare drum, the fatty schmaltz sizzling in the delicatessen frying pan, the fantastical realities brought to life by the writers Sholem Aleichem and Yitzhak Leib Peretz. It is the legacy of the Ashkenazi Jewish culture that thrived in Eastern Europe for nearly a millennia.\nAnd for a critical mass of Holocaust survivors in the years following liberation, it was these words and memories that empowered them to regain their lives and take back their humanity.\nYiddish is still the mother tongue for more than a million ultra-Orthodox Jews across the world. But since even before the establishment of the State of Israel – thanks to the Zionist movement’s enforcement of the newly revived Hebrew language and the ingathering of Jewish exiles from North Africa and Arab lands whose Judeo languages of their own so differed from that of Eastern European Yiddish – Hebrew has become the Jewish lingua franca.\nStill, a movement has been on the rise over the last two decades, among secular and more modern streams of Judaism alike, to revive the once-vibrant Yiddishkeit culture of their Eastern European ancestors. Universities on nearly every continent offer courses in Yiddish, based on the standard set by the Yiddish Scientific Institute (YIVO), which celebrated its 90th anniversary last year, and has successful centers in New York and Buenos Aires; Yiddish theater troupes have established themselves across Israel, Europe and North America and Yiddish cultural festivals draw thousands each year.\nIn the year since its creation, WJC’s International Yiddish Culture Center has faced increasing demand for its special breed of interdisciplinary Yiddish education, which is geared primarily toward providing formal and informal educators with the tools to successfully pass information on to their own students. From Ukraine to Uruguay, the center has provided seminars to more than 2,000 people of all ages, teaching them not just Yiddish words, but also about the vibrant folklore, literature, music, theater, film and media produced in the pre-war heyday of Yiddish culture.\nIt is perhaps only natural that a Yiddish renaissance might take place in cities like New York, which has the largest Jewish population outside of Israel and where some of the greatest Yiddish theater and culture arose in the early 20th century; it is the antidote to assimilation, an expression of belonging and of reclaiming tradition. YIVO recognized this potential early on and began offering an intensive language, literature and culture program in the late 1960s well before Yiddish revivalism crept back into fashion.\nBut one key evidence of both the need for and the success of the Yiddish cultural revival, is that it is also taking place in the very same Eastern European streets where it was born, in places where strong communities of Jews once lived and in which only a handful now remain. For years, Yiddish was a secret language in the Soviet Union, passed down through the generations as a testament to their Jewish identity. In the late 1980s, just before the fall of the Soviet Union, young Jews began gathering underground in Moscow, holding Yiddish seminars; many of these young participants moved to Israel in the subsequent years and are living a Jewish life they once could only dream of having.\nThe WJC’s center and its advisory board of eminent scholars don’t expect Yiddish to become a street language again, but it is adamantly striving to ensure that the culture it imbued over 1,000 years of history is not forgotten.\nBoth in Israel and in the Diaspora – particularly in Eastern Europe – reviving Yiddish language and culture is crucial for regenerating the Jewish life and identity nearly obliterated in the flames of the Holocaust and subsequent decades of intermarriage and assimilation.\nAs the old saying goes: Di gantse velt shteyt af der shpits tsung — the whole world rests on the tip of the tongue. And if a single word can affect the world, imagine the power of an entire language."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:64e73f1a-8c80-4eaf-a0f0-e6b2d7e878d4>"],"error":null}
{"question":"Hi! In terms of accuracy for respiratory assessment, which performs better - the 95.65% detection rate of the capnogram neural network system or the standard FiO2 calculation method (FiO2 x 5 for PaO2)?","answer":"The capnogram neural network system shows superior accuracy with a 95.65% detection rate and only a 4.34% error rate for identifying respiratory conditions. In contrast, the FiO2 calculation method (multiplying FiO2 by 5 to approximate PaO2) is a rough estimation tool rather than a precise diagnostic method. The capnogram system provides more detailed analysis through frequency components and can specifically identify asthmatic conditions, while the FiO2 calculation is a general approximation for oxygen levels.","context":["Frequency Analysis of Capnogram Signals to Differentiate Asthmatic and Non-asthmatic Conditions Using Radial Basis Function Neural Networks\nIn this paper, the method of differentiating asthmatic and non-asthmatic patients using the frequency analysis of capnogram signals is presented. Previously, manual study on capnogram signal has been conducted by several researchers. All past researches showed significant correlation between capnogram signals and asthmatic patients. However all of them are just manual study conducted through the conventional time domain method. In this study, the power spectral density (PSD) of capnogram signals is estimated by using Fast Fourier Transform (FFT) and Autoregressive (AR) modelling.\nThe results show the non-asthmatic capnograms have one component in their PSD estimation, in contrast to asthmatic capnograms that have two components. Furthermore, there is a significant difference between the magnitude of the first component for both asthmatic and non-asthmatic capnograms.\nThe effectiveness and performance of manipulating the characteristics of the first frequency component, mainly its magnitude and bandwidth, to differentiate between asthmatic and non-asthmatic conditions by means of receiver operating characteristic (ROC) curve analysis and radial basis function (RBF) neural network were shown.\nThe output of this network is an integer prognostic index from 1 to 10 (depends on the severity of asthma) with an average good detection rate of 95.65% and an error rate of 4.34%. This developed algorithm is aspired to provide a fast and low-cost diagnostic system to help healthcare professional involved in respiratory care as it would be possible to monitor severity of asthma automatically and instantaneously.\n1. Steven E. Weinberger, Barbara A. Cockrill, Jess Mandel, Principles of pulmonary medicine, 5th Ed., Elsevier Inc.,2008.\n2. Rhoades C, Thomas F. Capnography: beyond the numbers. Air Med J 2002; 21(2):43-8.\n3. Giner J, Casan P. Pulse Oximetry and Capnography in Lung Function Laboratories. Arch Bronconeumol 2004;40(7):311-4.\n4. Thompson JE, Jaffe MB. Capnography waveforms in the mechanically ventilated patient. Respir Care 2005;50(1):100-9.\n5. Smalhout B., Kalenda Z., An Atlas of Capnography, 2nd Ed., Kerckebosche Zeist Press, 1981.\n6. You B, Peslin R, Duvivier C, Vu V, Grilliat JP.Expiratory capnography in asthma. Eur Respir J 1994: 7(2):318-23.\n7. Yaron M, Padyk P, Hutsinpiller M, Cairns CB. Utility of the expiratory capnogram in the assessment of bronchospasm. Ann Emerg Med 1996; 28(4):403-7.\n8. Druck J, Rubio PM, Valley MA, Jaffe MB, Yaron M.Evaluation of the slope of phase III from the volumetric capnogram as a non-effort dependent in acute asthma exacerbation. Annual of Emergency Medicine 2007;50(3):130-6.\n9. Tan Teik Kean, M. B. Malarvili. Analysis of capnography for asthmatic patient, IEEE International Conference on Signal and Image Processing Applications 2009, 464-7.\n10. Swenson J, Henao-Guerrero PN, Carpenter JW. Clinical Technique: Use of Capnography in Small Mammal Anesthesia. Journal of Exotic Pet Medicine 2008:17(3):175-80.\n11. Kirkko-Jaakkola M, Collin J, Takala J. Bias Prediction for MEMS Gyroscopes, IEEE Sensors 2012; 12(6):2157-63.\n12. Facchinetti A, Sparacino G, Cobelli C. An Online Self- Tunable Method to Denoise CGM Sensor Data. IEEE Trans Biomed Eng 2010: 57(3):634-41.\n13. Gonzalo R. Arce, Nonlinear Signal Processing; A Statistical Approach, John Wiley & Sons, Inc., New Jersey, 2005.14. Gao S, Mateer T. Additive Fast Fourier Transforms Over Finite Fields. IEEE Transactions on Information Theory2010; 56(12):6265-72.\n15. Lauralee Sherwood. Fundamentals of Physiology: A Human Perspective, Thomson Brooks and Cole Inc.,2006.\n16. Soni RK, Jain A, Saxena R. An improved and simplified design of Pseudo-transmultiplexer using Blackman window family. Digital Signal Processing 2010;20(3):743-9.\n17. John L. Semmlow, Biosignal and Biomedical Image Processing, Marcel Dekker Inc., 2004.\n18. Alan V. Oppenhein, Ronald W. Schafer, Discrete-Time Signal Processing, Prentice Hall Signal Processing Series,3rd Ed., 2010.\n19. Hsu HW, Liu CM. Autoregressive Modeling of Temporal/Spectral Envelopes With Finite-Length Discrete Trigonometric Transforms. IEEE Transactions on Signal Processing 2010; 58(7):3692-705.\n20. Takalo RH, Ihalainen HH. Tutorial on Univariate Autoregressive Spectral Analysis Export. J Clin Monit Comput 2006; 20:379-88.\n21. Tracey Cassar, Kenneth P. Camilleri, Simon G. Fabri, Order Estimation of Multivariate ARMA Models. IEEE journal of Selected Topics in Signal Processing 2010;4(3):494-503.\n22. K J Blinowska, J Zygierewicz, Practical Biomedical Signal Analysis, Poland, CRC Press (Taylor & Francis Group, LLC), 2011.\n23. Fang-Xiang Wu, Wen-Jun Zhang, Dynamic-Model-Based Method for Selecting Significantly Expressed Genes From Time-Course Expression Profiles. IEEE Trans Inform Technol Biomed 2010; 14(1):16-22.\n24. Temko A, Lightbody G, Thomas EM, Boylan GB, Marnane W. Instantaneous Measure of EEG Channel Importance for Improved Patient-Adaptive Neonatal Seizure Detection. IEEE Trans Biomed Eng 2012;59(3):717-27.\n25. Ramachandran P, Lu WS, Antoniou A. Filter-Based Methodology for the Location of Hot Spots in Proteins and Exons in DNA. IEEE Biomed Eng 2012; 59(6):1598-1609.\n26. Acharya UR, Dua S, Du X, Sree S V, Chua CK.Automatic Diagnosis of Glaucoma Using Texture and Higher Order Spectra Features. IEEE Trans Inf Technol Biomed 2011; 15(3):449-55.\n27. M. D. Buhmann, Radial Basis Functions: Theory and Implementations, Cambridge University Press, Cambridge, 2004.\n28. Tiantian Xie, Hao Yu, Joel Hewlett, Pawel Rozycki, Bogdan Wilamowski. Fast and Efficient Second-order Method for Training Radial Basis Function Networks. IEEE Transactions on Neural Networks and Learning Systems 2012; 23(4):609-19.\n|Issue||Vol 12, No 3 (2013)|\n|Asthma Autoregressive (AR) modeling Capnogram Fourier analysis Radial Basis function (RBF) network|\n|Rights and permissions|\n|This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.|","- Why is high FiO2 bad?\n- What happens if your oxygen concentrator is set too high?\n- What is a good PF ratio?\n- What is a normal FiO2?\n- What does a high PaO2 mean?\n- How do you calculate PaO2 from FiO2?\n- What does fio2 stand for?\n- What is AP F ratio?\n- What does a low PaO2 indicate?\n- What is normal PEEP pressure?\n- What is FiO2 on ventilator?\n- What is a good peep level?\n- What is a normal PaO2 FiO2 ratio?\n- What is a normal PaO2 level?\n- How do you calculate PaO2?\n- Why is FiO2 important?\n- How do you increase your FiO2 level?\nWhy is high FiO2 bad?\nHyperoxia causes complex effects on several physiologic functions.\nIt may affect alveolar ventilation/perfusion (Va/Q) (50), may reverse hypoxic vasoconstriction (51, 52), may induce pulmonary toxicity (53, 54) and it may reduce tissue blood flow due to vasoconstriction (55)..\nWhat happens if your oxygen concentrator is set too high?\nWe do know however, high concentrations of oxygen over a period of time cause an overproduction of free radicals in the lungs. If unchecked, these radicals can severely damage or kill lung tissue. If left for a prolonged period of time the patient can suffer permanent lung damage.\nWhat is a good PF ratio?\nThe New P/FP Ratio of 300 to 200 is mild, 200 to 100 is moderate and less than 100 is severe Adult Respiratory Distress Syndrome (ARDS) for all the different levels of PEEP values.\nWhat is a normal FiO2?\nNatural air includes 21% oxygen, which is equivalent to FiO2 of 0.21. Oxygen-enriched air has a higher FiO2 than 0.21; up to 1.00 which means 100% oxygen. FiO2 is typically maintained below 0.5 even with mechanical ventilation, to avoid oxygen toxicity, but there are applications when up to 100% is routinely used.\nWhat does a high PaO2 mean?\nPO2 (partial pressure of oxygen) reflects the amount of oxygen gas dissolved in the blood. It primarily measures the effectiveness of the lungs in pulling oxygen into the blood stream from the atmosphere. Elevated pO2 levels are associated with: Increased oxygen levels in the inhaled air. Polycythemia.\nHow do you calculate PaO2 from FiO2?\nPaO2/FiO2The PaO2 rises with increasing FiO2. Inadequate or decreased oxygen exchange decreases the ratio.Normal PaO2/FiO2 is >400 mmHg.Approximate PaO2 by multiplying FiO2 by 5 (eg, FiO2 = 21%, then PaO2 = 100 mmHg)\nWhat does fio2 stand for?\nFraction of Inspired OxygenFor all supplemental oxygen delivery devices, the patient is not just breathing the direct oxygen, but rather is breathing a combination of room air plus the oxygen from the supplemental device.\nWhat is AP F ratio?\nThe P/F ratio equals the arterial pO2 (“P”) from the ABG divided by the FIO2 (“F”) – the fraction (percent) of inspired oxygen that the patient is receiving expressed as a decimal (40% oxygen = FIO2 of 0.40). A P/F Ratio less than 300 indicates acute respiratory failure.\nWhat does a low PaO2 indicate?\nIf a PaO2 level is lower than 80 mmHg, it means that a person is not getting enough oxygen. A low PaO2 level can point to an underlying health condition, such as: emphysema. chronic obstructive pulmonary disease, or COPD. pulmonary fibrosis.\nWhat is normal PEEP pressure?\nApplied (extrinsic) PEEP is usually one of the first ventilator settings chosen when mechanical ventilation is initiated. It is set directly on the ventilator. A small amount of applied PEEP (4 to 5 cmH2O) is used in most mechanically ventilated patients to mitigate end-expiratory alveolar collapse.\nWhat is FiO2 on ventilator?\nThere are a variety of different ventilator settings than can be used to support a patient’s breathing. … The concentration of oxygen in the air that we breathe is called the FiO2 (Fraction of inspired oxygen). If a patient is not receiving any additional oxygen, we often say that the patient is on an FiO2 of .\nWhat is a good peep level?\nApplying physiologic PEEP of 3-5 cm water is common to prevent decreases in functional residual capacity in those with normal lungs. The reasoning for increasing levels of PEEP in critically ill patients is to provide acceptable oxygenation and to reduce the FiO2 to nontoxic levels (FiO2< 0.5).\nWhat is a normal PaO2 FiO2 ratio?\nA normal P/F Ratio is ≥ 400 and equivalent to a PaO2 ≥ 80 mmHg.\nWhat is a normal PaO2 level?\nNormal Results Partial pressure of oxygen (PaO2): 75 to 100 millimeters of mercury (mm Hg), or 10.5 to 13.5 kilopascal (kPa) Partial pressure of carbon dioxide (PaCO2): 38 to 42 mm Hg (5.1 to 5.6 kPa) Arterial blood pH: 7.38 to 7.42.\nHow do you calculate PaO2?\nThe alveolar gas equation is of great help in calculating and closely estimating the partial pressure of oxygen inside the alveoli. The alveolar gas equation is used to calculate alveolar oxygen partial pressure: PAO2 = (Patm – PH2O) FiO2 – PaCO2 / RQ.\nWhy is FiO2 important?\nOxygen, we all need it! We do not need a lot of it under normal circumstances, with 0.21 being the fraction of inspired oxygen (FiO2) of room air. FiO2 is defined as the concentration of oxygen that a person inhales. … This allows the concentration of oxygen to be increased, potentially increasing the FiO2 to 100%.\nHow do you increase your FiO2 level?\nMain controlsincrease FIO2.increase mean alveolar pressure. increase mean airway pressure. increase PEEP. increase I:E ratio (see below)re-open alveoli with PEEP."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:4cc689d5-edaf-4cc7-b4ce-3b1fd92e1339>","<urn:uuid:42526fe2-cd37-477c-84b8-ea400b97eba5>"],"error":null}
{"question":"What is the cost difference between tooth bonding and replacing veneers?","answer":"Tooth bonding ranges from $300 to $600 per tooth, while replacing veneers is significantly more expensive, costing between $925 and $2,500 per tooth according to the American Dental Association.","context":["- What is tooth bonding?\n- How long does tooth bonding last?\n- Who is a candidate for tooth bonding?\n- How is Tooth Bonding Accomplished?\n- What is the Tooth Bonding Treatment/a>\n- Types of Tooth Bonding Procedures\n- How much does tooth bonding cost?\n- Advantages & Disadvantages of Tooth Bonding\n- What are the side effects?\nWhat is tooth bonding?\nMost often, bonding is used for cosmetic purposes to improve the appearance of a discolored or chipped tooth. It also can be used to close spaces between teeth, to make teeth look longer or to change the shape or color of teeth. Sometimes, bonding also is used as a cosmetic alternative to amalgam fillings, or to protect a portion of the tooth’s root that has been exposed when gums recede.\nBonding is a composite resin filling placed in the back teeth as well as the front teeth. Composites are the solution for restoring decayed teeth, making cosmetic improvements and even changing the color of your teeth or the reshaping of teeth. Bonding will lighten any stains you may have, close up minor gaps and can be used to correct crooked teeth. Basically, bonding will cover any natural flaws applying a thin coating of a plastic material on the front surface of your teeth. After this, your cosmetic dentist will apply a bonding material and sculpt, color and shape it to provide a pleasing result. A high-intensity light then hardens the plastic, and the surface is finely polished.\nHow long does tooth bonding last?\nWho is a candidate for tooth bonding?\nIf you have close, small gaps between your front teeth, or if you have chipped or cracked teeth, you may be a candidate for bonding. Bonding is also used for patients who have discolored teeth, uneven teeth, gum recession or tooth decay. Bonding material is porous, so smokers will find that their bonding will yellow. If you think you are a candidate for bonding, discuss it with your dentist.\nHow is Tooth Bonding Accomplished?\nBonding is among the easiest and least expensive of cosmetic dental procedures. The composite resin used in bonding can be shaped and polished to match the surrounding teeth. A very mild etching solution is applied to your teeth to create very small crevices in the tooth’s enamel structure. These small crevices provide a slightly rough surface permitting a durable resin to bond materials to your teeth. The resin is then placed on your tooth and high-intensity light cures the resins onto your tooth’s surface – with each individual layer of resin hardening in just minutes. When the last coat has been applied to your tooth, the bonded material is then sculpted to fit your tooth and finely polished.\nThe resin comes in many shades so that we can match it to your natural teeth. Due to the layers involved, this procedure will take slightly longer than traditional silver fillings because multiple layers of the bonding material are applied. Typically bonding takes an hour to two hours depending on your particular case.\nWhat is the Tooth Bonding Treatment\nYour dentist will use a shade guide to select the composite resin color that will match the color of the tooth most closely.\nOnce your dentist has chosen the color, he or she will slightly abrade or etch the surface of the tooth to roughen it. The tooth will be coated lightly with a conditioning liquid, which helps the bonding material adhere.\nWhen the tooth is prepared, your dentist will apply the tooth-colored, puttylike resin. The resin is molded and smoothed until it’s the proper shape. Then the material is hardened with an ultraviolet light or laser.\nAfter the bonding material hardens, your dentist will further trim and shape it. Then he or she will polish the material until it matches the sheen of the rest of the tooth surface.\nThe procedure usually takes about 30 minutes to an hour. If you’re having more than one tooth done, you may need to schedule several visits.\nTypes of Tooth Bonding Procedures\nThere are two types of bonding. What type is indicated in your situation depends upon whether you have a small area or a larger area that requires correction.\nFor small corrections\nThese are one appointment fillings which are color-matched to the tooth and are bonded to the surface for added strength. These are most appropriate for small fillings and front fillings as they may not be as durable for large fillings.\nFor larger corrections\nDental lab-created tooth-colored fillings require two appointments and involve making a mold of your teeth and placing a temporary filling. A dental laboratory then creates a very durable filling to custom-fit the mold made from your teeth. These fillings are typically made of porcelain. The custom-fit filling is then bonded to your tooth on your return visit. This type is even more natural looking, more durable and more stain resistant.\nHow much does tooth bonding cost?\nAs with all procedures, prices vary depending on your location. The cost of dental bonding will also vary with the extent of the bonding process you need. Many dental insurance plans cover most of the cost of the bonding, particularly when it is done for structural reasons.\nThe average cost of cosmetic dental bonding ranges from $300 to $600 per tooth.\nAdvantages & Disadvantages of Tooth Bonding\nAdvantages of dental bonding:\nEsthetics is the big advantage over silver fillings. As silver does not stick to teeth, entirely healthy tooth structure is usually removed to keep a silver filling in place. Composites permit your cosmetic dentist to remove only the decayed area of your tooth. Unlike silver fillings, composite bonding expands just like your teeth and are much less likely to cause cracks in your tooth. Composites bond directly to the tooth providing support. Composites can be used to fill in cracks, chips and gaps – and will match the color of your other teeth.\nDisadvantages of the dental bonding procedures:\nBonding with composites simply costs more in material and time.\nWhat are the side effects?\nThe composite resin used in bonding isn’t nearly as strong as a natural tooth. Biting your fingernails or chewing on ice or pens can chip the material. Bonding usually lasts several years before it needs to be repaired. How long it actually lasts depends on how much bonding was done and your oral habits.","Replacing existing veneers is very straightforward. The dentist files down the bonding agent that glued the previous veneers, then take an impression of your teeth. Once the new veneers are ready, they will be applied to your teeth using the same method as before.\nIs replacing veneers difficult?\nReplacing the veneer is a very similar process to the initial installation. Your dentist will file away as much of the old bonding agent and then take a mold of your teeth. The new veneer will then be creating and bonded back to your teeth.\nIs it painful to replace veneers?\nEvery phase of the veneer procedure should be comfortable and pain-free. The removal of enamel can create sensitivity, so when the tooth is prepared, the area will be numbed, just like when having a cavity filled.\nHow much does it cost to get veneers replaced?\nHow much do veneers cost? Veneers aren’t often covered by insurance because they’re considered a cosmetic procedure. In general, you can expect to pay between $925 and $2,500 per tooth, according to the American Dental Association. Composite veneers cost around $400 to $2,000 per tooth and last between 5 to 7 years.\nDo your teeth rot under veneers?\nWhile the dental porcelain used in your veneers will not decay, it is possible for cavities to form behind your porcelain veneers. When this happens, the resulting tooth decay will threaten the long term health of your teeth and potentially shorten the lifespan of your restoration.\nHow many times can you redo veneers?\nVeneers Can Last for Decades and Rarely Fail All at Once\nIt’s usually 1-2 at a time. This means you can replace them as needed. Steve chose to redo all of his veneers because he wanted all of them even brighter than the first set. If the rest of the veneers are ok, there is no need to redo all of them.\nHow do you remove old veneers?\nPush the veneer gently using an excavator, but do not use force to avoid breakage. Alternate between laser ablation and gentle pressure on the veneer until it is removed. Repeat steps one to three for the remaining veneers (Figs.\nHow do dentists remove old veneers?\nThe Process for Replacing Porcelain Veneers\nWith the veneer removed, the dentist cleans the tooth and then preps it This may involve the removal of just a bit of tooth structure to accommodate the new veneer or to address underlying structural issues. An impression is then taken of the prepped tooth.\nCan I get my veneers removed?\nVeneers can be removed and leave the tooth somewhat intact depending on the skill of the dentist, but there should not be any reason to do this. If the consultation with your dentist was comprehensive and your veneers were high quality and properly fitted to begin with, they shouldn’t need to be removed anytime soon.\nWhat are disadvantages of veneers?\nThe downsides of veneers\nDental veneers are irreversible because a dentist must remove a thin layer of enamel before they fit veneers over teeth. Removing a layer of enamel can make a tooth more sensitive to heat and cold; a veneer is far too thin to act as a barrier between the tooth and hot and cold foods.\nWhat’s better Lumineers or veneers?\nDental lumineers are suitable to treat discolored and unusually shaped teeth. They are smooth and slick to touch. Lumineers are transparent than porcelain veneers. It is why they are a better solution if you have severely discolored teeth.\nHow much would 4 veneers cost?\nPorcelain veneers range in price from $925 to $2,500, but average around $1500 per tooth. The cost may be greater if dental contouring is required. The procedure generally requires several visits. If more visits are required, or several veneers have to be placed, your overall cost will may increase.\nWhy you should not get veneers?\nWhile veneers do have both cosmetic and restorative functions, they are not a substitute for healthy teeth and gums. Veneers should never be used to mask serious dental problems. Any underlying issues with tooth decay or periodontal disease should be corrected before veneers are considered.\nDo veneers last a lifetime?\nWith the proper care, veneers do have the potential to last a lifetime. Even if your veneer becomes damaged or worn, we can replace it.\nDo all celebrities have veneers?\nCelebrities seem to have it all, but sometimes, they spend a lot of time and money to look that way. While these celebs have perfect teeth now, that wasn’t always the case. Here are 10 celebrities you didn’t know got veneers, or other major cosmetic dentistry done."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:c38b7ea0-2877-4fdf-9a82-5b226f4f4a0f>","<urn:uuid:009c37aa-f350-409a-a1fc-011027779ae6>"],"error":null}
{"question":"How do procedural benchmarks in structural analysis compare to traditional benchmarks in terms of their focus and purpose?","answer":"Procedural benchmarks, while sharing many characteristics with traditional benchmarks, specifically focus on modeling issues that analysts face and various procedures adopted in the analysis and assessment process. This is demonstrated in the analysis of fabricated structures where they examine aspects like intersections, reinforcement plates, and offset shell midsurfaces. In contrast, traditional benchmarks in Finite Element Analysis focus more on validating the accuracy and reliability of simulation software, as evidenced by NAFEMS' role in testing simulation software against benchmark Static, Thermal, Nonlinear, Frequency, and Linear Dynamic studies with known results.","context":["- Procedural Benchmarks for Common Fabrication Details in Plate/Shell Structures\n- State of the Art Review - Weld Simulation Using Finite Element Methods\n- FEM Idealisation of Joints\nIt was anticipated that the unique forum provided by FENET would provide an excellent basis for these studies. As part of this work package, three ‘procedural benchmarks’ were developed to reflect some of the modelling issues relevant to fabricated structures. These benchmarks and selected reference solutions, will provide a worthwhile educational resource. The benchmarks also formed the basis of the ‘round robin’ exercise. While the benchmarks have many of the characteristics of traditional benchmarks, they differ in that they are designed to focus on some of the modelling issues that analysts are faced with and the various procedures adopted in the analysis and assessment process.\nSection 1 of the report provides a brief overview of the various workshops involved in this study, as well as some background to the topic area. The specifications for the Procedural Benchmarks and pro-forma results forms are presented in section 2. These should provide a useful reference for any organisation intending to use them as part of, say, a staff development programme. Observations from the ‘round robin’ are included in section 3 and these detail some surprising results. In the first two benchmarks about half of the respondents provided results which suggested that they had made modelling errors. In the third example, only two out of ten respondents realised that this is a nonlinear geometric problem. Whilst some contributions were no-doubt completed under time pressure, it can be argued that this is a reflection of the everyday industrial environment for many engineers. The resulting levels of human error and lack of results checking, for what some might regard as simple case studies, must be of wider interest and concern. The general spread of results arising from the different modelling and assessment strategies should also be of interest. The outcomes certainly confirm the ongoing role that organisations such as NAFEMS have, in ensuring quality and promoting the education and development of analysts and engineers. Hopefully the educational and quality implications from the study will provide some impetus to general activity in this area. It is interesting to reflect on the fact that the same exercise and same general conclusions, could probably have been made 30 years ago.\nSelected reference solutions for the benchmarks are presented in sections 4-6 and these include an examination of the main issues identified in the benchmarks. The main issues selected for examination were:\n- The modelling and assessment of intersections;\n- The modelling of reinforcement plates;\n- The modelling of offset shell midsurfaces.\nAll of the details include welds and the modelling and fatigue assessment of these are obviously an integral part of the whole exercise. Assessment of the results from the benchmarks was made according to various industry standards and this is also reported. The appendices to this report, provides some useful background reference material on the modelling of welds and also some interesting observations on the modelling of plate/shell intersections.\nFirst Published - March 2005\nSoftback, 20 Pages\nWelds are often an essential part of engineering structures. Residual stresses introduced in the welded regions, due to the nonlinear thermal processes during welding, can have detrimental effects, such as stress corrosion cracking, hydrogen-induced cracking and reduced fatigue strength. It is therefore pertinent to simulate the process of welding to predict the behaviour of welded structures from finite element residual stress and deformation results.\nThis report introduces finite element volume methods for the modelling of welds and it depicts a brief history of the simulation of welds. A description of the heat flow processes and solid phase transformations is given in the theoretical background section. The procedure of thermal and mechanical finite element analyses is explained in the third section, titled Finite Element Weld Simulation, which also presents other examples of finite element analyses and describes the effects of solid phase transformations incorporated in the simulation of welds.\nIn the fourth section of the report, related research published in literature is discussed, proposing many modelling considerations which are relevant to weld simulation. This includes parametric studies and characterisation of residual stresses, the effect of material properties on residual stresses, three-dimensional geometric influences, an outline of the adaptive mesh technique and the shrinkage volume approach, and the combination of welding simulation with other heat transfer engineering processes.\nFriction stir welding is described in the penultimate section of the report, which is followed by a description of the process of inertia friction welding. The finite element simulation of the two types of friction welding is discussed.\nFEM Idealisation of Joints\nCompiled by: Peter Hopkins\nFirst Published - January 2012\nSoftback, 157 pages\nJoints are an integral part of products in all sectors of industry, Civil and Offshore, Power and Pressure, Aerospace, Land Transport, Consumer Goods and Biomechanical applications. Joints are important not only with respect to structural integrity and performance of the products themselves but in the jigs, fixtures and test hardware used throughout the product manufacture and qualification process.\nComponents are assembled together to maintain structural integrity and performance using a variety of methods, either temporary mechanically fastened allowing for repeated disassembly and reassembly necessary for maintenance activities or permanent attachment.\nMechanically fastened joints may be temporary such as: bolts into nuts, anchor nuts or tapped holes, or permanent such riveted joints. Permanent joints also include for example, bonded, single lap, double lap, welded, fillet welds, spot welds, seam welds and soldered or brazed.\nTo maintain performance the joints need to survive all the loading environments experienced during the manufacture, assembly and testing and to deliver the required product performance throughout the in service life. These can be steady state or cyclic from vibration, shock and thermal environments or combinations of such loads at various points in the products life cycle.\nCompanies in the various industry sectors will have their own internal processes and methods for assessing structural integrity of the joints used with respect to their particular product or will make use of data from engineering bodies or standard engineering texts such as those listed below for example:\n- Engineering Science Data Unit, ESDU, who provide data items on various engineering analysis methods and materials.\n- Mechanical Engineering Design by J.E. Shigley, C.R. Mischke and R.G. Budynas.\n- ECSS-E-HB-32-23A Space Engineering, Threaded Fasteners ESA-ESTEC.\n- VDI 2230 Part 1Systematic Calculation of High Duty Bolted Joints - Joints with One Cylindrical Bolt\n- Handbook of Bolts and Bolted Joints by J. H Bickford and S. Nassar.\nProduct structural integrity is now routinely assessed using simulation techniques. Therefore it is necessary to represent the wide variety of joints used in product assemblies with an idealisation at system level to represent the stiffness characteristics and to recover loads for application to more detail models to assess the integrity of the joint itself.\nThe availability of sources of information in the public domain on idealisation of joints in simulation of the performance of bolted, riveted, welded, brazed and soldered joints is not well defined.\nThis document provides a synopsis of a NAFEMS seminar on Idealisation of Joints held in Wiesbaden in April 2010 including a summary of the industry sectors that presented information, the scope of information presented and copies of papers presented.\n£86 | $111 | €98\n£177 | $228 | €201\nOrder Ref: B05","Finite Element Analysis (FEA) is the simulation of physical phenomena using a numerical technique called the Finite Element Method (FEM). Over the past few decades, numerical techniques have been developed to address Partial Differential Equations (PDE). A PDE is a mathematical equation that involves multiple independent variables, an unknown function that is dependent on those variables, and partial derivatives of the unknown function with respect to the independent variables. PDEs are commonly used to define multidimensional systems in physics and engineering. These differential equations are complex equations that need to be solved in order to compute relevant quantities of a structure, like stresses, strains and more, in order to estimate the structural behavior under given circumstances.\nToday, one of the most prominent methods to solve partial differential equations is through Finite Element Analysis (FEA). It gives an approximate solution to the problem using a numerical approach that gets the real result of these partial differential equations. Finite Element Analysis is widely used in many engineering disciplines for solving structural mechanics, vibration, heat transfer, and other real life problems.\nIn a nutshell, Finite Element Analysis (FEA) is a numerical method (Finite Element Method) used for the prediction of how a part or assembly in your design behaves under real life conditions. It is also used as one of the key principles to develop leading simulation software. Engineers leverage on Finite Element Analysis to run virtual experiments, find weak spots, areas of tension, in their design and reduce the need for physical prototypes which can be quite costly.\nFinite Element Analysis (FEA) is used to predict the behavior of mechanical and thermal systems under their operating conditions, to reduce the design cycle time, and to improve overall system performance.\nThe geometry starts in the CAD environment. Hence, the first step is to migrate the CAD model into a form ready for FEA. Geometric Representation creates the geometric features of the system to be analyzed and stored in a CAD database.\nDiscretization of geometry\nSplits the geometry into relatively small and simple geometric entities, called finite elements. This discretization process is better known as mesh formulation. The elements are called “finite” to emphasize the fact that they are not infinitesimally small, but only reasonably small in comparison to the overall model size.\nDevelops the equations that describe the behavior of each finite element. Material properties for each element are considered in the formulation of the governing equations. This involves choosing a displacement function within each element. Linear and quadratic polynomials are frequently used functions.\nObtains the set of global equations for the entire model from the equations of Individual elements. The loads and support (boundary) conditions are applied to the appropriate nodes of the finite element mesh.\nSolution of equations\nProvides the solution for the unknown nodal degrees of freedom (or generalized displacements).\nObtains visualization plots for quantities of interest, such as stresses and strains.\nThe use of Finite Element Analysis (FEA) originated from the need to solve complex structural analysis problems in civil and aeronautical engineering. Now, its applications span across various areas like computer and industrial applications, Electromagnetic fields, Biomedical, Thermal Analysis, Fluid Flows, etc. Here, we'll discuss the common Finite Element Analysis applications:\nStructural analysis involves determining the behavior of a structure when it is subjected under certain conditions like loads resulting from gravity, wind, or even in extreme cases natural disasters (e.g., earthquakes). Using basic concepts of applied mathematics, any built structure can be analyzed.\nFinite Element Analysis (FEA) is part of a multi-disciplined engineering approach. While Computational Fluid Dynamics (CFD) gives a detailed simulation of fluid flows, many engineers still rely on FEA to determine the effects of fluid flows on a piece of equipment being analyzed and how that equipment will hold up over time.\nThe increased demand for high performance machines is making Thermal Analysis a crucial part in their development. When addressing thermal challenges, engineers use a variety of tools and the most common solutions used are Computational Fluid Dynamics (CFD) and Finite Element Analysis (FEA).\nHeat transfer analysis with FEA is used in cases where the heat convection coefficient can be assumed constant over the part's surface or when the coefficient is known with sufficient accuracy. Heat transfer between solids such as convection conduction and thermal radiation can be analyzed with FEA along with thermal expansion and structural stresses from thermal gradients in the part.\nThe use of Finite Element Analysis (FEA) to solve boundary-value problems in a variety of engineering fields could be a powerful technique. Similarly, FEA has been widely used for simulating electromagnetic fields in antennas, radar scattering, microwave and RF engineering, high speed and frequency circuits, wireless communication, electromagnetic compatibility, photonics, remote sending, biomedical engineering and space exploration.\nThe most crucial aspects when choosing an analysis software are its accuracy and reliability. How do you ensure that results from a particular FEA software can be trusted? Fortunately, there is an independent group that inspects the credibility of a simulation software. The National Agency for Finite Element Methods and Standards (NAFEMS) tests simulation software against benchmark Static, Thermal, Nonlinear, Frequency, and Linear Dynamic studies with known results that have been validated mathematically and empirically.\nSOLIDWORKS Simulation is a user friendly analysis tool that enables designers and engineers to simulate and analyze design performance through the use of Finite Element Analysis (FEA). All study types available in SOLIDWORKS Simulation have been tested and validated by NAFEMS.\nSOLIDWORKS Simulation is fully integrated in your SOLIDWORKS CAD. The great thing about the SOLIDWORKS software is that it lets you carry out the tests and evaluations in the same environment letting you determine and analyze product performance early in the design process. This reduces the cost for physical prototypes and takes away the time to alternate between filed tests and modifications.\nSOLIDWORKS Simulation offers three simulation solutions design to meet the needs of different users:\nSOLIDWORKS Simulation Standard is a famous FEA software amongst mechanical engineers and product design engineers. This FEA software is typically used for structural, motion and fatigue analysis of parts and assemblies.\nSOLIDWORKS Simulation Professional includes more capabilities like thermal, frequency, drop test, buckling and optimization studies. This analysis software can also perform sequential multi-physics simulation.\nSOLIDWORKS Simulation Premium includes three advanced simulation studies: Non-Linear Static, Non-Linear Dynamic and Linear Dynamics. With this FEA software, engineers can efficiently evaluate their designs for nonlinear and dynamic response, dynamic loading and composite materials.\nFind out which SOLIDWORKS FEA software is right for you with SEACAD. As the leading SOLIDWORKS partner and reseller in Singapore, we provide access to SOLIDWORKS programs and tools that can help you develop complex solutions across various industries. We offer professional consultancy services, as well as training courses for our wide range of products."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:3ac3c0c9-cd65-4bd5-b5e8-3aaa0a61eea3>","<urn:uuid:560be509-f695-4e38-ac49-01e3ffa41d28>"],"error":null}
{"question":"How did NASCAR champion Tony Stewart connect with music, and what was Sam Bass's role in creating victory celebrations for NASCAR winners?","answer":"Tony Stewart appeared in 3 Doors Down's music video 'The Road I'm On' in 2003, racing against Dale Earnhardt Jr. in custom Chevy Tahoes. This led to a business relationship where Stewart drove a 3 Doors Down-sponsored No. 8 Chevy. As for victory celebrations, Sam Bass played a crucial role by designing the famous Gibson guitars that were presented to NASCAR champions in Victory Lane at Nashville Superspeedway, which became one of the most sought-after trophies on the Truck and Xfinity circuits.","context":["NASCAR’s relationship with musicians, particularly country stars, has been tight for some time. A whole bunch of NASCAR drivers have been the inspirations behind songs, starred in music videos, and even recorded music themselves. But, when you’re in the music business there is something particularly appealing about being in the presence of a NASCAR champion.\nIt’s not easy to win a NASCAR title. It takes the right driver, the right team, and the right time to create that special title-winning formula. The vibe of a NASCAR champion is infectious, in that it brings with it a sense of success and a determined spirit. When an artist is in the process of making a music video, they’re probably going to want someone in the business of winning to be on set. It’ll rub off on the entire creative process. Plus, the name recognition associated with the champion definitely provides a solid numbers boost.\nWith that in mind, it shouldn’t be all that surprising that these four iconic NASCAR drivers have “Cup Series champ” and “music video star” on their resumes.\n4 NASCAR Champs in Music Videos\nBill Elliott won the NASCAR Cup Series championship in 1988, and in 1996, he was featured in the music video for Alan Jackson’s “Who’s Cheatin’ Who.” Chase Elliott’s dad shared the spotlight with quite a few NASCAR legends, including Dale Jarrett, who won the title the year after Elliott in ’89, and Rusty Wallace, the 1999 Cup Series champ.\nAs it turns out, Elliott’s foray into country music started almost 10 years before his music video debut. In 1985, “Awesome Bill From Dawsonville” was one of 22 NASCAR drivers to provide lead vocals on an actual country album called “World Series of Country Music Proudly Presents Stock Car Racing’s Entertainers of the Year.” Elliott’s song “A Crazy Racin’ Man” focused on his NASCAR career.\nThe album, which also featured Dale Earnhardt, Cale Yarborough, and Bobby Allison, was a big-time flop, selling only 20,000 copies. It’s a good thing Bill Elliott stuck with his day job.\nDale Earnhardt Sr.\nSeven-time Cup Series champion Dale Earnhardt Sr. and 17-time CMA award-winners Brooks and Dunn were both fans of each other, as the song “Sunday Money” was a tribute to Dale, who responded to the musical homage by naming his yacht after it. But, despite the mutual respect between the trio, it wasn’t easy to convince Earnhardt to co-star in the music video for “Honky Tonk Truth.”\nThe Intimidator wasn’t intimidated by anything on the race track, but music was a whole different ball game. The plan for the music video was to have him dress up like Kix Brooks, strum the guitar, and lip-sync along to the song. Dale was apprehensive about the whole idea, insisting that he couldn’t sing or act in any way.\nEventually, Brooks and Dunn talked Dale into the gig, assuring him that they would cut out any scenes that he wasn’t happy with. Dale eventually came around to the idea, and with a little bit of liquid courage, showed up to the set in good spirits. As you can see from the video, it turned out pretty well!\nBrad Paisley and Jeff Gordon have been friends for a long time, so it was probably pretty easy for the three-time Grammy Award winner to convince the four-time Cup Series champion to star in the music video for his 2011 hit “Old Alabama.”\nThe prominent featuring of the members of renowned country band Alabama was more than enough to make this video a classic, but the NASCAR-based star power for the video, which was partially featured at the Hendrick Motorsports complex in Charlotte, North Carolina, took this video to another level. Three-time Cup champion Darrell Waltrip, legendary team owner Rick Hendrick, and Kelley Earnhardt Miller, Dale Earnhardt Jr’s sister and co-owner of JR Motorsports, starred alongside Gordon, who undoubtedly had an absolute blast filming the driving scenes.\nPaisley and Gordon have actually collaborated on various projects and stunts over the years. Ahead of the 2010 CMA Awards, which Paisley co-hosted with Carrie Underwood, the country star convinced Gordon to dress up just like him and walk on stage as if he were Paisley himself. The prank was a smashing success, as it’s clear that Gordon and Paisley practically look like twins.\nIt was Tony Stewart vs. Dale Earnhardt Jr. in the music video for the 3 Doors Down single “The Road I’m On.”\nFilmed in 2003, the video features Stewart, a three-time Cup champ, and Dale Jr. street racing custom Chevy Tahoes before continuing the race on a local short track. Getting paid to race? The gig had to be a no-brainer for Dale and Tony.\nAs it turns out, Stewart, Earnhardt, and 3 Doors Down vocalist Brad Arnold struck up quite the friendship during the shoot, which also led to a solid, albeit short-lived, business relationship.\nAt a Nationwide Series race in August 2003, Stewart drove a 3 Doors Down-sponsored No. 8 Chevy for Dale Jr.’s now-defunct racing team Chance 2 Motorsports. The car featured 3 Doors Down members on the hood and Arnold on the rear fenders.","To be a NASCAR legend, you typically need to be a top-notch driver, a master in the pit box, or a visionary team leader.\nSam Bass charted his path to stardom in another way, using colors, creativity and a passion for racing to design unforgettable car paint schemes, souvenir program covers and race trophies across decades.\nThe legendary artist, who died in February 2019 at age 57 after a courageous fight against kidney failure, became a fixture at Nashville Superspeedway when NASCAR first brought top-level racing to the Middle Tennessee track in 2001.\nSam helped design the famous Gibson guitars that Nashville champions received in Victory Lane and also created scores of memorable souvenir program covers that collectors still covet today.\nWith NASCAR’s return to Nashville Superspeedway, Sam’s legacy remains firmly entrenched at the track, with tributes in the Infield Media Center and a Gibson guitar still a part of the Victory Lane tradition for Nashville’s race winners.\n“Sam would be honored, and our family is proud, to have his name still associated with Nashville Superspeedway,” said Denise Bass, Sam’s wife of more than 37 years. “Sam was a people person and a great storyteller. He loved sharing the memorable experience of presenting the Nashville guitar trophy to each winner-and the fans remember.”\nSam’s journey began in Hopewell, Virginia (about 20 miles south of Richmond) where he was born in November 1961.\nIt didn’t take long for the love of his life to enter his world, as he met Denise when they were both in the seventh grade. They married in November 1981 and built a family. Today their daughter is 20 and son is 17.\nOther than his family, Sam had two overriding passions – art and music.\nRaised on country music, Sam was the lead guitarist for a band during his junior high and high school days. While art ultimately won out as a career choice, Sam continued to make music an important part of his life.\n“He was always happy with his decision, but motivated to bring all of his interests together to reflect in his work -NASCAR, music, pop culture, nostalgia,” Denise remembers. “That is what made his compositions so unique.”\nIt didn’t take long for Sam’s artistic talents to catch the eyes of the racing world.\nSoon, Sam was designing programs and trophies for nearby tracks such as Richmond International Raceway and Charlotte Motor Speedway as well as paint schemes for champion drivers, including Bobby Allison, Dale Earnhardt, Harry Gant, Tony Stewart and Rusty Wallace.\nTwo of his all-time favorite schemes are certainly well-known to NASCAR fans:\n• The famed Rainbow Warrior car for Jeff Gordon who visited Victory Lane countless times with the colorful No. 24 Chevrolet as he dominated racing in the 1990s.\n•A design nicknamed “Flashpoint” –a yellow and orange combination that Dale Earnhardt Jr. sported during one of his final full-time seasons.\n“He wanted to influence the look of the sport as much as he wanted to capture its feel and history,” Denise said. “Paint schemes became a dominant part of his portfolio and produced the looks for generations of drivers as one crossed over to the next.”\nBy the time America’s best drivers reached Nashville Superspeedway in 2001, Bass was already the first “Officially Licensed Artist of NASCAR.”\nAn earlier tour of the Gibson Guitars plant in Nashville led to several friendships the family maintains to this today. His association with Gibson also led him to create limited edition guitars featuring Dale Earnhardt, Dale Earnhardt Jr. and Richard Petty and also made him a natural choice to help design the Superspeedway’s signature trophy and souvenir program covers.\n“The Nashville guitar trophy became one of the most sought-after trophies on the Truck and Xfinity circuits,” Denise said. “The limited-edition driver guitars and especially the Nashville guitars paved the way for NASCAR banquet trophies and Daytona pre-race presentations. Having designed hundreds of guitars during his career, Sam’s desire was to create one that provided a broader canvas.”\nThrough these successes, Sam’s life was not without challenges. At age 27, Sam was diagnosed with Type-1 diabetes, which eventually led to his fatal illness. Sam was on the donor list to receive a kidney transplant when he died and he spent much time and effort on raising awareness of kidney disease and diabetes. When he passed, the tributes came pouring in from the racing community.\n“There’s not many people that you meet in your life that are so happy to see you every time they see you, and he was that way,” recalled Dale Earnhardt Jr. “He set such a great example for all of us on how to treat people and how to maintain relationships. He just seemed so grateful for everything that ever happened to him. I hope that he’s celebrated because he meant a lot to this sport.”\nNASCAR also released a heartfelt statement: “Though he may have never turned a lap or a wrench, few captured the essence of our sport through his work more than Sam Bass. He was a consistent presence in the NASCAR garage, and his ever-present smile and endearing personality welcomed all. Though we have lost a member of the NASCAR family, his legend will continue in his art –all of which illustrated the greatness of our sport and the talent of a true friend.”\nRecognizing and celebrating the past while pointing toward the future has been a key emphasis of Nashville Superspeedway President Erik Moses since he took the reins of the venue last summer.\n“One of our priorities since beginning the work of revitalizing the Nashville Superspeedway has been to determine how to appropriately honor Sam Bass’ memory and his influence on the sport,” Moses said. “As Denise and I continued to discuss Sam’s legacy and what our track meant to him, it became clear that the tribute had to be permanent and not simply a one-off recognition. We look forward to presenting that tribute to our fans at the Superspeedway for years to come.”\nOn Saturday, June 19, Sam’s son presented Moses with a custom guitar featuring Sam’s logo that will be permanently displayed in the Superspeedway’s Media Center.\n“Those who were there then have pressed for the tradition to continue in some way and they have shared the history of it, through social media, with a new generation of fans eagerly awaiting Nashville’s return to NASCAR racing through its inaugural Cup event,” Denise said.\n“Working with Erik and the Superspeedway team these last several months and looking back on Sam’s career and accomplishments has brought a lot of wonderful memories to the forefront for our family. We are sincerely grateful to them for their thoughtfulness and appreciation of the impact and talents Sam had and gave completely to the sport he loved so much.\n“I believe it’s natural for an artist to have so many creative ideas in his or her mind that it’s impossible to execute them all in one lifetime.\n“I believe they all leave something on the drawing table at the end. I would like to think that Sam left a concept that would have highlighted this inaugural event while reflecting the rich history of the past.”\nWhile Nashville Superspeedway’s Victory Lane gatherings will miss Sam’s joyous presence, his legacy will be ever present at the track."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:dc21c0e6-4a53-4e48-b7f8-db2d68fb1602>","<urn:uuid:d85c4c55-8e31-4e79-a010-b403130bea37>"],"error":null}
{"question":"I work in international trade. What are the key differences between Letters of Credit (LC) and Bank Guarantees (BG), and what are their respective processing times?","answer":"Letters of Credit (LC) and Bank Guarantees (BG) differ in several ways: In an LC, the seller gets payment guarantee from the buyer's bank for sale of goods, while in a BG, the beneficiary is paid when contractual obligations are not fulfilled. LCs require the bank to check shipping documents and ensure strict compliance with LC terms before payment, while BGs are more straightforward. In terms of processing time, Letters of Guarantee (BG) have less processing time compared to Letters of Credit, making them more efficient for business transactions.","context":["- What is financial bank guarantee?\n- What is a financial guarantee letter?\n- What is the process of bank guarantee?\n- What does LC mean in banking?\n- What is BG on bank statement?\n- What is the difference between LC & BG?\n- How does banker’s guarantee work?\n- What is BG limit?\n- What is the LC and what does it do?\n- What are the guarantees?\n- What is non financial guarantee?\n- What is difference between LC and BG?\n- What is BG commission?\n- What does devolvement of LC mean?\n- How does a Letter of Guarantee Work?\nWhat is financial bank guarantee?\nA financial guarantee is given to related parties if one company takes on the financial obligation of another company.\nA bank guarantee is a bank’s promise that liabilities of a debtor will be met if he does not fulfi l contractual obligations..\nWhat is a financial guarantee letter?\nA ‘Financial Guarantee Letter’ is an official declaration from your sponsor, on their letterhead, that they will sponsor you for study at Griffith University. The financial guarantee letter must include: sponsoring organisation’s name and contact details.\nWhat is the process of bank guarantee?\nTo request a guarantee, the account holder contacts the bank and fills out an application that identifies the amount of and reasons for the guarantee. Typical applications stipulate a specific period of time for which the guarantee should be valid, any special conditions for payment and details about the beneficiary.\nWhat does LC mean in banking?\nletter of creditA letter of credit, or “credit letter” is a letter from a bank guaranteeing that a buyer’s payment to a seller will be received on time and for the correct amount. In the event that the buyer is unable to make a payment on the purchase, the bank will be required to cover the full or remaining amount of the purchase.\nWhat is BG on bank statement?\nA bank guarantee is a type of financial backstop offered by a lending institution. The bank guarantee means that the lender will ensure that the liabilities of a debtor will be met. In other words, if the debtor fails to settle a debt, the bank will cover it.\nWhat is the difference between LC & BG?\nA bank guarantee is a promise from a lending institution that ensures the bank will step up if a debtor can’t cover a debt. Letters of credit are also financial promises on behalf of one party in a transaction and are especially significant in international trade.\nHow does banker’s guarantee work?\nA Banker’s Guarantee (BG) is essentially a guarantee from a bank, on behalf of a company, to fulfill payment or obligations of a contract to their BG beneficiary. … When the contract is fulfilled or payment made in full, the funds placed with the bank by the SME are released back to the firm.\nWhat is BG limit?\nCompany A, the beneficiary, requires company B, the applicant, to get a BG from its bank as a condition of beginning work. The bank is the issuer, and in this case, would have to pay for the project to be completed if company B fails to do so. The limit is the maximum amount of the BG.\nWhat is the LC and what does it do?\nWhat is the locus coeruleus and what does it do? … It is now known that the LC is the primary site of norepinephrine production in the brain. The nucleus sends norepinephrine throughout the cerebral cortex as well as to a variety of other structures including the amygdala, hippocampus, cerebellum, and spinal cord.\nWhat are the guarantees?\nA guarantee is a legal promise made by a third party (guarantor) to cover a borrower’s debt or other types of liability in case of the borrower’s default. The time a default happens varies, depending on the terms agreed upon by the creditor and the borrower. … Loans guaranteed by a third party are called guaranteed loans …\nWhat is non financial guarantee?\nNon-financial guarantees means that the banks meet the applicant’s request, issuing for the beneficiary a written guarantee committing to pay an amount money to the beneficiary in case that the applicant does not fulfill certain non-financial types of transactions under the responsibility or obligation.\nWhat is difference between LC and BG?\nUnder an LC, the seller gets guarantee on payment of his sale of goods from the buyer’s bank. … However, in a bank guarantee, the beneficiary is paid on non fulfillment of obligation as per contract of BG.\nWhat is BG commission?\nBased on the type of the BG, fees are generally charged on a quarterly basis on the BG value of 0.75% or 0.50% during the BG validity period. Apart from this, the bank may also charge the application processing fee, documentation fee, and handling fee.\nWhat does devolvement of LC mean?\nThe LC is opened, goods are shipped, goods are received, usance given and on due date payment is made by the buyer. … On the due date, if the borrower does not arrange funds for payment, as a banker you normally give some grace time say 3 days and even then if the payment is not made, the LC is said to be devolved.\nHow does a Letter of Guarantee Work?\nA letter of guarantee is a document issued by your bank that ensures your supplier gets paid for the goods or services it provides to your company, in the event that your company itself can’t pay. In that case, your bank will pay your supplier up to a specified amount.","11 Sep Trade Finance Products | What are the Advantages and Disadvantages?\nAt EIMF we constantly expand our topics for professional training whilst continuing to work closely with industry experts who can offer sound practical workshops. One of our new associates this season is Demetris Zacharoudes who will present for us, both in Nicosia and Limassol, a very exciting workshop on Trade Finance . Demetris also shares with us some of his know-how through this very interesting article:\nHow do the main Trade Finance products work; their advantages and disadvantages.\nTrade Finance products are provided by Banks and their aim is to reduce the payment risk to the Exporter by reducing the Counterparty Risk (i.e. risk of default of the Importer) and the Country Risk (i.e. risk of default of the Importer’s country and/or the imposition of capital controls). The main Trade Finance products are Bills for Collection, Letter of Guarantee and Letters of Credit. But how do these products work and what are their advantages and disadvantages to Importers and Exporters?\nBills for Collection\nUnder Bills for Collection the Exporter ships goods, collects all shipping documents and presents same to their bank with instructions to remit documents to the Importer’s Bank. According to the Exporters instructions, the Importer’s bank releases the shipping documents to the Importer ONLY against the Importer’s (a) Sight payment, or (b) Acceptance (i.e. promise to pay at a future date). The bank does not check the documents and does not provide its undertaking for payment to the Exporter (unless the Exporter requests for a guarantee/avalization)\nAdvantages to the Importer: Provides comfort that goods have already been shipped and is less costly than Letters of Credit or Letter of Guarantee in terms of banking fees.\nAdvantages to the Exporter: Provides comfort that, in case of Sight payment, the Importer will not be able to obtain the documents for customs clearance and take possession of the goods unless payment is affected. In case of Acceptance, the Exporter will have the Importers promise in writing that they will pay for the goods at a future date; hence in case of non payment, the possibility of success in possible court proceedings will be on the Exporter’s side.\nLetters of Credit\nLetters of Credit are issued by banks under the Importer’s instructions. This is an irrevocable undertaking of the bank to pay the Exporter provided the latter presents the shipping documents as per the terms of the Letter of Credit. After receipt of the Letter of Credit the Exporter ships goods and presents the shipping documents to the Importer’s bank. The Importer’s bank checks the documents and, if they comply, the latter will proceed with the payment to the Exporter or promise to pay at future date (as per the terms of the Letter of Credit) and release documents to the Importer in order clear goods from customs.\nAdvantages to the Exporter: Maximum protection for the Exporter\nAdvantages to the Importer: Comfort that the Importer will be able to clear goods from customs as the payment depends on strict compliance of the documents to the LC terms\nDisadvantages: Maximum banking fees as the bank guarantees the payment and also checks the documents\nLetters of Guarantee\nThe Importer’s bank first issues a Letter of Guarantee under which the bank guarantees the payment of the goods to the Exporter. The Exporter ships goods and remits the shipping documents directly to the Importer which clears the goods from customs and pays the Exporter directly through a SWIFT or SEPA payment. If the Importer defaults the Exporter will claim payment from the Importer’s bank. A Letter of Guarantee may cover a single or multiple shipments.\nAdvantages to the Exporter: Maximum protection as the Exporter has the bank’s undertaking for payment of the goods\nAdvantages to both parties: Less processing time than Bills for Collection or Letters of Credit\nDisadvantages to the Importer: More banking fees than Bills for Collection\nIf you are interested in learning more about Trade Finance view our upcoming courses."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:05afee0f-e29e-48cd-a3a0-373ed7e50e03>","<urn:uuid:2dae48bf-6c00-4b8b-ac98-4d4d9bffb36c>"],"error":null}
{"question":"What's the relationship between plate discipline metrics and count-specific outcomes for fastballs?","answer":"Z-Swing percentage (swings at pitches in the strike zone) shows minimal correlation with performance metrics, with the strongest being just -0.150 with BABIP. For fastballs, the outcomes vary significantly by count - batters achieve their highest slugging percentage per swing in 3-0 counts at 0.381, while performing worst in 0-2 counts with 0.163 slugging percentage per swing.","context":["As many of you now know, last week we unveiled some tremendous new metrics. Available on individual player pages as well as the leaderboards, you now have access to plate discipline metrics for pitchers and pitch type statistics for hitters. The former includes information along the lines of how often a pitcher induced a swing out of the zone, in the zone, as well as his percentage of first-pitch strikes. The latter includes the percentages, and velocities, of pitches seen for hitters, as well as his percentage of first-pitch strikes seen.\nI wrote a bit of an introduction to these new statistics last week, and David has written several glossary-type entries as well. This is the type of information that has piqued my interest for a long, long time, and it now adds another dimension to evaluations. For instance, did you know that Johan Santana posted an O-Swing % (percentage of pitches out of the zone that batters swung at) of 30.1 in 2005 and 2006, which decreased to 28.2% in 2007, and 26.8% this past season?\nUsing the new statistics, I decided to run some correlations to see if certain statistics held strong relationships to each other. First, here are the results for correlations run between the percentage of first-pitch strikes and six prominent evaluative statistics:\nK/9: 0.194 BB/9: -0.719 WHIP: -0.515 BABIP: 0.096 ERA: -0.31 FIP: -0.406\nThe results here are not that shocking, or at least they should not be. Getting ahead of the hitter is generally considered key for the pitcher. Doing so, in theory, should correlate quite strongly to any metric involving walks. As we can see, there is a very strong relationship between the percentage of first-pitch strikes and the walks per nine innings issued by pitchers. The relationship loses a bit of its strength when hits allowed are added to the equation in the form of WHIP, but the -0.719 correlation between F-Strike% and BB/9 is actually the strongest of any that I ran. Here are the results for O-Swing % and the same six evaluative metrics:\nK/9: 0.281 BB/9: -0.493 WHIP: -0.462 BABIP: 0.036 ERA: -0.362 FIP: -0.428\nHere, the results are a bit different. Nothing is incredibly strong or on the same wavelength of strength as the FStrike-BB/9, but we have a few relationships of moderate strength. What exactly is O-Swing? It is the percentage of pitches that a pitcher threw out of the zone, that a hitter swung at. With this in mind, we might initially expect that pitchers with the highest percentages in this area would strike more batters out, walk less, and therefore be very effective in the ERA and FIP department. One thing to keep in mind, though, is the percentage of pitches that these pitchers throw in and out of the zone.\nJake Peavy and Barry Zito, for instance, were amongst the bottom in terms of percentage of pitches thrown in the zone, at around 47%. However, Peavy induced many more swings on these pitches than Zito, which is a big reason for the difference between the two, since their percentages of pitches in and out of the zone were virtually identical. When we have pitchers with different percentages in the mix, as is the case in the correlations using O-Swing, the results should not be as concrete. Overall, the strongest relationship here also involves BB/9, as the idea goes back to the Peavy/Zito example: Peavy gets swings and outs on pitches out of the zone, Zito does not. The higher the percentage is of swings out of the zone, the better the chance is that the BB/9 will be lower.\nLastly, Z-Swing%, which is still a bit curious. For instance, does a pitcher want a higher or lower percentage here? I would venture a guess that a lower percentage would be better, as the pitch is already in the zone and therefore very likely to be called a strike. A hitter failing to swing will take a called strike. It probably is not as important as FStrike or O-Swing, but here are the correlations:\nK/9: -0.067 BB/9: -0.014 WHIP: -0.037 BABIP: -0.150 ERA: -0.027 FIP: 0.087\nWell, I guess it really doesn’t matter for pitchers, as the percentage of swings induced on pitches in the strike zone does not share anything close to a strong relationship with any of the above six metrics. Interestingly enough, the highest correlation for Z-Swing involved BABIP, which was the lowest for F-Strike and O-Swing. The -0.150 isn’t significant by any means, though, so nothing should be taken away by that. At the very least, these results show what we would generally expect: the more first-pitch strikes, the lower the rate of walks or vice versa, and inducing swings out of the zone can result in better rate and run prevention stats.","Dirty Jobs: part 2\nLast week I looked at how pitchers approached each count, based on the amount of fastballs thrown and where they were thrown. Today I'm going to wrap up the topic, looking at what generally happens after the pitcher releases the ball and the hitter has to make a decision.\nThe most basic decision a hitter has to make at the plate (after determining what pitch is coming) is whether to swing or not so the next facet of each count I looked at was how often hitters took a pitch in each count. To remain consistent with the other results I've found, I only looked at fastballs and the table below shows how often fastballs were taken in each count, along with how often the pitch was either a ball or a strike. The most obvious thing is how often 3&0 fastballs are taken, especially for strikes. I realize there are a lot of good explanations/reasons for this behavior, but it seems that hitters are sacrificing a huge opportunity by taking so many pitches in these situations. A 3&1 count is still a hitter's count, so the actual loss of the strike doesn't hurt the batter too much, but they are ceding one their most potentially productive counts by showing pitchers they rarely swing in in it. A generic 3&0 pitch is a strike only 60% of the time, compared with the average across all counts of 63%, but that's not nearly enough of a difference to justify taking 93% of pitches.\nCount Take% Called Strike% Ball% Called Strike/Ball Ratio 3&0 93% 59% 33% 1.77 0&0 71% 32% 40% .81 2&0 59% 28% 31% .90 1&0 57% 24% 33% .71 0&1 54% 12% 42% .28 0&2 53% 5% 48% .09 1&1 47% 12% 35% .33 3&1 45% 17% 28% .60 1&2 43% 5% 38% .14 2&1 40% 11% 30% .36 2&2 35% 5% 30% .18 3&2 25% 4% 21% .20\nIf the batter is able recognize a 3&0 pitch as a fastball out of the pitcher's hand he's at even more of an advantage. 3&0 fastballs are strikes 67% of the time, which is higher than the average for fastballs among all counts (64%) and when batters do swing at 3&0 fastballs, they are very successful, posting the highest Slugging Percentage by swings (TB/Total Swings) for any count. I would think that success would encourage more swinging on 3&0, but it apparently doesn't. I know that I'm making this sound overly simplistic, and there are certainly valid reasons why different hitters might not swing at a 3&0 fastball, (among others, they could be looking for a specific pitch or a specific location), but I think there's an element of risk-aversion on the part of the batter to avoid \"wasting\" a 3&0 count and making a visible out right then.\nI'm not sure how much more I'm advocating swinging at 3&0 fastballs, but if the whole point of a hitter's count is to force the pitcher into throwing more fastballs, then taking almost all of those fastballs can't be a good decision, especially when the pitch is nearly twice as likely to be a strike than a ball. Taking the pitch might not be as big of a problem as I'm making it out to be because even though a 3&1 count is a (slightly) worse hitter's count than 3&0, in terms of seeing fastballs, the two counts are very similar. This leads to the question, in which count is it worst to take a strike in? The table below has the FB% for each count, along with the FB% for the count that results from taking an additional strike and the difference between the two. Obviously it's suicide to take a called third strike, so those bottom four counts aren't very interesting, What is interesting is the top of the chart. Taking a 3&0 strike leaves the batter in roughly the same position he started in, at least in terms of possibly seeing a fastball. The lack of a \"penalty\" for taking a strike combined with the potential of getting a walk might contribute to the higher than normal take-rates in 3&0. The similarity in terms of seeing fastballs between 0&1 and 0&2 further emphasizes how important first pitch strikes are for a pitcher. 0&2 is obviously a better pitcher's count because the batter has a smaller margin for error, but in terms of fastball selection, once that first strike happens, the batter has a huge hole to dig out of.\nCount FB% FB%-Called Diff. 0&1 48% 47% 0.00 3&0 78% 76% -0.02 1&1 49% 44% -0.05 1&0 59% 49% -0.10 2&0 70% 59% -0.10 0&0 59% 48% -0.11 2&1 59% 47% -0.13 3&1 76% 61% -0.14 1&2 44% 0% -0.44 2&2 47% 0% -0.47 0&2 47% 0% -0.47 3&2 61% 0% -0.61\nGoing back to the first table for a second, another interesting element is how the frequencies of taking a fastball for a called strike organize the counts based on the number of strikes a hitter has. When hitters have two strikes, regardless of the number of balls he has, there is only about a 5% chance of him looking at strike three. When he has one strike, there is about a 12% chance of taking strike two and with zero strikes and zero, one or two balls, a there is about a 28% chance of the batter taking strike one, but in a 3&0 count, that percent nearly doubles to 59%.\nI mentioned that batters had the best results in 3&0 counts, and I based that on the slugging percentage per swing in each count. This is very similar to slugging percentage for balls in play, except swinging strikes and foul balls are added to the denominator. This is a more granular metric than anything else I've seen and measures the value of a swing. To give a feel for the size of these values, the league average (for all types of pitches) is .273, Alex Rodriguez led the league at .324 and among non pitchers, Jason LaRue was last, posting a .114. .270 and above is a pretty good performance, while below .180 is poor. (These are different than the values I posted on Saturday which were slightly off). This isn't a measure of the absolute value a player, but measures the value of one swing of his bat, something like his skill for recognizing which pitches to swing at and then hitting those pitches hard. The table below shows the SLGSWING in each count (for fastballs), and the rankings of the counts is very similar to how they've been ranked with other metrics.\nCount SLGSWING 3&0 0.381 3&1 0.333 2&0 0.298 2&1 0.267 3&2 0.256 1&0 0.247 0&0 0.233 2&2 0.205 1&1 0.202 0&1 0.192 1&2 0.190 0&2 0.163\nThe original question that prompted this article asked about classifying 2&2 and 0&1 counts and the way hitters and pitchers approached each count. I would call both counts pitcher's counts but in an 0&1 count, the fewer strikes gives hitters a much bigger margin for error and allows them to be relatively selective about which pitch they swing at. However, an 0&1 count also allows pitchers to be less concerned with forcing a strike than they are in a 2&2 count. 0&1 has some advantages for both batters and pitchers, although the pitcher's advantage is dominant. In a 2&2 count, the batter and pitcher are under different pressures. A batter can't afford to be very selective because he only has one strike left, but a pitcher doesn't want to throw a ball and go to 3&2. The batter is again in a worse spot, making it a pitcher's count, but if 0&1 is a count where both the batter and pitcher are under pressure to maximize their advantage, in a 2&2 count it seems like both players are under pressure not to screw up."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:c9087128-5025-4e92-98ad-d2a71681748c>","<urn:uuid:7ff23b32-f9e1-4a83-a45e-8ed46b02ad48>"],"error":null}
{"question":"What are the key material properties needed for wind turbine blades, and what maintenance challenges do these blades face as they age?","answer":"Wind turbine blades require materials with specific properties: high specific stiffness to limit tip deflections and reduce gravity-induced loading, excellent fatigue resistance to minimize degradation, and they must provide long-term mechanical performance at moderate cost. Fiber-reinforced plastics (FRPs) are a prime material choice meeting these requirements. However, as turbines age, they face increasing maintenance challenges. Blades commonly develop issues such as extensive surface coat cracking, flaking, erosion, chord-wise and span-wise cracking in the max chord and transition area, leading edge erosion, and interior defects. These issues lead to higher operation and maintenance costs than during early years of operation, often exceeding initial projections. Without proper inspection and maintenance, on-site repair becomes increasingly costly and impractical, potentially requiring solutions like re-blading or full refurbishment.","context":["Advances in wind turbine blade design and materials offers a comprehensive review of the recent advances and challenges encountered in wind turbine blade materials and design, and will provide an invaluable reference for researchers and innovators in the field of wind energy production, including materials scientists and engineers, wind turbine. Fibre-reinforced plastics represent a prime material choice for wind turbine blades in terms of structural efficiency since the high specific stiffness limits tip deflections, reduces gravity-induced loading and decreases rotor inertia furthermore, the excellent fatigue resistance of frps helps to minimise material degradation and maintenance. Suggested citation:5 manufacturing processes for rotor bladesnational research council 1991 assessment of research needs for wind turbine rotor materials technologywashington, dc: the national academies press doi: 1017226/1824. Wind turbine rotor blades are a high-technology product that must be produced at moderate cost for the resulting energy to be competitive in price this means that the basic materials must provide a lot of long-term mechanical performance per unit cost and that they must be efficiently manufactured. Title: carbon fibre – challenges and benefits for use in wind turbine blade design author: chris monk created date: 10/9/2014 9:47:27 am. Materials used to manufacture rotor blades for wind turbines are subject to special requirements the number of load cycles and the load variability are far beyond what is encountered by other structures in aviation, shipbuilding and bridge building. This chapter deals with loads on wind turbine blades it describes the load generating process, wind fields, and the concepts of stresses and strains aerodynam. Materials review materials for wind turbine blades: an overview leon mishnaevsky jr id, kim branner, helga nørgaard petersen, justine beauson, malcolm mcgugan and bent f sørensen department of wind energy, technical university of denmark, 4000 roskilde, denmark [email protected] (kb.\nMaterials issues for turbines for operation in ultra-supercritical steam ig wrighta, pj maziasza, fv ellisb, tb gibbonsc, and da woodfordd abstract coal-fired supercritical-steam power plants are currently operating with steam temperatures at the inlet to. Each of the turbine sections such as nozzles, blades, turbine discs etc presents a range of materials and design issues for current and future turbines that are dependent on their size, operation and duty cycle imposed. Turbine blade mechanical properties: gas turbines since there birth have always been limited in thrust and efficiency due to the availability of materials that can be used in the engine gas turbines run most at extremely high temperatures and with high centrifugal pressures therefore the materials required to manufacture turbines must. Blades with a specific focus on blade aerodynamics, materials used in the production of steam turbine blades, and the factors that cause turbine blade failure and.\nBuy advances in wind turbine blade design and materials (woodhead publishing series in energy): read books reviews - amazoncom. The parts of a jet engine—they can number 25,000—are made in various ways the fan blade is made by shaping molten titanium in a hot press. Making lighter aircraft engines with titanium aluminide blades the current state of net shape casting titanium aluminide is a new material that will lead to much lighter aircraft engines, and thus lighter aircraft net shape precision casting is the latest development in the ancient technique of casting, and is used to mass-produce hundreds of turbine blades. Materials used in gas turbine blades and so on • modern gas turbines have the most advanced and sophisticated technology in all aspects vibration oxidation effects hot on the blades interdiffusion thermal corrosion fatigue aircraft severe moderate severe severe power generator moderate severe moderate light marine.\nSome information on windmill blades, materials and options aluminium extruded blades these aluminium blades are long lasting and use the proven geo222 profile suit wind turbines in the range of 200 watts to 2kw understanding swept area a look at how the swept area, and therefore the power, of a turbine changes with diameter. This paper presents a critical review of the existing literature of gas turbine materials the paper will focus light on above issues and each plays an important role within the gas turbine material literature and ultimately influences on planning and development practices it is expected that this comprehensive contribution will be very beneficial to. The university institute of ceramic technology agustín escardino of the universidad jaume i in castellón, spain, is participating in the development of new materials that are resistant to extreme climate for the manufacture of wind turbines the aeroextreme project, led by company siemens gamesa, will be operational until the end of 2018.\nAn example of a thermoplastic material, which can be used in the production of wind turbine blades, is called pa-6 (this material is currently being researched by delft university) thermoplastic materials have a higher resistance to abrasion and fatigue (higher toughness) than most thermoset materials. The choice of materials for turbines kaplan turbine spiral casing runner guide vanes stay vanes draft tube cone draft tube cone runner lower cover upper cover turbine shaft spiral casing governing ring guide stay vanes vanes stay ring dajiang power plant, china power: 129 mw head: 18,6 m flow rate: 825 m3/s kaplan turbines spiral casing materials. Ichpsd-2015 114 turbine blade materials used for the power plants exposed to high silt erosion – a review m padhy iter, s’o’a university.\nAnalysis of turbine blade made of composite materials used in steam turbines vishnu vipin department of mechanical engineering federal institute of. Generator, with either four or six poles gearless wind turbines with multi-pole generators are increasingly replacing the gearbox-equipped machines there are numerous other possibilities, however wood epoxy is an alternative blade material and some machines have two blades. Materials we are specialized in the forging and manufacturing of turbine blades in a wide range of materials, including stainless steel, titanium and nickel alloys our materials are bought only from certified suppliers working in the power generation and oil & gas markets and in compliance with the customer specifications and the most.\nDistribution category uc-261 sand92-7005 unlimited release printed august 1992 fatigue of fiberglass wind turbine blade materials jf mandell, rm reed, dd samborsky. Study of materials used in gas turbine engine and swirler in combustion chamber material used in gas turbine blades: turbine blades of a gas turbine engine are very prone to damage from flying debris, moreover it also sustain thermal stresses and local overheating on its surface of a gases that are coming out from combustion chambers so use. Gas turbine materials- current status and its developmental prospects-a critical review adinesh kumar1, ssathyanarayanan1, sayantan datta gupta1, drmnageswara. In the longer term, there is scope for improving the materials used in wind turbine blades as indicated previously, wind turbine blades need to be strong, stiff and light two particular weaknesses of laminated fiber-reinforced composites are their low tensile and shear strength in the out-of-plane direction and the fact that developments that.","As today’s operating wind turbine assets age, owners and operators are being straddled with higher operation and maintenance costs than those experienced during the early years of operations; in many cases, more than those assumed within the pro forma. These cost increases are particularly common for wind turbine rotor blades. Even many of those operators who have signed long-term maintenance and service contracts will discover they aren’t immune to these rising costs.\nMost owners and operators do not perform full-scale rotor blade inspections on the operating fleet after end-of-warranty, and are typically only carried out after the field technicians start to report increased signs of damage or when higher associated rotor blade maintenance costs dictate that the inspections be performed to get a better understanding of the conditions of the rotor blades on-site. Once the rotor blades reach this condition, on-site/on-tower repair becomes much more costly and impractical, and other options must be considered. These are commonly:\nA. Allow the turbines to operate with minimal maintenance and plan for near-term re-powering of the site\nB. Re-blading the wind turbines with newer blades of the same or greater length\nC. Refurbishment of rotor blades\nD. Some hybrid solution of b and c\nMost owner/operators do not have the in-house rotor blade knowledge required to perform the comprehensive assessment of their rotor blades in order to understand their current operating condition. For this reason, many third party independent service providers (ISP’s) are brought in to perform these inspections and aid in the decision-making process for addressing issues with the rotor blades. Provided that the condition of the rotor blades is not so deteriorated that options “A” or “B” are the only feasible solutions and on-site repair is no longer an economical option, most companies prefer option “D”.\nOption “D” is generally used due to the ramp-up time associated with the refurbishment process, as most spare rotor blade sets are not normally readily available on-site. At minimum, a lag-time of 4–6 weeks can be expected from the time the rotor blades are removed from the turbine until the refurbishment is completed and the blades are ready to be re-installed. Subsequent rotor blade sets can be expected to be available at much lower lag-times. In order to minimize this downtime cost, new rotor blade sets are purchased, when possible; when not possible, older rotor blades that can be matched into a set are sought and purchased. As is most often the case, these older rotor blades require some degree of refurbishment before being installed, so in many instances these rotor blades are the first to go through the refurbishment process.\nWhat is involved in the refurbishment process?\nFrom the inspection findings, a detailed understanding of the wind farm rotor blades condition is known. When the discovered damage includes defects such as extensive surface coat cracking; flaking; erosion; chord-wise and span-wise cracking in the max chord and transition area; leading edge (LE) erosion; and interior defects affecting a high percentage of the rotor blade population; an off-site refurbishment plan is usually found to be the most cost effective solution—if repair is even economically feasible. In order for an owner and/or operator to proceed with this process, a number of steps need to be taken and items considered:\nStep 1: Project Tender\nIf the work is to be performed by third party contractors, the work should be tendered to multiple potential contractors. Within the tender, a detailed scope of the work to be performed provided with the request that a fixed price for refurbishment be provided. For this reason, most owners and operators work diligently—with outside consultants when required—to establish which of the common defects require repair; and ensure that these defects are repaired under the standard scope of repair. Time and materials rates are also requested for defects requiring repair that are out of the original scope of refurbishment definition. Figure 1\nEstablishing, before submitting the request for tender, which rotor blade repairs need to be included as part of the fixed set refurbishment cost is critical to obtaining a higher level of cost certainty for the project. However, careful consideration must be given to which repairs are and are not critical, as this can greatly influence the overall set refurbishment cost. When replacement rotor blades are available, refurbishment costs above 40–60 percent of new rotor blades costs are generally considered as the upper limit. At a minimum, the following classes of defects require repair:\n1. Interior and exterior structural defects that will have an effect on the safe operation of the rotor blade during the expected operating life\n2. Lack of continuity in lightning protection system down conductor\n3. Blocked drain holes\n4. Missing and damaged aerodynamic elements\nAdditional repairs will influence the overall set refurbishment cost. However, careful consideration must also be given to the following:\n1. The condition of the LE of the rotor blade commonly deteriorates. Based on an assessment of the current condition at the time of refurbishment and the local site conditions, the application of a supplementary LE protective coating is typically performed.\n2. A number of interior and exterior structural defects will be discovered during inspection following operation of the rotor blades. Not all of these defects will have an effect on the safe and continued operation of the rotor blades within the rotor blade design life. Taking a conservative approach when selecting which defects require repair will increase the refurbishment cost, but will likely decrease the long-term operating costs.\n3. Re-application of the final surface coat following completion of repairs is recommended for three main reasons:\na. A very patchwork surface results from the refurbishment process. This will leave the final surface condition of the rotor blade very unaesthetic.\nb. Wear of the surface coating will occur as the rotor blades continue to operate. Re-coating will ensure areas of thin coating are repaired.\nc. A refurbished surface coating will facilitate defect discovery during subsequent rotor blade inspections.\nIf, as is commonly requested, the contractors request access to a sample of the rotor blade population prior to responding to the request for tender, this access should be granted. It can be expected that through this inspection, the individual contractors will gain a better understanding of the rotor blade condition, and be able to provide more accurate pricing. Figure 2\nIt is also important that, as a requirement of responding to the tender, the contractor is able to provide demonstrated knowledge of the specific rotor blade materials and design as well as the general practices within the rotor blade repair industry. If this information cannot be provided, the contractor must be able to describe the processes that will be employed to determine this information and ensure the integrity and continued safe operating ability of the refurbished rotor blades.\nStep 2: Selection of Contractor\nSelection of the contractor to perform the work cannot be awarded solely on a lowest-tendered-quote basis. Careful consideration must be given during this process and individual site visits to assess the individual contractor’s capacity to perform the required work should be performed. Figure 3\nDepending on the scale of the wind farm, the number of years planned for refurbishment and the capacity of the individual contractors responding the request for tender, multiple contractors may be required to complete the refurbishment project.\nStep 3: Development of Standard Repair Procedures\nIf possible, the development of standard repair procedures should be performed prior to commencing with the rotor blade refurbishment process. Individual procedures for all of the common defects specified within the request for tender are required. This is an arduous task, and the required development of 10–20 procedures or more, is not uncommon for the full scale refurbishment process. However, as this generally set-up as a milestone event in the process, the contractor has the required encouragement to perform and complete this task. Figure 4\nAccordingly, standard repair procedures contain at least the following information:\n1. Specific defect condition for which the standard repair procedure applies\n2. Acceptance limits for the individual defects before repair is required\n3. Materials to be utilized\n4. Allowable ambient environmental conditions\n5. Specific and detailed instructions for performing the repair\n6. Reference to quality control check and hold points\nStep 4: Quality Assurance\nA quality system designed to ensure that all inspections, repairs, and documentation are being performed as per project expectations; and where available, defined policy, is required before beginning with the refurbishment project. Once the quality system is implemented, it must be clear that at every stage of the refurbishment process a quality control plan is present and being followed. Insufficient quality control will lead to variability in the refurbished rotor blades end quality, potentially leading to increased operating costs. Periodic audits of this system should be considered as an integral part of the project due diligence. Figure 5\nStep 5: Performance of Standard and Out of Scope Repairs\nAll defects that have a standard repair procedure must be repaired following the prescribed procedure. This ensures repair and refurbished rotor blade final condition consistency. Any and all defects that are to be repaired according to a specific standard repair procedure that are not repaired as per the standard repair procedure should be recorded as a non-conformance.\nOut-of-scope repairs are for defects that require repair without a standard repair procedure. The sub-process for developing, approving, and performance of out of scope repairs must be decided prior to commencing with repair portion of the refurbishment process. As these repairs represent a cost increase to the fixed rotor blade set refurbishment cost, the scope of the repair must be clearly defined so that cost and quality assurance can be tracked and maintained.\nStep 6: Documentation\nAs part of the delivery package for each rotor blade set, the following documentation, at minimum, should be requested and provided:\n1. Interior and exterior inspection findings and defect disposition\n2. Repairs performed and procedures used\n3. Rotor Blade Set Balancing\na. Individual Rotor Blade Mass\n1. Inclusive of mass added and location added at to balance\nb. Centre of Gravity\nc. Individual Rotor Blade Static Moment\nStep 7: Rotor Blade Set Matching\nFollowing the refurbishment of the rotor blades, the deviation between the individual rotor blade static moments is almost always above the allowable limit, necessitating the need for mass addition to balance the rotor blade set. Establishment of what this acceptance limit is should be performed prior to commencing with the refurbishment process. In all instances, if an OEM specified imbalance limit is available, it should be utilized. In the event no OEM specified limit is available, technical rationale for the acceptance limit to be utilized is required. Figure 6\nAs part of the original rotor blade design, sealed compartment(s), which are accessible from the exterior of the blade, are bonded in the rotor blade to allow for the addition of mass in order to match the individual static moments within the rotor blade set. Unfortunately, the records for the mass added to these compartments during original manufacture are not commonly available. Additionally, in many cases these compartment(s) have already been filled to max capacity. For this reason, as part of the refurbishment process, if not already defined, limits must be established for the allowable mass quantity addition and location(s) for balancing rotor blade sets. Uncontrolled mass addition to the rotor blade may lead to the development of non-design operating characteristics, affecting the safe operating condition of the turbine.\nAlthough the rotor blades to be refurbished began as matched rotor blade sets, following refurbishment, all of these blades will not be able to be placed in their original matched sets, due to the mass addition limits defined above. For this reason, a minimum work in progress (WIP) of approximately 3 sets is recommended. With the increased number of blades in WIP, and the common practice of some wind turbine OEM’s to have rotor blades manufactured from different rotor blade OEM’s at the same wind farm, owners and operators must be diligent in ensuring that only like operating characteristic and aerodynamic profile rotor blades are matched and whenever possible, that rotor blades are matched in sets by manufacturer and mold.\nStep 8: Re-Installation on Turbine\nIncoming inspection of the rotor blades prior to re-installation on the rotor hub is a due diligence check that is required. Transportation damage is commonly found, and should be repaired at this time, as the ease of access to the rotor blades and repair will provide long-term cost savings.\nAdditional due diligence checks such as testing of rotor mass imbalance and aerodynamic imbalance should be considered. Failing to perform these due diligence checks may lead to greater than design loading in the rotor blades and through the wind turbine drive train and structure.\nStep 9: Development of Comprehensive and Thorough Long Term Maintenance Plan\nThrough the refurbishment process substantial costs will be incurred. In most cases, these costs could have been minimized through the development of a thorough and comprehensive long term maintenance and inspection program implemented during the original project commissioning. To avoid Einstein’s definition of insanity and repeating the original process and expecting a different end result, it is imperative that a program be developed. If an in-house model is not already available, owners and operators are encouraged to work with the contractor(s) performing the refurbishment and other industry experts to develop a cost effective program that remains sufficiently comprehensive."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:3a97fb0c-e8a7-4f82-b178-692aa2074e31>","<urn:uuid:a8f47322-aea4-4695-84d0-b9ee42d564a7>"],"error":null}
{"question":"For someone new to vehicle recreation, which has more startup costs - investing in a boat or an RV?","answer":"RVs generally have lower startup costs compared to boats. RV prices range from $10,000 to $300,000, with moderate campers around $20,000 and fifth-wheels around $40,000. While motorhomes start around $100,000 and require ongoing maintenance of about $117 monthly, boats require significantly more initial and ongoing investments. Boats need extensive equipment including navigation systems, communication devices, safety gear like EPIRBs and flares, and numerous mechanical components that must be regularly maintained or replaced like propellers, shaft seals, steering systems, and bottom paint. Additionally, boats require specialized maintenance tasks and often professional servicing that adds to the overall cost of ownership.","context":["Are you on the fence about buying an RV? That can be a good thing. Purchasing a recreational vehicle is a serious investment, so you should consider the potential downsides before making your purchase. Plus, we have experienced RV owners and travelers sharing their buying advice.\nPotential reasons to not buy an RV\nFirst of all, we aren’t saying RVs are bad. Recreational vehicles, motorhomes, campers, and more provide a wonderful way to travel across the country. But they aren’t for everyone, so if you’re on the fence, this information could help you make a final decision.\nAs Sonda Rochelle from Axles Addict shares, even under the appropriate circumstances, people should be prepared because problems can be more common than most people realize. You don’t want to end up as part of an RV horror story.\n1. RVs could be more expensive than expected\nRVs can cost between $10,000 to $300,000 depending on different styles and features. A moderately equipped camper that’s pulled behind a truck may only cost $20,000, and a fifth-wheel could be around $40,000. Also, you need a truck or SUV with enough power to pull it. Motorhome prices usually start around $100,000, according to the Camper Report.\nDepending on how you finance your RV, it could generate a high-interest rate. According to RV Guide, if you borrow $30,000 to purchase an RV with a 5 percent interest rate over five years, then you’ll have nearly $4,000 in interest costs over the lifespan of the loan.\nAfter you buy your RV, you have to pay to maintain the coach. Otherwise, it may quickly deteriorate and lose its value. The average cost of maintenance for RVs on a monthly basis due to upkeep and repairs is about $117 per month.\nRosanna T. Mitchell, the creator of A Pragmatic Lens shared, “We love having our camper, and it has been a life-saver this year with all the pandemic woes. That being said, there are two main things that potential buyers should know about.\n- RV ownership is not carefree. You are basically responsible for the maintenance of another tiny house. This is not only in relation to regular maintenance and damage repair, but also you will need to study and become acquainted with the ins-and-outs of your rig and be ready to troubleshoot on the spot. It is a learning curve, for sure.\n- It may not be as cost effective as expected. Most people would imagine that owning an RV will save money because airfare would not be an issue. Yet, even when camping in free sites, after adding up the purchasing costs, insurance, gas mileage, RV maintenance, repairs, and storage, it may actually result in a fairly expensive way to travel.\nAll in all, we don’t actually see RV travel as particularly convenient or cheap. There are far more convenient ways to explore, such as staying in furnished state park cabins, for example.\nSo why do we do it? Well, it is all about the experience and the memories we make in our tiny home on wheels.”\n2. RV travel is dangerous\nTruck drivers have to become officially trained and certified before they get behind the wheel of a semi. However, RV drivers can get in and just go. But driving or hauling a recreational vehicle is dangerous.\nThe extra weight and longer length of trailers can require a longer stopping time. You could face blind spots that block smaller vehicles from your vision. Driving is also exhausting, and driving tired is the equivalent of driving drunk.\nYou might have to park your RV in a crowded campground with children and other obstacles to avoid. Plus, you may face inclement weather and traffic jams.\nWhile we’re covering dangerous situations, it’s a good time to mention that allowing your children to freely move about your RV while in motion is illegal.\nAccidents may also occur while the RV is parked. People can trip out of the high entry points, items may shift during transit and fall when opening storage cabinets, RVs can catch fire, and more.\n3. RVs can be labor intensive\nYour work isn’t done when you buy a recreational vehicle. You have to maintain it and plan out your trips. You can’t just wing vacations and park anywhere. Plan a route ahead of time and secure parking spots. This way, you won’t be stuck searching for a place to pull over and sleep safely.\nPacking up your RV and preparing for trips can be a lot of work. Will you need snow gear? Do you need food? Does your dog need its crate? Do you have enough bedding to keep people warm?\nAre you prepared with a first aid kit? Some campgrounds are in remote areas that may not have instant access to what you need. On that note, they may be far away from medical facilities as well.\nPlus, the cleanup. Recreational vehicles make it easy to track in mud. Plus, it’s easy to spill food and drinks. You may need to sweep and wipe down counters daily.\nRemember that rodents may want to make their home in your RV, so set up deterrents. You may want to wash all the bedding at home and figure out how to get rid of musty smells after camping.\nAlso, you need to dump your family’s poop at the end of the trip. So, that might not smell pretty or go exactly as planned. It’s not like a hotel, where everything is taken care of for you. But if you can handle the hard work and potential risks, then your RV will be very rewarding.","Boat Safety Checklist For Summer\nBy Tom Neale\nGood spring prep increases safety and fun for the entire boating season. Follow these tips to help you build your own list\nClick here to download the printable version\nCheck things that might have deteriorated during winter, including exterior wiring connections for navigation lights, dock lines (chafing or freeze damage), life jackets (mold, rotten threads or fabric), paper charts, pipes including cockpit drains that were subject to ice damage, and hose clamps.\nMove the rudder. If it turns with undue resistance or if there is too much play, find the cause and fix it. Check steering and control systems (cables, control box, linkages, hydraulic systems). Replace swollen, stiff, or rusty control cables.\nReplace deteriorated zincs. Clean contacts of wires attached inside the hull to zinc bolts. If the wire is corroded at the terminal, replace with tinned boat wire.\nCheck engine oil levels, particularly if your boat was left in the water. High levels may indicate water intrusion that requires work immediately to save the engine. Check oil reservoirs including tilt/trim on outboards, some windlasses, and hydraulic fluid in steering systems.\nCheck that vent hoses haven't become clogged.\nIf your boat has been under cover, check for deck leaks, particularly around wooden areas. Run water from a hose over possible problem areas.\nCheck for freeze damage where water may have entered confined areas. Examples are around gel coat cracks, the rudder shaft seal, prop shaft seal, and thru-hull fittings. Examine cored transoms on outboard boats for cracks that could have allowed water into the transom.\nFor I/Os, carefully check the bellows for any deterioration.\nCheck and test communications equipment. Test EPIRB/PLB batteries, and verify that the registration is current. Verify that your MMSI is correctly entered into your DSC radio, and/or update your information at www.BoatUS.com/MMSI.\nTest navigation equipment. Depth-finder transducers may be damaged by cold, particularly if water has migrated through cracks in the plastic.\nLoad test batteries. Check the electrolyte level and specific gravity if applicable. A simple voltage reading with a volt/ohm meter won't tell the whole story, nor will just testing to see if it can turn over the engine.\nReplace fire extinguishers if needed. Invert hand-helds, tap hard on the bottom with the palm of your hand, and shake. Do automatic extinguishers need servicing?\nReplace flares if they show any sign of damage or are outdated.\nIs your toolbox wet inside from condensation or leaks? Are tools such as pliers and adjustable wrenches rusted?\nTest bilge pumps and alarms. If the float and alarm switches for your bilge pump(s) can't be activated manually, or you can't reach them, use a hose to fill the bilge enough to see that the pumps and alarms work.\nTest bilge blowers and check their hoses for tears or disconnected fittings.\nCheck for moisture in the fuel tank. If you didn't leave your tank topped up to almost full with appropriate additives and you've been using E10 gas, you may have water in the tank from condensation and phase separation, which could damage your engine. Draining and replacing the fuel is sometimes needed. If it is, hire a qualified professional. Replace fuel filters even if new.\nCheck fuel lines and fittings. Look for signs of leaks such as discoloring around a fitting. Replace any line or fitting looking impaired, per ABYC and USCG standards or better. Secure any loose lines.\nCarefully examine the galley stove. Check connections and plugs for the electric stove and check the burners. For gas stoves, check fittings, line, and emergency shutoff solenoid valve and solenoid wire connections at the switch and solenoid.\nCheck end plates of engine heat exchangers for white or greenish discoloration indicating water seeping past seals. Many manufacturers recommend these seals be replaced every year. Check all other water seals, including around the raw-water pump, freshwater pump (where it mates to the front of the block and weep hole underneath). Look for signs of corrosion, salt, or antifreeze residue.\nRemove antifreeze in drinking lines. Check heads and hot-water heater for cracks, even if you drained them or added antifreeze.\nTest smoke and carbon monoxide alarms and change batteries.\nCheck prop(s) for dings, bent blades, or damage. Consider sending them to a prop shop for refurbishing. Grab the shaft and try to wiggle it. If there's play, the cutlass bearing needs replacing. Check the hull and all underwater components. If you have a bolted-on keel, check for seepage or signs of rust or other deterioration.\nCheck thru-hull fittings and hoses before launch, lubricate (per the manufacturer's instruction), and work each.\nClean raw-water strainers for the engine, generator, air conditioning, head, and any others. Check gaskets and/or O-rings.\nFor waxed twine stuffing boxes for the rudder and prop shaft, replace the twine and tighten. While the seal is disassembled, check the shaft where it's normally concealed by the seal, for crevice corrosion or wear, particularly if the boat sits for long periods without running. Don't over-tighten; a slight drip is OK. Tighten again once you've run the boat, if needed. Inspect \"drip-less\" shaft seals including the lubricating water hose for free flow of water to the fitting, if you have that type.\nWhen the boat is launched, check bilges, all thru-hull fittings, below-water hoses, and any other relevant areas for seepage.\nRun the engine(s) at the dock at idle, or slow for at least 15 minutes, and then away from the dock, at varying speeds, but within easy towing distance.\nSailboats: In addition to the above, check and service winches, furling gear, blocks and cars, all standing rigging before the sails are put on, then check and work all running rigging with the sails on. Check swage fittings for any signs of cracking or other deterioration. Also check tangs on the mast where stays are attached. Check chain plates above and below deck for cracks or other deterioration, and check the structure where plates come through the deck for leakage or deterioration. Look for broken strands in stainless cable. Remove any tape covering turnbuckles or other areas, inspect underneath and replace tape if needed after servicing.\nA fresh coat of bottom paint\nAuthor checks \"under the hood\" of his outboard.\nCheck hoses for cracking or other signs of deterioration.\nBottom is freshly painted, engine and other equipment checked, and boat is ready for spring launch.\nIf your sailboat keel is bolted on, check the bolts and the keel itself.\nCheck shaft inside boat, shaft log and stuffing box. Replace stuffing if needed.\nClick here to download the printable version\nTo Home Page\nCamachee Cove Yacht Yard worker removing shaft during spring maintenance.\nThis spring checkup revealed the need for a new shaft, cutlass bearings, and reconditioning of the prop. This is more than routine yearly maintenance.\nLook for signs of electrolysis or other corrosion. This strut was replaced in the below photo.\nNew struts, paint, zincs and ready for launch.\nTurn through hull valves and service if needed.\nCheck wires for chafe.\nCheck hoses and hose clamps. The bottom one could use replacement.\nCheck anchoring equipment. You may want to reverse the chain on an all-chain rode.\nCheck for chafe and wear of canvas and lines. Replace as needed.\nCheck rudder and rudderpost. There is a problem here. Check if rudder turns freely.\nCheck and clean running gear, replace zincs as needed."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:fb48863e-9457-4fe5-8ae0-ce69f05c201f>","<urn:uuid:bb36f68d-9e9b-4eb0-9d28-beac2d2230b9>"],"error":null}
{"question":"How do nanobubbles and blood vessels differ in terms of their stability under pressure?","answer":"Nanobubbles and blood vessels exhibit different stability characteristics under pressure. Nanobubbles demonstrate remarkable 'superstability,' withstanding shock waves with pressure swings over 60 atmospheres without being affected. In contrast, blood vessels operate under much lower pressures - the arterial system contains blood under relatively high pressures compared to the venous system, but even arterial pressures only reach around 30 mm Hg near arterioles and decline to 18 mmHg or less near venules. Blood vessels rely on their three-layered structure (tunica externa, media, and interna) for stability rather than inherent pressure resistance.","context":["Focus: The Little Bubbles that Could\nAn air bubble will live peacefully on the side of your glass of water for a long time, but shrink that bubble to the nanoscale, and the overwhelming force of surface tension should prevent it from ever forming. Yet researchers have been observing them for several years, thanks to recent advances in imaging technology. The puzzle of nanobubbles has deepened with the 18 May Physical Review Letters, in which a team reports that nanobubbles can withstand shock waves exerting wide swings in pressure, a sign of what they call superstability. Aside from any impact on solving the riddle, the results suggest that tiny bubbles could be useful pieces of nanotechnology, perhaps serving to protect delicate machinery in future biochemical devices.\nSurface tension, the force that makes water bead up on a freshly waxed car hood, also keeps a submerged air bubble as round as possible. Surface tension pushes in on the bubble, and the smaller the bubble, the higher the air pressure inside the bubble must be to keep it from collapsing. For a 100-nanometer-diameter bubble, the pressure should theoretically be at least five times that of the atmosphere, easily high enough to force the gas to dissolve into the surrounding liquid. Blip, the bubble should disappear. But it doesn’t.\nIn 2000 Japanese researchers discovered these surprisingly small bubbles on a silicon surface covered with water . They were using the new imaging technology called “tapping mode” atomic force microscopy (AFM), where a tiny lever vibrates up and down to tap the sample. In these and many other experiments, nanobubbles behave differently than ordinary bubbles, says Andrea Prosperetti, of Johns Hopkins University in Baltimore. Researchers still don’t agree on how nanobubbles survive, although they have proposed several different theories.\nDetlef Lohse of the University of Twente in the Netherlands and his colleagues decided to check the bubbles’ ultimate stability by subjecting them to severe stress. They created nanobubbles in a drop of water on a silicon wafer coated with a hydrophobic material and imaged it with AFM for the “before” picture. Next they submerged the wafer in a water bath and blasted it with a powerful, 6-microsecond-long shock wave produced by the same kind of generator that can destroy kidney stones. Each pulse included positive and negative pressures over 60 atmospheres. This level of abuse would ordinarily create a drastic inflation of bubbles, in a process called cavitation, but the “after” image of the nanobubbles showed them to be unaffected. Based on the bubbles surprising survival, the team dubbed them “superstable.”\nThis stability could be useful, says team member Bram Borkent of the University of Twente. Chemical analysis devices now under development will need to move fluid through nano-channels cut into solids, but pushing the fluid through such tiny spaces is hard. “If you can create these bubbles on purpose then you can cover a nanochannel with bubbles,” which might reduce the friction, says Borkent. Other researchers envision using nanobubbles as computer memory structures or to remove blood clots. “Wherever surface effects are important,â€ Borkent says, â€œnanobubbles may play an important role.”\nProsperetti sees the new work as part of a larger story. “It is another tantalizing example of the surprises that lie in the gray area of nano–the not-so-small to be quantum, but not-so-large to be fully macroscopic.”\n- N. Ishida, T. Inoue, M. Miyahara, and K. Higashitani, “Nano Bubbles on a Hydrophobic Surface in Water Observed by Tapping-Mode Atomic Force Microscopy,” Langmuir 16, 6377 (2000)","1. Blood flows through a network of specialized vessels that can be broadly categorized as arteries, capillaries, and veins.\n1. The walls of arteries and veins contain three layers, the tunica externa, tunica media, and tunica interna.\n2. The tunica externa forms a connective tissue sheath that stabilizes the position of the vessel.The tunica media contains smooth muscle and connective tissue fibers, often elastin. The tunica interna includes the endothelium and its underlying elastic membranes.\n1. The arterial system includes the elastic arteries, muscular arteries, and arterioles. As you proceed toward the capillaries the number of vessels increases but the diameter of the individual vessels decreases and the walls become relatively thin.\n1. Capillaries may be continuousor fenestrated. Sinusoids are irregular, flattened capillary channels found in specialized tissues.\n2. Diffusion across capillary walls depends on the organization of the endothelium, the size of the diffusing molecule, and its lipid solubility. Suggestion: Emphasize that exchange with the interstitial fluids only occurs across capillary walls.\n3. Individual capillaries are usually part of an organized capillary plexus, or capillarybed. Precapillary sphincters determine the relative volume of flow through each of the capillaries.\n4. Blood flow through a capillary plexus changes asvasomotion occurs. The entire network may be bypassed by blood flow through arteriovenous anastomoses or via preferred channels within the capillary plexus.\n1. Venules are small veins that collect the blood leaving a capillary network. They merge into medium-sized veins ; these convey the blood to large veins including the superior and inferior venae cavae.\n2. The arterial system contains blood under relatively high pressures. Venous blood is under relatively little pressure, and special valves are necessary to prevent the backflow of blood.\n3. Pressure changes in the pleural cavity as a result of respiratory movements assists in moving blood towards the heart.\nThe Distribution of Blood:\n1. Peripheral venoconstriction may represent an important means of increasing the blood volume following hemorrhaging.The venous reserve accounts for about 20 percent of the normal blood volume.\nTHE CIRCULATORY SYSTEM\nThe Pulmonary Circulation\n1. The pulmonary circuit includes the pulmonary trunk, the right and left pulmonary arteries, pulmonary capillaries, and four pulmonary veins that empty into the left atrium.\nThe Systemic Circulation\nThe Arterial System:\n1.The ascending aorta gives rise to the coronary circulation. The aortic archcommunicates with the descending aorta.\n2.The aortic arch gives rise to the brachiocephalic, left common carotid, and left subclavian arteries.The right common carotid and right subclavian arteries arise from the brachiocephalic.\n3. The carotid divides into an external carotid servicing the superficial structures of the head and an internal carotid that enters the skull at the carotid foramen.\n4. The vertebral arteriesarise from the subclavians, ascend within the transverse foramina of the cervical vertebrae, and enter the skull through the foramen magnum.\n5. The carotids supply the rostral portions of the cerebrum, and the vertebrals supply the rest of the brain. Anastomoses between these vessels at the circle of Willisensure a reliable supply of blood.\n6. The capillaries of the brain are relatively impermeable to materials other than dissolved gases; this represents the blood- brain barrier.\n7. The descending aorta travels through the thoracic cavity within the mediastinum. The diaphragm marks the boundary between the thoracic and abdominal divisions of the descending aorta.\n2. The thoracic aorta provides blood to the intercostal and superior phrenic arteries. The abdominal aorta services the inferior phrenics as well as the celiac, superior and inferior mesenterics, the gonadal, thesuprarenal, the renal, the lumbar, and the common iliac arteries.\nThe Venous System:\n1. Peripheral structures often receive two sets of veins, superficial and deep. Shunting blood from one to another can play an important role in thermoregulationby encouraging or preventing heat loss to the external environment.\n2. The small veins of the brain drain intovenous sinusesof the dura, such as the superior sagittal sinusof the falx cerebri. These sinuses drain into the internal jugular vein. The internal jugular vein descends parallel to the course of the superficial external jugular vein draining superficial structures.\n3. The venous distribution generally resembles that of the arteries. Thesubclavian veins are joined by the external and internal jugulars to form the brachiocephalics (innominates). These unite to form the superior vena cava , which also receives blood from the body wall via the azygos complex of veins.\n4. Blood returning fom the pelvis and legs enters the common iliac veins. These unite to form the inferior vena cava. The IVC also receives blood from the lumbar, gonadal, renal, suprarenal, phrenic, and hepatic veins en route to the heart.\n5. The digestive viscera are drained by tributaries of the hepatic portal vein. This vessel breaks down into sinusoidal networks within the liver, where it mixes with arterial blood delivered by the hepatic artery, a branch of the celiac trunk.\nDevelopment of the Circulatory System\n1. During fetal development the umbilical arteriescarry blood to the placenta. It returns via the umbilical vein and enters a network of vascular sinuses within the liver. The ductus venosus collects this blood and returns it to the inferior vena cava.\n2. At this time the interatrial septa are incomplete, and the foramen ovale allows the passage of blood from the right atrium to the left atrium. The ductus arteriosus also permits the flow of blood between the pulmonary trunk and the aortic arch.These connections are closed shortly after birth.\n1. Cardiovascular regulation maintains adequate blood flow to vital tissues and organs.\n2. Flow rate is directly proportional to the pressure applied. Blood flows from regions of higher pressure to areas of relatively lower pressure.\n3. Friction against the walls of the vessels and interactions between fluid molecules create a resistance that opposes fluid movement. The flow rate is inversely proportional to the resistance.\n1. Blood circulates because the heart establishes a pressure gradient, with the pressure highest at the aorta and lowest at the entrance to the right atrium.\n2. The difference in pressure from one end of the systemic circuit to the other represents the circulatory pressure.\n1. Arterial pressures are highest during ventricular systole and lowest in ventricular diastole. The difference between these pressure readings constitutes the pulse pressure , and the mean arterial pressure lies partway between the diastolic and systolic pressures.\n2. Blood pressure can be measured by determining the force exerted against the walls of muscular arteries; values are reported as systolic/diastolic pressures.\n3. The blood pressure and pulse pressures fall as you proceed along the arterial network toward the peripheral capillaries due to elastic components in the vascular walls. Arteriolar pressures are relatively steady at about 30 mm Hg.\n4. Capillary pressures decline from 30 mm Hg near the arterioles to 18 mmHg or less near the venules.\n1. Arterial pressures determine the rate of peripheral blood flow; venous pressures affect the venous return, and so influence cardiac output.\n2. Venous pressures are low, and several mechanisms assist in propellng blood to the heart: valves, muscular pumps, and the thoracoabdominal pump are important factors that assist the venous return.\n1. Most of the total peripheral resistance is provided by the peripheral resistance of the arterioles.\n2. Vasodilation reduces peripheral resistance; vasoconstriction increases it. The status of peripheral arterioles is controlled by the vasomotor center in the medulla, which determines the vasomotor tone.\n1. The relationships between flow, pressure, and resistance can be summarized as: F (flow rate) = P (pressure)/R (resistance). For the arterial system, this relationship can be restated as:\nCO = AP (arterial pressure) X PR (peripheral resistance)\n(cardiac output is flow)\nVariations in Cardiac Output:\n1. Cardiac output may vary as the result of local factors, autonomic controls, or the presence of hormones.\n2. Local factors include intrinsic regulation (Starling's law), changes in ionic composition, and physical damage to the myocardium.\n3. Autonomic commands are issued by the cardiac centers of the medulla.\n4. Hormonal control involves the adrenal hormones epinephrine and norepinephrine.\nVariations in Peripheral Resistance:\n1. Alterations in peripheral resistance may occur under local control, in response to commands issued by autonomic centers, or in response to circulating hormones.\n2. Local adjustments occur as smooth muscles of the precapillary sphincters contract or relax in response to alterations in the characteristics of the surrounding interstitial fluid.\n3. Autonomic commands are issued by the vasomotor center of the medulla.\n4. Most of the hormones that affect peripheral resistance cause vasoconstriction; examples include ADH, angiotensin II, epinephrine and norepinephrine. Atrial natriuretic factor causes vasodilation.\nAlterations in Blood Pressure:\n1. Cardiac output and peripheral resistance are controlled directly, by local, neural, or hormonal mechanisms. Blood pressure is controlled indirectly, primarily via alterations in cardiac output, peripheral resistance, and blood volume.\nHOMEOSTASIS AND CARDIOVASCULAR FUNCTION\n1. Vasomotion in peripheral tissues does not normally affect blood pressure. But if a general vasodilation occurs, blood pressure and peripheral blood flow declines.\n2. Central control mechanisms respond to changes in blood pressure or alterations in the concentration of carbon dioxide or oxygen in the blood or cerebrospinal fluid.\n3. Short-term responses adjust cardiac output and peripheral resistance to stabilize blood pressure and maintain tissue blood flow.\n4. Long-term adjustments involve alterations in blood volume that affect cardiac output and the transport of oxygen to peripheral tissues.\n5. The nervous and endocrine systems direct short-term responses; hormones and local factors control long-term adjustments. Neural Mechanisms and the Short-Term Regulation of Blood Pressure\nThe Baroreceptor Reflexes:\n1. A rise in blood pressure inhibits the cardioacceleratory center, stimulates the cardioinhibitory center, and inhibits the vasomotor center. Cardiac output and peripheral resistance decline, and so blood pressure falls as a result.\n2. A decline in blood pressure stimulates the cardioacceleratory center, inhibits the cardioinhibitory center, and stimulates the vasomotor center. The increased cardiac output and peripheral resistance elevates blood pressure and improves peripheral blood flow.\n3. Examples of baroreceptor reflexes include the aortic, carotid sinus, and atrial reflexes.\nThe Chemoreceptor Reflexes:\n1. The chemoreceptor reflexes respond to alterations in dissolved oxygen and/or carbon dioxide concentrations at the carotid bodies, the aortic bodies, or the cerebrospinal fluid.\n2. Decreased oxygen or increased carbon dioxide concentrations leads to elevations in peripheral resistance, cardiac output, and blood pressure, increasing peripheral blood flow.\nAutonomic Activation and Higher Centers:\n1. The cardiac and vasomotor centers may also be influenced by activities in other areas of the brain. Sympathetic activation leads to stimulation of the cardioacceleratory and vasomotor centers; parasympathetic activation stimulates the cardioinhibitory center.\nHormones and Cardiovascular Regulation\n1. Hormones involved with blood pressure regulation over the short-term include epinephrine and norepinephrine. These hormones promote peripheral vasoconstriction and venoconstriction, and increase the cardiac output.\n2. Antidiuretic hormone and angiotensin II promote peripheral vasoconstriction in addition to their other functions.\n3. ADH and aldosterone promote the retention of water and electrolytes and stimulate thirst.\n4. Erythropoietin stimulates red blood cell production, adding to the volume of whole blood and assisting in the delivery of oxygen to peripheral tissues.\n5. Atrial natriuretic factor encourages fluid loss, reduces blood volume, and inhibits thirst.\nLocal Factors Affecting Blood Volume and Blood Pressure\n1. Local controls exist because changes in capillary hydrostatic and osmotic pressures alter the rates of filtration and resorption across capillary walls.\n2. Under normal conditions a capillary loses slightly more fluid than it gains along its length. The excess interstitial fluid gets removed by the lymphatic system.\n3.Shifting the dynamic center can result in the recall of fluidsor the production of tissue edema.\nPATTERNS OF CARDIOVASCULAR RESPONSE\nExercise and the Cardiovascular System\n1. During exercise blood flow to the skeletal muscles increases, at the expense of circulation to non-essential organs.Cardiac output can rise 6 times, as the result of increased heart rate and a reduced end systolic volume.\n2. Cardiovascular performance improves with training. Athletes have a larger stroke volume, slower resting heart rates, and increased cardiac reserves.\nThe Cardiovascular Responses to Hemorrhaging\nThe Elevation of Blood Pressure:\n1. Hemorrhaging provokes an increase in cardiac output, a mobilization of venous reserves, peripheral vasoconstriction, and the liberation of hormones that promote the retention of fluids and the manufacture of erythrocytes.\n2. A fall in blood pressure at the carotid sinus to below 50 mm Hg will produce the central ischemic response. If tissue controls override this mechanism, a circulatory collapse occurs and death results.\nThe Restoration of Blood Volume:\n1. After a severe hemorrhage, several days pass before blood volume returns to normal. Adjustments over this period are primarily directed by ADH, angiotensin II, aldosterone, and erythropoietin."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:6b9db2ec-1103-4b20-b071-b2d484e55929>","<urn:uuid:5c3c970e-4dc1-480f-9b36-cf8c9693ed87>"],"error":null}
{"question":"What are universities' roles in addressing sustainability, and how does distance learning affect art students' motivation in higher education?","answer":"Universities play a crucial role in addressing sustainability as society's pinnacle of knowledge generation and dissemination. They are expected to develop future citizens, guide policy development, exchange knowledge, and support communities while using academic freedom for innovation. Universities are particularly influential due to their role in educating future leaders and decision-makers. Regarding distance learning's effect on art students' motivation, research shows that while it isn't a major obstacle for learning, internally motivated students prefer traditional learning. The main challenges include technical issues like multiple learning platforms and poor internet connections, organizational problems like time management, and psychological factors such as information overload and lack of social exchange.","context":["Society increasingly expects higher education to play a leading role in addressing the sustainability crisis. This trend seems to have started since Agenda 21, which directed universities worldwide into the driving seat of society’s quest for sustainability.\nWhat’s more, there seems to be a moral responsibility angle, based on the argument that higher education has to some extent contributed to society’s sustainability crisis in the first place. For example, on one hand universities are organised around disciplines, resulting in what is termed as ‘knowledge silos,’ without much conversation between disciplines. On the other hand, sustainability issues are complex and require a holistic approach to knowledge generation and dissemination. This indicates a mismatch between society’s needs and higher education’s provision in teaching and research.\nSuch issues have led to analyses like that of UNESCO (2012), ‘Our current knowledge base does not contain the solutions to contemporary global environmental, societal and economic problems.’ The sustainability crisis is therefore usually interpreted as ‘a knowledge crisis’ (Parker 2010).\nIn modern society university is the pinnacle of knowledge generation and dissemination. Because of their crucial role in educating and training future leaders and decision-makers, they can have great influence on their knowledge, skills and values. Besides the relative freedom that higher education enjoys gives it room for innovation for sustainability. So, not only higher education should play a leading role in sustainability, it has great potential to do so.\nThere is, however, no firm agreement on what higher education’s sustainability leadership role should be. In general, it could be viewed as comprising ‘developing future citizens, guiding policy development, exchanging knowledge, supporting communities, and using academic freedom to fuel further enterprise and innovation’ (Ryan et al 2010: 116) with a view of building a sustainable world.\nWhat has been higher education’s response to this sustainability challenge, especially in the UK? The next post will look at that.\nResources on higher education’s role in sustainability\n2012Greening Universities Toolkit (draft) (UNEP)\nThe role of higher education in addressing sustainability (Defra)\nHow can higher education institutions become more sustainable? (The Guardian)\nSustainable development in higher education: 2008 updateto strategic statement and action plan (HEFCE)\nSustainable Development in Higher Education: CurrentPractice and Future Developments (The Sustainability Information Teaching Exchange)\nSustainable Development on Campus (International Institute for Sustainable Development IISD)\nSustainable Development (HEFCE)\nSustainable Development (SFC)\nPeople’ssustainability treaty on higher education (University of Gloucestershire)\nReferences & Bibliography\nBeringer, A. and Adomssent, M. (2008) Sustainable university research and development: inspecting sustainability in higher education research. Environmental Education Research, 14(6), pp.607-623.\nChalkley, B. and Sterling, S. (2011) Hard times in higher education: the closure of subject centres and the implications for education for sustainable development (ESD). Sustainability, 3, pp. 666-677.\nDjordjevic, A. and Cotton, D.R.E. (2011) Communicating the sustainability message in higher education institutions. International Journal of Sustainability in Higher Education, 12(4), pp. 381-394.\nFerrer-Balas, D; Lozano, R; Huisingh, D; Buckland, H; Ysern, P; Zilahy, G (2010) Going beyond the rhetoric: system-wide changes in universities for sustainable societies, Journal of Cleaner Production, 18, pp.607-610\nMinguet, P.A., Martinez-Agut, M.P., Palacios, B., Pinero, A. and Ull, M.A. (2011) Introducing sustainability into university curricula: an indicator and baselines survey of the views of university teachers at the University of Valencia. Environmental Education Research, 17(2), pp. 145-166.\nParker, J. (2010) Competencies for interdisciplinarity in higher education. International Journal of Sustainability in Higher Education, 11(4), pp.325-338.\nRyan, A., Tilbury, D., Corcoran, P.B., Abe, O. and Nomura, K. (2010) Sustainability in higher education in the Asia-Pacific: developments, challenges, and prospects. International Journal of Sustainability in Higher Education, 11(2), pp.106-199.\nScott, W. and Gough, S. (2007) Universities and sustainable development: the necessity for barriers to change. Perspectives: Policy and Practice in Higher Education, 11 (4), pp. 107-115.\nStephens, J. C. and Graham, A.C. (2010) Toward an empirical research agenda for sustainability in higher education: exploring the transition management framework. Journal of Cleaner Production, 18(7), pp. 611-618.\nUNESCO (2012) Education for Sustainable Development [Online] Available from: http://www.unesco.org/new/en/education/themes/leading-the-international-agenda/education-for-sustainable-development/education-for-sustainable-development/ (Accessed 27 November 2012)\nWaas, T., Verbruggen, A. and Wright, T. (2010) University research for sustainable development: definition and characteristics explored. Journal of Cleaner Production, 18, pp. 629-636.\nWright, T.S.A. and Wilton, H. (2012) Facilities management directors’ conceptualizations of sustainability in higher education. Journal of Cleaner Production, 31, pp. 118-125.\nPhotograph by Asitha Jayawardena","INFLUENCE OF DISTANCE LEARNING ON THE ACADEMIC MOTIVATION OF STUDENTS FROM SPECIALTIES IN THE FIELD OF ARTS\nKeywords:distance learning, motivation, student, art\nThis article discusses the problem of the impact of distance learning on the motivation to learn in students studying arts. Learning arts such as music, dance, and fine arts via distance learning is proving to be a real challenge for students and teachers. The aim of the present study is to determine how distance learning affects the learning motivation of students studying art at the university.\nMethod: a questionnaire for measuring academic motivation and a modified scale for measuring students' engagement in online learning, and a survey.\nSample: 109 students from first to fourth-year in university, studying specialties related to music, fine arts and dance\nResults: The results showed that distance learning did not prove to be a major obstacle in establishing the learning material, but students who are internally motivated to learn prefer traditional learning. The main difficulties in distance learning turned out to be technical - the use of too many and different learning platforms, lack of a good internet connection, organizational - time management difficulties, easy distraction and psychological factors such – overwhelming written information, the high demands of the teachers, and a lack of social exchange. Correlation analysis found that students who prefer traditional education have a high intrinsic motivation to study art-related disciplines. ANOVA did not establish statistically significant differences in the factors, course and specialty.\nBolliger, D. U., and Martindale, T. (2004). Key factors for determining student satisfaction in online courses. Int. J. E-Learn. 6, 61–67.\nCarini, R.M., Kuh, G.D. & Klein, S.P. (2006). Student Engagement and Student Learning: Testing the Linkages. Res High Educ 47, 1–3. https://doi.org/10.1007/s11162-005-8150-9\nChin Choo Robinson & Hallett Hullinger (2008) New Benchmarks in Higher Education: Student Engagement in Online Learning, Journal of Education for Business, 84:2, 101-109, DOI: 10.3200/JOEB.84.2.101-109\nJeongju Lee & Hae-Deok Song & Ah Jeong Hong, (2019). \"Exploring Factors, and Indicators for Measuring Students’ Sustainable Engagement in e-Learning,\" Sustainability, MDPI, Open Access Journal, vol. 11(4), pages 1-12, February.\nLee, Y., and Choi, J. (2011). A review of online course dropout research: Implications for practice and future research. Educ. Technol. Res. Dev. 59, 593–618. doi:10.1007/s11423-010-9177-y\nMaina-Okori N.M. (2019). Sustainability Domains in Higher Education. In: Leal Filho W. (eds) Encyclopedia of Sustainability in Higher Education. Springer, Cham. https://doi.org/10.1007/978-3-319-63951-2_489-1\nMcPhee, I., and Söderström, T. (2012). Distance, online and campus higher education: reflections on learning outcomes. Campus Wide Inf. Syst. 29, 144–155. doi:10.1108/10650741211243166\nRadoslavova, M., Velichkov, A. (2005). Metodi za psihodiagnostika, Sofiya, izd. Pandora –prim\nRoddy, C., Amiet, D. L., Chung, J., Holt, C., Shaw, L., Mckenzie, S., Garivaldis, F., Lodge, J. M., & Mundy, M. E. (2017). Applying best practice online learning, teaching, and support to intensive online environments: An integrative review. Frontiers in Education, 2, . https://doi.org/10.3389/feduc.2017.00059\nHow to Cite\nCopyright (c) 2021 Author\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nThe author is the copyright holder. Distribution license: CC Attribution 4.0."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:472e1ccb-0d1b-44f3-8ec0-31cb525865e3>","<urn:uuid:8da937ad-74b1-4bfb-99c2-ba384a3425dd>"],"error":null}
{"question":"What are the main learning mechanisms through which attitudes are formed?","answer":"Attitudes are formed through three main learning mechanisms: Classical Conditioning, where associations are formed automatically (like associating a friend with bad news); Instrumental Conditioning, where attitudes develop through rewards and punishments; and Observational Learning, where individuals absorb attitudes by observing others, such as children mimicking their parents' behaviors and attitudes. These mechanisms shape our attitudes often without conscious awareness.","context":["- Article's photo | Credit Javatpoint\nConsider your thoughts on controversial topics such as dowry deaths, sexual harassment, prejudices, and superstitions. Ever pondered where these attitudes stem from? Were you born with them, or did life experiences shape your perspectives? This article delves into the fascinating world of attitude formation, exploring the various ways attitudes take shape within us.\nUnderstanding the Dynamics of Attitude Formation: Exploring Origins and Influences\nDefining Attitude Formation:\nAt its essence, Attitude Formation denotes the evolution from a neutral stance on an object to the development of a positive or negative outlook. It involves the journey we take from having no opinion on something to developing a positive or negative stance towards it.\nAttitude formation is like a canvas coming to life, initially blank, then progressively splashed with vibrant hues and textured strokes, each representing an encounter, experience, or influence that shapes our opinions and biases until a distinct picture emerges — our stance on the world.\nCould a simple glance or whispered conversation be enough to set in motion the machinery that shapes our attitudes, influencing how we choose to express them?\nSocial psychologists believe it's not just about personal experiences, but also about exposure, learning, social comparison, and even genetics. This complex mix shapes how we see the world and express ourselves.\nImagine encountering a brand logo repeatedly. Through this mere exposure, as Zajonc (1968) called it, our fondness for the object, usually positive, grows. This phenomenon extends across various stimuli, including foods, photographs, words, and advertising slogans. Contrary to the adage, familiarity often breeds not contempt, but comfort.\nExample The enduring success of the Marlboro man advertising campaign, designed to associate filtered cigarettes with enhanced manhood, exemplifies how repeated exposure can solidify positive attitudes over time.\nResearch underscores that the impact of mere exposure is most potent when occurring randomly and moderately over time. Excessive exposure diminishes the effect, emphasizing the nuanced relationship between familiarity and attitude formation.\nAttitude Formation by Learning\nEarly theorists believed attitudes followed the same learning principles as other behaviors, suggesting that attitudes undergo automatic reinforcement through:\n- Classical Conditioning: Imagine associating a friend with bad news every time you meet them. Soon, you might find yourself dreading their company – even though they had nothing to do with the bad news! This is classical conditioning in action, shaping our attitudes without our conscious awareness.\n- Instrumental Conditioning: We learn to behave and believe in ways that bring rewards and avoid punishments. Children pick up on parental approval and disapproval, shaping their attitudes and behaviors accordingly.\n- Observational Learning: We don't just learn from direct experiences; we observe and absorb from those around us. Children often mimic their parents' behaviors and attitudes, even if not explicitly taught. Similarly, media and peer pressure can play significant roles in shaping our opinions.\nAttitude Formation by Social Comparison\nSocial comparison serves as another mechanism for attitude formation, where individuals compare their views with others to validate their ideas. Alignment with the opinions of admired or respected individuals may lead to attitude changes, fostering a desire for similarity with those we esteem.\nThe Genetic Factor in Attitude Formation\nWhile experiences play a crucial role, genetic predispositions might influence specific attitudes, like those on political or religious issues. These genetically influenced attitudes tend to be stronger, more resistant to change, and can even fuel dislike towards opposing viewpoints.\nUnderstanding attitude formation empowers us to be more conscious of the influences shaping our opinions. By acknowledging the impact of mere exposure, conditioning, social comparison, and even genetics, we can approach our own and others' opinions with greater insight and empathy.\nRemember, our attitudes are not static; they're constantly evolving under the influence of various forces. By understanding these processes, we can become more aware of how our own opinions are formed and how we can approach others with open minds and a willingness to learn from different perspectives."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:be30a377-b213-44c9-9039-04053bfe69c1>"],"error":null}
{"question":"What was the longest Olympic ban in history? How many years did it last?","answer":"South Africa holds the record for the longest Olympic ban, lasting 28 years due to apartheid. The country was banned in 1963 and wasn't invited to participate again until 1991, just before the 1992 games in Barcelona.","context":["Has A Country Ever Been Banned From The Olympics? Oh Yeah\nThe International Olympic Committee (IOC) dropped a bombshell on Dec. 5 when they announced that they were outright banning Russia from participating in the 2018 Winter Olympics in Pyeongchang, South Korea. The ban comes after the IOC found Russia guilty of running an extensive, state-sponsored doping program and interfering with the anti-doping lab in the 2014 Sochi games. The punishment is definitely harsh, but Russia isn't the first country banned from the Olympics. Not even close.\nAs recently as 2016, countries have been barred from participating in the Olympic games. Kuwait, for example, was banned from the 2016 summer games in Rio de Janeiro after the country passed a law allowing interference with national sports federations, an IOC \"no-no,\" according to The Washington Post. Kuwaiti athletes were able to participate as neutrals under the Olympic flag — similar to stipulations for Russian athletes, though admittedly the requirements for Russia are much stricter due to the extent to which the country went to cheat in the 2014 games.\nSimilarly, India was banned for the first few days of the 2014 games in Sochi for corruption, according to the BBC.\nBut the practice of banning individual countries isn't anything new.\nIn fact, it dates back almost 100 years.\nThe 1920 games in Antwerp, Belgium — right after World War I ended — included 29 countries, according to the Olympic Games website. What they fail to mention, however, is who was barred from participating. According to Canadian Olympic Team's official website, the aggressors who started WWI — Germany, Austria, Hungary, Bulgaria, and Turkey — were not invited to the games.\nSimilarly, Germany and Japan, the Axis powers in World War II, were not invited to the 1948 games in London.\nThe longest ban probably goes to South Africa, which was banned from competing in any Olympic games for 28 years due to apartheid. In 1963, the committee announced that South Africa had until the end of the year to address racial discrimination in their country, or they would not be allowed to participate in the 1964 games in Tokyo. South Africa wasn't invited to participate until 1991 — just before the 1992 games in Barcelona.\nThe (kind of) country formerly known as Rhodesia, now known as Zimbabwe, was also banned from the Olympics in 1972. Rhodesia was formed as a whites-only government, and their racist politics led the IOC to outright ban them from the 1972 games in Munich, four days before the games began — thanks to uproarious protests from African countries. The country was dissolved and reformed as Zimbabwe before they had had a chance to compete again.\nPlenty of other countries have been banned at various times in Olympic history, as well, including Taliban-ruled Afghanistan, which banned women from participating, and so was banned, in turn, by the IOC from the 2000 Sydney games. After the fall of the Taliban, Afghanistan was invited back to the 2004 games in Athens.\nThe IOC has a habit of using bans to respond to social ills.\nThis is perhaps why they felt it necessary to not only ban Russia, but to pass other sanctions, including a $15 million fine (which will reimburse the IOC for the investigation and contribute to the creation of an Independent Testing Authority). The IOC is making no secret of the fact that they are trying to send a message about doping.\nSeveral officials associated with the 2014 doping scandal have been banned from participating in the Olympic games ever again. Numerous Russian athletes who were found to have been using performance enhancing drugs have had their medals vacated. Russian athletes who wish to participate in the 2018 games may only do so after going through a rigorous application process, proving they're clean, and they may only compete as neutrals, under the Olympic flag.\nIt may seem harsh on the face, but according to the IOC, Russia's cheating is \"unprecedented.\" Russia has been found guilty of funding a large, state-sponsored doping program, the likes of which the world has never seen. The Russian Ministry of Sport was additionally found to have put together a team that, in the dead of night during the 2014 games, \"tampered with more than 100 urine samples to conceal evidence of top athletes’ steroid use throughout the course of competition,\" per The New York Times.\nAfter the IOC announced that it would ban Russia from the 2018 games, quite a few Russian athletes and government officials said the punishment was too severe, per both The Washington Post and The Moscow Times. For example, national ski jumping champion Irina Avvakumova reportedly said, “I don’t know how other athletes will react, but I haven’t trained for years to go somewhere to not compete on behalf of my country. It’s not sportslike to go to the Olympics just in order to be there.\"\nIOC President Thomas Bach said of Russia's cheating, \"This was an unprecedented attack on the integrity of the Olympic Games and sport.\" He called the sanctions \"proportional\" to the crime, \"while protecting the clean athletes.\""],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:bb6780d2-0016-4b6d-af56-10769306a1c6>"],"error":null}
{"question":"Can you please explain how our understanding of probability changes when comparing the initial game show choice (3 doors) versus after one door is revealed? I am student trying to learn this concept.","answer":"Initially with 3 doors, you have a 1/3 chance of picking the correct door with your first choice. When one losing door is revealed, many people incorrectly believe the probability becomes 50-50 between the two remaining doors. However, the probability actually stays at 1/3 for your original door and becomes 2/3 for switching to the other unopened door. This is because the host's action of revealing a losing door doesn't change the original probabilities - your first choice still has a 1/3 chance of being right, while the probability that the prize is behind 'one of the other two doors' remains at 2/3. When the host shows you which of those two doors is incorrect, the entire 2/3 probability transfers to the remaining unopened door. This is demonstrated through both logical analysis and simulations, which show that switching doors wins approximately 2/3 of the time while staying with your original choice only wins 1/3 of the time.","context":["Supposing you are a participant on a game-show where the host presents you with three doors to choose from, behind one of which there is some desirable prize, such as a million dollars which you will win if only you have chosen the door behind which it lay. You are given a chance to choose one of the three doors as an opening gambit – then before the door is opened to you, your host does you the kindness of revealing that behind one of the other two doors, there was nothing (or else at least there was not the prize – popularly people say there was a goat instead; imagine whatever you like). Now, the contestant is always asked whether they would like to rethink their selection, and so you are, as expected, asked by the host to reconsider your selection. Most contestants think that the odds of them having the correct door is actually 50%, since there are only two doors, and behind one of them there is a million dollars. Suppose you stay with your door, and you will most likely loose – you may not immediately realize it but your chance of winning with the door you originally chose was exactly 1/3, and it would have been 2/3rds if you had decided to change your selection from your door to the other door.\nDraw it out if you must:\n[_1_] [_2_] [_3_]\nSupposing that you chose door 1, and the host shows you that door 2 has nothing desirable behind it. Notice that the host has just shown you one of the two remaining doors which he knows there is nothing behind. You might be tempted to think that, since it is true that there is one million dollars behind one of two remaining doors, either door you choose is equally likely to be the correct door, and there is a probability of 50% for either. However, that’s not really the case – two thirds of the time the door you start with will not have the prize, and the remaining door will.\nDo it trial and error – or methodologically. Let’s imagine that you decide to never change your choice first choice. First, you choose door 1, and it is behind door 1, and you are shown door 2, and thus you choose door 1 – you are correct. Second, you choose door 1, and it is behind door 2, and you are shown door 3, and you choose door 1 – you loose. Third, you choose door 1, it is behind door 3, you are shown door 2, and you choose door 1 – you loose. Those are all the possibilities, and you lost 2/3 times by sticking with the same original choice. In other words, in two out of three logically possible worlds, sticking with your choice yields the result that you do not get the prize.\nSuppose you do just the opposite. Suppose you say you will always switch instead. First, you choose door 1, and it is behind door 1, you are shown door 2, and thus you choose door 3 – you loose. Second, you choose door 1, it is behind door 2, you are shown door 3, you choose door 2 – you win. Third, you choose door 1, it is behind door 3, you are shown door 2, you choose door 3 – you win. You just won 2/3 times by methodologically changing your selection of doors.\nIf this isn’t clear why the previous paragraph covers all the logically possible scenarios (somebody might think ‘why not start picking door 2 at first instead of door 1’ or something like that -without realizing that the situation is just going to be logically equivalent), you can write it out the longer way substituting all instances of ‘door 1’ in the paragraph above for ‘door 2’, ‘door 2’ for ‘door 3’ and ‘door 3’ for ‘door 1’; then afterwards run through them again once you substitute from the same above paragraph ‘door 1’ for ‘door 3’, ‘door 2’ for ‘door 1’ and ‘door 3’ for ‘door 2’ – you will have literally imagined all the logically possible worlds one at a time (though anyone who realizes that no matter which door you choose the formula is going to work out exactly the same way, yielding 2/3, can just move on immediately). Either way, you will come out with 6/9 times, or 2/3 times, that you win if you decide to switch doors, and you will loose that same ratio of times if you do not switch.\nCuriously, most people think this is just completely counter-intuitive. It overturns all our intuitions about probability, because we tend to think that if presented with two doors, and you know behind one of them there is a million dollars, then either one you choose you will retain a probability of 50% chance of being correct. However, in the case of the Monty Hall problem, this is demonstrated to be untrue.\nIs this really untrue? No. If you were presented with two doors and simply told that behind one of them lay a prize, you would intuitively think that you had a fifty-fifty chance of choosing the correct door, and you would be correct. Your intuition is not at fault – rather, your assessment of your situation is at fault – you aren’t really being given a choice between two doors, but rather between one door and “one of the other two doors”.\nThe problem is precisely that when shown one of the doors, we immediately think that the game has changed – we think that instead of being offered 3 doors, we are now just being presented with two doors, and this is the whole mistake. The game hasn’t changed – the chances one of the two doors we didn’t choose was the right one was 2/3, and since the host knows which one it is, the host chose to reveal the door that it wasn’t – however, the one door which you didn’t originally choose retains that whole 2/3 chance of being the correct one, because the one door isn’t one door, it is just ‘one of the two doors’ which you didn’t choose. The problem is not with our intuitions about probability (those are correct) but with our assessment of the situation in which we find ourselves.\nIn other words, the hosts action isn’t a game-changer at all, and that’s the whole trick of it. All you know is that the door you chose has one third a chance of being true, while it is 2/3 likely that “one of the other two doors” has the prize. When the host shows you that he knows which door it isn’t behind, it still remains true that it is 2/3 likely that the prize is behind “one of the other two doors”. In essence your choice is either door 1, or else “one of the other two doors”.\nTherefore, the Monty Hall problem is not a good example of our intuitions about probability being incorrect, but a good example of a language-trick; what has changed, we are tempted to think, is the grammar of the game, but the grammar of the game hasn’t changed at all. Once one understands the philosophy of language, one can plausibly discern more easily that the host’s action is not a speech-act with respect to the game of choosing whether it is behind your door, or else one of the other two. The fact that the host has narrowed down for you which of the other two it isn’t, doesn’t change the fact that choosing the other door represents choosing “one of the other two doors.” I think this becomes clearer if the host were to, instead of showing us one of the doors it isn’t (since there is bound to be one), the host were simply to say “ok, how about this, I will give you the choice between the prize being behind the door you chose, or else the prize being behind one of the two doors you didn’t choose – which say you?” without him showing you anything, you would immediately see that, of course, it is 66.6% likely that “the prize is behind one of the two doors you didn’t choose”.","I have always been amused and intrigued by responses to “The Monty Hall Problem”, especially when I talk about it to audiences with a high concentration of engineers and mathematicians. If you are familiar with it, but you’ve always struggled with an unsettled feeling of “this can’t be right”, read further and let me know if my explanation of the solution helps to alleviate the discomfort. If you are not familiar, I guarantee you will give your brain a workout by reading on.\nFirst posed to statisticians in 1975, “The Monty Hall Problem” is well-known among academics because it still sparks debate. Many seem to think that disagreements about its solution stem from issues in the clarity of the problem, but I contend that it really stems from human flaws in the way that we process information.\nI often discuss this problem in statistics and cognitive psychology courses for several reasons. It is a great exercise in probability calculation and it can be used to teach basic mathematical modeling (and its purpose). An added benefit, since almost all of my students were psychology majors, is that it also illustrates a flaw in human cognition as well as a pattern of problem solving. Even a knowledgeable statistician feels the need to run simulations to see the solution in action. Even then, fully grasping the mechanisms behind the answer often requires brute force cognition.\nIn general, human beings have a very difficult time wrapping their brains around concepts of probability. It is much like a visual illusion; we know that the lines are parallel/the circles are the same size/there is no motion, but we can’t make our brains process it in a way that represents that reality. It’s just not how our visual system works. I hypothesize that one of the reasons that probability is such a difficult field for most people is that it involves theory and models, which are distinct from observations and we must represent them differently in our minds to properly deal with them. Applications of probability often involve switching gears from the realm of models to data or vice versa and this is where I think most mathematicians get side-swiped in The Monty Hall Problem.\nIn essence, here’s the problem:\nYou are a contestant on Let’s Make a Deal! and Monty loves your creative costume (a teddy bear carrying a human doll), so he calls on you to make a deal. Monty says, “There are three doors – Door #1, Door #2, and Door #3. Pick one and you get to keep whatever is behind it.”\nYou’ve seen the show (you weren’t just walking down Ventura Boulevard in a teddy bear costume for fun), so you know that it is highly likely that there is a coveted BRAND NEW CAR! behind one of those three doors. If you choose wrong, however, you might end up with an ostrich…\nYou choose Door #3.\nMonty then says, “Let’s see what’s behind Door #1!” and the door opens to reveal one of the many consolation prizes (and product placements), a lifetime supply of Rice a la Roly.\nCool! You might get that car after all!\nWell, the show was successful because the shell-game-huckster-style of Monty Hall rarely stopped there. In this case, he does what he often does, offers to let you switch from your first choice (Door #3) to the only remaining option, Door #2.\nShould you? Does it matter?\nNot the Problem\nBefore I get into the solution, let me first deflect a common complaint from mathematicians. The most well-known version of the problem, from its Wikipedia entry:\nSuppose you’re on a game show, and you’re given the choice of three doors: Behind one door is a car; behind the others, goats. You pick a door, say No. 1 [but the door is not opened], and the host, who knows what’s behind the doors, opens another door, say No. 3, which has a goat. He then says to you, “Do you want to pick door No. 2?” Is it to your advantage to switch your choice?\nThis version does not specifically state the name of the show or indicate the way that game shows of its era worked. If you have never seen the television show (i.e., you are younger than 35), or any game show of its kind, let me explain. Monty is in control of almost everything that happens. The only thing “contestants” can do is make choices when Monty offers them. As you will see, they had more control over their odds of winning than once thought, but Monty manipulates some of the build-up by choosing which items to reveal at different steps in the game.\nUnfortunately, many probability theorists and mathematicians took issue with the lack of clarity in the problem (context is important sometimes). This provides a face-saving ‘other version’ for the geeks who get it wrong the first time. But whenever I hear comments like, “Okay, given this version, that Monty knows where the car is.” I usually think, “Of COURSE he knows where the car is! There is no other way to play the game!” and wish that people were more able to accept that they are just as human as everyone else.\nThe problem itself is written clearly, though: it specifically states that a door without a car behind it is revealed before you are given the option to switch. If the situation was a fully-randomized, double-blind game (like “Deal or No Deal”), then the option to switch would not even be on the table if the car is behind the revealed door. There would be no problem in that case. Therefore, the problem is a question of whether you should switch in a controlled setting – one in which the only participant who doesn’t know the location of the big prize is you.\nThe issue of knowledge is a factor in our processing of the problem, but it’s not what Monty knows that matters. It’s what you (the subject of the problem) know.\nSo, let’s put that complaint behind us and get back to the problem.\nThe Answer, and How to See it for Yourself\nHopefully, if the problem is new to you, you’ve spent some time trying to solve it instead of going with your first gut feeling, which was probably, “It doesn’t matter.”\nIt does. You should switch.\nIf you don’t believe me, try running some simulations. You’ll have to run a lot in order to get a large enough sample to be certain to see the trend, but here are a few ways to do it:\n- Use your favorite program (MATLAB, R, etc.). There is a good database of pre-written simulators for this here. I am partial to Excel myself, even though it’s a bit more cumbersome. I just don’t remember enough code to use another program.\n- Use a web-based simulator. Do it at least a hundred times, choosing to switch for half of the trials, and keep a tally your results.\n- Use a die to simulate the outcome, assigning 1-3 to “Door #2″ and 4-6 to “Door #3″ (e.g., if you roll a 5, Door #3 is the one with the car). Roll at least a hundred times, choosing to switch for half of the trials (before rolling!). Keep a tally of the results.\nWhat you will see is that switching will result in winning a car in approximately 2/3rds of the trials while staying will only provide a win in 1/3rd of them.\nI know what you’re thinking. “But, there are only two doors left, so it should be 50/50!”\nWhy it is so Difficult to Accept\nHuman cognitive development is an interesting process. We learn to interpret information from the environment very quickly so that we can respond to that environment, but learning to reason hypothetically takes more time. Even adults with scientific training have a difficult time separating the concept of variables (each has a set of possible values) and data (values which are known).\nIn practice, hypothetical situations are often conditional (e.g., “If A, then B”). We tend to use information about what is to reason about what could be. We do this because it often works, but it is one of the ways in which our brains can lead us astray. For example, given the premise, “If I study, I will get a good grade on the exam”, what is the most sound conclusion when presented with a good grade? The most common response is, “I must have studied”, but that is not sound. In this premise, studying provides a guarantee for a good grade, but there is no statement that studying is the only way to get a good grade. It does not, for example, read, “If and only if I study…”\nIn the case of the Monty Hall Problem, the probability of winning is set before you pick a door. No matter which door you choose, the probability is 1/3rd. This is because there is a 1/3rd probability that the car is behind the door you chose given the information you had when you chose it. In reality, the car is behind one of the doors, so the probability it is behind Door #2 is 100% if it is there and 0% if it is not there. Probability is not a useful way to discuss what is or what happened; it is a tool for predicting what is likely to be true/happen.\nThe new information provided by revealing a loser changes the circumstances and this where we get trapped in our representations of models and data, possibilities and facts.\nYou had a 1/3rd chance of winning because there were three, equally-likely locations to choose from. It seems as if cutting the choice down to two should change the odds of winning to 1/2. It seems that way because we are focused on the probability that a given piece of information is true (e.g., that the car is behind Door #1) and not the probability that an event will occur (e.g., that we will win the car). The probability that we will win the car relies on the number of possible states of reality. This, in turn, initially relies on the number of locations for the car. When the situation changes, we try to adjust probabilities based on possible locations (which has changed) rather than on the number of possible states of reality (which has not).\nBasically, when Monty makes the second offer, the offer is to switch from the door we have (#3) to the door we don’t have (#1 or #2). It does not matter that only one of those doors is left; there is still only a 1/3rd chance that our door has the car and a 2/3rd chance that the set of the other two contains the car.\nIf you change the way you represent the problem from the beginning, the solution might seem more reasonable. Specifically, instead of thinking in terms of assigning probabilities to doors, think in terms of assigning probabilities to outcomes: winning versus losing.\nLet’s go step by step…\nMonty asks you to pick a door from three choices. Behind one of those doors is a car. There are three possible locations and it must be in one of them, so there are three possible states of reality.\nYou choose to bet on Door #3; there is a 1/3rd chance that you will win the car.\nThere is a 2/3rd chance that you will not win the car.\nThis would be true no matter which door you chose.\nMonty reveals that one of the remaining doors is a loser. At least one will be a loser since there is only one winner and you can choose only one. The car, however, does not move. Even though there are only two locations left, so there are still three possible states of reality. What’s changed is that we now know more about each of those possible states (there are fewer locations for the car to be):\nSo, if we model the problem in terms of the probability of winning with Door #3, the model itself does not change after the losing door is revealed. What changes is that we would no longer want to choose that door, so it is no longer among our options. This leaves us with only two options: keep the door we have or switch to the remaining door. The odds of winning/losing with Door #3 have not changed, but eliminating an option allows us to make a better choice – switch.\nBarbara Drescher is a former educator and researcher, having taught research methods, statistics, and cognitive psychology at CSU Northridge for a decade. At ICBSEverywhere.com, Barbara evaluates claims and studies, discusses education, and promotes science and skepticism."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:8f119e60-413a-48e3-b616-de8c08ee6bde>","<urn:uuid:0403a403-4ab0-4c4e-94b9-7d8967fdd539>"],"error":null}
{"question":"How do the side effects compare between second-generation antihistamines used for urticaria and allergy immunotherapy shots?","answer":"Second-generation antihistamines are generally safer, without the sedative and anticholinergic side effects of first-generation antihistamines. Allergy immunotherapy has different risks - the most common side effect is local redness, itching, or swelling at the injection site, but it can rarely cause more serious systematic reactions including itching, hives, throat tightness, wheezing, shortness of breath, dizziness, and in extremely rare cases, life-threatening reactions.","context":["Urticaria is a superficial swelling of the skin (epidermis and mucous membranes) resulting in a red, raised, itchy rash that can be localised or widespread. Angio-oedema is a deeper form of urticaria that causes swelling in the dermis and submucosal or subcutaneous tissues. The name urticaria is derived from the common European stinging nettle Urtica dioica.\nUrticaria is a common condition, affecting approximately 15% of people at some point during their lives. It can be acute or chronic, with the acute form being much more common. Acute urticaria is most common in children and affects women more commonly than men, particularly in the 30-60 age range. It is also more common in atopic individuals. Chronic urticaria affects between 1-2% of the population at some point during their lives, and approximately two-thirds of those affected are women. It has several genetic and autoimmune associations.\nAcute urticaria typically occurs due to type I hypersensitivity reactions, which are allergic reactions provoked by re-exposure to a specific antigen, called the allergen. These reactions are mediated by IgE and tend to occur 15 to 30 minutes from the time of exposure to the allergen. The reaction results in the activation of mast cells in the skin resulting in the release of histamine and other mediators. These chemicals cause capillary leakage, which causes the swelling of the skin, and vasodilation causing the erythematous reaction.\nAn identifiable trigger is only found in around 50% of cases of acute urticaria. Common triggers include:\n- Allergies, e.g. foods, bites, stings, drugs.\n- Skin contact with irritants, e.g. chemicals, nettles, latex.\n- Physical stimuli, e.g. firm rubbing (dermatographism), pressure, extremes of temperature.\n- Viral infections.\nChronic urticaria can also occur due to type I hypersensitivity reactions, but in around 50% of affected people, type II hypersensitivity reactions are implicated. These reactions, which are sometimes referred to as cytotoxic hypersensitivity, occur when antibodies produced by the immune response, bind to antigens on the patient’s own cell surfaces.\nUrticaria can be classified according to the timescale over which it occurs:\n- Acute urticaria – where symptoms last less than six weeks (and usually develop and resolve very quickly, often within 48 hours).\n- Chronic urticaria – where symptoms persist for six weeks or longer, on a nearly daily basis. Fewer than 5% of cases last longer than six weeks.\nChronic urticaria can be further classified as\n- Chronic spontaneous urticaria – which has no identifiable external cause but may be aggravated by heat, stress, certain drugs, and infections.\n- Autoimmune urticaria – which is characterised by the presence of IgG autoantibodies to the high-affinity receptor for IgE (Fc epsilon R1).\n- Chronic inducible urticaria (CINDU) – which occurs in response to a physical stimulus and can be further classified according to its cause as aquagenic, cholinergic, solar, cold, heat, dermatographism, delayed pressure, vibratory, and contact urticaria.\nThe typical skin lesion seen in urticaria is the wheal (or weal). Wheals typically consist of three features:\n- A central swelling that is red or white in colour and usually surrounded by an area of redness (the flare).\n- They are generally very itchy, and this is sometimes accompanied by a burning sensation.\n- They are usually fleeting, with the skin returning to its normal appearance within 1–24 hours.\nWheals can vary significantly in size from just a few millimetres to lesions that can be as large as 10 cm in diameter. There may be a single lesion, or numerous lesions and lesions may coalesce to form large patches. They may also be associated with swelling of the soft tissues of the eyelids, lips and tongue (angio-oedema).\nThe diagnosis is usually made clinically, and investigations are not usually required, particularly for acute urticaria. A detailed history should be taken to attempt to identify any possible triggers.\nChronic or recurring cases may require investigations, and these should be guided by history and presentation.\nFor mild cases with an identifiable trigger, urticaria is often self-limiting with no treatment required. If a trigger is identified, the patient should be given clear advice on avoidance tactics.\nThe current NICE guidelines advise that people requiring treatment should be offered a non-sedating (second-generation) antihistamine for up to six weeks. Conventional first-generation antihistamines (e.g. promethazine and chlorpheniramine) are no longer recommended for urticaria because they are short-lasting, have sedative and anticholinergic side effects, can impair sleep, learning and performance and can interact with alcohol and other medications. Lethal overdoses with first-generation antihistamines have also been reported.\nExamples of second-generation antihistamines include cetirizine, loratadine, fexofenadine, desloratadine and levocetirizine. If the standard dose (e.g. 10 mg for cetirizine) is not effective, the dose can be increased up to fourfold (e.g. 40 mg cetirizine daily). There is not thought to be any benefit from adding a second antihistamine. Terfenadine and astemizole should not be used, as they are cardiotoxic in combination with certain drugs, e.g. erythromycin and ketoconazole.\nAntihistamines should be avoided where possible in pregnancy. There are no systematic studies of safety in pregnancy, and chlorphenamine is often the first choice if an antihistamine is required in this situation. Loratadine or cetirizine are preferred in breastfeeding women.\nIf symptoms are severe, a short course of an oral corticosteroid can be given (e.g. prednisolone 40 mg for up to seven days), in addition to the second-generation antihistamine.\nThe current NICE guidelines recommend referral to a dermatologist or immunologist for:\n- People with urticaria that is painful and persistent (suspect vasculitic urticaria)\n- People whose symptoms are not well controlled on antihistamine treatment. Secondary care treatment options include leukotriene receptor antagonists (montelukast and zafirlukast), cyclosporine, omalizumab, mycophenolate mofetil, or tacrolimus.\n- People with acute severe urticaria which is thought to be due to a food or latex allergy.\n- People with forms of chronic inducible urticaria that may be difficult to manage in primary care, for example, solar or cold urticaria.\nThe current NICE guidelines recommend referral to a clinical psychologist for people whose symptoms are adversely affecting their quality of life, for example, causing significant social or psychological problems.\nUrgent hospital admission is indicated if acute urticaria rapidly develops into angio-oedema or anaphylactic shock.\nHeader image used on licence from Shutterstock\nThank you to the joint editorial team of www.mrcgpexamprep.co.uk for this ‘Exam Tips’ post.","Allergy Immunotherapy (Allergy Shots)\nALLERGY INJECTIONS OR IMMUNOTHERAPY – FAQ\nWhat is immunotherapy?\nImmunotherapy involves injections of the very things to which you are allergic called allergens. For example, if your skin test showed that you are allergic to trees; small amounts of tree pollen extract will be used in your allergy vaccine.\nWill my insurance cover my allergy injections?\nInsurance coverage varies. Some insurance companies require that you pay a co-payment for an allergy injection. Please call our billing office if you need more information.\nHow do allergy shots work?\nBy gradually increasing the dose over time, your body will build up a resistance to the allergen. You will then have fewer symptoms when exposed naturally. If you are getting shots for your tree allergy you should notice less sneezing, itchy eyes, etc. in the spring when tree pollen is in the air. Allergy shots help only with those allergens included in your injections. Tree pollen shots will not help a cat or dust allergy.\nHow soon should do the shots start to work?\nBecause the injections are started at a very low dose, they generally do not start working for about three to six months. Sometimes it takes a year or two to notice significant improvement.\nHow often do I get the injections? Allergy shots are given weekly in the buildup period, which is generally about six months or 25 weeks. Once a fairly high dose called maintenance is reached, the shots are gradually given less often. They are usually continued every four weeks. During a pollen season more frequent injections can be helpful. Injections also may be given twice a week, with a separation of at least 2 days. This cuts down on the buildup time to 13 weeks.\nWhen can I get the shots?\nIncluded is an injection schedule with our office walk-in hours. If your primary care physician’s office is more convenient for you, we will provide complete instructions so you may take your allergy vaccine there. A physician must always be available in the office when you receive your injection. The precautions listed on this sheet are even more important to follow if your injections are given at another doctor’s office. If you have a fever or are feeling ill you may not be able to receive an injection on that day. You should call ahead if unsure.\nHow long do I need to take shots for?\nAllergy injections are usually given for three to five years, though a few people benefit from a longer course of treatment. If the injections do not start to help within two years they should probably be stopped or changed. A yearly checkup with the doctor to review progress is required while on injections.\nWhat happens if I miss a shot?\nMissing an occasional shot or being late does not affect the long-term benefit. The doctor or nurse will adjust the dosage to a safe level. If you miss shots for many weeks you may not build up to a high enough dose for them to be helpful. Consult with our clinical staff if you will need to miss several visits.\nAre allergy injections safe?\nAllergy injections have been used for more that 80 years with no serious long-term side effects. They can cause an allergic reaction starting shortly after the shot is given.\nWhat are the side effects of allergy injections?\nMore serious allergic reactions, called systematic reactions are rare. Symptoms include itching, hives, throat tightness, wheezing, and shortness of breath, dizziness or light-headedness. Life-threatening reactions are extremely rare but a few deaths have occurred after allergy shots. For this reason you should always stay in the doctor’s office for 30 minutes after your injections no matter how long you have been getting them.The most common side effect of an allergy injection is a small amount of redness, itching or swelling at the injection site. Sometimes a person will notice some mild allergy symptoms within 24 hours after a shot. Tell the nurse at your next injection visit so an adjustment in dose can be made if it is indicated.\nHOW CAN I PREVENT SERIOUS REACTIONS? – Take these precautions!\n- Wait the 30 minutes in the doctor’s office after injections.\n- Inform the nurse before your next shot of any swelling at the injection site that appears larger than the size of a quarter.\n- Always, call us immediately or go directly to the emergency room if you notice any breathing difficulty, generalized itching or rash, or feel especially ill shortly after your shot.\n- Let us know as soon as possible if you become pregnant or plan on becoming pregnant.\n- Inform us of any new medications that you are taking. Certain medications such as beta-blockers (taken for high blood pressure, glaucoma, migraine headaches and other diseases) or monoamine oxidase inhibitors can be dangerous if you have a reaction to a shot. PATIENTS CANNOT BE TAKING BETA-BLOCKERS AND GET ALLERGY INJECTIONS.\n- Asthma patients need to let us know if they are having any current problems. When peak flow readings are less than 70% of your best, you should not receive a shot that day. Speak with our clinical staff if you are unsure.\n- If you have a fever or are feeling ill, you may not be given a shot. You will not be given an injection if you have hives or wheezing. Call ahead or inform clinical staff before your injection.\n- Avoid strenuous exercise at least an hour after your injection.\nShould I still take allergy medication?\nThe goal of immunotherapy is to control your allergy symptoms without medications. You may still need to take some medications to control your allergy symptoms. Using a medication will not decrease the benefits of allergy shots.\nDo I still continue to see the allergy doctor?\nYes. A yearly visit is needed to monitor your allergies and general health, update progress, reassess medications and reissue any prescriptions. A more frequent visit may be required for asthmatic patients. If you receive injections at another office, you should bring your injection schedule with you as well as any empty serum vials if you are renewing a serum prescription at the same time.\nHow long will my current vaccine last? Your vaccine expires in one year. Your new vaccine will be a little stronger (vaccine loses some of its potency over time) so the dosage will be cut back slightly and you will have to build back up. You can do this by coming in twice a week, weekly or every two weeks. It usually takes 3 doses to be back up to maintenance.\nWE WANT ALLERGY INJECTIONS TO IMPROVE THE QUALITY OF YOUR LIFE. IF YOU HAVE OTHER QUESTIONS OR CONCERNS, PLEASE TALK WITH US."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:bb198b2c-073b-4280-ad1d-7d107aab5cc2>","<urn:uuid:4834d423-2911-45bc-a87d-798bf3b971ce>"],"error":null}
{"question":"I'm a nutritionist looking to understand food behaviors: What are the biological reasons behind children's picky eating, and how can community gardens help address nutritional challenges?","answer":"Children's picky eating is actually an evolutionary feature that develops when they start walking. It's characterized by food neophobia, peaking around ages 2.5-3 and persisting until age 9. This behavior exists because as children become mobile, they need to protect themselves from potentially dangerous foods - with bitter tastes (common in vegetables) potentially indicating toxicity and sour tastes suggesting spoilage. As for community gardens, they help address nutritional challenges by providing fresh, affordable produce in urban areas where access to nutritious food is limited, especially in low-income neighborhoods with transportation constraints. They also offer educational opportunities about food growing and harvesting, helping people understand where their food comes from and its nutritional value.","context":["Written By: Alan Greene, M.D.\nMeal times are a unique chance for the whole family to gather around and invest in a common experience. This is an opportunity to share and bond with one another. There are, however, a few things that may make mealtime somewhat anxiety-inducing, including children who are very choosy about what they will or won’t eat.\nSome children are particularly sensitive to taste, scents, or textures. Regardless, most children go through a phase called food neophobia that begins around the time they start to walk. Neophobia is characterized by a fear of new things, particularly foods or unfamiliar elements in the environment. It slowly increases from about age 2.5 to 3, and then remains steady until around 9 years old.\nThe reason for this is an evolutionary intelligence – as children begin to walk and are able to toddle away from their caregivers, there is an increase in the likelihood of encountering something dangerous, whether it is an animal, a potentially toxic food, or unsafe territory in general. Taste plays a very important role in this phenomena. Sweet and salty flavors the body generally recognizes as safe and nutrient-rich. Bitter flavors, often present to some degree in vegetables, can also indicate potential toxicity. Sourness tells the body that a food may be spoiled, and therefore dangerous to ingest. It is important to recognize, then, that “picky eating” is a feature, and not a flaw. It is a reflection of the body’s ability to distinguish between safe and potentially harmful foods, which is quite remarkable.\nBuilding Your Baby’s Nutritional Intelligence\nThe easiest time to help develop your baby’s nutritional intelligence (the term I use to describe the ability to recognize and enjoy healthy amounts of good food) is during the third trimester of pregnancy. During that window babies are tasting what mom eats as they swallow amniotic fluid. In fact, babies have more tastebuds at that time than any other in their lives.\nIn order for children to be open to a wider variety of foods, specifically fruits and vegetables, it’s most helpful to introduce them before they are walking well, around 15 months. At this point their palate and overall chemistry is more receptive to new foods. During this time, babies are also more likely to enjoy the foods they’ve already tasted in utero.\nDo As I Do\nOther things that have a major influence on what children will eat include watching what their parents and older siblings eat. There is often a fear that something unfamiliar will taste bad, and so the goal is to decrease fear and increase familiarity. Studies have shown that even if a child sees their parents choose a particular brand one time, a year later they are still predisposed to choose the same brand. It is important not to underestimate how much children learn by observation.\nBy the same token, other trusted “authority figures” also have an influence. Like it or not, children gravitate towards packaging with cartoon characters. In one experiment, children were 50% more likely to want broccoli when it had a sticker of Elmo on it. This can be helpful when it’s applied to healthy foods, but not so much when you’re trying to avoid over-processed foods covered in fun cartoon pictures.\nOvercoming Picky Eating\nEven if children were not exposed to a wide variety of fruits and vegetables at an early age, there is research showing there are ways to overcome this. A study done to encourage cats to eat bananas, which they typically shy away from, concluded that when a sense of pleasure was associated with an undesirable food, over time the flavor would become desirable, even when the pleasure reward was revoked. The fascinating aspect was that baby kittens were watching their mothers eat the bananas, and were not exposed to the same pleasure reward. The kittens still willingly ate the bananas simply because they learned from watching their mothers do it.\nPreparing food together is another way in which children will be more excited to try new things. Children want to be included in all aspects of daily life, and they want to be praised for what they have done. Cooking together naturally makes kids more excited to try the end product. Studies have shown that involving children in preparing and cooking food makes them much more likely to eat vegetables, and even more so if they were able to pick the vegetables out themselves. To create an even deeper connection, growing and nurturing vegetables in the garden, then picking them and preparing them together makes for the most rewarding experience of all.\nAnother way to incorporate new foods is to include something new and different into something children already like, such as adding vegetable toppings to pizza, or mixing them into a sauce. These can be seen as “flavor bridges.” It can also be helpful to reduce foods with added sugars. This gives the palate a break, and helps it to recognize the natural sweetness in fruits and veggies, making them more enticing.\nAre Bribes Okay?\nSome parents may be familiar with the idea of offering a sweet treat in exchange for finishing a serving of veggies. Historically there has been some controversy around whether “bribery” is an effective tactic. However, a study from The Journal of Health Economics showed that when a reward was put in place for eating more fruits and vegetables, not only did that increase their consumption, but children were still more likely to make healthy choices even after the reward was removed. In this sense it’s possible that children were encouraged to overcome a fear of unfamiliar flavors, able to make them more familiar, and thus more likely to be eaten in the future.\nAbove all it’s important to be patient, and try, try again. If children are resistant to healthy foods, encourage a culture of the “no, thank you,” bite. Even if a child thinks they don’t want a certain food, make it a habit to include everything on the plate at each meal, with the expectation that they must try at least one bite of everything. Once kids are solidly in the neophobia stage, on average it may take up to 89 tries before a new food becomes likeable. It may sound like a lot, but each individual bite is a step towards that goal. The “no, thank you,” bite also teaches a culture of politeness and gratitude around food. Even if a child doesn’t like a particular food, they can still learn to be grateful for the long and laborious process of growing, shipping and preparing the food before it lands on their plate.\nFor more ways to encourage healthy food choices, listen to our podcast here.","The Michigan Urban Farming Initiative is a 501(c)(3) nonprofit organization that seeks to engage members of the Michigan community in sustainable agriculture. We believe that challenges unique to the Michigan community (e.g., vacant land, poor diet, nutritional illiteracy, and food insecurity) present a unique opportunity for community-supported agriculture. Using agriculture as a platform to promote education, sustainability, and community—while simultaneously reducing socioeconomic disparity—we hope to empower urban communities.\nOur core values reflect what is truly important to MUFI and its members. They provide the foundation for our motives, strategies, and implementation. The core values of MUFI are as follows\n\"Give a man a fish and he will eat for a day; teach him to fish and he will eat for life.\" By viewing urban farming and gardening as an educational opportunity we are hoping to provide a long term solution to the problem of food insecurity in urban areas. When people develop new skills they become actively engaged in the learning process. Through a combination of workshops and fieldwork, we hope to educate the citizen farmers and provide hands-on experience necessary for successful food production.\nMost food is currently produced with synthetic fertilizers and pesticides that are derived from fossil fuels. Fields are often planted with monoculture crops that deplete the soil of the same nutrients. This food is then processed and transported far distances by diesel burning semi trucks. The reliance of our nation on the finite resources of fossil fuels is polluting our atmosphere and compromising our security. Growing food locally minimizes environmental impacts on a local and global scale. When waste is recycled and crops are rotated, minimal fertilizer is needed to maintain high yields. Through using abandoned and vacant land, we also lessen the need to develop more farmland in rural areas.\nBy building relationships with the community we are better able to acheive our objective. Far to often organizations identify struggling populations, raise some money, build something, and give themselves a pat on the back as they leave, never to be seen or heard from again. This is not an effective strategy in engendering long term and sustainable change.\nSome challenges we hope to target:\nSome recent additions to the Michigan scenery include abandoned buildings and houses, unkempt land, and other poorly used spaces. Redeveloping these locations into food producing plots would be make them valuable assets to any community.\nWith the current state of Michigan's economy, a large community of unemployed people exists. These people are not bound by the constraints of 9-to-5 employment and may have more time available to participate in community service projects. Community farming can support a healthy lifestyle, especially in times of limited income.\nAccess to nutritious food\nUrban areas have particular difficulty providing consistent access to nutritious food and fresh produce. Such circumstances are particularly acute in low-income neighborhoods, where people may not have access to transportation. Local urban gardens and farms provide a source of fresh, affordable produce available to the whole community.\nFood miles and nutrition information\nMany people are disconnected from their food and where it comes from. We intend to provide ongoing educational opportunities for the community concerning the growing and harvesting of produce, in addition to its nutritional value. We want people to develop a certain consciousness about where their food comes from and their role in the process."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:95aa790c-4274-4bd2-a8d8-6808fddfb757>","<urn:uuid:d0f2ab56-9423-4883-8ce7-b0409a87d0f2>"],"error":null}
{"question":"What is the geographical significance of the Mississippi River, and what environmental challenges does it currently face?","answer":"The Mississippi River is one of the United States' defining geographic features, being the fourth longest river in the world and neatly dividing the country into east and west. It begins in Minnesota and flows south through ten states before emptying into the Gulf of Mexico. The river and its tributaries drain more than half of all U.S. states. However, the river currently faces significant environmental challenges, particularly due to climate change and illegal levee development. Climate change is causing more frequent and severe flooding in the Upper Mississippi River Basin, with the magnitude of 100-year flood events increasing by 20% over the past 500 years. Additionally, unauthorized levee raising between Muscatine, Iowa, and Hamburg, Illinois has occurred without proper permits, which destroys critical riverside habitat and increases flood heights downstream.","context":["Map Of The Mississippi River\nHere we will provide you with maps of the Mississippi River. The river is so large that we felt it necessary to provide multiple maps of this diverse region.\nAs you may well know the River is so large that it actually splits the Unites States in half. Here is a map of the Mississippi River that illustrates that fact:\nAnd since the River is so long, stretching from Minnesota to the Gulf of Mexico, here are two maps. One map will show the Upper Mississippi River and the other the Lower Mississippi River:\nThe Mississippi RIver is such an important waterway for transporting goods and services, here is a map of the ports that are located in Louisiana:\nThese maps should provide you with a great perspective on this wonderful River. “Old Man River,” “Big Muddy,” “Fathers of Waters,” and the mighty Mississippi, all refer to the longest river in North America. Historians believe the name is based on Native American descriptions of the great waterway. No matter what name people use for it, there are a number of reasons the Mississippi River is one of the United States’ defining geographic features. That is the reason we have provided so many maps here for you to enjoy.\nThe river neatly divides the country into east and west, and its place in history as the portal to the west, is exemplified by the dramatic Gateway Arch in St. Louis. More recently, critics and fans alike use terms like “East Coast” and “West Coast” to describe hip hop’s primary division. Further, individuals routinely think of the country as divided by the Mississippi, despite the fact that the waters have not been an impediment to travel for many years.\nNo less than ten states share at least one shore with the river and, more impressively, the Mississippi and its tributaries drain more than half of all U. S. states. It is the fourth longest river in the world. These dramatic features could not help but impact the country’s growth, and cement a place for it in the nation’s culture and mythology. The maps above show those tributaries and the vast length of the river.\nThe maps above show how the Mississippi follows begins in Minnesota, then continues almost directly south meeting the states of: Wisconsin, Iowa, Illinois, Kentucky, Missouri, Tennessee, Arkansas, Mississippi and finally slices through Louisiana, and emptying into the Gulf of Mexico. Six tributaries of the main river, the Missouri, Illinois, Arkansas, Ohio, Tennessee and Red Rivers further subdivide the country, and during its primary growth phase, the main river and tributaries provided means for the rapid movement of people and supplies.\nWhile the Mississippi bisects the country, the state of Louisiana is divided by both the mighty river and the Atchafalaya River, an important fact given the larger river’s tendency to deviate towards the smaller. A permanent change in the Mississippi’s course would have innumerable consequences for the Delta area generally, and Louisiana specifically.\nNative Americans, explorers, river boat captains, businessmen and writers all helped create the great river’s image. Two of the nation’s most storied authors, Mark Twain and William Faulkner, used the Mississippi in their works. In fact, for Twain, the it is all but a cornerstone for his writings. For authors, and the nation, the waterway was, and perhaps always shall be, a symbol of the country’s dynamic growth and accomplishment.\nWe hope you find these maps of the Mississippi River helpful.","Climate change and illegal levee development threaten public safety, river health\nEileen Shader, American Rivers, (570) 856-1128\nElliot Brinkman, Prairie Rivers Network, (217) 344-2371, x202\nChristine Favilla, Sierra Club, (618) 462-6802\nDavid Stokes, Great River Habitat Alliance, (314) 918-1007\nMelissa Samet, National Wildlife Federation, (415) 762-8264\nRob Moore, Natural Resources Defense Council, (312) 651-7923\nMaisah Khan, Missouri Coalition for the Env., (314) 727-0600, x113\nWashington, D.C. – American Rivers today named the Upper Mississippi River among America’s Most Endangered Rivers® of 2019, citing the grave threat that climate change and illegal levee raises pose to public safety and river health. American Rivers and its partners called on state and federal agencies to prohibit the reckless raising of levees and promote better flood protection solutions.\n“The America’s Most Endangered Rivers report is a call to action to save rivers that face a critical decision in the coming year,” said Eileen Shader with American Rivers. “It’s time to stop the illegal levee development on the Upper Mississippi that is putting people and river health at serious risk.”\n“We are already feeling the impacts of climate change in the Midwest, including more frequent and severe flooding, and it’s only going to get worse. Abusing and degrading the Mississippi River will make us more vulnerable to these threats. Protecting and restoring the river will make us better prepared to face future floods and safeguard communities. It’s our choice to make.”\nThe Upper Mississippi River is threatened by levees that are being raised (i.e., made taller) without required permits and approvals. Eighty miles of levees between Muscatine, Iowa, and Hamburg, Illinois, have been raised without obtaining the required state or federal approvals. These levees not only destroy critical riverside and floodplain habitat for fish and wildlife, they also make flood heights higher and increase flooding downstream. For example, during the most extreme flood events, Hannibal, Missouri, is projected to experience an additional foot or more of floodwaters because their neighbors have raised their levees without regard to the impacts.\nAmerican Rivers and its partners called on the U.S. Army Corps of Engineers, the Federal Emergency Management Agency, and the states of Illinois, Iowa and Missouri to take corrective action to stop and resolve these levee violations. Further, the groups called on the agencies to advance 21st century flood protection solutions that deliver multiple benefits to people and nature.\n“We have to stop the circle of absurdity where we spend enormous sums of money to build larger levees which make the next flood even worse and costs us millions more in emergency funds, only to have the entire cycle repeat year after year,” said David Stokes with the Great River Habitat Alliance.\n“We must move beyond this outdated vision of flood control that foolishly relies on bigger and higher levees and floodwalls to a new vision that makes room for rivers and allows nature-based solutions to protect us,” said Elliot Brinkman with the Prairie Rivers Network.\nThe threats posed by these unlawful changes are real and getting worse as climate change is leading to more frequent floods and intense storms in the Upper Mississippi River Basin and across the country. The three highest-volume rain storms ever recorded in the U.S. have occurred in the last three years, in line with climate scientists’ projections that extreme downpours in the U.S. could increase by 400 percent by the end of this century. Munoz et al. (2018) determined that the magnitude of 100-year flood events in the Mississippi Basin has increased by 20 percent over the past 500 years, with much of that increase being caused by the combination of river engineering and climate change. The increased risk of flooding is the reason some levee districts have pursued higher levees, but their actions are intensifying the impacts of flooding for their neighbors.\nThe Mississippi River is a globally significant flyway used by hundreds of species of birds and provides unique habitat for fish, mussels, reptiles and mammals. This significant ecosystem supports commercial and recreational fishing, hunting and boating, which contribute $24.6 billion to the region’s economy and an estimated 421,000 jobs. The river has been the lifeblood of many cultures throughout history and has served as the inspiration for a rich heritage of American music, art and literature.\nThe annual America’s Most Endangered Rivers® report is a list of rivers at a crossroads, where key decisions in the coming months will determine the rivers’ fates. Over the years, the report has helped spur many successes including the removal of outdated dams, the protection of rivers with Wild and Scenic designations, and the prevention of harmful development and pollution.\nSome portion of the Mississippi River was previously included on this list in 1991-1992, 1994-1996, 2000-2001, 2004, 2011, 2014 and 2018. Other rivers in the region listed as most endangered in recent years include the Buffalo National River (2019, 2017) and Middle Fork Vermilion River (2018).\nAmerica’s Most Endangered Rivers® of 2019\n#1 Gila River, New Mexico\nGov. Grisham must choose a healthier, more cost-effective way to provide water to agriculture than by drying up the state’s last major free-flowing river.\n#2 Hudson River, New York\nThe U.S. Army Corps of Engineers must consider effective, nature-based alternatives to storm-surge barriers that would choke off this biologically rich tidal estuary.\n#3 Upper Mississippi River, Illinois, Iowa, Missouri\nState and federal agencies must enforce laws that prohibit illegal levees, which increase flood risk for communities and degrade vital fish and wildlife habitat.\n#4 Green-Duwamish River, Washington\nLocal leaders must produce a flood protection plan that safeguards communities and restores habitat for chinook salmon — fish that are essential to the diet of Puget Sound’s endangered orca whales.\n#5 Willamette River, Oregon\nThe U.S. Army Corps of Engineers must immediately improve 13 dams to save wild chinook salmon and steelhead from going extinct.\n#6 Chilkat River, Alaska\nThe Japanese investment firm, DOWA, must do the responsible thing and back out of a mining project that could decimate native salmon.\n#7 South Fork Salmon River, Idaho\nThe U.S. Forest Service must safeguard endangered fish by denying a mining proposal that could pollute this tributary of the Wild and Scenic Salmon River.\n#8 Buffalo National River, Arkansas\nGov. Hutchinson must demand closure of an industrial hog-farming facility that pollutes groundwater and threatens endangered species.\n#9 Big Darby Creek, Ohio\nLocal leaders must use state-of-the-art science to craft a responsible development plan that protects this pristine stream.\n#10 Stikine River, Alaska\nThe International Joint Commission of the United States and Canada must protect the river’s clean water, fish and wildlife, and indigenous communities by stopping harmful, polluting mines.\n2019’s “River of the Year”: Cuyahoga River, Ohio\nAmerican Rivers celebrates the progress Cleveland has made in cleaning up the Cuyahoga River, fifty years since the river’s famous fire that sparked the nation’s environmental movement.\nABOUT AMERICAN RIVERS\nAmerican Rivers believes every community in our country should have clean water and a healthy river. Since 1973, we have been protecting wild rivers, restoring damaged rivers and conserving clean water for people and nature. With headquarters in Washington, D.C., and offices across the country, we are the most effective river conservation organization in the United States, delivering solutions that will last for generations to come."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:a290e9db-eb8c-4f79-8b0f-e9d198bd4409>","<urn:uuid:5946720d-7051-47b0-a1fd-3eb3da363daa>"],"error":null}
{"question":"When should you see a doctor for allergies vs colds?","answer":"For allergies, you should see a doctor when allergy medications aren't working, for preventive visits before allergy season, or if you can't determine whether allergies are causing your symptoms. For colds, medical attention is needed if fever (over 100 degrees F) lasts more than three days, as this could indicate a bacterial superinfection. Additionally, you should consult a doctor if symptoms change to include ear pain, thick yellow/green nasal discharge for more than 10 days with headaches and pressure, worsening cough with fever and breathing difficulty, sudden onset of sore throat with high fever, or stiff neck and wheezing, as these could indicate bacterial infections requiring antibiotics.","context":["If you find yourself suffering from sneezing, itchy eyes, and pressure headaches every spring and fall, welcome to the seasonal allergy club. Here are some common questions we get about pollen allergies — and how to survive them without losing your mind.\nHow do I know if I have allergies, a cold, or a sinus infection?\nIt can be tough to tell sometimes. Unlike allergies, a cold or respiratory infection often brings on a fever, so that’s a key indicator. If your nasal congestion clears up within one to two weeks, chances are you had a cold rather than allergies. Seasonal allergy symptoms tend to last three weeks or more, depending on the types of pollen you’re allergic to. Lastly, differentiating between allergies, cold, and sinus infection is one of those times when you actually can consider mucus color. A thin, clear discharge generally points to allergies, while an opaque, yellow/green, or thick discharge usually indicates a cold. There’s a common misconception that yellow or green mucus signals a bacterial infection, but that’s not a reliable indicator. Other allergy symptoms include throat irritation, occasional wheezing, skin rash, fatigue, and frequent sneezing.\nSore throat? Try Sore Throat Tea.\nI’ve had symptoms for three weeks without a fever, but my mucus has changed from clear to green. Should I see a doctor?\nYou should consult your health care provider whenever your allergy medications aren’t working, for a preventive visit before allergy season, or if you can’t tell whether allergies are causing your symptoms. Even if it turns out you don’t have a cold or sinus infection, your appointment will give you the opportunity to collaborate with your health care provider on a treatment plan for your nasal allergies.\nIs it possible to prevent allergies?\nYou can’t prevent seasonal allergy symptoms from occurring, but you can do things to minimize the impact of the symptoms on your quality of life. Here are some steps I recommend you take before and during allergy season:\n- Start a steroid nasal spray about four to six weeks ahead of pollen season.\n- Stay indoors during peak pollen hours, such as mornings. On high-pollen days, stay indoors as much as possible.\n- Do your seasonal cleaning before pollen’s in the air. You may still have to cope with dust allergies while cleaning, but at least you won’t have to deal with dust and pollen.\n- Keep windows closed when pollen counts are high.\n- Place air filters inside your air conditioning vents to prevent pollen from getting blown into your living space.\n- Get a dehumidifier. These devices also filter the air in your home, which may help improve your indoor air quality.\n- Remove outer garments outside and shake them off before bringing them indoors. Even though you may not be able to see pollen on your clothing, it’s there.\n- Wash pollen down the drain by showering as soon as you get home. Pollen sticks to your hair and skin, so rinsing it off will help keep it out of your bed, carpet, and furniture.\nDon’t miss Tried and True Treatments for Allergy Symptoms.\nWhat allergy medications do you recommend?\nFor most people, over-the-counter allergy medications are a good first-line defense. You might need to try different pills to find the one that works best for you. Use a non-sedating allergy pill for daytime, and diphenhydramine (Benadryl) at night as needed. If allergy pills alone aren’t working, consider trying Nasacort brand steroid nasal spray, which is now available over the counter. It’s the same strength as the original prescription Nasacort, and you should use it according to the package directions.\nNote: It takes up to a week for Nasacort to reach maximum effectiveness, whereas pills work within half an hour. You can use both allergy pills and Nasacort at the same time for best effect.\nAre allergy shots effective?\nHere’s some good news for needle-phobes: Immunotherapy — the medical term for allergy shots — is now available in a sublingual form, which doesn’t involve needles. Instead, a drop of liquid that contains allergens is placed under your tongue, where it’s absorbed by your system. You still have to see a specialist and sit in the waiting room for a short period after receiving immunotherapy because of the very rare risk of a severe allergic reaction. For these reasons, I only recommend immunotherapy after pretty much all else has failed, or for people with severe symptoms.\nCan neti pots help treat allergies?\nI’m a fan of nasal saline irrigation. If you use a neti pot, be sure to use only distilled water and to clean and dry the pot thoroughly between uses. Although rare, there’s a risk of parasitic infection when using tap water in neti pots. And if you don’t dry the pot thoroughly, you’re inviting mold growth. It’s also fine to simply pick up a bottle of saline nasal irrigation at the drugstore. It’s sterile, and the bottle can be recycled. I’ve found saline irrigation works better for chronic allergies rather than seasonal, but it can’t hurt to try it.\nIs there any way technology can help me manage my allergies?\nYes! Several pollen-forecasting apps are available for smartphones. Some of these apps can give a very precise pollen forecast for your location, so you can plan your activities accordingly. And if you like to exercise outdoors, it’s very helpful to have an hourly pollen forecast at hand.\nI’ve tried everything, but I’m still miserable. Any suggestions?\nConsider complementary and alternative medicine (CAM) solutions. I often recommend acupuncture for seasonal allergies, along with the herb stinging nettle in a tea or capsules. Keep in mind these work best as preventive options, so start well before allergy season. For more options, talk to your health care provider.\nSeasonal allergies can be annoying to deal with, but you can minimize their impact through preventive measures, medication, and even immunotherapy. Don’t let pollen season get the best of you this year!\nLooking for more information about allergies? Read All About Allergies.\nThe One Medical blog is published by One Medical, an innovative primary care practice with offices in Boston, Chicago, Los Angeles, New York, Phoenix, the San Francisco Bay Area, Seattle, and Washington, DC.\nAny general advice posted on our blog, website, or app is for informational purposes only and is not intended to replace or substitute for any medical or other advice. The One Medical Group entities and 1Life Healthcare, Inc. make no representations or warranties and expressly disclaim any and all liability concerning any treatment, action by, or effect on any person following the general information offered or provided within or through the blog, website, or app. If you have specific concerns or a situation arises in which you require medical advice, you should consult with an appropriately trained and qualified medical services provider.\nGet WellThis link opens the post, \"Breast Cancer Rates Are Rising in Black Women. Here's What You Can Do\"\nBreast Cancer Rates Are Rising in Black Women. Here's What You Can DoFeb 10, 2016\nGet WellThis link opens the post, \"Flu Season 2016: Facts and FAQs\"\nFlu Season 2016: Facts and FAQsSep 19, 2016","Why does my child get so many colds?\nThe common cold (also known as an upper respiratory infection) is caused by a virus. Trouble is, there are at least 200 different known viruses, with new ones occasionally appearing. Children develop immunity to the viruses one cold at a time. Remember all the colds you’ve had over your lifetime? Your child has to get every one — and more — to be immune to all cold viruses.\nExpect the average toddler to have six to 12 viral infections a year. School-age children have fewer infections, and the number drops to about three a year for teens. Since each infection often takes more than a week to completely run its course, it may seem like children are constantly sick, especially during the colder months when more viruses are circulating.\nHow do I know if what my child has is just a cold?\nThe common cold is a group of symptoms that includes runny nose, fever, sore throat, and cough. Your child may also have red eyes and swollen lymph nodes on either side of his neck. If he has those symptoms, he very likely has the common cold, which results in more sicktime for the average American than all other illnesses combined.\nWhen should I call the pediatrician?\nCall if your child’s symptoms go beyond that of the common cold. If your child’s fever (over 100 degrees F) lasts for more than three days, for example, you may be dealing with a superinfection (a bacterial infection on top of a virus).\nThat complication, which results when cold viruses lower the body’s defenses and allow bacteria to gain a foothold, needs antibiotics. If your child also has ear pain, difficulty breathing, a stiff neck and wheezing, or a sore throat with a high fever, he may have a bacterial infection and need antibiotics.\nYou should suspect an additional infection if your child’s cold symptoms change. Call your doctor if you observe any of the following:\n- Complaints of ear pain, which may signal an ear infection\n- A thick yellow or green nasal discharge for more than 10 days with headaches and pressure over the forehead and under the eyes, which may be a sinus infection\n- A worsening cough accompanied by fever and difficulty breathing, which could be signs of pneumonia\n- A sore throat that comes on abruptly with a high fever and tender, swollen lymph nodes in the neck, which may mean your child has strep throat\n- A stiff neck or wheezing\nAll of the above illnesses may involve bacteria and require antibiotic treatment.\nHow can I protect my child against colds?\nFrequent hand-washing is one of your best weapons against the common cold. Cold viruses are spread by body secretions that penetrate the protective mucous membranes of the eyes, nose, and throat. The hardy viruses can survive for a few hours on hands, tissues, countertops, and toys. In addition to encouraging your child to wash his hands often, teach him to cover his mouth and nose when he coughs or sneezes, and to dispose of tissues in the trash to avoid spreading germs. (Instead of covering his mouth with his hands when a tissue isn’t available, teach him to cough into the angle of his elbow.) If he’s around people with colds, explain that he shouldn’t touch his eyes and nose because he might get germs and become sick.\nHow do I treat my child’s cold when he gets one?\nSince there’s no way to speed up the healing process, the best you can do is manage the symptoms. Offer your child plenty of water and soup: His body is working harder to fight off the infection, and fevers increase fluid requirements. Warm liquids, like chicken soup and vegetable broth, soothe the throat and temporarily unclog congested nasal passages.\nWhat should I do if my child’s nose is stopped up?\nNasal congestion is the most aggravating symptom of the common cold; you can expect toddlers and preschoolers to waken at night because of it. Teach your child to sniff up the mucus and swallow it as well as to blow into a tissue. A humidifier or a cool-mist vaporizer can prevent nasal passages from drying out, which makes the mucus more difficult to remove. You can also put warm water or salt water drops (1/4 teaspoon of salt dissolved in 8 ounces of water) in your child’s nose and then suck the liquid and softened mucus out with a nasal bulb syringe, which you can find in a drugstore.\nWhy doesn’t my doctor prescribe antibiotics for a cold?\nAntibiotics are only effective against bacteria; they do nothing for the viruses that cause the common cold. Also, overusing antibiotics leads to widespread resistant bacteria, which circulate freely among children, especially in daycare.\nKeep in mind that yellow or green nasal discharge is not a call for antibiotics unless it lasts for more than 10 days or there are other signs of sinus infection.\nShould I use over-the-counter medicines?\nIn September 2007, pharmacy companies voluntarily recalled over-the-counter cough and cold medicines for children under 2. But many experts felt this move did not go far enough. According to testimony from the American Academy of Pediatrics, over-the-counter (OTC) cough and cold medicines “do not produce any discernable health benefits” in children under 6.\nIn January 2008, the U.S. Food and Drug Administration issued a public health advisory recommending that these drugs not be used to treat children under 2. While the FDA is still in the process of reviewing information about the safety of the medicines for children aged 2 through 11, the Consumer Healthcare Products Association, a trade group for drug manufacturers, announced recently that drug companies will voluntarily modify the labels of OTC cough and cold medicines to say “do not use” in children ages 4 and under.\nUntil these issues are resolved, the FDA recommends the following:\n- Don’t give children medications labeled only for adults.\n- Check the “active ingredients” section of the DRUG FACTS label of the medicines you choose, and follow the dosing directions carefully.\n- Use only measuring devices that come with the medicine or those specially made for measuring drugs.\nIf you want to try an OTC medication for older children, follow these rules of thumb:\n- Choose a product with a single active ingredient tailored to your child’s symptoms, and talk with your doctor or health care professional if you have any questions.\n- Don’t give a separate acetaminophen fever-reducer if you use an OTC product with more than one ingredient. (Many OTCs already have acetaminophen as an ingredient and the extra dose could be too much). And never give aspirin to a child under 20 with a fever; it has been associated with Reye’s syndrome, a rare but potentially life-threatening condition.\n- Buy OTC cough and cold medicines with child-resistant safety caps, when available.\n- Feel free to buy less expensive in-house or generic brands of decongestants; they work just as well as name brands with similar ingredients. One widely used component in decongestants is pseudoephedrine, which can dry up the nose but can also cause excitement, a fast heart rate, and sometimes nightmares. Be sure to read all warnings on the label before using any decongestant.\n- Use decongestant nasal sprays only for 3 to 5 days. After that, your child’s nose will become accustomed to them, and you’ll see a rebound effect of even more stuffiness.\n- Consider using antihistamines only at night. Their main ingredient is usually diphenhydramine, chlorpheniramine, or brompheniramine, which help stop the nose from running and itching, but also cause sleepiness and irritability.\n- Check the labels of all medications to get the correct dosage for your child. The label should tell you what dose is right for your child’s weight.\n- Stop giving the OTC medication if it isn’t helpful or if you notice side effects.\nWhat about natural treatments?\nNever use Ma huang, also known as ephedra or ephedrine, a plant-derived decongestant, for a child. Its strength can vary widely, and the Food and Drug Administration has linked it (in adults) to 16,000 adverse reactions, including high blood pressure, irregular heartbeat, seizures, heart attack, and stroke.\nThere are no scientific studies to support the safety of any herbal remedies in children, and many herbalists recommend against using herbs to treat children. Herbs are not regulated as medications, so their potency and purity isn’t controlled.\nSome parents have asked about zinc lozenges thought to modify the strength and duration of cold symptoms in adults. The unpalatable lozenges, which must be sucked every few hours for the first days of a cold, haven’t been studied in children, but they might be worth a try. Remember: Children under 4-years-old should not suck on lozenges because they’re a choking hazard.\nUnless you’re vegetarian, you may want to stick with chicken soup, whose healing powers have actually been documented in a scientific study. If you’d like more ideas, The Holistic Pediatrician by Kathi J. Kemper is an excellent source of research-based information on natural remedies.\nHow can I boost my child’s immune system?\nVitamin C may modify cold symptoms, although the high doses required can cause diarrhea. Long-term effects of megavitamin C are unknown, so stick to a multivitamin. Make sure your child is eating a healthy diet, resting enough, and (when he’s not sick) getting plenty of exercise.\nNational Institute of Child Health & Human Development\nPantell, Robert H. M.D., James F. Fries M.D., and Donald M. Vickery M.D. Taking Care of Your Child: A Parent’s Illustrated Guide to Complete Medical Care, Eighth Edition. 2009. Da Capo Lifelong Books.\nTaverner D et al. Nasal decongestants for the common cold. Cochrane Database of Systematic Reviews. Issue 3. 2006.\nHerbal Remedies and Children: Do They Work? Pediatrics. Volume 112, Number 1. July 2003. http://pediatrics.aappublications.org/cgi/content/full/112/1/S1/240\nFood and Drug Administration. Sales of Supplements Containing Ephedrine Alkaloids (Ephedra) Prohibited.\nConsumer Healthcare Products Association. Press Release: Makers of OTC Cough and Cold Medicines Announce Voluntary Withdrawal of Oral Infant Medicines. October 11, 2007.\nPublic Citizen. Children Under 12 Should Not Be Given Cough, Cold Medications, Public Citizen Tells FDA. October 19, 2007. http://www.citizen.org/pressroom/print_release.cfm?ID=2529\nWorstPills.org. Children 12 and under should not be given cough and cold meds. October 2007.\nFood and Drug Administration. Public Health Advisory: Nonprescription Cough and Cold Medicine Use in Children. August 2007.\nFood and Drug Administration. FDA Statement Following CHPA’s Announcement on Nonprescription Over-the-Counter Cough and Cold Medicines in Children. October 2008."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:a9bd6049-0113-432c-8159-bdba37fad336>","<urn:uuid:10b28196-1b00-4dea-9f93-29fce4d6a638>"],"error":null}
{"question":"I'm planning holiday treats! Are the flavors used in traditional Polish Jewish cakes similar to those found in Purim hamantaschen?","answer":"Yes, there are several flavor similarities between traditional Polish Jewish cakes and Purim hamantaschen. Both culinary traditions use ingredients like poppy seeds, dried fruits, and nuts. Polish Jewish cakes often featured raisins, almonds, cinnamon, and fruit fillings, while hamantaschen are typically filled with similar ingredients like jam, ground poppy seeds, or dried fruits like cherries and cranberries. The use of citrus is also common in both - Polish Jewish baking incorporated lemon rind and orange rind, while hamantaschen recipes may include orange juice or lemon zest.","context":["The Taste of Tradition: The Lasting Influence of Jewish Cuisine in Poland\ndefault, The Taste of Tradition:\nThe Lasting Influence of\nJewish Cuisine in Poland, An 11.94-metre-long ‘chałka’ (challah), attempt by the PSS Społem bakery to break the Guinness World Record in chałka baking, photo: Jarosław Kubalski, full_chalka_forum_770.jpg\nThe cuisine of the Ashkenazi Jews who lived in Central and Eastern Europe before 1939 drew inspiration from a variety of culinary traditions – much like the cuisine of other Jewish ethnic groups.\nIf local dishes fitted in with kosher principles, they were adapted to to the form of Jewish cuisine. That is why, ‘in each particular country, Jewish people would assimilate a variety of local dishes, while giving them a particular character. Besides that, they use a bounty of scents and spices – a remnant of the East, and lots of onion in rather unexpected combinations’, as one pre-war journalist wrote in Bluszcz (Ivy), an illustrated women’s weekly.\nJewish Life in Poland Shown by Polish Jewish Painters\nAfter reading Rebeka Wolff’s 19th-century book Polska Kuchnia Koszerna (Kosher Polish Cuisine), the emerging image of Polish Jewish cooking is one of crude, yet refined and tasty food. The book, which was highly popular and enjoyed numerous reprints, is now also available in digital libraries. It provides recipes for completely unknown dishes, with examples of foods that used to be typical of old Polish cuisine (such as półgęski, which have recently been rediscovered by Poles).\nstandardowy [760 px]\nJewish merchants in Słomniki (1918-1933), source: Narodowe Archiwum Cyfrowe\nIn the 1920s, a prominent French gastronome of Polish origin, Professor Edward Pożerski de Pomiane, conducted research on the foods of Polish Jewish communities. He found their dishes to be rich in nutmeg, vanilla and orange rind. His Cuisine Juive: Ghettos Modernes (Jewish Cuisine: Modern Ghettos) was published in France, and it comprised a collection of recipes from Polish Jews which he had collected in different cities across Poland.\nApart from the significant quantities of spice, the recipes also included sweet and sour flavours, thanks to the addition of vinegar and lemon, sugar, and onion – a flavour that was in fact a favourite tang in the Polish cookery of the Baroque period. Wolff thus pointed to various ‘principles of taste’:\nPolish Cuisine Rediscovered: A Modern Take on Polish Food Fights for its Place in the World Culinary Landscape\nWhen more dishes are served, they have to be varied. Thus, before a sour fish, for example, a sour and savoury soup would be out of place, but prior to a fish cooked on butter, such a soup is very appropriate. After a sour vegetable, soured meat is not a match…\nSome dishes were only served during holidays or Shabbat, since, as can easily be guessed, Jewish cuisine is filled with symbolic references, and most holiday dishes posses a hidden significance. Putting it in very simplified terms, kosher cooking was meant to protect from all that was unhygienic and sinful.\nThe fact that a few million Jews lived on the historic territory of Poland must have also influenced Polish cuisine. It would not be an exaggeration to say that both culinary cultures are intertwined. Dishes as popular in the everyday as gołąbki and even potato pancakes likely have Jewish origins.\nJewish Theatre in Poland: Fragments of an Illustrious History\nIn Poland – in Warsaw and Kraków in particular – several restaurants specialise in dishes of the Polish Jewish tradition, but almost none of these restaurants are kosher.\nNot just gefilte fish\nGefilte fish was traditionally served on Rosh Hashanah, or the Jewish new year holiday. Balls made of minced fish (carp, pike and other species), were blended with seasoning and matzo. In Galicia, the balls were often prepared in a sweet-flavoured manner, with onion, raisins and almonds. In the north, they were flavoured with spicy seasoning. The border between regions where the fish was served sweet and where it was spicy bore the moniker of the ‘gefilte fish border’. In spite of the lack of a prevailing recipe, all of the cooking directions share the indication of serving the fish cold, in jelly.\nBefore 1939, in regions with a high Jewish population, various roadside inns and small provincial restaurants were famous for their excellent ‘Jewish fish’ recipes. Many visitors from the surrounding areas would travel in order to taste the dish. An iconic recipe of the Polish Christmas table, ‘Jewish carp’ is a cousin of the gefilte fish (this dish originates from Galicia and instead of fish balls, it is usually made with a slice of the fish meat).\nScrumptious Seafood: Treasures from Polish Waters\nIn the past, the carp used to be very popular in Jewish cuisine. It was prepared with all kinds of sweet and sour sauces and seasoning. It was cooked with onion, vinegar, beer, sugar, spices, or pepper. Similar seasoning was also used to prepare pike, bass and tench. There were also numerous recipes to prepare little fish. Smelt were cooked in beer and vinegar, with the addition of butter, salt, sugar and spices. And one cannot fail to mention the herring – a crucial item of merchandise. Before the war, it was one of the cheapest snacks available in inns. But it was also abundantly served in Jewish homes. There, it was often soaked and added to sweet and sour salads, marinated or fried. There were even herring dumplings.\nJewish cookery holds duck meat in high esteem. The so-called gęsi pipek, a dish made of the stuffed and baked skin of a goose’s neck, is still often served by restaurants that specialise in Jewish cuisine. The neck was stuffed with potatoes and onion or alternately with kosher beef. It is worth noting that in Galicia, the name gęsi pipek also indicated a food made of cooked goose stomachs.\nBagels & Bialys: New York Food Staples with Polish Roots\nThe goose was baked in its entirety and stuffed with apples, soured cabbage (known to the English speaker under its German name, sauerkraut), or chestnuts, much like in the case of Old Polish recipes (with the additional tip in the recipe indicating that ‘bread rind makes the sauce more tasty and gives is a dark brown colour’). The goose offal was used either for making broth or for a garlic gravy, and the livers fried with apple and sugar.\nAdditionally, goose lard was indispensable in the kitchen, because the Talmud didn’t allow for a preparation of meat with butter – in accordance with the commandment that ‘thou shalt not seethe a kid in its mother’s milk’. Beef and veal tallow was forbidden, as it came from the animals’ rear parts.\nGoose fat was used to make delicious rind scratchings, and it was also the most popular kind used for the preparation of meat. Halves and quarters of the bird were smoked to make półgęsek, a delicacy known also in Old Polish cuisine. This dish is slowly being restored to the culinary memory of Poles – a pierced, salted goose is kept in a barrel for a couple of days, and then rubbed in bran and ready to cook after a total of eight days. Sour goose, on the other hand, is a type of spread made with the addition of veal limbs, vinegar, and seasonings.\nWhat Is Jewish Music? An Interview With Michael Guttman\nThis dish would not exist without cholent (Polish: czulent), a traditional Shabbat dish. In the past, a dish with all ingredients of this one-pot food would be taken to a nearby baker on Friday afternoon. He would water the mixture and put it into the oven overnight. The oven was built especially to make cholent, which would stay inside until noon on Saturday, when it would be ready (and very hot). As it is known, lighting a fire is traditionally not allowed on Shabbat. Cholent frequently included beans, kosher beef as well as onion and spices, with the occasional addition of groats and potatoes.\nBread & pastries\nThe challah (or chałka, as it is known across Poland) – a yeast pastry in the shape of a braid – was one of the basic Jewish pastries on Polish territory. It was usually baked for the Shabbat, but not exclusively so. During certain other holidays, it would also take on a different shape. For Rosh Hashanah, for example, it was baked in a round shape. Braided pastries are widely available in Polish bakeries, especially during Christmas. Matzo, a yeastless and unleavened flat bread made from special flour, was usually eaten during Pesach (Passover). It was also used in the preparation of various other dishes. For example, it was soaked in and fried with eggs.\nImmigrant Cuisine in Warsaw: From Pol-Viet to Georgian Bread\nThe bagel is also a pastry which is associated with Jewish cuisine. A kin of the Cracovian obwarzanek (which is also cooked before baking), the bagel was most likely invented in Kraków, although some sources indicate Białystok as its source. The first mentions of the bagel date to the 17th century. At the threshold of the 20th century, Jewish emigrants ‘transported’ the bagel to New York.\nAnother pastry very popular in the southeastern area of Poland around Lublin is the so-called cebularz (‘cebula’ means onion in Polish). It is a wheat bun with onion which can still be bought in nearly every bakery of the region. It’s worth noting that it made its home in Polish cuisine thanks to the Jewish people who lived in the Old Town of Lublin. As the name itself indicated, the savoury bun is baked with onion, with the addition of poppy seeds. The first records of recipes that would be passed down through generations are dated to the 19th century. By the time of the early 20th century, and right before the war, cebularz had grown to become a delicious speciality of the whole Lubelszczyzna region.\nThe Bitter-Sweet Story Of Wedel, Poland’s Famous Chocolatier\nKnishes, kugel, holishkes, latkes\nKnishes (of Ukrainian origin) are a type of flour or potato dumplings, stuffed with all kinds of different flavours. These dumplings were baked in the oven until the skin became crunchy and brown. Most frequently, they were stuffed with onion, chopped liver or cheese. Ashkenazi Jews used to eat holishkes (also known as gołąbki) – cabbage leaves stuffed with minced meat, served in a sweet and sour tomato gravy.\nKugel was also one of the traditional Shabbat or holiday dishes. It was served either as the main course, a side dish or a dessert. It is baked with potatoes, eggs and onion, at times also seasoned with groats rice, or cabbage. The sweet version used pasta and fruit, such as raisins. A similar dish is also known in Polish cuisine, especially in the east of Poland – although there it is called a potato babka (babka ziemniaczana).\nA Sweet Treat Fit for a King: Baba, or Poland’s Gift to the World of Pastry\nIt’s difficult to state where potato pancakes first emerged. One of the theories claims that they appeared in Polish cooking thanks to the Jewish tradition of the latkes, which are traditionally eaten during Hanukkah. The cuisine of the Ashkenazi Jews also knows numerous flour dishes which are very much similar to the Polish pierogi and paszteciki.\nTzimmes & Jewish caviar\nTzimmes is a name given to sweet Jewish dishes, most often served as dessert, though not exclusively so. The name has entered Polish language as a designation for something delicious and rare. This meaning is accounted for in the lyrics of a song of the same name, performed by Marta Bizoń to the accompaniment of Leopold Kozłowski.\nTzimmes is associated with Shabbat and other festive occasions, although only those which are joyful in their character. In a book entitled Kuchnia Żydowska (Jewish Cuisine), Katarzyna Pospieszyńska stated that: ‘this sweet and juicy dish is often simmered or baked for a long time until it gains a golden colour and magnificent taste – a symbol of a happy year.’ There are a variety of tzimmes served as the main course (which include meat), and others made only with fruit and vegetables which are served as dessert. In the south and east of Poland, the sweet kind was made with carrot, sugar, cinnamon and dried fruit and nuts. But tzimmes can also contain tomatoes, potatoes, carrots, apples, pineapple, prunes and boiled beef.\n7 Must-Try Polish Cakes & Pastries\nA popular snack is the so-called Jewish caviar: chopped chicken livers fried with onion and served with hard-boiled egg and cumin. The dish is served in restaurants offering the food of the Ashkenazi Jews, which also often includes egg salad with onion.\nNumerous other dishes which are more or less close to foods of old Polish cuisine were also popular in Jewish cooking. Various meats were baked in sour flavoured marinate (also used for the preparation of fish); bird meat and veal and beef offal were also popular in both traditions. Meatballs were made with chopped beef and either fried or baked. There were meat and fish pâtés. Vegetables were served with a sweet and sour flavour, and fruit was served with the addition of onion and spices. Ashkenazi Jews also ate pickles – cucumbers and cabbage, as well as vegetables and fruit marinated in vinegar. The latter were a popular ingredient accompanying fish salads.\nBoth Jewish and Polish cuisine was known for serving different kinds of broths, which were sometimes even called ‘Jewish penicillin for colds’. Broth from kneidlach (matzo balls) was made for the Pesach as well as other occasions. Jewish cuisine was also familiar with broths, with the so-called lane kluseczki (literally poured-in noodles, made by pouring the pastry directly into a hot soup), or with the addition of groats (oat, buckwheat or millet) or vegetables – such as cauliflower, asparagus and other seasonal ingredients. The broth was also served with kreplach dumplings (stuffed with meat or with vegetables, depending on the occasion). Edward Pożerski de Pomiane wrote about kreplach with potato, onion and mushroom.\nFrom Soup to Nuts: Poland's 10 Most Peculiar Soups\nAt Pesach, Jews also cooked red borscht made with beetroot acid – a dish which was very popular among Poles as well. Rebeka Wolff proposes a version seasoned with sugar, lemon rind, cinnamon and thickened with matzo, ‘this borscht tastes like a savoury wine soup’. Other vegetable and meat soups were also certainly popular, and they were prepared in accordance with kosher principles. Recipes for lemon and fruit soups which were cooked by Jews can also be found in historic books devoted to old Polish cuisine.\nApart from ordinary strucla, butter and sugar strucla was also frequently baked. There were also yeast cakes with crushed almonds, butter cakes and babkas stuffed with raisins, poppy seeds, almonds, araq liquor and cinnamon. Naturally, there were also cakes with seasonal fruit (such as apples, cherries, and gooseberries).\nMakagiki is also a dish which is derived from Jewish cuisine. It is made of poppy seeds and nuts fried in honey or syrup, and it was usually made for the Purim. There are many sweet dishes which are very familiar to any Pole – such as rice with apples and pasta with sugar and cinnamon, or sweet cheese dumplings – which were also very well-known in Ashkenazi Jewish cuisine. Much like pascha – a cottage cheese, cream and dried fruit dessert which can now be encountered in Polish, Russian and Jewish cuisine.\nAntique Cookbooks: The Meals of Poland's Past\nOriginally written in Polish by Magdalena Kasprzyk-Chevriaux, translated by Paulina Schlosser, Jan 2015","A cultural truth: if you’re celebrating a Jewish holiday, someone’s been spending quality time in the kitchen to make it happen. Every holiday has its special food, whether it’s latkes for Chanukah, apples and honey (or honey cake) for Rosh Hashanah, or matzoh for Passover. But my favorite holiday treats are the hamantaschen baked for Purim, the early-spring, kid-friendly holiday that celebrates the bravery of Queen Esther, a wife of the Persian king Ahasuerus, in revealing a plot by Haman, the king’s advisor, to destroy a community of Persian Jews. By exposing her own previously concealed Jewish identity, Esther, the king’s favorite, was able to plead for mercy for her people, and Haman and his plotters perished in their stead.\nIt’s a holiday that’s cultural and historical, rather than specifically religious, and is celebrated accordingly as a kind of Jewish Mardi Gras, with reveling, drinking, dressing up, house-to-house visits, and (my favorite), mishloach manot, gifts of food shared with friends and neighbors. And always included in the mishloach manot are hamantaschen, triangular cookies with a plump center pocket filled with jam, ground poppy seeds, or chocolate. Some say they represent Haman’s hat, others his ears or his pockets. However you translate them, they’re a delicious, buttery cookie with a sweet and tasty filling.\nThis gluten-free version uses a mixture of potato starch, tapioca flour, and rice flour. We’ve also taken a little inspiration from a recent posting about koolooschen on Jewcy, Tablet magazine’s lifestyle blog. Writer Rachel Harkham’s invention is a cardamom-scented cookie that’s part Persian koloocheh, part Eastern European hamantaschen. For filling, she suggests whole or pureed dried sour cherries; we suggest raspberry jam or a tangy filling made from dried cherries, dried cranberries, and golden raisins.\nRecipe: Gluten-Free Hamantaschen\nYou can fill these with any number of fillings. Tradition calls for apricot or prune lekvar, made from dried fruit simmered with citrus juice, sugar, and water until soft, then pureed. You can also use a thick fruit spread, like raspberry or blueberry. We’ve included a recipe for a tart cherry-cranberry filling, made from dried sour cherries and dried cranberries. Note that the dough needs to be chilled for a few hours before rolling. If your dough remains too soft for rolling, you can roll it into balls instead. Poke the tip of your finger into the center to make an indentation for the filling, add a spoonful of jam or filling, then gently push the sides into a triangle shape.\n- 8 tbsp (1 stick, 4 oz) butter or margarine, softened\n- 1/2 cup + 2 tbsp sugar\n- 1/2 tsp grated lemon rind\n- 1 egg\n- 1 tsp rosewater, orange-blossom water, or vanilla extract\n- 1 tbsp fresh lemon juice\n- 1 1/3 cup white rice flour\n- 1/4 cup tapioca starch\n- 1/3 cup potato starch\n- 1 tsp ground cardamom\n- 1 1/4 tsp baking powder\n- 1/4 tsp salt\nGluten-Free Cookie Dough\nTart Cherry-Cranberry Filling\n- 1/2 cup dried sour cherries\n- 1/2 cup dried cranberries\n- 1/2 cup golden raisins\n- Juice of 1 orange\n- 1 tbsp water\n- 2 tbsp sugar or honey, or to taste\n1. Mix lemon rind and sugar together. Using a hand-held or stand mixer or a wooden spoon, cream butter with sugar until fluffy. Beat in egg, orange juice, and rosewater, orange-blossom water, or vanilla. In a separate bowl, whisk together flours, potato starch, baking powder, cardamom, and salt. Stir flour mixture into butter mixture, mixing gently until a soft dough is formed.\n2. Flatten into a round, wrap in plastic wrap or pop into a resealable plastic bag. Chill in the refrigerator for at least 2 hours or overnight. (Otherwise, dough will be too sticky to roll out.) While dough is chilling, making filling.\n3. To make filling: Place ingredients in a small, heavy saucepan. Warm over low heat, stirring frequently, until fruit has softened and plumped up, raisins have plumped up, about 6 minutes. Let cool for a few minutes, then transfer to a food processor or blender. Puree until smooth. Taste and add more sugar or honey, as needed.\n4. Preheat oven to 350ºF. Lightly dust a large wooden cutting board or countertop with rice flour. Because this dough tends to be sticky, it’s easiest to roll it out with a sheet of waxed paper between the dough and the rolling pin. This will prevent the dough from sticking and tearing as you roll. Roll out dough into a broad round, about 1/4 inch thick.\n5. Using a 3-inch cookie cutter or a drinking glass, stamp out circles of dough. Using a spatula, move the circles onto a cookie sheet, leaving an inch or so between each one. It’s important to fill the rounds on the cookie sheet (rather than on the countertop) as they are hard to move without tearing once they’re filled.\n6. Place a generous spoonful (about 1 tsp, depending on the size of the round) of filling in the center of each round. Fold the right and left sides of the circle into the middle and pinch the top into a point. Fold the bottom half up to meet the folded-in sides. Pinch each side to seal, forming a triangle with a patch of filling peeking out from the middle.\n7. Bake for 15-18 minutes, until cookies are pale golden brown around the edges. Let cool on a rack."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:277103e4-5a5e-4c74-b974-c1f110a0592d>","<urn:uuid:7f061f81-85a8-4a44-951d-2bf62900cc72>"],"error":null}
{"question":"What similarities are there between the Late Ordovician extinction event and the dinosaur extinction event in terms of their global impact?","answer":"Both extinction events were massive global catastrophes. The Late Ordovician extinction killed 26% of all families and 60% of all genera worldwide, making it the second-biggest mass extinction in the fossil record. The dinosaur extinction event 65.5 million years ago wiped out more than half of all species on Earth, including dinosaurs, pterosaurs, and many marine reptiles. Both events were accompanied by dramatic environmental changes - the Ordovician event involved rapid climate change with major sea level fluctuations and changes to ocean chemistry, while the dinosaur extinction involved worldwide wildfires, forest destruction, tsunamis, and a global winter caused by the asteroid impact.","context":["Take a look at the article below. It comes from the Orlando Sentinel, from the Tribune newspapers, identifying the fact that it has now been declared to be settled science that the cause of extinction of the dinosaurs as well as some 50% of the species on earth around 65.5 million years ago was a massive asteroid that struck the Gulf of Mexico.\nAs you may know, the debate has been hotly contested for some thirty (30) years with about a half dozen plausible theories as to what caused the extinction of the dinosaurs. Based on that, European researchers assembled what they called “a K-T boundary dream team” of 45 internationally reknowned scientists from all different disciplines to evaluate the issue.\nTheir findings were published in the Journal of Science.\nIn any event, just a quick comment is in order.\nWhen we consider the fact that a seven and a half mile wide asteroid traveling at ten times the speed of a bullet hit the earth, creating a billion times more energy than the atomic bomb dropped on Hiroshima (wiping out half of all known species in what has become the greatest extinction event of all time), it appears to me that random events are just that: random events.\nYes, I understand that once in 65.5 million years is not particularly proximate as a risk, but as we take a look at the most recent random events in Haiti and in Chili, this random of all random events makes consideraton of the results particularly compelling.\nSo, in any event, I’m printing the article here and asking you to think about it: particularly because a random act of violence is what many of us experienced over these past two years at the hands of the financial institutions on Wall Street, most of which survived and none of which have accepted responsibility for the chaos they created.\nIt must have been random indeed!\nA ‘dream team’ of researchers concludes that the massive Gulf of Mexico impact 65.5 million years ago — not volcanoes or multiple impacts — indeed caused the greatest extinction event of all\nBy Thomas H. Maugh II\nIt’s official: The extinction of the dinosaurs and a host of other species 65.5 million years ago was caused by a massive asteroid that crashed into the Gulf of Mexico, creating worldwide havoc, an international team of researchers said Thursday.\nThe 7.5-mile-wide asteroid was traveling at a speed about 10 times that of a rifle bullet when it hit, releasing a billion times more energy than the Hiroshima atom bomb. The impact blew dirt and rock around the world, set massive wildfires, knocked down forests worldwide, triggered massive tsunamis and earthquakes of magnitude 11 or larger and even caused parts of the continent to slip into the ocean.\nThose events wiped out more than half of all species on Earth in what has been called the greatest extinction event of all time. The species lost included not only the dinosaurs, but also the bird-like pterosaurs, large marine reptiles and many smaller land and sea creatures, clearing the way for the emergence of mammals as the dominant life form on the planet.\nAll of this may sound familiar. In fact, the idea was proposed 30 years ago by Nobel laureate physicist Luis Alvarez and his son Walter after they found abnormally high concentrations of the element iridium in sediments from what was then known as the K-T boundary. This 65.5-million-year old layer of Earth separates fossils of the Cretaceous period from those of the Tertiary period.\nIridium is rare on Earth, but common in space, and the Alvarezes proposed that a giant asteroid had hit the Earth, producing the sudden decline in species diversity previously observed at the K-T boundary — which is how the boundary got its name in the first place.\nThen, in 1991, researchers discovered a 120-mile wide, 1.5-mile deep crater called Chicxulub in Mexico with the same age as the K-T boundary. Most considered it the smoking gun for the extinction.\nIn recent years, however, some scientists have speculated about alternative causes for the extinction, arguing that it could have resulted from multiple asteroid impacts or, more likely, massive volcanic eruptions at the Deccan Traps in India.\nTo settle the question, European researchers decided to assemble what Kirk R. Johnson of the Denver Museum of Nature and Science called “a K-T boundary dream team,” a collection of 45 internationally renowned scientists in a broad spectrum of disciplines to analyze the possible causes of the extinctions. Funding came from the National Science Foundation in the United States and from similar groups in other countries.Their conclusions will be published Friday in the journal Science.\n“The answer is quite simple,” Johnson, a co-author and spokesman for the group, said in a telephone news conference. “The crater really is the culprit.”\nThe aftereffects from the impact “shrouded the planet in darkness and caused a global winter, killing off many species that couldn’t adapt to the hellish environment,” co-author and Earth scientist Joanna Morgan of Imperial College London said in a statement.\nProponents of a multiple-impact theory have cited the fact that there appear to be several distinct layers of comet debris near the Chicxulub crater and evidence for many species surviving after the impact, only to go extinct later. But the research team concluded that those anomalies were created by jumbling of strata when debris flowed back into the crater after the impact.\nThere are now more than 350 sites around the globe where researchers have studied the K-T boundary — now officially called the Cretaceous-Paleogene, or K-Pg, boundary. (K is the traditional abbreviation for the Cretaceous.) The sites that are distant from Chicxulub show evidence of only one impact.\nTiming is the problem with the Deccan traps. A variety of data show that there were three major pulses of volcanic activity, one about 400,000 years before the K-Pg boundary, one about 200,000 years after and one sometime in the middle.\nIf the volcanism were the cause of the extinction, “we would expect to see events in the biological world associated with it, but we don’t,” Johnson said. In fact, the first pulse of volcanism was associated with a slight global warming and an increase in biological diversity.\nMoreover, the amount of sulfur injected into the atmosphere by the volcanoes — assumed to be the cause of their global effects — was actually many factors of 10 smaller than that ejected in the impact.\nAnd what does it matter? “There are a lot of asteroids out there now,” Johnson said, and it is important to know what would happen if one were to strike the Earth.\nCopyright © 2010, The Los Angeles Times\nOne Response to “Scientists: A Random Act of Violence”\n1. resveratrol supplements Says:\nMarch 14th, 2010 at 12:29 pm\nI can’t say I completely agree regarding some thoughts, but you’ve got a unique perspective. Anyway, I enjoy the quality you bring to the blogosphere and that this isn’t just another abandoned, made-for-adsense blog! Take care…","|Carn Owen: how the grey\nrocks of a\nMid-Wales hillside record drastic past climate change!\n& Dyfi Valley landscapes Slide-Library - Click HERE\nversion of this post for\npeople familiar with geology and climate science can be found HERE.\ntimes have you read that? It is usually a sign that the person making\nthe statement has run\nout of other arguments with which to attempt to rubbish climate\nSo: what's the answer?\nSure it has! The rocks here in Mid-Wales were formed at\nwhen a remarkable period of climate\nchange occurred - and they record its passing. However, I should add an\nafternote. The change was accompanied by the second biggest\nmass-extinction in the fossil\nrecord of the last 542 million years - from the Cambrian Period until\nthe present day.\nrolling hills of the Cambrian Mountains that form the backbone of\nMid-Wales are made up of grey rocks that on first appearance are rather\nboring - slates, shales and sandstones. It's therefore surprising to\nknow that they record such a drastic event. Before we go into how they\ndo, let's have a quick look at the geological timescale, so that we can\nset the coordinates on our Tardis for a closer look at that time:\nI've abbreviated the Period names because\nthere wasn't enough room! The relevant ones here are:\nORD = Ordovician Period, 488.3-443.7 and SIL =\nSilurian Period, 443.7-416 million years ago.\nWe'll go up into the mountains, heading for the hill of Carn Owen that\noverlooks the Nant-y-moch reservoir. Carn is Welsh for Cairn, and at\nof Carn Owen there is indeed a Bronze-Age ring-cairn, looking\nout over the vast landscape:\nCarn Owen have been heavily quarried in the past, and two of\nthe smaller quarries reveal something rather interesting. The photo\nbelow shows the western one. Straight away,\nthe rocks can be seen to be forming layers, and the layers are all\ntilted at the same angle to the\nright (eastwards). These are sedimentary rocks - they are made up of\nsediment - sand, silt and mud. They were deposited underwater on an\nancient seabed, in the late Ordovician Period, about 444 million years\nBecause one layer of sediment gets\ndeposited on top of another, the higher layers in the photo are younger\nthan the lower layers. And because the rocks are tilted, if in this\nimmediate area you go from left to right (west to east) you will be\nlooking at younger and younger layers of sedimentary rock.\nThe rocks that form the Cambrian Mountains\nmight all look grey from a distance, but when you get close-up to them,\nthey are a bit more interesting! The thick layers in the quarry are\nsandstone and this is what it looks like in person.\nFull of small grains of white quartz, it is pale, hard and splintery.\nsome other places locally, it also contains rounded\nquartz pebbles. Now, in order to carry coarse-grained\nsediments such as sand and pebbles to their resting-place, you need\nstrong currents. Geologists have worked out that the sandstones were\ndeposited by what are called Turbidity Currents - mixtures of water,\nsand and other debris, that poured from the shallower waters of what is\nnow East Wales, down the slopes into the deep-sea basin that occupied\nMid-Wales back then.\nThe next picture shows the second, or eastern quarry face. As I\nexplained above, we have climbed a little higher up through the\nsequence of rock-strata. In fact this small rock-face marks the\nboundary between the Ordovician and Silurian Periods, 443.7 million\nyears ago. The boundary is at a change in the rocks - can you see it?\nIt's right in the middle of\nthe rock-face. To the left are grey rather\nsolid-looking rocks, from the last few thousand years of the Ordovician\nmiddle they are overlain by rather\nflaky, rusty-coloured rocks. The\nrusty-coloured rocks belong to the lowermost Silurian Period.\nHere is a\nclose-up of the rocks at the top of the Ordovician - they really are\ngrey and boring!\nBut here are the rocks from the Lower\nSilurian - far more to see here, just in terms of colour alone. This\nrock is very fine-grained - it is a mudstone. Although dark grey where\nfreshly broken, it has weathered in many places to\ncolour. That is because the mudstone contains a lot of pyrite - iron\nsulphide - that turns to rusty iron oxides when exposed to air and\nmoisture - just like iron does in time. The technical name for this\nrock is \"hemipelagite\" - translated this means \"deep sea mud\".\nThe Lower Silurian rocks also\ncontain a lot of fossils, which makes them even more interesting\ncompared to the Ordovician sandstones. The commonest fossils are\ngraptolites (below). This specimen, with individuals 4-5cm in length,\nshows how they can be beautifully preserved by pyrite that has\ntheir remains. So what were graptolites? They were free-floating\nthat existed, like modern jellyfish or plankton, close to the sea\nsurface, drifting freely with the currents of the seas.\nPyrite is everywhere in these Lower\nSilurian mudstones, as this photo taken down a special microscope\nshows. Here, the pyrite shows up as bright areas, the biggest of which\nis only a millimetre across. For pyrite to be this abundant, the\nenvironment on the sea-bed must have been one where free oxygen was\nscarce or absent, like the bottom of a stagnant pond, full of stinking,\nsulphurous black mud.\nSo, in a\nsequence of rocks across just a short distance on this mid-Wales\nhave seen a change over less than two million years from\nhigh-energy, turbulent conditions, when sands and pebbles were\ndeposited, to a stagnant, deoxygenated undersea\nplain where fine, muddy sediment only accumulated slowly and fossil\nremains were preserved in pyrite. What happened?\nTHE GEOGRAPHY OF THE LATE ORDOVICIAN, 444 MILLION YEARS AGO\nover the past few hundreds of millions of years\nhow, due to movements of the Earth's tectonic plates,\nhave drifted around, collided and split up through time. The globe\nbelow is such a reconstruction: is shows how things\nwould have looked at the end of the Ordovician Period, when the rocks\nof Carn Owen were being deposited. England, Wales and southern Ireland\nwere part of a block of continental crust known to geologists as\nAvalonia, which lay at a low\nlatitude in the Southern Hemisphere and the South Pole was straddled by\nthe large continent of Gondwana.\nZooming in, this is what the area that is now England and Wales would\nhave looked like at the time of the deposition of the sandstones of\nHow do we know? By geological mapping - of the types of rocks and their\nfossils - we can reconstruct what the environment was like at any given\ntime in the past. It gets much harder the further back you go, because\nthe very old rocks of a billion years and more in age generally don't\ncontain fossils and have been so messed-with by heating and pressure.\nBut in the Ordovician and Silurian Periods, we can get a reasonable\npicture of what it was like. 440 million years ago sounds like a long\ntime, but when the Earth is 4,600 million years old, it doesn't sound\nlike so much!\nThe next map is just a few million years before:\nAnd the next one is the same area in the early Silurian. Look how the\nland of the Midland Platform was submerged, then it emerged from the\nsea, then by the early Silurian it was being flooded again. It has been\nestimated that a sea-level fall-and-rise of at least 80 metres took\nplace to bring this about. What caused it?\nThat's a good question. First let's think about the consequences of the\nThis sudden (in geological terms a couple of million years is sudden)\nretreat and readvance of the sea had\nprofound effects. For a start, the retreat caused extensive areas of\nshallow sea, teeming with life, to become dry land. That\nwas one factor in the great\nmass-extinction that accompanied the changes - loss of habitat.\nlike that of today: the\nfirst primitive land-plants only appeared during the Ordovician and it\nwas only later in the Silurian that they really became widespread. As a\nconsequence, with nothing much holding the ground together, loose\nwas readily eroded from the newly-emerged land by rainfall and rivers,\nswept along to the edge of the deeper water and trundled down\ninto its depths by powerful submarine currents, covering the basin\nfloor with mud, silt, sand and, in places, pebbles.\nThus were the sandstones of Carn Owen deposited.\nWhen sea-levels rose again in the early Silurian, flooding back over\nthe land, the erosion of\nsediments by rainfall and rivers stopped and the deepwater area\nbecame isolated from the sediment source, so that only the\nfinest muds settled out over its depths, onto which the remains of the\ngraptolites and other creatures settled out from the near-surface\nhigh above. In this way, the pyrite-rich fossil-bearing mudstones came\nSo what caused this sudden double-flip in sea-levels? We have to look\nat the rocks that were deposited over the great southern continent,\nGLACIERS IN THE SAHARA? THE REMNANTS OF GONDWANA\nGondwana, the huge continent that\nstraddled the South Pole in the late Ordovician, has long since split\nup into fragments that have drifted away in all directions. Bits of it\nnow make up Africa,\nSouth America, India, Antarctica and Australia. In several of these\nmodern continents, late Ordovician rocks occur that have features that\ncan only have had\none origin - from glaciers and ice-caps. Such \"direct indicators\" of\nancient glaciers that are preserved in the geological record include\nthings like striated (scratched) rock surfaces - ice-sheets full\ndebris, like giant sheets of sandpaper, do a good job of grinding down\nthe rocks underneath. The\nstriking image below shows an\nit illustrates an Ordovician\nglacially-striated rock surface in the\nLibyan part of the Sahara Desert!\nThere are also \"indirect indicators\" of ancient glaciation: the\nfall (and subsequent rise) in sea-levels, recorded on Carn Owen by\nchanges in rock-types described above, is to be found in marine rocks\nof the same age all over the world - therefore the sea-level changes\nwere \"eustatic\" - global - in nature. In other words, there was a\ncooling, leading to a major ice-age, with global sea-level\nfalls due to so much water being locked up in ice, followed by a global\nwarming, melting the ice and bringing worldwide sea-levels back up\nagain. How, then, did it happen?\nTHE DRIVERS OF CLIMATE: TECTONICS, ATMOSPHERE AND SUN\nseveral major drivers of global climate that, working together, could\nhave made this happen.\nThe geographical arrangement of the continents\ndue to plate tectonics affects the flow of air in the atmosphere and\nthe currents of water in the oceans, moving warm and cold air and water\nabout. The composition of\nthe atmosphere, in terms of greenhouse gases like carbon dioxide,\naffects its ability to lose heat to outer space or retain it. The\nenergy output from the sun affects the\namount of energy reaching Earth.\nTaking geography first, having the continent of Gondwana over the South\nPole would be advantageous if you want ice-caps to start forming: it\nwould have played a role similar to that of Antarctica today. The poles\nare always the coldest parts of the planet and are even colder if they\nhave continents stuck over them. So that's\none box ticked.\natmospheric composition, carbon dioxide levels\nduring the Ordovician were much higher than today,\nrunning at several thousand parts per million (ppm). Through\nbut very warm climate compared to today.\nIt seems that a fairly stable carbon-cycle\nexisted, with a balance between carbon sources such as volcanoes and\ncarbon sinks such as oceans.\nHowever, in the late Ordovician, the stability seems to have\nbeen disturbed. It is thought that this was because volcanic activity\ndied away, so there was less carbon dioxide being added to the\natmosphere. At the same time, continental collisions led to new\nmountain ranges being formed in places such as what is now eastern\nNorth America, with erosion and weathering of rocks. Rock-weathering is\na tremendous carbon dioxide sink: the gas dissolves in water, which\nthen falls as rain: rainwater carrying dissolved carbon dioxide is\nweakly acidic and reacts with certain common minerals that make up many\nrocks. The result is solutions carrying carbonate in solution plus\nmetals such as calcium and magnesium: in seawater, these drop out of\nsolution to form limestones. So the carbon dioxide gets locked away in\nlarge amounts. There is a lot of evidence for\nexactly this scenario to have happened in the late Ordovician, in\nextensive areas where sediments from such new mountain ranges would\nhave been deposited, the resultant carbon sink causing\ncarbon dioxide levels to drop from about 5000ppm down to 3000ppm or\nstill sounds high but we haven't looked at all the major drivers of\nclimate yet: we still have one more to think about. What about\nThe Sun is a main sequence star - it behaves in a reasonably\npredictable way over thousands of millions of years. Solar\nenergy output is thought to have\nincreased steadily by about 10 per cent per billion\nyears of Earth's history and it\nin the late Ordovician, it would have been 4-5 per cent dimmer\nthan it is today.\nWhat difference would that shortfall in solar energy make? Global\nclimate models tell us that, at the moment, polar ice can persist\nwhen carbon dioxide\nlevels drop below 500 ppm (of course, they are below that at the moment\nand have been for 25 million years or so).\nThe same models, adjusted for factors like\nthe the dimmer sun of the late Ordovician, predict that for glaciers to\nhave formed at the poles back then, carbon dioxide levels would need to\nbe below 2240-3920 ppm. So: combine a\ndimmer sun, a\nsudden drop in carbon dioxide levels and a large continent over the\nSouth Pole and\nthat appears to have been enough to get the temperature down and allow\nglaciers to develop, due to several major\nclimate drivers working together.\nRAPID ENVIRONMENTAL CHANGE AND MASS-EXTINCTION\nAccording to the fossil record, during this late Ordovician ice-age,\n26 per cent of all\nfamilies and 60 percent of all genera of life\nworldwide died out.\nsecond-biggest such mass-extinction event in the fossil record - there\nbeing five really big ones, including the K-T event that killed off the\ndinosaurs and the end-Permian or P-T event that was the biggest of the\nlot, when life on Earth was almost totally wiped out. The diagram below\nshows how the Late Ordovician event sits with the others.\nWe know that\nthe late Ordovician mass-extinction accompanied a period of rapid\nclimate change - a significant cooling that lasted less than 2 million\nyears before a return to very warm conditions. That caused major\nin sea level and changes to\nocean temperatures and chemistry. Oceanic changes are blamed for\nthe loss of coral-reefs: although a few corals\nsurvived, living reefs themselves disappeared from the face of the\nEarth and it took as much as 6 million years for them to reappear,\nmaking the event the ﬁrst true ‘reef gap’ in the geological record.\nThere is an important lesson to be learned in this story. That is that\nany major and geologically rapid climate shift in either\ndirection - warmer or cooler - from a stable state brings with it\ndrastic environmental changes that can have severe effects on\necosystems and create the danger of extinction\nevents. These may occur due to actual habitat-loss, such as the\ndraining of the Ordovician shelf-seas, or due to changes in, for\nexample, oceanic chemistry that are too rapid for evolution to adapt\nto. That's an important point: life can thrive in both very warm\n(\"Hothouse\") and very cold (\"Icehouse\") climates. It is when the rate\nof transition from one to another is geologically too rapid for\nadaptation that the big problems occur.\nOur burning of the fossil\nfuels on such a massive scale over just a couple of centuries risks\nbringing about such changes unless we change the way we obtain and\nconsume energy, by moving more and more to renewable energy sources and\nin increasing the efficiency and reducing the wastage that is currently\npresent in our energy-use. The \"end-game\" in terms of Earth's\ntemperature is less relevant: it is the speed at which we get there\nthat really matters. Several degrees in a couple of centuries: this is\na serious possibility if we carry on as usual. In geological terms this\nis lightning-like in its speed. It leaves the Ordovician changes stuck\nbehind, still in the starting-blocks - and remember what happened back\nthen. Nobody wants to bring about the sixth mass-extinction, but unless\nwe find a way through this minefield, that scenario will not go away\nIt's surprising what the geology of a couple of small, grey crags on a\nMid-Wales hillside can tell us!\nBACK TO WEATHER-BLOG MENU\nWelsh Weather & Dyfi Valley landscapes Slide-Library - Click HERE"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:5b5ab9e7-5fac-4aad-82fb-025ad88b93b8>","<urn:uuid:026c345f-9ec8-4304-ac3a-c02c3ac5659a>"],"error":null}
{"question":"How do exercise recommendations differ for people with arthritis versus those with polymyalgia rheumatica in terms of managing stiffness and pain?","answer":"For arthritis, exercise is strongly recommended and considered one of the best treatments for stiffness and pain. People with arthritis are encouraged to do both cardiovascular and resistance exercises, though they should focus on low-impact activities if necessary. They should perform proper warm-ups, especially to address morning stiffness, and can engage in activities like circuit training, yoga, and resistance training. For polymyalgia rheumatica, the primary treatment approach focuses on corticosteroid medications rather than exercise. While the condition causes muscle pain and stiffness particularly in the shoulder and hip areas, treatment typically involves prescribed prednisone at moderate doses, which usually provides rapid symptom improvement within a few days.","context":["Arthritis, whether it be osteoarthritis, rheumatoid or some other form, becomes more common with age, but that doesn’t mean you still can’t get a calorie-blasting, body-pumping workout. One of the best things for stiff, achy joints is movement. Research shows cardiovascular AND resistance exercise offer benefits for people with arthritis. People with arthritis who don’t exercise are at high risk for loss of bone mass, muscle atrophy, loss of flexibility and decreased functionality.\nHow does exercise help arthritis symptoms? Regular exercise strengthens the muscles that support the joints to help stabilize them. Plus, it helps with weight loss, and when you shed those extra pounds, it takes pressure off of the joints. Did you know each extra pound that you carry around adds four to six times greater pressure on your joints?\nAccording to the Arthritis Research Primary Care Center at Keele University in England, 25% of cases of knee arthritis can be blamed on being overweight or obese. Losing weight, if you’re overweight, is one of the best ways to take pressure off your joints and exercise can help you do that.\nOne of the biggest problems arthritis sufferers experience is stiffness, especially early in the morning. Exercise increases flexibility and range-of-motion and helps reduce rigidity. Unfortunately, there’s still the perception that exercise is bad for joints and can lead to joint damage, despite lack of scientific evidence. In fact, a number of studies show that regular exercise actually lowers the risk for arthritis of the knees and improves symptoms in people who already have the disease. As a result of research like this, the American College of Rheumatology encourages people with arthritis to stay active.\nGuidelines for Exercising with Arthritis\nDepending upon the extent of your arthritis, your doctor may recommend limiting the amount of high-impact exercise you do, activity that involves running or jumping. No problem. You can still get an effective cardiovascular workout with low-impact step training, spin classes, brisk walking, and circuit training.\nNot that you shouldn’t take precautions when exercising with arthritis. If you do high-impact workouts, alternate them with low-impact ones and give yourself adequate recovery time between your workouts. If you experience pain during a workout or feel stiff or sore afterward, you’re overdoing it. Remember, low impact doesn’t have to mean low intensity. If you experience a flare-up, take a day off and do stretching exercises to work on flexibility.\nCircuit training can be another joint-friendly way to exercise. Circuit training combines the benefits of a cardiovascular workout, assuming you don’t rest between exercises, with muscle endurance and can be a safe and effective form of exercise if you have arthritis. If a particular circuit training exercise feels uncomfortable, you can always modify it.\nRelieve Stiffness Before Exercising\nIf you have arthritis, you’re probably familiar with the stiffness you feel when you first get out of bed. Spend 10 minutes or so doing range-of-motion exercises to work out some of the stiffness and increase your flexibility. Then head for the shower and repeat range-of-motion exercises as the warm water rains down on your muscles and joints. The warm water will ease any stiffness and discomfort you might be having. If you exercise in the morning, do this before beginning your workout. Cold, stiff muscles and joints increase the risk of injury. If you have a choice on when to exercise, later in the day is best when your body temperature is higher and your muscles are joints are the most flexible\nResistance Training with Arthritis\nResistance training is crucial for strengthening the muscles that surround and support your joints and for preserving muscle and bone tissue. Strengthening the muscles over a joint helps the muscles better absorb shock. Better shock absorption means less stress on your joints. Plus, exercise is essential for bone health. Arthritis and osteoporosis is a double whammy – so make sure you’re giving your bones the stimulation they need to stay strong.\nIf you can’t do high-impact exercise, the type that maximally boosts bone growth, resistance training assumes even greater importance. During resistance training, muscles pull on bones and stimulate the production of new bone. The best resistance exercises for bone growth are “closed chain” ones where hands or feet are fixed to the floor such as squats, lunges, and push-ups.\nIf you have arthritis in your hands, you may find gripping heavy weights uncomfortable. Another alternative for stiff or painful hands is to use resistance bands rather than weights. You can still get a full-body workout using resistance bands without the discomfort of grasping heavy weights. Resistance training strengthens not only the muscle group you’re targeting, but stabilizer muscles as well.\nDon’t Skimp on the Warm-Up\nBefore doing any form of exercise, get the blood flowing to your muscles and joints by doing a warm-up. Warming up is vital if you suffer from joint stiffness or arthritis. The average person can get by with a 5-minute warm-up, but 10 minutes or even longer is best if you have arthritis. Do dynamic stretching exercises, not static ones, during the warm-up.\nThe best dynamic stretching exercises are those that mimic the movements you’ll do during your workout. If you’re working the lower body, air squats, leg swings and walking lunges without resistance are good warm-up exercises. Save the static stretches, where you hold the stretch for 20 or 30 seconds, until the end of your workout, as part of the cool-down. Avoid any type of ballistic or bouncing stretches, and be careful not to overstretch arthritis joints.\nKeep Your Workouts Varied\nDoing a variety of types of workouts not only reduces boredom it will keep you from overusing the same muscles and joints. Make yoga workouts a part of your routine too to help with flexibility. Just remember, if something hurts, don’t do it or modify the exercise.\nThe Bottom Line\nContrary to popular belief, exercise is safe and beneficial for people with arthritis. As a precaution, talk to your doctor first.\nArthritis Foundation. “Exercising with Osteoarthritis”\nIDEA Health and Fitness Association. “Training Clients With Arthritis”\nMed Page Today. “Weight Still Top Risk Factor for Knee Arthritis, Pain” 12/29/14.\nRheumatology (2001) 40 (4): 432-437. doi: 10.1093/rheumatology/40.4.432.\nAmerican College of Rheumatology. “Exercise and Arthritis”\nRelated Articles By Cathe:","Here’s a quick language lesson: “Myalgia,” the second half of “fibromyalgia,” comes from the combination of two Greek combining forms1. The first is “myo,” which means muscle2. The second, a variation on “algo,” “algia,” means pain3. Muscle pain is a hallmark of fibromyalgia, but it’s not the only chronic illness that’s named after it’s primary symptom. Take for example, polymyalgia — a condition that may sound similar to fibromyalgia, but has little in common with it.\nWhat Is Polymyalgia?\n“Polymyalgia rheumatic is an inflammatory condition that causes pain and stiffness primarily involving the shoulder and hip girdles,” Paul Sufka, M.D., a board-certified rheumatologist, told The Mighty. “We don’t know what causes the condition, but it is seen almost exclusively in adults over the age of 50.”\nPolymyalgia rheumatica (PMR) is believed to be an autoimmune condition that causes your immune system to attack your connective tissues4. It typically can be treated effectively with low-dose steroids and has a good prognosis. Unlike fibromyalgia, which is still a relatively controversial diagnosis, there is at least 100 years of medical knowledge behind how to manage polymyalgia1.\nWhat Causes Polymyalgia?\nThe cause of polymyalgia is still a mystery to experts. What we do know is the symptoms may be directly related to the inflammation the condition causes in your muscles4. Beyond that, Dr. Sufka said research suggests polymyalgia may be caused by a combination of genetics and environmental factors, especially infection5, 7. Other evidence suggests polymyalgia might be related to immune system issues or aging in general6.\nWho Gets Polymyalgia?\nPolymyalgia may be caused in part by aging since most people who experience polymyalgia are between 70 and 80 years old, and occurs in almost no one under the age of 501. Polymyalgia affects approximately 50 out of every 100,000 people over the age of 50 worldwide7. Similar to fibromyalgia, polymyalgia is more common in women1. Caucasian people, especially in Northern European and Scandinavian countries or with Viking ancestry are also most likely to develop polymyalgia7.\nWhat Are the Symptoms of Polymyalgia?\nThe core symptoms of polymyalgia include stiffness and pain in your neck, shoulder and hip muscles4. Most people with polymyalgia, about 70 to 95% of patients, will notice stiffness and pain in their shoulder muscles first, which then gradually moves to your neck and hip areas on both sides of your body7, 10. Your symptoms may have a rapid onset and seem to come on out of the blue — you were previously healthy but over a period of two weeks or less found pain and stiffness limit your agility7.\nApproximately 40% of people will experience additional polymyalgia symptoms, including4, 10:\n- Low-grade fever\n- Flu-like symptoms\n- Weight loss\nWith polymyalgia, it’s common to experience difficult stiffness right after you wake up, which can make some activities like putting on your socks and shoes very difficult7, 8. You may also experience stiffness during the day if you’ve been sitting for a long period of time, but once you get moving, you can typically go about your usual routine7. Raising your arms above your shoulders is also a common difficulty with polymyalgia because it can impact your upper arms8.\nMany people who have polymyalgia are at risk for experiencing depression, which may be your first true symptom of the condition before you notice the pain7. This can be exacerbated because developing polymyalgia symptoms suddenly can be scary — you might fear you’ll no longer be able to do the activities you enjoy or have anxiety about aging7. Polymyalgia can worsen your other chronic illness symptoms because of how it affects your system.\nPolymyalgia rheumatica causes muscle pain and stiffness in the neck, shoulder, and hip. Stiffness is most noticeable in the morning or after resting. https://t.co/EUrJfZXYrX #polymyalgia pic.twitter.com/sUEXGtE7m8\n— NIAMS/NIH/DHHS (@NIH_NIAMS) October 17, 2019\nHow Is Polymyalgia Diagnosed?\nThere isn’t a single test that can detect polymyalgia, so your doctor will make a diagnosis based on your symptoms, family medical history, a physical examination and blood tests to measure the level of inflammation in your body and to rule out other potential conditions9. Because polymyalgia is an autoimmune disease and the pain symptoms can look similar to other chronic illnesses, like rheumatoid arthritis or fibromyalgia, ruling out other possibilities is important8.\nWhile no blood test can definitely determine you have polymyalgia, your doctor will likely order want to determine if you have higher than expected levels of inflammation, a reliable sign you may have polymyalgia5, 7, 8. “In most cases, if inflammatory markers are normal, the patient is unlikely to have polymyalgia rheumatica,” said Sufka.\nFor example, your doctor might order an erythrocyte sedimentation rate (ESR) or “sed rate” test, a blood test that measures inflammation in your body by looking at how fast your red blood cells settle at the bottom of a test tube8, 11. A blood test can also measure your C-reactive protein (CRP) levels, a protein made by your liver, which can also indicate higher levels of inflammation.\nHow Is Polymyalgia Treated?\nThe good news is polymyalgia is a very treatable condition by reducing your inflammation4, 5 . For most patients with a polymyalgia diagnosis, this means prescription corticosteroids, particularly prednisone4, 5, 8. With medication, you may see a major improvement in your polymyalgia symptoms in just a few days. If after two to three weeks on the medication you don’t get better, it’s likely you don’t have polymyalgia but another condition8.\n“Most people who develop polymyalgia rheumatica are initially treated with corticosteroids — usually prednisone starting at moderate doses of around 15 or 20 mg per day — and then slowly tapered over several months,” said Sufka. “Most patients have a rapid improvement of their symptoms once prednisone is started.”\nThough corticosteroids are extremely effective, medications like prednisone have some pretty serious side effects that occur in at least 50% of patients, such as irritability and mood swings, facial swelling (or “moon face“), weight gain or hot flashes10, 12. You won’t need to take the corticosteroids for the rest of your life, however8, 10. Your doctor should find the lowest dose of the medication to manage your symptoms and most patients can stop taking corticosteroids after one to three years, depending on the severity of your symptoms8.\nDepending on the course of your illness, there’s also a chance your polymyalgia symptoms can relapse. Sufka said relapse occurs in about 50% of patients5, 10. Research suggests that patients who experience a relapse were more likely to have started treatment on higher doses of corticosteroids or faster medication tapers at the end of treatment10. Sufka said newer research suggests the “immunosuppressive medication methotrexate” can also be used to treat polymyalgia and prevent relapses5.\nWhat’s the Difference Between Polymyalgia and Fibromyalgia?\nApart from their name and main symptom, fibromyalgia and polymyalgia have little in common. Even the muscle pain might be slightly different — some experts suggest polymyalgia muscle pain doesn’t include the tenderness you may experience with fibromyalgia muscle pain15. The way each condition affects your body is very distinct and the treatment methods are also very different5, 13, 14.\n“Sometimes differentiating these two conditions can be difficult, especially because there isn’t an extremely specific blood test for polymyalgia rheumatica, and there isn’t a blood test for fibromyalgia,” said Sufka, adding:\nThe main difference is that polymyalgia rheumatica is an inflammatory condition, and fibromyalgia is a non-inflammatory condition. Because of this, polymyalgia rheumatica responds to treatment with anti-inflammatory medications. … Alternatively, fibromyalgia is a non-inflammatory condition caused by the nervous system that results in pain amplification, and is associated with fatigue, ‘brain fog’ and sleep problems.\nWhile it’s possible to have polymyalgia and fibromyalgia at the same time, the bigger risk when you’re diagnosed with polymyalgia is developing another inflammatory condition called giant cell arteritis.\nPolymyalgia rheumatica gives muscle PAIN without TENDERNESS.\nFibromyalgia and statin induced myositis is associated with tenderness. Fibromyalgia has very severe SLEEP DISTURBANCE\n— Conrad Fischer (@SeeFisch) July 23, 2019\nWhat Is Giant Cell Arteritis?\n“Giant cell arteritis (GCA) is another inflammatory condition that causes inflammation of the large blood vessels, most frequently affecting the head and scalp,” Sufka said. “The most common symptom of GCA is headache, as well as jaw pain that worsens with chewing, tenderness when touching the scalp over the temples. In some cases, GCA affects the arteries going to the eyes, which can result in blindness.”\nApproximately 10 to 20% of people diagnosed with polymyalgia will develop GCA7, 10. About 40 to 60% of people diagnosed with GCA have polymyalgia, though polymyalgia is two-thirds more common10.\nThe reason that polymyalgia and GCA are linked is unclear, but some experts believe both illnesses could be related to similar genes and may be different ways the same disease process manifests7. You can develop GCA before, during or after your polymyalgia diagnosis10. GCA is a serious condition that your doctor should monitor for regularly if you have polymyalgia10.\nOur #longread for the #weekend: Giant cell arteritis and polymyalgia rheumatica: current challenges and opportunities\nRead it for #free here https://t.co/ploWrPdIzF #SharedIt pic.twitter.com/oLfSULffsb\n— NatRevRheumatol (@NatRevRheumatol) November 3, 2018\nIf you’re concerned about your pain symptoms, check with your doctor. Even if you’re diagnosed with fibromyalgia — and have a hard time finding ways to manage your symptoms — know there are many treatment options out there to help with chronic pain, fatigue and your other fibro symptoms. To learn more, check out The Mighty’s Guide to Fibromyalgia, and post on The Mighty using the hashtag #Fibromyalgia to connect with others who have been there.\nYou can also check out these other Mighty stories that have helped others dealing with chronic illness and chronic pain:\n- How to Survive the Side Effects of Prednisone\n- 7 Things Your Doctor Won’t Tell You When You’re Diagnosed With Chronic Illness\n- 15 Tips for Coping With Chronic Pain\n- The Top 10 Ways I Combat Chronic Pain\n- Tsai, Y. (2018). PAIN MANAGEMENT: Polymyalgia and fibromyalgia are different. The Daytona Beach News-Journal. Retrieved from: https://www.news-journalonline.com/entertainmentlife/20181231/pain-management-polymyalgia-and-fibromyalgia–are-different\n- Dictionary.com. (2002). Myo-. Retrieved from https://www.dictionary.com/browse/myo-.\n- Dictionary.com. (2002). Algia. Retrieved from https://www.dictionary.com/browse/algia?s=t.\n- Clauw, D. (n.d.). Difference Between Fibromyalgia and Polymyalgia. Retrieved from https://www.arthritis.org/living-with-arthritis/tools-resources/expert-q-a/fibromyalgia-questions/polymyalgia-fibromyalgia.php.\n- Sufka, Paul. (2019). What is polymyalgia versus fibromyalgia? [Email interview].\n- National Institute of Arthritis and Musculoskeletal and Skin Diseases. (2016). Polymyalgia Rheumatica: Causes. Retrieved from https://www.niams.nih.gov/health-topics/polymyalgia-rheumatica#tab-causes.\n- Milchert , M., & Brzosko, M. (2017). Diagnosis of polymyalgia rheumatica usually means a favourable outcome for your patient. Indian Journal of Medical Research, 145(5), 593–600. doi: 10.4103/ijmr.IJMR_298_17\n- Sufka, P. (2019). Polymyalgia Rheumatica. Retrieved from https://www.rheumatology.org/I-Am-A/Patient-Caregiver/Diseases-Conditions/Polymyalgia-Rheumatica.\n- National Institute of Arthritis and Musculoskeletal and Skin Diseases. (2016). Polymyalgia Rheumatica: Diagnosis. Retrieved from https://www.niams.nih.gov/health-topics/polymyalgia-rheumatica#tab-diagnosis\n- Salvarani, C. G., Cantini, F. G., & Hunder, G. G. (2008). Polymyalgia rheumatica and giant-cell arteritis. The Lancet Seminar, 372(9634), 234–245. doi: 10.1016/S0140-6736(08)61077-6\n- U.S. National Library of Medicine. (n.d.). Erythrocyte Sedimentation Rate (ESR): MedlinePlus Lab Test Information. Retrieved from https://medlineplus.gov/lab-tests/erythrocyte-sedimentation-rate-esr/.\n- Wyant, P. (2018, November 16). 15 ‘Embarrassing’ Side Effects of Prednisone We Don’t Talk About. Retrieved from https://themighty.com/2018/11/embarrassing-prednisone-side-effects/.\n- The Mighty. (2019, April 1). What Is Fibromyalgia? Retrieved from https://themighty.com/2019/04/what-is-fibromyalgia/.\n- Matsumoto, A. (2007, April 24). Polymyalgia and Fibromyalgia. Retrieved from https://www.hopkinsarthritis.org/ask-the-expert/polymyalgia-and-fibromyalgia-2/\n- Fischer, C. [SeeFisch]. (2019, July 22). Polymyalgia rheumatica gives muscle PAIN without TENDERNESS. Fibromyalgia and statin induced myositis is associated with tenderness. Fibromyalgia has very severe SLEEP DISTURBANCE [Tweet]. Retrieved from https://twitter.com/SeeFisch/status/1153468637770080261"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:7f281531-13d9-49a3-a728-2987ccd2f150>","<urn:uuid:efda6e42-d809-4632-ba23-21875461af48>"],"error":null}
{"question":"How to perform initial tuning steps for a Polaris PRO RMK snowmobile?","answer":"To initially tune a Polaris PRO RMK snowmobile, follow these steps: 1) Warm the sled to proper operating temperature, 2) Adjust the green fueling mode to where the sled sounds best just before clutch engagement, 3) Set the yellow fueling mode to an AFR of 14, 4) Set the red fueling mode to an AFR of 13, 5) Set the yellow/blue mode between 5,000 and 6,000 RPM, and 6) Set the red/blue mode between 6500 and 7200 RPM. The green/blue mode should be determined based on riding style.","context":["Tuning a two-stroke snowmobile is a challenging and rewarding process. The lack of two-stroke SAE documentation specifying AFR values for optimal horsepower and torque means that determining the best settings for an AFR+ required more effort from the end user versus a four-stroke. This being said, tuning a snowmobile for optimum performance is undeniably satisfying.\nGeneral Characteristics of Two-Strokes\nWhen tuning two-stroke snowmobiles it is important to keep some general characteristics in mind. These items fall into two categories.\nFueling Needs: The fueling needs of two-stroke motors typically vary significantly from four-strokes. In general, two strokes run more lean (an AFR between 13 and 14.5 should be targeted) vs. four-strokes. There is also less variance in optimal targeted AFR values through-out the power curve. For instance, a targeted AFR of 13 may work perfectly fine for both acceleration and wide open throttle.\nTemperament: Two-stroke snowmobiles are stereotypically temperamental. Each snowmobile should be treated as a special case and may require specific knowledge and development of testing and tuning procedures. Fueling needs can vary greatly from sled-to-sled based on numerous factors.\nTuning a Polaris PRO RMK\nThis guide focuses on setting the AFR+ for the Polaris PRO RMK. The general guidelines associated with tuning a PRO RMK can apply to all snowmobile applications. However, as stated above, every snowmobile is different and requires special attention.\nBefore attempting to set the AFR+ for a PRO RMK it is important to have a good understanding of AFR values and the functionality of the AFR+. To brush up on your knowledge review the following videos.\nThe steps and resources provided below will ensure you are on the right track towards achieving optimal fueling with an AFR+. It should be noted that these recommended AFR+ settings only apply to a perfectly functioning snowmobile.\nTo begin setting the AFR+ start with these settings:\nGreen Fueling Mode:\n(1) Warm the sled to proper operating temperature.\n(2) Adjust the green fueling mode to where the sled sounds the best just before clutch engagement\nYellow Fueling Mode: Set at an AFR of 14\nRed Fueling Mode: Set at an AFR of 13\nGreen/Blue Mode: The setting of this mode should be determined by the user based on riding style\nYellow/Blue Mode: Set between 5,000 and 6,000 RPM\nRed/Blue Mode: Set between 6500 and 7200 RPM\nIt is useful to understand that the settings for the AFR+ switch point modes (yellow/blue and red/blue) should follow the exhaust valve positions of the PRO RMK. This rule holds true for other sled models as well. Information on exhaust valve position can be found in the service manual for your snowmobile.\nThe above settings SHOULD have a PRO RMK running close to optimal. After these settings have been entered ride the sled and take note of any potential areas in the power curve that could be improved. If the sled is running poorly it can be assumed that there is a factor affecting the optimal functioning of the AFR+ in relation to the specific sled. If the sled is running well it is time to do some testing to fine tune your AFR+.\nTests Procedures for Fine Tuning the AFR+\nWarm sled to standard operating temperature\nBased on the initial test ride decide which part of the power curve you want to analyze\nAccelerate the snowmobile until the area of the power curve you are testing is reached then turn the sled off\nProceed with one or more of the following tests\nSpark Plug Test: For two stroke motors a spark plug test is a quick way to determine how the PRO RMK is generally running. While a spark plug test for a four stroke motor will not give you much information the existence of residue from oil bi-products that remain on a two-stroke spark plug will provide some indication of how the snowmobile is running. Do your research if you are unfamiliar with how to conduct a spark plug test.\nEGT Test: Exhaust temperature is another reliable test that should be performed if the proper equipment is available.\nIf you are happy with the data you have collected change the settings on the AFR+ accordingly. Repeat the above procedure until you are satisfied with the performance of your PRO RMK through-out the entire power curve."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:e42c2c31-6439-48a2-8c36-6a84fbc61ad7>"],"error":null}
{"question":"What did the USS Intrepid and USS Princeton have in common regarding their physical characteristics?","answer":"Both were Essex-class aircraft carriers. The USS Princeton was specifically mentioned as one of the 'long-hull' Essex-class carriers, and throughout her career retained the classic appearance of a World War II Essex-class ship. The Intrepid was also an Essex-class aircraft carrier, as mentioned in the documents where it's described as a '900-foot-long ESSEX class aircraft carrier'.","context":["The Intrepid Aircraft Carrier to Return to its Home at Peir 86 in\nMuseum Will Re-Open to the Public on November 8th\nFollowing Final Exhibit Installation\n(New York, NY, October 2, 2008) – The Intrepid will make her historic journey home today from her temporary home at the Staten Island Homeport where the former aircraft carrier has undergone nearly two years of bow-to-stern refurbishment, to the Hudson River Park’s Pier 86 on Manhattan’s West Side. The ship will be towed by four tugboats up the Hudson River to her permanent home on West 46th Street.\nThe entire project – which also included the complete rebuilding of Pier 86, the refurbishment of 16 of the 30 historic aircraft on board, the redesigning and installation of new exhibits within the Intrepid Sea, Air & Space Museum, an inside-and-out paint job for the 65-year-old aircraft carrier, and restoration of spaces never before available for public viewing– cost approximately $115 million and took less than two years. When it re-opens to the public on Saturday, November 8th, the Intrepid Sea, Air & Space Museum will be completely redesigned with hands-on and interactive exhibit technology and array of new and exciting programming.\nDuring her journey home, 250 former crewmembers will “man the rails” one last time. All are making their way to New York City for this last trip at their expense. Many of these heroes were present for her departure from Manhattan in 2006, and have anxiously awaited her return. His Excellency Archbishop Celestino Migliore, Permanent Observer of the Holy See to the United Nations will bless the Intrepid and crewmembers prior to the ship’s departure.\nIntrepid’s trek will begin at the Staten Island Homeport, where former crewmembers will participate in a ceremonial sendoff, including a traditional line toss. Rear Admiral James L. (Doc) Abbot Jr. (ret.), 82, former commanding officer of the USS Intrepid in 1960-1962, will take the helm once more, “pronouncing orders” authorizing the ship’s movement. The ceremonies will be highlighted by colorful water sprays from New York City Fire Department boats, USCG Seneca and USACE vessels. In addition, WWII Patrol Torpedo Boat PT 728 will accompany the Intrepid on her voyage home. The PT 728 is one of only three surviving WWII Patrol Torpedo Boats and has recently undergone a major overhaul including installation of deactivated torpedoes and machine guns.\nAs she leaves Staten Island, the Intrepid will travel through the harbor past the Statue of Liberty and Ellis Island. As she passes opposite Lower Manhattan, she will pause, all engines to neutral, at the World Trade Center site in a momentary tribute to those who lost their lives at Ground Zero on September 11, 2001. Former crewmembers will unfurl a 60-foot American Flag, the same flag that flew over Liberty Plaza in the days immediately following the September 11 attacks and inspired countless New Yorkers who went to work and school in Lower Manhattan each day.\nIntrepid is expected to reach Pier 86 at West 46th Street at approximately 1:45 p.m. At that time, tugboat captains will confer on tidal conditions and determine when they will begin to push the Intrepid back into her berth alongside the newly rebuilt pier.\nThe day’s events will also include a reception ceremony at Pier 86, with scheduled remarks from City Council Speaker Christine Quinn, Borough Presidents Scott Stringer (Manhattan), Helen Marshall (Queens), Marty Markowitz (Brooklyn) and Adolfo Carrion (Bronx).\nIn the weeks after the Intrepid’s return, the Growler submarine, which is currently being restored, and the British Airways Concorde, will also return to Pier 86 in time for the museum’s November 8th reopening. Tickets to visit the new Intrepid can be purchased through the website www.intrepidmuseum.org.\nThe funding for the refurbishment of the Intrepid was a public-private partnership that included federal, state and city funding, in addition to private and corporate donations.\n“We are so grateful for the support of our devoted board members and friends as well as the corporations and foundations that have contributed,” said Intrepid Co-Chairman Charles de Gunzburg. “Without their help, this project would not have been possible and the future of the Intrepid Sea, Air & Space Museum would have been in doubt. This project is a terrific example of how a strong public/private partnership actually should work and works so well.”\n“As everyone remembers, our original plans got a little ‘muddied’ when we first tried to move the Intrepid out in 2006 due to her propellers being firmly stuck,” said Bill White, President of the Intrepid Museum. “With the help from our friends in the military, including the Army Corps of Engineers and USN, and further dredging, our second attempt to move her was successful. We could not do this without the support of our nation’s military – many of whom will be with us today on our voyage back.”\nIntrepid officials were carefully monitoring weather reports up until the time of today’s departure due to a chance of heavy winds. The move would have been postponed if winds exceeded 20 knots (23.0 MPH) in any direction except westerly or if over 25 knots (28.8 MPH) westerly. If the winds warranted postponement, the move would have been made tomorrow at the highest tide.\n“And of course, none of this would have been possible without the backing of our landlords, The Hudson River Park Trust, its chairman Diana Taylor and President Connie Fishman,” continued White. “They have been great partners in this project and we are looking forward to having our new Pier 86 be a wonderful addition to the Hudson River Park.”\nAccording to Susan Marenoff, Executive Director of the Intrepid Museum, before the Museum closed for renovations, it was receiving over 750,000 visitors a year. “We are looking forward to opening our doors to the public beginning Saturday, November 8th,” she noted. “With all the new exhibits and state-of-the-art attractions, the Intrepid will be of interest to those who have been here before and first-timers.”\nPerkins+Will, a Chicago based firm, was selected to work with the Intrepid’s exhibits department and curators to redesign the Intrepid’s hangar deck exhibition space. When the Museum reopens, visitors will experience a world-class environment showcasing the latest in hands-on and interactive exhibit technology, ensuring that visitors to the Intrepid from across the U.S. and around the world will have a fun, inspiring and educational experience they’ll never forget.\n“We also have new sponsors including an unprecedented level of partnership with Bank of America, and new services for our visitors including a new food service provider, new flight simulators, a new website, and an expanded retail store,” said Marenoff.\nWhile the Intrepid was on leave, the museum’s education team continued to offer programming such as its “Intrepid Sails” which went to schools, community-based organizations and special events at the New York Hall of Science in Queens.\n“We are very much looking forward to having student groups return to the museum to be able to take advantage of our many unique conferences and programs that have been so successful in the past,” said Marenoff. “We know the Intrepid Sea, Air & Space Museum will again be a truly enjoyable and educational experience and on everyone’s ‘must-see’ list.”\nWhen the museum closed on Sunday, October 1, 2006, it had been at its home at Pier 86 for 23 years. The USS Intrepid was one of the most successful ships in U.S. history, but while in New York, it became a national historic landmark and one of the city’s most unique attractions.\n“This is a truly historic and exciting time for the Intrepid. We are excited to be returning to Pier 86 and continuing the mission of Zachary and Elizabeth Fisher, our founders, to honor our heroes, educate the public, and inspire our youth,” added White.\nABOUT THE INTREPID:\nOne of the world's largest maritime museums, the Intrepid Sea, Air & Space Museum, is housed aboard the 900-foot-long ESSEX class aircraft carrier Intrepid, which saw service during World War II, the Cold War, and the Vietnam War. It also served as a prime recovery vessel for NASA during the Mercury and Gemini space programs before it was retired in 1974. Four years later, Zachary Fisher established the Intrepid Museum Foundation for the sole purpose of \"saving the Intrepid for generations to come.\" Listed in the National Park Service’s National Historic Landmark register, the Intrepid Sea, Air & Space Museum was opened as a symbol of peace and education in August 1982. The Intrepid has been undergoing extensive renovations since fall of 2006 and will return to its home at Pier 86 in October 2008. Prior to leaving its home on the Hudson River at West 46th Street, the Intrepid attracted more than 750,000 people each year from around the world, from children to senior citizens and world leaders. It also hosted more than 150 special events, and served as a focal point during New York’s annual Fleet Week celebration. The Intrepid produces a wide range of public programming, and is recognized worldwide as an historic icon that makes history come alive for visitors through its unforgettable series of dynamic and interactive exhibits. For more information, visit www.intrepidmuseum.org","USS Princeton (CV-37)\nUSS Princeton underway\n|Career (United States)|\n|Namesake:||Battle of Princeton, 1777|\n|Operator:||United States Navy|\n|Builder:||Philadelphia Naval Shipyard|\n|Laid down:||14 September 1943|\n|Launched:||8 July 1945|\n|Commissioned:||18 November 1945|\n|Decommissioned:||21 June 1949|\n|Recommissioned:||28 August 1950|\n|Decommissioned:||30 January 1970|\n|Renamed:||PCU Valley Forge to PCU Princeton|\n|Reclassified:||CVA-37, CVS-37 and LPH-5|\n|Fate:||Sold for scrap in 1971|\n|Class and type:||Essex-class aircraft carrier|\n27,100 tons standard\n888 feet (271 m) overall\n93 feet (28 m) waterline\n28 feet 7 inches (8.71 m) light\n8 × boilers\n4 × Westinghouse geared steam turbines\n4 × shafts\n150,000 shp (110 MW)\n|Speed:||33 knots (61 km/h)|\n|Complement:||3448 officers and enlisted|\n4 × twin 5 inch (127 mm)/38 caliber guns\n4 × single 5 inch (127 mm)/38 caliber guns\n8 × quadruple Bofors 40 mm guns\n46 × single Oerlikon 20 mm cannons\n4 inch (100 mm) belt\n2.5 inch (60 mm) hangar deck\n1.5 inch (40 mm) protectice decks\n1.5 inch (40 mm) conning tower\n|Aircraft carried:||As built:\nUSS Princeton (CV/CVA/CVS-37, LPH-5) was one of 24 Essex-class aircraft carriers built during and shortly after World War II for the United States Navy. The ship was the fifth US Navy ship to bear the name, and was named for the Revolutionary War Battle of Princeton. Princeton was commissioned in November 1945, too late to serve in World War II, but saw extensive service in the Korean War, in which she earned eight battle stars, and the Vietnam War. She was reclassified in the early 1950s as an attack carrier (CVA), then as an Antisubmarine Aircraft Carrier (CVS), and finally as an amphibious assault ship (LPH), carrying helicopters and marines. One of her last missions was to serve as the prime recovery ship for the Apollo 10 space mission.\nAlthough she was extensively modified internally as part of her conversion to an LPH, external modifications were minor, so throughout her career Princeton retained the classic appearance of a World War II Essex-class ship. She was decommissioned in 1970, and sold for scrap in 1971.\nConstruction and commissioning\nThe ship was laid down as Valley Forge — one of the \"long-hull\" Essex-class — on 14 September 1943 at the Philadelphia Navy Yard. She was renamed Princeton on 21 November 1944 to commemorate the light carrier USS Princeton (CVL-23), which was lost at the Battle of Leyte Gulf on 24 October 1944. The new Princeton was launched on 8 July 1945, sponsored by Mrs. Harold Dodds, and commissioned on 18 November 1945, Captain John M. Hoskins in command.\nAttack carrier (1945–1954)\nThen transferred to the Pacific Fleet, she arrived at San Diego, departing again on 3 July 1946 to carry the body of Philippine President Manuel L. Quezon back to Manila for burial. From Manila, Princeton joined the 7th Fleet in the Marianas, becoming flagship of Task Force 77 (TF 77). In September and October 1946, she operated in Japanese and Chinese waters, then returned to the Mariana Islands where she remained until February 1947. In 1947 she had Carrier Air Group 13 on board, and in October 1948 evacuated dependents from Tsingtao, returned to San Diego Dec 1948 and unloaded CAG13 Maneuvers in Hawaiian waters preceded her return to San Diego until 15 March. She cruised the West Coast, Hawaiian waters, and the Western Pacific (1 October – 23 December) in 1948. She then prepared for inactivation, and on 20 June decommissioned and joined other capital ships in the Pacific Reserve Fleet.\nReactivated with the outbreak of hostilities in Korea 15 months later, Princeton recommissioned on 28 August 1950. Intensive training refreshed her Reservist crew, and on 5 December she joined TF 77 off the Korean coast, her planes and pilots (Air Group 19) making possible the reinstitution of jet combat air patrols over the battle zone. She launched 248 sorties against targets in the Hagaru area to announce her arrival, and for the next six days continued the pace to support Marines fighting their way down the long, cold road from the Chosin Reservoir to Hungnam. By the 11th, all units had reached the staging area on the coast. Princeton 's planes, with other Navy, Marine, and Air Force squadrons, then covered the evacuation from Hungnam through its completion on the 24th.\nInterdiction missions followed, and by 4 April Princeton 's planes had rendered 54 rail and 37 highway bridges inoperable and damaged 44 more. In May, they flew against the railroad bridges connecting Pyongyang with Sunchon, Sinanju, Kachon, and the trans-peninsula line. Next, they combined close air support with raids on power sources in the Hwachon Reservoir area and, with the stabilization of the front there, resumed interdiction. For much of the summer they pounded supply arteries, concentrating on highways, and in August Princeton got underway for the U.S., arriving at San Diego on the 21st.\nOn 30 April 1952, Princeton rejoined TF 77 in the combat zone. For 138 days, her planes flew against the enemy. They sank small craft to prevent the recapture of offshore islands; blasted concentrations of supplies, facilities, and equipment behind enemy lines, participated in air-gun strikes on coastal cities, pounded the enemy's hydroelectric complex at Suiho on the Yalu River to turn off power on both sides of that river, destroyed gun positions and supply areas in Pyongyang; and closed mineral processing plants and munitions factories at Sindok, Musan, Aoji, and Najin.\nReclassified CVA-37 (1 October 1952), Princeton returned to California on 3 November for a two-month respite from the western Pacific. In February 1953, she was back off the Korean coast and until the end of the conflict launched planes for close air support, \"Cherokee\" strikes against supply, artillery, and troop concentrations in enemy territory, and against road traffic. She remained in the area after the truce on 27 July, and on 7 September got underway for San Diego.\nAnti-submarine carrier (1954–1959)\nIn January 1954, Princeton was reclassified CVS-37 and, after conversion at Bremerton, Washington, took up antisubmarine/ Hunter-Killer (HUK) training operations in the eastern Pacific. For the next five years she alternated HUK exercises off the West Coast with similar operations in the western Pacific and, in late 1957-early 1958, in the Indian Ocean–Persian Gulf area.\nAmphibious assault carrier (1959–1970)\nReclassified again, 2 March 1959, she emerged from conversion as an amphibious assault carrier, LPH-5. Capable of transporting a battalion landing team and carrying helicopters in place of planes, Princeton 's mission became that of vertical envelopment—the landing of Marines behind enemy beach fortifications and providing logistics and medical support as they attack from the rear to seize critical points, cut enemy supplies, sever communications, and link up with assault forces landed on the beaches. Since this was a Marine Corps mission, Marines made up a major portion of the ship's company in the Air, Operations, and Supply Departments.\nFrom May 1959 – January 1960, Princeton trained with Marine units from Camp Pendleton, then deployed to WestPac to train in Okinawan waters. For the next three years, she followed a similar schedule, gaining experience in her primary mission. Interruptions came in October 1961 when she rescued 74 survivors of two merchantmen Pioneer Muse and Sheik grounded on Kita Daito Shima and in April 1962 when she delivered Marine Corps advisors and helicopters to Sóc Trăng in the Mekong Delta area of the Republic of Vietnam (South Vietnam). From September–November 1962, Princeton served as flagship of Joint Task Force 8 during the nuclear weapons test series, Operation Dominic.\nIn October 1964, Princeton exchanged WestPac training for the real thing as she returned to Vietnam and joined the Pacific Fleet's Ready Group in operations against North Vietnamese and Viet Cong forces. Combat operations, interrupted in November for flood relief work, continued into the new year, 1965, and culminated in May off Chu Lai as she carried out her primary mission, vertical envelopment, for the first time in combat.\nReturning to her homeport, Long Beach, California, after that operation, she transported Marine Aircraft Group 36 to Vietnam in August, and in February 1966 got underway for another tour in the combat zone. Relieving Okinawa (LPH-3) as flagship for the Amphibious Ready Group, she engaged the enemy in operations \"Jackstay\", 26 March – 6 April, to clear the Rung Sat Special Zone of Viet Cong guerrillas, and \"Osage\", 27 April – 4 May, to protect Vietnamese in the Phu Loc area from Viet Cong \"harassment.\"\nSearch and destroy missions against Viet Cong and North Vietnamese Army units followed as Princeton provided transportation, medical evacuation, logistics and communication support for the amphibious operation \"Deckhouse I\", 18 – 27 June, in the Song Cau district and the Song Cai river valley, then supported 1st Air Cavalry and 101st Airborne units engaged in \"Nathan Hale\" to the south of the \"Deckhouse I\" area. \"Deckhouse II\" and support for \"Hastings\" followed as Navy, Marine, and Army units again combined, this time to impede enemy infiltration from the DMZ.\nAfter \"Hastings\", Princeton sailed for home, arriving on 2 September. She deployed again to Vietnam from 30 January–19 June 1967, and again ranged along that long embattled, highly indented coast. In March, she assisted in countering an enemy threat to the Marine artillery base at Gio Linh and evacuated wounded from Con Thien mountain. In April, she participated in \"Beacon Star\", in the Khe Sanh area, and supported search and destroy operations in conjunction with \"Shawnee\". In May, her helicopters lifted Marines to the DMZ to block enemy forces withdrawing across the Bến Hải River.\nA much-needed overhaul followed Princeton 's return to the west coast, and in May 1968 she again sailed west to Vietnam. There, as flagship for Amphibious Ready Group Alpha, she provided amphibious assault carrier services for operations \"Fortress Attack\" III and IV, \"Proud Hunter\", \"Swift Pursuit\", and \"Eager Hunter\". In December, she returned to the United States and in April 1969 she was designated the prime recovery ship for Apollo 10, the lunar mission which paved the way for Apollo 11 and the first manned landing on the Moon. Apollo 10 was recovered in the South Pacific on 26 May.\nOn 30 January 1970, Princeton was decommissioned and struck from the Naval Vessel Register, and sold for scrapping in May 1971.\nPrinceton received the Navy Unit Commendation for four periods: 5 December 1950 to 10 August 1951, 15 April to 18 October 1952, 13 March to 15 May 1953, and 11 June to 27 July 1953.\nThe flag of USS Princeton is now in Princeton University in the University Chapel.\nThe USS Princeton was used during the filming of the 1952 Monogram Pictures feature Flat Top\n|Wikimedia Commons has media related to USS Princeton (CV-37).|\n- This article incorporates text from the public domain Dictionary of American Naval Fighting Ships. The entry can be found here."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:52295cf9-f971-4d98-8802-fb58765b0d37>","<urn:uuid:ff3f8323-89d1-421c-ac55-34b652d07d4d>"],"error":null}
{"question":"Can both Buddha's hand citron and cherimoya fruits be stored at cold temperatures?","answer":"Yes, both fruits can be stored in cold temperatures, but with different considerations. For Buddha's hand citron, keeping it in a closed office over a weekend shows it tolerates room temperature well and releases a pleasant aroma. For cherimoya, ripe fruits should be refrigerated and eaten within one or two days. Hard, unripe cherimoyas can be ripened at room temperature.","context":["Winter is the season for citrus fruit, and January is the month for breaking out of old routines, so stop staring at your navel and learn about one of the weirder citrus varieties.\nI’ll never forget the day one of my general botany students brought to class a Buddha’s hand citron, pulled from a tree right outside our classroom. I had only recently moved to northern California from Indiana, and I’d never seen anything like it: it was a monstrous mass of a dozen pointed twisted fingers splayed irregularly from a stout base. It had the firm heft and girth of a grapefruit and the unmistakable pebbled skin of a citrus fruit, so I wondered whether my student had found a grossly deformed grapefruit; but the oil in the peel smelled heavenly and not at all like a grapefruit. In class we cut through a big finger and found no juicy segments, just white citrus pith all the way through.\nWe eventually discovered that this fascinating fruit was a Buddha’s hand citron, Citrus medica variety sarcodactylis, meaning fleshy (sarco-) fingered (-dactyl) citron. Since that day many years ago I’ve become an unapologetic (if surreptitious) collector of the fruits from that same campus tree. The citrons do not drop from the tree on their own, yet I often find one or two lying nearby, probably torn off by a curious tourist or student and then abandoned. Obviously these fruits need a good home, and where better than the window sill in my office?\nThe first time I left one closed up in my office over a weekend, I opened the door on Monday morning to a waft of fruity floral aroma. It turns out that many people in China and India use the fruit to scent the air, although in west Asia and Europe the fleshy fingers are more often candied or used to flavor alcohol. I do both: the fruits make my office smell nice until they are fully yellow, and then I cook them.\nIt can be difficult or expensive to get your own hands on a fingered citron, but it’s easy to find a navel orange almost any time of the year. Fortunately, the patterns underlying the morphology of the fingered fruit can also be seen in an everyday navel orange. Between our photos of Buddha’s hands and your own navel orange, you should be able to follow along at home.\nCarpels and segments\nA Buddha’s hand citron is a fruit, and like all simple fruits, it is the mature ovary of a flower. Citrus fruits – from kumquats to lemons to oranges to grapefruits – are technically berries because they are fleshy throughout and do not have hard pits or tough papery bits inside. More specifically, they are a special type of berry called a hesperidium, derived from the original name Linnaeus bestowed upon the citruses in lovely and colorful reference to the Golden Apples of the Hesperides. (For a lively detailed discussion of orange etymology, see Mabberley 2004).\nPlant ovaries (and resulting fruits) are made up of the lower parts of one or more carpels, the fundamental units of the “female” part of the flower (the gynoecium). Carpels arise at the tip of a flower extremely early in development, and in many species they gradually fuse together to form a single solid ovary with one or more styles and stigmas.\nUnder a microscope, the tip of a developing orange flower looks like a tall fluted bundt cake or a French cannelé, each hump a tiny incipient carpel (see images in Lima and Davies 1984). But by the time an orange flower opens, the outside of the ovary is totally smooth.\nThe carpels that make up a mature orange or grapefruit or lemon are obvious: they are the familiar (and convenient) individual wedge-shaped segments. Among my favorite botany fun facts is that the long pulpy sacs that fill the segments and cushion the seeds are actually individual hollow hairs, filled with juice.\nFingers and the failure to fuse\nThe gnarled fingers of a Buddha’s hand citron are simply separate carpels that somehow failed to fuse. This is evident when you look closely at the flowers and very young fruits of a Buddha’s hand. Instead of a smooth ovary made of united carpels, the young carpels of a fingered citron are already going off in their own directions.\nMany non-fingered citron varieties (such as the etrog) contain very little juicy pulp and are valued primarily for their scented zest and their very thick rinds, which are made into jam or candied. Cheap candied citron peel haunts the dreams of fruitcake haters, but homemade versions are amazing. The spongy white part of the rind (albedo) soaks up a lot of sugar syrup, and the zest (flavedo) provides an intense citrusy zing. Fingered citrons are even better suited for candying because they are basically nothing but rind and their fingered shape maximizes the ratio of flavedo to albedo. My recipe for homemade candied Buddha’s hand is below.\nFingered citrons also fail to make seeds and must be propagated intentionally by humans, so it was surprising when a recent study (Ramadugu et al., 2015) found a great deal of genetic diversity among eleven distinct varieties of fingered citrons from Yunnan Province in China, the presumed birthplace of fingered citrons. The authors speculate that the fingered mutation may have arisen independently several times from different non-fingered varieties, and that the trait also could have been passed on to hybrid offspring through pollen. Some varieties of the non-fingered types are noticeably ridged (see photos in Ramadugu et al. 2015), so perhaps there is a genetic predisposition towards making fingers. I have not found any studies that have specifically identified the mutation responsible for the fingered trait, but several genes are good candidates. (If you are familiar with research in this area, please leave a comment).\nFingers and navels\nFailure to fuse explains the separate fingers of a Buddha’s hand, but it doesn’t explain why there is usually more than one ring of them. To gain insight into this part of the mystery, we need to do a little navel gazing. This kind will be productive, I promise.\nSo what is the navel in a navel orange? Navel orange flowers make one normal ring of regular well-behaved carpels (the bundt cake described above) but then they produce a smaller second ring inside of that. The second ring becomes the “navel.” Sometimes the carpels in the second ring fail to fuse, and each is surrounded by rind, contributing to the iconic belly button look. Sometimes the rind around the second ring just gets lumpy and pushes itself into the segments of the first ring. Meanwhile, the tissue surrounding the first ring of carpels grows up around the second ring leaving a small opening through which the navel shows through (Lima and Davies 1984).\nA similar developmental pattern happens in fingered citrons, but to a more extreme degree. I’ve seen what look like three rings of carpels, although they are so irregular, it is hard to tell for certain that there are not four.\nBuddhas aren’t bitter\nCitrons (varieties of Citrus medica) represent one of the three original Citrus species from which most of our edible citrus fruits were derived through hybridization. The other two are Citrus maxima (pummelo) and Citrus reticulata (mandarin), and several recent phylogenetic studies have supported the monophyly of these three groups (e.g. Carbonell-Caballero et al. 2015).\nCitrons and mandarins are not bitter, whereas pummelos are. The bitterness of a citrus fruit depends on which of two enzymes is acting on a precursor molecule to generate a bitter flavanone or a flavorless one. Some hybrid species, such as grapefruit and bitter orange, express the bitter-flavanone-producing enzyme inherited from their pummelo parent. Interestingly, sweet oranges also have a pummelo parent; however in sweet oranges, a mutation in the bitterness gene disabled the pummelo version of the enzyme, leaving only the sweet mandarin parent’s version to operate in the fruit (Frydman et al. 2013).\nBecause citrons are not bitter, their zest can be used as an interesting substitute for lemon or orange zest, and it is not necessary to purge the rinds of bitterness before making them into candy garnishes.\nWhen my botany class met again after the Buddha’s hand show-and-tell, my student admitted to having served the prize fruit to his friends, sliced into vodka tonics. Well, that was one perfectly good way to use a Buddha’s hand, but below are two of my favorites.\nCandied Buddha’s hand citron\n- Wash a large fingered citron to remove any dirt clinging to it. You may have to separate or remove some of the fingers to get it all clean.\n- Chop the solid part of the fruit into chunks or sticks, as you wish. The pieces will shrink only slightly, so cut them to be close to their final size.\n- Cut the fingers into chunks or slice them lengthwise for curls. Very small fingers can be left whole.\n- Make a sugar syrup in the ratio of 2:1 sugar to water. For a large fruit, two cups of sugar in a cup of water should be more than enough. Ordinary table sugar works best in this recipe because it does not burn too quickly and it allows the citron flavors to shine through.\n- Heat the sugar and water gently to dissolve the sugar and add the chopped citron.\n- Continue to simmer the citron in the syrup, stirring frequently, until the pieces appear translucent and the syrup is thick but not caramelized.\n- Let the citron cool in the syrup, then transfer the pieces to a large plate and spread them out.\n- Any leftover syrup can be used as syrup or stirred into the marmalade recipe below.\nIf you plan to put them in fruit cake or figgy pudding, you can let the pieces dry for a couple of hours and refrigerate them until you are ready to bake. If you want them for candy or garnish, let the pieces dry for a couple of hours and roll them in powdered sugar to coat thoroughly.\nThey are beautiful strewn on top of an orange soufflé, straight from the oven, or on a scoop of ice cream.\nBuddha’s hand marmalade\nTo be honest, I just use the recipe at the Earthy Delights Recipe Blog.\nMy only comment on this recipe (besides that it’s very good!) is that my marmalade sometimes comes out too thick, with too much citrus chunk and not enough gelled matrix. The gelled matrix can also be too thick. Because there is so much pectin in the white (albedo) part of the fruit, this problem might be avoided by using only the pieces that have zest on them and leaving out some of the central mass of the fruit. No matter the texture, though, it tastes wonderful.\nCarbonell-Caballero, J., Alonso, R., Ibañez, V., Terol, J., Talon, M., & Dopazo, J. (2015). A phylogenetic analysis of 34 chloroplast genomes elucidates the relationships between wild and domestic species within the genus Citrus. Molecular biology and evolution, 32(8).\nFrydman, A., Liberman, R., Huhman, D. V., Carmeli‐Weissberg, M., Sapir‐Mir, M., Ophir, R., … & Eyal, Y. (2013). The molecular and enzymatic basis of bitter/non‐bitter flavor of citrus fruit: evolution of branch‐forming rhamnosyltransferases under domestication. The Plant Journal, 73(1), 166-178. http://onlinelibrary.wiley.com/doi/10.1111/tpj.12030/full\nLima, J. E. O., & Davies, F. S. (1984). Secondary-fruit ontogeny in navel orange. American journal of botany, 532-541.\nMabberley, D. J. (2004). Citrus (Rutaceae): a review of recent advances in etymology, systematics and medical applications. Blumea-Biodiversity, Evolution and Biogeography of Plants, 49(2-1), 481-498.\nRamadugu, C., Keremane, M. L., Hu, X., Karp, D., Federici, C. T., Kahn, T., … & Lee, R. F. (2015). Genetic analysis of citron (Citrus medica L.) using simple sequence repeats and single nucleotide polymorphisms. Scientia Horticulturae, 195, 124-137. http://dx.doi.org/10.1016/j.scienta.2015.09.004\nFor more about the origins of oranges: Wu, G. A., Prochnik, S., Jenkins, J., Salse, J., Hellsten, U., Murat, F., … & Takita, M. A. (2014). Sequencing of diverse mandarin, pummelo and orange genomes reveals complex history of admixture during citrus domestication. Nature biotechnology, 32(7), 656. http://www.nature.com/nbt/journal/v32/n7/full/nbt.2906.html","Cherimoya is a heart-shaped fruit grown on a mostly evergreen tree native to Ecuador and Peru.\nCherimoya fruit range in weight from about ¼ to 2½ pounds. The fruit has a dull green skin with thumbprint-like indentations edged in brown. Inside is a creamy, custard-like pulp with a flavor reminiscent of banana, pineapple, and pear.\nCherimoya is fast-growing, mostly evergreen tree. Cherimoyas can grow to 20 to 30 feet tall and about as wide. It can be trained and pruned to a lower height. Young branches grow opposite one another forming a natural espalier.\nCherimoya is sometimes called custard apple or sherbet fruit.\nCherimoya can be eaten chilled “on the half shell”. The flesh can be scooped with a spoon or diced and added to fruit salads or pureed for sherbet, ice cream, and daiquiris. Remove seeds before serving.\nHere is your complete guide to growing cherimoya.\nBest Climate and Site for Growing Cherimoya\n- Cherimoya is a subtropical plant or mild temperate climate; it will tolerate light frosts. Mature trees will be injured or killed at 25° In chilly winter regions plant cherimoyas in a sunny, south-facing location.\n- Cherimoyas require 50 to 100 chilling hours each winter in order to leaf out and set fruit in spring; chilling hours are hours at 43°F or lower. In temperate climates, the tree loses its leaves for a short period from late winter to early spring\n- Cherimoyas do not grow well where summer heat is dry. Cool coastal summer regions are best for growing cherimoya.\n- Plant cherimoyas in full sun. Be aware that leaves and fruit can sunburn if cherimoya is planted where the summer sun is intense and hot. If summers are very hot, plant cherimoya where it will get morning sun but will be sheltered from hot midday and late afternoon sun.\n- Cherimoya can be trained to espalier against a wall. Young branches grow opposite one another forming a natural espalier.\n- Plant cherimoyas in compost-rich, loamy soil that is well-drained. The optimum pH range is 6.5 to 7.6.\n- A cherimoya tree has both male and female flowers, but the two are not open at the same time. Female flowers open first for about 36 hours; the male flowers open after.\n- Cherimoyas are best hand pollinated. Collect pollen from the tan-colored anthers of male flowers with a small artist’s brush then apply the pollen to open female flowers. If no female flowers are open, save the pollen in a closed container in the refrigerator. Hand pollinate every two days during flowering to ensure pollination.\n- Pollination by insects is unlikely; few insects visit the flowers.\n- Cherimoya can grow to 30 feet tall and wide; some cultivars are smaller. Consider the size of the tree at maturity when planting.\n- Prepare a planting site in full sun that is sheltered from a prevailing breeze or wind.\n- Work well-rotted compost or manure into the soil.\n- Dig a hole half again as deep and twice as wide as the tree’s roots. Add a cupful of all-purpose fertilizer to the bottom of the hole.\n- Put a tree stake (or support wires for a fan) in place before planting. Drive the stake into the ground to the side of the hole to at least 2 feet deep. Cherimoyas are shallow-rooted; they should be staked at planting time.\n- Set the plant in the hole so that the soil mark from the nursery pot on the stem is at the surface level as the surrounding soil. Spread the roots out in all directions.\n- Re-fill the hole with half native soil and half aged compost or commercial organic planting mix; firm in the soil so that there are no air pockets among the roots. Water in the soil and create a modest soil basin around the trunk to hold water at watering time.\n- Secure the tree to the stake with tree ties.\n- After planting, water each tree thoroughly and fertilize with a high-phosphorus liquid starter fertilizer.\nContainer Growing Cherimoya\n- Cherimoyas are difficult to grow in containers because they grow a taproot and their size at maturity.\nCherimoya Care, Nutrients, and Water\n- Keep the soil evenly moist but not wet; cherimoyas are susceptible to root rot in wet soil. Do not water cherimoya trees when they are dormant.\n- Cherimoya will drop leaves if the roots go dry during the growing season. If leaves drop, fruit can suffer sunburn.\n- Feed cherimoya with a balanced organic fertilizer such as 10-10-10 or less. Feed trees every three months from midwinter to harvest.\nTraining and Pruning Cherimoya\n- Train cherimoyas to two scaffold branches at two feet above the ground. Prune young scaffolds back to about two feet. From the laterals, save the strongest shoots growing at 60 to 90 degrees and remove the others.\n- Each year when new growth starts, prune to keep the next harvest within easy reach; remove two-thirds of the past year’s growth leaving six to seven buds on each stem.\n- Each year remove broken, diseased, or crossing branches.\n- Prune cherimoyas when they are dormant.\nHarvesting and Storing Cherimoya\n- Cherimoya fruit is ripe when it gives slightly to thumb pressure. The fruit is overripe when it browns.\n- Hard, unripe cherimoyas can be ripened at room temperature.\n- Ripe cherimoyas should be refrigerated and eaten within one or two days.\n- Cherimoya can be propagated by seed; the seed is viable for two to three years if kept dry. Sow seed in deep containers; cherimoya forms a taproot.\nCherimoya Problems and Control\n- Yellow leaves are an indication the soil may be too dry, or the weather is too cold, not necessarily a lack of nitrogen.\n- Mealybugs suck sap from leaves and young branches; wash mealybugs off plants with a strong spray of water.\n- Snails will climb the tree and eat foliage; place copper tape or apply Tanglefoot (sticky goo) to masking tape placed around the trunk; these will prevent snails and other insects from reaching foliage and fruit.\n- Crown rot can kill trees planted in perpetually wet soil.\n- Verticillium fungal disease can cause branches and leaves to brown and die back; remove diseased branches and foliage and place them in the trash.\nFall and Winter Cherimoya Care\n- Protect young trees from frost; cover the tree with a plant blanket or wrap the trunk and scaffold branches with lengths of sponge foam.\nCherimoya Varieties to Grow\n- ‘Bays’: flavor good, almost lemony; round, medium-size fruit with light green, skin; broad tree to 20 feet tall.\n- ‘Big Sister’: good flavor; large, very smooth fruit; often self-fruitful.\n- ‘Booth’: flavor reminiscent of papaya; medium size, conical fruit; seedy; among hardiest of cherimoya; grows 20 to 30 feet tall.\n- ‘Chaffey’: very lemony flavor; small to medium size, round fruit; fast-growing in coastal areas.\n- ‘Ecuador’: good flavor; medium size, dark green fruit; spreading tree; cold tolerant.\n- ‘El Bumpo’: very flavorful; medium-size, conical fruit; skin is soft and almost edible.\n- ‘Honeyhart’: excellent flavor, very juicy; medium size fruit with smooth yellowish-green skin; ripens in winter.\n- ‘Knight’: very sweet; the flesh has a slightly grainy texture.\n- ‘Libby’: Sweet, strong flavor; round conical fruit; early harvest; large tree.\n- ‘McPherson’: flavor reminiscent of banana; small to medium fruit is conical and dark green; few seeds; the tree grows to 30 feet.\n- ‘Nata’: sweet-acid flavor balance; smooth, light green fruit to 2 ½ pounds; thin skin; tends to self-pollinate.\n- ‘Ott’: very sweet, medium-size fruit; yellow flesh; seedy; matures early.\n- ‘Pierce’: high sugar content; medium-size, elongated and conical fruit; smooth skin.\n- ‘Sabor’: very good flavor; small to medium-size fruit.\n- ‘Whaley’: good flavor; medium to large, elongated, conical fruit.\n- ‘White’: weak mango-papaya flavor; juicy flesh; large fruits to 4 pounds; tree grow to 35 feet.\nAlso of interest:"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:2a2c26fa-6eaa-46d7-9543-1ebbc9a069e6>","<urn:uuid:32ac2636-5d59-4efc-9feb-ea95984985e6>"],"error":null}
{"question":"What are the 4 basic elements needed for effective soundproofing?","answer":"The four basic elements of soundproofing are: 1) Decoupling, which prevents sound vibrations from traveling directly through walls, 2) Absorption using Sound Attenuation Batts (SABs) to reduce air cavity resonance, 3) Mass, which makes walls denser and harder for sound to move through using materials like QuietRock, and 4) Damping, which reduces drywall vibration using products like Green Glue between standard materials.","context":["Photo by Haggard Home Cabinetry & Design – Look for basement design inspiration\nBasic elements of Sound Proofing.\nThe first element is decoupling. Keep in mind that sound is nothing more than a vibration. The vibration will travel (conduct) easily if there is a nice solid direct pathway to follow, like a string between two cans. If we cut the string, however, we “decouple” the pathway, and the sound vibration stops (no conduction).\nObviously for soundproofing, we want less sound vibration to travel from one side of the wall to the other. It is therefore enormously beneficial if we decouple the framing in our walls and ceilings. It’s simple, inexpensive and highly effective.\nAir cavities will resonate. Ever “heard the ocean” in a seashell? Ever blow across the top of a bottle and heard the sound? Both sounds are actually the trapped air resonating. A hollow wall will also trap air that will resonate. When the wall is vibrated by sound (from your neighbor), the air in the wall cavity is also vibrated, just like a drum. This air cavity is another means for sound vibration to travel from one side of the wall to the other.\nEven though the wall framing is decoupled, the vibrating air cavity will still transmit some sound through the wall between the wall studs. Simple Sound Attenuation Batts (SABs) will absorb some of this (absorption).\nInsulation helps and should be done if possible, but the vibration reduction is smaller than the other 3 Elements. You can use other insulation materials as well like mineral wool. The key is to keep the density low. Don’t compress or pack the insulation.\nA very important element. For sound to conduct through a wall, it has to actually move the wall ever so slightly. Enter: MASS. In this case, we simply mean to make the walls as dense or heavy as you can. Materials like QuietRock (equivalent to 8 sheets of sheetrock) and Mass Loaded Vinyl are very effective.\nNote that adding mass improved things. It’s harder for sound to move this heavier wall. You will still hear low frequencies (bass) quite easily.\nThe last element for soundproofing. If we could reduce the drywall from vibrating in the first place, it would make the job of mass, insulation and decoupling easier and much more effective. After all, standard drywall is a HUGE surface area that is vibrating.\nThere are several products available that damp drywall. The highest performance for the lowest cost is Green Glue. Used between standard drywall, plywood, or subflooring, Green Glue damps a higher amount of vibration than any other material available.\nDamping the drywall on the sound producing side of the wall reduced all vibration immediately. Again, the decoupling, absorption, and mass all have less of a job to do, resulting in significantly improved performance.\nFind it all at:\n- Sound Proof Insulation – Mineral Wool Insulation / Sound Attenuation Batt Insulation (SAB)\n- Mass Loaded Vinyl MLV – Sound Barrier SB#1 SB#2\n- Soundproof Drywall QuietRock\n- Acoustical Caulk Green Glue\n- Resilient Isolation Channel & Clips Resilmount\n- Spray Foam Triple Foam, Great Stuff etc…\n- Acoustical Panels – SOUNDSTOP, Homasote\nSOUNDSTOP wood fiber sound board that is a high-quality, cost-effective solution to all sound deadening, soundproofing, or sound insulation needs. These acoustical panels come in 1/2″ 4′ x 8′ sheets and can be used in homes and commercial buildings where airborne noise and sound transmission from room to room needs to be eliminated. The product also blocks outside noise from heavy traffic, blaring horns, airports, children playing, or other exterior noises that are a concern.\nSOUNDSTOP takes shock or sound vibrations that travel through drywall and stops the movement of the sound or shock to the other side. The acoustic panel deadens sound transfer from shared walls, corridors, media rooms, workshops, etc.."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:e33b5233-dc2b-44dc-a7f8-44524d75c617>"],"error":null}
{"question":"What are the benefits of health screening tests for dog breeding, and how does denial impact breeders' approach to these tests?","answer":"Health screening tests are beneficial as they help identify potential hereditary issues even when no obvious problems exist, allow breeders to make informed breeding decisions, and help avoid producing puppies with health problems. They're particularly important since some conditions like Collie Eye Anomaly may go undetected for generations without proper screening. Regarding denial, it can manifest in several ways: some breeders avoid testing by claiming they've never had problems, others dismiss screening tests that don't give black-and-white results, and some may let their emotional attachment to their dogs cloud their judgment when receiving bad news. This denial can lead to missing serious conditions, as demonstrated by the example of elbow dysplasia which went undetected in Australian Shepherds until widespread screening began in Europe.","context":["Breeding Griffon Bruxellois is for the knowledgeable breeder. There are health issues to breed out and structural concerns to keep the breed away from. We’ve asked Jessica Darkle from the Griffon Bruxellois Club of Great Britain to write this introduction to Brussels Griffon breeding.\nA small, square, solid dog with bags of character and personality. They are monkeyish in appearance with a short muzzle, lustrous eyes and an intelligent expression. They are very comical and like to play the clown if they can. They are extremely loyal and affectionate to their family and some may be a little reserved with strangers.\nThe First World War and Second World War proved to be a disastrous time for the Griffon Bruxellois breed. War time is difficult on any dog breed, and the recovering numbers after the First World War were set back by increased vigilance in breeding away from faults such as webbed toes.\nBy the end of the Second World War, Belgium had almost no native Griffon Bruxellois left, and it was only through the vigilance of dedicated breeders (in the U.K. particularly) that the breed survived at all.Wikipedia\nWhen we talk about the Griffon, we must establish the differences between the Brussels Griffon (Griffon Bruxellois), the Belgian Griffon (Griffon Belge), and the Petit Brabançon.\nIn the UK we recognize only 2 varieties — the Griffon Belge is only a separate variety in other countries such as the United States.\nBoth the Griffon Bruxellois and Petit Brabançon are identical in the head, body shape, and size and come in 3 recognized colors. Red (various shades), Black and Tan and Black. The Griffon Bruxellois has a rough coat and grows a beard and mustache. The Petit Brabançon is smooth coated on the head and body and therefore much more easy to care for.\nBoth varieties are equally lovable and easy to live with.\nGrooming Of Griffon Dogs\nThe Griffon Bruxellois has a rough coat and grows a beard and mustache. It needs to be hand stripped or clipped to keep the coat tidy. Hand-stripping is quite hard work but can be mastered by most people. This is how the dog’s coats are prepared for the show ring. Hand-stripping helps to maintain the harshness and color of the coat. Or you can use a professional dog groomer.\nSome people just clip the dog’s coat but you loose some of the typical appearances if you do this. In between stripping the dogs should be combed through 2-3 times a week to remove dead coat and untangle the beard and mustache, which are longer than the coat on the body.\nThe Petit Brabançon is smooth coated on the head and body and therefore much more easy to care for. It needs only minimal grooming. Brushing with a soft bristle brush or hound glove will be enough to remove dead hair and keep a healthy shine on the coat.\nThe dogs need not be bathed unless they have managed to find something horrid to roll in.\nSome dogs get a skin fold over the nose as they mature, you will need to clean in between the folds on a daily basis with something that does not irritate the skin or eyes. Baby wipes can be good for this.\nEars should be cleaned with a proprietary dog ear cleaner when necessary.\nTeeth can be brushed with a small toothbrush and canine toothpaste.\nNails should be trimmed once or twice a month. Less, if they do wear down naturally with walking on a hard surface.\nHealth Concerns When Breeding Griffon Bruxellois\nThe Griffon is a tough, healthy little dog. But once they reach adulthood, like all dogs, they can suffer from a number of problems.\nSyringomyelia is a chronic progressive disease in which longitudinal cavities form in the cervical region of the spinal cord. This characteristically results in wasting of the muscles in the hands and a loss of sensation.\nAll dogs used for breeding must be MRI scanned and KC/BVA graded to determine whether or not they have Syringomyelia before they are bred from. All breeders must follow the published guidelines before mating two individuals.\nSyringomyelia is a neurological problem that is seen in a number of Toy breeds. The Club has been actively participating in research to try to eliminate this disease.\nThe Griffon can suffer from Patellar luxation (slipping kneecaps), this complaint is common to many small breeds.\nThe Griffon club has a Patella grading scheme. Dogs are graded by a vet from 0 (normal knees) to 4 (permanently dislocated knees). Dogs graded 3 and 4 should not be bred from unless the dislocation is due to injury only.\nAll Griffons should be eye tested for the presence of inherited and congenital eye disease. They should be tested before being bred and then at least every 3 years. There is a KC/BVA scheme that all breeders should take part in. Always ask to see the parent’s certificates before you buy a puppy.\nBrachycephalic Obstructive Airway Syndrome (BOAS)\nThe Griffon is a brachycephalic breed which means it has a short nose and a wide round head. This can cause breathing problems if too extreme.\nAll Griffons should be able to tolerate moderate exercise without having noisy breathing. Dogs with noisy breathing should not be bred from as they will pass this on to their puppies.\nUsing Inbreeding and Linebreeding\nInbreeding is something that should only be done by or on the advice of experienced breeders. Inbreeding can concentrate on the good things as well as the bad. Most good breeders will know all the dogs for many generations behind their dogs.\nExperienced Griffon breeders will know where it is safe to have a relative appearing on multiple occasions in a pedigree and where it is not. Very close inbreeding may be used occasionally to try to achieve a specific aim.\nMany British breeders will use a tool called Mate Select to work out the coefficient of inbreeding for any given mating and means that everyone can get an accurate measure of inbreeding. This helps them to be aware of the risks as well as the benefits.\nBreeding Practices When Working With Griffons\nMost breeders do take part in some health testing. If you are interested in a puppy the level of health testing should be one of the first questions you ask. All good breeders will be happy to show you their certificates and discuss any concerns you have. If they do not, simply do not buy a puppy from them, especially a puppy or dog that will be part of your breeding stock.\nThe Griffon is not a breed that can be puppy farmed so there are relatively few breeders compared to most Toy breeds. The majority of Griffon breeders rear their puppies in the home under good conditions. Always visit the puppies at the breeder’s house and ask to see the puppies with their Mother.\nIn the United Kingdom, the Kennel Club Accredited Breeder Scheme sets out minimum standards for breeding Griffons and the rearing of puppies, breeders who belong to this scheme have been checked by the Kennel Club as reaching these standards. These breeders also have to give full documentation and information with every puppy they sell, they have to undertake lifelong support for the new owner.\nThere are also many good breeders who are not members of any scheme.\nAlso, expect to be interrogated by the breeder. All good breeders take the utmost care to ensure that you can provide a suitable home and that you and the puppy will be the best possible match before letting one of their precious puppies go to a new home.\nPlease do not be offended by any question breeders may ask you, they are just trying to make sure that the puppy will have a permanent loving home.\nThe Griffon is a specialist breed. It is not a breed for the inexperienced. Take the time to learn as much as possible before embarking on breeding a litter.\nLitter sizes range from 1-6 puppies and need a lot of care for the first few weeks. It is not possible to breed them for profit.\nMake use of the combined knowledge of the breed clubs who are always happy to help and advise and make sure you use all the Kennel club’s “Mate Select” tools.\nKeep the health of the breed to the fore at all times and always put your dogs first.\nThe breed clubs are working hard to find a way to eliminate any health problems from the breed. Breeders are becoming more open to and aware of new technology and ways of breeding Griffon Bruxellois. I hope that in 20 years the Griffon will be just as popular and even more healthy.","If nothing’s gone wrong, what’s the need for health screening?\nby C.A. Sharp\nFirst published in Double Helix Network News Winter 2013\nThere are those who argue that there is no point in doing a health screening test unless you know you have a problem. Some of these individuals are speaking only about a particular test, but others extend it to all of them: Hips, elbows, eyes, the various DNA tests, etc. The reasoning they give to support their stands varies. Some say they’ve never had a problem in their dogs so they don’t need to check. Others imply that anyone who does test obviously has issues. There are those who dismiss any screening test that doesn’t give black-and-white results. A few will shrug and observe that some health issues are already so common in the breed that screening won’t make any difference – or that others are so rare there couldn’t possibly be any reason for testing for them ever.\nThe truth is there are no lines or even individual dogs that don’t have the potential to produce some sort of health issue. This isn’t meant as an indictment of purebreds; there aren’t any mutts out there that don’t have a few “bad” health genes, either. Same goes for you and me.\nFor breeders, though, following a screening protocol reflective of the issues in your breed and your line is good breeding practice, even when the results of some tests may not always provide nice neat answers. We owe it to our dogs, our breed, and the people who live with the dogs we produce to take reasonable precautions, including the use of screening tests, to avoid producing puppies that will at some point in life develop a hereditary health problem.\nThe devel you don’t know\nEven if you haven’t observed health issues in your dogs, the potential might still be there.\nAustralian Shepherds are active dogs who frequently exhibit a strong will to do some sort of work, be it the breed’s traditional stockwork or the plethora of more recently adopted activities people enjoy with their dogs. People who keep their dogs really active often assume that if they can work or perform, there can’t be any problem. But a high-drive dog may also have a high pain threshold, or the dog may be so focused on the activity it pursues that it will work in spite of pain. You cannot manage common pain-causing conditions like hip or elbow dysplasia in a breeding program without a thorough screening regimen for breeding dogs.\nCollie Eye Anomaly (CEA) isn’t much on anyone’s mind anymore even though it was a frequent issue in the 1980s and early 1990s. Research published in 1991 established that the disease in Aussies was a recessive. Those breeders who had the issue took steps to breed away from it. Recently it has begun showing up more frequently, possibly because it has fallen off most breeders’ radar.\nThe most common eye defect in dogs with CEA is choroidal hypoplasia (CH, called chorioretinal dysplasia in Europe.) While al affected dogs have CH they can also have optic nerve coloboma or retinal detachment, both of which can be blinding. Dogs with only CH have functional vision, so a breeder will never notice it simply by observing the dogs. To make CEA even trickier, CH often cannot be seen during an ophthalmologist’s exam once the pigment develops in the back of the eye at about 6-7 weeks of age. Since the disease is recessive it only happens when two carriers are bred and then only to roughly a fourth of the offspring. With all of this, if a breeder doesn’t consistently check every puppy in every litter before 6-7 weeks, CEA could go undetected for generations. Fortunately, there is also a DNA test which works no matter what age the dog is and will identify carriers, whose eyes are normal, as well as affected and clear dogs. Once the presence of the CEA mutation has been identified, related breeding stock should be screened and preference given to clear offspring to carry on with until all are out of two tested-clear parents, referred to as “cleared by parentage.”\nMost of our dogs’ inherited health issues are far more complex in inheritance than CEA and the only way to know for sure if there is risk in the pedigree is to know as much as possible about your dog’s direct progenitors and other near relatives. Unfortunately, that information can be hard to come by. Some people won’t share health information, others will outright lie, and a few unscrupulous souls will even resort to fraudulent practices by slipping in a ringer for a screening exam, or even a DNA test, if they know their dog won’t pass. The really creative may forge certificates. A good rule of thumb is: If a breeder is known to be unscrupulous, don’t deal with him, no matter how good his dogs are or how often he wins.\nSources of reliable health screening data, like the Orthopedic Foundation For Animals’ health database or those operated by some European kennel clubs can be extremely helpful. Even so, you can’t assume that no information on your dogs’ relatives means you are in the clear. Where reporting screening results is voluntary some breeders may elect not to list affected dogs with the registry. If a relative has not been screened or the results of that screening aren’t shared, you have no idea whether the dog is clear or not.\nThere is another devil we don’t know when it comes to health: Ourselves. Almost everyone in this game is there because they love dogs. Our own dogs are very special to us. They are our companions, members of the family and partners in our activities. When something goes awry we can let our feelings cloud our reason. If the dog is a breeding animal, discovering that it has or carries a health problem can feel like a blow to the gut. Denial is a normal emotional reaction to bad news and every breeder should be aware of this for her dogs’ sake. When something unwanted happens with one of your dogs audit your reactions and behavior to make sure you aren’t fooling yourself. If you have any doubt, there is no doubt: Do the necessary screening tests and take the appropriate steps to minimize the risk to your dogs and the breed.\nIf you don’t look at the trees in the forest, are they really there?\nFor most of the history of the Australian Shepherd, nobody checked for elbow dysplasia. In the US elbow screening still isn’t common. With ED, it’s easy to convince yourself that there is nothing wrong if you aren’t screening. Some affected dogs show no sign of elbow pain, others only have occasional front-end lameness which may shift from one side to the other. If your dogs are regularly involved in vigorous physical activity through work or competitive training, the occasional minor limp is easy to dismiss as a minor sports injury.\nWhen the Aussie population began to grow in Europe, where elbow exams are as expected as those for hips, cases started turning up regularly. I have been gathering health data on Aussies since the mid-1980s and for a long time had little information on elbow dysplasia in the breed. But when I first started getting data from the Swedish Kennel Club’s on-line database, there were roughly the same number of dogs listed as having ED as those with hip dysplasia. These dogs were not all related; their pedigrees included many of the lines available in Sweden at that time. This indicated that our failure to do elbow screening on our side of the Atlantic is missing a disease that has been under our noses for a long time.\nElbow screening is one of the exams required for Canine Health Information Center (CHIC) certification through OFA. Even so, it still isn’t standard practice when it should be. When you don’t bother to look for something, odds are you aren’t going to find it.\nKnow what you are looking for\nSometimes a breeder who might otherwise do screening tests might skip one because he mistakenly thinks his dogs are clear. There are a variety of diseases that were once cumulatively referred to as Progressive Retinal Atrophy (PRA.) Now that scientists can track down the individual genes that cause disease they have discovered that there are several genetically distinct types. PRA of any type is rare in Aussies, but most of those who do have it have the progressive rod-cone degeneration (prcd) form, which is what people typically test for.\nHowever, centralized PRA and other retinal diseases have been reported in Aussies and tests are available for some of these. Failure to do annual eye exams through at least 10 years of age, or assuming that any retinal defect found can’t be inherited because the dog or its parents have cleared the prcd test has the potential to result in increased frequency of those other and presently rare forms of retinal disease because breeders are makng mating choices based on false assumptions.\nGiven the multiple forms of PRA, if an Aussie is diagnosed with the disease it would be wise to confirm the diagnosis by having it tested for prcd (which Optigen, the lab offering the test, will do for no charge.) If the test confirms the diagnosis, breeding relatives need to have the test, too, so their status is known. If a dog with a PRA diagnosis does not have prcd, Optigen will also screen the sample to see if it might have one of the other forms for which they have a test.\nThe flip side of “scorched earth”\nThere are those who are overzealous about screening, especially through DNA tests. They advocate a short-sighted and ill-advised policy of eliminating from breeding any dog found to have a genetic mutation. The truth is all dogs have genetic mutations, but we can only test for a very few of them. Other people exhibit the flip side of the “scorched earth” approach, equally short-sighted and ill-advised, which is often expressed by a shrug of the shoulders and the declaration, “It’s everywhere, what difference does it make?”\nThis attitude allows those who employ it to cloak themselves in a comfortable blanket of denial and avoid the inconvenience of doing disease screening that may alter their breeding plans. While test results may mean you ought not to breed certain dogs having those results, especially from DNA tests, enables you to make informed breeding decisions that could help you breed away from disease at issue.\nShades of grey\nDNA tests are becoming more and more common. They are more useful for a breeder than other types of screening tests because they reveal the dog’s genotype: You know exactly what gene forms the dog can pass along. Initially, the tests were for simple dominant or recessive mutations. The results revealed the dog’s genotype and enabled breeders to avoid producing affected offspring by not doing crosses which could produce them. But as time went by, researchers began identifying genes that were not so simple. Many of these confer risk factors: Their presence does not necessarily mean the dog will get sick but rather that it has a heightened chance of doing so. Because these tests don’t offer black-and-white results, some advocate not using them at all.\nThis is yet another form of denial. Results showing the presence of this type of mutation, even in a double dose, don’t necessarily mean the dog should be washed out of a breeding program. How frequent is this mutation in the breed? Is the disease potentially lethal or does it cause serious quality of life issues? While dogs affected with serious diseases ought not to be bred further, often – depending on the specific disease – healthy dogs, even with two copies, might be bred to clear-tested mates. Within a few generations the mutation can be significantly reduced in frequency or, if it is uncommon, eliminated entirely. With the tests for these risk-factor mutations a breeder is in a far better position to decide the best crosses to make to avoid the disease.\nThe finger-pointing game\nYou can set yourself up for an unpleasant surprise by assuming that breeders who do a screening test, particularly one that isn’t considered more-or-less standard in the breed, are the only ones whose dogs have that disease. Those who are most scrupulous about screening tests are frequently the same people who will be most open about health issues they have encountered in their dogs. Their diligence and honesty may give the misleading impression that their lines have more problems than others simply because the information has been made available. Those who don’t bother to screen, or claim they’ve never had any problems may simply not know because they don’t check or follow up on dogs they’ve sold. And there are the few hard cases around who spear tar on the ethical to advance their own agendas: The deliberate, and often loud, dissemination of innuendo and disinformation is an effective way to redirect attention away from one’s own issues.\nAn ounce of prevention\nThe truth is, screening tests can be very useful breeders’ tools. They are not all one in the same and you need to educate yourself about what the results mean, both for the health of the dog and for your breeding program. You also need to know which tests should be standard in the breed and which used only in certain circumstances. If the latter, know what those circumstances are. Far better to get unpleasant news then proceed based on that knowledge than to avoid screening and blindly hope for the best.\nIf someone asks, “Why bother?” Respond, “Because we must, for the benefit of our breed, our breeding programs and our dogs.”"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:122b1ffa-8844-4847-96be-20aff9cf1c3b>","<urn:uuid:280f1954-5be7-4391-aa1f-b4052f17da68>"],"error":null}
{"question":"What are the key considerations for designing a truly energy-efficient laboratory building instead of following passive building standards?","answer":"To design truly energy-efficient laboratories, it's essential to consider the specific characteristics of the building and its use, rather than relying on standardization. The approach should focus on coordinating all project-related needs and possibilities, balancing energy efficiency, cost-effectiveness, functionality, and comfort. Using efficient heat recovery and reuse of WRG can reduce overall heating demand by 43%, heating power demand by 28%, and air conditioning cooling by 2%. However, the design must be tailored to the laboratory's specific requirements rather than following residential passive building standards, as laboratories have fundamentally different energy needs and usage patterns.","context":["When designing energy-efficient buildings, “passive” building standards are often required. However, some of the standards and boundary conditions do not play their due role in laboratory building design.\nIs it economical to build a laboratory according to passive low energy standards? This article gives detailed answers.\nFigure 1. Energy consumption needs to be optimized in the lab building as well.\nThe special requirements of laboratory work tend to make passive low-energy building standards difficult to implement, while also increasing construction costs. The insulation standards of passive building construction can even have the opposite effect.\nComparing passive building construction with laboratory building construction, we clearly found that implementing passive building standards does not necessarily lead to high energy-saving effects, but can achieve energy-saving characteristic values without taking other measures. In other words: the feasibility of implementing passive building standards has great limitations.\nScience time is up!\nThe concept of passive building construction usually refers to the use of special efficient insulation measures to create a comfortable temperature environment (shown in Figure 2) in the room only by heating or cooling the fresh air. In such buildings, there is no need for active heating of traditional building techniques to heat the interior of the building.\nIn order to accurately assess whether a building can meet passive building requirements, people have established requirements, specific objectives, and constraints to achieve passive building construction, and as a guide for building design work.\nThe above criteria are based on the technical standards of residential buildings and describe the indoor temperate climate: 15 kWh/m2a per square metre and 120 kWh maximum disposable energy demand. /m2a. It should be noted that disposable energy is energy obtained from natural energy carriers or natural resources.\nIn order to use, store and transport such a disposable energy source, it must first be converted into a secondary energy source, for example into thermal energy. In the process of energy conversion, energy loss will inevitably occur, reducing the amount of energy actually used by consumers. The possibility of using disposable energy directly is low and not significant.\nIn order to be able to compare different types of disposable energy sources, such as comparing one-time energy sources with different characteristics, different types of energy carriers have a one-time energy coefficient. Using this weighting factor, you can get the best energy demand, which is the maximum 120kW/(m2a) in the active building construction standard.\nCompared to residential buildings, laboratory buildings have high indoor loads and high ventilation during laboratory work. For example, the indoor load is 80W/m2, of which the laboratory equipment consumes 55W/m2, the illumination is 15W/m2, and the personnel is 10W/m2. In addition to ventilation and ventilation, the laboratory ventilation is usually 25m3/m2/h. According to the indoor lighting amount of 15W/m2, the total power consumed for 2,500 hours per year is 37.5 kWh/m2a. Considering the one-time energy factor of 2.6, the total amount of disposable energy required is about 97.5 kWh/m2a.\nIn addition, ventilation and ventilation equipment also consume a certain amount of electrical energy. The power energy consumed by ventilation and ventilation can be calculated at around 25W/m2. According to the calculation of 2500h per year, the annual energy consumption of ventilation and ventilation is about 62.5kWh/m2a; the conversion to one-time energy is 163 kWh/m2a.\nIn this way, the single-use energy required for lighting and ventilation alone is as high as 260.5 kWh/m2a. Assume that the demand for electric energy from other electrical equipment, laboratory equipment, and air-conditioning equipment is 38.5 kWh/m2a (approximately 100 kWh/m2a of disposable energy), and the total amount of disposable energy is 360.5 kWh/m2a. This value is already three times higher than the value specified in the passive building standard; although it is assumed that central heating has been fully adopted, no form of disposable energy is consumed.\nIn practice, it is generally assumed that the heating demand of a passive building is 20 kWh/m2a. This data can also be adopted in order to make comparisons and estimates closer to reality. As a result, the total energy consumption required for a hypothetical laboratory building is as high as 380.5 kWh/m2a (see Figure 3).\nDue to different uses, the laboratory's minimum energy requirements are much higher than the energy requirements of passive low-energy residential standards. The passive low-energy residential standard allows for a maximum disposable energy requirement of 120 kWh/m2a, which only meets the needs of laboratory lighting and ventilation, and all laboratory equipment in the laboratory cannot be used.\nTable 1. Comparison of traditional building methods and passive low-energy building methods\n\"Passive\" insulation regulations have almost no advantages\nThe peripheral structure of the building also has a strong influence on the heating and cooling effects of the laboratory building. A number of specific architectural design thermal simulations clearly point out that the use of passive building standards in laboratory buildings has a certain degree of significance, but it has no economic significance in the construction process.\nBy implementing the EnEV 2009 standard, building a laboratory with passive low-energy buildings can reduce the overall heating demand of buildings by 25%, the required heating power demand by 10%, and air-conditioning cooling by 1%. If the use of efficient heat recovery and reuse of WRG can even reduce the overall heating demand by 43%. The required heating power demand is reduced by 28% and air conditioning cooling is reduced by 2%. The capital expenditure required to achieve these savings is not proportional to the relatively improved energy efficiency.\nTherefore, the current passive low-energy residential building standards do not apply to laboratory buildings with higher energy requirements. Higher payouts from laboratory building planning and investment, especially from the perspective of the typical life of a laboratory building, are not economical.\nIn order to be able to build truly energy-efficient laboratories, it is necessary to consider the characteristics of the building, especially the characteristics of its use: characteristics that cannot be achieved through standardization. It still needs to be as it is before: to coordinate all project-related needs and possibilities, and to coordinate energy efficiency, cost-effectiveness, functionality and comfort."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:5d4d853b-128e-471f-afd7-4d7bc1080b9d>"],"error":null}
{"question":"What is the relationship between Sartre's concept of absurdity in human existence and the philosophical debate between determinism and free will? How do both perspectives address the fundamental nature of human choice and consciousness?","answer":"Sartre's concept of absurdity, as illustrated through Roquetin's experiences in Nausea, emphasizes the disturbing nature of fully comprehending existence and the importance of conscious choice in creating meaning. This connects to the determinism-free will debate, where determinists argue all events are caused by previous events, while different views like compatibilism suggest that free will can coexist with determinism. Sartre's focus on the overwhelming nature of consciousness and choice aligns with questions about human agency that both determinists and free will advocates grapple with. While Sartre emphasizes the importance of authentic choice in creating meaning despite life's absurdity, compatibilists attempt to reconcile deterministic causation with meaningful choice by defining free will as the ability to act according to one's own motivations, even if those motivations are themselves determined.","context":["Sartrian themes exist as repeated thoughts but stand as philosophies based on existentialism (what many would call a pessimistic or possibly realistic view of being). Existentialism encompasses many thoughts such as the idea that someone can only judge one’s life at the end. It also includes the idea that nothing really matters except our choices for they put meaning to our lives. Jean Paul Sartre also discusses the absurdity of life and how if we truly realize life, it is too much to bear. In addition, Sartre wrote about existence and essence as well as his ideas on “good” and “bad” faith. Sartre had a wide range of topics which he implemented through the stories of the people in his works.\nIn many of his writings, Jean Paul Sartre depicted what he called “bonne foi” (good faith) and “mauvaise foi” (bad faith).A person with good faith would recognize failed choices and take responsibility for his or her decisions. Such is the case with Inez in No Exit. After reaching hell where she will rest for all eternity, Inez admits to her mistakes in her life and identifies them without reserve. She accepts her fate of imprisonment with two other people. Similarly, Hugo in Sartre’s play, Dirty Hands, baffles through life, constantly questioning his decisions, wondering who prevails as politically superior and whether he should assassinate a great thinker, Hoederer. He eventually resolves to kill him, but it is only later, directly preceding his death, that he realizes what it all means. He states right beforehe lets the group kill him, “I have not yet killed Hoederer… But I am going to kill him now, along with myself.” It is in this moment, at the very end, that he distinguishes his choices and meaning in life. At the same time, Sartre includes several characters with bad faith who have the attributes of pride or a worry about their appearance. They continually focus on what others think of them and do not recognize their faults. Estelle and Garcin in No Exit both do not admit to their crimes until cornered into professing their truths. Estelle acts as a rich woman who never really did anything wrong and Garcin acts as a brave journalist who died fighting for peace. In truth, Estelle murdered her own child and Garcin ran away from his military duties out of fear. In this current hell, Garcin realizes, “if there’s… one person to say… that I’m brave and decent and the rest of it—well, one person’s faith would save me.” Yet it is too late: he made his choices and would not admit to them later. No one trusts or has this “faith” in him; thus he is doomed to an eternal hell. Also, in Nausea, a character referred to as the Self-Taught Man becomes disgruntled near the end, not feeling a purpose, touches a young boy in public and has an unhappy fate. He gave up, sensing that his choices didn’t matter at all. Therefore he, too, had bad faith. Also, Anny, Antoine’s former girlfriend, meets with Antoine again and counts him as a “milestone” in her life. She depends on him as a justification for her life. She has not created her own meaning and as a result, has bad faith. The contrast between these characters of good and bad faith exemplifies Sartre’s ideas that we must focus on ourselves and our decisions.\nSartre also often portrayed the complete absurdity of our existence. He demonstrated this by explaining that if we fully comprehended the disgusting nature of living, we could not bear it. In the case of Roquetin in Nausea, Roquetin often faces the ugliness of his surroundings and is consumed by a dizziness and nausea. At times, Sartre describes certain body parts as filth which causes Roquetin to feel revulsion at himself and those whom others might describe as desirous. Another source of his sickness is through the choices he faced: where he should go to eat, whether he should visit Anny, etc. He helps this by having a daily routine, one that is safe and repetitive (free from choice). However, Roquetin can never escape the mysteries of life; even when he peers into a mirror, his eyes become locked and he cannot stop looking for hours because he is “lost in the vagueness” which makes him “dizzy.” After looking at himself for a long period of time, he realizes, “what I see is well below the monkey, on the fringe of the vegetable world, at the level of a jellyfish.” Looking a himself fully perplexes him to the point where he stares and “the eyes, nose and mouth disappear: nothing human is left.” Sartre questions reality and what is really alive. If we do not even know ourselves, we do not know if life is true. This absurdity, in full awareness, is disturbing.\nYet at the same time, there is a freeing quality of awareness—realizing the question of life also ironically allows him to grasp his existence. Roquetin repeats the phrase “I exist” countless times (perhaps in an effort to remind himself that he is real). Yet he eventually absorbed his existence and knows that there is no method of escape from it. For a great deal of the time, Roquetin hides behind his focus of study the Marquis de Robellon by fulfilling his life through this historical persona. Despite this, Roquetin eventually uncovers this truth fro himself and concludes to never hide behind this mask. Again, he facilitates the importance of choice.\nAcceptance appears as another common theme in the works of Sartre. In No Exit, Garcin accepts that Inez is his hell, and he must wait for her change of mind about his cowardice before he can be free. Estelle begs him to throw Inez out and “shut the door,” but he will not. Eventually he knows; “Dead! Dead! Knives, poison, ropes—all useless. It has happened already… Once and for all. So here we are forever… and ever… well, let’s get on with it.” He accepts the truth that hell is other people (another of Sartre’s themes) and goes on living out his hell. Similarly, Hugo accepts his fate by announcing himself, “unsalvageable” (a harsh word to describe a human) for the party and gives up his life. Unhappiness and evil are others; yet we must accept this and continue with our lives.\nSartre’s themes are more than just themes relating human behavior. They transcend people and space. They act as maps showing us how to live in this absurd world. All his ideas overlap and lead to a Sartrian universaltruth that nothing truly matters but what we make of life through decisions and faithfulness to ourselves.","Compatibilism is the belief that free will and determinism are mutually compatible and that it is possible to believe in both without being logically inconsistent.\nCompatibilists believe freedom can be present or absent in situations for reasons that have nothing to do with metaphysics.They say causal determinism does not exclude the truth of possible future outcomes.\nSimilarly, political liberty is a non-metaphysical concept.Statements of political liberty, such as the United States Bill of Rights, assume moral liberty: the ability to choose to do otherwise than one does.\nCompatibilism was championed by the ancient Stoicsand some medieval scholastics (such as Thomas Aquinas). More specifically, scholastics like Thomas Aquinas and later Thomists (such as Domingo Báñez) are often interpreted as holding that a human action can be free even though the agent in some strong sense could not do otherwise than he did. Whereas Aquinas is often interpreted to maintain rational compatibilism (i.e., an action can be determined by rational cognition and yet free), later Thomists such as Báñez develop a sophisticated theory of theological determinism, according to which actions of free agents, despite being free, are, on a higher level, determined by infallible divine decrees manifested in the form of \"physical premotion\" (praemotio physica), a deterministic intervention of God into the will of a free agent required to reduce the will from potency to act. A strongly incompatibilist view of freedom was, on the other hand, developed in the Franciscan tradition, especially by Duns Scotus, and later upheld and further developed by Jesuits, esp. Luis de Molina and Francisco Suárez. In the early-modern era, compatibilism was maintained by Enlightenment philosophers (such as David Hume and Thomas Hobbes).\nDuring the 20th century, compatibilists presented novel arguments that differed from the classical arguments of Hume, Hobbes, and John Stuart Mill.Importantly, Harry Frankfurt popularized what are now known as Frankfurt counterexamples to argue against incompatibilism, and developed a positive account of compatibilist free will based on higher-order volitions. Other \"new compatibilists\" include Gary Watson, Susan R. Wolf, P. F. Strawson, and R. Jay Wallace. Contemporary compatibilists range from the philosopher and cognitive scientist Daniel Dennett, particularly in his works Elbow Room (1984) and Freedom Evolves (2003), to the existentialist philosopher Frithjof Bergmann. Perhaps the most renowned contemporary defender of compatibilism is John Martin Fischer.\nCompatibilists often define an instance of \"free will\" as one in which the agent had freedom to act according to their own motivation . That is, the agent was not coerced or restrained. Arthur Schopenhauer famously said, \"Man can do what he wills but he cannot will what he wills.\"In other words, although an agent may often be free to act according to a motive, the nature of that motive is determined. This definition of free will does not rely on the truth or falsity of causal determinism. This view also makes free will close to autonomy , the ability to live according to one's own rules, as opposed to being submitted to external domination.\nSome compatibilists will hold both causal determinism (all effects have causes) and logical determinism (the future is already determined) to be true. Thus statements about the future (e.g., \"it will rain tomorrow\") are either true or false when spoken today. This compatibilist free will should not be understood as some kind of ability to have actually chosen differently in an identical situation. A compatibilist can believe that a person can choose between many choices, but the choice is always determined by external factors.If the compatibilist says \"I may visit tomorrow, or I may not\", he is saying that he does not know what he will choose—if he will choose to follow the subconscious urge to go or not.\nAlternatives to strictly naturalist physics, such as mind–body dualism positing a mind or soul existing apart from one's body while perceiving, thinking, choosing freely, and as a result acting independently on the body, include both traditional religious metaphysics and less common newer compatibilist concepts.Also consistent with both autonomy and Darwinism, they allow for free personal agency based on practical reasons within the laws of physics. While less popular among 21st century philosophers, non-naturalist compatibilism is present in most if not almost all religions.\nA prominent criticism of compatibilism is Peter van Inwagen's consequence argument.\nCritics of compatibilism often focus on the definition(s) of free will: incompatibilists may agree that the compatibilists are showing something to be compatible with determinism, but they think that this something ought not to be called \"free will\". Incompatibilists might accept the \"freedom to act\" as a necessary criterion for free will, but doubt that it is sufficient. Basically, they demand more of \"free will\". The incompatibilists believe free will refers to genuine (e.g., absolute, ultimate) alternate possibilities for beliefs, desires, or actions, rather than merely counterfactual ones.\nCompatibilism is sometimes called soft determinism (William James's term) pejoratively .James accused them of creating a \"quagmire of evasion\" by stealing the name of freedom to mask their underlying determinism. Immanuel Kant called it a \"wretched subterfuge\" and \"word jugglery\". Kant's argument turns on the view that, while all empirical phenomena must result from determining causes, human thought introduces something seemingly not found elsewhere in nature—the ability to conceive of the world in terms of how it ought to be, or how it might otherwise be. For Kant, subjective reasoning is necessarily distinct from how the world is empirically. Because of its capacity to distinguish is from ought, reasoning can 'spontaneously' originate new events without being itself determined by what already exists. It is on this basis that Kant argues against a version of compatibilism in which, for instance, the actions of the criminal are comprehended as a blend of determining forces and free choice, which Kant regards as misusing the word \"free\". Kant proposes that taking the compatibilist view involves denying the distinctly subjective capacity to re-think an intended course of action in terms of what ought to happen. Ted Honderich explains his view that the mistake of compatibilism is to assert that nothing changes as a consequence of determinism, when clearly we have lost the life-hope of origination.\nFree will is the ability to choose between different possible courses of action unimpeded.\nDeterminism is the philosophical belief that all events are determined completely by previously existing causes. Deterministic theories throughout the history of philosophy have sprung from diverse and sometimes overlapping motives and considerations. The opposite of determinism is some kind of indeterminism or randomness. Determinism is often contrasted with free will.\nWill, generally, is the faculty of the mind that selects, at the moment of decision, a desire among the various desires present; it itself does not refer to any particular desire, but rather to the mechanism responsible for choosing from among one's desires. Within philosophy, will is important as one of the parts of the mind, along with reason and understanding. It is considered central to the field of ethics because of its role in enabling deliberate action.\nIncompatibilism is the view that a deterministic universe is completely at odds with the notion that persons have a free will; that there is a dichotomy between determinism and free will where philosophers must choose one or the other. This view is pursued in at least three ways: libertarians deny that the universe is deterministic, the hard determinists deny that any free will exists, and pessimistic incompatibilists deny both that the universe is determined and that free will exists.\nFatalism is a philosophical doctrine that stresses the subjugation of all events or actions to Fate or destiny, and is commonly associated with the consequent attitude of resignation in the face of future events which are thought to be inevitable.\nIndeterminism is the idea that events are not caused, or not caused deterministically.\nLibertarianism is one of the main philosophical positions related to the problems of free will and determinism, which are part of the larger domain of metaphysics. In particular, libertarianism is an incompatibilist position which argues that free will is logically incompatible with a deterministic universe. Libertarianism states that since agents have free will, determinism must be false. One of the first clear formulations of libertarianism is found in John Duns Scotus; in theological context metaphysical libertarianism was notably defended by Jesuit authors like Luis de Molina and Francisco Suárez against rather compatibilist Thomist Báñezianism. Other important metaphysical libertarians in the early modern period were René Descartes, George Berkeley, Immanuel Kant, and Thomas Reid. Roderick Chisholm was a prominent defender of libertarianism in the 20th century, and contemporary libertarians include Robert Kane, Peter van Inwagen and Robert Nozick.\nHard determinism is a view on free will which holds that determinism is true, and that it is incompatible with free will, and, therefore, that free will does not exist. Although hard determinism generally refers to nomological determinism, it can also be a position taken with respect to other forms of determinism that necessitate the future in its entirety. Hard determinism is contrasted with soft determinism, which is a compatibilist form of determinism, holding that free will may exist despite determinism. It is also contrasted with metaphysical libertarianism, the other major form of incompatibilism which holds that free will exists and determinism is false.\nPredeterminism is the philosophy that all events of history, past, present and future, have been already decided or are already known, including human actions.\nPeter van Inwagen is an American analytic philosopher and the John Cardinal O'Hara Professor of Philosophy at the University of Notre Dame. He is also a Research Professor of Philosophy at Duke University each Spring. He previously taught at Syracuse University and earned his PhD from the University of Rochester in 1969 under the direction of Richard Taylor. Van Inwagen is one of the leading figures in contemporary metaphysics, philosophy of religion, and philosophy of action. He was the president of the Society of Christian Philosophers from 2010 to 2013.\nRobert Hilary Kane is an American philosopher. He is Distinguished Teaching Professor of Philosophy at the University of Texas at Austin, and is currently on phased retirement.\nTheological determinism is a form of predeterminism which states that all events that happen are pre-ordained, and/or predestined to happen, by one or more divine beings, or that they are destined to occur given the divine beings' omniscience. Theological determinism exists in a number of religions, including Jainism, Judaism, Christianity and Islam. It is also supported by proponents of Classical pantheism such as the Stoics and Baruch Spinoza.\nIn philosophy, moral responsibility is the status of morally deserving praise, blame, reward, or punishment for an act or omission performed or neglected in accordance with one's moral obligations. Deciding what counts as \"morally obligatory\" is a principal concern of ethics.\nKantian ethics refers to a deontological ethical theory ascribed to the German philosopher Immanuel Kant. The theory, developed as a result of Enlightenment rationalism, is based on the view that the only intrinsically good thing is a good will; an action can only be good if its maxim—the principle behind it—is duty to the moral law. Central to Kant's construction of the moral law is the categorical imperative, which acts on all people, regardless of their interests or desires. Kant formulated the categorical imperative in various ways. His principle of universalizability requires that, for an action to be permissible, it must be possible to apply it to all people without a contradiction occurring. Kant's formulation of humanity, the second section of the Categorical Imperative, states that as an end in itself humans are required never to treat others merely as a means to an end, but always, additionally, as ends in themselves. The formulation of autonomy concludes that rational agents are bound to the moral law by their own will, while Kant's concept of the Kingdom of Ends requires that people act as if the principles of their actions establish a law for a hypothetical kingdom. Kant also distinguished between perfect and imperfect duties. A perfect duty, such as the duty not to lie, always holds true; an imperfect duty, such as the duty to give to charity, can be made flexible and applied in particular time and place.\nFrankfurt cases were presented by philosopher Harry Frankfurt in 1969 as counterexamples to the principle of alternate possibilities (PAP), which holds that an agent is morally responsible for an action only if that person could have done otherwise.\nMetaphysics is the branch of philosophy that investigates principles of reality transcending those of any particular science. Cosmology and ontology are traditional branches of metaphysics. It is concerned with explaining the fundamental nature of being and the world. Someone who studies metaphysics can be called either a \"metaphysician\" or a \"metaphysicist\".\nIllusionism is a metaphysical theory first propounded by professor Saul Smilansky of the University of Haifa. Although there exist a theory of consciousness baring the same name (illusionism), It is important to note that the two theories are concerned with different subjects. Illusionism as discussed here, holds that people have illusory beliefs about free will. Furthermore, it holds that it is both of key importance and morally right that people not be disabused of these beliefs, because the illusion has benefits both to individuals and to society. Belief in hard incompatibilism, argues Smilansky, removes an individual's basis for a sense of self-worth in his or her own achievements. It is \"extremely damaging to our view of ourselves, to our sense of achievement, worth, and self-respect\".\nFree will in antiquity is a philosophical and theological concept.\nDickinson Sargeant Miller was an American philosopher best known for his work in metaphysics and the philosophy of mind. He worked with other philosophers including William James, George Santayana, John Dewey, Edmund Husserl, and Ludwig Wittgenstein.\nIn the theory developed by Domingo Báñez and other 16th and 17th century Thomists, physical premotion is a causal influence of God into a secondary cause which precedes and causes the actual motion of its causal power : it is the reduction of the power from potency to act. In this sense, it is a kind of divine concursus, the so-called concursus praevius advocated by the Thomists."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:c501b715-5d22-4be0-845b-9d521e2de32c>","<urn:uuid:9483c194-c173-4ae0-bd8b-a82912451b17>"],"error":null}
{"question":"How does the Japanese prefabrication industry combine large-scale industrialization with energy efficiency, and what technological innovations are being implemented in modern prefab homes?","answer":"The Japanese prefabrication industry achieves large-scale industrialization through automated processes and high production volumes, with companies producing up to 150,000 housing units annually. They utilize standardized processes, just-in-time manufacturing, automation, and flow line-like production in their factories. For energy efficiency and technological innovation, modern prefab homes incorporate features like spray-foam insulation, high-quality solar panels, automated LED lighting, green appliances, and energy-efficient doors and windows. Companies like Sekisui House have introduced advanced prefabricated homebuilding technology called Shawood, which uses computer-aided precision engineering to create components that are more resilient to natural disasters and enable faster, more precise construction processes.","context":["Journal of Construction Innovation: Information, Process, Management,\nVolume 12 issue 2 https://doi.org/10.1108/14714171211215921\n© Emerald Group Publishing Limited 2012\nTechnische Universität München\nTechnische Universität München\nEvolution of large-scale Industrialisation and Service -Innovation in Japanese Prefabrication Industry\nPurpose – The Japanese prefabrication industry not only has automated its processes to a high extent, but it also innovates due to the fact that it delivers buildings of outstanding quality accompanied by a multitude of services. In order to explore and specify the concepts and parameters that have driven this industry, Japan’s prefabrication industry, its cultural, economic and technological surrounding, as well as the applied processes, technologies and economic strategies, have to be illustrated and analyzed. The proposed research aim was to identify, describe and analyse these concepts and their related parameters, as well as to recognize the most influential drivers for the future, that provide an indication into which direction the industry could evolve.\nResearch Methodology – Being aware that literature does not provide relevant information and data, which would allow the authors to explore concepts and parameters explaining the success of the Japanese prefabrication industry, the authors performed field surveys, visited factories, R&D centres and sales points of all major Japanese prefabrication companies. In some cases authors also interviewed general managers, researchers and developers, and academicians at Japanese universities. Based on an extensive literature review in the area of product development, production technology, modularisation, mass customisation, and innovation, the authors qualitatively and quantitatively analysed all major prefabrication companies according to a fixed scheme.\nFindings – The concepts and parameters identified and analyzed in this paper, demonstrate that the Japanese prefabrication industry, which is leading in large-scale industrialization, nowadays focuses towards services that are related to the building’s utilization phase, rather than delivering products. By involving customers it enhances the companies’ customer relations, creating thus competitive advantages.\nOriginality/ value – Overall the paper shows that Japanese prefabrication industry rather acts like a “production industry” than a “construction industry”. Similar to many other high-tech industries, Japan’s prefabrication industry incorporates latest product and process technologies and combines automation, products and services into complex value-capturing systems.\nKeywords: Building Production, Automation in Construction, Product and Service Innovation, Mass Customization, Japanese Prefabrication Industry\nCategory: Research/ Case Study\nPrefabrication holds a considerable share in the housing market in many European countries, such as Germany (15%), Austria (up to 33%), France (5%), Spain (5%) and the Scandinavian Countries. In Germany, currently about 20,000 prefabricated houses are delivered per year by companies as Bien-Zenker AG (580 houses/ year), Huf Haus GmbH (450 houses/ year), Weber Haus GmbH (600 houses/ year) and Kampa House GmbH (1,086 houses/ year). The advantage of buying prefabricated buildings is the low price compared to conventionally built buildings and the relatively rapid delivery (about six days period of delivery). However, the European prefabrication industry cannot really be considered industrialized. According to classical theory (e.g. Henry Ford’s Mass Production) as well as modern concepts (e.g. flexible manufacturing, mass customization), industrialization implies that large scales of products were produced using latest production technology, automation, robotics, Information and Communication Technology (ICT), in order to deliver high quality products with reasonable costs. In Europe, no prefabrication company has yet reached the critical mass of an annual production, which would allow for investment in efficient processes and automation. It rather seems that conventional construction processes have been shifted to the factory and combined with elements of shop floor production. Furthermore, prefabrication in Europe has predominantly found its niche in the low cost market, which doesn’t allow for product or service innovation. Unlike Japanese prefabricated buildings, the choices available to European customers to individualize those houses are rather low.\n预制建筑在许多欧洲国家的房屋市场占有相当大的份额，如德国(15%)，奥地利(33%)，法国(5%)，西班牙(5%)和斯堪的纳维亚国家。在德国，Bien-Zenker AG公司(580套房屋/年)、Huf Haus公司(450套房屋/年)、Weber Haus公司(600套房屋/年)和Kampa House公司(1,086套房屋/年)每年大约交付20,000套预制房屋。购买预制件建筑的优点是，与传统建筑相比，价格低廉，交货相对迅速(交货期约为6天)。然而，欧洲的预制工业不能真正被认为是工业化的。根据经典理论(例如亨利•福特(Henry Ford)年代大规模生产)以及现代概念(如柔性制造、大规模定制),工业化意味着大规模生产的产品都是使用最新的生产技术,自动化,机器人技术,信息和通信技术(ICT),以提供高质量的产品和合理的成本。在欧洲，还没有一家预制建筑公司达到年生产的临界量，这将允许对高效流程和自动化进行投资。传统的施工流程似乎已经转移到工厂，并与车间生产的要素结合在一起。此外，预制在欧洲已经在低成本市场找到了它的利基，这是不允许产品或服务创新。与日本的预制建筑不同，欧洲客户可以选择个性化的房屋是相当低的。\nIn Japan, the prefabrication industry mainly deals with middle to high-end market (Matsumura, 2007; Linner, 2007), for which it delivers highly customized and reliable products that are equipped with the latest technology available and accompanied by a variety of services. Above all, prefab houses are well known for being earthquake resistant. Various maintenance models guarantee the building’s functionality for at least thirty years, and some companies offer even sixty years of service. Prefabrication companies altogether build about 150,000 housing units per annum. The following can be considered as a comparison: In 2008, the same amount of building permissions in private housing in Germany were issued both in conventional and prefabricated construction. Already in the nineteen-seventies, Sekisui Heim’s legendary M1 reached an annual production volume of more than 3,000 buildings. A high annual production rate on a steady level allowed the investment in component systems, expensive manufacturing technology (e.g. production lines, automation, and robotics, advanced logistic systems) and extended customer services, which are labelling the uniqueness and strength of the Japanese prefabrication industry today. Moreover, the productivity of this industry has become so high, depending on the capacity utilization, that three to four customized buildings per employee can be realized annually (Sekisui Heim, 2008; Sekisui House, 2008). Japan’s housing industry is amongst the strongest industries worldwide. However, it has undergone a steady change and decline since the 1990’s. The maximum production peak was reached in 1994 with 573,173 newly constructed housing units. In 2000, about 450,000 units were constructed and in 2009 the construction went down to just 318,000 units. During peak times, the percentage of prefabricated houses, which were entirely prefabricated, was about 18 to 19 %. Today’s quota has decreased to just 13 to 15%, depending on the region (Sekisui Heim, 2010; Toyota Home, 2010). However, also in conventional construction, a high amount of prefabricated elements were used, which increases the actual percentage of prefabrication in the whole building industry, although it is difficult to give numbers. The prefabrication of entire buildings can be broken down into about 80% steel-based building kits, 15% wood-based building kits and 5% concrete-based building kits.\n在日本，预制建筑行业主要面向中高端市场(Matsumura, 2007;Linner, 2007)，为其提供高度定制和可靠的产品，配备了最新的技术，并伴随各种服务。最重要的是，预制房屋以防震而闻名。各种维修模式保证建筑物至少30年的使用寿命，有些公司甚至提供60年的使用寿命。预制建筑公司每年总共建造约15万套住房。以下可以作为比较:在2008年，德国颁发了相同数量的私人住宅建筑许可，无论是传统建筑还是预制建筑。早在十九世纪七十年代，Sekisui Heim的传奇M1就达到了年产3000多栋建筑的规模。在稳定水平上的高年生产率使得在组件系统、昂贵的制造技术(如生产线、自动化和机器人、先进的物流系统)和扩展的客户服务方面的投资成为可能，这些都标志着今天日本预制制造行业的独特性和实力。此外，这个行业的生产力已经变得如此之高，取决于产能利用率，以至于每个员工每年可以实现3到4栋定制建筑(Sekisui Heim, 2008;Sekisui房子,2008)。日本的房地产业是世界上最强大的产业之一。然而，自20世纪90年代以来，它经历了一个稳定的变化和衰退。1994年达到了生产的最高高峰，新建了573 173套住房。2000年，大约建造了45万套住房，而到了2009年，建筑数量下降到只有31.8万套。在高峰期，全是预制房屋的比例约为18%至19%。根据地区的不同，如今的配额已减少到只有13%至15%(塞基斯·海姆，2010年;丰田的家,2010)。然而，在传统建筑中，也使用了大量的预制构件，这增加了整个建筑行业中预制构件的实际百分比，尽管很难给出具体数字。整个建筑的预制可以分解为大约80%的钢基建筑组件，15%的木材基建筑组件和5%的混凝土基建筑组件。\nTable 1: Maximum production peaks in housing of main players of the prefabrication industry.\nSource: Yearly financial reports of Sekisui House, Daiwa House, Sekisui Heim and Toyota Home∗\nSekisui House, which remains the main player in Japan’s prefabrication industry, reached its peak in 1994 with a production of 78,275 housing units. At this time, Sekisui’s quota of the total building construction market was 5,3%. Both Sekisui House and Daiwa House, the second largest player in Japan’s prefabrication industry, tried to encounter the decline in the market by going into a developer position. Houses and apartments were developed, planned and constructed in order to rent them out later. These houses and apartments are also based on mass customisable housing kits and ensure that the capacities of expensive automated production facilities are utilised to a maximum.\nSekisui House仍然是日本预制建筑行业的主要参与者，它在1994年达到顶峰，生产了78275套住宅单元。在这个时候，Sekisui的总建筑市场份额是5,3%。Sekisui House和大和房建(Daiwa House)都试图通过成为开发商来应对市场下滑。大和房建是日本预制建筑行业的第二大开发商。房屋和公寓的开发、规划和建造是为了以后能出租。这些房屋和公寓也是基于大规模定制的住房套件，并确保昂贵的自动化生产设施的能力得到最大限度的利用。\nTable 2: Housing production of main players in 2009.\nSource: Yearly financial reports of Sekisui House, Daiwa House, Sekisui Heim and Toyota Home ∗\nTo address the decrease in demand and to build up new ways of value creation, all main players are focusing more and more on the building’s utilization phase, building performance and advanced Building Information Modelling (BIM), in order to manage the building’s life-cycle and provision of more and more services accompanying the hard physical product.\nComparing the European and the Japanese prefabrication market using the aforementioned facts and figures, it can be concluded that prefabrication in Japan is much more industrialized. This means that companies have higher production volumes, along with standardized processes, just-in-time, automation and flow line-like production, (together with human work activity), in all factories of the four biggest prefabrication companies. Despite this high degree of industrialization, prefabricated buildings are considered as the most customized, reliable, technology-equipped and properly designed buildings in Japan. Above all, Japanese prefabrication companies have managed to bundle a variety of customer services to their buildings. Thus, Japanese prefabrication industry is not only to a high extend industrialized but at the same time innovative, as it delivers buildings of outstanding quality (customized, earth quake resistant, properly designed, reliable, equipped with the latest technology). Now this industry even starts to create a completely new business services. In order to explore and specify the concepts and parameters that have driven this industry its’ cultural, economic and technological surrounding as well as applied processes, technologies and economic strategies have to be described and analyzed. As in many success stories of complex socio-technical systems (Fujimoto, 1999), we assume that the success of Japan’s prefabrication industry today, was also based on a complex relation of concepts and parameters, that evolved out of historic and cultural developments. The research aim is to identify, describe and justify these concepts and related parameters, and to recognize the most likely and influential drivers for the future, that provide an indication in which direction the industry could evolve.\nLarge scale industrialization and automation is a rare phenomenon that was not only discussed theoretically, but also applied in real life. The Moma Catalogue (Bergdoll and Christensen, 2008), accompanying the 2008 exhibition “Home Delivery”, is one of the most recent collections of concepts, projects and real-life examples of prefabricated buildings. It clearly revealed that most ideas about prefabrication remain abstract concepts (Archigram, Metastadt), others are only built once or a few times, and only a few have reached a reasonable production number (e.g. some systems of Jean Prove). Similarly, none of the building systems mentioned, analysed and discussed in the book “New Perspective in Industrialization in Construction – A State-of-the-Art Report”, published in 2010 and co-authored by experts from CIB’s TG 57 (Girmscheid and Scheublin, 2010), has reached an extent of production that is comparable to the mentioned Japanese systems. Comparable amounts, or a comparable degree, of automation has not been achieved even in the prefabrication of concrete elements for the civil engineering and infrastructure sector (Bock and Linner, 2010; Girmscheid, 2010). Furthermore, in the area of open building systems, most systems discussed are conceptual or experimental (Habraken, 2000; Kendall and Teicher, 2000). As mentioned in literature, you cannot extract guidelines that determine success, especially when dealing with concepts or projects, which are applied in larger scales. Due to cultural barriers and limited access of foreigners to the Japanese society, economy and technology, only a few international publications about the Japanese prefabrication market have been proposed. In (Wimmer, 2009), the author visited Sekisui House and mainly discusses the resource efficiency of the Japanese prefabrication industry. In (Johnson, 2008), economic concepts of the Japanese prefabrication market are discussed and comparisons to the market of the United Kingdom are made. Nevertheless, the author does not mention in detail the production technology and the service aspects. Similarly, (Andersson, et al., 2010) deals with the application of building information modelling in the Japanese prefabrication industry, without mentioning the context in which this happens. In general, researchers of Japanese prefabrication companies do not easily publish about their own technologies. One of the rare publications about the production system (Furuse and Katano, 2006), was issued in the International Symposium on Automation and Robotics in Construction held in Tokyo. Further books, brochures and websites, directly from the Japanese prefabrication companies, are predominantly written in Japanese language and do not reveal details about strategies and technologies. Being aware of the situation that literature does not provide relevant information and data, which would allow the authors to explore concepts and parameters in order to explain the success of Japanese prefabrication, the authors started a field survey. Visits to factories, R&D centres and sales points of all four mentioned prefabrication companies, were conducted. The authors also interviewed general managers, researchers and developers (Sekisui House, 2008 and 2009; Daiwa House, 2008; Sekisui Heim, 2008 and 2010; Toyota Home, 2008, 2009 and 2010). Furthermore, they interviewed researchers at the University of Tokyo and the Kyushu University, which were familiar with the subject. Based on extensive literature review in the area of product development, production technology, modularization, mass customization and innovation (Baldwin and Clark, 2000;Ohno, 1988; Maraghy and Wiendahl, 2009; Piller, 2006; Chesbrough, 2011; Forza and Salvador, 2007), the authors qualitatively and quantitatively analyzed all four mentioned companies during their field survey, concerning the designed scheme to reflect the value chain:\n大规模工业化和自动化是一个罕见的现象，不仅在理论上被讨论，而且在现实生活中得到了应用。现代艺术博物馆(Moma)的目录(Bergdoll和Christensen, 2008年)，伴随着2008年的“送货上门”展览，是预制建筑的最新概念、项目和现实范例的集合之一。它清楚地揭示了大多数关于预制的想法仍然是抽象的概念(Archigram, Metastadt)，其他的只是建造了一次或几次，并且只有少数达到了合理的生产数量(例如一些系统的Jean Prove)。同样,没有提到的建筑系统,分析和讨论了在书中新的视角在工业化建设一个先进的报告,出版于2010年,由专家从CIB年代TG 57 (Girmscheid Scheublin, 2010),已达到一个程度比得上提到日本的生产系统。即使在土木工程和基础设施部门的混凝土构件预制中，也没有实现可比较的数量或程度的自动化(博克和林纳，2010年;Girmscheid, 2010)。此外，在开放建筑系统领域，讨论的大多数系统都是概念性或实验性的(Habraken, 2000;Kendall and Teicher, 2000)。正如在文献中提到的，您不能提取决定成功的指导方针，特别是在处理概念或项目时，它们被应用于更大的规模。由于文化障碍和外国人接触日本社会、经济和技术的渠道有限，有关日本预制建筑市场的国际出版物寥寥无几。在(Wimmer, 2009)中，作者参观了Sekisui House，主要讨论了日本预制行业的资源效率问题。在(Johnson, 2008)中，讨论了日本预制市场的经济概念，并与英国市场进行了比较。但是，在生产技术和服务方面，作者并没有详细提及。类似地，(Andersson等人，2010)处理了建筑信息建模在日本预制行业中的应用，但没有提及发生这种情况的上下文。一般来说，日本预制建筑公司的研究人员不会轻易发表他们自己的技术。关于生产系统的罕见出版物之一(Furuse和Katano, 2006)在东京举行的建筑自动化和机器人技术国际研讨会上发表。其他直接来自日本预制建筑公司的书籍、小册子和网站主要是用日语写的，没有透露有关战略和技术的细节。了解到文献没有提供相关信息和数据的情况，这将允许作者探索概念和参数，以解释日本预制的成功，作者开始了实地调查。他们走访了上述四家预制建筑公司的工厂、研发中心和销售点。作者还采访了总经理、研究人员和开发人员(Sekisui House, 2008年和2009年;大和房建,2008;Sekisui Heim, 2008年和2010年;丰田家居，2008,2009和2010)。此外，他们还采访了熟悉这一课题的东京大学和九州大学的研究人员。基于对产品开发、生产技术、模块化、大规模定制和创新领域的广泛文献综述(Baldwin和Clark, 2000;Ohno, 1988;Maraghy和Wiendahl, 2009年;皮,2006;两,2011;Forza和Salvador, 2007)，作者在他们的实地调查中定性和定量地分析了所有提到的四家公司，关于设计方案来反映价值链\n1. Evolution of the company and the companies systems\n2. Design Aspects: product structure, modularization, design for production\n3. Degree of customer integration\n4. Supply chain management\n5. Off-Site Process: Automation and production technology, factory layouts\n6. On-Site Process: completion process on site\n7. Life-cycle management and customer relationship\n8. Service Innovation\nAccording to their analysis, it was realised that Sekisui House and Daiwa House have a high production outcome, but compared to Sekisui Heim and Toyota Home, they have a lower degree of automation and work tasks done in the factory. Toyota Home performs about 85% of all works in the factory, (highest degree of prefabrication), but hardly ever produced more than 5,000 houses per annum. The Sekisui Heim company was the most interesting to the authors. Sekisui Heim has applied a flow production on the production line which was very close to automotive industry, regarding organisation and appearance. Sekisui Heim has a medium sized production outcome and a prefabrication degree of about 80%. In this paper, the authors therefore focus on Sekisui Heim and identify, describe and analyse parameters that are relevant for efficient large scale industrialization. The other companies will be analysed in further papers. Comparative analysis of all four companies mentioned above will be also realized in further papers.\n根据他们的分析，他们意识到Sekisui House和大和House的生产效率很高，但与Sekisui Heim和丰田Home相比，他们的自动化程度较低，工作任务也在工厂完成。丰田家居完成了工厂85%的工作，(最高程度的预制)，但几乎从来没有生产超过5000个房子每年。对作者来说，Sekisui Heim公司是最有趣的。在组织和外观方面，积水海姆在生产线上采用了非常接近汽车工业的流程生产。Sekisui Heim拥有中等规模的生产成果和约80%的预制度。因此，在这篇论文中，作者将重点放在了Sekisui Heim上，并识别、描述和分析了与大规模高效工业化相关的参数。其他公司将在进一步的论文中进行分析。以上四家公司的对比分析也将在以后的论文中实现。\nThe remainder of the paper is structured as follows: First, an analysis of how the historic and cultural development led to the evolution of large-scale industrialization is presented, by cross-linking to Sekisui House and Toyota Home, as all major companies have influenced continuously each other. Secondly, the strategies, processes and technologies deployed at Sekisui Heim today are depicted. Thirdly, it is shown that customer services play an important role in Sekisui Heim’s success and that the proposed analysis has revealed the fact that more service innovation is under development in the R&D centres of the prefabrication companies.\n本文的剩余部分结构如下:首先，通过交叉链接Sekisui House和Toyota Home，分析历史文化发展如何导致大规模工业化的演变，因为所有的大公司都是相互影响的。其次，描述了目前Sekisui Heim所采用的策略、过程和技术。第三，客户服务在Sekisui Heim的成功中发挥了重要作用，所提出的分析表明，更多的服务创新正在预制建筑公司的研发中心发展。\nFinally, the importance of this development for a changing housing industry is discussed. A summarization of the findings concludes the proposed paper.\nHistoric and Cultural Development: the Evolution of Prefabrication in Japan","It’s not new-fangled gadgetry that makes the New American Home, a model reflecting the latest in construction and design standards, so cutting-edge. The 6,428-square-foot, five-bedroom property, which was unveiled at January’s International Builders’ Show in Las Vegas, doesn’t spare technology—check out the “transitional” outdoor entertainment space with a custom fireplace and “vanishing edge” pool. But it’s the multitude of ways the home slashes energy costs that’s most noteworthy.\nLocated in Henderson, Nev., overlooking the Las Vegas skyline, the home was constructed using the industry’s most advanced building products and techniques to optimize energy efficiency, according to the National Association of Home Builders. The property is expected to achieve LEED Emerald-level status—the National Green Building Standard’s highest efficiency rating.\nThe home features spray-foam insulation, high-quality solar panels, automated LED lighting and green appliances, and the most energy-efficient doors and windows, according to Drew Smith, a Florida energy and green building consultant who worked on the project. While the average new home has a HERS rating of 100, the New American Home scored –16. “That’s tied for the lowest for a New American Home,” Smith says. “Solar made a difference. Without it, [the home’s HERS rating] was 49. And for a house that large, that’s the lowest HERS index we have ever seen. The calculated energy savings is about $4,000 a year because of the solar and total design of the home.”\nThe building standards of the project reflect how home builders across the country are prioritizing high-performance construction practices, NAHB officials say. A new study from the association shows that 91% of builders are incorporating energy-efficient practices—including tight building envelopes and high-performance ventilation systems—and 69% do so on a majority of their projects. “These findings complement the results of a recent study where home buyers ranked high-performance products and practices among the top features they want in a home,” says John Barrows, chair of the NAHB’s Sustainability and Green Building Subcommittee. “This shows us that the value of home performance is increasing among builders and consumers.”\nBut while builders credit consumer demand for prompting increased eco-friendly construction, they say even higher demand is necessary to influence more meaningful growth in the green-home market. Only a third of builders—still an impressive number—identify as green, according to Donna Laquidara-Carr, industry insights research director at Dodge Data & Analytics, which conducted the NAHB study.\nA majority of builders and remodelers say their customers perceive green homes to be more expensive than traditional homes, Laquidara-Carr says. Still, about 70% of single-family home builders believe their customers will pay more for a green home, she adds.\nOther Emerging Building Trends\nThe NAHB released a separate study naming the home trends, buyer preferences, and must-have features for 2020, including energy efficiency. Efficient lighting, programmable thermostats, and Energy Star–rated appliances are the green features most likely to be included in new homes this year, according to Rose Quint, the NAHB’s assistant vice president of survey research.\nAdditionally, the study shows that the average home size continues to decrease after peaking at 2,689 square feet in 2015. It has fallen four years in a row to its current 2,520 square feet and is at its smallest since 2011. The majority of both first-time and repeat buyers say they would rather have a smaller home with high-quality amenities than a bigger home with fewer features, Quint says. The percentage of homes incorporating four or more bedrooms, three-plus bathrooms, and garages for three or more cars has also dropped to levels not seen since 2012, she adds. “This points to an industry trying to meet the demands of the entry-level home buyer. Builders are struggling to meet these demands because of factors such as restrictive zoning regulations and lot prices, with the price of a new lot in 2019 averaging $57,000.”\nThe NAHB also highlighted home design trends coming in the future, including:\n- Colorful kitchens incorporating aqua, dark woods, and new, colored textures.\n- Crisp colors paired with warm woods.\n- Expansive, large-format windows.\n- High-quality signature front entries and improved streetscapes.\n- Nontraditional storage solutions. Instead of cabinetry, designers are opting for shelving, both as a storage solution and a design element.\n- Seamless indoor and outdoor connections.\n- Increased use of mixed metals, materials, and textures, including wallpaper, to add depth to designs.\n- Wood detailing to create texture.\nPrefab Technology Evolves\nJapan’s largest home builder, Sekisui House, and its American subsidiary, Woodside Homes, used the backdrop of January’s Consumer Electronics Show in Las Vegas to unveil its prefabricated homebuilding technology, which can speed up construction, address a shortage of skilled labor, and build homes more resilient to natural disasters.\nThe companies provided tours of a multimillion-dollar concept home in a luxury Vegas enclave as part of the unveiling. The companies’ leaders say the two-story, 7,200-square-foot home, with four bedrooms and five-and-a-half baths, showcases design and construction systems and techniques that are unlike any the U.S. homebuilding industry has ever used.\nThe model home was built using a technique called Shawood, in which lumber, with the aid of computers and automation, is precision-engineered, cut, and drilled in a factory near Tokyo and shipped to the U.S. The pieces were put together in the field using number sequences on a blueprint as a guide. It’s a holistic system that has been used in Japan for more than two decades, covering the home’s framing, foundation, and flooring, the companies’ officials say. That enables a simpler, faster, and more precise building process, including fire-resistant porcelain siding that also helps protects the home from earthquakes, hurricanes, and other natural disasters, says Joel Abney, vice president of operations at Woodside Homes.\n“The goal is to showcase how the trends are changing in housing and what the future can look like,” Abney says. “There hasn’t been much technological advancement overall in single-family home construction because we’re building the same way in the U.S. as we were 20, 50, and 100 years ago. This is looking at advancing it to that next level. The home provides U.S. companies with a template for how to build houses that are significantly more resistant to Mother Nature’s forces than the traditional American stick-frame structure.”\nSource: Realtor Magazine\nAuthor: Buck Wargo"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:450e43ed-3e20-47c3-91c2-813d01e7f1b7>","<urn:uuid:0a0236b4-927d-4c56-b79d-b023c3f06545>"],"error":null}
{"question":"Are both Tkvarcheli and Chernobyl considered ghost towns due to war and disaster?","answer":"Yes, both locations are considered abandoned or 'ghost towns', but for different reasons. Tkvarcheli became largely abandoned after the war with Georgia in the early 1990s following the dissolution of the Soviet Union, with many abandoned apartments, buildings and factories, particularly in the Akarmara area. Today, it offers tourists a glimpse into a typical abandoned Soviet town. Chernobyl and nearby Pripyat were abandoned due to the 1986 nuclear disaster, which released massive amounts of radioactive material, forcing the evacuation of over a million people from surrounding regions. Both sites now feature numerous abandoned structures, though Chernobyl remains highly radioactive while Tkvarcheli is safe to visit.","context":["Tkvarcheli (Tkvarchal) Overview\nTkvarcheli, or Tkvarchal in the Abkhaz language, is a town in the unrecognized country of Abkhazia.\nAkarmara, an area within the town, is home to many abandoned apartments, building and factories. It has been abandoned since the war with Georgia in the early 1990s, following the dissolution of the Soviet Union.\nWar in Abkhazia\nDuring the war in Abkhazia, Tkvarcheli was under siege by Georgian forces as Georgia saw the town as a main choke point for the war.\nAs such, the town became a primary Abkhaz stronghold against further Georgian advances. The siege lasted from October 1992 to September 1993 but was ultimately unsuccessful and the war lead to Abkhazian sovereignty.\nAs the town was under siege, nearby villages were subject to ongoing battles between Georgian and Abkhaz forces. Russian aid was critical for the defence of Tkvarcheli, as it suffered a humanitarian crisis during the length of the siege.\nAccording to the last pre-war census in 1989 done by the Soviet Union, Tkvarcheli had a population of 21,744. Abkhaz (42.3%), Russians (24.5%) and Georgians (23.4%) populations account for the ethnic makeup of the town.\nThe Russian military actively intervened in the crisis in Tkvarcheli. Russian forces successfully delivered both humanitarian and military support to Tkvarcheli during the course of the siege.\nIn addition, Russia until this day, has been closely allied with Abkhazia in its conflict with Georgia.\nAllegedly, both Russian-trained and Russian-paid fighters were transported to the area to take part in the fighting against Georgian forces as well, arriving by means of helicopter to the besieged town.\nThe Georgians lost Gagra to Abkhazian forces commanded by Chechen warlord Shamil Basayev, who at the time was the commander in-chief of the forces of the Confederation of Mountain Peoples of the Caucasus. Georgian troops responded to this defeat by shelling Tkvarcheli.\nIn February 1993, Abkhaz fighters attacked the village of Kvirauri, just outside of Tkvarcheli, and took approximately 500 prisoners. They threatened to kill the prisoners unless Georgian forces ended their offensive in a neighboring district.\nA temporary ceasefire allowed the Russians to carry out its largest humanitarian operation in Tkvarcheli in June 1993, evacuating several hundreds of civilians through the corridor offered by the Georgian army.\nThe Georgians claimed, however, that a great deal of weaponry and ammunitions were simultaneously delivered to Tkvarcheli by Russian forces, under the cover of a humanitarian operation.\nOn July 14, 1993, there was a huge turn of events for the besieged town of Tkvarcheli. Russian forces ousted Georgian units from the main heights around Tkvarcheli - the main strategic points for the continuation of the siege of the city.\nAlthough, Abkhaz forces still failed to completely breach the siege at this point, the town was far less vulnerable afterwards and was a major victory for the future unrecognized country.\nAbkhazia: Unrecognized Country\nOn September 16, 1993, Abkhaz and other Caucasus forces, launched simultaneous attacks against Georgian forces in Sukhumi, Ochamchira and against Georgian forces blockading Tkvarcheli.\nThe capital city, Sukhumi, was successfully taken by Abkhaz forces 11 days later, shortly before breaking the siege of Tkvarcheli 2 days after that.\nBy September 29, 1993, the siege had been lifted and sovereignty over the city was exercised by the de facto government of Abkhazia in Sukhumi.\nTourism in Abkhazia\nWith many abandoned and destroyed buildings, Abkhazia sets the perfect atmosphere for unchartered and unique adventures. Whether you're an adventure traveller or simply interested in exploring abandoned Soviet-era buildings, the opportunity to see authentic abandoned places in Abkhazia is huge.\nMany of the major towns have small areas and districts that have not yet been rebuilt since the wars, such as Old Gagra and parts of Sukhumi.\nAlthough there are no official maps of the ruins and abandoned areas, it's very easy to locate these sights in Abkhazia. In addition to buildings, Abkhazia has several abandoned palaces, churches, factories and railway stations across the country.\nThe train graveyard in Sukhumi has old Soviet trains just waiting for adventurous tourists to come and check out.\nExploring the abandoned towns and districts is the ultimate activity for the eager travel photographer or adventure traveller.\nSince these building were at the centre of conflict and have remained untouched since the time of the Soviet Union, all travellers should exercise caution upon entering any building and permission should be sought from your guide prior to making entrance.\nTkvarcheli however, is home to the largest collection of Soviet-era abandoned structures in the country. Once a prime example of a working town in the Soviet Union, much of the town is now abandoned and considered a 'ghost-town.'\nToday, the abandoned town is open for curious travellers to explore on a tour to Abkhazia.\nIt offers tourists the chance to take a peak into a typical Soviet town that has remained untouched since. Although untouched by humans, the forest has been slowly taking back the town.\nIt is an authentic glimpse into what was regarded as a 'perfectly designed Soviet city.' Entering Tkvarcheli is a real opportunity to see history up close and see what life looked like up until war ravaged the area following the dissolution of the Soviet Union.\n“In better times, working and living in Tkvarcheli was regarded a privilege - not just by miners, but by engineers and academics as well. The town was created to embody a dream. An idea to create a perfect city pervaded the minds of Soviet architects and practitioners after the Second World War when a lot of cities were rebuilt almost from scratch,” Russian photographer Maria Gruzdeva stated.\nSalat, P (2019). \"Goats, ghosts and golden years — dispatch from an abandoned city in Abkhazia.\" https://jam-news.net/goats-ghosts-and-golden-years-dispatch-from-the-abandoned-city-in-abkhazia/\nVassiliev, N. (2017). \"Tkvarcheli Ghost Town. http://inrussia.com/tkvarcheli-ghost-town","An abandoned city, rusting buildings, rise in cancer and thyroid cases, deformed children and animals… we all know about the Chernobyl Nuclear Disaster that took place in Ukraine more than 25 years ago. We present some of the most striking Chernobyl Nuclear Disaster facts, along with the timeline of its events, causes, casualties, summary, and long-term effects.\nDid You Know?\nWithin the nuclear plant’s ‘exclusion zone’, an area of 4 square miles consists of the Red Forest, which got the name because of the dark-brown colored pine tree trunks. This happened due to the absorption of extremely high radioactive particles that blasted out through the nuclear plant towards the surrounding areas. This location is one of the most radioactively contaminated regions on our planet.\nOn 26th April, 1986, an explosion took place at the Chernobyl nuclear plant located in Ukraine, releasing massive amounts of radioactive material into the atmosphere, which also spread to the neighboring countries like Belarus, Russia, and several other European regions. Apart from the recent Fukushima nuclear disaster, the catastrophe at Chernobyl is also rated at Level 7 on the International Nuclear Event Scale. This is the highest level that can be attributed to such a disaster.\nThis accident directly claimed about 32 lives when it occurred, and people died due to acute radiation syndrome as they were exposed to extremely high doses of radiation. Till date, numerous physical deformities and cancer cases are still reported, which are linked to radiation exposure from this incident in the neighboring areas of Chernobyl and Pripyat. Damages amounted to around USD 18 billion as a result of this event, along with evacuation of several people. At least 500,000 workers were recruited and assigned in the rescue and relief mission; many of them were called the ‘liquidators’.\nLocation and Brief Overview of the Chernobyl Nuclear Disaster\nOn the incident day, some tests were going on at the nuclear station’s reactor 4, which is in proximity to the city of Pripyat, situated near the Belarus border and the Dnieper River. Suddenly, a surge in electrical supply took place, which initiated an emergency shutdown of the reactor core. But, during this time, another large surge occurred, which was beyond the withstanding capacity of the reactor vessel.\nThe chamber exploded causing the neutron moderator to get exposed to air, resulting in a chain reaction. The graphite moderator also exploded causing a fire, and hence a large amount of radioactive material was released in the atmosphere, which spread to neighboring areas. More than half of the fallout was concentrated in Belarus. Russia, Ukraine, and Belarus were the three countries, which suffered most due to the radioactive fallout.\nCauses and Timeline of the Chernobyl Nuclear Disaster\nBefore we go on to the timeline of this event, let us understand in brief the test that was carried out on that fateful day:\nWhen a nuclear reactor does not receive coolant flow, much radioactive heat is released as reactions start immediately. In this case, the emergency cooling systems get switched on, but their diesel generators take some time to start (at least 1 minute for running on full power). This was the same situation considering Chernobyl reactor no. 4, and it was considered unsafe for radioactive reactions to take place without any emergency cooling effect. It was decided to use the rotational power of steam turbines to run the systems before the diesel generators took control.\nBefore 1986, several test were being carried out, though without much success. A decision was taken to alter the electrical power supply in the next test carried out on 26th April, 1986. The procedure began with an emergency power shutdown, but the entire program was not followed according to the prescribed regulations. The management had not taken prior approval of the relevant organizations who oversaw Soviet nuclear programs. Also, it was mandatory that the power produced by the plant should not go below 70 MW, but this factor was not considered while undertaking the test.\nAt 1 p.m., on 25th April, the day shift of workers and engineers start decreasing the power level to 50% of the usual level of 3200 MW. A nearby power station experiences a power level drop, and it is decided to postpone the test so that electricity demand will not be hampered.\nAround 11 p.m., emergency core cooling systems are shut down, which are not supposed to affect the power output, to aid the postponed test. The day shift workers leave after their time is done, but are supposed to have finished the test on that day itself. The night shift workers do not arrive until midnight, and hence, the core shutdown continued as they do not have ample time to carry out the test.\nThe power continues to decrease, and the night-shift workers start the control rod movement. They are initially assigned only to decrease the decay heat from previous day’s preparations. Just after midnight, the radioactive core power goes below 700 MW to about 500 MW, as officers insert the rods more than necessary. Thus, reactor core poisoning starts taking place, i.e., in this case, formation of a xenon isotope in an unstable manner.\nAround 12.30 a.m., on 26th April, the output power is less than 5% as that of the total expected limit, leading to an increase in xenon poisoning. The power is so low that it cannot be used to take the rods out again, which had the probability of reduction in poisoning effect. The rod control system is forced to shut down. This results in a small increase in power but not enough to run the system.\nTill now, the rods are near their original upper limits, but the situation is difficult due to unstable core temperature levels, low power levels, irregular supply of coolant liquid, etc. This set off several alarms in the power station. The operators decide not get bothered by the alarms, and till 12.45 a.m., the experiment and testing is continued.\nAt about 1 a.m., power is increased till 200 MW, and water pumps are started to help in maintaining the coolant temperatures. But, the cooling does not take place as less time is available to cool down the rods. Water heat up and reaches near its boiling temperature, but now, it is released in a large quantity in an uncontrollable way.\nAt 1.15 a.m., the water finally lowers down the rod temperatures, but the flow goes beyond the prescribed limit, and a loss of steam pressure is reported, triggering off more alarms. Water starts absorbing a lot of neutrons emitted by the rods. A couple of water-circulation pumps are turned off, and a few control rods that can be removed manually are taken out so that power can be kept stable.\nOn the contrary, all these steps lead to a very unstable situation, and hence officers begin the actual experiment at 1.20 a.m. Supply of steam to the turbines is stopped, and many steam voids are formed inside the core. The ability of water to absorb neutrons started deceasing, leading to a consequent increase of reactor power and increase in steam generation.\nA chain reaction starts and power increase takes place continuously. The reactor undergoes emergency shutdown and the process to insert the rods is started, which generally takes place in about 20 seconds. But, one of the graphite rod cores has a faulty design, and coupled up with an unstable situation the reactor power does not stop decreasing.\nA massive power surge occurs as the rods are inserted leading to a continuous production of steam. The reactor power is above 30,000 MW, which is more than the upper limit needed for nuclear operations. Steam starts leaking from the reactor ,and as the pressure builds up even more, the entire assembly explodes at around 1.23 a.m. This also leads to the fracturing of other fuel channels.\nJust after a few seconds, a second explosion occurs, wherein few pieces of the graphite core with intact radioactive control rods are ejected outside, giving rise to the radioactive fallout spread. This explosion is attributed to the release of hydrogen as graphite starts reacting with the supplied water. The entire reactor is shut down but the damage has already been done.\nChernobyl Nuclear Disaster Pictures\nThe slide-show below shows various places that were immediately abandoned following the explosion, and also shows the new confinement construction.\nAbandoned Building in Pripyat\nAbandoned School Bunk Beds in Pripyat\nAbandoned School Classroom\nBuilding in Ruins with USSR Symbol\nDerelict Amusement Park\nDestroyed Cooling Towers at Chernobyl\nDestroyed Reactor No. 4\nDestroyed Sports Hall\nSwimming Pool Ruins\nNew Containment Structure\nShort and Long-term Effects\nHealth Effects on Humans\nApart from the death toll of 32 people, which resulted from acute radiation syndrome, at least 30 more died from prolonged radiation sickness (ARS) till 2008. Several children lost their lives due to thyroid problems, and more than 130 cases of confirmed ARS have been recorded. It has been estimated that there might be at least 16,000 cases of thyroid cancer in the surrounding regions till the year 2065, due to radiation exposure from this disaster. Also, till the same year there might be at least 25,000 cases of other cancer types among people who lived in the Chernobyl-Pripyat region.\nAccording to estimates, about 180 metric tons of nuclear material in the form of dioxides and fuel products was present in reactor no. 4, and at least 30% of the entire amount got released into the atmosphere. After the explosion, the city of Pripyat was not evacuated immediately, but consequently several people started to fall sick and at least 200 people (mostly from the rescue and relief crew) had to be admitted in hospitals immediately. At least a million people were evacuated from the surrounding regions including Pripyat. The government predicted that in the next few years, cancer rate would rise by at least 2%.\nOn of the worst and most horrifying health effects caused were the mutations; many children had deformed body parts, missing fingers and eyes, missing limbs, deformed skulls, etc. Several cases of dipygus (multiple limbs) were seen in both humans and animals. Apart from thyroid-related diseases, the Down’s Syndrome also affected children after the radioactive fallout.\nEffects on Plants and Animals\nIn addition to the Red Forest mentioned above, several patches of land became barren shortly after the incident as floral species absorbed the radiation particles. Several animals who were present in that area died and few experienced a halt in the reproduction process. Some animals suffered from thyroid problems and showed stunted growth. On the reactor walls and outer portions, a type of radiotrophic fungi started growing; they absorb the nuclear energy for their growth. Such fungi were collected by a robot, which was directed into the reactor for further analysis after the disaster.\nIn Germany, it was discovered that the radioactive content in hunted wild boars had increased, and hence it was banned to kill any wild animal for meat in this country. In Norway, the cattle and other livestock had to be given clean and decontaminated food for several months before they and their future generations could be used as meat. Cesium isotopes had the largest impact on the food chain in the United Kingdom region mainly regarding animal husbandry of sheep and their usage for wool and meat. Most of the restrictions were lifted in the last decade. Mutations in animals consisted of reduced brain size, albinism, uneven pair of body parts like limbs, eyes, deformed toes and fingers, etc.\nAfter the incident, considering the aftermath, several concerns were raised about the safety and operations of nuclear power stations all over the world. The development of such stations decreased drastically in the Soviet Government. After many protests by anti-nuclear agencies, several precautions were undertaken at numerous nuclear stations and the secretive transparency of techniques used was reduced to avoid such incidents in the future.\nA New Containment Structure (NSC) is currently being constructed, which will cover up the entire nuclear reactor no. 4, including the sarcophagus built by the liquidators after the accident took place. This sarcophagus consisted of reinforced steel and concrete, and the purpose was to decrease the amount of radiation spreading out from the reactor’s core chambers. The NSC is being designed to completely stop any radioactive particles from being emitted, so that the interior parts would be preserved."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:6ec376f0-40fa-4d62-8e31-13509246f990>","<urn:uuid:4e3a8442-3e50-435c-a6f2-782e7166e12a>"],"error":null}