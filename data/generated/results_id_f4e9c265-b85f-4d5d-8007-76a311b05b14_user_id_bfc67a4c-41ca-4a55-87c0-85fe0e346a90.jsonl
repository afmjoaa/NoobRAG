{"question":"How do Juneteenth and Labor Day differ in their journey to becoming national holidays in the United States?","answer":"Juneteenth and Labor Day took different paths to becoming national holidays. Labor Day became a federal holiday in 1894, following the deaths of workers during the Pullman Strike in Chicago. After this tragedy, Congress voted to approve the legislation and President Grover Cleveland signed it into law. In contrast, Juneteenth became a federal holiday much more recently, when President Joe Biden signed it into law on June 17, 2021, after the Senate unanimously approved the bill and the House passed it by a vote of 415 to 14. Both holidays commemorate significant moments in American history - Labor Day honors the American labor movement and workers' contributions, while Juneteenth commemorates the end of slavery in the United States, specifically when more than 250,000 enslaved Black people in Texas received news of their freedom on June 19, 1865.","context":["- What is Juneteenth, and why is it in the news?\nElicit or explain that Juneteenth is an annual holiday commemorating the end of slavery in the United States. It has been celebrated by African-Americans every June 19 since the late 1800s.\nJuneteenth has its roots in the moment of emancipation in Texas. More than 250,000 enslaved Black people received news that they were free on June 19, 1865 — more than two years after President Abraham Lincoln’s Emancipation Proclamation.\nJuneteenth is in the news because on June 17, 2021, President Joe Biden signed a new law making Juneteenth a legal public holiday throughout the U.S. The U.S. Senate had unanimously approved the bill, and the House then passed it by a vote of 415 to 14.\n- Invite students to share what they know about this holiday, which is also known as Emancipation Day, Black Independence Day, or Jubilee Day. What personal experiences do they have of this holiday?\nThis Is Why Juneteenth Is Important for America\nNext, invite students to watch the following 3-minute video from The Root: This Is Why Juneteenth Is Important for America.\nAcknowledge that the video contains a lot of information. Play it once all the way through, then ask students what stood out for them about the piece. Ask students if they’d like to watch it again in shorter segments, rewatching each of the segments before discussing them in more detail. (Note that you can see a minute by minute transcript by clicking on the three dots below the video and selecting Open Transcript.)\nAsk students some or all of the following questions:\nPart 1: The Emancipation Proclamation (0:00-1:50 min)\n- What does the video tell us about Abraham Lincoln and the Emancipation Proclamation?\n- What does the video tell us about who freed enslaved people?\n- What happened on June 19, 1865?\n- When was Lincoln’s Emancipation Proclamation? What happened in between?\n- Who was Major General Gordon Granger and how was he involved with freeing enslaved people in Galveston, Texas?\n- Did it result in enslaved people actually gaining their freedom?\n- How did white people respond?\nPart 2: Reconstruction Era (1:50-2:29 min)\n- What does the video say about how Black people responded?\n- When was the first public Juneteenth event? How was it celebrated?\n- Where did it go from there?\nPart 3: Jim Crow and the Civil Right Movement (2:29-2:55 min)\n- What happened with the Juneteenth celebration in the 20th century? Why?\n- What did the Poor People’s March of 1968 do for the holiday?\n- What happened in Texas in 1980 with regard to Juneteenth?\n- What happened in Congress in 1997? What did Juneteenth come to be known as?\nPart 4: Today and Looking Back Through History\n- What are your thoughts and feelings about Juneteenth becoming a national holiday?\n- The video describes Juneteenth as more than a celebration of freed enslaved people. Explain.\nAsk students next:\n- Now that you’ve watched the video, what questions do you have about Juneteenth, or what would you like to learn more about?\nConsider charting students’ questions about Juneteenth. Have them research their questions themselves, or provide them with an article or range of articles. (See some listed at the bottom of this lesson.) Ask them to come back to class with the answers they found.\nA New National Holiday\nHave students read aloud several comments from lawmakers who helped win passage of the legislation creating a federal Juneteenth holiday.\n- “It’s a long journey, but here we are. That racial divide has fallen out of the sky and we are crushing it to the earth. . . . This bill and this day is about freedom.” – Rep Sheila Jackson Lee (D-Tex.).\n- Making Juneteenth a federal holiday “does not right the wrongs of our nation’s past, but it finally gives recognition and voice to those who suffered.” – Sen. Edward J. Markey (D-Mass.)\n- “I would hope that we would not cash in substantive change for an opportunity to commemorate. I think commemoration ought to drive change and not be a substitute for change.” – Sen. Raphael G. Warnock (D-Ga.)\n- What stands out for you in these quotes? Why?\n- What does Senator Warnock mean when he says the bill should “not be a substitute for change”?\nElicit or explain that while the Juneteenth bill passed with almost universal support, Congress continues to be deadlocked on more substantive priorities for Black leaders and others, including a law guaranteeing voting access in the face of Republican-led state laws restricting it, and a federal policing overhaul.\n- What is one thought or feeling you have about Juneteenth?\n- What kind of celebration will you have or would you like to have?\nResources on Juneteenth\n- National Museum of African American History and Culture The Historical Legacy of Juneteenth\n- Juneteenth.com Juneteenth.com World Wide Celebration!\n- New York Times So You Want to Learn About Juneteenth?\n- Teaching Tolerance Teaching Juneteenth\n- PBS blog What Is Juneteenth?\n- The Atlantic Balancing the Ledger on Juneteenth","Retirees may not look forward to national holidays in quite the same way that working stiffs do. But that doesn’t mean they don’t get to celebrate Labor Day with everybody else! It’s fun to fire up your grill and host a Labor Day feast commemorating the unofficial end of summer, but it’s also important to take some time to learn about why Americans celebrate this holiday.\nLabor Day is a U.S. federal holiday that is celebrated on the first Monday in September of each year. This special day honors the American labor movement and is dedicated to the contributions that American workers have made to the social and economic labor movement. The holiday was first celebrated on Tuesday, September 5, 1882 in New York City. By 1884, the first Monday in September was selected as the official holiday, which was originally proposed by the Central Labor Union. The union also urged other organizations in cities across the country to follow their example.\nWhile we celebrate Labor Day each year in September, there is some speculation on who founded the holiday more than 100 years ago. According to documents from the U.S. Department of Labor, it appears that either Peter J. McGuire or Matthew McGuire was one of the founders of Labor Day. Some records show that Peter J. McGuire, general secretary of the Brotherhood of Carpenters and Joiners and cofounder of the American Federation of Labor, was the first to suggest the holiday to honor the workingmen of America. But other records show that Matthew McGuire, a machinist, proposed and founded the holiday in 1882 while serving as the secretary of the Central Labor Union in New York.\nAlthough it is uncertain which McGuire founded Labor Day, what is clear is that Labor Day was proposed in 1882, the Central Labor Union adopted the holiday and appointed a committee to plan a demonstration and picnic, and the Knights of Labor organized the first parade in New York City.\nIn 1887, Oregon was the first state in the country to make it an official public holiday until it became an official federal holiday in 1894 when 30 U.S. states officially recognized and celebrated Labor Day. It was also known as a “workingmen’s holiday” and the idea spread throughout labor organizations and in many industrial centers of the country.\nLabor Day became a national holiday following the deaths of workers during the Pullman Strike of 1894 in Chicago. The workers died during riots where they faced off against the U.S. Army and U.S. Marshals Service. This tragedy brought much more attention to the workers and their cause as well as the U.S. Congress and President Grover Cleveland. After hearing about the deaths, Congress voted to approve legislation to make Labor Day a national holiday and President Cleveland signed it into law six days after the end of the Pullman Strike.\nOnce the nationwide holiday was established, the first proposal outlined how Labor Day should be celebrated and observed. It outlined that the holiday should be celebrated with a street parade to exhibit to the public the strength of the community and labor organizations as well as a festival for workers and their families, which ultimately became the pattern for celebrating Labor Day each year. Since its origins as an official holiday, Labor Day has transformed from just being celebrated by workers and their families to people across the country taking the day to spend time with friends and family in various ways.\nWhile some communities may still host Labor Day parades and festivals, others celebrate the day by hosting a BBQ, shopping at retail stores and saving on discounts, or just relaxing at home. And although retirees are no longer working, they can still find ways to commemorate those American workers who contributed to the economic and labor forces of the country. And that often includes celebrating their own accomplishments!"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:71497b82-2407-492b-b241-99f8067f41f8>","<urn:uuid:6c05be60-2626-422f-a206-f278f1c062be>"],"error":null}
{"question":"How do organizational culture and human factors influence strategic change implementation, and what are the common barriers to acceptance?","answer":"Organizational culture and human factors are fundamental to strategic change implementation, requiring time and leadership to modify culture to fit new goals. The reward system must be aligned with corporate objectives and carefully designed to avoid conflicts of interest. Common barriers to acceptance include tradition, loyalty to existing relationships, insecurity about new skills requirements, and fear of power loss. Employees may resist due to poor employer relations, lack of involvement in the process, or insufficient support and training. To succeed, change efforts need widespread commitment throughout the organization, proper resource allocation, and adequate time flexibility. Management must demonstrate clear benefits of the change and cultivate collaborative working values while maintaining patience throughout the process.","context":["Strategic Change Management Implementation Process\nStrategic change management has always been a challenging task for firms. It is not good enough to have a great strategy; the firm must be able to execute the strategy. Many leaders assume that if they state what the strategy is the people below them will just go ahead and implement. This is wishful thinking. Strategic change involves clear communication, changes in routines, changes in culture and changes in attitude: human beings hate change. Unfortunately for firms, the need to quickly and smoothly implement strategic change is becoming increasingly important. The rapid development and adoption of new technologies and increased competition from the BRIC economies are changing the competitive environment. The firms that can adapt effectively will be the ones that survive; those that are slow and clumsy will die.\nThe process outlined here is based on the work of Professor Alexander Roberts of Edinburgh Business School. More extensive details on strategic change management and making strategies work can be found through the Edinburgh Business School website.\nOverview of the process\nReview the strategy\n- Clarify with those involved what the strategy and the desired outcomes are. Don’t take it as a given that everyone understands that strategy and the expected outcomes. Don’t be surprised if you find that people have interpreted the meaning of the strategy in different ways. If there is no shared understanding of the strategy and its purpose, don’t be surprised to see people pulling in different directions.\n- What are the external forces that might affect the outcome?\n- Measure and track these forces using Key Environmental Indicators (KEI)\n- Be prepared to make changes to strategy if environment factors demand them\nCritical Success Factors (CSF)\n- Things that go right in order to reach objectives\n- Can be organization based, industry based or environmental based\n- Between 3 and 5 CSFs is ideal. Too many CSFs cause confusion\n- Measure with a set of indicators for each CSF. Key Performance Indicators (KPI) might contain financial and non-financial measures\nCritical Business Activities (CBA)\n- CBAs are the activities that need to be done to make CSFs successful (e.g. logistics, HR)\n- Measure and track with Activity Performance Indicators (API). Each CBA needs a set of APIs. These can be financial or non-financial. In fact it is important to have some non-financial APIs. Financial measures often give signals that do not reflect the holistic value of the activity to achieving goals.\nNotes on using the process\nSuccessful identification of CBAs underpinning CSF is key to success or failure of strategic change management. CBAs should be viewed from a wider strategic organizational standpoint. Why?\n- Financial emphasis and cost reporting creates a strong functional and departmental emphasis\n- Day-to-day management focuses on the department first then the function\n- Danger of concentrating on efficiency of parts rather than effectiveness of the whole\n- Narrow focus creates the opportunity for “empire building”\n- CBAs often cross functional boundaries\nHuman factors are essential to change and implementing strategic change management efforts.\n- People can assist or impede change\n- Need to motivate people to implement the changes\n- There will probably be a need to change the culture to fit the goals\n- Culture change requires time and leadership\n- If the culture risk are too high then a different strategy may be required\n- The reward system will need to be aligned to the overall corporate goals\n- Rewards need a performance measurement system to be effective\n- The measurement system and the reward system need to be carefully designed to avoid conflict of interest (e.g. personal or departmental rewards misaligned with corporate goals might lead to people being rewarded for sabotaging the strategy)\nFinally, remember that Peter Drucker said “What gets measured gets managed”. Measuring CSFs will lead to success.","How organizations manage resistance to change\nSevere competitive and economic pressures that organizations face today were unthinkable a few decades ago. In order to shed excess costs and to respond more nimbly to customers and competitors, they are being urged to adopt new organizational forms, tightened inter organizational linkages and improved management practices (cf. Miles and Snow 1980, Johnston and Lawrence, 1988). Any change in organization is followed by a kind of resistance from its employees. In this assignment a few methods that can be used to overcome change in the organization are described.\nTechnology developments, social and demographic shifts, competition of changing market and economic issues, tend an organization to implement change in it as well. The rapid and dynamic change in market has increased consumerism. Whether it is an automobile industry or cosmetic industry or IT industry, consumer today has lots of choices these days that they need not have to wait for longer for any product. This changing market scenario imparts a message to managing bodies that the way of work should also change with the changing market. From managerial point of view a change is referred to as change in work pattern, work routine and work culture inside the working atmosphere. Change is normally a reaction to changing commercial, technological, economical, structural and strategic environment in which the company operates (Barbara Senior, Organizational Change). For example; departmentalization, job redesign, implementation of an international division are the examples of structural changes whereas work processes, methods and equipments are technological changes.\nChange should be welcomed as it can produce positive benefits for the individuals, bring opportunities for personal change and development, reduces boredom of work, provides new challenges and an opportunity to participate and shape the outcome. But unfortunately as change is accompanied by resistance, it is very important that the Change Manager anticipate and plan strategies for dealing with resistance not only at the introduction of change but also for monitoring the change over long term (Ronald, G and Smith, J 1995). It is helpful to understand why people resist change, because understanding this allows us to plan strategies to reduce resistance from the beginning. Kotter and Schlesinger identified the basic reasons of resistance to change are communication gap and inadequate information that creates misunderstanding, sense of insecurity, different assessment of situation and disagreement over advantages and disadvantages. Moreover, individuals are more concerned with the implications for themselves (Management by Robbins and Coulter).\nOrganizations do not change, individuals do. No matter how large is the project you are taking on, the success of project ultimately lies with each employee doing their work differently multiplied across all of employees impacted by the change (Web 1). Individual barriers to change include- tradition and set ways; loyalty to existing relationships; failure to accept the need for change; insecurity; preference for the existing arrangements; break up of work groups; different person ambitions; fear of power; skills and income; inability to perform as well in the new situation as for example, when quality control methods based on statistical models were introduced into manufacturing units, the quality control department have to learn the new methods. Some may fear that they will be unable to do so and may develop negative attitude towards the change or perform poorly if required to use the new methods. Sometimes change is resisted because of failures in the way it is introduced to the employees and the management fails to explain the need for change and its future benefits. Poor employer relations, lack of involvement in process and failure to offer support and training for the introduced change are the other reasons for change resistance (Web 2).\nResisting change takes many forms (Web 3) and the more obvious form is of active resistance, objection and refusal to cooperate with the change occurs. Sometimes, resistance appears to be individual and sometimes it is clearly situational. It may be passive in which colleagues agree to a change but are unwilling or unable to implement something new. This subtle form of resistance is dealt with more difficulty. For example, at a staff meeting everyone agrees to follow a new procedure, but after several weeks it is being discovered that the procedure has not been implemented yet. Another example of this kind is the introduction of new computers at the new place but virtually no one is using them for the purpose for which they are intended, since the staff had their own machines. The employee consents to change by agreeing to it but later he only changes to appear cooperative, but in fact he is doing most things the way he was before the change.\nAt the moment the change program is announced, many employees will employ tactics to protect themselves, their turf, and ultimately their place in the organization.Ã‚Â Some will aggressively challenge the necessity for change. This is a time waster and thus prevents critical objectives from being met. Every person who facilitates the change process must work diligently to build consensus. The employee must be assured that every idea is worth considering. If anyone argues, he or she can be asked to explain why he or she feels the way they do and ask for three or four suggestions for making the process work.Ã‚Â Some managers and members of the leadership team will avoid change by passively refusing the commitment to the process. Often these leaders will resist the change effort by being unavailable for meetings, denying resources, or withholding feedback. “The leadership” is a particularly difficult foe, because change efforts often require the use of resources managed by the leadership, such as time and money. Without these resources change efforts are likely to fail. Accountability with consequences is the primary means for assuring leadership participation. Many employees and organizational leaders search for personal or professional diversions during the change process that will ultimately hinder the effort. A distracted individual can undermine the change effort by not being present physically or mentally when his or her critical input is needed. Not being mindful of change creates an unnecessarily difficult experience for every member of the team. Such carelessness calls to mind the wasted energy expended when one runs against the wind. Change efforts provide an opportunity for every one affected to secure a new place in the organization or make a decision to seek a better fit elsewhere.\nKen Hultman argues that while no-one is a perfect change agent, managers have to be impeccable role models for bringing up a successful change. The essential attributes of such a person include the ability to be a clear thinker who is able to get a view about organizational situation and reach at logical conclusions. Hultman suggests few things in creating the right environment for change to occur. Firstly we must do things to establish a positive climate (p172) and secondly we must attempt to create environmental conditions that encourage an interest in improvement. Managers must demonstrate that how changes will improve employees circumstances and that there are opportunities in the change such as enabling colleagues to increase their knowledge and skills leading to genuine achievements and progress They must cultivate a value for collaborative working among staff and colleagues need each other to complete their tasks, it is easier to develop values of co-operation and mutuality. Whatever are the circumstances management must stay calm. At the heart of HultmanÃ¢Ã¢â€šÂ¬Ã¢â€žÂ¢s analysis is a set of humanistic values along with an assumption that one cannot even hope to influence another colleague without firstly demonstrating that they will have their needs met in some way. It is likely to be counterproductive by getting impatient, exasperated and angry.\nBeing a change manager it is his/her duty to reduce the resistance towards change and towards change and to increase the enthusiasm and level of commitment for the change. While likely to encounter the people who resist change, people who welcome change will also be encountered and by knowing the reasons for their acceptance to change, the communication plan will be better formulated. People will accept change when they see possibility that they will gain something from the change. The gain may be either personal like, money; increased job security; status; self satisfaction; less effort and time and gain in better personal contact or other like it provides new challenges, likeness of the source, reduction in boredom etc.\nIn order to reduce resistance to change, the manager should involve people affected by change, actively seeking their thoughts and reactions to proposed changes. They must develop a proper attitude towards resistance to change and realize that it is neither good nor bad. The best way to minimize resistance to change is to involve those responsible for implementing it and those affected by it. People are more motivated towards successful completion when they feel that they are the valued participants in planning and implementing the change. Also ensure that people from all the levels of organization are involved in planning the change process and they should be listened carefully. In the early stages, manager should not launch into lengthy diatribes justifying the change as people are not interested in that. They want to be heard and have their concerns attended to. They must recognize that it takes time to work through reactions to change. Then people should be engaged in dialogue about the change. They should do this only after understanding the specific concerns of others completely. Change must be realistic, achievable and measurable.\nCommunication and education is helpful method to sort out the things when resistance is due to lack of information or inappropriate information and analysis. Though time consuming, this method provides great employee support if persuaded. When cause of resistance is difficulty in adjustment to changes, management support and facilitation do work at times. This is expensive and still unreliable way to overcome the change. Manipulation of some information is necessary some times in order to avoid negative reactions by the employee. The people that easily accept changes and get adapted to changing atmosphere can set an example for others and hence they follow the suit. Therefore, they should be the first target of change program.\nThree basic steps- planning, implementation, and evaluation of outcomes of both the plan and implementation are involved in the change process. Resistance to change should be dealt ideally with planning and early stages of implementation. For proper planning for change, a manager must consider about how and when the change is needed and the way it should be communicated to the employees for their better support. Managers should pay attention to the focus of change, the amount of change, and the rate of change in order to implement change. Evaluation of outcomes of change is also very important as all the change efforts are result oriented. If change is not monitored, its effectiveness cannot be measured. This can be done by collecting data and comparing the results against original goals.\nTo wind up at the end of an interesting discussion we can conclude that a degree of resistance is normal since change is disruptive and stressful but in general, most people have mixed reactions towards purposed change, so the change agents can be helpful in highlighting the positive aspects in realistic manner. Although most people feel comfortable with minor changes, no one can live and work by yesterdayÃ¢Ã¢â€šÂ¬Ã¢â€žÂ¢s reality. Managers must reduce change in very effective, meaningful and healthy way without hurting the sentiments of the employees. By providing resources to support the changes, allowing enough time and flexibility and with the widespread commitment of people throughout the organization, change efforts will succeed.\n(2) Hultman, K. (1998), Making Change Irresistible: Overcoming resistance to change in your organisation, Davies-Black Publishing, Palo AltoOrder Now"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:1c35bbde-54ff-4825-8e4f-218e0f454c7c>","<urn:uuid:1a1104dc-49cd-429f-a39e-8094db2177d4>"],"error":null}
{"question":"What are the key differences between three-tab and architectural shingles in terms of durability and cost?","answer":"Architectural shingles are more durable and cost-effective in the long run compared to three-tab shingles. While architectural shingles cost 40-50% more initially ($35-$45 per bundle vs $25-$30 for three-tab), they have longer warranties (minimum 30 years vs 20-25 years for three-tab) and can withstand winds up to 130 mph compared to three-tab's 60 mph tolerance. Architectural shingles are built with two laminated layers, making them better suited to handle elements, while three-tab shingles are thinner with a single layer of materials.","context":["Estimated Cost Comparison Table\n$1.50 – $3 / sqft\nAverage Cost Estimate\n$3.50 – $5 /sqft\n$5 – $8 /sqft\n|Type of Roofing Shingle||Basic 3-Tab||Mid-Range Shingles||Mid-High Range Architectural|\n|Existing Shingle Removal||If Needed||Yes / No||Yes|\n|Hurricane Zone?||No||As Needed||As Needed|\n|Life Expectancy||15-20 Years||15 – 30 Years||25 – 35 Years|\n|Underlayment Condition||Remain||Remain / Repair||Remain / Repair|\n|Roof Features and Construction||Few / None||Average||Average / Difficult|\n|Installed By||DIY / Helper||DIY / Pro||Roofing Pro|\n|Disposal Costs||None||$400 / Included||Included|\n|Permits and/or Inspection||$0 – $250||$150 – $250||$150 – $250|\nSections: Overview | Product Costs | Installation Cost | DIY or Pro | Web Compare | Shared Pricing\nOverview of Asphalt Shingle Roofs\nAsphalt shingles remain the top choice in residential roofing. They’re available in a wide range of styles to suit your home’s architecture and deliver 20-30 years of durability. Asphalt shingles cost is among the most affordable of any roofing option. Three-tab and basic dimensional shingles update a home’s curb appeal when you’re ready to sell, while high-end architectural shingles from CertainTeed, Owens Corning and GAF add distinction to a home you intend to live in for decades to come.\nThis Costimates entry covers the cost to shingle a roof whether you’re building a new home, adding a layer of shingles or tearing off existing shingles to start fresh. The cost estimate covers various grades of material plus underlayment and other accessories often overlooked in shingle cost estimates. Roofing labor cost per square is listed separately in the table above and again down below. We’ve gathered online estimates from around the Web and given homeowners space to share their asphalt roofing costs too. Your research here will allow you to put together an accurate estimate for the type of shingles you plan to use and the size and complexity of your roof.\nShingle Roof Cost Details\nMaterial Cost Factors\nAsphalt shingles cost for materials and installation ranges from cheap to moderate compared with other roofing materials based on these factors:\n- Does Your Old Roof Need to Be Removed? – Tearing off one or two layers of old shingled roofing and then paying for disposal will increase your total cost to shingle a roof. In most cases, 2 layers of shingles is the maximum allowed standard. If you have a single layer of shingles now, you may be able to lay the new roof directly on top of it.\n- Scope of the Work – The most affordable asphalt shingle roof cost is done by adding a new layer over existing shingles. When roofing new construction or after a tear-off of old roofing, new underlayment, moisture barrier in valleys and along eaves and a row of starter shingles increases material and labor costs. Replacing damaged roof sheathing raises roofing costs even higher.\n- The Grade of Shingle you Use—Your options are affordable 3-tab shingles with warranties of 20-25 years, mid-range dimensional 30-year shingles or top of the line architectural shingles with a lifetime warranty.\n- Specialty Shingles – Expect to pay a slight premium for shingles that are resistant to the elements such as Owens Corning TruDefinition Storm (wind), CertainTeed Landmark IR (impact), GAF Timberline Cool Series (solar/UV) and Owens Corning Supreme AR (algae).\n- Choosing a complete Roof System to Get a Better Warranty – Top brands like CertainTeed, GAF, Owens Corning and Tamko offer multiple warranty options. Better warranties cover 100% of replacement costs for defective materials for more years before the warranty starts being prorated by 2-4 percent per year. Some also offer a warranty on labor beyond the standard 0-12 months. For example, CertainTeed offers three options: General Warranty (short with basic coverage), Sure Start (longer/better) and Sure Start Plus (longest/best). To get the better warranties, you need to use several of the brand’s premium roofing system products such as underlayment, moisture barrier, ridge vent and starter shingles, usually at a premium cost.\n- Who Does the Work – DIY homeowners pay for materials, supplies and tools only. Unlicensed roofers cost less than licensed roofing contractors, but you typically get what you pay for in the workmanship of the installation. Also, if you want a shingle brand’s premium warranty, you’ll have to hire a brand-certified roofer, and they’re rates are typically higher than average.\n- Enhanced Installation – When installed with standard techniques, most dimensional shingles have a 110MPH wind warranty. To increase the wind warranty to 130MPH, a good idea in high-wind areas like High-Velocity Hurricane Zones (HVHZ) and “tornado alley,” extra nails, roofing cement and materials are required, so cost is higher.\n- Roof Pitch —The cost of a new roof rises as the pitch or the roof gets steeper because there are more square feet of roof, and roofing it is more difficult.\n- The Complexity of the Roof—Roofs with hips instead of gables, more than four corners and obstacles such as skylights cost more to roof than simple roofs.\n- Delivery Charges – Delivering the materials to your home and transporting them to the roof can be extra charges when you buy the materials and hire a roofing contractor separately. These costs are typically included in the total cost of a new roof when hiring a contractor to manage the entire process.\nCost of Supplies\nThe scope of your project will determine the supplies required from the following list:\n- Tear off with repairs: All supplies in the list\n- New construction and tear off with no repairs: All supplies except sheets of roof sheathing\n- Adding a second layer of asphalt shingles: Shingles, ridge and hip shingles, nails, ridge vent\nHere is roofing cost per square (100 square feet) and per linear foot based on the material’s application.\nRoofing Supplies Prices:\n- $38-$42 per square | Roof sheathing – 7/16” sheets\n- $24-$40 per square | Roofing underlayment (Roofing paper)\n- $2.65-$4.25 per square | Roofing nails\n- $1.35-$2.65 per linear foot | Moisture barrier for valleys, eaves and rakes\n- $3.50-$5.50 per linear foot | Ridge vent\n- $.35-$.70 per linear foot | Drip edge\n- $3.10-$6.25 per linear foot | Ridge and hip shingles\nAsphalt Shingle Prices:\n- $60-$80 per square | Basic 3-tab shingles\n- $80-$95 per square | Better 3-tab shingles\n- $85-$100 per square | Basic dimensional/architectural shingles\n- $100-$120 per square | Mid-range dimensional / architectural shingles\n- $110-$165 per square | Premium dimensional / architectural shingles\nPopular Asphalt Shingle Prices:\nPerhaps it will help you determine a roofing cost per square foot if we list shingle costs per square for popular brand lines:\n- GAF Royal Sovereign: $65-$80\n- CertainTeed XT25: $72-$86\n- Owens Corning Supreme: $68-$84\n- Mid-grade dimensional/architectural:\n- GAF Timberline and Timberline HD: $118-$148\n- CertainTeed Landmark Premium: $125-$140\n- Owens Corning Duration: $108-$124\n- Tamko Heritage: $84-$105\n- Premium dimensional/architectural:\n- GAF Woodland: $148-$162\n- CertainTeed Highland Slate: $150-$170\n- Owens Corning Duration Designer Series: $110-$140\n- Tamko Heritage Woodgate: $175-$200\nPermits, Inspection, Related Costs and Installation Time\nPermits and Inspection Cost\nNow we’ll review asphalt roof installation cost including permits and labor.\n- $125-$300 | A permit is required for installing a new roof. One or two inspections will be included. The size of the home is considered in many communities when determining the cost of a roofing permit.\nRelated Costs and Installation Time\nOnly consider the costs that apply to your new roofing project:\n- $25-$250 | Total delivery charges for materials, when applicable\n- $100-$750 | Cost to raise shingle bundles to the roof, when applicable\n- $90-$165 per square | Removal and disposal of one or two layers of shingles and roofing paper\n- $115-$235 per square | Installing new asphalt shingles and required accessories\nAsphalt Roof Install Time Schedule\nThe average home in North America is +/- 2,000 square feet, though the average size of new homes is about 2,500 square feet. Here’s a timeframe that fits the average home.\n- 1 day: If necessary, remove old roofing and make necessary repairs to the roof sheathing.\n- 1 day: If necessary, install roofing paper. Begin installing shingles.\n- 1-2 days: Complete roofing most homes.\n- 3-4 days: Complete roofing homes with large and/or complex roofs.\nDIY or Hire a Pro?\nI’ve shingles a roof before, but it’s not one of those things an everyday DIY homeowner is going to like to do. I installed a metal roof on a storage building in my backyard, and long long ago, decided I would re-shingle the roof on our home when we lived in the Northeast. All I can say is, due to a new roof being a costly home repair, is it’s not really worth the time to do it as compared to hiring a professional. You need too many tools and skills to get it done properly in an acceptable amount of time.\n- Requires a lot of friends or helpers to move along quickly.\n- Unless you like carrying 40 pound bundles of shingles up a ladder, you’ll need special tools to hoist them up for you.\n- Hot, back-breaking work under a time schedule to beat bad weather.\nI’ll leave whole-house roofing to a pro every single time now.\nAsphalt Shingle Roof Replacement Costs from the Web\n|Costimates||$4.20 / sqft installed||$1.50 – $8 / sqft|\n|HowMuch.net||$2.90 / sqft installed||$1.20 – $4 / sqft|\n|RoofingCalc.com||$4.75 / sqft installed||$3.50 – $5 / sqft|\n|HomeWyse.com||$3 / sqft installed||$2.50- $3.5 / sqft|\n|Suggest a Cost Comparison from Another Website|\nCosts Submitted by Homeowners and Pro’s","Trying to figure out the different roofing types, roof designs and styles, and which roofing materials work best for your home can be overwhelming. There are many varieties to pick from depending on your roof type, the look you want, and your budget. Below, we take a look at the different types of roofing materials, designs and styles.\nRoofs may seem like a basic part of home construction, but there is a lot to know about them! There are a number of different types of roof shingles you can have on your home, and they range in cost, durability, lifespan, and the potential effect on your insurance premium.\nWhat are Asphalt Shingles Made out of?\nThere are two basic ways that asphalt shingles are manufactured.\nFiberglass. Fiberglass base mat coated with the asphalt and then ceramic granules. Architectural shingles are typically made in this fashion, making them more fire resistant and incredibly durable for the cost.\nOrganic base-mat. Rather than using a fiberglass base-mat, this method uses an organic material, such as felt paper, which is soaked in asphalt. They are then coated in an additional layer of asphalt, which the ceramic granules are then embedded into. While they are considered to be more malleable than their fiberglass counterparts, they are heavier and can become deformed over the years.\nArchitectural Shingles vs. Three-Tab Shingles\nAsphalt shingles are made up of fiberglass layered between asphalt & ceramic covered granules. The result is a water-resistant coating that can deflect UV light. While this is the basic composition of asphalt shingles, there are some variations to consider.\nTwo of the most common varieties are architectural & three-tab shingles. If you’re a new homeowner or renovating your home roof on a budget, these are the two options that may be worth your while. Here is a breakdown of the pros & cons of each shingle style.\nThree-tab shingles are relatively thin as they’re built of a single layer of materials. Architectural shingles, on the other side, have two asphalt shingles stripes laminated together. This layering creates a shingle that is better suited to hold up against the elements.\nOn average, the three-tab shingle comes with a warranty of almost 20, 25, or 30 years. Architectural shingles have minimum warranties of about 30 years. In general, three-tab shingles have the life expectancy & can only handle winds up to 60 mph as opposed to its counterpart’s tolerance for 130 mph gusts.\nDurability Winner: Architectural Shingles\nWhile a house roof serves a practical purpose, it can also be a great opportunity to add a personal touch on your property. That is why homeowners want to explore a wide range of aesthetic options when it comes to roofing materials. The three-tab shingle doesn’t provide much flexibility in this regard. It comes in one shape & size and has a flat complexion.\nHowever, the layered effect of the architectural shingle leaves room for diverse shapes & sizes. Homeowners can choose from different dimensional looks as well as colors & textures. The versatility of architectural shingles is an attractive feature of this option.\nVersatility Winner: Architectural Shingles\nThe architectural shingles are the all-around superior product, which is why more homeowners are picking it over the three-tab option. When it comes to cost, architectural shingles typically runs $35-$45 per bundle. Three-tab shingles cost an average of $25 -$30 per bundle.\nOn the surface, the architectural shingles are 40-50 percent more expensive. Still, it is a better investment, because the life expectancy of the three-tab shingles is, at best, half that of architectural shingles. Homeowners who spend less initially end up shelling out much more to install and replace three-tab options.\nPrice Winner: Architectural Shingles\nPopular Roofing Exhaust Vents\nOff Ridge Vents\nOff ridge vents are advantageous when the actual ridge line of the roof is small. This can happen with complex roofs and homes that do not have one long, continuous ridge line for a traditional ridge vent to run across. Adding an off-ridge vent or two to these types of roofs can provide an added punch of ventilation to areas that don’t have enough.\nThough they sound similar in name, an off-ridge vent is only similar to a ridge vent because they both sit close to the crest of your roof. In fact, “off ridge vents” are much more similar to box vents than they are to ridge vents!\nOverall, off ridge vents are not a very popular style of vent and not one we recommend when compared to other, more effective exhaust roofing vents. Off ridge vents are not as effective as full ridge vents because they are much smaller and do not sit as high on the roof. Their size prohibits them from expelling a large amount of hot air and their location restricts their ability to vent the absolute hottest air, like a ridge vent.\nBox vents are similar to off-ridge vents but are a much more popular venting solution.\nOne of the main similarities to an off-ridge vent is that the first step for installation is cutting a hole in the roof for the vent to sit over. Another similarity is that box vents are generally installed in bunches across the roof in order to add extra ventilation. Just one or two box vents is not nearly enough to vent your entire roof!\nThe design of a box vent is more square than it’s off-ridge counterpart, hence the name; box vent. There are a wide range of sizes available to match what’s needed for your space. The most common sized box vent on the market today is 18 inches by 18 inches.\nBenefits of Hiring Professional Roofing Contractors\nProfessional roofers have been in the industry for many years and have worked on different types of roofing projects for different clients. This means that they can deliver excellent results without any mistakes. They will ensure that your roof remains safe and undamaged for a long time. They will carry out new installations or repairs with perfection using the right products and techniques.\nHiring a roofing contractor to install a new roof or repair a damaged roof is quite cost-effective. These professionals will get roofing materials at a much lower cost than you because they have built long-term relationships with suppliers. In addition, they have the right tools needed to complete the work depending on the roof situation. If you calculate the expenses of purchasing the needed tools and materials, you will find that the cost will be much higher.\nThe main benefit of hiring roofing expects is the fact that they provide quality materials. This will ensure that the roofing services or repairs last for a very long time. Roofing specialists know the materials that are of premium quality and those that suit your specific roof. In addition, they have access to quality materials at wholesale costs.\nExpert roofers will offer a warranty on both materials and labor that they have used. This will give the homeowner peace of mind because the repair or new installation is protected irrespective of what happens after the project is complete. If there are any issues with the roof within the given time frame, it will be inspected and the problem will be resolved without extra charges.\nHiring a professional roof replacement expert is more beneficial than doing it on your own. Roofing contractors will have access to the right supplies, materials, and equipment to carry out the job successfully.\nIn addition, they are able to overcome any issue or challenges that they can detect after carrying out a detailed roof inspection. Therefore, if you want the best services in the industry, you should hire an experienced and licensed roofing contractor."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:942bf13c-b5cf-435c-9f19-5618a7f828dd>","<urn:uuid:3384120d-ff96-4e07-8fe3-271261af6a01>"],"error":null}
{"question":"What role can re-ordered churches play in local community life?","answer":"Re-ordered churches can provide vital local services and act as points of connection at the heart of local life. Through re-ordering, churches can transform from purely ecclesiastical spaces into community spaces that bring villages closer together, serving both the church family and the wider community while maintaining their role as places of worship.","context":["For many of us, our local church is the most prominent building in our community. Yet despite their visual prominence, many church buildings are under-utilised and are increasingly marginalised. We enjoy working with churches and the wider community to ‘re-order’ and re-imagine, providing solutions that place these buildings back at the heart of local life.\nThe Three Bs of Church Re-ordering\nWe have found that it is useful when considering a re-ordering project to think about it using the following three key concepts: belief, business and building.\nBelief as a vital part of church re-ordering\nThe concept of “Belief” is vital in re-ordering because it can create a shared vision and a common purpose for a building that is viewed sometimes solely seen as an ecclesiastical space.\nFor the church family there is belief in the mission of the church to reach out to the local community. For the wider community there is belief in the importance of securing the future of a culturally significant and valuable building or the need to create a community space that will bring a village closer together.\nTo achieve a common vision for a church building, we spend time listening to many and seek to find the common threads that can bring communities together using the church building. To develop a project that is sustainable and effective in the short and long term, as many people as possible need to believe in the project and be ready to support it.\n“This project has built deeper relationships amongst and between the church community and the wider community and both are flourishing as a result.”\nRevd Simon Lockett, Vicar of St Peter’s Church, Peterchurch\nBusiness at the heart of church re-ordering\nThe sad but simple truth is that many churches are no longer financially viable as they currently stand. For many churches the traditional method of funding, by the weekly collection, is no longer proving sufficient to meet the needs of the building. Further, many church buildings are not equipped or designed to meet the needs of the wider range of activities that today’s Church is involved in as part of its wider mission.\nIf a church is to thrive it needs to secure new sources of revenue and redefine its space in ways that enable it to function both now and into the future.\nThis almost invariably means re-ordering the building to create a space that can be used for a variety of uses including as a place of worship. To provide these new ‘services’ churches also need to be more business minded and to make the most of the opportunities that exist around them.\nIn this way, a church’s role in its local community can be redefined and re-imagined through its provision of vital local services and by being a point of connection that re-establishes it at the heart of local life.\n“This is a very good example of a church expanding its role in village life through the provision of new services – to the young and to the old. As well as transforming a gloomy, damp space, the work has redefined the church’s role in its village.”\nACE/RIBA AWARDS 2011 Jury’s citations\nCommunion Design (Winner) St Peter’s Church, Peterchurch, Herefordshire\nRe-ordering sensitively and successfully\nAll churches are locally if not nationally significant and therefore many are listed buildings. As such, they are extremely sensitive to change.\nA successful church re-ordering needs to create a space that meets the needs of its brief. It also needs to be carried out with real sensitivity and understanding of the importance and significance of the building or its atmosphere of peace and spirituality.\nAs conservation architects we have an understanding of conservation philosophy and an appreciation of what is special and valuable about a building. We also have the expertise to develop solutions that protect the fabric and integrity of the building as well as enable it to be accessible and useful to those that need it most.\nA visionary project to provide high quality catering, toilet and storage facilities concealed in freestanding oak boxes also included under floor heating, making this once draughty building warm and welcoming at all times and for everyone – including worshippers.”\nMrs Diana Evans, Head of Places of Worship Advice, English Heritage on St Peter’s Church, Hereford\nProjects that Communion has worked on include:\n“On behalf of all of us who met you last Wednesday at Bridge Sollars just a thank you for taking the time and trouble to meet and explain to us some of the thinking and rationale behind the work you have completed. We were much impressed, not just by the clean and modern design, but also by the transformation in the level of activity to be found in all the churches. We now realise better the potential of our own building and hope to convey it to our own church members.”"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:5a6fc4f1-6717-483b-aa1e-14dda839b112>"],"error":null}
{"question":"What roles do spices and aromatics play in both Andalusian cuisine and Vietnamese pho ga?","answer":"In Andalusian cuisine, Moorish influences brought spices like cinnamon and nutmeg, which are used in meat and fish dishes. For Vietnamese pho ga, aromatic spices including star anise, coriander, fennel seeds, cinnamon stick, and cardamom are essential for flavoring the broth, along with charred onions and ginger.","context":["Guide to Spanish Regional Cuisine: Southern Spain\nGuide to Spanish Regional Cuisine: Southern Spain\nThe 800-year reign of the Moors in Spain has left its mark in the southern region of Andalucía, more so than in others, in more ways than one. Language, architecture, music, dancing and, of course, in the cuisine.\nWhen the Moors invaded Spain and settled on the peninsula, they brought with them their own taste of home, which is still notable in the food of this region today.\nAs well as presenting the natives with new methods of farming and construction, they also introduced a diverse array of crops, spices, ingredients and flavours that have had a lasting effect on the Spanish cuisine in this region for all this time.\nFish and seafood\nWhile much of Andalucía is situated on the coast, fish and seafood make up much of the staple diet of this region. The southern coast of Spain produces some of the best quality fish in the whole of Europe and much of it is kept in the region and enjoyed by residents and visitors alike. Fish is often baked, fried, deep-fried or grilled and served on its own with lemon, with salad or a light dip. Or it is made into soups and stews and served with fresh, crusty bread. Seafood is generally deep-fried, often in batter or flour, and served on a platter with just a wedge of lemon. You can’t visit Andalucía without trying their grilled sardines or deep-fried squid.\nThe Arabs brought the orange and lemon with them – and lots of them at that. With hot summers and mild winters, the region is ideal for cultivating most fruits, and you will also find an abundance of strawberries, melon, pomegranates and figs growing there. Figs grow in the wild and Granada was named after the pomegranate, which thrives there. This region is famous for using fruit in its savoury cooking, an influence from the Moors, and you will often taste meat or fish dishes flavoured with orange, not to mention sweet spices such as cinnamon and nutmeg.\nSome of the best jamón serrano (cured ham) comes from the mountainous areas of Andalucía. The Sierra de Aracena is one of least visited and remote parts of the region and this is where the little village of Jabugo is located. While small in size, Jabugo is big in reputation as it not only produces the best and tastiest jamón ibérico, but this is also where you get the equally revered pata negra cured ham. The black Iberian pigs are mainly fed on a diet of acorns and olives for most of their lives and they are also allowed to roam free in the oak groves, therefore, getting plenty of fresh air and exercise. The best treatment of these pigs, produces the best quality ham.\nOlives and olive oil\nOlives come in all shapes and sizes in Andalucía and are present at every meal in some shape or form. Olive oil is used in most dishes and extra virgin olive oil is drizzled over the morning ‘tostada’ at the first meal of the day. Olives are stuffed, marinated or eaten as they are, often served as an accompaniment to a glass of cold beer on a hot summer’s day, or all year round, come to think of it. Olive oil and sherry vinegar, also produced in the region, are drizzled over salads that is usually served at lunch and dinner before the main meat or fish dish.\nAndalucía is the world’s largest producer of olive oil and a third of all the olive oil in Europe comes from this region. Andalucía really is the land of the olive.\nOne cannot discuss the food of southern Spain without mentioning the tapa. The tradition of serving small portions of fish, meat or salad dishes on a small plate to go with a cold drink is now popular all across the country and is what Spain is known for in culinary terms. However, the humble tapa originated here in Andalucía. It is still tradition in the majority of establishments, especially in Granada, for bar owners to offer a free tapa with every drink purchased, and you can easily bar-hop your way around a town enjoying delicious tapas and drinks along the way.\nPopular dishes from so\nThis is probably the most well-known dish to originate in the south of Spain and is served across the country, mainly in hot weather. Gazpacho is a delicious, chilled, raw soup, served as a starter, made from tomatoes, cucumber, peppers, olive oil and vinegar. It is often garnished with croutons, diced cucumber, onions or red peppers on top.\nFritura de pescado\nThis is also another popular dish that is predominantly served in coastal regions where fish and seafood are aplenty. It basically consists of various white fish and seafood, such as prawns and squid, which are deep-fried in batter or flour and served on a platter with a lemon wedge and garlic mayonnaise as a dip. It is generally served as a starter and goes well with wine or beer. It is very traditional in Málaga, Cádiz, Almería and even inland in Córdoba and Sevilla.\nHabas a la rondeña\nThis dish basically consists of broad beans and jamón serrano. The dish has extended all over the country and is typically referred to as “Spanish-style” beans, yet habas a la rodeña originated in the town of Ronda. The dish is served warm and also contains tomatoes, onions, red peppers and hard-boiled egg.\nRabo de toro\nThe rabo del toro is the tail of the bull, and this dish is better known as braised oxtail stew. It is actually one of the most delicious and popular dishes to originate from Andalucía, and more concretely, from Córdoba, and is one of the most typical dishes of Spain, funnily enough. The ox or bull’s tail is cooked slowly over a low heat with vegetables and red wine or sherry. Tradition says that after a bullfight, the tail of the bull that had just been killed would be used for this dish, although this does not happen nowadays.\nPescado a la sal\nThis is a typical Spanish dish that involves baking a whole fish, such as sea bass or sea bream, whilst it is covered in a thick coat of salt. If done properly, the result is a beautifully cooked and moist fish that looks wonderful as it is being presented on a plate to the diner. It is usually served with garlic mayonnaise or a mild parsley sauce.\nFideos a la malagueña\nThis is a great dish for pasta lovers. It originated in Málaga and is basically the same as a paella, but instead of using rice, you use their local spaghetti. The rest is the same as for a seafood paella – shellfish, prawns, clams, crab, peppers, saffron etc.\nThis is a typical fish soup flavoured with oranges from the Cádiz region that are usually exported to the UK to make marmalade. The broth of the soup has a bitter orange flavour and it also contains bread, potatoes and clams.\nHuevos a la flamenca\nEggs are used a lot in Andalucían cooking and this dish is often seen served in many of the restaurants in Seville. It basically consists of eggs that are divided into individual clay ramekins and baked in the oven with vegetables, jamón serrano and chorizo.\nTocino de cielo\nThis is a popular dessert from the region, especially Jérez de la Frontera, that literally translates to “bacon from heaven”. We would know it as crème caramel or flan. It is made from caramelised egg yolks, sugar and water. The recipe dates back to around 1325 and is said to have been created by the nuns in Jérez with the egg yolks that were given to the convents from the eggs that were leftover by the wine producers of the area.","Instant Pot Pho Ga (Vietnamese Chicken Noodle Soup)\nPho Ga, classic Vietnamese chicken noodle soup made easy in the pressure cooker. Authentic tasting pho at home, as good as your favorite Vietnamese restaurant.\nPrep Time 15 minutes\nCook Time 30 minutes\nTotal Time 45 minutes\n- 1 tbsp canola oil\n- 1 large onion, peeled and halved\n- 1 2-inch piece ginger, peeled, thickly sliced and bruised\n- 3 whole cloves\n- 1 tbsp coriander seeds\n- 1 star anise pod\n- 1 tsp fennel seeds\n- 1 1-inch piece cinnamon stick\n- 1 cardamom pod, lightly smashed\n- 6 chicken thighs, bone-in, skin-on (can use 6-8, depending on size)\n- 8 cups water\n- 1 small fuji apple, peeled, diced\n- ½ cup coarsely chopped cilantro\n- 2 tbsp fish sauce\n- 1 tbsp rock sugar (or rock candy)\n- 1 tbsp kosher salt\n- 1 pkg (14-oz.) Bahn Pho flat rice stick noodles\n- For serving: Sriracha sauce, sambal oelek, hoisin sauce, bean sprouts, fresh herbs like Thai basil, cilantro and/or mint, jalapeno slices, lime wedges\nPrepare pho noodles. Place in large bowl and cover with boiling water for 30 to 45 minutes, stirring occasionally to loosen up. Set aside.\nHeat oil in insert in Instant Pot on sauté setting until hot. Add halved onions and ginger, cut side down, and cook without moving, until well charred, about 5 minutes. Add spices (cloves, coriander, star anise, fennel, cinnamon stick and cardamom) and cook 1 more minute until fragrant. Turn off sauté function.\nAdd water to the pot and then carefully add the chicken, diced apple, chopped cilantro, fish sauce, rock sugar and salt. Seal the pressure cooker and cook at high pressure for 15 minutes.\nWhen done, turn off Instant Pot and let pressure naturally release for 10 minutes, release any remaining pressure manually and carefully open lid. Remove chicken thighs to plate and cool slightly.\nPour broth into cheesecloth-lined strainer (or fine mesh strainer) into a clean pot, discard solids. Skim any scum and some fat, if necessary, leaving some surface fat on the broth. Bring to a steady simmer on the stove. Taste for seasonings, adding more salt, pepper and/or fish sauce, if needed.\nRemove skin from chicken and meat from bones, shred. Prepare soup bowls; put a serving of strained noodles in bottom of each bowl, top with some chicken, then ladle hot broth over top. Serve immediately with condiments, letting each person choose toppings as they wish. Happy slurping!\n- I like to use chicken thighs, but you can also use a whole chicken, no larger than 4 pounds and reduce the amount of water to 7 cups. Be sure to stay under the maximum fill line on the pot. Shred desired amount of chicken and save the rest for another use, or leftovers.\n- I have not had to skim fat from the broth, if you have an excessive amount, then skim some of it off. But you want surface droplets of chicken fat on the broth, so don't get rid of all of it.\n- I find soaking the rice noodles in boiling water (for 30 - 45 minutes) to work just fine but you could follow package instructions, or soak and then cook noodles for just a couple of minutes. Be careful, boil too long and they become gummy!"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:a307018f-bccc-4b23-99e9-611fc5ad3534>","<urn:uuid:cb7b084a-b020-4055-a951-f0b162592a92>"],"error":null}
{"question":"I'm studying media evolution. How do newspaper names reflect older business models, and how does this parallel current publishing industry changes?","answer":"Newspaper names historically reflected their business models - many used terms like Advertiser, Commercial, or Mercantile by 1820 to emphasize their commercial focus. Today's publishing industry is experiencing similar business model disruption, comparable to how Japanese automakers disrupted Detroit by entering at the low end. Traditional publishers, like historical newspapers that had to adapt their names and business approaches, are facing challenges from new digital distribution channels and consumption methods, shifting from centralized bookstore distribution to electronic formats, much as movies evolved from theater-only viewing to streaming services.","context":["- Historic Sites\nCourants, Messengers, And A Plain Dealer\nHow your paper got its name\nOctober 1994 | Volume 45, Issue 6\nIs your newspaper a Gazette ? A Journal ? Do you read a Gleaner or a Quill or a Bee ?\nNewspaper names are a catalog of history and motive. Some were chosen because they seem traditional, like Gazette or Journal ; others reflect a sentiment of the namer. A Journal originally was a work that contained extracts from a recent book, while a Gazette , according to Voltaire, was “a relation of public affairs.” The distinction between the two was soon lost, and both journals and gazettes came to contain both political and cultural information.\nMany names, like Courier or Dispatch , refer to old means of disseminating news. When the Hartford Courant was founded in 1764, Courant was a fairly common newspaper name, for a courant was someone who ran from village to village spreading the news. Later the runner carried a written message, and eventually the message itself became known as a courant. (Hence the French phrase for being up on things: au courant .) Messenger and Mercury also refer to someone who carries news.\nCommerce entered the newspaper world in the 160Os in France, where Théophraste Renaudot decided that people who had goods and services to sell needed an effective way of making contact with potential buyers. The result eventually combined news of warfare and politics with solicitations for employment, trade, and barter. In England Marchamont Nedham, a pragmatic fellow who changed sides twice during the Civil War, participated in “Offices of Intelligence,” which essentially sold information on goods and services. Thus the name Intelligencer for a newspaper.\nThe advent of mail service enabled newspapers to be more regional. Post-Boy , Post-Man , and Flying Post in the early 170Os told Londoners their papers were up-todate. The name Post is still used, of course: St. Louis PostDispatch , the Washington Post , and the New York Post .\nThe telegraph enabled newspapers to carry recent news from beyond their immediate region, and newspapers with Telegram , Telegraph , and Signal reflected this competitive stance, while World and Globe heralded the papers’ new, wider orientation.\nCommercial news became increasingly important as an element of competition. In the mid-1700s Advertiser began to replace Post in newspaper titles, and by 1820 more than half of all newspapers in seven of the largest U.S. cities featured Advertiser , Commercial , or Mercantile in their titles.\nSometimes the origin of a name is shrouded in anecdote. According to Peter Bhatia, former managing editor of the Sacramento Bee , “Truth be told, the origin of the Bee ’s name isn’t officially known. But the common wisdom is that the paper’s founders more than a century ago sought a newspaper with the industriousness of a bee.”\nThe South Bay Daily Breeze (Torrance, California) was begun in 1894 by S. D. Barkley, who is said to have remarked, “I’m going to start a newspaper in this town tomorrow and call it the Breeze, because the breeze always blows here.”\nNaming isn’t always that casual. The sponsors who named the Blade (Toledo, Ohio) made a deliberate choice that worked on several levels. Blade referred both to swords, the most famous product of Toledo, Spain, and to a boundary war between Ohio and Michigan over the Toledo area. The paper’s initial editorial made the brave declaration that “we should prefer to keep our blade always in its scabbard and hope not to be compelled to use it often in the offensive. . . . But we hope it will always leap from its scabbard whenever the rights of individuals or of the community shall be infringed.”\nOf course, politics are often suggested in newspaper names— Republic, Democrat, Independent, Patriot —and the wrong one can be a problem, as the proprietor of Milledgeville, Georgia’s Federal Union discovered during the Civil War. After Fort Sumter the paper changed its name to the Confederate Union . After Appomattox it reverted to the Federal Union .\nNames often reflect editorial statements, and few can have enjoyed a grander explanation than that offered by Brainard W. Maples in the inaugural issue of his Norwalk, Connecticut, paper The Hour on May 6,1871: “When we had decided to commence the publication of this paper, our first perplexity was for a name. Like young parents, we have been puzzled what to christen our offspring....\n“A name must be appropriate, expressive and in harmony with the object which it is intended to imply.\n“Our paper will give an epitome of the occurrences of The Hour, making thereon such comments as may seem called for and proper. With The Hour come our duties and our responsibilities, to The Hour they are confined, and within The Hour must be completed. Beginning with time and continuing to eternity, The Hour embraces all that is of interest to humanity here and affords the opportunity to prepare for the hereafter. The Hour is our theme, our opportunity and our limit, and we have selected it as a name.\n“Our duty now is to make THE HOUR a pleasant, useful Hour, that may not pass unheeded or unread. . . .”\nThe final word on titling a newspaper must go to A. N. Gray and J. W. Gray, the two brothers who took over the Cleveland Advertiser and renamed it in 1842: “We offer no apologies for changing the name of this paper but the Scripture command—’Put not new wine into old bottles, lest they break.’. . . We think the good taste of our readers will sanction the modest selection we have made. Had we called it the Torpedo , timid ladies never would have touched it. Had we called it the Truth Teller , no one would believe a word in it! Had we called it the Thunder Dealer or Lightning Spitter , it would have blown Uncle Sam’s mail bags sky high. But our democracy and modesty suggest the only name that befits the occasion, the PLAIN DEALER .”\nThe name has always intrigued people, among them that indefatigable correspondent Winston Churchill, who said, “I think that by all odds, the Plain Dealer has the best newspaper name of any in the world.”","Publishing is undergoing disruption.\nHarvard Business School Professor Clayton Christensen introduced the world to his analysis of technological changes in his seminal “The Innovator’s Dilemma,” where be coined the word (and idea) of “disruption.”\nLong standing firms (incumbents) suddenly lose the power and dominance to disruptors who enter the market either at the low end, or in places that the incumbent does not serve; then the disruptor gets a toe-hold, and starts to move up the food chain where the profits are better.\nIt’s happening in publishing, just as it happened in automobiles when the Japanese entered the car market at the low end and pretty much overturned Detroit’s dominance.\nPeter Drucker tells us about how products work in markets. First, there is the product itself; second, the channel of distribution; and third, the market itself (i.e., how the product is consumed). Managers can order a product change more easily than a distribution change or a market change.\nConsider motion pictures, going back to the days of silent movies. It was a very cumbersome process—requiring consumers to go to a theater at set times to sit before a screen as a group and watch what the film maker had to show.\nToday, while theaters are still a factor, movies are streamed directly to homes and watched commercial free. Odd, the place we see commercials now is in theaters. Moreover, we watch the films whenever we want.\nAt one time television almost drove movie theaters into oblivion. Television monetized itself based on the concept of the commercial—radio had been doing that for some time. Yet, with the advent of the DVR, such as TiVo, commercials could be skipped by a process of fast-forwarding.\nAnd of course, Netflix takes it to a whole other level, not to mention series such as Game of Thrones, True Blood, and True Detectives.\nBack to Drucker. The product has changed a bit, yet if someone from 1920 were to see a current movie, they’d pretty much get that that’s what it was…a movie.\nBut the channels of distribution, and how we consume the movie, have changed quite a bit and the old business models have been disrupted.\nPeople got rich off those business models and anyone who’s seen Hollywood mansions knows that for a time the model worked well for the owners.\nPublishing is going through much of the same—disruption.\nLike the theaters of old, until recently books had to be “consumed” in special place—bookstores and libraries. With Amazon and Kindle and other electronic means, publishing is shaking out and the publishers largely do not like what is happening.\nDetroit did not make long-term headway by pointing out how “crummy” the first Japanese cars were, though students of the early 1970s can go back and see there were all sorts of articles out there to prove just that.\nHowever, with time, the Japanese automobile manufacturers came up-market all the way to the Lexus.\nPublishers are trying to use the same argument about the e-press. “There are a lot of crummy books out there,” they say, and they’d be right, but anyone who’s spent any time at an antiquarian books store will probably say that it has always been so.\nIt’s not whether a book is good or bad, but how does it navigate the channels of distribution and how is it consumed? The existing channels are being disrupted, just like movie theaters went to their nadir as television made distributed home-viewing more popular than centralized theater-viewing. Everyone’s said, “I’ll wait for it to come out on DVD,” or is that Blu-ray or Netflix now?\nAt bottom, the artist (the writer) has not changed, though we’ve given up our candle-lit rooms with inkwells and quills for the halo light of laptops with word processors integrated with grammar and spellcheckers, dictionaries, and thesauruses.\nWas it Marx who said that the Workers would do well to seize the means of production? Those of us who were early adopters of word processors, decades ago, whether we knew it or not, were doing exactly that—seizing the means of production.\nBut the other part of equation—distribution and consumption—are now being disrupted as other industries were in other eras.\nWe are now seizing the means of distribution and consumption, and like Detroit, there may be no way to stop this.\nIndeed, Elvis has left the building."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:782aca46-23ff-40cc-a9fc-4330bc0f6bc2>","<urn:uuid:76d268d6-be78-478c-849e-65e3d7b55c1b>"],"error":null}
{"question":"How do Hume's and Ted Honderich's views on causation differ in their approach to necessity and determinism?","answer":"Hume questions whether the principle that everything must have a cause is necessary or even true, suggesting we can imagine things beginning to exist without causes. In contrast, Honderich develops a 'conditionals theory' of causation that emphasizes necessary connections, arguing that when a match is struck, it had to light (was necessitated) in that situation. Honderich explicitly critiques Hume's regularity theory, noting that it incorrectly implies that yesterday causes last night simply because they regularly occur together. While Hume focuses on questioning causal necessity, Honderich defends a form of determinism based on conditional connections in the world.","context":["|A QUICK TOUR OF\nCAUSATION, PROBABILISTIC CAUSATION,\nDETERMINISM, AND FREEDOM AND RESPONSIBILITY\nA talk by Ted Honderich to the Centre Cournot, Paris\n16 Oct 2008, 18.00\nThere is also a French version of this quick tour.\nCAUSATION AS CONDITIONAL CONNECTIONS IN THE WORLD\nStrike a match. It lights. We call these events cause and effect. What do we mean?\n1. Since the striking (s) happened, in the situation as it was, including oxygen, so too did the lighting (l) follow. By way of a label, the striking required the lighting.\n2. If the striking hadn't happened in the situation, where for example the match wasn't held in the flame of another match, neither would the lighting have followed. The striking was required for the lighting.\nThe same two kinds of conditional connections in the world, each dependent on the situation, hold between each event in certain sets of events that we can call causal circumstances for the lighting. A causal circumstance (cc) included the event that for some reason we pick out and call the cause -- the striking (s).\n3. There was also a different and fundamental kind of conditional connection, an independent one, between cc and the lighting. Since cc occurred, whatever the situation had been, the lighting would still have occurred. Expressed differently, since cc occurred, whatever else had been happening, l would still have occurred. By way of a label, the causal circumstance necessitated the lighting.\n4. Since l occurred, cc would still have occurred before it in the absence of another causal circumstance for l. The causal circumstance cc was necessary for l.\nThis independent-conditionals theory of standard effects, at bottom about whatever-else connections in the world, is not open to the great to the regularity or constant conjunction theory of David Hume -- in short the objection that Hume's theory makes last night the effect of yesterday, since days and nights go together regularly. On the conditionals theory, it is not true that yesterday caused last night. This is because last night would not have happened whatever else had been happening in addition to yesterday. Last night would not have happened if the solar conditions changed at the end of yesterday.\nThe conditionals theory of causation is also not open to objections against David Lewis's related counterfactual theory in terms of a logic of possible worlds.\nThe central claim in the theory of probabilistic causation, however it is to be understood, is said to be that causes raise the probability of the occurrence of their effects. Speaking generally, A causes B iff P(B | A) > P(B | not-A).\nBetter expressed, an event A causes an event B iff A makes B more probable than not-A would, other things being equal, even if A leaves B hardly probable at all.\nTo this idea, well introduced by Christopher Hitchcock, an advocate of it, in an article in the Stanford Encyclopedia of Philosophy, there are many objections.\n(i) Almost all of us believe, despite the temptation of common interpretations of Quantum Theory, that when the match was struck, in the situation as it was, it had to light. The lighting was necessitated. If the match hadn't lit, we wouldn't for that reason think that if it had lit, that lighting wouldn't have been necessitated. Rather we would think that something was missing in the situation when it didn't light. So the simplest and to my mind the overwhelming response to probabilism, as an account of our conception of causation and also what it is, is that it wipes out this proposition of the necessity of effects.\n(ii) It is a related fact, too often passed by quietly, that probabilistic causation leaves the actual occurrence of events, certainly events with a probability of less than 1, without any explanation at all. If probabilistic causation is put in place of causation, there is no answer at all to why an event actually occurred, as distinct from being probable. It is not too much to say that on this view, reality is mystery. So this view is remote from causation as we think about it and know it.\n(iii) The indeterminist interpretations of the basic mathematics or formalism of Quantum Theory are a mess. This is so in the more deferentially-expressed admissions of those philosophers and theoreticians of science, including scientists, most disposed to the interpretations. The fact that inconsistent, weird, and mysterious interpretations are tolerated is an unfortunate effect of a rightful hegemony of science. It is is no significant argument that the theory 'works'. Classical mechanics worked. Inconsistent theories can work.\n(iv) I am no physicist, but able enough to read the principal expositions of interpretations of Quantum Theory by physicists for non-physicists. It is common to read or to understand or to suspect that the items taken as non-effects are also not events. This plethora of ideas as to non-effects is about items that do not happen. But no determinism is the theory that propositions or numbers or other abstract entities are effects. No determinist has ever contemplated that the number 1, as against inscriptions of it etc, is an effect.\n(v) Indeterminists face a dilemma. Given the fact of macro-determinism, including the macro-determinism of the brain, either there is no micro-indeterminism underneath or else it cancels out or anyway doesn't translate or amplify upwards into the macro-world. So, it must seem, the micro-indeterminism of interpretations of Quantum Theory is either false or irrelevant.\n(vi) It is argued that interpretations of Quantum Theory establish, about events of which we are sure that they cause cancer, that these events are undetermined or unnecessitated. So, unless we take causation to be probabilistic, we will have to be agnostic about well-supported or even best-supported causal claims. A reply is that if this is about what causation is taken to be, or what it would be if it existed, there is a less confusing and maybe less confused response. If later unnecessitated events are somehow explained by prior events in some unnecessitating way, then the prior events are not causes, whatever else is to be said of them. But it doesn't matter too greatly who gets possession of the word 'cause' and like words. What matters is clear distinctions, evidence etc.\n(vii) Some probabilists are motivated, as they say, by the proposition that smoking doesn't always cause cancer. Or the proposition, to use our example, that s-like events don't always cause l-like events. So, they then conclude, smoking causes cancer only in the sense of making cancer more probable, as s made l more probable. Whatever motivating their proposition does, it is no argument at all against the independent-conditionals theory. That theory itself asserts that s only requires l and doesn't necessitate it.\n(viii) It is objected that the complexity of the independent-conditionals theory, like related theories, makes it less appealing than what is unkindly taken to be Hume's simple theory. It is in fact supposed that the acute Hume thought that the cause that is the striking of a match is always followed by the effect of its lighting. I can't believe it, despite his quick words. In any case, it is no serious objection that the conditionals theory is more complicated, partly since any arguable theory of probabilistic causation must be as complicated.\n(ix) Probabilists seem to assume that probability judgements are not a matter of causation, but rather an independent kind of explanation. Suppose three brothers, all heavy smokers, die of cancer, and the fourth brother, also a heavy smoker, is not looking good. If a probability statement is offered as a ground for a sad prediction about him, what can it be based on but causal reasoning of the kind sketched above? Of course the probability statement is logically consistent, for example, with somebody's idea about a God who occasionally flies into a temper about too much smoking in a family, and commands a death that otherwise would not have happened. But the probability judgements of the rest of us just are results of ordinary causal reasoning, aren't they? At bottom, they rest on a good idea of a kind of causal circumstance for something, and the knowledge that all of or a lot of or some of one exists. This objection can be made in connection with the various general theories or interpetations of probability judgements -- in terms of what are spoken of as upshots among equally probable ones, good or logical reasons, frequency of past events, subjectivity, and objective or real chance in the world.\n(x) The probabilistic causation theory faces problems. A golfer hits a bad stroke and the ball goes off the fairway into the rough. This makes it less probable than a good stroke would have been that the bad stroke will be a hole-in-one. But the ball bounces off a tree and ends up in the hole. Probabilistic causation says the bad stroke didn't cause the hole-in-one. All theories face problems, of course. Probabilistic causation needs epicycles and contortions to deal with its problems.\n(xi) Another problem is causal direction or priority or assymetry. Causes explain their effects and make them happen, but effects do not make explain their causes or make them happen. To be still more brief, it strikes me that the account of this fact given by the conditionals theory is more persuasive than accounts given by probabilistic causation.\nThere can be a conceptually adequate theory of human determinism -- determinism having to do with our choices and decisions and other antecedents of our actions. It rests on an adequate conception of causation, say causation as conditional connections. It also rests on a conceptually adequate theory of consciousness, and its relation to the brain.\nThe theory, in brief, has three parts. (1) Conscious events are in some kind of lawlike connection with simultaneous neural events, but are not identical with them. Two such events are in some whatever-else connection but not exactly causal connection. They are different only in being simultaneous. They can be called psychoneural pairs. (2) These pairs are effects of certain sequences of causal circumstances and effects. (3) Actions are effects of causal sequences beginning with psychoneural pairs.\nThe principal and great evidence for the theory is standard neuroscience. On the basis of this evidence, and also because of the weakness noted above of arguments based on interpretations of Quantum Theory, determinism remains at least a reasonable assumption.\nFREEDOM AND RESPONSIBILITY\nThe tradition of Compatibilism in English and American philosophy asserts that our single, settled and fundamental conception of freedom is voluntariness. It is, in short, doing what we want, action that is the effect of causation internal to us rather than external to us. Hence we are free when we are not in jail, or otherwise constrained or compelled. The opposed tradition of Incompatibilism asserts that our conception of freedom is or includes origination or free will. This is initiation of action that could have been otherwise given things exactly as they were -- initiation that is uncaused but still within the control of the actor.\nThus Compatibilism is that determinism and freedom co-exist. Determinism is logically compatible or consistent with freedom and the absolutely inseparable fact of responsibility. Determinism is compatible with our single, settled and fundamental conception of freedom and also with our one attitude of holding people responsible for their actions and crediting them with responsibility for them. Incompatibilism is that determinism and freedom cannot co-exist. Determinism is logically incompatible with freedom and responsibility.\nTo me, and to an increasing number of philosophers, Compatibilism and Incompatibilism, of which endless proofs have been attempted, are both provably false -- because of their shared presupposition that we have but one settled conception of freedom and responsibility. On the contrary, we have both ideas -- freedom as voluntariness and freedom as origination. They inform such attitudes as hope for the future and such institutions as rights and punishment by the state. What is true, by way of a label, is Attitudinism.\nThe true problem of determinism has seemed to me not whether Compatibilism or Incompatibilism is true, but only the practical problem of how to give up our conception of freedom as origination and the related attitudes having to do with responsibility.\nOne postscript now seems to me advisable. It may not be possible to avoid feeling that one has had an independence in one's life, that one has been in a way accountable. This is not origination, but it does or would explain the standing we take ourselves to have, our difference from everything else that is subject to determinism.\nThis independence may be a matter of a special explanatoriness in our lives of the line of causes (within a causal sequence) which consists in our ongoing neural existence. Also of a new conception of consciousness, at bottom an externalist conception of perceptual consciousness as consisting in worlds of perceptual consciousness. They have a dependency on us neurally as well as the micro-world underneath them. We are in a way creative of a reality, or jointly sustaining with respect to it. It is worth remembering, too, from Kant and others, that we can be free in in the sense of acting rightly, say in accord with the Principle of Humanity, as against being subject to the desires of self.\nReading on probabilistic causation:\nChristopher Hitchcock, 'Probabilistic Causation', Stanford Encyclopedia of Philosophy, http://plato.stanford.edu/ Sept 6, 2002.\nEllery Eells, Probabilistic Causality, Cambridge University Press, 1999.\nPaul Humphreys, The Chances of Explanation, Princeton University Press, 1989. David Lewis, Philosophical Papers, Vol. 2, Oxford University Press, 1986.\nJohn Mackie, The Cement of the Universe, Oxford University Press, 1974.\nHugh Mellor, The Facts of Causation, Routledge, 1995.\nWesley Salmon, 'Probabilistic Causality', Pacific Philosophical Quarterly, 1980. Brian Skyrms, Causal Necessity, 1980.\nHonderich publications on causation, determinism, freedom etc:\nÊtes-vous libre? Le problème du déterminisme, Syllepse, 2008.\nHow Free Are You? The Determinism Problem, 2nd ed. Oxford University Press, 2002 -- also German, Greek, Italian, Japanese, Spanish, Swedish, Polish and Romanian translations.\nOn Determinism and Freedom, collected papers, Edinburgh University Press, 2005. A Theory of Determinism: The Mind, Neuroscience and Life-Hopes, OUP, 1988, of which Chs 1-6 are the paperback Mind and Brain and Chs 7-10 the paperback The Consequences of Determinism, OUP, 1990.\nOn Consciousness, EUP 2004.\nAnthony Freeman, ed., Radical Externalism: Honderich's Theory of Consciousness Discussed, Imprint Academic, 2006.\nPunishment: The Supposed Justifications Revisited, Pluto Press, 2005.\nPapers on causation etc at http://www.homepages.ucl.ac.uk/~uctytho/\nTo the French version\nHOME to T.H. website front page\nHOME to Det & Free website front page","It is Hume’s opinion that the principle “whatever begins to exist must have a cause of existence” is not obvious and maybe even false.\nHe considers and rejects the following argument in favor of the principle:\nAll the points of time and place, say some philosophers, in which we can suppose any object to begin to exist, are in themselves equal; and unless there be some cause, which is peculiar to one time and one place, and which by that means determines and fixes the existence, it must remain in eternal suspense; and the object can never begin to be, for want of something to fix its beginning.\nBut I ask; Is there any more difficulty in supposing the time and place to be fixed without a cause, than to suppose the existence to be determined in that manner? The first question that occurs on this subject is always, whether the object shall exist or not? The next, when and where it shall begin to exist? If the removal of a cause be intuitively absurd in the one case, it must be so in the other: And if that absurdity be not clear without proof in one case, it will equally require one in the other. (A Treatise on Human Nature, 1.3.3, 4)\nIt’s not just the time and place that must be determined but also the essence of the object that will begin to exist. We may call the event random, but at some point, even a second before it occurs, there must be determination. The roulette will stop spinning and reveal definite values. Then we should be able to predict that, for example, a chair precisely described will pop into existence tomorrow at 11 am and right in front of me.\nHume argues that we can imagine this to occur without any cause. The question, however, is what or who do we consult to make even this limited prediction? What do we inspect and study in order to find all this information out? Now while it is tempting to answer that it’s precisely the cause of the chair, let’s slow down a bit.\nSuppose it’s written in some database cleverly etched onto helium atoms to which we have managed to gain access that the chair will appear thusly. Why believe this information? Probably because similar entries have yielded correct predictions. Now we have 2 events appearing close to each other: a new row is added to a certain table in the database and then the thing specified, at the prescribed time and place (in that row’s fields), appears out of nowhere. The connection between the database and real-world events cannot be doubted. There is no coincidence; the former anticipates and predicts the latter. This is now a law of nature, discoverable by experiment and reason. Now if the database itself is not the cause of the events recorded in it, some definite X must connect the two. Some X must one way or another force the universe to obey the instructions written in the table. X must read each entry in the table and act accordingly.\nThis X is what all men call the cause(s) of the chair in front of me at 11 am.\nSo then, yes, everything that begins to exist has a cause."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:de876c92-0fb4-4e71-9d2f-1090d2812a87>","<urn:uuid:5748983d-ca7c-4dbf-b081-78e164d82ded>"],"error":null}
{"question":"How do the diagnostic approaches differ between spinal muscular atrophies and ALS?","answer":"The diagnostic process for spinal muscular atrophies includes family history assessment, electromyography, nerve conduction studies, and blood tests to detect the defective gene in 95% of cases, with occasional muscle biopsy. For unborn children with family history, amniocentesis can be performed. ALS diagnosis is more complex and often challenging, with almost half of patients initially misdiagnosed. It requires a comprehensive evaluation by a neurologist with neuromuscular expertise, including detailed neurological history, physical examination, blood tests, genetic testing, nerve conduction studies, electromyography, and MRI imaging. This thorough process is necessary because approximately 15% of patients initially diagnosed with ALS actually have different conditions that can mimic ALS.","context":["The five main types of spinal muscular atrophies cause various degrees of muscle weakness and wasting.\nDepending on the type, people may be confined to a wheelchair, and life span may be shortened.\nThe diagnosis, suggested by symptoms, is based on family history, tests of muscle and nerve function, and blood tests to detect the defective gene.\nThere is no cure, but physical therapy and use of braces can help.\n(See also Overview of the Peripheral Nervous System Overview of the Peripheral Nervous System The peripheral nervous system refers to the parts of the nervous system that are outside the central nervous system, that is, those outside the brain and spinal cord. Thus, the peripheral nervous... read more .)\nSpinal muscular atrophies are usually inherited as an autosomal (not sex-linked) recessive trait Recessive disorders Genes are segments of deoxyribonucleic acid (DNA) that contain the code for a specific protein that functions in one or more types of cells in the body. Chromosomes are made of a very long strand... read more . That is, two genes for the disorder are required, one from each parent. These disorders may affect the brain and spinal cord (central nervous system), as well as peripheral nerves.\nThere are five main types of spinal muscular atrophy.\nSymptoms of SMAs\nSymptoms of the first four types of spinal muscular atrophy first appear during infancy and childhood.\nSpinal muscular atrophy type 0, the most severe form, begins to affect the fetus before birth. The fetus does not move as much as expected during late pregnancy. Once born, the baby has severe weakness and lacks muscle tone. Reflexes are absent, and joint movement is limited. Both sides of the face are paralyzed. Birth defects of the heart are also present. The muscles that control breathing are very weak. Infants often die within the first months because they cannot breathe adequately, resulting in respiratory failure Respiratory Failure Respiratory failure is a condition in which the level of oxygen in the blood becomes dangerously low or the level of carbon dioxide in the blood becomes dangerously high. Conditions that block... read more .\nIn spinal muscular atrophy type 1 (infantile spinal muscular atrophy or Werdnig-Hoffmann disease), muscle weakness is often apparent at or within a few days of birth. It is virtually always apparent by age 6 months. Infants lack muscle tone and reflexes and have difficulty sucking, swallowing, and eventually breathing. Death occurs in 95% of children within the first year and in all by age 4 years, usually due to respiratory failure.\nIn spinal muscular atrophy type 2 (intermediate form of Dubowitz disease), weakness typically develops between age 3 and 15 months. Fewer than one fourth of children learn to sit. None can crawl or walk. Reflexes are absent. Muscles are weak, and swallowing may be difficult. Most children are confined to a wheelchair by age 2 to 3 years. The disorder is often fatal in early life, usually because of respiratory problems. But some children survive with permanent weakness that does not continue to worsen. These children often have severe curvature of the spine (scoliosis).\nSpinal muscular atrophy type 3 (juvenile form or Wohlfart-Kugelberg-Welander disease) begins between age 15 months and 19 years and worsens slowly. Consequently, people with this disorder usually live longer than those with type I or II spinal muscular atrophy. Some of them have a normal life span. Weakness and wasting of muscles begin in the hips and thighs and later spread to the arms, feet, and hands. How long people lives depends on whether respiratory problems develop.\nSpinal muscular atrophy type 4 first appears during adulthood, usually between the ages of 30 and 60 years. Muscles, mainly in the hips, thighs, and shoulders, slowly become weak and waste away.\nDiagnosis of SMAs\nA doctor's evaluation\nElectromyography and nerve conduction studies\nA blood test for the abnormal gene\nSometimes biopsy of a muscle\nDoctors usually test for spinal muscular atrophies when unexplained weakness and muscle wasting occur in young children. Because these disorders are inherited, a family history may help doctors make the diagnosis.\nElectromyography and nerve conduction studies Electromyography and Nerve Conduction Studies Diagnostic procedures may be needed to confirm a diagnosis suggested by the medical history and neurologic examination. Electroencephalography (EEG) is a simple, painless procedure in which... read more help confirm the diagnosis. The specific defective gene can be detected by blood tests in 95% of affected people (genetic testing).\nOccasionally, biopsy of a muscle is done.\nIf there is a family history of one of the disorders, amniocentesis Amniocentesis Prenatal diagnostic testing involves testing the fetus before birth (prenatally) to determine whether the fetus has certain abnormalities, including certain hereditary or spontaneous genetic... read more can be done to help determine whether an unborn child has the defective gene.\nTreatment of SMAs\nPhysical and occupational therapy\nBraces and assistive devices\nDrugs that may improve muscle function and delay disability and death\nThere is no cure for spinal muscular atrophies.\nPhysical therapy and wearing braces can sometimes help. Physical and occupational therapists can provide adaptive devices to enable children to feed themselves, write, or use a computer.\nNusinersen may slightly improve muscle movement and may delay disability and death. Nusinersen is injected into the space around the spinal cord. Before injecting the drug, doctors often numb the injection site with a small amount of local anesthetic. Then they insert a needle between two vertebrae in the lower spine, as is done for a spinal tap Spinal Tap Diagnostic procedures may be needed to confirm a diagnosis suggested by the medical history and neurologic examination. Electroencephalography (EEG) is a simple, painless procedure in which... read more (lumbar puncture). Nusinersen is initially given in four doses over a period of 2 months. Then it is given at regular intervals every 4 months.\nTwo new drugs have been developed.\nOnasemnogene abeparvovec-xioi is used to treat some children under 2 years old. Only one dose is given. It is given intravenously over 1 hour. It appears to help children meet developmental milestones Childhood Development Between the ages of 1 and 13, children's physical, intellectual, and emotional capabilities expand tremendously. Children progress from barely tottering to running, jumping, and playing organized... read more , such as sitting without support, eating, rolling over, and walking independently.\nRisdiplam is used to treat adults and children 2 years old and over. It is given as a liquid or through a feeding tube once a day. It may delay death and reduce the need for mechanical ventilation Mechanical Ventilation Mechanical ventilation is use of a machine to aid the movement of air into and out of the lungs. Some people with respiratory failure need a mechanical ventilator (a machine that helps air get... read more to help with breathing.\nGene therapy for spinal muscular atrophy has recently become available. It is injected into a vein. Only one injection is required.","About Amyotrophic Lateral Sclerosis:\nAmyotrophic lateral sclerosis or ALS, also known as Lou Gehrig disease, is a serious neurological disorder that is frequently fatal. ALS affects the motor nervous system, leading to voluntary muscle weakness, and can affect, in about 50% of cases, behavior and cognition. Patients with ALS typically present with hand or foot weakness (limb onset ALS) or trouble speaking or swallowing (bulbar onset ALS). The disease is progressive and spreads from one limb to another. Muscle twitching is almost always present. Serious complications arise when the disease affects the swallowing muscles which can lead to aspiration pneumonia or when it affects the respiratory muscles. \"A form of progressive dementia called frontotemporal dementia may emerge in some ALS patients. The patient’s behavior and cognition may be affected and the patient’s family or friend may notice personality changes, disinhibition, uncontrollable laughter or crying, word finding difficulties and executive impairments. Median survival of ALS patients is 2-5 years but 20% of patients survive beyond 5 years.\nNon-motor symptoms in ALS patients are common and are a direct result from the weakness. Sensory complaints are not typical but patients may have muscle or joint pain because of decrease in mobility, cramps and occasional numbness and tingling. Constipation is frequent. Patients may notice increase in their saliva and drooling. Insomnia can occur because of difficulty turning in bed, anxiety and depression. Stomach discomfort or liver disease may occur as a side effect of Riluzole, a medication used to slow down the progression of ALS.\nALS diagnosis is made by a neurologist with expertise in neuromuscular disorders. The diagnosis can be difficult to make, especially early in the disease. This explains why almost half of patients with ALS are initially misdiagnosed. In addition, about 15% of patients with a different condition are initially told they have ALS. A number of other diseases may mimic ALS making a comprehensive diagnostic evaluation important. Diseases that may mimic ALS include: Multifocal motor neuropathy, cervical myelopathy with radiculopathy, benign fasciculation syndrome, post-polio syndrome, spinobulbar muscular atrophy, late onset Tay-Sachs disease, paraneoplastic and autoimmune motor neuropathies etc.\nDuring the visit, the patient should expect a detailed neurological history, a thorough physical and neurological examination, blood tests, genetic testing, nerve conduction studies and electromyography and MRI imaging. These different investigation methods can help assess whether the patient has ALS or a condition that may mimic ALS.\nLiving with ALS:\nLiving with ALS is challenging for the patient, their family and their friends. Patients may have to change their life goals. Living life fully is still possible and one should not give up hope as research in the field is advancing at a fast pace. Although the disease is progressive, there are, in general, no surprises; meaning that there is no sudden paralysis or inabiltiy to move the arm or leg.\nVisiting a multidisciplinary ALS clinic every 2-3 months can help maintain a good quality of life, prevent potentials complications, and has been shown to prolong life by an average of 12 months. The multidisciplinary team will evaluate and treat both the motor and non-motor ALS symptoms, the behavioral and cognitive aspects of ALS. The multidisciplinary team consists of a physician with expertise in ALS disease, a neurologist with expertise in frontotemporal dementia, a physical therapist, an occupational therapist, a respiratory therapist, a speech therapist, a registered dietician, a social worker, a clinic administrator and an ALSA and or a MDA representative. The ALS clinic also works closely with the genetic department who provide whole exome capture, a gastrointestinal physician when a peg tube is needed and a pulmonary physician if tracheostomy is being considered (tube placed through the windpipe to take over breathing when diaphragm muscles become weak)."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:7b29387b-feaa-4a85-a53b-19cee77c29bb>","<urn:uuid:7a5b3bbe-fe16-41dc-bfe9-5cd861030e36>"],"error":null}
{"question":"How do environmental emergencies and welding activities affect respiratory health?","answer":"Environmental emergencies involving gas releases can impact respiratory health, requiring people to stay indoors for safety. In welding specifically, exposure to welding fume causes severe respiratory issues - 40-50 welders are hospitalized with pneumonia annually (with 2 deaths per year), 9 welders claim benefits for work-related asthma problems yearly, and welding fume is classified as potentially carcinogenic to humans. The fume contains harmful gases like nitrous oxide, carbon dioxide, carbon monoxide, and ozone that can cause throat and lung irritation.","context":["Environmental Emergency (E2) Information From Celanese\nWhat is Ethylene?\nEthylene is a colorless, flammable gas with a sweet odor and taste. Ethylene is produced naturally, through the metabolic processes of plants and microbes or via other events such as volcanic activity, forest fires, combustion of fossil fuels, and processing of natural gas. Ethylene has also been known to impact vegetation.\nHow can ethylene affect my health?\nHealth assessments of the effects of ethylene indicate that no long-term effects can be attributed to exposures of high concentrations.\nWhat Is Propylene?\nPropylene is a colorless, flammable gas with a faint petroleum odor. Propylene is a naturally occurring gas that is emitted from many plants and is a component in natural gas, volcanoes, and from incomplete biomass combustion. Propylene does not appear to have any harmful effects to terrestrial plants or small mammals even when they are exposed to very high concentrations in air\nHow can propylene affect my health?\nShort term exposure to moderate levels of propylene can cause headaches, dizziness and nausea.\nWhat is Vinyl Acetate?\nVinyl Acetate is a flammable, colorless liquid that has been described as having a sweet, fruity odor or sharp, sour odor. It is very soluble in water although adverse effects to plants from VA in ambient air have not been identified. Vinyl Acetate poses a health hazard in high concentrations and is not naturally found in the environment. Vinyl Acetate is typically manufactured and is widely used as a building block for other types of materials such as ethylene vinyl acetate copolymers.\nHow can VA affect my health?\nPermitted levels of VA should not cause short- or long-term health effects. VA, however can be an irritant to the eyes, nose, and throat at sufficiently high concentrations, and exposure to significantly elevated inhaled doses for a sufficient duration can also lead to more serious respiratory and eye irritation as well as damage to the lining of the nasal and respiratory tract. Based on available data, throat irritation is the most sensitive effect of short term exposure to sufficiently high concentrations of VA.\nWhat is an E2 Scenario?\nAn E2 scenario at the Celanese facility is an uncontrolled, unplanned, or accidental release of any of the aforementioned substances that could reasonably result in a fire, explosion or toxic impact to the environment or surrounding community.\nCelanese Integrity Management Systems\nCelanese is committed to the health and safety of our employees, contractors, communities and the environment. Our rigorous integrity management systems were created and dedicated to managing risks to people, the environment and the community. These management systems strive for excellence by ensuring we are following all applicable regulations and best practices, tracking metrics to demonstrate system effectiveness and identifying opportunities for improvement through self-audits and third-party audits.\nEmergency Preparedness Plans\nCelanese is dedicated to being prepared in the event an emergency should occur. Our Emergency Preparedness Management System aims to rapidly respond to an incident while keeping the safety of personnel, the environment and the community paramount. Keys elements include but are not limited to:\n- Well defined response plans\n- Trained personnel\n- Emergency response equipment available in pre-defined locations\n- Regularly conducting simulated drills, ranging from simple head-counts to full-scale “catastrophic” events\nIn the event of an emergency with potential offsite impacts, such as a gas release, fire or explosion, communication with personnel, local authorities and the surrounding community will be provided as soon as possible with regular updates until the emergency is resolved. Communications may come from the Alberta Emergency Alert System, directly from local authorities, and/or telephone via the Celanese Communications/Public Information Officer (if applicable). The SIA UpdateLine (1-866-653-9959) will also provide up to date information regarding the emergency. Unless otherwise instructed, the following guidelines should be followed during any emergency:\n- Go/remain indoors\n- Tune into the SIA UpdateLINE (1-866-653-9959)\n- Tune into the local media or Alberta Emergency Alerts\nShould you require additional information, please contact Celanese at 780-468-0800.\nIf you want to learn more about E2 Regulations, click here.","- 40-50 welders are hospitalised every year with pneumonia caused by welding fume\n- 2 of these welders die every year\n- 9 welders every year suffer from work-related asthma problems that lead them to claim benefits\n- Stainless steel fume contains harmful chromium oxide (Cr₂O₃) and nickel oxide (NiO)\n- Welding fume is classified as potentially carcinogenic to humans¹\nUnderstanding the problem\nThe fume given off by welding and hot cutting processes is a varied mix of airborne gases and very fine particles which, if inhaled, can cause health problems. Harmful gases that may be present in the fume include nitrous oxide (N₂O), carbon dioxide (CO₂), carbon monoxide (CO), argon (Ar), helium (He) and ozone (O₃) .\nFour of the leading occupational health hazards in welding are respiratory problems, vibration issues, noise-related problems and musculoskeletal disorders (MSD). Let’s take a look at all of these in a little more detail and also consider how to manage the risks involved:\n- Pneumonia – Welders are particularly prone to a lung infection that can lead to severe and sometimes fatal pneumonia. While modern antibiotics usually cure the infection, severe cases lead to 40-50 welders being hospitalised every year. Of these cases, around 2 will be fatal\n- Occupational asthma – Even though around 9 workers each year get asthma so badly that they are able to claim benefits, a recent study by HSE determined that welding fume could not be conclusively proven to cause asthma. HSE still recommends welders protect themselves and control welding fume to lowest possible levels\n- Cancers – Welding fume is internationally classified as potentially carcinogenic to humans. This classification includes all welding fume although it is primarily associated with stainless steel welding\n- Metal fume fever – Many welders report flu like symptoms after welding, particularly at the start of a working week, but this does not usually have any lasting effects. Unfortunately, the stories that suggest drinking milk before welding prevents you getting metal fume fever are simply not true\n- Irritation of throat and lungs – Gases and fine particles in welding fume can cause dryness of the throat, tickling coughing or a tight chest. Ozone is a particular cause of this when TIG welding stainless steels and aluminium. Extreme exposure to ozone can cause fluid on the lungs\n- Temporarily reduced lung function – Overall lung capacity and peak flow are affected by prolonged exposure to welding fume, but the effects are not permanent\nFor detailed information on how to manage all of these issues, take a look at the COSHH Guidelines.\nRespiratory Protective Equipment (RPE) can help you address the problem of fume if other preventative measures do not reduce it below the workplace exposure limit (WEL). Follow our RPE checklist to ensure you are giving your workers the correct protection:\n- Disposable face masks can also offer protection for short jobs. Just like reusable respirators, these should be fit tested on the individual as one type of mask does not fit all\n- Reusable respirators should be inspected every month and records must be kept\n- Battery powered filtering welding helmets are more expensive. This type of equipment can last a long time if well looked after, and may be a cost effective option in the long term²\nIn terms of hand-arm vibration, there are particular risks with tasks like grinding and needle scaling that are closely associated with the welding process. The Control of Vibration at Work Regulations 2005 require you to make sure all risks are controlled and that you provide instruction and training to employees on the risks involved and the actions being taken to deal with them.\nThese regulations include an exposure action value (EAV) and an exposure limit value (ELV) based on a combination of the vibration at the grip points on the equipment and the time spent gripping it. To better understand these values, check out the HSE hand-arm vibration exposure calculator.\nBy complying with the regulations and you will help to prevent disability from Hand-Arm Vibration Syndrome (HAVS) and vibration-related Carpal Tunnel Syndrome (CTS). Some employees may develop early signs and symptoms of HAVS and CTS even at low exposures, but your health surveillance should identify any issues early on. Appropriate action will alleviate the problem. While HAVS can be prevented, once the damage has been done it cannot be cured.\nCertain cases of HAVS and all cases of vibration-related CTS must be reported to HSE in accordance with the Reporting of Injuries, Diseases and Dangerous Occurrences Regulations (RIDDOR).\nWith the exception of TIG-Welding, electric arc welding can generate harmful levels of noise. The other tasks that welders will typically do and the working environment are also particularly noisy. This list gives you a good idea of the noise levels associated with different tasks within the welding process:\n- TIG – up to 75 dB(A)\n- Manual Metal Arc (MMA) – 85-95 dB(A)\n- Metal Inert Gas(MIG) – 95-102 dB(A)\n- Plasma cutting (hand-held up to 100A, cutting up to 25mm only – 98-105 dB(A)\n- Flame gouging – 95 dB(A)\n- Flame cutting – up to 100 dB(A)\n- Air arc gouging – 100-115 dB(A)\n- Deslagging/chipping – 105 dB(A)\n- Grinding – 95-105 dB(A)\nThe best way to manage the problem is to eliminate the noisy process altogether, for example by buying in material cut to size by the supplier, though this may not always be feasible. Following the hierarchy of control, the next best options are substitution, engineering controls, administrative controls such as training and work scheduling and then finally personal protective equipment (PPE).\nEar plugs, ear muffs or other hearing protection should be selected on the following criteria:\n- Ability to reduce the noise exposure\n- Compatibility with other items of PPE, such as welding helmets (useful options include slim-line ear muffs with a neck band rather than a head band)\n- Suitability for the activity and working environment\nIf PPE is provided, it is vital your workers are given appropriate training to ensure they wear their protection in the correct way and at all required times.\nMusculoskeletal Disorder (MSD)\nThe manual handling carried out by welders that are repeated regularly or involve twisting and turning into awkward postures can be particularly hazardous.⁵\nThe Provision and Use of Work Equipment Regulations 1998 require you to consider the risks to the health and safety of workers when selecting the equipment they will use. This includes manual handling risks. Choosing the right tool will reduce the chances of:\n- Personal suffering caused by musculoskeletal disorders\n- Financial burden of sickness absence and increased insurance premiums\n- Reduced productivity\n- Restricting earning potential of employees unable to return to the same type of work⁶\nHealth and Safety within the welding industry has improved significantly over the last few years, although it would be fair to say that there is still work to be done.\nFor a summary of this blog, download our Top Health Hazards Facing Welders Quick Guide.\nIf you’d like to know more about the long-term health and safety issues in metal processing and how best to deal with them, call the 3M helpline on 0870 60 800 60 for an expert opinion.\nTo check out our webinars visit 3M Safety Spotlight"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:2108a2ed-91f8-42c9-9ff7-35a9209b6938>","<urn:uuid:33076410-c5fd-42b6-b90f-b6ddb7167463>"],"error":null}
{"question":"Do both social business and sustainable entrepreneurship prioritize environmental consciousness?","answer":"Yes, both models prioritize environmental consciousness. Social business explicitly includes being environmentally conscious as one of its seven key principles. Similarly, sustainable entrepreneurship focuses on environmental impacts as a core component, considering environmental concerns alongside social and economic impacts when creating long-term value, rather than pursuing profit at the expense of environmental concerns.","context":["Coined by Noble Laureate Professor Yunus, the idea of social business is considerably new. In his book ‘Building Social Business’, Professor Yunus distinguished social business with a social enterprise or any other type of social venture. Also known as enlightened capitalism, the emerging field of social business focuses on using business methods and practices to achieve positive social changes. With an often-ambiguous identity, social businesses aim to promote social welfare as primary driver, while also making a profit. The main difference of a social business with any other traditional for-profit social enterprise is that the investors, or the shareholders, do not receive any share of the profits. They only get their invested amount back after a certain time. The profits are re-invested to expand the business with the vision to increase impact. Stakeholders receive a dividend on their investment in the form of social change. There are two main components of social business model:\n- Value proposition – Who are the customers and what is being offered to them?\n- Value constellation – How do the value is being delivered to the customers? This involves not only the company’s own value chain but also its value network with its suppliers and partners.\nAccording to Professor Yunus these two components need to fit together like pieces of a puzzle in order to generate a positive profit equation, which is the financial translation of the other two, and includes how value is captured from the revenues generated and manifested in both economic and social profit equation. Social business is founded on seven key principles:\n- A business objective to overcome poverty, or one or more problems (such as education, health, technology access, and environment) which threaten people and society; not profit maximisation.\n- Financial and economic sustainability\n- Investors get back their investment amount only. No dividend is given beyond investment money\n- When the investment amount is paid back, company profit stays with the company for expansion and improvement\n- Gender sensitive and environmentally conscious\n- Workforce gets market wage with better working conditions\n- Do it with joy\nThere are two types of social businesses:\nType 1 – Benefit maximising SB\nSocial businesses that focus on providing a social benefit rather than on maximising profit for the owners, and that are owned by investors who seek social benefits such as poverty reduction, health care for the poor, social justice, global sustainability, and seeking psychological, emotional and spiritual satisfactions rather than financial reward. Typically, Type One social businesses are undertaken by large corporations or investors who are also keen to make an impact through positive social change.\nType 2 – Profit maximising SB\nIn most cases Type Two of social businesses are owned and operated by the poor or disadvantaged. In Bangladesh, social business commenced with the inception of Grameen Bank. From the beginning, Grameen Bank lent money to many poor villagers, mainly women, to start their own businesses to earn profit for making a living. Though the loan amount was small, it was a great help for those who had barely enough to feed themselves. In this type of social business, the social benefit is derived from the fact that the dividends and equity growth produced by the business will benefit the poor, thereby helping them to reduce their poverty. The following figure shows the foundation of the social business model –\nLundstrom, A., & Zhou, C. (2013). Rethinking Social Entrepreneurship and Social Enterprises: A Three-Dimensional Perspective into Social Entrepreneurship. International Studies in Entrepreneurship, 29, 71-89.\nYunus, M. (2010). Building social business: The new kind of capitalism that serves humanity’s most pressing needs. Dhaka: The University Press Limited.\nYunus, M. (2017). Social business entrepreneurs are the solution. In The Future Makers (pp. 219-225): Routledge.\nYunus, M., Moingeon, B., & Lehmann-Ortega, L. (2010). Building social business models: lessons from the Grameen experience. Long Range Planning, 43(2), 308-325.","Sustainable Entrepreneurship: Myths Vs. Facts\nSustainable entrepreneurship is an approach to conducting business that focuses on creating long-term value while considering environmental, social, and economic impacts. It challenges the traditional notion that profit should be pursued at the expense of environmental and social concerns. There are several myths and misconceptions surrounding sustainable entrepreneurship that need to be debunked in order to understand its true potential.\nMyth 1: Sustainable Entrepreneurship is Expensive\n- Fact: While the initial investment might be higher, sustainable entrepreneurship can lead to significant cost savings in the long run. Energy-efficiency measures, waste reduction, and responsible sourcing can all result in reduced operational costs.\n- Fact: Sustainable practices often bring about process improvements and innovation, leading to increased efficiency and productivity.\n- Fact: Companies embracing sustainability have access to various government incentives, grants, and subsidies that can offset initial costs and provide financial support.\n- Fact: Customers, investors, and stakeholders increasingly favor companies with sustainable practices, creating new business opportunities and enhancing competitiveness.\n- Fact: Building a sustainable brand can lead to increased customer loyalty and trust, resulting in stronger relationships and higher sales.\nMyth 2: Sustainable Entrepreneurship is Limited to Certain Industries\n- Fact: Sustainable entrepreneurship is applicable across industries and sectors, with opportunities for innovation and impactful change in every sector.\n- Fact: From renewable energy to eco-friendly manufacturing and responsible agriculture, sustainable practices can be integrated into any business model.\n- Fact: The key is to identify the environmental and social impacts of the industry and develop strategies to mitigate them.\n- Fact: Collaborative efforts between businesses, governments, and communities can lead to system-wide changes that make sustainable entrepreneurship feasible in even the most challenging industries.\n- Fact: Sustainable entrepreneurship often involves cross-sector partnerships and collaborations, leading to innovative solutions and shared value creation.\nMyth 3: Sustainable Entrepreneurship is a Distraction from Profit-Making\n- Fact: Sustainable entrepreneurship is not an either-or proposition; it’s about finding ways to create economic value while considering social and environmental aspects.\n- Fact: By embracing sustainability, entrepreneurs can tap into new markets, attract socially conscious customers, and gain a competitive advantage.\n- Fact: Sustainable practices lead to cost savings, resource efficiency, and risk reduction, ultimately contributing to long-term profitability.\n- Fact: Businesses that fail to adapt to changing consumer preferences and regulatory trends risk losing relevance and market share in the long run.\n- Fact: A focus on sustainability can drive innovation and differentiation, enabling entrepreneurs to create unique value propositions in the marketplace.\nMyth 4: Sustainable Entrepreneurship is a Marketing Gimmick\n- Fact: Sustainable entrepreneurship goes beyond marketing and branding; it requires a genuine commitment to integrating environmental and social considerations into the core business strategy.\n- Fact: True sustainability involves measuring, managing, and improving the company’s impact on people, planet, and profits.\n- Fact: Companies engaging in greenwashing, which involves falsely presenting themselves as sustainable, risk damaging their reputation and facing backlash from customers and stakeholders.\n- Fact: Genuine sustainability efforts require transparency, accountability, and continuous improvement, which cannot be achieved through mere marketing tactics.\n- Fact: Building a sustainable business requires a holistic approach, aligning values, operations, and stakeholder engagement to create meaningful and lasting change.\nMyth 5: Sustainable Entrepreneurship is a Trend\n- Fact: While sustainability has gained significant attention in recent years, it is not just a passing trend.\n- Fact: The challenges posed by climate change, resource scarcity, and social inequality make sustainable entrepreneurship a necessity for long-term societal and economic well-being.\n- Fact: International frameworks such as the United Nations’ Sustainable Development Goals (SDGs) provide a clear roadmap for businesses to integrate sustainability into their strategies.\n- Fact: The global community’s collective commitment to addressing these challenges ensures that sustainable entrepreneurship will remain a priority for years to come.\n- Fact: Entrepreneurs who seize the opportunity to innovate and build sustainable businesses have the potential to thrive in the evolving marketplace.\nSustainable entrepreneurship is not just a buzzword; it represents a fundamental shift in how businesses operate and create value. By debunking the myths surrounding sustainable entrepreneurship, it becomes evident that it offers numerous benefits, from cost savings and increased competitiveness to innovation and long-term profitability. Embracing sustainability is not only a responsible choice but also a pathway to success in the dynamic and evolving business landscape."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:2c5de741-84e3-4bdf-b8f2-96a775a7a37f>","<urn:uuid:5c44fd3c-1694-4fc9-8237-41e828eb284a>"],"error":null}
{"question":"How much potential water savings can be achieved through vacuum-based toilet technology?","answer":"Vacuum-based toilet technology can reduce water consumption by up to 90% compared to conventional systems. While traditional toilets use about 9 liters per flush (or 36 liters per person daily), vacuum systems like those used in airplanes and the No-Mix Vacuum Toilet use significantly less. The No-Mix system specifically could save around 160,000 liters of water per year for every 100 flushes in a public restroom setting, using just 0.2-1 liters per flush depending on the type of waste.","context":["On board the plane, if there is something that often tickles our curiosity, it is the toilet. A real mystery, say the passengers in chorus. So many questions cross our minds: where can the droppings be evacuated when we are several thousand meters above sea level and there is no sewage system nearby? And why is the toilet flush so noisy? The answer is simple: because of the vacuum and lack of water. You think that this device is not at all the same as that of the house. The flow is completely different. While a conventional flush consumes about 9 liters of water (or nearly 36 liters per day / pers), it is clear that this type of rinsing is impossible in airplanes. You would have to transport tons of water! Not only would it cost far too much, but it would also take up a lot of space and consume more kerosene.\nTo overcome this problem, aeronautical engineer James Kemper devised an ingenious alternative: a vacuum system. Since the 80s, this option, which reduces water consumption to 90%, has been used in all commercial aircraft. How does it work? The toilet is emptied by a strong suction. That’s why you hear this funny deafening noise. In its portal, the German airline Lufthansa gives more details: “A valve opens and the contents of the bowl are immediately sucked in by negative pressure, a bit like a vacuum cleaner.” Therefore, just after pressing the flush button, the low pressure produces a loud noise, but it lasts only two to three seconds.\nWhere does the feces end up from the toilets on board?\nAnother recurring question: what is the evacuation system for urine and feces? Be aware that there is also a well-crafted process to deport them. Once the flush is fired, this residue ends up in a collection tank in the fuselage. You are really curious and you want to know more about this journey? Be aware that a mixture of water and anti-rot chemicals (in small quantities) are used to detach the sticky substances from the Teflon bowl and ensure transport to these famous blue tanks. And of course, it is only after landing that these tanks end up being emptied using special vehicles. Fascinating, isn’t it?\nWhen can I use the toilet on the plane?\nYou are a little more enlightened on the overall process of how the toilet works on the plane. Okay now, you’d mostly like to know why you can’t use them whenever you want, right? You probably think that there is no harm in going to pee, before the plane takes off or even on landing. Very bad idea, you see. You could even block the plane itself! The American newspaper “Milwaukee Journal Sentinel” reported an astonishing anecdote: at Delta Airlines, passengers were even ejected from the plane because they had relieved themselves shortly before takeoff. Unbelievable but true!\nIt turns out that take-off and landing are the most sensitive moments of the flight. No wonder the flight attendants ask you to turn off your electronics, raise the seat, fasten your seat belt or fold up the small table. Same principle for the use of toilets. It is strictly forbidden to have access to it, or even to walk in the corridor. Better than that: The American website “Bussines Insider” revealed that flight attendants were required to inform pilots immediately when a passenger was still locked in the toilet at that time. And while this traveler relieves himself quietly carefreely, there is panic in the cockpit! Pilots must immediately interrupt their preparations for take-off or landing. Result of the races: this can cause long delays and cause inconvenience to other passengers. Therefore, it is better to follow these strict rules and only go to the toilet when the belt signal is turned off, including during boarding. Again, all of these precautions are only applied for your own safety and that of other travelers.","After a year and half of research, scientists from Nanyang Technological University (NTU) have found a way to reduce the amount of water needed for flushing and reuse human waste. Their invention involves a new toilet system (dubbed the No-Mix Vacuum Toilet) that can turn human waste into electricity and fertilizers while reducing the amount of water needed to flush by up to 90 percent – in comparison to the currently used toilet systems in Singapore.\nHow does this innovative system reduce the amount of water used? By using vacuum suction technology – which is used in airplane lavatories–which would only require 0.2 liters of water to flush liquids and one liter to flush solids. This is a significant reduction from the currently existing conventional toilet which uses 4 to 6 liters of water with each flush. Estimates have speculated that if this system were to be installed in a public restroom, they would save 160,000 liters of water each year per hundred flushes!\nCurrently the scientists are trying to conduct a trial by installing the prototypes of this toilet in two NTU restrooms, and then to share this technology with the world in the next three years. But as Associate Professor Wang Jing-Yuan, Director of the Residues and Resource Reclamation Centre (R3C) at NTU points out, conserving water is not the only benefit and goal of this system – the goal is to “have a complete recovery of resources so that none will be wasted in resource-scarce Singapore.”\nProfessor Wang shares:\n“Having the human waste separated at source and processed on-site would lower costs needed in recovering resources, as treating mixed waste is energy intensive and not cost-effective. With our innovative toilet system, we can use simpler and cheaper methods of harvesting the useful chemicals and even produce fuel and energy from waste.”\nThe No-Mix Vacuum Toilet has two separate chambers that separate the liquid waste and the solid waste. The liquid waste would then get diverted to a processing facility where nitrogen, phospherous and potassium can be recovered and reused for fertilizer. The solid waste would be sent to a bioreactor where it would be digested to release a bio-gas containing methane–methane is odorless and can be used to replace natural gas used in stoves for cooking, and can be converted to electricity.\nIf this new toilet is functional and lives up to its claims, then this could help save water around the world. Flushing the toilet is one of the top uses of water in a home: It has been estimated that the average toilet can use up to 3.5 to 5 gallons of water per flush. Jack Sim, the founder of the World Toilet Organization, told Time Magazine that “chances are you’re dumping up to 22 liters of drinkable water every day, one three- to six-liter flush at a time.”\nBut that’s not the only problem. After flushing, a lot of resources are spent to clean flushed water and thereby, increases the carbon footprint of toilets. Rose George, author of The Big Necessity: The Unmentionable World of Human Waste and Why it Matters shares that “the sewage system uses as much energy as what the largest coal fire station in the [country] produces.”"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:85daff1c-b283-4a01-9527-2780919fd115>","<urn:uuid:3e0e2c99-3a4a-4317-a87a-eb9c0a23fa3d>"],"error":null}
{"question":"How do silage yields compare between high-density corn planting and non-GMO corn cultivation methods?","answer":"High-density corn planting can improve forage yields without negatively affecting nutritional quality of silage, though this depends on harvest timing. When harvested at early stages (70% moisture), higher plant densities don't show yield advantages since smaller individual plant weights aren't compensated by more plants per acre. However, at more advanced maturity (65% moisture), higher densities typically show better yields due to increased grain weight. For non-GMO corn cultivation, the focus shifts more to pest management, particularly regarding corn rootworm control, which requires either high-rate seed treatments or granular soil insecticides to maintain yields, especially in second through fourth year corn fields where rootworm risk increases significantly.","context":["The author is assistant professor, department of dairy science, Virginia Tech.\nFarmers often boost corn plant densities as a means to maximize forage yields or replenish silage stocks. In the past few years, our team at Virginia Tech has performed several on-farm plant density research projects to evaluate the impact of changing plant densities on forage yield and quality.\nHere are some of our results, which elicit a number of considerations when changing corn planting rates.\nA higher corn plant density can boost forage yields (see table), although this response was not always observed in every study. The typical observation was that higher corn plant densities resulted in smaller plant weights. However, the smaller biomass per plant is often compensated by more plants per acre, resulting in similar forage yields.\nThere is a balance between the number of plants and the weight of the individual plants. This balance, which determines the existence of differential forage yields, is “broken” depending on maturity at harvesting. Based on observations from our studies, when a crop is harvested at early stages of maturity (namely early dent or close to 70 percent moisture), a greater number of plants cannot compensate for the smaller weight of the individual plants. Therefore, silage yield is similar.\nWhen a crop is harvested at a more advanced maturity (half-milkline or closer to 65 percent moisture), the larger plant has more grain weight, leading to a yield advantage for higher plant densities. If the forage grower is planning an early harvest of the corn crop, then raising the population of plants to maximize forage yield might not be beneficial.\nA higher corn plant density typically reduces the number of kernel rows per ear and the number of kernels per row within an ear.\nKernel development is a determinant of forage yield and quality, so having a reduction in the number of kernels per plant does not seem very appealing to the farmer. However, we still need to consider that there are more plants per acre at higher planting densities, which typically translates into greater yields of grain and possibly starch. Therefore, don’t base you opinion of potential silage yield and quality on the appearance of the ears.\nAs mentioned above, higher plant densities typically result in plants with a lower number of developed kernels per ear. Fewer developed kernels are often viewed as having reduced concentrations of starch and higher concentrations of fiber (in other words, lower energy concentrations). Contrary to this belief, several recent studies have shown that boosting corn plant density has minimum (if any) effects on the nutritional composition and the digestibility of the resulting silages.\nDespite several recent studies evaluating the effect of plant density on forage yield and quality, only a few have evaluated its interaction with fertilizer management strategies. Our research team performed an on-farm study in southwest Virginia last summer (2017) where corn was planted at three densities (26,000, 30,000, or 35,000 plants or acre) with either a single (45 pounds per acre) or double (90 pounds per acre) pass of nitrogen right before elongation of the internodes. To our surprise, doubling the sidedressed nitrogen raised the forage yield by only 5 percent.\nIt is worth to mentioning that a very severe drought around silking time was observed during the summer in southwest Virginia. This may have masked any beneficial effects of additional nitrogen fertilization on forage yields. Despite the severe drought, the higher planting rates did not result in lower forage yields. We are still analyzing the samples for nutritional quality so stay tuned for future updates.\nPlanting more corn seeds per acre will raise input expenses. Whether this practice is economically viable will depend almost entirely on obtaining better forage yields. A simple partial budget and sensitivity analysis will help decide if boosting plant densities makes economic sense.\nIn conclusion, higher corn plant densities can improve forage yields without negatively affecting the nutritional quality of the silage. The likelihood of better forage yields is highly influenced by environmental conditions and maturity at harvest. Lower forage yields per acre due to higher plant densities are unlikely under typical growing conditions.\nThis article appeared in the February 2018 issue of Hay & Forage Grower on page 24.\nNot a subscriber? Click to get the print magazine","- Managing Corn Rootworm in Non-GMO Corn\n- Low Weed Densities in Conventional and Organic Soybean in 2017\n- Organic Soybeans Yield 55 Bushels/Acre ……………. but Conventional Beans Yield 60 Bushels/Acre\nBill Cox1, Eric Sandsted1, R.J. Richtmyer2, and Phil Atkins2\n1Soil and Crop Sciences Section, 2New York Seed Improvement Project\nWe initiated a 3-year study at the Aurora Research Farm in 2015 to compare different sequences of the corn, soybean, and wheat/red clover rotation in conventional and organic cropping systems under recommended and high input management during the 36-month transition period (2014-2017) from conventional to an organic cropping system. We provided a detailed discussion of the various treatments and objectives of the study in a previous soybean article (http://blogs.cornell.edu/whatscroppingup/2015/09/16/emergence-early-v2-stage-plant-populations-and-weed-densities-r4-in-soybeans-under-conventional-and-organic-cropping-systems/). This article will focus on 2017 yields, the first year that organic soybean would be eligible for the organic premium.\nCorn preceded soybean in the rotation in this study. The fields were plowed on May 17 and then cultimulched on the morning of May 18, the day of planting. We used a White Air Seeder to plant the treated (insecticide/fungicide) GMO soybean variety, P22T41R2, and the non-treated non-GMO variety, 92Y21, at two seeding rates, ~150,000 (recommended input) and ~200,000 seeds/acre (high input). The varieties are not isolines so only the maturity of the two varieties and not the genetics are similar between the two cropping systems. We treated the non-GMO variety in the seed hopper with the organic seed treatment, Sabrex, in the high input organic treatment. We planted soybean in the typical 15” row spacing in the conventional cropping system and the typical 30” row spacing (for cultivation of weeds) in the organic cropping system.\nWe applied Roundup (Helosate Plus Advanced) on June 21 at ~32 oz. /acre for weed control in conventional soybean (V4 stage) under both recommended and high input treatments. The high input soybean treatment in the conventional cropping system also received a fungicide, Priaxor, on August 2, the R3 stage. We used the rotary hoe to control weeds in the row in recommended and high input organic soybean at the V1-2 stage (June 2). We then cultivated close to the soybean row in both recommended and high input organic treatments at the V3 stage (June 12) with repeated cultivations between the rows at the V4-V5 stage (June 22), the V5-V6 stage (June 28), the R1 stage (July 5), and the R2-3 stage (July 20).\nWe estimated soybean plant densities in all treatments at the V2 stage (June 2), just prior to the rotary hoeing operation. We then estimated soybean plant densities at the V2-3 stage (June 12), just before the first close cultivation between the rows, to determine the extent of rotary hoe damage to soybean. We estimated weed densities at the R4 stage (August 14th) by counting all the visible weeds along the 100 foot plot across the entire 10 foot plot width. We harvested all treatments on September 26 when conventional and organic soybean averaged ~11.0% moisture.\nOrganic vs. conventional soybean densities averaged ~4.25% higher at the V1 and V2-3 stages (Table 1). As mentioned in a previous article (http://blogs.cornell.edu/whatscroppingup/2017/06/06/soybean-emergence-and-early-plant-densities-v1-v2-stage-in-conventional-and-organic-cropping-systems-in-2017/), we attributed the differences in plant densities to different varieties as opposed to different cropping systems. Weed densities were exceedingly low in conventional soybean, which received a timely single application of Roundup at the recommended rate (Table 1). Weed densities were much higher in organic soybean but still relatively low. As mentioned in a previous article, (http://blogs.cornell.edu/whatscroppingup/2017/08/31/low-weed-densities-in-conventional-and-organic-soybean-in-2017/), we did not expect the higher (than conventional) but relatively low weed densities in organic soybean to greatly influence yield.\nNevertheless, conventional soybean did yield ~8% greater than organic soybean in 2017 (Table 1). This is the first time in the 4-year study that conventional soybean did yield greater than organic soybean. Yield did have a negative correlation with weed densities (-0.52) perhaps because only 1.48 inches of precipitation was recorded at Aurora in August, the R3 to R5 stage in soybean in 2017. Another contributing factor may have been the difference in row spacing between the two cropping systems. In a previous study at the same site (https://scs.cals.cornell.edu/sites/scs.cals.cornell.edu/files/shared/documents/wcu/WCU20-2.pdf), 15-inch soybean yielded 8.5% greater than 30-inch soybean.\nHigh input soybean did not yield greater than recommended input soybean in the conventional cropping system (Table 1). The conventional treatment was planted at 200,000 seeds/acre and received a fungicide application at the R3 stage. Unlike 2015 when August conditions were dry (1.36 inches) and the fungicide application may have improved plant health, leading to an 8% yield increase (http://blogs.cornell.edu/whatscroppingup/2015/11/09/soybean-yield-under-conventional-and-organic-cropping-systems-with-recommended-and-high-inputs-during-the-transition-year-to-organic/), the fungicide application did not increase soybean yield in 2017. As in numerous other studies, we did not observe a yield increase with seeding rates at 200,000 seeds/acre compared with the recommended 150,000 seeds/acre in conventional soybean.\nAs in previous years, the high input (200,000 seed/acre seeding rate and organic seed treatment), and recommended input (150,000 seeds/acre) treatments in organic soybean yielded similarly (http://blogs.cornell.edu/whatscroppingup/2016/11/28/organic-soybean-once-again-yields-similarly-to-conventional-soybean-during-the-second-transition-year/). Some organic growers plant soybean at higher seeding rates to help control weeds. Weed densities were similar between the high and recommended input treatments in organic soybean so yields were once again similar for the 3rd consecutive year in this study.\nIn conclusion, conventional soybean yielded 8% higher than organic soybean for the first time in the 3rd year of this 4-year study. Organic soybean, however, would be eligible for the organic premium this year because 36 months have elapsed since the last application of synthetic fertilizer or pesticides on the fields used in this study. Consequently, organic soybean, despite the 8% lower yield, would be more profitable to the transitioning organic soybean grower in 2017, especially at the recommended 150,000 seed/acre seeding rate and no organic seed treatment.\nElson J. Shields, Entomology, Cornell University\nAn increasing number of dairy producers are being asked by their milk processors to seriously consider producing milk from dairy cows fed non-GMO forages and grains. Many milk producers feel the pressure to comply with the request in order to preserve their milk market. The decision to grow non-GMO corn impacts both the weed control program and management of corn rootworm.\nA review of the biology of corn rootworm is a good starting point for this discussion. Adult corn rootworm emerge from existing corn fields around the first of August, where the larvae have been feeding on corn roots. Adults begin emerging around corn pollen shed and start feeding on corn pollen. After about 3-weeks, the females begin to lay eggs in existing corn fields. The eggs overwinter with the larvae hatching the following May. If the field is planted to corn, then the larvae start feeding on corn roots, but if the field has been rotated to another crop, the newly hatched larvae die. Since eggs are laid in existing corn fields, first year corn has zero risk of corn rootworm damage in NYS. In terms of risk, second year corn fields have a 25-35% chance of risk for rootworm damage, third year corn a 50-70% risk of losses from corn rootworm larval feeding and fourth year corn risk for rootworm losses is between 80-100%.\nAdult rootworm scouting procedures are available and help to decide if the field is medium-high risk for larval damage the following growing season. Adult beetle scouting occurs around pollination and is conducted for three subsequent weeks. If beetle counts average 1 beetle per plant and the females have mature eggs, the field is at risk for larval feeding damage.\nAdult beetle scouting does not account for the subsequent larval mortality when the soils are waterlogged during the hatching period, so using adult counts to estimate risk usually overestimates risk. Assessment of the rootworm larval population after hatch is difficult and very labor intensive.\nSince first year corn has zero risk from rootworm larval damage, the standard seed treatment (Poncho, Cruiser 250) is all that is needed for protection from germinating-seedling damaging insects. Second through fourth year corn need some protection for potential corn rootworm larval feeding.\nRisk of rootworm feeding damage increases with the duration of continuous corn within a field. Since first year corn has zero risk from rootworm and second year corn has reduced risk (25-35%), a shortened corn rotation reduces the need (and cost) of rootworm management. Producers who can only grow two years of corn before rotating to a non-corn crop can frequently grow corn without any extra rootworm management expense.\nHigh Rate of Seed Treatments:\nThe high rate of seed treatment (Poncho 1250, Cruiser 1.25) offered for seed corn has activity on corn rootworm larvae. In growing seasons with adequate rain fall and moderate corn rootworm pressure, the high rate of seed treatment will provide adequate protection for the crop. In situations where the field has high rootworm pressure, the insects often overrun the insecticide, resulting in economic root feeding damage. High rates of seed treatments are also challenged to provide adequate control during times of limited rain fall during June-July or in times of excessive rain fall during the same time period. Use of the high rate of seed treatment is best matched to the second and third year of continuous corn when rootworm pressure is lower.\nLiquids Soil Insecticides:\nThe use of liquid soil insecticides (Capture, Force) mixed with the liquid starter fertilizer and applied in-furrow has become a popular option for growers without granular insecticide boxes on their corn planter. Past research in NY has consistently shown that either of these two insecticides applied in this manner are highly variable in control. In talking with farmers who are avid supporters of the use of liquids in this manner, they frequently admit to control failures consistent with the research results. The major issue with this application method is not the efficacy of the insecticide on the rootworm larvae, but the timing of the application with a liquid formulation. Application of a soil insecticide at planting is introducing the soil insecticide into the soil environment three to four weeks before corn rootworm larvae begin to hatch. It has always been a challenge for soil insecticides to still be in the root zone 3-5 weeks after application so the insecticide can be present to kill the newly hatched larvae. Granular insecticides bridge this time period with the slow-release properties of the granule. High-rate seed treatments bridge this time period with the slow-release properties of the seed coating. Liquid insecticides mixed with liquid fertilizer and applied in the seed furrow at planting does not have any slow release properties. Heavy rainfall events in the 3-5 weeks between planting and rootworm hatch flush the liquid insecticide out of the root zone along with the starter fertilizer. Heavy rain fall events during May-early June is not an unusual event in NY. Highly variable efficacy of liquid insecticides applied at planting are more directly linked to the lack of a slow release formulation and being flushed out of the root zone than the efficacy of the insecticide against rootworm. In their best years in research plots, liquid insecticides were also challenged to suppress heavy populations of rootworm larvae. Liquid insecticides applied in-furrow with the starter fertilizer is not recommended for rootworm control in NYS.\nGranular Soil Insecticides:\nGranular soil insecticides applied at planting were the primary management strategy before the introduction of seed treatments and rootworm-active GMO trait in the corn. They remain a very effective tool to manage corn rootworm and were left behind due to the convenience of the newer technologies. For producers who have insecticide boxes for their planters, granular soil insecticides provide a more reliable management tool than either high-rates of seed treatments or liquid insecticide applied in-furrow with the liquid fertilizer. Each different granular insecticide has its strengths and weaknesses and I will try to summarize them below.\nForce 3G is a widely used soil insecticide in corn production. It performs best when soil moisture is adequate to in excess due to its low solubility in water. If a producer calibrates accurately, use rates can be reduced to 75% of the label rates for moderate rootworm populations. In dry years, even a full label rate faces challenges controlling rootworm populations due to the insecticide’s low solubility in water. Force is effective across all soil PH ranges.\nCounter 15G, 20G:\nCounter 15G, 20G is an effective soil insecticide against corn rootworm larval populations. This insecticide performed best in dry to moderately wet soils, but was challenged to perform adequately under conditions of excessive rainfall. Counter has a higher solubility in water than Force and effective across all soil PH ranges. With the introduction of ALS inhibitor herbicide, Counter was shown to have a serious interaction leading to plant injury. An introduction of a 20 CR (controlled release) granular was attempted with variable results. Due to the herbicide interactions, the use of Counter was significantly reduced by producers.\nLorsban 15G has been an effective soil insecticide against low to moderate populations of corn rootworm larvae. However, this insecticide is PH sensitive and cannot be used in soil PH above 7.8. In the high PH soils at the Cornell Musgrave Farm, Lorsban was deactivated by the high PH before rootworm hatch. In soils with PH of 7 or lower, this material can provide effective control against low to moderate rootworm population levels.\nIf producers are serious about growing non-GMO corn, they need to invest in granular insecticide boxes for their planter if they do not have them. Granular insecticides are the only reliable way to control corn rootworm larvae across corn rootworm population levels and weather conditions. Liquid insecticides in liquid fertilizer applied at planting demonstrates highly variable control and is a poor investment. High rate of seed treatments are effective against low to moderate population levels of rootworm larvae and are sensitive to soil moisture levels. Granular insecticides provide the most consistent control of corn rootworm. However, in most situations, the corn planter operator needs to have a Pesticide Applicator License."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:2e486317-6b63-4a0e-ad7a-e885471626c5>","<urn:uuid:494ab9f7-48ea-4982-83f1-3dbe24f7da1d>"],"error":null}
{"question":"How do sediment-related impacts differ between large rivers like the Mississippi and smaller stream systems in terms of their effects on aquatic habitats?","answer":"In large rivers like the Mississippi, sediment impacts lead to the filling of backwater marshes, floodplain lakes, and burial of mussel beds and deepwater pools. This results in the loss of aquatic plants due to turbid waters blocking light penetration and the erosion of islands that eliminate mast-producing forests. In smaller stream systems, excessive clay concentration can cause direct effects including mortality, reduced physiological function, and habitat alienation, as well as indirect effects such as decreased rates of growth, reproduction and recruitment linked to reduced food supply. Both systems experience significant habitat degradation, but the scale and specific manifestations of sediment impacts vary between large rivers and smaller streams.","context":["The most often heard claims in support of large scale hydroelectric development are: (1) hydropower generation is 'clean', (2) water flowing freely to the ocean is 'wasted', and (3) local residents (usually aboriginals) will benefit from the development. These three claims are critically examined using case histories from Canada and elsewhere in the world. The critique is based mainly on journal articles and books, material that is readily available to the public, and reveals that the three claims cannot be supported by fact. Nevertheless, large scale hydroelectric development continues on a worldwide basis. The public needs to be well informed about the environmental and social consequences of large scale hydroelectic development in order to narrow the gap between its wishes for environmental protection and what is really occurring.\nModifications of lower watersheds such as water abstraction, channel modification, land-use changes, nutrient enrichment, and toxic discharge can set off a cascade of events upstream that are often overlooked. This oversight is of particular concern since most rivers are altered by humans in their lower drainages and most published ecological investigations of lotic systems have focused on headwater streams. Factors contributing to ecological processes or biophysical legacies in upper watersheds often go unacknowledged because they occur at disparate geographic locations downstream (e.g., gravel mining, water abstraction, dams) with significant lag times. This paper considers examples of how alterations to streams and rivers in their lower reaches can produce biophysical legacies in upstream reaches on levels from genes to ecosystems. Examples include: 1) genetic- and species-level changes, such as reduced genetic flow and variation in isolated upstream populations; 2) populations-and community-level changes that occur when degraded downstream areas act as population \"sinks\" for \"source\" populations of native species upstream or, conversely, as \"source\" populations of exotic species that migrate upstream; and 3) ecosystem-and landscape-level changes (e.g., nutrient cycling, primary productivity, regional patterns of biodiversity) that can occur in headwater systems as a result of downstream habitat deterioration and hydrologic modifications. Finally, a case study from my own research illustrates the importance of careful consideration of downstream-upstream linkages in formulating research questions, designing experiments, making predictions, and interpreting results. The effects of dams and associated water abstraction in lowland streams of Puerto Rico has forced my colleagues and me to re-evaluate the results of ecological research that we have conducted in highland streams over the past decade and to redirect our research that we have conducted in highland streams over the past decade and to redirect our research to consider downstream-upstream linkages.\nPringle, Catherine, Inst. Of Ecology, Univ. of Georgia, Inst. Of Ecology, University of Georgia, Athens, GA, 30602-0000\nThe Mississippi River is the hardest working river in America: a central artery for commerce, a stormwater management system for the two-thirds of the nation, the central flyway for 40% of the nation's migratory waterfowl. Each of the river's distinct forms of habitat is disappearing: backwater marshes dominated by emergent plants are filling in or, alternatively, becoming open, lifeless turbid waters; floodplain lakes have filled with silt; aquatic plants are not replaced because perpetually turbid waters block light penetration; sediment buries mussel beds and deepwater pools' islands erode, eliminating mast-producing forests. High water tables undermine floodplain forests, which lack higher ground to replace themselves. Restrictions of fish movement by the dams makes the decline of habitat in a particular pool more significant by blocking fish access to habitats in another pool. These problems are exacerbated by current river uses, and by past and present land uses that have altered basinwide hydrology and accelerated the rate at which sediment enters the river. Sediments and nutrients enter the river at unsustainable rates due to past and present land use practices that increase erosion and eliminate wetlands and stream-side buffers. Commercial and recreational vessels resuspend sediments in the water column, blocking light penetration and contributing to the loss of backwaters. Even in its reduced for, the Upper Mississippi represents the last piece of Midwestern America's Great Rivers, supporting migrating waterfowl, endangered mussel species and the most ancient lineage of fish in North America. Whether this system continues to survive and flourish depends on whether dynamic river forces can be sufficiently restored to make the river system self-sustaining. Preserving and restoring the Upper Mississippi and Illinois rivers requires three types of actions: 1. Recreate dynamic river forces to achieve self-sustaining habitat restoration 2. Minimize the operational impacts of the navigation system 3. Achieve no net increase in sediment by 2010\nFaber, Scott, American Rivers\nexcerpts of executive summary used as abstract.","Effects on aquatic habitat and fish\nStudy unit, which have an associated effect on fish community composition effects of water quality and habitat on composition of fish communities in the upper colorado river basin —jeffrey r deacon and scott v mize natural and human factors in the upper colorado river basin result in differences in water quality and fish habitat. The effects of chanellization and channel restoration on aquatic habitat fish assemblages and aquatic habitat of chanellization and channel restoration on. The effects of the flood on aquatic habitats in the delta provided insight for interpreting fish community trends main channels, side channels, and backwaters became narrower, smaller, and less numerous after the flood, whereas sandbar occurrence increased. These threats impact mangroves, seagrass, saltmarsh and coastal lagoon communities which serve as refuge to marine species land clearing, dredging, agriculture, reclamation and estuarial developments as well as removal of riparian vegetation, increased sediments, nutrients and pollutants in riverbanks and beds and the removal of organic. Effects of water-level fluctuations on the fisheries of we were able to evaluate the effects of low water level on fish but assessing effects of water. Drought’s impact on fish and wildlife warm water temperatures also increase predation on juvenile salmon and steelhead by warm water fish species. This special issue of the journal water will be devoted to “the effects of aquatic habitat restoration or degradation on fish production” aquatic habitats worldwide are in a state of flux sea level is rising, the oceans are becoming more acidic and patterns of precipitation are changing dramatically. Potential effects of sediment on fish and their habitat it also refers to criteria and guidelines that were developed for the protection of aquatic resources from sediment.\nOther aquatic vertebrates which eat fishes (mainly reptiles, amphibians and birds) may also disappear image: preparing land for cultivation after the forest was burnt this practice in northeastern india leads to soil erosion and siltation in headwaters of major rivers, thus destroying fish habitats photo: waikhom vishwanath. Involving phytoplankton, zooplankton, insects, freshwater molluscs, and fish at each trophic level, excessive concentration of clay can cause direct effects (mortality, reduced physiological function, and habitat alienation) and indirect effects (decreased rates of growth, reproduction and recruitment) linked to reduced food supply. Effects of stream acidification and habitat on fish populations of a north american river barry p baldigo and gregory b lawrence us geological survey, water resources division, 425 jordan road, troy, ny 12180, usa. Jacob wittman will present his honors presentation on thursday (4/14) from 4 pm in 370 kottman hallhis presentation is effects of aquatic habitat degradation on hybridization between bluegill and green sunfish. Effects of multiple low-head dams on fish, macroinvertebrates, habitat abstract—we examined the effects of low-head dams on aquatic biota, habitat.\nFish assemblages, habitat conditions and changes in aquatic effects of livestock on fish assemblages can be exacerbated. Effects of suction dredge mining on oregon fishes and aquatic habitats, supplemental information oregon chapter american fisheries society march 2015. About half of fishes live in freshwater terrestrial to assess the effects of angling-induced and among separate bodies of terrestrial aquatic. 10 background of the study in new south wales, the department of primary industries (dpi) is responsible for execution of the fisheries management act 1994 as such, dpi functions to maintain and protect aquatic habitats through sound and effective habitat.\nIssue indicator #1 ‐ sediment effects on aquatic habitat and biota iron creek contains fish because the type of aquatic habitat required for. Aquatic habitats and species the blunt face and bluenose shiners and the effects of habitat changes on their concerns regarding fish and wildlife habitat.\nEffects on aquatic habitat and fish\nAquatic habitat is a unique investigation designed to study small fresh water fish including the medaka the effects of radiation and the biological.\n- Climate change is one of the most critical long-term threats to fish population and habitat population resilience aquatic habitat effects of fire on habitat.\n- Urban development results in multiple stressors that can degrade aquatic ecosystems by altering the hydrology, habitat, and chemistry of streams results of the usgs investigation of the effects of urbanization on stream ecosystems (euse) found that no single environmental factor was universally.\n- Effects of changing climate on aquatic habitat and connectivity ecology and evolution frog observation data were compiled by the us fish and.\n- Habitat management aquatic habitats if we want to catch fish and to enjoy our natural environment we must all look after our aquatic habitats.\nFish and wildlife response to farm bill conservation practices 83 effects of conservation practices on aquatic habitats aquatic habitat conditions. Scientific data leads the way the refuge, and its partners, assess, manage, and monitor aquatic habitat conditions and distribution of fish species, including biological characteristics develop management activities to protect and restore habitats and assess effects of implementing aquatic habitat management activities at the. The effect of fish and aquatic habitat complexity on amphibians the effects of fish on assemblages of effects of habitat complexity and predator identity on. Aquatic habitats and species effects on the aquatic and terrestrial populations that depend on them will be project impacts on essential fish habitat. Effects of multiple low-head dams on fish, macroinvertebrates, habitat, and water quality in the fox river, illinois can have dramatic effects on rivers and aquatic."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:7ee21f45-6b98-4bf3-8365-147c8f712521>","<urn:uuid:5361d42f-af05-4487-bef7-bd8004870068>"],"error":null}
{"question":"What differences exist between the archival preservation methods used for the Woman documentary and the Women's Labour History Project interviews?","answer":"The Woman documentary's content was preserved through multiple formats: a theatrical release, a book, and an immersive traveling exhibition, ensuring all 2000 interviewed women would be represented in some way. The Women's Labour History Project interviews, on the other hand, were preserved through a more traditional archival approach, with the original reel-to-reel audio recordings being copied to audiocassettes and later digitized to create .wav preservation copies. The project also maintained written transcripts and detailed finding aids in the Archives Reading Room.","context":["Directed by Anastasia Mikova and Yann Arthus-Bertrand, Woman is a remarkable documentary, which brings to the screen almost 2000 women, from 50 countries. It focuses on what it means to be a woman in today’s world by looking at various topics in women’s lives, both big and small. It was graded on Baselight by freelance colourist, Stéphane Azouze, who had worked with Mikova and Arthus-Bertrand as DIT (Digital Imaging Technician) on an earlier project called Human.\nHuman saw Mikova and Arthus-Bertrand interview men, women and children all over the world. “Men were really happy to share their story,” Mikova, who was then the first assistant to the director, recalled. “They were proud and quite at ease talking in front of the camera. But the women at first would be quite suspicious – why were we asking all these questions?\n“However, once in front of the camera, it was like they had been waiting for this moment their whole lives – things that they had kept inside for many years could finally be released. It was so powerful and so incredible.”\nThe two directors began working on the project four years ago, before the #metoo movement and the Weinstein affair and were not surprised by the events that unfolded.\n“I have been doing documentaries for many years and in some countries, 10 or 15 years ago, it would have been almost impossible to find a woman who would be able to share her story and intimate experience in front of a camera,” explained Mikova. “So, seeing more and more women willing and ready to be heard, was a big difference and we thought, ‘OK, the time has come, let’s create a window for them’. And that’s where this project began.”\nMikova and Arthus-Bertrand have worked together for 12 years, first on an ecological TV show for French television. Then she joined Arthus-Bertrand at his small, tightly-bound production company to create these epic documentaries. “A project like this takes three or four years,” Mikova said. “It’s a very long time, so it creates quite a strong relationship across the team.”\nHaving spent the first year in planning and preparation, over the next two years they shot almost 2000 interviews, in 80 sessions and 50 countries. Mikova did as many of the interviews as she could, but had a trusted team of five journalists, only women, to support her as well as several camerawomen and cameramen – one of which was Mikova’s husband, Dimitri Vershinin, who worked on Human and Woman and was already comfortable with the specific format of the interviews the two directors wanted to reproduce everywhere. They also worked with local journalists, again only women, in every country where the interviews took place to find the women to interview and prepare each shooting in advance.\n“Once you have 2000 interviews with women, many of whom have opened up for the first time in their lives, the challenge is to go deep into yourself to determine which ones you should use,” she said. “We would gather topics – like sexuality, motherhood, empowerment, education or work – then we would listen for hours and hours to make a selection on which we all agreed.\n“That first version was maybe eight or nine hours long!” Mikova recalled. “Starting from there, to get it down to one hour 45 minutes was really difficult, because we had to lose so many interviews that we really wanted to keep. It took us almost a year to come up with the final editing, which is long for a documentary. Thankfully, we were chosen to premiere the film at The Venice Film Festival, which gave us a deadline to work towards. If not, I’m afraid we’d still be editing the film today…!”\nThe editing process was made slightly easier in the knowledge that accompanying the film will be a book and an immersive exhibition that will travel the world. “All of the women will exist in some way in our project,” Mikova insisted.\nThe nature of the content means that the filming style is extremely simple and intimate. “We set up simple studios, sometimes in incredible locations like in the middle of the desert,” Mikova explained. “It was always the same thing: you see just a face, on a neutral background, with one light, all very minimalistic.”\nArmed with all this material, the directors turned to freelance colourist Stéphane Azouze. Azouze came to the Baselight seat via a very unusual route: as a camera operator and shading supervisor, also specialized in aerial photography. He first met Arthus-Bertrand when Yann needed helicopter shots for a feature project he was creating in 2006 – another two-year documentary project called Home – and became DIT.\nFor Human, the previous project from Mikova and Arthus-Bertrand, Azouze was colour and workflows supervisor, ensuring every operator stuck to the defined set-ups and protocols. He also worked on the dailies with colourist Gilles Granier at Technicolor. “Gilles is fascinating by his ability to artistically respond to creators demands, with great teaching skills,” Azouze said. “He gave me the bug. With my experimented shading operator’s eye I understand colour and contrast, so I had a great foundation to work on.”\nAzouze then went on to establish himself as a colourist, not least on television projects for Arthus- Bertrand.\nWoman was shot on a mixture of cameras: The Canon C300 was largely used for the interviews, with the RED Dragon and Helium, Sony F55-F65, and drone cameras when needed, for beauty shots.\nFor the interviews, the directors and Azouze set a clearly defined format, with a standard studio portrait background, the same light, and laser-measured distances between the lens and the subject. “But the results are uneven,” explained Azouze. “And that’s normal, because each of the operators has a different eye, and with only one source of light it was sometimes hard to take the person off the black backdrop.”\nTo put the interviews in context there were also sequences of the home and work environments of the women, but also more artistic sequences of women’s lives. They are seen working in hundreds at a clothing factory (DOP Daniel Meyer); dancing on a skyscraper by the vertical performers troop Bandaloop, filmed by James Adamson; scenes of joy among groups of young girls attending a music festival (DOP Bruno Cusa); and even underwater shots (DOP Denis Lagrange) with a Japanese free diver Sachiko Fukumoto. And there is also a sequence led by photographer Peter Lindbergh and filmed by French cinematographer Caroline Champetier, which featured women of all ages undressing in front of the camera, confronting us to real women’s bodies: young, old and some of whose bodies show the serious effects of illness. These intimate images were shot in the studio set-up: very bright and close to the backdrop.\n“There was no question of using make-up: we wanted the women to be as natural and realistic as possible,” said Azouze, “But, on the other hand, if they came to the interview with make-up on, we let them be.”\n“We wanted the colourist to see the natural beauty of all these women, and the natural aspect of who they are,” added Mikova. “We really wanted to portray the feeling that these were real women.”\nThe first challenge for the grade was to bring all the content together, and to accommodate some difficulties of the C300 on wide shots. “Some shots did not correspond to the pre-set we had agreed, which in turn meant that working in the specified Canon log colour space did not work and I had to choose another starting base by hand,” Azouze recalled.\nOne thing that did please Azouze was the ability to compensate for the C300 without generating a lot of noise. “It works very well on Baselight,” he said. “I was amazed: on some shots I usually have to do a noise reduction pass, I didn’t have to with Baselight.”\nThe nature of the documentary meant that there were few grading ‘tricks’, but it was necessary to add some interpretations into the grade. “Champetier / Lindbergh’s sequence is almost black and white,” said Azouze. “The woman who testifies immediately before it cries and turns red during the interview; she was very emotional. So, I animated with several keyframes to desaturate gradually to arrive on something that moved naturally into this black and white sequence, so there is not too much of a break.\n“Then I noticed that this black and white sequence came out looking green, despite there being no green in the image,” he continued. “This is a well-known problem of persistence of vision. You have a succession of coloured images (mainly faces in close ups) so your eyes accommodate to it. When you switch to black and white, it comes out green – it’s crazy!”\nAzouze completed the grade in one of the Baselight suites at Mikros Image in Paris. “We wanted a master for theatre, which led us to projection and Mikros.” The grade took 25 days in the summer of 2019, plus an extra block of five days to cover a complex end credits sequence with a montage of all 2000 faces.\n“I used Film Grade and Base Grade a lot, along with Texture Highlight and Texture Equaliser,” he said. “The tracking I absolutely love, and on the creativity and finesse level I feel I can go further with Baselight.”\nAnastasia Mikova took on responsibility for supervising the finish. They agreed the look together. “It came with the flow,” according to Azouze. “I made proposals at the very beginning, and she guided me on colours and on renderings of certain people. We ended up together for the final grade, but the base was largely there.\n“I feel like I’m going further on Woman with Baselight,” he added. “The look overall is quite desaturated, because Anastasia wanted something subtle – that we do not focus too much on the clothes, that the skins do not catch the eye too much. The stories are not always cheerful, so we tried to remain discreet and true.”\nWoman was premiered at the Venice Film Festival on 1 September 2019.","Title and statement of responsibility area\nWomen's labour history interview collection (Sara Diamond interviewer)\nGeneral material designation\n- Sound recording\n- Textual record\n- Records in electronic form (digitized)\nOther title information\nTitle statements of responsibility\n- Source of title proper: Title of the collection is based on the contents.\nLevel of description\nEdition statement of responsibility\nClass of material specific details area\nStatement of scale (cartographic)\nStatement of projection (cartographic)\nStatement of coordinates (cartographic)\nStatement of scale (architectural)\nIssuing jurisdiction and denomination (philatelic)\nDates of creation area\n1978-2016, predominant 1978-1980 (Creation)\n- Diamond, Sara\nPhysical description area\n32 cm of textual records\n43 sound recordings\nPublisher's series area\nTitle proper of publisher's series\nParallel titles of publisher's series\nOther title information of publisher's series\nStatement of responsibility relating to publisher's series\nNumbering within publisher's series\nNote on publisher's series\nArchival description area\nName of creator\nScope and content\nThe Women's Labour History Project documents the histories of women who were active in the trade union movement in British Columbia from 1890s onwards. The project was initiated by Sara Diamond, an undergraduate history student at SFU, who conducted the interviews. She received financial support from the British Columbia Summer Youth Employment Fund. Additional funding was received from many other sources, including The Canada Council, and the Federal Department of Human Resources. Diamond provides a description of her research methodology in a report included as Appendix A1, \"Women's Labour History Project\" (available in the hard-copy finding aid only).\nThe collection consists of 43 interviews conducted by Sara Diamond with women in the labour movement in British Columbia. The women discuss their childhoods, family lives, careers, social issues such as childcare and birth control, economic situations such as the depresssion and post-war employment, and the working conditions that led them to become union activists. A summary of each interview is provided in Appendix 1, \"Women's Labour History Project\" (available in hard-copy finding aid only).\nThe collection contains audio recordings and transcripts.\nImmediate source of acquisition\nThe audio interviews and transcripts were donated by Sara Diamond. Material in the appendices was added by the archivist. Note that one transcript (for the Pearl Wong Moreau interview, file F-67-1-0-0-25) was created in 2016 by a researcher (Robin Folvik) from the audio recording and donated to the Archives.\nThe arrangement of the material was provided by the archivist. Originally transcripts and the audio recordings were included in the same series; in September 2010, the archivist moved the audio recordings into a separate series and re-assigned item numbers.\nLanguage of material\nScript of material\nLocation of originals\nAvailability of other formats\nRestrictions on access\nThere are no restrictions on the materials. Access to the sound recordings is normally provided via the copies on cassette. For some interviews, a digital copy made be available; consult the reference archivist for more details.\nTerms governing use, reproduction, and publication\nLists of the audio recordings and transcripts are available. Summaries of the interviews and Diamond's report on the history of the project, a bibliography, and an article by Diamond on women in the labour movement are available in the hard copy version of the finding aid maintained in the Archives Reading Room.\nGenerated finding aid\nAll accessions have been processed as of September 2002. No further accruals are expected.\nThe original recordings were made on reel-to-reel audio tapes; for each interview, two additional copies were subsequently made on audiocassette. Audio cassettes were digitized to create .wav preservation copies.\nStandard number area\nPlace access points\nName access points\nGenre access points\nDescription record identifier\nRules or conventions\nLevel of detail\nDates of creation, revision and deletion\nFinding aid prepared by Frances Fournier, Enid Britt (August 1999).\nUpdated by Enid Britt (May 2006). Updated by Shyla Seller (August 2018)."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:bbb16210-211e-4529-a981-ef793febf87e>","<urn:uuid:94516c94-f34a-40a1-be00-60ffd9b85e85>"],"error":null}
{"question":"As someone looking into content analytics tools, could you explain how the data integration capabilities compare between marketing scorecards and Google Optimize when it comes to tracking performance metrics?","answer":"Marketing scorecards and Google Optimize have distinct approaches to data integration and metrics tracking. Marketing scorecards primarily rely on Google Analytics data to analyze top-performing content and track marketing KPIs, focusing on metrics like traffic, engagement, and effectiveness. On the other hand, Google Optimize offers more comprehensive integration capabilities, connecting with both Google Analytics and Google Ads to provide a complete view of website performance. It tracks specific metrics like conversion rates, click-through rates, bounce rates, and revenue. Google Optimize also allows for more sophisticated data utilization through features like audience targeting based on behavior and characteristics, while marketing scorecards focus on identifying common elements among successful content pieces to create guidelines for future content development.","context":["Content marketing takes a lot of careful planning and consideration. Not every piece will be a winner with your target audience. But every once in a while, a writer and production team spin a bit of certified gold, and create a piece that engages your audience, supports your marketing goals and furthers your brand’s mission.\nAnd a win like that feels just like this.\nBut what exactly is it about this piece that attracts and maintains the attention of readers? And what specific elements are the most helpful to the marketing success of your business?\nIt’s important that such a key piece of content isn’t a one-off, and that your brand is able to repeat this success, time and time again.\nHere’s where your marketing scorecard comes in.\nSocial Media Examiner Garrett Moon explained it’s important for marketers to analyze their latest, greatest pieces of content and use these insights to create a marketing scorecard. This scorecard can then be used to gauge the potential success of future pieces of content, and shape your content marketing work in a way that fosters continuous improvement. The final goal here is balance – you should be able to compare each piece of content to your scorecard, and check off all the boxes of your marketing KPIs included in the card.\nWhat is a marketing scorecard?\nA marketing scorecard helps you and your marketing team outline the specific elements and common denominators among your most successful pieces. This way, you can pave the way for future content to align with these same key performance indicators, thereby helping to ensure that it will resonate with your target audience and further your overall marketing goals.\nWhy is a marketing scorecard important?\nYour scorecard will be unique to your business because it will be based on the specific metrics that have worked for your content in the past. Having a scorecard means you don’t waste resources, time and effort creating content that might miss the mark with your customers, or doesn’t support the metrics that align with your marketing campaign goals.\nWhat’s more, over half of all marketers – 61 percent – have found that leveraging analytics is one of the best ways to boost campaign ROI. A proven strategy like this can have a significant impact: Jeff Goins, who first developed the concept of a marketing scorecard, included goals like increasing traffic, engagement and effectiveness. After putting the scorecard system in place, Goins and his team saw a 150 percent increase in traffic, and double the number of comments per post.\nTL;DR: Your competitors are likely using a scorecard or some other type of metric-based system to gauge their content, and the results of using one are hard to argue with.\nHow do I make my scorecard?\nThe elements included in your scorecard will be tailored to your brand. The first step here is to analyze your top performing pieces of content, based on data and insights from your Google Analytics account. No two scorecards are the same; goals and marketing KPIs change from company to company.\nTake a look at your top 10 posts from the previous 12 months. Do a little digging and take the time to identify the common elements or patterns within these pieces. As you read through your top 10, ask yourself questions like:\n- What kind of post is it? A list? A how-to piece? A topical piece based on current news, or something more evergreen?\n- What’s the word count? Does your audience appreciate longer pieces, or shorter content that’s easier to digest?\n- Does the post include or point to free or downloadable resources (templates, worksheets, checklists and other assets)?\n- Does it include or lead to other types of content, like videos or ebooks?\n- What specific value does the post provide to readers? It’s important to identify the element that they can’t get elsewhere, like a hot take on a certain study, thought leadership around a specific topic area or comprehensive event coverage.\nAnalyzing these factors is considerably telling. For instance, you may notice that how-to posts featuring videos work especially well for your business, or that longer pieces garner more page views and more time on the page.\nBut don’t stop there.\nNext, take these metrics or marketing KPIs and build them into your scorecard. Let’s take a look at some example criteria:\n- A relevant and engaging topic that aligns with your company’s offerings and your core audience’s interests.\n- Keyword optimization, including the keywords and long-tail phrases that your brand wants to rank for, as well as those that already have substantial traffic. You should also consider organic ranking difficulty and make sure you goals are attainable.\n- The inclusion of statistics and data points backed by trusted sources.\n- Comprehensive coverage that tells the whole story and provides value to readers. Examining and touching upon all areas of the topic is important – readers will notice when content seems one-sided or “spun.”\n- Actionable insights and a CTA that guides readers to the next best step. It’s important to leverage your blogs to lead prospects through the sales funnel, and one of the best ways to do this is by offering additional, and more in-depth assets on a particular topic. This allows readers to go further and explore more on their own.\n- The inclusion of graphical elements, like custom-designed graphics, gifs or an infographic.\nIt’s important to remember, though, that every business’ scorecard will be different. The elements that resonate and work well for one brand’s target audience may not land as well for another company’s audience.\nIn addition, keep in mind that your scorecard may be shorter and not include as many factors at first, particularly if you’re just starting out with a scorecard initiative. On the other hand, brands with more well-defined marketing strategies and mature campaigns can create a more comprehensive scorecard.\nOnce completed, you can use your scorecard to measure the effectiveness of the content you’re creating. Your ultimate goal here is to check off all the boxes included in your scorecard, resulting in a successful and balanced piece of content that’s interesting, encourages engagement and supports your brand. If a piece doesn’t hit all the marks, you and your team will have a good idea of where improvements are needed, and what will take a piece from just okay, to great.","With Google Optimize, website owners can create experiments that test different variations of their website’s content, such as headlines, images, and call-to-actions. They can also test different design elements, such as color schemes, font styles, and layout. These experiments can help website owners identify the best-performing variations and optimize their website for maximum user engagement and conversions.\nHere are some benefits of using Google Optimize:\n- Easy to use: Google Optimize is designed to be user-friendly and easy to use, even for those without coding knowledge. Website owners can create experiments and variations using a simple drag-and-drop interface, without needing to modify their website’s code.\n- Customization: Google Optimize allows website owners to create experiments and variations that are tailored to their specific needs and goals. They can test different variations of their website’s content and design, and optimize their website for maximum user engagement and conversions.\n- Integration: Google Optimize can be integrated with other Google services, such as Google Analytics and Google Ads, to provide a more comprehensive view of website performance and user behavior. This integration allows website owners to track key metrics, such as conversion rates and revenue, and make data-driven decisions.\n- Personalization: Google Optimize allows website owners to personalize their website’s content and design based on user behavior, such as location, device, and referral source. This can help website owners deliver a more relevant and personalized experience to their users, which can lead to higher engagement and conversions.\n- Free: Google Optimize is a free tool provided by Google, which makes it accessible to website owners of all sizes and budgets.\n- Experiment types: Google Optimize supports several types of experiments, including A/B testing, multivariate testing, redirect testing, and personalization. A/B testing compares two variations of a webpage against each other to determine which performs better, while multivariate testing compares multiple variations of different page elements to find the best combination. Redirect testing allows website owners to redirect users to different pages based on their behavior, while personalization allows website owners to show different content to different users based on their behavior and characteristics.\n- Targeting: Google Optimize allows website owners to target experiments to specific audiences based on a variety of criteria, such as location, device, referral source, and behavior. This allows website owners to test variations of their website’s content and design on the most relevant audiences, and get more accurate results.\n- Goals and metrics: Google Optimize allows website owners to set goals and track metrics to measure the success of their experiments. These goals and metrics can include conversion rates, click-through rates, bounce rates, revenue, and more. By tracking these metrics, website owners can make data-driven decisions and optimize their website for maximum performance.\n- Integration with Google Analytics and Google Ads: Google Optimize can be integrated with Google Analytics and Google Ads, allowing website owners to track key metrics and make more informed decisions. This integration also allows website owners to import audiences from Google Analytics and Google Ads, which can be used for targeting experiments in Google Optimize.\n- Experiment reporting: Google Optimize provides detailed reporting on experiment performance, including statistical significance, confidence levels, and performance metrics. This allows website owners to see which variations of their website’s content and design are performing best, and make informed decisions about future optimizations.\nOverall, Google Optimize is a powerful tool that allows website owners and marketers to conduct experiments and optimizations on their website, and make data-driven decisions based on user behavior and performance metrics. By using Google Optimize, website owners can improve user engagement, conversions, and overall website performance, and stay ahead of the competition."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:2b086caf-5020-4703-8aa7-0e996ffb0771>","<urn:uuid:999a7587-78a8-4a12-9cac-155fa3083739>"],"error":null}
{"question":"What are the financial challenges authors face from publisher contract terms, and how do international author organizations address unfair contracts?","answer":"Authors face severe financial challenges from publisher contract terms, including delayed royalty payments and reduced earnings through deep discount clauses that can cut royalties to pennies per copy sold. Publishers routinely use contract provisions to base royalties on net amounts rather than list price, then further reduce the royalty rate by up to two-thirds. To combat such practices, international organizations like the IFJ/EFJ have launched campaigns against unfair contracts, encouraging authors to reject right-grabbing contracts and advocating for collective agreements that protect authors' rights, including maintaining exclusive rights and requiring authorization for any modification of their work.","context":["From The Authors Guild:\nIn our last installment of the Fair Contract Initiative, we detailed how publishers’ outdated accounting practices consistently delay and minimize authors’ royalty payments. But that’s not the end of the story. In another common practice, publishers routinely use contract provisions to slash authors’ royalties to mere pennies per copy sold.\nStandard trade royalties are based on a percentage of the publisher’s list price. But publishers have come up with a variety of clever methods to base royalties on the much lower net amounts they actually receive from booksellers and wholesalers. Then they add insult to injury by cutting the royalty rate itself by as much as two-thirds. When an author gets paid on less than half the list price, that’s bad enough. When an author gets paid only one-third the normal rate on that reduced price, the word “pittance” seems appropriate.\nSo-called “deep discount” clauses let publishers offer titles to booksellers and wholesalers at big markdowns. They stipulate that a publisher’s sale at a discount of over 55%, for example (a number that appears to be the new standard), the author’s royalty suddenly drops from, say, 15% of list price to 15% of the far smaller amount the publisher actually receives. A standard deep discount clause looks something like this: “On copies of the Work sold by the Publisher at a discount of greater than 55% from the publisher’s retail price through channels outside of ordinary retail trade channels, the author will be paid a royalty of 15% of the Publisher’s net proceeds.” (Many smaller publishers, which pay royalties on net proceeds to begin with, often slash the royalty rate in half on discounts from 50–70%, and by 2/3 for greater discounts.) Thanks to that drop in royalty payments the publisher makes out like a—well, the word “bandit” springs to mind.\nIt seems fair that when a publisher sells a book at a deep discount, the author’s take might be reduced proportionally. But there’s no proportionality in many standard “deep discount” clauses.\n. . . .\nWe’ve seen these discount double-crosses applied for sales to book clubs and book fairs, for “special sales” in bulk outside the usual book trade, for large-print editions, for export editions. Let’s say the publisher sells our sample book in bulk for just $2.00. The discount double-crossed author would get one thin dime per copy, a royalty cut of an astounding 93%—even though the net to the publisher would decline by less than 33%.\n. . . .\nEven crazier, some reductions can apply even to direct sales from publishers to readers, despite the fact that the publisher gets to keep the share of the transaction that would normally go to a retailer or wholesaler. If anything, an author’s royalty rate on such direct sales should be higher than normal.\n. . . .\nThe documented decline in authors’ incomes stems in part from these unconscionable reductions in royalty payments. Unless publishers begin to see authors as partners rather than patsies, many authors will no longer be able to afford to deliver publishers the quality work the industry was built on.\nLink to the rest at The Authors Guild and thanks to Jacqueline for the tip.\nPG says some authors get excited when they see their books in Costco. Unfortunately, it’s almost certain that their Costco sales will fall under the deep discount royalty structure, generating only tiny royalties.\nThen, there are publishers who sell virtually everything at “deep discount” so the author never receives the royalty rates that are listed first and most prominently in their publishing contract.\nPG has mentioned this before, but perhaps it bears repeating. During PG’s legal career, he has helped clients with a wide range of business contracts, including agreements prepared by many of the largest and most successful companies in the world.\nStandard publishing contracts from large traditional publishers stand out in the constellation of business contracts for their one-sidedness and, in some cases, outright duplicity for anyone who fails to read them very carefully. The way that Randy Penguin and its cohorts write their standard contracts is not the way that Apple, Microsoft, Morgan Stanley, Bank of America, Disney, Intel, Hewlett-Packard, American Express, Merrill Lynch and similar entities write their contracts.\nPG doesn’t agree with many initiatives undertaken by the Authors Guild, but he’s pleased to see their latest efforts to shine a light on some of the most abusive contract provisions routinely employed by Big Publishing.\nHowever, the cynic in PG holds little hope that AG’s efforts will bring about any meaningful reform. Treating authors badly is too much a part of the corporate and cultural DNA of traditional publishing to change. These dinosaurs will die before they evolve.","Fair Contracts for Journalists\nFair Contracts for Journalists Campaign\nThe growing trend among media organisations to use right-grabbing contracts has become a matter of great concern for the International and the European Federation of Journalists (IFJ/EFJ).\nAs a result, the IFJ/EFJ have launched a European wide campaign against right-grabbing contracts and demanding fair payments to journalists.\nAuthors’ rights for all (2000)\nDroits d’auteur pour tous (2000)\nHow to enforce journalists’ authors’ rights:\nI. Inform journalists about their rights\nThe authors’ rights of journalists are protected by international treaties and national laws. These rights not only include the economic rights (the right to earn money from a creative work) of authors but also their moral rights. Moral rights are guaranteed in national laws.\nThe national legal frameworks in most European countries recognise unwaivable moral rights of authors. The exceptions are the UK, Ireland and the Netherlands where moral rights can be waived – and publishers therefore frequently demand this.\nMoral rights are defined in international treaties and national laws. Article 6b of the Berne Convention for the Protection of Literary and Artistic Works stipulates that: “…the author shall have the right to claim authorship of the work and to object to any distortion, mutilation or other modification of, or other derogatory action in relation to, the said work, which would be prejudicial to his honour or reputation.”\nII. Say “NO” to right-grabbing contracts, advocate collective agreements Journalists shall never sign right-grabbing contracts.\nJournalists who sign the right-grabbing contracts not only lose the right to use their works autonomously, but also the (moral) right to protect the integrity of their works. The IFJ/EFJ encourage Member Unions to advocate collective agreements or model contracts that include an authors’ rights clause. The clause shall stipulate that:\n1) All authors’ rights in the work shall remain with authors who will retain their exclusive rights. The licence granted to publish or broadcast the work will be limited to the first publication/broadcast only. Unless there is express written agreement to the contrary, the licence shall expire within a certain period as permitted by national law after the delivery date. The publisher/broadcaster shall not make the copies available without the permission of the author after the licence expires.\n2) Any modification of the work shall be subject to prior authorisation by the author.\n3) Publisher/broadcasting company agrees that the following credit line (name of the author, date) shall accompany every publication or broadcast of the material.\nWhat is a rights-grabbing contract?\nContrary to international and national laws, right-grabbing contracts often demand journalists to sign away their authors’ rights, including both economic and moral rights.\nThese contracts demand that, journalists shall:\n– assign to the publisher a worldwide, exclusive right to use, reproduce, display, modify and distribute his/her work in all types of platform, known or future;\n– allow the publisher to transfer his/her works to third parties without additional payment to the author and exploit his/her works in any way the publisher deems necessary.\nWhat do unfair contracts look like ?\nSee the sample contracts here:\nSample I is from an international news organisation based in the US.\nSample II is from a news agency in Madrid, Spain.\nSample III is from a news magazine in Madrid, Spain.\nSample IV is from a German publishing house.\nSample V is from a German scientific magazine.\nLAUNCH YOUR NATIONAL CAMPAIGN NOW!\nThe IFJ/EFJ are calling on member unions to launch their national campaigns against right-grabbing contracts and demand that responsive democracies treat journalism as a public good, defend the authors’ rights of journalists, and reward journalists fairly.\nTel:+32 2 235 22 16"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:e56fd422-3cab-449d-b8bc-ea7c1735edd3>","<urn:uuid:3a1daccc-5241-4945-893b-4a3029ee7277>"],"error":null}
{"question":"Could someone explain the key differences between scholarships designed for overcoming adversity in general and those specifically targeting Black nurses in terms of their eligibility requirements and application components?","answer":"Scholarships for overcoming adversity typically require essays about personal hardships and are open to all students, as seen in examples like the Nigel Blythe-Tinker Scholarship ($10,000) and Courage to Grow Scholarship ($750). In contrast, Black nurse scholarships have more specific requirements, including racial identity verification, nursing program enrollment, and often NBNA membership. They also typically require additional documentation such as official transcripts, professional headshots, and letters of recommendation from nursing schools. While adversity scholarships focus on past struggles, Black nursing scholarships emphasize both academic achievement and commitment to addressing healthcare disparities.","context":["Scholarships for Those Who’ve Overcome Adversity, Including Single Parents\nCollege costs can cause many potential students to think twice about pursuing a college education. Scholarships are a great and often overlooked, way to get financial assistance to attend the school of your dreams. Most students know about the standard scholarship opportunities that their high school offers, as well as the merit and honors scholarships that most colleges offer to newly accepted students. However, there are a ton of outside scholarships from organizations that many people don’t consider.\nMany of these lesser-known scholarships cover specific populations or have very specific criteria to meet. If you’ve had struggles in your life, such as being raised by a single parent, coming from a low-income family, had a parent pass away, or struggled with a serious illness during your schooling years, you may benefit from scholarships that are specifically for individuals that have overcome adversity.\nHere are a few examples to get you started along with some other places to look.\nScholarships for Overcoming Adversity\nFor those with an environmentally conscious interest, the Ted Rollins Eco Scholars program is a must see. This scholarship is for any students majoring in business, sustainability, or any related fields. Designed to help drive businesses towards becoming greener and increasing their return on investment for environmental projects, this rewards the recipient a $1000 scholarship twice yearly.\nThe Nigel Blythe – Tinker Scholarship is for students that have had hardships in their life, but have still achieved success. It’s designed to reward their efforts and provide further financial support. An essay is required, and the grand prize is a $10,000 scholarship.\nThe Courage to Grow Scholarship is designed to help students that might not otherwise be able to afford a college education a small hand-up to help offset their expenses. This is a $750 scholarship that is awarded based on a 250-word essay. You can apply for this directly at the link.\nThe Kristina Flores Overcoming all Odds Scholarship is for high school students that have faced extremely adverse conditions, making it difficult to complete high school or pursue secondary education. It also provides scholarships for trade schools, as well as traditional colleges and universities.\nThe Kopfler-Hermann Overcoming Adversity Scholarship is sponsored by Kopfler-Hermann law firm in Louisiana. As personal injury attorneys, they see firsthand the effect that accidents can have on someone’s life and want to give back by providing a $1000 scholarship for a student has overcome adversity.\nOther websites with scholarship opportunities\nSome websites offer lists of scholarships available for anyone that has experienced hardships and overcome adversity. Some of the more popular and known sites include FastWeb, Peterson’s, and Cappex. These sites are devoted to finding scholarship opportunities.\nOther sites that have lists of scholarships available include the following:\nTime.com has a list of the biggest and best scholarships for each school year and includes a specific page dedicated to those students that have struggled.\nPocketSense has a blog post on scholarship opportunities for students with disabilities, those facing hardships, and those struggling financially. Details are provided on each scholarship.\nCollegeScholarships.org is another website that has a page devoted specifically to students that face a medical condition. There are links to various organizations depending on the diagnosis that you or your family members have.\nFinancialAidFinder.com has a page with scholarships available for those that have experience hardships, as well, and includes scholarships for those that are relatives of victims of 9/11.\nThere are many opportunities available for those that need financial assistance for college, regardless of your situation.","Individuals of African American descent or Blacks living in the US who are enrolled or accepted in nursing programs are eligible for several funding opportunities to support their journey into the nursing field. Curated in this blog post are the best black nurse scholarships and if you fall into the criteria, then go ahead and apply to receive the awards.\nThere is a shortage of black nurses in the US, often referred to as the underrepresented group in the nursing profession. There are many reasons for this shortage and financial constraint is one of them and you already know, that nursing is an expensive field of study to delve into. So, it is not common to find people, blacks included, who couldn’t go to nursing school because they could not afford it.\nTo solve this issue and provide unrestricted access to nursing education, scholarships, and grants are provided by governments and other agencies which include hospitals and nursing organizations. These scholarships and grants, often termed “financial aid” do not have to be repaid, unlike a student loan.\nBut to ensure that the financial aid gets to the deserving students, there are set requirements and eligibility criteria that applicants must fulfill. There are also nursing scholarships that are directed towards a specific population like black nurse scholarships curated here and nursing scholarships for minorities which I published recently.\nAside from scholarships, there are also other ways you can go to nursing school for free, exploring other options will be beneficial to you as you will not have to worry so much about finance while studying.\nNow, about the shortage of African American nurses in the US and lack of finance being one of the causes, specific scholarships are offered to current and aspiring black nurses as a counter to this issue. Offering financial aid specifically for black nurses will remove the financial barrier that restricts them from going to nursing school and inevitably increase the number of blacks in the profession.\nIn addition to the black nurse scholarships curated here, you can also apply for government grants for nursing school and scholarships for minorities in healthcare as well as other scholarships whose requirements you meet and then direct the fund or award towards their nursing education. The more scholarships you can apply for, the better as you have a higher chance of receiving more funds. This way you will complete and earn your nursing degree with little to no student debt.\nTable of Contents\nBest Black Nurse Scholarships\nSuppose you are a Black or African-American aspiring to become a nurse or already in a nursing program. In that case, the scholarships curated here are specifically for you to apply for and receive funding to support your career. In as much as the scholarships are for black nursing students, each of them has varying requirements which you have to review carefully before applying.\nThe following are the black nurse scholarships:\n- Evelyn Paige Parker Scholarship Fund\n- Black Nurses Association of the Greater Washington, DC Area, Inc. Scholarships\n- Central Florida Black Nurses Association of Orlando, Inc. Scholarships\n- Kyanna Black Nurses Scholarship\n- The Colorado Council of Black Nurses (CCBN) Scholarship Program\n- Gateway to Success Minority Nursing Scholarship\n- NBNA Scholarship Program\n- Chapman Urban Scholars Program Fund\n- National Medical Fellowships Scholarships and Awards\n- The Cleveland Council of Black Nurses (CCBN) Nursing Scholarship Program\n- William K. Schubert Minority Nursing Scholarship\n- Marmie Garland Scholarship Fund\n1. Evelyn Paige Parker Scholarship Fund\nFirst on our list of best black nurse scholarships is the Evelyn Paige Parker Scholarship Fund established in 2004 by the Pittsburgh Black Nurses In Action to attract African-American nurses to the Pittsburgh area. The scholarship is awarded annually and the amount varies from year to year.\nTo be considered for the Evelyn Paige Parker Scholarship Fund, you must have completed, at least, one full year of nursing school or be enrolled in a nursing second-degree program, be in good academic standing at the time of application with at least a 3.0 GPA, and must currently be taking at least 6 credit hours per semester. The scholarship application is online.\n2. Black Nurses Association of the Greater Washington, DC Area, Inc. Scholarships\nThe above-named association provides four scholarships annually to African-American licensed nurses and student nurses who are residents of the District of Columbia and adjacent counties of the State of Maryland. Each of the scholarships has its specific requirements, application deadline, and reward amount.\n3. Central Florida Black Nurses Association of Orlando, Inc. Scholarships\nFor the past 40 years, the Central Florida Black Nurses Association of Orlando (CFBNA) has been offering scholarships to assist students pursuing a nursing degree. To apply, you must be a member of CFBNA, a resident of Orange, Seminole, Lake, or Osceola County, accepted or currently enrolled in an accredited nursing program, demonstrate financial need, a minimum of 2.5 GPA, and proof of participation in community activities.\nDocuments required for application include an essay, an official transcript from your current nursing school, two letters of recommendation, and a clear headshot photograph.\n4. Kyanna Black Nurses Scholarship\nAfrican American students enrolled in a nursing major at an accredited nursing school in the US can receive scholarships from the Kyanna Black Nurses Association. The scholarship is offered annually, you have to submit a recent transcript from your college, an essay, and two letters of recommendation as part of the application component.\nTo be eligible, you must be actively enrolled in an undergraduate or graduate nursing program, have a CGPA of 2.6 or higher, and your college should be in Jefferson County, Frankfort, Elizabethtown in KY, or Floyd and Clark counties in southern Indiana.\n5. The Colorado Council of Black Nurses (CCBN) Scholarship Program\nHere is another black nurses foundation that offers scholarships to African-American students in nursing programs. This scholarship program from CCBN is available to students in high school, technical school, and undergraduate programs appropriate to the nursing track.\nParticipants of the CCBN scholarship program must participate in the CCBN Scholarship Soiree, an annual scholarship award ceremony held annually by CCBN.\n6. Gateway to Success Minority Nursing Scholarship\nGateway to Success Minority Nursing Scholarship is funded by Clarkson College, a college with one of the best nursing programs in the region. The Gateway scholarship supports the cost of nursing at Clarkson College for select racial minority students each year. To be eligible, you must have a high school GPA of 2.5 on a scale of 4.0, meet the eligibility requirements for employment at Nebraska Medicine, and must complete the summer internship program at Nebraska Medicine.\nThe scholarship value is $14,000 per student for an academic year and is awarded for 4 years provided you maintain a 2.5 GPA for scholarship renewal. Applying for this scholarship also means that you intend to pursue a BSN degree at Clarkson College. Since blacks are part of the ethnic minority groups, this makes them eligible to apply for the Gateway to Success Minority Nursing Scholarship.\n7. NBNA Scholarship Program\nThe National Black Nurses Association (NBNA) Scholarship Program is the biggest and most popular nursing scholarship for African Americans. The scholarship program provides annual financial assistance to Black nurses at all levels (LPN to Doctorate). You must be a member of NBNA and be currently enrolled in a nursing program to be eligible. You must also be in good academic standing and have at least one full year of school remaining to qualify for the award.\nApplication materials include a resume or CV, an official transcript from an accredited nursing school, a 2-page personal statement or essay, two letters of recommendation, and a clear professional headshot. The scholarship value ranges from $1,000 to $15,000.\n8. Chapman Urban Scholars Program Fund\nThis scholarship was created to help underrepresented students in science or social service disciplines to follow their dreams. Blacks are one of the underrepresented groups and nursing is a science discipline that makes blacks in nursing programs eligible for the award but it will be a competitive one compared to the others on this list that are only meant for black nursing students.\nYou can apply for the scholarship while you are still in high school but you have to be a high school senior in Pittsburgh Public School or a Pittsburgh Public Charter School. However, if you were already an undergraduate at the time you’re seeing this, you can still apply provided you graduated from Pittsburgh Public School or a Pittsburgh public charter school.\nIn addition, you must have a minimum high school GPA of 2.7 as shown in your official high school transcripts, demonstrate community involvement, and provide evidence of acceptance into nursing school. The scholarship value is $5,000.\n9. National Medical Fellowships Scholarships and Awards\nAfrican-American/Black students who want to pursue nursing can also take advantage of the NMF scholarships to finance their nursing education.\nThe National Medical Fellowships offers various scholarships to promote diversity within the medical field including nursing. Therefore, the scholarships are targeted to those from minority ethnic groups enrolled in any healthcare program such as nursing at an accredited institution in the U.S. Then you can be eligible to apply for the National Medical Fellowships Scholarships and Awards.\nYou must demonstrate leadership abilities and be committed to serving medically underserved communities to be eligible. Other requirements for the scholarship include a student aid report, an official letter of financial award offer, a current professional school transcript, and other personal documents. Application deadlines and procedures can be found in the link below.\n10. The Cleveland Council of Black Nurses (CCBN) Nursing Scholarship Program\nCCBN offers annual scholarships to Black students currently enrolled or recently accepted in a nursing program at the undergraduate or graduate level and are in good academic standing. Requirements for the application include two letters of recommendation, an essay, an official transcript from nursing school or high school, and a letter from your Dean of Nursing stating the student is in good standing in the nursing program and requires an additional academic term to complete the nursing program.\n11. William K. Schubert Minority Nursing Scholarship\nThe William K. Schubert Minority Nursing Scholarship is offered by the Cincinnati Children’s Hospital Medical Center to increase the diversity of RNs in the hospital. If you are looking to become a pediatric nurse and you come from one of the racial-ethnic minority groups, you can qualify for this scholarship.\nTo be eligible, you must have enrolled or accepted into a prelicensure RN bachelor or master’s degree program at an accredited nursing school on a full-time or part-time basis. Applicants must also have and maintain a minimum of 2.75 GPA on a 4.0 scale. The scholarship award is $2,750 per year, the funds are sent directly to recipients’ schools to be used for tuition and fees only.\n12. Marmie Garland Scholarship Fund\nLast but not least is the Marmie Garland Scholarship Fund offered to African-American students with an interest in nursing. Applicants must demonstrate financial need, maintain a CGPA of 3.0, and exhibit leadership potential. The scholarship award is $3,000 and it is offered annually.\nTo recap, Blacks in the nursing profession are underrepresented and one of the major causes of this is financial constraint, no thanks to the cost of nursing schools in the US, this killing the dreams of African-Americans who wish to become nurses. And some of them do not even have access to student loans.\nTo counter this challenge, scholarships are provided by nursing associations across the country to help African Americans pay for all or part of their tuition so that they can go to nursing school and become the kind of nurse they have always dreamt of becoming.\nThis blog post gathers all the best black nurse scholarships and puts them under one umbrella so that interested students can have access to them at the same time without going through the stress of searching for them on the web, which can be a daunting task. You can apply for as many of the scholarship requirements and criteria you fulfill to increase your chances of winning at least one of the awards.\n- 15 Theology Scholarships for African Students\n- 17 Best Catholic Nursing Scholarships\n- 10 Best Scholarships for Nursing Students in Georgia\n- 12 Best Black Catholic Scholarships\n- 9 Scholarships for Black Students at PWI"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:482f86ab-cb53-4898-b49c-10037b18027e>","<urn:uuid:d2f173cb-c4c3-45af-b2b4-8212d3978bef>"],"error":null}
{"question":"What are the key structural differences between DNA and RNA molecules, particularly in terms of their helical arrangement and base composition?","answer":"DNA and RNA have several key structural differences. DNA is arranged as a double helix with two polynucleotide chains, while RNA exists as a single-stranded molecule. Regarding base composition, DNA contains the bases Adenine, Guanine, Thymine, and Cytosine, whereas RNA replaces Thymine with Uracil. Additionally, DNA uses deoxyribose as its sugar component, while RNA uses ribose. DNA's double helix structure includes ten base pairs per complete turn spanning 34 Angstrom units, while RNA's single strand can sometimes be duplicated but doesn't form this characteristic double helix.","context":["Introduction to DNA and RNA\n*Please note: you may not see animations, interactions or images that are potentially on this page because you have not allowed Flash to run on S-cool. To do this, click here.*\nIntroduction to DNA and RNA\nMany observations contributed to the evidence from which the structure of DNA was eventually deduced by Watson and Crick:\n1. Chemical analysis.\nAnalytical techniques of pure DNA revealed the basic constituent molecules, but did not show how they were joined together.\n2. Chargaff's work on base equivalence.\nChargaff analysed the DNA from a wide variety of organisms and found that the ratio between Purines and Pyrimidines was constant. He also showed that the ratios of Adenine to Thymine and of Guanine to Cytosine were consistent.\nLater this indicated the A-T and G-C bonding in DNA.\n3. Franklin and Wilkins' work on X-ray crystallography.\nDNA is a very ordered molecule and has a consistent symmetry. The technique of X-ray crystallography can reveal the pattern of such regular molecules. A beam of X-rays passing through the molecule is scattered by the atoms to give a distinctive X-ray diffraction pattern, which may be photographed and measured.\nRibonucleic acid (RNA) is also a polynucleotide. The chain of nucleotides is formed in exactly the same way as in DNA, but the molecule has some very important differences:\n- It is a single stranded molecule.\n- The pyrimidine Thymine never occurs but is always replaced by Uracil, another pyrimidine. (Think \"No cup of T for U!\")\n- It is much smaller than DNA.\n- It comes in three different forms, ribosomal, transfer and messenger.\nRibosomal RNA is 80% of the total RNA in a cell. It is involved with the formation of ribosomes and is therefore important as the site of protein synthesis in a cell.\nMessenger RNA is 3-5% of the total RNA in a cell, depending on the protein synthesis activity at the time. It forms in the nucleus and is used to communicate the genetic code in the allele to the ribosome during protein synthesis.\nTransfer RNA is a clover leaf shaped molecule and is up to 15% of the total RNA in the cell. It is involved in carrying the amino acids through the cytoplasm to their correct places in a growing polypeptide chain.\nThe DNA X-ray diffraction photographs showed the molecule was a double helix and also revealed the exact pitch of the helix, together with the layer distances. Each layer is of course one base pair.\nIn 1953, James Watson and Francis Crick, working at the Cavendish Institute in Cambridge put forward their model for the structure of DNA. This work was based on the previous research carried out by Rosalind Franklin and Maurice Wilkins into the X-ray diffraction patterns of crystalline DNA.\nDNA is a polymer of nucleotides.\nNucleotides are made up of:\n- a phosphate.\n- a sugar - deoxyribose.\n- a base - either adenine, guanine, thymine or cytosine.\nIn DNA the sugar is always the same but each nucleotide will have only of the four nitrogenous bases. The phosphate sugar and base are linked together:\nDNA is a macromolecule polymer made of subunits called nucleotides. The nucleotides are arranged in two chains which are coiled into a spiral shape called a double helix.\nThe nucleotides are linked together, the sugar of one with the phosphate of the next:\nDNA is the molecule from which the gene alleles on the chromosomes are made.\nAs with all nucleotides, those in DNA have three parts. These are a pentose sugar called deoxyribose, a phosphate group and a nitrogenous base.\nThe sugar and the phosphate are exactly the same in every nucleotide, but the base varies. There are four bases in DNA and each nucleotide contains one of them. The bases are called Adenine, Guanine, Thymine and Cytosine. (A,G,T and C for short).\nThe nucleotides are joined in a specific order. The order of the nucleotides means that the bases they contain are in a certain order, it is this order which forms the genetic code.\nLook at these diagrams showing the sugar and the phosphate. Note that the sugar has its carbon atoms numbered according to their positions in the molecule.\nThe sugar and phosphate join together to make the backbone of the DNA molecule. The 3' carbon on one sugar is joined to the 5' carbon on the next by means of a phosphate bridge, like this.\nDiagrammatically shown as:\nEach time the sugar joins to a phosphate, a molecule of water is eliminated in a condensation reaction.\nThis sugar-phosphate-sugar bond is called a phosphodiester bond.\nThe process repeats so that a very long chain of nucleotides is made, a polynucleotide. Note that the bases protrude from the side of the chain.\nThere will be a spare 5' sugar atom at one end of the chain and a spare 3' atom at the other. The chain thus has a 3' to 5' direction reading up the page.\nIn DNA a second polynucleotide chain forms next to the first, but this runs in the opposite direction. The chains are therefore described as antiparallel.\nThe bases now find themselves opposite one another and bond together with weak hydrogen bonds. When this occurs Adenine always pairs with Thymine (A-T) and Guanine with Cytosine (G-C). There is a good reason for this complementary pairing.\nAdenine and Guanine both have a double ring structure and are classified chemically as Purine bases.\nA purine molecule looks like this:\nThymine, Uracil and Cytosine all have a single ring structure and are classified as Pyrimidines.\nTheir molecules look like this:\nWhen the base pairs form, a consistent spacing is obtained between the polynucleotide chains.\nThe whole double chained molecule is formed into a double helix spiral, caused by the bond angles between each base pair. Each complete turn of the spiral includes ten base pairs. This takes up a distance of 34 Angstrom units.","RNA is the acronym for ribonucleic acid. It is a nucleic acid that is responsible for transferring genetic information from DNA in order to synthesize proteins according to the functions and characteristics indicated. RNA is present in the cytoplasm of eukaryotic and prokaryotic cells. Also, the RNA is composed of a simple chain that can sometimes be duplicated. It is made up of bonded nucleotides that form chains. Each nucleotide consists of: a sugar (ribose), a phosphate group and 4 nitrogen bases (adenine, guanine, uracil and cytosine).\nRNA carries the genetic information of DNA for the synthesis of the necessary proteins. That is, the RNA copies the information of each DNA gene and then goes to the cytoplasm, where it binds to the ribosome to direct protein synthesis.\nRNA begins to be studied in 1868 by Friedrich Miescher, also, he was the first person to investigate DNA and promote the study of nucleic acids. The international abbreviation is RNA for its acronym in English for ribonucleic acid. Click to see more definition.\nIn reference to the above, we can distinguish the interaction of various types of RNA in genetic expression, among which we have:\n- Messenger RNA(mRNA): known as coding RNA, it has the genetic code that determines the scheme of amino acids to form a protein;\n- RNA transfer(tRNA): is responsible for carrying amino acids to ribosomes in order to incorporate them into the process of protein synthesis, likewise, it is responsible for encoding the information that messenger RNA possesses to a protein sequence and, finally,\n- Ribosomal RNA(rRNA): it is part of the ribosomes and acts in the enzymatic activity, it is responsible for creating the peptide bonds between the amino acids of the polypeptide in the protein synthesis process.\nIt is also worth mentioning ribozyme, which is a type of RNA with a catalytic function capable of carrying out its self-duplication when there is an absence of proteins. This characteristic is of great importance, since it has to do with the hypothesis that RNA was one of the first forms of life, prior to DNA, and that it allowed the first cell to form, since it contains stored genetic information and can self-duplicate.\nRNA and DNA\nBetween ribonucleic acid (RNA) and deoxyribonucleic acid (DNA) there are differences in their structure and function. RNA groups its proteins in a single helix while DNA groups them in a double helix. The nucleotides that make up the RNA are made up of ribose, a phosphate group and four nitrogenous bases: adenine, guanine, cytosine and uracil.\nThe nucleotides that form the DNA, on the other hand, are made up of deoxyribose, a phosphate group and four nitrogen bases: adenine, guanine, cytosine and thymine, and it is always found in the nucleus. In reference to its functions, the DNA selects, stores and saves the genetic code, in turn, the RNA transmits the genetic code stored by DNA, that is, it serves as a messenger.\nU.S. Counties by State\n- Barbour County, Alabama\n- Bristol Bay Borough, Alaska\n- Coconino County, Arizona\n- Baxter County, Arkansas\n- Amador County, California\n- Arapahoe County, Colorado\n- Litchfield County, Connecticut\n- Sussex County, Delaware\n- Bay County, Florida\n- Bacon County, Georgia\n- Kalawao County, Hawaii\n- Bannock County, Idaho\n- Bond County, Illinois\n- Bartholomew County, Indiana\n- Allamakee County, Iowa\n- Atchison County, Kansas\n- Anderson County, Kentucky\n- Ascension Parish, Louisiana\n- Cumberland County, Maine\n- Baltimore County, Maryland\n- Bristol County, Massachusetts\n- Allegan County, Michigan\n- Becker County, Minnesota\n- Amite County, Mississippi\n- Atchison County, Missouri\n- Blaine County, Montana\n- Arthur County, Nebraska\n- Douglas County, Nevada\n- Cheshire County, New Hampshire\n- Burlington County, New Jersey\n- Chaves County, New Mexico\n- Bronx County, New York\n- Alleghany County, North Carolina\n- Benson County, North Dakota\n- Ashland County, Ohio\n- Atoka County, Oklahoma\n- Clackamas County, Oregon\n- Armstrong County, Pennsylvania\n- Newport County, Rhode Island\n- Allendale County, South Carolina\n- Bennett County, South Dakota\n- Benton County, Tennessee\n- Angelina County, Texas\n- Cache County, Utah\n- Caledonia County, Vermont\n- Alleghany County, Virginia\n- Benton County, Washington\n- Boone County, West Virginia\n- Barron County, Wisconsin\n- Campbell County, Wyoming\n- Meanings of Acronym EHE - Describing the meanings of the acronym “EHE” in a thousand words provides ample space to explore its diverse interpretations across various contexts. “EHE” can represent concepts ranging from education and health to technology and organizations. Let’s delve into these meanings...\n- Meanings of Acronym SM1 - The acronym “SM1” has several potential meanings and interpretations, depending on the context in which it is used. These interpretations span a variety of industries, fields, and domains, each contributing to the multifaceted meanings associated with this acronym. To provide...\n- Meanings of Acronym HUR - According to abbreviationfinder, the acronym “HUR” can have multiple meanings and interpretations across various fields, industries, and contexts. These meanings may vary depending on the specific domain in which the acronym is used. In this explanation, I will explore some...\n- Meanings of Acronym JPK - The acronym “JPK” can have various meanings and interpretations across different domains and contexts. According to abbreviationfinder, I will explore some of the possible meanings of “JPK” in different fields. Please note that these meanings might not be exhaustive or...\n- Meanings of Acronym PBJ - According to abbreviationfinder, the acronym “PBJ” can have various meanings and interpretations depending on the context in which it is used. Acronyms often represent organizations, technical terms, concepts, or phrases in a shortened form. Here, we will explore some of...\n- Meanings of Acronym DTO - The acronym “DTO” holds a range of meanings and interpretations across various contexts, reflecting its versatility and adaptability in different fields. Acronyms are concise tools that encapsulate complex ideas into a few letters. In the case of “DTO,” its meanings...\n- Meanings of Acronym FDJ - According to abbreviationfinder, the acronym “FDJ” is associated with several meanings depending on the context in which it is used. It’s important to note that acronyms can have different meanings in different industries, regions, or domains. Below, I’ll explore some...\n- Meanings of Acronym DM1 - According to abbreviationfinder, “DM1” does not have a widely recognized or established meaning on its own. However, I can speculate on potential meanings based on common uses of abbreviations and context. Keep in mind that meanings can vary depending on...\n- Creedmoor, North Carolina - According to toppharmacyschools, Creedmoor is a town located in Granville County, North Carolina. It is situated in the northeastern part of the state, roughly 40 miles northeast of Raleigh and 20 miles south of Durham. The town spans across 8.5...\n- Cramerton, North Carolina - According to toppharmacyschools, Cramerton, North Carolina is located in the heart of Gaston County, just outside of the larger city of Charlotte. The town is situated along the south bank of the Catawba River and is bordered by both South..."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:6e808594-1f1f-43a4-b2f2-8329d720235b>","<urn:uuid:98738f6f-5940-4fea-b16d-1bd9894193d3>"],"error":null}
{"question":"What were the differences in bird diversity between cattle-grazed and wildlife-grazed areas?","answer":"Bird diversity was significantly lower in cattle-grazed areas compared to areas grazed by wildlife and bison. This was demonstrated through both traditional bird population census methods and soundscape analysis measurements of biophony, indicating that cattle grazing has a negative effect on bird populations and habitat quality.","context":["Researchers walking to a bird-watching site\nat UNDERC. L-R: Jenny Lesko, Nick\nAnderson, Nick Kalejs, Jack McLaren\nNoise in nature\nOur lives are filled with noise like cars, music, and conversation. In comparison, Nature and the outdoors seem quiet.\nBut next time you take a stroll outside, ask yourself: is everything truly silent? You may surprise yourself when you slow down and truly listen to your surroundings. You may hear:\n- A robin singing loudly for its mate\n- A flock of sparrows chatting in a high pitch on a nearby wire\n- A flock of geese honking high overhead\n- And much more\nThe truth is that healthy Nature is full of noise. Birds, insects, mammals, and even fish make noise to communicate, establish territories, and find mates. Very little in nature is totally silent.\nThis noise you’re hearing has been given a technical term: the Soundscape. While knowledge of the soundscape’s existence is obvious, we are just now beginning to understand what it is, its significance, and how it’s exciting potential in a management and sustainability context.\nA Western Meadowlark making a racket\nSoundscapes as a tool for management, conservation, and sustainability\nSoundscapes may also have an applied use. For example, one area has more individuals of a noisemaking animal type (let’s say birds), and one area has fewer birds. We expect the soundscapes of the two areas to differ, because the area with the high bird density will be louder than the one with low bird density. By capturing and analyzing the sounds, we may be able to determine what the abundance, richness, and diversity of birds are in that area.\nThe soundscape could potentially tell us even more. In the previous example, perhaps the difference in the number of birds is due to differing habitat types. The habitat difference could be as radical as forests vs. grasslands, or it could be from something as subtle as what kind of large animals graze that area. Analyzing the soundscape could help managers parse out subtle yet important differences in habitat, particularly in regards to habitat degradation through human activity.\nPutting the Soundscape to the Scientific Test\nI designed and conducted a 10 week long project to test the soundscape as a method for monitoring bird communities and habitat health.\nThe Palouse Prairie in Northwest Montana was a perfect as a study site: nearly all noise in the landscape comes from birds, so it presents an easy, controlled environment for studying the soundscape.\nPalouse Prairie in spring\nIn addition, the Palouse Prairie has three distinct yet similar habitats:\n- Land grazed American Bison (Bison-Grazed)\n- Land grazed by cattle (Cattle-Grazed)\n- Land by small Wildlife (Wildlife-Grazed)\nI tested across these habitat types and to see if soundscape analysis would be an effective tool for evaluating habitat.\nI started my study by conducting a soundscape analysis. I utilized simple sound recording devices to record sound at pre-set intervals at pre-determined locations. These sound recordings were plugged into a computer to get raw data, called “soundscape metrics”, the most important of which is Biophony, or the “loudness” or “power” of sounds from biological sources.\nSimultaneously, I went into the field and conducted a classical bird population census. If there are similarities between the two data sets, then the hypothesis that soundscape analysis can be used to monitor bird populations is supported.\nSongmeter at study site\nResults: is soundscape analysis a game changer?\nMy results confirmed the hypothesis that soundscape analysis could be used as a tool to monitor bird diversity. In the graph below, the soundscape metric “biophony” has a fairly strong positive linear relationship with the diversity I actually found in the wild.\nFig 1. Biophony tracks bird diversity\nThe next graph below shows how bird diversity (measured using Shannon’s Diversity Index, a mathematical measure of diversity) was much lower in the cattle-grazed area than the wildlife- and bison-grazed areas. That means that cattle are having some kind of negative effect on the birds, and therefore the habitat.\nFig 2. Bird diversity in 3 habitat types\nAnother, similar graph shows that the soundscape metric Biophony reflects that same pattern.\nTherefore, the soundscape can be used to tell the difference between habitat types.\nFig 3. Biophony in 3 habitat types\nSoundscapes and Sustainability\nThe results of my project are significant for research, land management, and sustainability.\n- Research: It is now possible to use soundscape analysis to monitor bird communities. That is a big deal for bird population monitoring programs and ornithologists.\n- Land management: Soundscape analysis can be used to quickly and easily evaluate the quality of rangeland, helping managers determine how to best use the land for conservation and agriculture.\n- Sustainability: Endangered and threatened species call the grassland home. Soundscape analysis will help evaluate what habitat is important and deserves more protection.\nRachel Carson, in her famous book Silent Spring, was perhaps the first to use the soundscape in a sustainability context. She showed that people care enough about the soundscape to create policy change. My research furthered our understanding and practical use of the soundscape."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:79da85e7-8b89-4a63-a125-cc1f235282a0>"],"error":null}
{"question":"What are the experimental implications of the constant fraction model in bouncing objects, and how does this relate to Zeno's philosophical challenge to ancient Greek thought?","answer":"The constant fraction model in bouncing objects shows experimentally that each bounce decreases by the same fraction of the previous height, rather than by a constant amount. This has been verified by dropping balls from different heights, where they consistently lose the same proportion of height with each bounce. This experimental evidence connects to Zeno's broader philosophical challenge to ancient Greek beliefs about continuous motion. While Zeno argued against the idea that 'everything is in constant movement' through his paradoxes, the bouncing ball experiment provides a physical demonstration of how infinite subdivisions can occur within finite time periods, offering a practical context for examining these fundamental questions about the nature of motion and time.","context":["Experiment of the Month\nZeno's motion paradoxes all have to do with dividing time into ever smaller slices. We can experimentally realized such a sequence of ever-smaller time intervals by watching a bouncing ball.\nThe figure at the right represents a ball dropped from an initial height h0 . When it bounces it loses energy, so that the next bounce height, h1 , is smaller. The model used here is that the fraction of energy lost on each bounce is a constant, independent of energy.\nIn the figure, the ball is given a small horizontal velocity to the right, so that it bounces further to the right each time. The horizontal position axis acts as a time axis because the horizontal velocity remains constant.\nThe model implies that each successive bounce is less than the previous bounce by the same fraction. In the figure, each bounce is half as high as the previous bounce. We call the fraction remaining g , so that\nh1 = g h0 .\nIn classroom discussion, we consider another possibility; that the bounce decreases by a constant amount, rather than a constant fraction. This model readily predicts the number of bounces for the ball to come to rest. However, experiment (dropping the ball from several different heights) quickly shows that this model is not close to the ball's behavior. It is more accurate to say that the ball looses (for example) half of its height on the first bounce, no matter that the original height from which it was dropped.\nWith the constant fraction model, it seems that the ball will require an infinite number of bounces to come to rest. Intuition says that it will take an infinite amount of time for this to occur. Analysis however, shows that in this model, the ball can make an infinite number of bounces in a finite amount of time. The reason is that the time between bounces decreases as the bounce height decreases.\nThe idea that the ball executes an infinite number of bounces in a finite amount of time is perhaps the most interesting response to Zeno's paradoxes. We have experimental evidence that an infinite number of events can occur in a finite amount of time. Mathematically, we have use the fact that (for some cases) the sum of an infinite number of terms is a finite number.\nUsing the labels in the diagram, we show that the time from t=0 to t=t1 is given by\n(t1 - 0) = (8h1/g)(1/2)\nIn words, the bounce-to-bounce time is proportional to the square root of the bounce height.\nUsing the fact that h2 = g h1, we see that\n(t2 - t1) = (g8h1/g)(1/2) = (t1 - 0)(g)(1/2)\nAdding up all the bounce-to-bounce times (from n=0 to infinity) gives the total time to come to rest:\nWith a (g)(1/2) defined as b, this becomes\nand the sum has the result\nWe usually determine g by measuring the initial drop height and the first bounce height, h1 , so that the equation above is a natural way to express the time to come to rest. An alternative is to notice that the time between the first and the second bounces is just the first term in that expression, so that\nFrom this point of view, b is best measured by measuring the time for the second bounce and the first for the second bounce: b is the ratio of those times. In this way, the time to come to rest is predicted from the times for the first two bounces.\nIn a later exercise, we consider the decay of the motion of a pendulum. To a good approximation, the pendulum also loses a constant fraction of its amplitude with each swing. The difference is that the period of the pendulum does not go to zero as the amplitude decreases - it remains nearly constant. Because of this the model predicts an infinite numbers of swings occur before the pendulum comes to rest. This behavior manifests itself in the (approximately) exponential decay of the pendulum amplitude.","Zeno of Elea (c. 490 – c. 430 BC) is one of the most enigmatic pre-Socratic philosophers. Though none of his own works have survived, there are fragmentary mentions of his on the classics like Aristotle and Plato. He was a member of the Eleatic School and, according to Plato at least, aimed to reinfoced Parmenides’s arguments (Parmenides being the founder of the school). While we know very little of Zeno himself, other than some hearsay (Plato says that he was tall and handsome, for instance, and that he was Parmenides’ lover),1 we do have two very interesting paradoxes. I had written these two years back, but for some reason thought that they deserved to be together (as only one of them really gets enough traction, while the other is ignored). And they are also still incomplete, as two more paradoxes remain: one called the Millet Seed (which seems to be a variation of sorites paradox) and the paradox of place – as we have almost nothing to work with though, analysing these paradoxes would be a mere speculation and detraction. So what follows below are two variations of the famous paradoxes of Zeno: Achilles and the tortoise, and the arrow paradox.\nZeno’s paradox of motion – Achilles and the tortoise\nThe most famous of Zeno’s paradoxes, and also the one with amusing historical examples: Zeno’s paradox of motion. In one version of the paradox Zeno proposes that there is no such thing as motion. There are many variations, and Aristotle recounts four of them, though essentially one can call them variations of two paradoxes of motion. One concerning time and the other space. Let us focus on space and recount the Achilles and tortoise paradox:\nin a race the quickest runner can never overtake the slowest, since the pursuer must first reach the point whence the pursued started, so that the slower must always hold a lead”; and “the non-existence of motion on the ground that that which is in locomotion must arrive at the half-way stage before it arrives at the goal (Aristotle’s [amazon asin=0199540284&text=Physics], Book VI.9).\nThough intuitively illogical, there is some sense in this. Right?\nThe description is more complicated than the paradox actually is. Zeno’s point is simply that space is divisible, and because it is divisible one cannot reach a specific point in space when another has moved from that point further. Let us take an example:\nThe distance is 1000x (where x is the measure unit for distance – mile, meter, whatever).\nLet us also say that Achilles give 100x head start to the tortoise.\nWhenever Achilles reaches 100x in time t1, the tortoise would have moved further (for instance, 150x).\nThus, when Achilles reaches 150x in time t2, the tortoise would have moved even further to 175x.\nZeno’s point is that given these conditions, Achilles cannot catch up with the tortoise because space can be infinitely divided into smaller units still – where the tortoise will always be a fraction of space ahead.\nThere are by now numerous ‘solutions’ to this paradox, (some have written a 272 pages long [amazon text=book on Zeno’s paradoxes&asin=0452289173]). Then again, as Aristotle pointed out already, this is not really a paradox, but poor physics. Anyone with high-school level of physics will see the problem: both Achilles and tortoise will have stopped moving as such at some point in time.\nPerhaps by today’s standards we can say that this paradox is a challenge to conventional physics. But what if that is Zeno’s point with the paradox? – A challenge to all of Ancient Greek thought that everything is in motion, always – a challenge to Heraclitus ‘everything is flux’ view pointed out in the Ship of Theseus paradox (fragment DK B12). For this, it is best to look at Zeno’s point on motion in relation to time.\nZeno’s paradox of motion – The arrow\nSo let us focus on this other aspect of the paradox of motion in. With Achilles and the tortoise, Zeno’s paradox points towards motion being inconceivable due to infinite divisibility of space (Aristotle calls it “bisection”). Here, let’s refer to time. Zeno’s paradox is best explained through his example of a flying arrow. As Aristotle describes the paradox:\nif everything when it occupies an equal space is at rest, and if that which is in locomotion is always occupying such a space at any moment, the flying arrow is therefore motionless”, or simply “the flying arrow is at rest (Aristotle’s [amazon asin=0199540284&text=Physics], Book VI.9).\nSomething can fly, while being at rest – makes sense right? Aristotle certainly thought it didn’t. He has a quite simple answer to the paradox:\nThis is false, for time is not composed of indivisible moments (Aristotle’s [amazon asin=0199540284&text=Physics], Book VI.9).\nComplex explanation by Aristotle, so let us take another quote that explains this one [sidenote: a quote that explains a quote that explains a quote, etc. – does that count as infinite regress?]:\nSo while it is true to say that that which is in motion is at a moment not in motion and is opposite some particular thing, it cannot in a period of time be over against that which is at rest: for that would involve the conclusion that that which is in locomotion is at rest (Aristotle’s [amazon asin=0199540284&text=Physics], Book VI.8).\nThen again, this is not really an answer to the paradox, but a difference of opinion – a different view on cosmology. As pointed out in the example of Achilles and the tortoise, Zeno wanted to challenge the traditional view that everything is in constant movement. Positing, as Aristotle does (or later Bergson), that time cannot be thought of as being composed of ‘indivisibles’ [sidenote: what strange language English is – indivisibles], does not really help us with the paradox. Bergson thought of motion as we do in contemporary physics: motion must involve both time and space. Thus, to disprove motion as such, both paradoxes should involve both time and space (and not individually at each occasion).\nHaving said that, if we accept Zeno’s cosmology, even though physics would refute it, the paradox does not have a solution. What Zeno seems to suggest is this: imagine someone takes a picture of the arrow while it is flying and shows it to you. Will you agree that at that particular moment, in that instance of taking the picture, the arrow was at that space and did not move? If you agree, then you agree with Zeno; if not, you agree with Aristotle.\nDespite this, there have been a couple very humorous observations. There is a myth of Diogenes of Sinope (or rather, Diogenes the cynic – or should I say Diogenes the dog as that translates; Diogenes the dawg?). Upon hearing Zeno’s arguments, instead of getting into the discussion, he simply got up and walked. Now isn’t that the best solution to the paradox? Technically that’s cheating, but what else would you expect from Diogenes.\nOn the left there is another one, from Randall Munroe.\nHave a look at these two paradoxes as well:\n- Further applications of Zeno’s paradox: The Ross-Littlewood paradox\n- Further applications of Zeno’s paradox: Thomson’s lamp"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:af8bd26d-8ca2-4b3b-bcea-5a1dd053dc80>","<urn:uuid:d1f65c0d-d89f-4d5d-a98c-830f4c6c9853>"],"error":null}
{"question":"I'm concerned about aesthetics and practicality - what are the post-procedure eating restrictions and appearance considerations for both amalgam and composite fillings?","answer":"Amalgam fillings require waiting 2 hours after placement before eating to allow proper hardening, otherwise the filling may break. They also create a permanent darkening of nearby tissues and gums. In contrast, composite (tooth-colored) fillings can be used immediately after placement with no waiting period. While composites are much more aesthetically pleasing by matching tooth color, they may show some color change over time due to food and drinks. Both types require proper maintenance, but composite fillings provide superior aesthetic results despite being less durable.","context":["by Vu Le\nby Vu Le\nAlmost all FDA approved whitening products use hydrogen peroxide or carbamide peroxide. White strips will get easy cases about as well as a dentist strength product. Just like it's harder to bleach some hair platinum blonde than others, and it's harder to get some teeth whiter than others. You can generally try over the counter strips first.\nVu Le, DDS has been a proud Cigna in-network PPO provider since 2005. If your employer offers the Cigna Dental Preferred Provider Organization coverage, you will have lower out of pocket costs by choosing a provider like us. We encourage everyone we care about to choose a PPO plan over an HMO plan. Not only will you have a wider choice of providers, but you will tend to experience a better level of care.\nWhat is the Cigna DPPO?PPO stands for Preferred Provider Organization. Insurance companies like Cigna have to collect premiums from employers, then use that money to reimburse doctors for health care services. The remaining difference after administrative costs is their profit margin. In order to offer lower premiums, insurers negotiate a reduced fee schedule with doctors. Providers who accept these reduced fees get to join the PPO network. In exchange for accepting the lower rates of payment, doctors get increased access to patients. When a doctor performs a service on you, a claim is sent to Cigna. The insurer reviews the claim, then pays an in-network doctor a reduced PPO contract fee. The patient is responsible for the difference between the PPO contract fee and the amount paid by the insurance company. Many offices, including our own, collect a deductible and/or an estimated co-payment on the date of service. If the amount of insurance company payment is less than estimated, a supplemental bill is sent to the patient. If the insurance company pays more than expected, a refund check is sent to the patient instead. If an out-of-network dentist is chosen, then all of the same services can be performed, but there will be a higher out of pocket cost. This is because the doctor charges their usual, non-discounted fee. On the positive side, you can select any dentist, not just those who are signed up with the carrier. Services still have to meet the same minimum requirements to get insurance company payment. For example, you must have a significantly damaged tooth to have the insurance carrier pay for a crown. The main benefit of a PPO is that it gives you, the patient, a balance of cost reduction and access to a wide selection of providers. A less visible, but more important advantage to most PPO's is that it still leaves room for an honest dentist to make a living. Cigna offers one variation on the PPO, the EPO. The Exclusive Provider Organization is basically a PPO plan, only without the option of paying more to see an out of network dentist.\nWhy not an HMO?HMO stands for Health Maintenance Organization. In the dental field, it barely fits the definition of insurance. The doctor is given $1-3 per month for every patient assigned to them by the insurance company. When the patient comes in for a service, the only other payment the doctor receives is a deeply discounted co-payment. Checkups, x-rays and cleanings are routinely done without charge to the patient. Unlike a PPO, the HMO provider receives no other money from the insurance company, outside of the trivial monthly capitation checks. So for $12-36 per year, the doctor is supposed to provide two free cleanings and checkups. With a hygienist costing $50 per hour, the dental office loses money on every routine visit. The HMO dental office is effectively forced to find treatment in order to turn a profit, where a PPO office has a decent chance to break even. This is why corporately run HMO dental groups tend to have lower review scores than PPO providers. (read our reviews here) There are exceptions, but HMO patients have the odds inherently stacked against them.\nExperience QualityWe've worked very hard to provide a high level of service with friendly staff and the latest technology. Please contact us to schedule an appointment. If you are in open enrollment season, please contact your HR department to choose the PPO plan option.\nby Vu Le\nThe best dental material for a filling will always be the one you never had to use. ALL dental materials have pros and cons. Because the human mouth is a rough place to be, all dental materials eventually fail. This is why oral hygiene and diet are so important.\nOne dental material that has been around for centuries is amalgam, a mix of metals used to fill cavities in teeth. Today’s alloys are predominantly mercury and silver. There’s a lot of good things about amalgam. It’s very forgiving of technique, blood and saliva. Even the most mediocre practitioner can place a long lasting amalgam, though it still takes some skill to shape them well. When they are used appropriately and proficiently, silver fillings easily last decades--I have two fifteen year old silver fillings in my mouth as I write this. Many of my senior patients have fillings placed in their teens. You don’t need very fancy equipment. It’s very inexpensive to buy and fast to condense, which in turn makes fillings inexpensive. That makes it a natural fit for HMO, non-profit and government funded clinics. Amalgam is arguably still a good choice for cases where the patient cannot remain still for long periods of time (think cerebral palsy) and sedation is not an option.\nSo why did I stop placing amalgam over nine years ago? The easiest reason is consumer demand: silver is just plain ugly in the age of white composites and ceramics. Nobody wants it, and since we opened the practice in 2005, only one or two patients requested silver in order to save $20 on a filling. So it became economically foolish to maintain inventory on something nobody wanted. We are in “the OC”, after all. The second reason I stopped placing it was the lack of adhesion; silver fillings don’t really stick to teeth, whereas white fillings are bonded. This means when you chew side to side, white fillings provide slightly more support. We’ve seen lots of cusps fracture away from a large silver filling. When you have a large filling with thin walls of enamel on either side, those thin walls can break off. White fillings hold the tooth together a bit better, which results in a stronger tooth overall. At least half of the crowns I do today are from the overly large silver fillings of yesterday. The third reason I don’t like silver fillings is the darkening of nearby tissues. The metal ions from the silver filling leach into tooth and gums nearby, creating a permanent darkening. If a scrap of silver filling gets trapped in the gums, it makes a black tattoo. In the tooth, it often creates a dark discoloration that bleaching cannot resolve.\nMost dentists, including myself, place tooth colored composite resin as filling material. Compared silver amalgam, it’s much more technique sensitive. Even a drop of stray saliva or blood will cause a filling to fall out or fail prematurely. No composite has the compressive strength or wear resistance of amalgam. So while you could build an entire tooth out of amalgam in a pinch, you would never want to do so with composite resins. The average lifespan of a white filling is short--around 3 to 5 years per Delta Dental statistics. But composites are much more esthetic, more repairable, and as mentioned before they are bonded to the tooth with adhesive. This not only reinforces the tooth, it also allows for smaller, less invasive reduction of tooth structure, particularly on root surfaces, in between back teeth, and especially on front teeth. Repairing a chipped front tooth used to require a crown; now it can often be done with a simple composite bonding.\nOur policy on tooth restoration is to do the most conservative restoration possible, with increasing consideration towards durability. In practical terms, that means white fillings for small to medium sized cavities, and ceramic inlays, onlays and crowns to address larger issues. We do not recommend the removal of silver fillings to treat or prevent medical conditions, in accordance with California law. Of course, we do recommend the prompt replacement of ANY dental filling material if there is evidence of fracture, recurrent decay, or leakage. And if you’d like to make your silver-filled mouth beautiful again, we can do that, too.\nWhich brings me back to the point I made before. Every dental material has its place, both ugly old silver fillings and the newer white composite resins. More and more, we are placing even longer lasting dental ceramics. But all of them can eventually fail. This leads to a larger filling, inlay, onlay or crown. No matter how good our restorative options get, no matter how good your dentist is, the most effective dental material of them all may not be gold, silver, ceramic or composite. It just might be good old dental floss.","Once the tooth cavities are detected, they should be treated as soon as possible. The treatment for tooth cavities is done by filling. Filling is an implementation to regain the natural shape of tooth before decaying, through which the tooth cavity is filled with suitable materials to restore its function and aesthetics after the decay in the tooth is cleared away. While the filling material is being chosen, the position and function of the tooth are taken into account. Since the chewing function of the teeth is basically performed in molar teeth zone, these teeth are exposed to more pressure. For this reason, more durable filling material needs to be used when molar teeth are filled. For the incisor teeth, on the other hand, a filling material compatible with the natural color of teeth is preferred, so that the filling cannot be noticed as much as possible.\nAMALGAM DENTAL FILLINGS\nThe amalgam dental fillings are also known as silver fillings. Amalgam is obtained by mixing the silver, tin, and copper alloy with mercury. The amalgam fillings are being used for approximately 150 years.\nAdvantages of the Amalgam Dental Fillings\nThey are the longest lasting and cheapest filling material.\nTheir implementation takes less time compared to the other filling materials.\nDisadvantages of the Amalgam Dental Fillings\nThey are not aesthetically appealing.\nSome might be allergic to the mercury inside.\nThere are debates on the mercury inside the amalgam dental fillings. However, since the mercury combines with the other metals inside, its chemical structure changes and it becomes harmless.\nPoints to Take into Consideration in Amalgam Dental Fillings\nSince it takes approximately 2 hours for the amalgam filling to harden, eating should be avoided during the first 2 hours after filling. If something is eaten within this period, the filling may break due to the force it is exposed since it is not hardened enough yet. A corrosion layer forms on the amalgam in the course of time. In order to prevent this, the amalgam has to be waxed. However, the waxing operation can be performed 24 hours after filling. In case there are too many amalgam fillings in the mouth, sensitivity may occur due to electrification when there is a metal such asfork, inside the mouth. If it is certain that the sensitivity occurs due to this metal, the filling material may be replaced with a non-metal one.\nCOMPOSITE (TOOTH COLOR) DENTAL FILLINGS\nSince the composite dental fillings have the same color as the tooth, they are also called white dental fillings. Although they could be used for only the incisor teeth when they first developed, their resistance against the chewing force has been increased and the wearing rates have been decreased by the time, and they can be used in the rear molar teeth now.\nAdvantages of the Composite Dental Fillings\nThe greatest advantage of the composite dental fillings is that they are aesthetically appealing.\nThey can be waxed right after the filling is done.\nThey can be used not only for restoring the tooth decays but also for cosmetic operations by changing the shape and color of the teeth.\nDisadvantages of the Composite Dental Fillings\nThe composite dental fillings are applied layer by layer, and they are hardened with a special light. Therefore, their preparation takes more time and it is harder compared to the amalgam fillings.\nTheir price is a little bit higher than the amalgam fillings.\nAlthough the composite dental fillings have been made quite durable recently, they are not as strong as the amalgam dental fillings.\nA color change may be observed on teeth in long term.\nPoints to Be Considered\nBeing careful while biting with wide fillings will prolong the life of the filling.\nAlthough the composite fillings are waxed well, color change may be seen in the course of time due to food and drinks.\nLike the amalgam fillings, there is no limitation such as not eating during the first two hours after filling.\nFor the teeth that are restored due to abrasion, hard toothbrushes should not be used and the attention should be paid to the brushing method."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"future"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:a252bdf0-d27f-417f-a2da-1afb341aa7c8>","<urn:uuid:60d166b6-c03f-4a66-b98b-a0e0957e0c0f>"],"error":null}
{"question":"I'm researching Masonic history and symbolism for a school project. What is the historical significance of the York event in 926, and how does it relate to modern Masonic emblems and symbols?","answer":"The York event of 926 was a pivotal moment in Masonic history when Edwin, son of King Athelstan, called a General Assembly of Freemasons at York. This gathering revived the institution and established a new code of laws for the Craft. This historical event has been widely accepted by both Operative Freemasons before 1717 and modern Speculative Freemasons as a factual occurrence. The significance of York is reflected in how Masonic emblems and symbols are used today. For instance, the Square and Compass, which are universally recognized as emblems of Craft Masonry, represent the moral teachings and spiritual measurements that were likely formalized during this period. These symbols, individually and together, convey powerful lessons that form the basis of Masonry - the Square represents morality and truth, while the Compass teaches Masons to measure their conduct and desires.","context":["Legends and Myths\nThe difference between legends and myths involves two dimensions: the degree to which the narratives in questions are sacred, that is, emotionally important and intensely real to the tellers; the degree to which they are grounded in historical, geographical, and scientifically plausible (at least at the moment when the assessment is made) reality. What separates these two genres is the extent to which the tellers themselves assess the tales in question as real beyond any question and culturally important, not whether they are any more or less objectively real.\nThat is why myths tend to emphasize the actions and include narratives about how the human society and other important phenomena began and their interactions with mortals.\nLegends by definition focus primarily on an historical ambiance. Not that every important character in a legend is a real historical figure whose objective existence can be documented; far from it. But the folks that populate legends operate for the most part in a recognizable historical and temporal framework rather than in the realm of once upon a time or the dream-like world inhabited by gods and goddesses and, with some major exceptions, are generally subject to the same constraints that affect all life forms on this planet.\nIn short, the Craft narratives clearly belong to the genre of legend, regardless of whether there are any objectively documented, historical prototypes of Freemasons.\nThe legend of Lamech’s Sons and the Pillars\nThe traditional history of Masonry now begins, in the legend of the Craft, with an account of the three sons of Lamech, to whom is credited the discovery of all sciences. But the most interesting part of the legend is that in which the story is told of two pillars erected by these sons, and on which they had inscribed the discoveries they had made, so that after the threatened destruction of the world of knowledge which they had gained might be handed on to the human race after the Flood. (Josephus, Antiquities of the Jews, B. I., II, Whiston’s translation).\nThe legend of Hermes\nThe next part of the legend of the Craft which claims our attention is that which relates to Hermes, who is said to have found one of the pillars erected by the sons of Lamech, and to have given to mankind the sciences written on it. This story may, for distinction, be called The legend of Hermes. As to Hermes, the legend is not altogether without some historical support, although the story is in the legend mythical, but of that kind which belongs to historical myth. He was said to be the son of Taut or Thoth, whom the Egyptians made a God and placed his image beside those of Osiris and Isis. To him they credited the invention of letters, as well as of all the sciences, and they esteemed him as the founder of their religious rites.\nThe Tower of Babel\nUnlike the legend of Hermes, the story of the Tower of Babel appears in the Halliwell poem, which shows that the legend was the common property of the various writers of these old manuscripts. In the second of the two poems, which as we have seen are united in one manuscript, the legend of Babel, or Babylon, is given.\nThe legend of Nimrod\nThe universal sentiment of the Freemasons of the present day is to confer upon Solomon, the King of Israel, the honor of being their first Grand Master. But the legend of the Craft had long before, though there was a tradition of the Temple in existence, given, at least by suggestion, that title to Nimrod, the King of Babylonia and Assyria. It had credited the first organization of the fraternity of craftsmen to him, in saying that he gave a charge to the workmen whom he sent to assist the King of Nineveh in building his cities.\nThat is to say, he framed for them a Constitution, and, in the words of the legend, this was the first tyme that ever Masons had any charge of his science. It was the first time that the Craft was organized into a fraternity working under a Constitution of body of laws. As Nimrod was the autocratic maker of these laws, it necessarily resulted that their first legislator, creating laws with his unlimited and absolute governing power, was also their first Grand Master.\nThe legend of Euclid\nHaving settled the foundation of Freemasonry in Babylon, the legend of the Craft next proceeds by a quick change to tell the history of its movement into Egypt. This Egyptian account, which in reference to the principal action in it has been called the legend of Euclid, is found in all the old manuscripts. This legend is the opening feature of the Halliwell poem, being in that document the beginning of the history of Masonry; it is told with very much detail in the Cooke MS., and is apparently copied from that into all the later manuscripts, where the important particulars are about the same, although we find a few things told in some which are left out of others.\nThe legend of the Temple\nFrom this account of the exploits of Abraham and his pupil Euclid, and of the invention of Geometry, or Freemasonry in Egypt, the legend of the Craft proceeds, by a rapid stride, to the story of the introduction of the art into Judea, or as it is called in all of them, the land of the behest, or the land of promise. Here it is said to have been principally used by King Solomon in the construction of the Temple at Jerusalem. The general details connected with the building of this edifice, and the help given to the King of Israel, by Hiram, King of Tyre, are told either directly or at second hand, through the Polychronicon, from the first Book of Kings, which, in fact, is referred to in all the manuscripts as a source of information. (As it is said in the Bible, in the third book of Kings, are the words of the Cooke MS. In the arrangement of Scripture as then used, the two books of Samuel were called the first and second of Kings. The third book of Kings was then our first, according to the present practice.)\nThe extension of the Craft into other Countries\nThe legend of the Craft next proceeds to tell us how Freemasonry went into divers countryes, some of the Masons traveling to increase their own knowledge of their art, and others to use elsewhere abroad that which they already possessed. But this subject is very briefly treated in the different manuscripts.\nThe Halliwell poem says nothing of the progressive march of Freemasonry, except that it details almost as an actual event the ill-use of the Four Crowned Martyrs (Quatuor Coronati) as Christian Freemasons, in the reign of the Roman Emperor Diocletian, and we should almost be led to believe from the course of the poem that Freemasonry went directly into England from Egypt. The Cooke MS. simply says that from Egypt, Freemasonry went from land to land and from kingdom to kingdom until it got to England.\nWe find the later manuscripts are a little more definite, although still brief. They merely tell us that many skilled craftsmen traveled into various countries, some that they might acquire more knowledge and skill, and others to teach those who had but little skill. Certainly there is nothing that is myth or fable in this account. Every authentic history of architecture agrees in the claim that at an early period the various countries of Europe were traveled by bodies of builders in search of work in the building of religious and other edifices. The name indeed of Traveling Freemasons, which was given to them, is familiar in architectural history books.\nSufficient for the present, for us to show that in this part, as elsewhere, the legend of the Craft is not a mere fiction, but that the general statement of the spread of Freemasonry through-out Europe at an early period is proved by historical evidence. When we examine the legend of the Craft, it will be found to trace the growth of Freemasonry through its several stages of progress from Babylon and Assyria to Egypt, from Egypt to Judea, from Judea to France, and from France to England. Accepting Freemasonry and the early art of building as meaning the same thing, this line of progress will not be very different, with some necessary variations, to that assumed to be correct by writers on architecture. But the study of this subject belongs not to that which went before, but to the historical period of the Society, that is based on the evidence of fully accredited records.\nThe legend of Charles Martel and Namus Grecus\nThe legend now gets near the field of authentic history, but still having its traditional character, goes on to tell, but in a very few words, of the entry of Masonry into France. We have this account given in the language of the Dowland MS. Now, this legend is repeated, almost word for word, in all the later manuscripts right up to recent times. But it is not even mentioned in the earliest of the manuscripts – the Halliwell poem – and this proves again that the two sets of recorded events and traditions are copied from quite different sources. This whole subject is so closely connected with the authentic history of Masonry, having really passed out of the pre-historic period, that it claims a future and more detailed study in its proper place.\nThe legend of St. Alban\nThe legend of the Craft now goes on to tell of the history of the bringing of Freemasonry into England, in the time of St. Alban, who lived in the 3rd century. The legend referring to the first martyr of England is not mentioned in the Halliwell poem, but it is first found in the Cooke MS., in the following words: And sone after that come seynt Adhabell into England, and he convertyd seynt Albon to cristendome. And seynt Albon lovyd well masons, and he gaf hem fyrst her charges and maners fyrst in Englond. And he ordeyned convenyent to pay for their travayle.\nLater manuscripts, for some time, say nothing of St. Adhabell. When we get to the Krause MS. in the beginning of the 18th century, we find mention of St. Amphibalus, who is said in that document to have been the teacher of St. Alban. But this St. Amphibalus, of which the Adhabell of the Cooke MS. is seemingly in error in spelling, is so doubtful a person, that we may rejoice that the later copyists have not as a rule thought proper to follow the Cooke document and give him a place in the legend. However, the name is not entirely mythical as we find it in the writings of Robert of Monmouth, 1140, as well as, for example, in the William Watson MS., 1687.\nA very interesting point of the legend of the Craft to which our attention may be directed, is that referring to the organization of Freemasonry at the city of York in the 10th century. This part of the legend is of much importance. The prehistorical here verges so closely upon the historical period, that the true account of the rise and progress of Freemasonry can not be justly understood until each of these elements has been carefully attached to the proper period. This subject will therefore get critical attention.\nThe legend of York\nThe decline and decay of all architectural art and enterprise having lasted for so long a period of time in Britain, the legend of the Craft proceeds to account for its revival in the 10th century and in the reign of Athelstan. His son Edwin called a meeting, or General Assembly, of the Freemasons of York in the year 926, and there revived the institution, giving to the Craft a new code of laws. Now it is impossible to attach to this portion of the legend, absolutely and without any reservation, the taint of fiction. The gathering of the Craft of England at the city of York, in the year 926, has been accepted by both the Operative Freemasons who preceded the Revival in 1717, and by the Speculatives who succeeded them, up to the present day, as a historical fact that did not admit of dispute. The two classes of the legends – the one represented by the Halliwell poem. and the other by the later manuscripts – agree in giving the same statement. The Cooke MS., which holds a middle place between the two, also contains it. But the Halliwell and the Cooke MSS., which are of older date, give more fully the details of what may be called this revival of English Freemasonry. Thoroughly to understand the subject, it will be necessary to compare the three accounts given in the several sets of manuscripts.\nDouglas Koop and J. P. Jones, the Genesis of Freemasonry, London 1978\nAlber G. Mackey, Encyclopedia of Freemasonry, Chicago 1950-1966\nPaul Naudon, Les Origines religieuses et corporatives de la Franc-Maçonnerie, Paris 1972-1979\nJean-Pierre Bayard, Le Compagnonnage en France, Paris 1982\nKeith B. Jackson, Beyond the Craft, Shepperton 1980-1982\nTravaux de la Loge nationale de recherches Villard de Honnecourt, G.L.N.F.\nthe records of the Vialardi di Sandigliano Foundation Museum and Center for History and Humanities","Masonic Emblems & Symbols ~ Definitions and Differences\nWhen speaking of Emblems and Symbols within Masonic and Freemasonic Bodies, it is important to remember the difference between the two words. While any Emblem may be said to be a Symbol, or symbolic of a notion or idea; all Symbols are not Emblems. More specifically, an Emblem is- more or less- a logo which represents “something”, such as an appendant body or an organization. It then follows that a Symbol might be recognized as an Emblem, but more often than not, a Symbol will be used to convey an idea or a frame of reference to an idea.\nConsider the Square and Compass.\nWhen placed together, the Square and Compass are universally recognized as the emblems of Craft Masonry. But they are also, in and of themselves, symbols used to convey powerful lessons which form the basis of Masonry. However it works out, Masonic writer and historians have used the terms interchangeably for a good while, and for the most part, things work out in spite of the ambiguity which sometimes arises because of that use or miss-use.\nConsider the Square\nA carpenter might utilize a square to create lines that are true to each other and which he is able to duplicate throughout a project, always rendering the same true and square perspective. Masonically speaking, the square is similar in that it provides an ever present basis for morality, which should always be square and true with that of our Brethren and fellow beings.\nThe Compass, without the Square\nThe compass- within the building and navigational trades, is an instrument that can be used to trace constant lines, and to measure distance. Masonically, the compass accomplishes nearly the same feat, albeit in a much more spiritual manner. Masons are taught to measure their desires and fervor, as passion, by the always true and predictable compass. In this way, exact boundaries are measured for a Mason’s conduct within and without Lodge.\nWhile this holds true for the Square and Compass, it might not always be the case for others within the Fraternity. There are many other symbols within Craft Masonry that one will learn of as he progresses through the degrees. As such, it is not appropriate to visit all of them in this forum. However, let us briefly discuss some of the Emblems representing different bodies and aspects of the Freemasonic family. These are commonly known, which are above or beyond the first 3 degrees of Craft Masonry, as Appendant Bodies.\nSquare and Compass- These Symbols together constitute the Emblem of Craft Masonry.\nEmblem of the Scottish Rite (Northern Jurisdiction U.S.A.)\nA member of the Scottish Rite seeks to:\n- Exault the dignity of every person, the human side of his daily activities, and the maximum service to humanity.\n- Aid mankind’s search in God’s universe for identity, for development and for destiny, and thereby produce better men in a better world, happier men in a happier world and wiser men in a wiser world.\nThe Scottish Rite is one of the appendant bodies of Freemasonry that a Master Mason may join for further exposure to the principles of Freemasonry.\nAttainment of the third Masonic degree that of a Master Mason, represents the attainment of the highest rank in all of Masonry. Any Master Mason stands as an equal before every other Master Mason, regardless of position, class, or other degrees.\nAdditional degrees are sometimes referred to as appendant degrees, even where the degree numbering might imply a hierarchy. Appendant degrees represent a lateral movement in Masonic Education rather than an upward movement. These are not degrees of rank, but rather degrees of instruction.\nShriners International is a fraternity based on fun, fellowship and the Masonic principles of brotherly love, relief and truth. There are approximately 375,000 members from 191 temples (chapters) in the U.S., Canada, Mexico and the Republic of Panama.\nShriners International support Shriners Hospitals for Children, a one-of-a-kind international health care system of 22 hospitals dedicated to improving the lives of children by providing specialty pediatric care, innovative research and outstanding teaching programs. Since 1922, Shriners Hospitals for Children have significantly improved the lives of more than 865,000 children. The Shrine was originally founded in New York City in 1872.\nEmblem of York Rite Masonry\nThe York Rite is a general term used to describe three appendant organizations within U.S. Freemasonry – Royal Arch Masons, Cryptic Masons, and the Knights Templar.\nIn order to join these additional Masonic bodies, you must first become a Freemason.\nThe man who comes to the Grotto had his Masonic origin in a Symbolic Lodge. Where he was taught to revere the name of God and pray for guidance. He came on through the degrees where his knowledge of the dignity of Masonry was expanded and the Grotto was provided for him as a place where he might join with his brothers in letting human instincts for fun, pleasure and amusement prevail.\nThe Grotto was not and is not a place for Masons to exercise practices that would not be tolerated in the Masonic bodies, or which are taboo among gentleman. The Grotto stands for all of Masonry and wholesomeness in life. It stands for letting in the sunshine on discouragement, grief and woe and it stands for substituting hope for despair at all times.\nWithout the formalities that attend the Degrees in Masonry, it brings members in close fraternal touch and it breeds confidence among wearers of the Fez. The Grotto has always been made up of men from all walks of life …men you are proud to know …men who travel by preference along the sunny side of the street and who play a fair game all the time.\nMembership in the Grotto gives a man the right to wear its emblem and to participate in its good work, as well as enjoy all its pleasures. For more information click on the link below.\nFive Pointed Star- The Emblem of the Order of the Eastern Star\nWhat our Beautiful Star Represents!\nThe 5 points of the star refer to stories which are inspired by biblical heroines in the Bible.Each biblical figure is represented by a color on each of the star’s 5 points.\nBiblical Heroines Reflected As Eastern Star Symbols\nAdah,(Blue), Jephthah’s daughter, (from the Book of Judges)\nSymbol: A sword and shield, symbolizing how she sacrificed her life to save her father’s honor.\nSymbol: A sheaf of barley (grain), representative of Ruth, the poor widow in the Bible, who gathered left over barley stalks as her means of survival.\nEsther, (White), the wife\nSymbol: A crown and scepter. Queen Esther was a great and noble spirited biblical queen, known for her willingness to sacrifice her life for her people.\nMartha, (Green), Lazarus’s sister, (from the Gospel of John)\nSymbol: The broken column, symbolic of the uncertainties in life.\nElecta,(Red), the mother, (the “elect lady”, friend of St. John, from II John)\nSymbol: The cup, symbolic of charity and hospitality. Her color, red, is symbolic of love. She accepted God’s will in spite of persecution.\nInside the center of the star, a pentagram (5-sided figure) with an altar is the logo’s focal point. The open book upon the altar signifies obedience to God’s word.\nEach of the 5 points of the star are represented by a woman who represents that character within the lectures.\nEach woman is dressed in a costume, symbolic of which of the 5 heroines she represents.\nEach of these biblical characters share a lesson in the Masonic virtues:\n(From Mackey’s Revised Encyclopaedia of Freemasonry, page 303, copyright 1929)\n- Adah – Respect to the binding force of a vow\n- Ruth – Devotion to religious principles\n- Esther – Fidelity to kindred and friends\n- Martha – Undeviating faith in the hour of trial\n- Electa – Patience and submission under wrong\nEmblem of Job’s Daughters\nJob’s Daughters International is an organization of young women with members in the United States, Canada, Australia, the Philippines and Brazil. We have fun together at activities such as swimming parties, dances, family picnics, slumber parties, miniature golf, marching in parades and so much more. In Job’s Daughters you will make new friends that will last a lifetime.\nJob’s Daughters perform service projects to help their community and the less fortunate. We actively support the Hearing Impaired Kids Endowment (HIKE) Fund, which purchases hearing assisting devices for hearing impaired children.\nJob’s Daughters can qualify for various scholarships that are offered on a state and national level. We also gain valuable leadership experience, serve as part of a team, and learn democratic principles as they run their own meetings, decide our own activities, and plan our own events.\nIn short, Job’s Daughters International offers the qualities that today’s young women want and need from organizations that earn their commitment: fun, friendship, helping others and the chance to learn organizational and leadership skills.\nEmblem of Rainbow Girls\nRainbow is a nonprofit organization that strives to give girls the tools, training, and encouragement to let their individual spirits shine bright. By providing members with a safe, fun, caring environment where responsible, older girls can interact and mentor younger girls through family involvement.\nRainbow Girls Are Busy!\nBy participating in fun events and activities, keeping up with new friends, and traveling to different cities and states, members experience the excitement of what a productive life has to offer.\nRainbow Girls love a challenge!\nFrom volunteering for different charities, to working to improve their public speaking, to mastering the lessons of Rainbow, members tackle lots of challenging hurdles but ultimately walk away with a sense of pride and accomplishment.\nRainbow Girls are ready for life!\nWhether they dream of becoming a successful veterinarian, a talented musician, or a loving mother, Rainbow Girls learn that they can accomplish anything and that they can make a difference.\nEmblem of DeMolay\nDeMolay is an organization dedicated to preparing young men to lead successful, happy, and productive lives. Basing its approach on timeless principles and practical, hands-on experience, DeMolay opens doors for young men aged 12 to 21 by developing the civic awareness, personal responsibility and leadership skills so vitally needed in society today. DeMolay combines this serious mission with a fun approach that builds important bonds of friendship among members in more than 1,000 Chapters worldwide.\nDeMolay alumni include Walt Disney, John Wayne, Walter Cronkite, football Hall-of-Famer Fran Tarkenton, legendary Nebraska football coach Tom Osborne, news anchor David Goodnow and many others. Each has spoken eloquently of the life-changing benefit gained from their involvement in DeMolay.\nThe Philalethes Society Emblem\nThe word Philalethes is derived from two Greek words, philos and alethes. Philos means love. Alethes means truth. Together, they mean “lover of truth.”\nThe sole purpose of this research society is to act as a clearinghouse for Masonic knowledge. It exchanges ideas, researches problems confronting Freemasonry, and passes them along to the Masonic fraternal or appendant bodies.\nThe Philalethes Society was founded in 1928 by a group of Masonic students. It was designed for Freemasons desirous of seeking and spreading Masonic light.\nMore Appendant Bodies\nThere are several more Appendant Bodies of the Freemasonic Institution, perhaps too numerous to mention in this short listing. However, this is a brief overview that might serve to satisfy some questions that you may have regarding this very diverse and rich in history group of organizations and the people who compose them. This is a cornerstone in Masonry- that it is an Institution made up of individuals, all working towards a common goal- the betterment of all mankind!\nSo Mote It Be!"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"future"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:b94aa5ca-a5ae-4da0-9c94-fc316bd3ac1c>","<urn:uuid:451495c0-4478-410d-9f61-94953f5f560e>"],"error":null}
{"question":"What are the main differences between records retention policies for law firms and HIPAA-compliant storage requirements for healthcare providers?","answer":"Law firms and healthcare providers have distinct storage requirements. Law firms must follow jurisdiction-specific retention periods and can destroy files after certain timeframes, with some exceptions like files with outstanding judgments or involving minors. They must also notify clients before destruction. In contrast, healthcare providers under HIPAA must maintain stricter ongoing security measures, including access control, audit controls, and person/entity authentication. While law firms can choose between physical or digital storage, healthcare providers using online storage must implement specific safeguards like encryption, authentication, and password protection to ensure patient information remains private and secure.","context":["It’s no secret that being a lawyer generates a lot of paperwork. Although there are some great online options available for storing documents (and even sharing them with clients), that doesn’t mean that it’s possible for every law office in the United States to become paperless. Many courts still require physical copies for filing as well as sending to the parties involved. Depending on the area of law in which you practice and how many clients you handle at any given time, you could end up with a lot of files. Your files will eventually become closed matters. So, what do you do with all of those closed files? The answer is: you consult your records retention policy.\nIf you don’t have a records retention policy, don’t worry. We’re going to walk you through how to create one. It’s much easier than you think (although it may take a little bit of time!).\nCan’t You Just Keep Everything Forever?\nIs a records retention policy a necessity? Can’t you just keep everything forever? There are three main issues with trying to hold on to every file until the end of time.\nYou’ll eventually run out of space. Keeping all of your files is impractical. At some point, you’ll run out of space for files. Then, you’ll need long-term storage solutions. You might have to move your old files to make room for your new ones. For virtual file storage, you’d need to make a substantial investment into your own equipment or virtual space.\nKeeping everything comes at a cost. Moving your files to a new location will cost you manpower as well as an increase in what you spend to store your files. Long-term storage solutions won’t be free. Virtual solutions may have a one time fee if you’re buying your own equipment, but you’ll also pay to have your physical files converted and moved. You may also need to pay for IT specialists to help move your files and keep your equipment in good shape. For cloud-based solutions, you’ll pay a monthly or yearly fee.\nYou could face an ethics complaint. Your jurisdiction has its own rule on how long you should keep files. If you keep files longer than you should, you could cause yourself to face an ethics investigation. If a client files a complaint and you still have the file that should have been destroyed, you could take away one of your defenses: that you no longer have the file. If you don’t have the file because you destroyed it when you were supposed to, the matter could be dropped. Of course, there’s also a chance that having the file could mean that it’s easier for you to defend against a frivolous complaint.\nNow that you understand the importance of a records retention policy, let’s look how you can create one.\nStep 1 – Know the Rule in Your Jurisdiction\nBefore you create and implement your records retention policy, check the rules in your jurisdiction to learn how long you should hold onto files. This amount of time will dictate when you act on your policy. Make sure that this date is clearly written into your policy and procedure manual for records retention and destruction.\nStep 2 – Outline Closing Procedures\nYour records retention policy should detail the closing procedures that should be used when a client matter concludes.\nStep 3 – Designate How Files Will Be Stored\nAre hard files converted to digital copies? Are you saving both physical and digital files? Also, don’t forget to address when and how you’ll return material that belongs to the client.\nStep 4 – Designate Where the Files Will Be Stored\nAre physical files stored on-site or off-site? If files are stored off-site, what are the procedures for getting the files to that location? Is there a phone number to call for pick-up? Are digital files stored on your own server or is it stored with a particular cloud-based provider?\nStep 5 – Explain When (and How) Files Should Be Reviewed for Potential Destruction\nWill someone check the files every month? Do you have a system in place to determine the age of a closed file at a glance? Make sure that determining which files are ready for destruction is as fast, easy, and accurate as possible.\nStep 6 – Determine Who Is Authorized to Make Retention and Destruction Decisions\nWhile you can have support staff actually destroy files, only attorneys should be authorized to decide what should be retained and what may be destroyed.\nStep 7 – List Which Files Should Never Be Destroyed Regardless of Age\nThere are some files that should never be destroyed. This includes files with outstanding judgments and files that have children who are minors and that haven’t become legal adults yet. Make sure that you list each file type that should not be destroyed along with if there is ever a time when the file may be eligible for destruction.\nStep 8 – List Which Document Types Should Never Be Destroyed\nGenerally, you won’t want to destroy any original testamentary documents or vital records. Do not destroy documents or items that clearly belong to your client. Do not destroy information that could be useful to the client in a matter where the statute of limitation hasn’t expired.\nStep 9 – Explain How to Contact Clients about Their Files\nBefore a file is destroyed, you should reach out to the client to make sure that they don’t want anything from it (or copies of it). Explain how contact should be made and whether the client is responsible for picking up the documents within a certain amount of time.\nStep 10 – Explain How to Destroy a File\nYou should explain which file destruction methods are approved and how the file should be destroyed. Stress the importance of protecting client confidentiality during the destruction process. The file that was destroyed should be logged in a document meant to track retention and destruction.\nTo learn more about records retention and destruction, read Megan’s original post and check out her file retention and destruction policy example.","RESOURCES - ARTICLE LIBRARY\nStaying HIPAA Compliant with Online Data Storage\nKeeping patient records secure and private is the concern of every hospital and health care provider, but they are often overwhelmed with years and years of patient information and the lack of adequate storage space. Destroying these health records in order to make room for more storage is often not an option. Patients want access to all of their health care records, and physicians need them in order to better diagnose patients. Online data storage is a way to satisfy all of these issues. Using online storage for these records allows easier access for patients, and offers easier sharing of patient information from hospital to physician, as well as from physician to physician. Storing health records online isn’t, however, without security concerns. Patients, hospitals, and physicians want assurance that these confidential records will remain safe, private, and secure, and will only be accessed by those authorized to do so.\nWhat is HIPAA?\nHIPAA or the Health Insurance Portability and Accountability Act of 1996 was created in order to protect health information and give patients certain rights regarding their private health information. It also allows for disclosure of health information necessary for patient care. This act specifies safeguards necessary for administrative, and physical and technical handling of patient health information.\nAccording to the U.S. Department of Health and Human Services (HHS.gov) HIPAA has many requirements and restrictions. It requires safeguards for:\n- Access Control\n- Audit Controls\n- Person or Entity Authentication\nAccess control is defined in the HIPAA Privacy Rule as “the ability or the means necessary to read, write, modify, or communicate data/information or otherwise use any system resource.” It should allow authorized users to only access the minimum amount of information necessary to complete job functions. The Access Control specification also requires the implementation of an exclusive user identification or user ID, and immediate access in case of an emergency.\nWhat Type of Security is Necessary?\nWhen dealing with patient records in an office, maintaining privacy and security usually involves storing patient files in locked cabinets where the files can be physically secured and visibly monitored at all times. When you are storing patient information online, certain precautions must be met in order to maintain the same security and privacy guaranteed each patient.\nWhile HIPAA permits patient records to be transmitted over the Internet, businesses will want a service that offers file encryption, authentication and password protection in order to secure the information. Although HIPAA does not require online data storage services to have encryption, it does require that patient information be adequately protected and accessible only to authorized persons. Encryption is the best way to protect that information and ensure authorized access to those records. It is also important to offer backup services in case of a virus attack, flood, or fire. Finally, the service must offer a method of tracking any security breach, as well as the ability to lock out former employees after they have left or been terminated.\nWhen storing patient information, it is important to stay HIPAA compliant, as the fines for not doing so are expensive. While online storage for health care businesses guarantee less worry, work, and expense for health care providers, the service is only as good as the security offered. Remaining HIPAA compliant is vital in order to continue a good business relationship with the health care industry.\nContent by Managed Services Provider University"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:b5658f4c-b5df-48a9-8756-eb9a95c155dd>","<urn:uuid:53c1e085-512d-454f-869c-1a294b08602e>"],"error":null}
{"question":"As an education consultant developing intervention tools, what practical approaches exist for identifying and addressing developmental needs in early childhood education?","answer":"There are systematic approaches using early intervention toolkits that help schools and nurseries quickly identify if children are struggling to reach age-related expectations. These tools focus on three prime areas: communication and language, personal, social and emotional development, and physical development. The process involves teachers and support staff using specific intervention cards containing practical activities to help children overcome learning barriers. This approach allows for quick and accurate intervention, enabling children to overcome any barriers to learning as rapidly as possible.","context":["A quarter of children start primary school in England without the necessary language and communication skills, a report has found.\nThe analysis by charity the Early Intervention Foundation (EIF) also discovered a postcode lottery for school-readiness across the country. Children in the North East, North West and West Midlands rank bottom for attainment in language and communication. The North East ranks bottom for personal, social and emotional development.\nAnd in all categories boys are underperforming compared to girls. In some categories the difference was as high as nine per centage points between boys in the North East and boys in the South East.\nEIF Chief Executive Carey Oppenheim said: “Too many children arrive for their first day at primary school lacking the broad range of skills they need to reach their full potential.\n“This can have damaging consequences which can last a lifetime. Especially as children with strong social, emotional and communication skills developed in childhood have a better chance of getting a good job and being healthy, than those who are just bright or clever. The gap in the development social and emotional skills between children growing up in poor and rich families begins at the age of three.”\nLinda Tallent has spent 20 years working in some of the most socially deprived schools in the country both in the North East and nationwide, as a teacher, school improvement officer and now education consultant and keynote speaker specialising in early years and primary education.\nShe founded the Tyneside-based Learning and Training Consultancy (LTC) four years ago to bring together specialists in a wide range of areas across the Early Years Foundation Stage and primary education. Together with her associates, they provide consultancy services, run conferences and workshops and have authored a number of publications.\nLinda hopes early years education in the region will be placed firmly in the spotlight at a conference organised by LTC in Newcastle on June 3 called Early Years: Love to Learn. Practitioners will be gathering from across the region to discuss what can be done to give children in the North East the essential skills they need to flourish.\nThe conference will also see her launch her latest project that has seen her channel her extensive experience into an innovative early intervention toolkit for teachers called the “itkit” that she hopes will transform the lives of children by ensuring they get the help they need as quickly as possible to reach their full potential.\n“Systematic early intervention is key to giving children the building blocks they need for a lifelong love of learning and can transform their lives,” says Linda.\n“There is no point waiting until children start school at five. We need to identify any developmental needs as early as possible and act on them as quickly as we can, with a personalised intervention programme. That is what the itkit aims to do.\n“We are moving towards a culture where it is accepted that early intervention will help ensure that children are ready for school. More nursery places are being made available for two-year-olds and we need to all work together with parents.\n“I hope this conference will also be a fantastic opportunity to bring early years and primary practitioners together so we can share ideas and best practice and work together to help children across the North East to reach their full potential.”\nThe itkit aims to equip schools and nurseries with the resources to quickly identify if any child is struggling to reach age-related expectations in the prime areas of learning: communication and language; personal, social and emotional development and physical development. Teachers and support staff are then directed to specific intervention cards that contain practical activities to assist the children in overcoming any barriers to learning.\nPupils at Battle Hill Primary school in Wallsend had fun testing out the itkit and putting the activities into practice.\nGina Parkinson, reception teacher and Foundation Stage manager at the school, said: “The children had lots of fun taking part in Linda’s activities.\n“Early intervention is the key to giving children the best start in life. The itkit is an excellent early intervention tool which will help support practitioners to intervene quickly and accurately therefore enabling children to overcome any barriers to learning as quickly as possible.”\nTo find out more about LTC and the conference visit http://www.learningandtraining.co.uk/ .E"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"future"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:36549088-bdbc-4f26-a38f-5856af4edc3c>"],"error":null}
{"question":"What is difference between interior and exterior waterproofing methods for basement?","answer":"Interior and exterior waterproofing methods differ significantly in their approach and implementation. Interior waterproofing repairs leaks from inside the home, involving installation of drainage membrane, weeping tile, and cement to manage water infiltration. It's particularly effective for hydrostatic pressure problems and areas with no exterior access. Exterior waterproofing, on the other hand, requires excavation to the bottom of footings, sealing walls with waterproofing membrane, and installing weeping tiles beside the footing. While interior methods focus on managing water that enters, exterior methods aim to prevent water from entering the walls and foundation altogether.","context":["Interior waterproofing (or sometimes known as internal waterproofing) repairs your basement leaks from the inside of your home and is the best solution for a damp or wet basement. When your basement becomes damp and moist, it is best to have an interior waterproofing done in your basement to prevent the growth of mold and mildew, which will create harmful smells and bacteria that affect your health. This is also one of the waterproofing solutions that will help take care of hydrostatic pressure that builds up and causes a wet basement.\nThe waterproofing materials include an interior air-gap drainage membrane, drainage tile (weeping tile), drainage tile connector pieces, 3/4 inch gravel, and cement.\nThe internal system method may be the best option depending on these 3 reasons:\n- The area cannot be addressed from the exterior due to confined space or no access.\n- The water problem is not a foundation leak but a hydrostatic pressure problem, which means the groundwater under the concrete slab raises up and down and causes the water to penetrate through the basement slab or where the wall meets the floor. This groundwater can also mean that there could be an underground creek where the house was built.\n- The Internal System is a cost-effective solution for unfinished living areas.\nBasement Waterproofing Is the Key To Preventing Mold and Promoting Air Quality\nMold can be a serious problem if it develops in your home. Mold will gradually destroy the area where it colonizes and has the potential to create health problems for your home’s inhabitants.\nMolds create allergens (substances that cause allergic reactions), as well as irritants and even toxic substances, called mycotoxins. Anyone inhaling or touching mold or mold spores can develop allergic reactions.\nPeople with bronchitis are more sensitive to Mold or poor air quality. Mold grows from tiny spores that are not visible to the naked eye but exist in both outdoor and indoor air. Mold can begin to grow indoors if it is present on a wet surface. The only thing mold requires to grow is moisture or water and dust. As the dust is everywhere, it can begin to grow indoors if it lands on a wet surface. That should tell you something about the hazards of a damp or wet basement.\nOne of the mistakes many homeowners make regarding water or dampness in their basement is they tend to let the problem go until the timing is more convenient to address it. That can be a huge mistake! Water damage tends to get worse with time and you don’t want to give mold a chance to develop. If it does, it’ll cost you more in the long run, trying to get rid of it, along with the potential health issues.\nIf you suspect you have mold in your home and you start looking for solutions, you’re likely to encounter the term “mold remediation,” which literally means remedying a mold problem. Since mold normally goes undetected until it’s in the advanced stages, enlisting the help of a certified inspector would be a wise choice. Certified mold specialists are equipped to deal with mold in the home and will be current with up-to-date products, procedures, and regulations.\nHowever, when it comes to keeping mold out of your home, few would argue that prevention is the best option. So once you notice a moisture/water problem, have it fixed or looked at sooner than later.\nInterior Drainage System Steps\nThe floor is broken around the perimeter in which the system is needed approx 6 to 10 inches away from the foundation wall.\nConcrete is then removed and the trench is dug out to accommodate the 3 inch\nPerforated weeping tile is installed\nThe weeping tile is installed and must be connected to a discharge such as a sump pump or a floor drain.\nThen a dimpled sheet of drainage membrane is installed on the interior exposed wall where the system has been installed.\nWe then install a layer of ¾ clear gravel over the weeping tile to gain maximum drainage to the tile.\nOnce the membrane and gravel are installed this will allow any water from above grade or below grade to drain in the system installed.\nThen a layer of concrete is installed over the exposed trench that will finish the floor back to its original level.\nCommon Exterior Leak Problems\nFloor and Wall\nInternal Waterproofing can add drainage on the inside of your home, rerouting water inflow to a proper point of discharge. The main benefit of installing an internal waterproofing system is that it has the ability to intercept water making its way through the wall, and from under the footing.\nBasement walls that are being penetrated will be drained out by our drainage membrane, providing a barrier between your wall and living area. Water travelling beneath the footing can show up as moisture spots and leaks on your basement floor, which will also be protected by the drainage board and drained through new weeping tiles.\nThe main component of any waterproofing system is a point to discharge the water in the system. When existing drainage has been blocked or there are no additional drainage options, your consultant may suggest a sump pump. A pit is dug out in your basement, which will be the lowest point of your waterproofing system. A high quality plastic liner is installed inside of the pit allowing water to flow into it through gravity drainage.\nWater build up is sensed by the trigger of a sump pump, which ejects water from the sump pit to the outside of the home. We only use top of the line pumps and plumbing products, and we also provide battery back-up pumps that will continue to work even when your power goes out. The City of Toronto is currently offering a subsidy for sump pump installations, so ask your sales representative to explain how you can save up to $1750.","Basement waterproofing is important to any basement as the basement is the part of your home that is most vulnerable to moisture problems.\nBecause it is underground, it is at risk of groundwater seeping into the foundation, especially during significant rainfall, severe storms, and floods. Places where the water table can raise due to an increase in ground water are especially vulnerable to water seeping into the home due to hydrostatic pressure, which creates pressure under the floors and against the walls, eventually forcing water into the home through any cracks or openings. It might also break through weak points, causing more cracks or openings. Additionally, the basement is typically constructed out of cement, which can also often “sweat,” dripping moisture due to humidity.\nIf the basement is not waterproofed, mitigating the risk of water entering, it can contribute to water damage and other problems, including mold growth and structural damage in the walls and foundation of your home. Therefore, before you convert your basement into a livable space, the very first thing you need to do is ensure it is waterproof. Luckily, there are some very simple solutions that protect your basement from excess moisture and flooding.\nInstalling drainage solutions in your basement, especially in the foundation, helps to keep the water out of your basement and remove the risk of water damage. Drainage systems in the interior of the home can help to drain any underground water from the foundation footers and basement floor to then channel it out through a drain, which can also connect to a sump pump. Other conduits may also be added to help guide the moisture to the drainage systems. For foundations made from concrete blocks, the best way to remove any water is to install an interior drain tile system. You can also choose to install a French drain if you have significant water issues in your basement. This system includes a continuous drainage system, and it requires having a 12-inch deep trench around the perimeter of your basement.\nSump pumps are one of the most essential tools to install in your basement to prevent flooding in your home. They work by pumping out any excess water that seeps into the home, removing the risk of water damage and other problems. The water comes typically enters into the sump pump from some type of drainage system, and the sump pump pumps it out through pipes to an area away from the house where it will not cause damage. They often use electricity, so in the case of any power outage, there should be a secondary option in place.\nSealing any cracks and openings in the walls and foundations is essential to protecting your basement from any water or excess moisture. If there are any cracks or pipe penetrations in a concrete foundation, you should seal them from the interior using epoxies or urethanes that are injected into the openings and will penetrate through the wall all the way to the exterior of the home. These can last for as long as ten years. Masonry foundations cannot be protected the same way from water seeping into the home. However, using sealants can prevent spalling, or a break down of the masonry surfaces, due to excess humidity.\nOne way to keep excess moisture out of the basement is to use waterproofing coatings along the walls and floors on the interior of the basement. This does not help mitigate any major leaks, but it does help if the major reason for water in the basement comes from humidity and condensation. It will also help with small cracks or leaks. You can also use special paint on the wall that provides protection from water.\nIn addition to installing drains and pump system and coating the inside of the basement, you should also look into waterproofing the exterior panels of the basement. This prevents the water from entering the walls and the foundation, which mitigates the risk of water damage. Waterproofing the exterior can be a more complicated project, unless it is done when the foundation is first installed. Typically, you will have to excavate all the way to the bottom of the footings. Then, you will seal the walls with a waterproofing membrane and place weeping tiles or other drainage solutions to the side of the footing.\nAdditional Basement Waterproofing Preventative Measures\nIn addition to installing drainage solutions and using sealants and waterproofing coatings on the walls and floors, you should also ensure that the downspouts of your gutters pour the water at least five feet from your home. This prevents the water from your gutters seeping into your basement and causing problems. You should also ensure that the slope of your yard goes away from the foundation. A flat yard or one that slopes towards your basement will cause water to pool around your foundation, which increases the chance of it dripping into your basement. Plants and shrubs should also be planted at least a few feet from the foundation to prevent excess pooling of water.\nHow you proceed with waterproofing your basement will be largely dependent upon its current condition, the risk of groundwater seepage, and more. The best waterproofing solution will include a combination of methods, such as installing a drainage system, using waterproofing membranes, and ensuring the exterior is also protected against water."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:7ea75e82-1071-451e-b841-918b9a0a5f99>","<urn:uuid:f30cba9c-4691-432a-8886-5c9f4ab65913>"],"error":null}
{"question":"What are the infrastructure components of OpenShift on GCP, and how does it handle container orchestration with Kubernetes?","answer":"OpenShift on GCP uses different types of Google Compute Engine virtual machines: nodes and masters. The nodes contain pods (groups of containers) that host containerized applications, while masters control the environment and provide services like authentication and API access. For infrastructure, it uses bastion hosts for SSH traffic, master instances for OpenShift components, application instances for user containers, and infrastructure instances for routing and registry. Kubernetes serves as the orchestration layer, managing the full lifecycle of container-based applications by automating tasks, controlling resources, and abstracting infrastructure. It coordinates cluster nodes and schedules container work, while seamlessly working with any type of software and infrastructure through the container runtime.","context":["This solution introduces Red Hat OpenShift Container Platform and the benefits of running OpenShift on Google Cloud Platform (GCP), such as:\nOpenShift enables running and supporting stateful and stateless applications without needing to completely re-architect your application.\nOpenShift uses the Red Hat Enterprise Linux operating system as its foundation, providing a stable and secure foundation for your applications.\nOpenShift natively integrates technologies such as Docker and Kubernetes, a powerful cluster manager and orchestration system for running your Docker containers.\nOpenShift equips customers with an enterprise-grade Kubernetes environment, and all the tools and services they need to create, edit, deploy, and manage container-based applications at scale across a hybrid cloud environment. The service integrates the architecture, processes, platforms, and services that empower development and operations teams to build applications that move their business forward. OpenShift enables mission-critical, traditional applications to coexist with cloud-native or container-based applications.\nThe following diagram provides an overview of OpenShift components and functionality, including GCP components that you can integrate seamlessly with your deployment.\nOpenShift utilizes two types of Google Compute Engine virtual machines: nodes and masters. Nodes contain pods, groups of one or more containers, that host your containerized applications. A native implementation of Kubernetes orchestrates these pods. The master controls and manages the OpenShift environment, and provides additional services such as authentication and the OpenShift API. The master also acts as a nexus point for your developers and system administrators, enabling them to interact with the platform using the tools and services they use every day. OpenShift uses Google Cloud Platform networking products to connect the machines together, and provide an external connection so your applications can talk to the outside world.\nOpenShift leverages several other GCP resources for critical components. Cloud Storage provides persistent storage for your applications. You can store your Docker images with OpenShift's default registry, or with Google Cloud Container Registry. Applications can leverage any Google service that fits your use case, such as Google BigQuery, or Google Cloud Machine Learning.\nRunning OpenShift on GCP\nOpenShift on GCP enables you to deploy stateful and stateless applications with nearly any language, framework, database, or service. Because OpenShift natively integrates Google technology such as Kubernetes, your applications are built and deployed on the same infrastructure and orchestrations as applications like YouTube, Google Drive, and more. OpenShift supports a broad array of GCP services.\nThe following diagram shows how OpenShift and GCP work together.\nOpenShift uses Compute Engine to deploy resources in a highly available configuration across regions and zones. The configuration leverages load balancing and scaling, Google DNS, Google OAuth, custom images, and persistent disks.\nDeployed instances support OpenShift in several ways.\nThe bastion host instance limits external access to internal instances by ensuring that all SSH traffic passes through the bastion host.\nMaster instances host OpenShift master components such as\netcdand the OpenShift API.\nApplication instances are destinations for user-deployed containers.\nInfrastructure instances contain the OpenShift router and registry.\nGoogle OAuth manages authentication. Persistent disks are used for instances and for persistent storage. Three load balancers distribute networking traffic to the external-facing and internal OpenShift API, external console access, and all services opened through OpenShift routing. Google Cloud DNS manages resource registration.\nWhen you deploy OpenShift on GCP, building, securing and load balancing your application is managed for you. To learn more about configuration options available for your deployment, see Deploying Red Hat OpenShift Container Platform 3 on Google Cloud Platform.\nSecurity and authorization\nOnce you've deployed your OpenShift environment, you can take additional steps\nto help secure your setup and configure access for users you've created on the\nLimitRanges enable configuring\ndifferent resource boundaries for these users, such as pod and container limits,\nobject counts, and compute resources. Quotas and limits are set per-project,\nbut you can create a quota-like limit on how many project requests an individual\nuser can make.\nOpenShift on GCP utilizes Google Cloud Identity Access Management (IAM) to securely control access to Compute Engine resources and services. For more information on security and authorization for OpenShift on GCP, see OpenShift's infrastructure user and role administration guide.\nYou can upgrade OpenShift either in-place, or using a blue-green deployment method.\nIn-place upgrades update all hosts in a single cluster. You can run this process manually, or automatically if you installed OpenShift using the quick or advanced installation methods.\nBlue-green deployments reduce upgrade downtime by creating a parallel environment where the new deployment installs. After the new deployment is verified, traffic can be switched over to it and rolled back if a problem occurs.\nFor more information, see OpenShift: Upgrading a Cluster.\nBacking up and restoring OpenShift\nBecause OpenShift runs on GCP, your system is fault-tolerant by design. To take additional precautions, you can create and manage on-demand and automatic backups.","Using Docker and Kubernetes in tandem streamlines the experiences and makes it easier for developers to create scalable applications, as well as allows teams to build cloud-native architectures or microservices more efficiently. It comprises one container or multiple containers wrapped together with the ability to share resources (including network, IP address, hostname, etc.) and communicate with each other, all deployed to a node as a single unit. Many containers can live in a pod, and they always scale together, but to optimize efficiency, avoid putting more containers than necessary into a pod. Pods themselves are considered ephemeral, too — they’re not meant to run forever, and once you delete a pod , you can’t bring it back.\nAfter receiving, it validates the REST requests, process, and then executes them. After the execution of REST commands, the resulting state of a cluster is saved in ‘etcd’ as a distributed key-value store. Kong powers reliable digital connections across APIs, hybrid and multi-cloud environments. Kubernetes alternatives made available free of charge typically select from among open source alternatives to provide these capabilities. These are often very good solutions for learning and small-scale use. To start the application, they “apply” the configuration to Kubernetes.\nKubernetes serves as the deployment and lifecycle management tool for containerized applications, and separate tools are used to manage infrastructure resources. Kubernetes is a platform for running your applications and services. It manages the full lifecycle of container-based applications, by automating tasks, controlling resources, and abstracting infrastructure. Enterprises adopt Kubernetes to cut down operational costs, reduce time-to-market, and transform their business. Developers like container-based development, as it helps break up monolithic applications into more maintainable microservices. Kubernetes allows their work to move seamlessly from development to production, and results in faster-time-to-market for a businesses’ applications.\nYour control over containers just happens at a higher level, giving you better control without the need to micromanage each separate container or node. Kubernetes runs on top of an operating system (Red Hat® Enterprise Linux®, for example) and interacts with pods of containers running on the nodes. The clusters are made up ofnodes, each of which represents a single compute host .\nWhen applications grow and occupy many containers over several servers, Kubernetes helps out with the API that manages place and mode of container operation by orchestrating cluster nodes and scheduling container work on nodes. Owing to the container runtime, Kubernetes works seamlessly with any type of software, written in any programming https://globalcloudteam.com/ language and with any infrastructure type — private, public and hybrid clouds, and on the premises. It helps deploy any application that can be put in a container and does so in a cost-efficient and streamlined manner. In Kubernetes, a service is a component that groups functionally similar pods and effectively load balances across them.\nKubernetes is cloud-agnostic and can also be run on-premise, avoiding any vendor lock-in. Having proven to be the tool of choice for container orchestration, Kubernetes is supported by all major cloud vendors, many of which also offer managed Kubernetes services. Originally developed by Google engineers to manage large clusters, Kubernetes is designed for scalability and reliability. Manual management clearly isn’t realistic, hence the need for a container orchestration tool. Mirantis Container Cloud provides you with a single set of APIs and tools to deploy, manage, and observe secure-by-default, certified, batteries-included Kubernetes and/or Swarm clusters on any infrastructure.\nWhat Is Managed Kubernetes?\nBy automating application-specific tasks, Operators allow you to more easily deploy and manage applications on K8s. The control plane has four primary components used to control communications, manage nodes and keep track of the state of a Kubernetes cluster. See how enterprises are using Kubernetes to build, deploy and run modern applications at scale.\n- Kubernetes assigns each node in a cluster (a group of a master/worker nodes) a block of IP addresses and in turn every pod in this node gets allocated an IP address.\n- The smallest unit in the Kubernetes object model that is used to host containers.\n- Kubernetes is an open source container orchestration platform that automates deployment, management and scaling of applications.\n- Terraform on Google Cloud Open source tool to provision Google Cloud resources with declarative configuration files.\n- Deploy and manage containerized applications consistently across on-premises, edge computing and public cloud environments from any vendor withIBM Cloud Satellite.\nKubernetes also has the advantage of having a large ecosystem of corresponding software projects and tools which can be made readily available to developers and IT engineers. Kubernetes is an effectively designed mechanism that manages the lifecycle of containerized applications. It can be defined as a system that disseminates valuable functioning and streamlines how applications work. Kubernetes, popularly referred to as “K8s,” is a segment of the Cloud Native Computing Foundation that facilitates the development of collective networking standards in cloud data software management. A cluster is a group of nodes; these are managed by the control plane. Kubernetes Operators allow you to encapsulate domain-specific knowledge for an application similar to a run book.\nLearn To Speak Kubernetes\nDocument AI Document processing and data capture automated at scale. DevOps Best Practices Processes and resources for implementing DevOps in your org. Education Teaching tools to provide more engaging learning experiences. Government Data storage, AI, and analytics solutions for government agencies.\nIf you had an issue with your implementation of Kubernetes while running in production, you’d likely be frustrated. Kubernetes is open source and as such, there’s not a formalized support structure around that technology—at least not one you’d trust your business to run on. With the right platforms, both inside What is Kubernetes and outside the container, you can best take advantage of the culture and process changes you’ve implemented. With the right implementation of Kubernetes—and with the help of other open source projects likeOpen vSwitch, OAuth, and SELinux— you can orchestrate all parts of your container infrastructure.\nAutomated rollouts and rollbacksYou can describe the desired state for your deployed containers using Kubernetes, and it can change the actual state to the desired state at a controlled rate. For example, you can automate Kubernetes to create new containers for your deployment, remove existing containers and adopt all their resources to the new container. An enterprise-ready Kubernetes container platform with full-stack automated operations to manage hybrid cloud, multicloud, and edge deployments. Container orchestration automates the deployment, management, scaling, and networking of containers.\nWhat Benefits Does Kubernetes Offer?\nSmart Analytics Solutions Generate instant insights from data at any scale with a serverless, fully managed analytics platform that significantly simplifies analytics. Architect for Multicloud Manage workloads across multiple clouds with a consistent platform. Financial Services Computing, data management, and analytics tools for financial services. It is a proxy service of Kubernetes, which is executed simply on each worker node in the cluster.\nThey need all the different parts of Kubernetes to be validated together, and supported by a single vendor. We offer a comprehensive suite of courses and certification exams to address the Cloud Computing objectives of administrators, developers, and architects. So, Docker is a containerization platform, whereas Kubernetes is a container orchestrator for platforms of the same kind. Built-in data encryption, vulnerability scanning and other capabilities enhance the security of Kubernetes. There’s a large developer community that produces various extensions to enhance off-the-shelf capabilities. The software can be consistently transferred among different types of environment.\nKubernetes Architecture And How It Works?\nThere are a number of factors to consider when selecting an installation type, including your available resources, various security needs and the level of maintenance you’re comfortable with. Furthermore, if you plan to use Kubernetes in production, you can manage it yourself or get assistance from a tutorial or one of the many certified Kubernetes providers. Nodes can be physical or virtual compute machines and their job is to run the pods with all the necessary elements.\nThe smallest unit in the Kubernetes object model that is used to host containers. Try the leading Kubernetes Storage and Data Protection platform according to GigaOm Research. Local SSD Block storage that is locally attached for high-performance needs.\nContainer Orchestration With Kubernetes\nIt would not be an exaggeration to define Kubernetes as a “game changer” when it comes to managingcontainerized applications. To be precise, Kubernetes is cluster management software that supervises the operations of multiple server computers and manages various programs on them. All these programs run in containers and remain isolated, eliminating manual processes and securing their development and deployment. Once you’ve decided to use containers in your environment, you can get started with your Kubernetes deployment.\nA Kubernetes cluster consists of at least one control plane and at least one worker node . It exposes the Kubernetes API through the API server and manages the nodes that make up the cluster. The control plane makes decisions about cluster management and detects and responds to cluster events. Our Cloud Infrastructure Container Engine for Kubernetes is a developer-friendly, managed service that you can use to deploy your containerized applications to the cloud. Use Container Engine for Kubernetes when your development team wants to reliably build, deploy, and manage cloud native applications.\nWhat Does Kubernetes Do?\nTogether, Pure Storage and Portworx provide a complete data storage infrastructure solution capable of delivering a Modern Data Experience™. Created by the same developers that built Kubernetes, Google Kubernetes Engine is an easy to use cloud based Kubernetes service for running containerized applications. GKE can help you implement a successful Kubernetes strategy for your applications in the cloud. With Anthos, Google offers a consistent Kubernetes experience for your applications across on-premises and multiple clouds.\nWith Istio, you set a single policy that configures connections between containers so that you don’t have to configure each connection individually. Developers manage cluster operations usingkubectl, a command-line interface that communicates directly with the Kubernetes API. It’s one of the most significant advancements in IT since the public cloud came to being in 2009, and has an unparalleled 5-year 30% growth rate in both market revenue and overall adoption. To implement this networking model Kubernetes uses the Container Network Interface specification.\nIt’s containers that enable the quick adjusting of software development and maintenance to changing business needs. That’s why efficient solutions for container orchestration have become a must-have for successful cloud software development projects, and Kubernetes is a quintessential example. On the other hand,worker nodes can be termed as the spine of your applications. An administrator or a DevOps team instructs the Kubernetes control plane, which then makes decisions about scheduling, identifying, and responding to cluster events. Depending on requirements, the number of worker nodes can be increased to enhance the capacity of clusters.\nA ReplicaSet in the Kubernetes is used to identify the particular number of pod replicas are running at a given time. It replaces the replication controller because it is more powerful and allows a user to use the “set-based” label selector. Typically, you would install Kubernetes on either on premise hardware or one of the major cloud providers."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:c699d29c-cdd1-4095-afd9-e591507903f0>","<urn:uuid:1605bb61-9a97-4e7d-8669-bde90f591726>"],"error":null}
{"question":"How do Shakespeare's early publishers and Walter Foster Publishing compare in their approach to making texts accessible to readers?","answer":"Shakespeare's early stationers (publishers, printers, and booksellers) and Walter Foster Publishing both aimed to make their content accessible, but through different approaches. The stationers were crucial in transforming Shakespeare's work from purely theatrical pieces into material objects that could be bought, read, collected, and annotated by readers. They played a key role in making Shakespeare's texts available through printed books. Similarly, Walter Foster Publishing was founded on the belief that art should be accessible to everyone, with Walter Foster believing that anyone who wanted to draw or paint could do so with proper guidance. The company continues this mission today by publishing art instruction books for various skill levels, from serious fine artists to weekend enthusiasts, making art education widely available through retail establishments and the book trade.","context":["Studies in Cultural Bibliography\nPublication Year: 2012\nRecent studies in early modern cultural bibliography have put forth a radically new Shakespeare—a man of keen literary ambition who wrote for page as well as stage. His work thus comes to be viewed as textual property and a material object not only seen theatrically but also bought, read, collected, annotated, copied, and otherwise passed through human hands. This Shakespeare was invented in large part by the stationers—publishers, printers, and booksellers—who produced and distributed his texts in the form of books. Yet Shakespeare's stationers have not received sustained critical attention.\nEdited by Marta Straznicky, Shakespeare's Stationers: Studies in Cultural Bibliography shifts Shakespearean textual scholarship toward a new focus on the earliest publishers and booksellers of Shakespeare's texts. This seminal collection is the first to explore the multiple and intersecting forms of agency exercised by Shakespeare's stationers in the design, production, marketing, and dissemination of his printed works. Nine critical studies examine the ways in which commerce intersected with culture and how individual stationers engaged in a range of cultural functions and political movements through their business practices. Two appendices, cataloguing the imprints of Shakespeare's texts to 1640 and providing forty additional stationer profiles, extend the volume's reach well beyond the case studies, offering a foundation for further research.\nPublished by: University of Pennsylvania Press\nIntroduction: What Is a Stationer?\nDownload PDF (274.5 KB)\nThe phrase “all this,” with respect to the present volume, refers to the imaginative writings of William Shakespeare. Preserved in print chiefly by the “labours,” “risks,” and “speculations” of dozens of printers, publishers, and booksellers, Shakespeare’s poems and plays in their earliest editions are evidence...\n1. The Stationers’ Shakespeare\nDownload PDF (208.1 KB)\nIn 2005 Forbes magazine ran a fluff piece on the annual revenue that might accrue to the Shakespeare estate were it in full control of his intellectual property and related brand identity; the conservative estimate was $15 million. That $15 million is, of course, only a small percentage of the annual revenue...\n2. Thomas Creede, William Barley, and the Venture of Printing Plays\nDownload PDF (320.1 KB)\nIn the history of English theater, 1594 was either a year of momentous importance, changing London’s theatrical landscape forever, or not particularly noteworthy, a year that saw some companies rise to prominence and others vanish just as they had in the past and would in future years. While the...\n3. Wise Ventures: Shakespeare and Thomas Playfere at the Sign of the Angel\nDownload PDF (281.0 KB)\nAccording to one estimate, Thomas Playfere was “crackt in the headpeece, for the love of a wench as some say.” The description is not what one would expect of the prestigious Lady Margaret Professor of Divinity at Cambridge, who soon after would begin preaching regularly at the court of James I, but...\n4. “Vnder the Handes of …”: Zachariah Pasfield and the Licensing of Playbooks\nDownload PDF (499.7 KB)\nSo Henry Chettle describes his dealing with the manuscript and the preparation of printer’s copy and licensing of Robert Greene’s Groats-worth of Wit (1592) in “To the Gentlemen Readers,” in Chettle’s Kind-harts dreame of probably 1593 (STC 5123). Licensed it might have been, but when Greene’s...\n5. Nicholas Ling’s Republican Hamlet (1603)\nDownload PDF (301.3 KB)\nSince the mid-1980s, historians have undertaken a broad reconsideration of the roots of the English Civil War. Dissatisfied with the assumption that a republican political temper emerged spontaneously in the late 1630s, that various social practices and customs kept sixteenth-century Englishmen from...\n6. Shakespeare the Stationer\nDownload PDF (420.1 KB)\nDid Shakespeare own his own playbooks? Although in an essay of this title Andrew Gurr is hesitant to answer “yes,” he nevertheless shows the strong likelihood that Edward Alleyn personally controlled a number of the dramatic manuscripts used by the Admiral’s Men—including many of...\n7. Edward Blount, the Herberts, and the First Folio\nDownload PDF (542.9 KB)\nRecent scholarship has helpfully shown how “ideological commitment was not the sole province of authors but also of printers-publishers,” thus qualifying the earlier assumption that “like the grocer and the goldsmith,” early modern stationers “were mainly interested in money.” This important shift in our...\n8. John Norton and the Politics of Shakespeare’s History Plays in Caroline England\nDownload PDF (469.1 KB)\nShakespeare’s history plays have often been read with one eye looking forward to the reign of Charles I and the outbreak of the English Civil War in 1642. The first writer to make this move was John Milton in Eikonoklastes, published only nine months after the execution of Charles I in January 1649. Milton...\n9. Shakespeare’s Flop: John Waterson and The Two Noble Kinsmen\nDownload PDF (655.2 KB)\nThe Two Noble Kinsmen is an oddball among Shakespeare’s printed plays. The 1634 first edition is the only Shakespearean playbook in which the Bard’s name appears on the title page alongside that of another playwright, John Fletcher. It is the only play now generally accepted as Shakespeare’s...\nAppendix A: Shakespearean Publications, 1591–1640\nDownload PDF (508.4 KB)\nAppendix B: Selected Stationer Profiles\nDownload PDF (1.1 MB)\nDownload PDF (903.4 KB)\nList of Contributors\nDownload PDF (152.3 KB)\nDownload PDF (172.7 KB)\nPage Count: 376\nPublication Year: 2012\nSeries Title: Material Texts\nSeries Editor Byline: Series Editors: Roger Chartier, Joseph Farrell, Anthony Grafton, Leah Price, Peter Stallybrass, Michael F. Suarez, S.J.","The preeminent publisher of art instruction books, Walter Foster Publishing began like many California entrepreneurial success stories: In 1922, Walter Foster, a caricaturist, sign painter and Vaudevillian performer, among other talents, started publishing instructional art books in his home in Laguna Beach. In the beginning, he did it all: writing, illustrating, printing, binding--all of it! Walter was driven by his belief that anyone who wanted to draw or paint could do so with just a little help and encouragement.\nEventually, Walter Foster Publishing, an imprint of the London-based Quarto Group, grew into a world-renowned, trusted publisher of quality art-instruction books. It continued to grow its line of books for adults and launched a children's drawing book program that includes several licensed titles with Disney, Nickelodeon, Dreamworks, and others.\n|Anne Landa (left) and Rebecca Razo|\nHowever, it's been over the past decade or so that Walter Foster has \"really hit its stride,\" says Rebecca Razo, publisher of Walter Foster and the forthcoming Laguna Press. \"Anne Landa, group publisher of Quarto Southern California, took over running the business in January 2015, at which time we revamped our program.\" This includes adding a wider range of titles, not just for serious fine artists, but also for art hobbyists, crafters, and weekend art enthusiasts, as well as introducing more intuitive approaches to teaching art. \"And we've modernized the aesthetic of our list across the board,\" Razo adds.\nWalter Foster books enjoy a longstanding, solid reputation in the art and craft market at such retail establishments as Hobby Lobby and Michaels, as well as art supply chains, including Blick and AC Moore. And its presence continues to grow in the book trade. \"One of the things we hear over and over again from people around the world is: 'I grew up with Walter Foster books!' \" Razo says. But the company also wants booksellers and librarians to know that, as Razo puts it, \"We're not just your parents' and grandparents' instructional-art book publisher.\" The list is \"as fresh and contemporary as it is practical and useful,\" she says, adding, \"And we've struck a perfect balance between continuing to serve the needs of our core audience--the serious fine artist--and publishing for those who just want to pass the time making great art.\"\n\"Being part of the Quarto Group has enabled Walter Foster to spread its wings,\" Anne Landa, group publisher, Quarto Southern California, says. Quarto's mission is, Landa continues, \"to make and sell great books that entertain, educate and enrich the lives of adults and children around the world. We're constantly looking for new ways to create and deliver content that people need.\"\nSince 1996, Walter Foster Publishing has been part of The Quarto Group, which specializes in illustrated nonfiction, is domiciled in the U.S., and is listed on the London Stock Exchange. Quarto has 48 imprints around the world that publish in 50 countries and 39 languages through traditional and non-traditional channels focusing on subjects that range from art 'how-to,' graphic design and home improvement to cooking, gardening, motoring and crafts. Its 400 employees are in the U.S., U.K., and Hong Kong. In the U.S., The Quarto Group has five creative hubs: in Lake Forest, Calif. (where Walter Foster is located); New York City; Beverly, Mass.; Minneapolis, Minn.; and Seattle, Wash., where becker&mayer, its the most recent acquisition, is based."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:29f9b09a-371a-4fa1-9dfa-76ebc12f6337>","<urn:uuid:16872fda-1515-44ec-addd-0f2a055bdbfe>"],"error":null}
{"question":"I work in mental health field and want know: what is cognitive empathy, and what burnout risks are associated with using it too much?","answer":"Cognitive empathy is the ability to understand how others feel and what they might be thinking, also known as perspective-taking. While it can be useful for motivating people and negotiations, excessive use of cognitive empathy without proper self-care can contribute to burnout. Healthcare workers who regularly employ empathy are at risk of compassion fatigue and emotional exhaustion, particularly when dealing with traumatized populations. Research shows that three-quarters of social workers experience burnout during their careers, often manifesting as reduced compassion, depression symptoms, and job cynicism.","context":["3 Types of Empathy - Goleman and Oprah\nBeing cool in crisis seems essential for our being able to think clearly. But what if keeping cool makes you too cold to care? In other words, must we sacrifice empathy to stay calm? That’s the dilemma facing those who are preparing top teams to handle the next Katrina-like catastrophe we might face. Which gets me to Paul Ekman, a world expert on emotions and our ability to read and respond to them in others. Paul and I had a long conversation recently, in which he described three very different ways to sense another person’s feelings.\nThe first is “cognitive empathy,” simply knowing how the other person feels and what they might be thinking. Sometimes called perspective-taking, this kind of empathy can help in, say, a negotiation or in motivating people. A study at the University of Birmingham found, for example, that managers who are good at perspective-taking were able to move workers to give their best efforts.\nBut there can be a dark side to this sort of empathy – in fact, those who fall within the “Dark Triad” – narcissists, Machiavellians, and sociopaths (see Chapter 8 in Social Intelligence) – can be talented in this regard, while having no sympathy whatever for their victims. As Paul told me, a torturer needs this ability, if only to better calibrate his cruelty – and talented political operatives no doubt have this ability in abundance.\nKatrina’s devastation, we all saw, was amplified enormously by the lackadaisical response from the very agencies that were supposed to manage the emergency. As we all witnessed, leaders at the highest levels were weirdly detached, despite the abundant evidence on our TV screens that the disaster’s victims were doubly victimised by the indifference to their suffering.\nCertainly empathy qualifies as one critical measure of the right leader in a crisis, along with being cool under pressure. But exactly what kind of empathy should we look for? When it comes to the right leader for a crisis, cognitive empathy alone seems insufficient .\nThen, Paul told me, there’s “emotional empathy,” – when you feel physically along with the other person, as though their emotions were contagious. This emotional contagion, social neuroscience tells us, depends in large part on the mirror neuron system (see Chapter Three in Social Intelligence). Emotional empathy makes someone well-attuned to another person’s inner emotional world, a plus in any of a wide range of callings, from sales to nursing – let alone for any parent or lover.\nOne downside of emotional empathy occurs when people lack the ability to manage their own distressing emotions can be seen in the psychological exhaustion that leads to burnout. The purposeful detachment cultivated by those in medicine offers one way to inoculate against burnout. But the danger arises when detachment leads to indifference, rather than to well-calibrated caring.\nFinally, there’s what Paul calls “compassionate empathy,” which I’ve written about using the term “empathic concern” (see Chapter Six in Social Intelligence). With this kind of empathy we not only understand a person’s predicament and feel with them, but are spontaneously moved to help, if needed.\nPaul told me about his daughter, who works as a social worker in a large city hospital. In her situation, he said, she can’t afford to let emotional empathy overwhelm her. “My daughter’s clients don’t want her to cry when they’re crying,” as he put it. “They want her to help them figure out what to do now – how to arrange a funeral, how to deal with the loss of a child.” Empathic concern was the vital ingredient missing in the top-level response to Hurricane Katrina.\nRespectfully quoted from Daniel Goleman’s website below.","Given the emotional and demanding nature of social work, burnout is a significant problem among social workers.\nAs burnout often results in negative emotional and occupational repercussions, it is essential for social workers to recognize the warning signs, practice prevention, and engage in adequate self-care.\nThis article will delve into these topics, while also describing helpful resources from PositivePsychology.com. In doing so, it will provide social workers with the tools and information needed to carry out their invaluable work.\nBefore you start reading, we thought you might like to download our three Stress & Burnout Prevention Exercises (PDF) for free. These science-based exercises will equip you and those you work with, with tools to manage stress better and find a healthier balance in your life.\nThis Article Contains:\nBurnout in Social Work Explained\nSocial work is a noble profession. It is often entered into by those who wish to help vulnerable populations to achieve justice and receive vital support and services.\nGiven the nature of the job, social workers are often exposed to various aspects of human cruelty (e.g., abused or neglected children, domestic violence, etc.). As such, social work requires a high level of empathy and compassion.\nAdditionally, a career in social work is generally highly demanding, with large caseloads and minimal compensation. Imagine, for example, performing a job in which your caseload outweighs your time, your clients are victims of chronic abuse, and you barely earn enough money to pay your mortgage.\nYou may also have a supervisor who is overworked and cannot recognize your accomplishments or needs. This situation may cause compassion fatigue, which involves stress and emotional fatigue that results from the chronic use of empathy to help those suffering from trauma (Figley, 1995).\nThe concept of social work burnout is well exemplified in an article by Smullens (2012, p. 1), who noted how her social work supervisor often came home from work exhausted, telling his wife “They [his clients] feel better, but I surely do not.” This experience has also been referred to as ‘secondary or vicarious trauma,’ which occurs when social workers take on their clients’ stress and vulnerabilities (Wilson, 2016).\nOf course, those who enter helping professions are often highly empathetic, caring individuals. As such, being exposed to their clients’ chronic challenges and health disparities becomes emotionally and physically taxing. This is especially likely when workloads and resources are mismatched to the degree that making a meaningful change is unlikely.\nWhen this happens, social workers are at a heightened risk for feeling frustrated, depleted, and, ultimately, burned out. Indeed, the research literature supports a connection between high levels of burnout and stress among social workers relative to other occupations (Lloyd, King, & Chenoweth, 2002).\nIndeed, in a study including 751 social workers, three-quarters of participants experienced burnout during their careers (Siebert, 2005). Considering these correlations, it is essential that social workers know the warning signs of burnout and take necessary steps toward preventing it.\n16 Warning Signs of Burnout in Social Workers\nIf you’re a social worker who is concerned about potential burnout, here are several research studies that have identified warning signs to keep in mind:\n- Lack of enthusiasm about work\n- Reduced compassion or empathy for clients\n- Mismatch between job rewards (e.g., compensation and recognition) and performance\n- A non-collaborative workplace\n- Feeling a lack of job control, which minimizes autonomy\n- Depression symptoms\n- Job cynicism\n- Sense of resignation about work\n- Quick-temperedness with colleagues or family\n- Self-medicating behavior\nKim, Ji, and Kao (2011)\n- Role ambiguity\n- High work challenges\n- Lack of autonomy\n- High role conflict\nSchaufeli, Leiter, and Maslach (2009)\n- Poor fit between values of the social worker and those of the organization\n- Workload responsibilities out of balance with the social worker’s needs and priorities\nOverall, if you find you dread going to work each day and are exhausted when you return home, you may be suffering from burnout. If this is a concern, it may be helpful to ask yourself the following questions as outlined by the Mayo Clinic (2021):\n- Are you feeling disillusioned about your career?\n- Are you having difficulty concentrating?\n- Do you no longer find satisfaction in your achievements?\n- Are you having trouble sleeping?\n- Are you trying to numb or distract yourself from your feelings?\n- Are you irritable with clients or coworkers?\n- Are you experiencing physical symptoms such as headaches or digestive problems?\nAlong with recognizing the red flags above, it also is important to remember that increased burnout is associated with fewer years of social work practice (Weekes, 2011).\nTherefore, if you are dedicated to social work, but new to the job, hang in there. As long as you know the warning signs and practice plenty of self-care, you will probably feel better as you accumulate experience and confidence in your role.\nPreventing Burnout: The Importance of Self-Care\nHave patience with all things. But, first of all, with yourself.\nFrancis de Sales\nEngaging in adequate self-care is essential among social workers, as it helps to protect against the stress that accompanies repeated exposure to traumatized populations.\nAccording to Salloum, Kondrat, Johnco, and Olson (2015, p. 54), trauma-informed self-care (TISC) involves “being aware of one’s own emotional experience in response to exposure to traumatized clients and planning/engaging in positive coping strategies.”\nBy engaging in TISC, social workers benefit from coping strategies that help to moderate the negative impact of working with highly vulnerable clients (Salloum et al., 2015). Research has indicated that TISC is related to higher levels of compassion satisfaction and reduced burnout among child welfare case managers (Salloum et al., 2015).\nSimilarly, Weekes (2011) assessed 185 members of the National Association of Social Workers for both burnout and self-care. The results showed that depersonalization and emotional exhaustion were significantly lower among those who engaged in higher levels of self-care.\nEngaging in self-care represents an important way for social service workers to experience greater job satisfaction with a diminished likelihood of burnout.\n30 Self-Care Activities for Social Workers\nAs important as it is to have a plan for doing work, it is perhaps more important to have a plan for rest, relaxation, self-care, and sleep.\nResearchers investigating the role of self-care in preventing the likelihood of burnout among social workers have reported many effective self-care approaches, with over 30 presented below.\n- Engage in physical or behavioral strategies (e.g., dancing, hiking, sports, deep breathing, etc.)\n- Engage in relational strategies (e.g., spend time with pets; talk about feelings with colleagues, significant others, supervisors, etc.)\n- Engage in cognitive strategies (e.g., distract yourself with music, movies, etc.; avoid exposing yourself to stress or trauma when outside of work, etc.)\nSalloum et al. (2015)\n- Seek supervision\n- Attend trainings on secondary trauma\n- Balance your caseloads\n- Ensure work–life balance\n- Seek continuing education on the effects of trauma\n- Take advantage of agency resources\n- Set realistic goals\n- Attend therapy as needed\n- Engage in stress management activities, such as meditation\n- Be cognizant of your emotional response to traumatized clients\nHere are 17 more self-care suggestions:\n- Don’t bring work home. Home should be your respite from the day’s stress. Keep it that way by avoiding discussions or reminders of work once your shift is over.\n- Take advantage of vacation days. Always, always use your vacation days. It is a wonderful way to restore your emotional wellbeing, along with your connection with significant others. Besides, you earned that time off.\n- Talk to friends and family. Sharing with others is a great way to get a reality check, as well as to generate ideas about how to avoid burnout.\n- Find an artistic release. Whether it’s music, drawing, working with clay, or any other artistic endeavor, engaging in art enhances a sense of flow and happiness.\n- Get plenty of sleep. Adequate sleep is essential to emotional and physical health. Do not skimp on it.\n- Reward yourself. Whether it’s a vacation or simply a cup of tea, take the time to reward yourself for your hard work.\n- Read a good book. Reading is a healthy way to escape from a stressful day. By taking the time to read, you will feel more relaxed and might even sleep better.\n- Avoid self-medicating. If you find yourself craving a drink or some other drug at the end of the workday, this could escalate into a problem. Try to find healthier coping mechanisms, such as exercise or talking to a friend.\n- Get a massage. If you enjoy massages and they are feasible, then go for it. Massage helps with both physical and emotional tension, and it is also a great way to reward yourself.\n- Go on outings. Simply getting away for the day or the weekend is often highly restorative.\n- Make your health a priority. If you are putting your health last, both your work and your health will deteriorate. Always make time for sleep, exercise, doctor visits, and healthy meals.\n- Don’t be too hard on yourself. Social workers deal with terrible trauma. And while they make a huge impact, they can’t save everyone. Know that you aren’t a miracle worker, but are doing the best you can.\n- Spend time in nature. Many people feel invigorated by a hike in the woods, canoeing, bird watching, going to the beach, or being around animals. If you enjoy nature, get outside and reap the rewards.\n- Pamper yourself. Remember: You are performing a highly demanding job that takes a lot out of you. Be kind to yourself.\n- Go for walks. Along with the benefits of exercise, walking may take your mind off of your workday while providing fresh air.\n- Take on a new hobby. Regardless of your skill level or interests, doing something hands-on (e.g., woodworking, knitting, gardening, etc.) is always good for the soul.\n- Ensure that the job is the best fit for you. If you are feeling burned out despite experiencing an adequate work–life balance and engaging in plenty of self-care, it may be time to examine whether you are in the right field. Your job should be rewarding and not leave you emotionally depleted. Remember, there is no shame in exploring other opportunities if you are consistently stressed and dissatisfied.\n15 Minutes a day to prevent burnout – Paul Koeck\nPositivePsychology.com’s Helpful Resources\nWe have many terrific resources here at PositivePsychology.com that help to identify, prevent, and cope with burnout among social workers.\nTo get you started, here are three useful articles from our blog:\n- Warning Signs of Burnout: 13 Reliable Tests & Questionnaires\nIf you are worried that you might be experiencing burnout, this article will help you identify key warning signs. It contains multiple questionnaires, tests, inventories, and checklists to help you recognize the signs and symptoms.\n- Self-Care for Therapists: 12 Strategies for Preventing Practitioner Burnout\nThis article is specifically aimed at promoting self-care among mental health professionals. It contains a background regarding the importance of self-care among this group, along with real-life examples of self-care plans, self-care strategies, helpful books, worksheets, and tips.\n- 12 Social Work Books Every Practitioner Should Read\nSeeing that reading is a good way to relax and escape a stressful day, these 12 books are great suggestions.\n- Strengths-Based Approach in Social Work: 6 Examples & Tools\nThis article is a wonderful read to learn how to focus on the client’s strengths, rather than deficits. A great way for the practitioner to also tap into their own strengths.\nFree Stress & Burnout Prevention Exercises\nFurther, if you’re a social worker looking to avoid burnout, why not show yourself some care with our free Stress & Burnout Prevention Exercises Pack. These exercises can help you identify domains in which you may be at risk of suffering from stress, as well as the potential benefits of stress for growth.\n- Strengthening The Work-Private Life Barrier\nThis exercise aims to help you identify the behaviors, beliefs, and conditions that create metaphorical “holes” in the barrier between work and private life. By completing the exercise, you can better develop a solid barrier between work and private life to help restore a healthy balance between the two.\n- Energy Management Audit\nThis brief, 16-item assessment helps you assess your energy levels across the physical, mental, emotional, and spiritual domains. Upon completion, you will have gained clear insight into your energy strengths and deficits, building awareness of these energy levels’ effects on daily functioning.\n- The Stress-Related Growth Scale\nThis 50-item assessment tool assesses positive outcomes following a stressful event (i.e., stress-related growth). By reflecting on your results, you can consider the positive benefits of challenging experiences for your relationships, thinking, and coping.\nGet access to all three exercises by downloading the exercise pack today.\n17 Stress-Management Tools\nFinally, if you’re looking for more science-based ways to help others manage stress without spending hours on research and session prep, this collection contains 17 validated stress management tools for practitioners. Use them to help others identify signs of burnout and create more balance in their lives.\nA Take-Home Message\nAs social workers well know, “anyone who confronts the system day in and day out will tell you that residual trauma is real” (Barnett, n.d.).\nThe tireless and honorable work of those who give a voice to the vulnerable takes its toll in terms of compassion fatigue and burnout.\nFortunately, by knowing the red flags and engaging in adequate self-care, these outcomes may be avoided or diminished. In doing so, social workers will be better able to experience a rewarding career that is of invaluable benefit to individuals, families, and society as a whole.\nWe hope you enjoyed this article; don’t forget to download our three Stress & Burnout Prevention Exercises for free.\n- Barnett, B. (n.d.). Retrieved on June 30, 2021, from https://www.goodreads.com/quotes/tag/self-care?page=2\n- Brost, A. (n.d.). Retrieved on June 30, 2021, from https://www.goodreads.com/author/quotes/18384561.Akiroq_Brost\n- de Sales, F. (n.d.). Retrieved on June 30, 2021, from https://www.goodreads.com/quotes/842634-have-patience-with-all-things-but-first-with-yourself-never\n- Diaconescu, M. (2015). Burnout, secondary trauma and compassion fatigue in social work. Social Work Review, 14, 57–63.\n- Figley, C. (1995). Compassion fatigue: Coping with secondary traumatic stress disorder. Brunner/Mazel.\n- Freudenberger, H. (1975). The staff burnout syndrome in alternative institutions. Psychotherapy: Theory, Research, Practice, Training, 12(1), 72–83.\n- Kim, H., Ji, J., & Kao, D. (2011). Burnout and physical health among social workers: A three-year longitudinal study. Social Work, 56(3), 258–268.\n- Lloyd, C., King, R., & Chenoweth, L. (2002). Social work, stress and burnout: A review. Journal of Mental Health, 11(3), 255–265.\n- Mayo Clinic, (2021, June 5). Job burnout: How to spot it and take action. Retrieved on June 30, 2021, from https://www.mayoclinic.org/healthy-lifestyle/adult-health/in-depth/burnout/art-20046642\n- Salloum, A., Kondrat, D., Johnco, C., & Olson, K. R. (2015). The role of self-care on compassion satisfaction, burnout and secondary trauma among child welfare workers. Children & Youth Services Review, 49, 54–61.\n- Schaufeli, W., Leiter, M., & Maslach, C. (2009). Burnout: 35 years of research and practice. Career Development International, 14(3), 204–220.\n- Siebert, D. (2005). Personal and occupational factors in burnout among practicing social workers: Implications for researchers, practitioners, and managers. Journal of Social Service Research, 32(2), 25–55.\n- Smullens, S. K. (2012). What I wish I had known: Burnout and self-care in our social work profession. The New Social Worker. Retrieved on June 30, 2021, from https:///www.socialworker.com/feature-articles/field-placement/What_I_Wish_I_Had_Known_Burnout_and_Self-Care_in_Our_Social_Work_Profession/\n- Weekes, J. (2011). The relationship of self-care to burnout among social workers in health care settings (Doctoral dissertation, Walden University).\n- Wilson, F. (2016). Identifying, preventing, and addressing job burnout and vicarious burnout for social work professionals. Journal of Evidence-Informed Social Work, 13(5), 479–483."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:92f59514-bdeb-48cc-9173-1e39270a1369>","<urn:uuid:63fc7856-514d-43df-9f8d-cbf3d04d4aac>"],"error":null}
{"question":"How do the High Priest's ceremonial garments relate to memory preservation, and how does this concept connect to modern Holocaust remembrance practices?","answer":"The High Priest's garments included 'avnei zikaron' (stones of remembrance) on the shoulders of the ephod and a breastplate with twelve stones representing the tribes of Israel. These stones served to keep the tribes before God's gaze and remind the priest of his service to all Israelites. This ancient concept of preserving memory through physical markers connects to modern Holocaust remembrance practices, particularly through the Stolpersteine project. Started in 1992, these 'stumbling stones' are installed in pavements outside the former homes of Holocaust victims, bearing their names and life dates. Unlike traditional memorials, these stones create an immediate connection to the individuals who once lived there, making people literally 'stumble' upon history and remember those who were murdered by the Nazis.","context":["The list of what the High Priest should wear when carrying out his duties is long and detailed. The Hoshen (a breastplate); The Ephod, a kind of tunic made with gold, blue, purple and scarlet, fine twisted linen threads. It would have two onyx stones, each engraved with six of the names of the tribes of Israel, and they would be embedded in a gold setting on the shoulders of the garment; A gold frontlet to be worn on the forehead, with the inscription “Kodesh l’Adonai” (Holy to God); A fringed tunic, a headdress, a sash, and linen trousers. The Hoshen was fixed by chains to the shoulders of the ephod and carefully connected, the urim and tumim were placed within it, and twelve different precious stones arranged in four rows of three, one for each of the tribes of Israel.\nThe clothing was fringed, with pomegranates and golden bells around the hem of the robe so that it would make a sound when the High Priest walked in the sanctuary, and people would be able to hear him.\nIf all this sounds a little familiar, it is because we dress our scrolls in similar fashion. Tunics of rich materials, beautifully embroidered; crowns and bells – called rimonim, pomegranates, that tinkle when we carry it; a breastplate – hoshen.\nSeveral times we are told that the High Priest’s clothes are for honour and beauty – kavod v’tiferet. And we have taken from this the idea of adorning our synagogues and Sifrei torah for the same purpose – hiddur mitzvah – beautifying a mitzvah -being the principle behind the decoration of our ritual objects, about the three statutory meals on Shabbat, about creating an aesthetic in our lives that not only glorifies God but makes us more aware of the beauty of our world.\nThere is much of the language of the text that we don’t really understand: – what exactly is an ephod? Why did the priest wear a gold engraved plate on his forehead? Why would having bells and pomegranates on the hem of his robe mean that he would not die? What really were the urim and the tumim? Where they objects of divination? How were they used and how does that fit into the ritual system being designed here? There are so many opaque words and unanswerable questions in this text, but this year one particular expression caught my attention:\nוְשַׂמְתָּ֞ אֶת־שְׁתֵּ֣י הָֽאֲבָנִ֗ים עַ֚ל כִּתְפֹ֣ת הָֽאֵפֹ֔ד אַבְנֵ֥י זִכָּרֹ֖ן לִבְנֵ֣י יִשְׂרָאֵ֑ל וְנָשָׂא֩ אַֽהֲרֹ֨ן אֶת־שְׁמוֹתָ֜ם לִפְנֵ֧י יְהוָֹ֛ה עַל־שְׁתֵּ֥י כְתֵפָ֖יו לְזִכָּרֹֽן:\nYou shall place the two stones on the shoulder-pieces of the ephod, remembrance-stones for the children of Israel. Aaron shall carry their names before God on his two shoulders as a remembrance. Exodus 28:12\nוְנָשָׂ֣א אַֽ֠הֲרֹ֠ן אֶת־שְׁמ֨וֹת בְּנֵֽי־יִשְׂרָאֵ֜ל בְּחֹ֧שֶׁן הַמִּשְׁפָּ֛ט עַל־לִבּ֖וֹ בְּבֹא֣וֹ אֶל־הַקֹּ֑דֶשׁ לְזִכָּרֹ֥ן לִפְנֵֽי־יְהוָֹ֖ה תָּמִֽיד:\nAnd Aaron shall bear the names of the children of Israel in the breastplate of judgment upon his heart, when he goes in to the holy place, for a memorial before the Eternal continually. (28:29)\nIt was, at first, the two engraved stones on the shoulders of the ephod – “avnei zikaron” – “stones of remembrance” that I noticed – avnei zikaron. I have recently returned from Lausanne, where with my brother and sister we dedicated a new stone on the grave of my grandfather, who had died there from damages he had originally acquired in Dachau. Having eventually got to a clinic in Switzerland, stateless and without access to any of his assets, he had died and been buried by the community there. My grandmother had arranged a stone to mark the grave, my father had had it repaired, but on a recent visit we saw that his grave was essentially unmarked – the composite the stone had been made from had not held the letters of his name. Here, to all intents and purposes, lay the body of an unknown man.\nWe arranged a stone to go onto his grave, and while the stone on a grave is usually called in Hebrew a “matzevah”, from the standing stone marking the grave of the matriarch Rachel, this felt more like an even zikaron, a stone to provoke memory. We felt it was important to not only mark the grave and give our grandfather back his name, but to create something that would cause an onlooker to think about him and to learn something of his essence. So we added his title – Landgerichstrat – County Court Judge. And we added the name of my grandmother buried in Lugano, of my father buried in Bradford, and the name of his aunt Helene who died in Theresienstadt. We added the dates of their lives, their relationship to my grandfather and the places where they were born and died. And at the foot of the stone is the acronym found on so many Jewish graves – taf nun tzaddi beit hei – t’hi nishmato tzrurah bitzrur ha’hayim – may their souls be bound up on the threads of life.\nSeventy years after his death, we, his descendants whom he never knew and could not even have imagined, found great meaning in creating for him an even zikaron – a memorial stone that not only gave him back his name, but in some way brought him back into the fabric of life. It gave him a measure of dignity; it recorded that here lay a man who loved and was loved, who had had learning and held a respected career, whose family had become scattered – and worse – because of forces we can still not really understand.\nSo much memory was encapsulated in the engraving. Four names and their relationship to the man lying there. A status in society; six towns in four different countries. We stood around that snowy grave under a winter sun and told family stories, traced the journey that had led this man whose family had been in the Lower Saxony area for hundreds of years, to a lonely grave far from those who had loved him. We remembered our father whose yahrzeit, like that of his father, fell that week and how, through him, we had come to know and root ourselves in a world that no longer really exists, yet continues in memory, in some artefacts, and in words.\nI have consecrated many gravestones in cemeteries in several countries on different continents, as well as memorial plaques in libraries and synagogues – of family, friends and congregants. But I never understood as I understood then the power of a stone that records and remembers when all else seems to have passed into history, the power of avnei zikaron.\nThere is a strong idea in Judaism that a person is not forgotten as long as their name is remembered. This is why the museum dedicated to the Shoah in Israel is called Yad v’Shem – a name taken from Isaiah (56:5) which reads “To them will I give in my house and within my walls a place and a name (Yad v’Shem) better than sons and of daughters: I will give them an everlasting name that shall not be cut off” – and is designed to hold memory, to be a place which records and names all those who have no descendants to memorialise them, no one to speak their name and tell their story. Talmud says that when we teach what we have learned from someone else, we do so b’shem omro- in their name – and Talmud tells us the lips of deceased teachers move in the grave when we do so – they are continuing to teach and so still attached to life. We name our children for dead relatives; we blot out the name of Amalek from under heaven – (quite literally in the case of torah scribes who test their pens by writing the name of Amalek on some parchment and crossing it out). The book of Proverbs tells us that “the memory of the righteous is a blessing, but the name of the wicked rots away (shall be forgotten.)” Again and again, remembering someone’s name is seen as synonymous with keeping them from the ultimate oblivion of death;\nThe stones on the breastplate of the High Priest that kept the twelve tribes of Israel before the gaze of God also had the effect of reminding the priest that his service to God was in the name of and on behalf of every single Israelite. And the Midrash tells us that they were avnei zikaron not only in order that God would remember, but that the Priests would remember.\nThe Stolpersteine project is another way to keep alive those whose memory was almost entirely obliterated. The artist Gunter Demnig began a project in 1992 to remember the victims of National Socialism, by installing commemorative brass plaques in the pavements of their last address of choice. The ordinary cobblestones on the pavements outside their homes are replaced, putting in their place stones with a plaque that bears a simple inscription – the name, date of birth and the date and place of death, if known of each individual. One stone per person. The stones are positioned outside the houses of Jews, Roma, Sinti and others who were murdered by the Nazi regime. Stolpersteine, stumbling stones, can be found in Germany, Austria, France, Hungary the Netherlands, Belgium the Czech republic, Norway, Italy, the Ukraine, Greece, Sweden, Denmark, Finland and more recently Spain…. What began as a mainly artistic endeavour has turned into a powerful aid for people to create memory, to bring back to life in some way those who disappeared, murdered, their bodies unburied and desecrated. It is a measure of the power of this project that to get one installed will take well over a year, so long is the waiting list of those who wish to commemorate family.\nThe original meaning of the word stolpersteine used to be “an obstacle”, something that prevented you getting to your goal; but that has changed, the focus is drawn to the immediate now rather than on the horizon. They are designed to provoke thought, to make us see the world around us a little differently for a moment, as the people who once walked those streets until taken away and murdered, come to focus and live for us for a short while. So now one stumbles over the stone in the pavement and stops, reads, thinks of the individuals and the families who lived in the house or apartment adjacent. Tragically they are also the focus of those who do not want to be reminded, do not want to accept any role in remembering. We know that in December last year twenty of them, which commemorated members of two Italian Jewish families – the Di Consiglio family and the Di Castro family – were hacked out and stolen in Rome, others have been defaced or vandalised.\nWe are told that the High Priest Aaron wore bells on his clothing so that he didn’t die. It is not really clear how death was prevented, but what is clear is that the people could hear him moving around in that sacred space. People being aware of him somehow kept him from death. It is our memories and the stories we tell of those we love that keep them living in some very real way. Their bodies may die but the memory lives on strongly. And the best way we can keep their memory in public attention is to inscribe it on a stone – their names, relevant dates, reminders of the person they were, reminders that they had lived a life, had been bound up in the threads of a fabric in which we too are bound up.\nThe Avnei Zikaron in the clothing of the High Priest were there primarily to remind both God and human beings of the importance of our history together, of the relationship to each other that has given meaning to both parties. Stones of memory mean that as long as we will not forget each other we won’t completely die, and that when we die we will not be completely forgotten. And that matters.\nThe acronym “taf nun tzaddi beit hei” is found on Jewish graves the world over, and refers to the idea that the life being recorded here is not completely ended, but its threads are connected to the continuing future – be it through descendants or stories, be it through the impact the person had on others, their teachings, their behaviour, their actions. After we had recited the psalms, sung the El Malei Rachamim, spoken the words of Kaddish Yatom the mourners kaddish, after we had shared memories and stories of a man we never knew except through his impact on our father, and stories and memories of our father, our grandmother, and the elderly woman murdered in Theresienstadt after 80 years of life in a quiet village tending the family synagogue and the family shop, we bent down and placed on my grandfather’s grave some small stones, one for each of us, one for our parents, and one for each of our children. And then one for the soon to be born baby of the next generation of our family. Stones put down on sacred space as avnei zikaron, for life goes on.\nsermon at lev chadash February 2019","The internationally recognized date for Holocaust Remembrance Day corresponds to the 27th day of Nisan on the Hebrew calendar. In Hebrew, Holocaust Remembrance Day is called Yom Hashoah. In 2017 it falls on April 24th, my father’s birthday. He would have turned 78 years old.\nYom Hashoa is about remembering the victims of the Holocaust. It is about telling the story, so we will never forget.\nGermany has started telling a story with “Stolpersteine” (Stumbling Stones) since 1992. The story continues to be told, as more and more Stolpersteine are being embedded in sidewalks. As one walks along German cities, one can “feel” these Stolpersteine and is reminded that a Jewish individual and their family lived at that spot and fell victim to the Nazis. It is a different type of story than a memorial, a monument, a plaque or a book. You don’t hear or watch the story, you see and feel the story of how lives were taken, families broken apart and history changed forever.\nStolpersteine are explained in Wikipedia as:\nA stolperstein from German, literally “stumbling stone”, metaphorically a “stumbling block” or a stone to “stumble upon”, plural stolpersteine) is a cobblestone-size concrete cube bearing a brass plate inscribed with the name and life dates of victims of Nazi extermination or persecution. The stolperstein art project was initiated by the German artist Gunter Demnig in 1992, and is still ongoing. It aims at commemorating individual persons at exactly the last place of residency—or, sometimes, work—which was freely chosen by the person before he or she fell victim to Nazi terror\nLet me tell you my family’s story, so we will not forget…\nMy grandfather, Siegfried Rosenthal, never spoke to me about his experiences the night of November 9th, 1938, Kristallnacht. The night that the SS came to his door and arrested him in front of his aging father, seven year old son and pregnant wife for simply being a Jew and taken to the Concentration camp Oranienburg. He did write his thoughts down at one point. I am glad he did, otherwise his story, his voice would have been lost to me, my daughters and grandchildren. Little did I know that there was much more to the story…\nThat brings up the question of each of our own responsibility of telling our stories, so they will not be forgotten. My grandfather wrote his story on a typewriter. That piece of paper was passed down to my father and then to me. How long, how many generations will it take before that paper gets lost, destroyed or vanishes forever?\nWhen my own children were in Middle School, I offered to give a presentation about my family’s story to their Social Studies class. Traditionally (here in the US) it is the time that students get an introduction to the Holocaust. Back in 2002, I created a PowerPoint slidedeck to aid me in the presentation, a tool I had available at that time. I was continued to be asked to share this presentation many times in person in other classes and schools over the years that followed. In 2009, I shared the slidedeck on Slideshare.net.\nIn 2010, as I continued to think about the importance of storytelling, media literacy and teaching students with “their” media in order to reach theme and make connections. I felt, it was time to once again re-tell my grandfather and our family’s story. Trying to make the words jump off that paper, that my grandfather wrote so many years ago with a typewriter. I am sad, that the technology was not as readily available before 1993, when he passed away. I could have filmed and recorded him easily to capture him, his personality, his voice…\nThis is the story, we knew about until a few months ago in November 2016, I received an email from David Jany inquiring if I was a descendant of Max and Siegfried Rosenthal? We started an email exchange and he let me know that he was a historian for the German Firefighter Association and that he had been researching my great-grandfathers story (Max had been a prestigious member of that association for over 40 years and one of the “best known citizen” of the city of Wattenscheid). He found my slidedeck from almost 10 years ago and was able to get in touch with me. In one of our conversations, which quickly moved on from email to phone calls and Facebook video calls, he shared that his area of interest was to find out more about the volunteer firefighters and their relationship with their Jewish comrades once the Nazis swept the country. This was how my great-grandfather became the focus of his research. (I remember hearing from an older cousin of my father’s who was present at my great-grandfather’s funeral, that no one from his firefighter or WWI comrades attended the funeral out of fear for repercussions for themselves from the Nazis.)\nDavid Jany’s research filled in many black holes (or misinformation that we had) about my great-grandfather’s history. There still seems to be mysterious circumstances surrounding his death, beyond that we had assumed that he had died of a broken heart, after his realization that his son, Siegfried, was taken to a concentration camp during the Kristallnacht and he blamed himself and his refusal to leave Germany in 1936.\nWe found out that:\n- his official death certificate said he died on December 21, 1938 (although his gravestone says December 26, 1928)\n- he might have died from the consequences of having been beaten (not from old age or consequences of an STD as stated on a death certificate)\n- his house was “sold” on December 27, 1938 -immediately a few days after his death. (Was this orchestrated? Was my grandfather still in the concentration camp Oranienburg, when the sale of the family home was taking place? What happened to my pregnant grandmother once the house was sold from underneath her? Where did she stay until they were able to escape in June 1939 towards South America?)\n- the family home still exists today. Hüller Strasse 10 in Wattensheid (City of Bochum today)\n- my great-grandfather’s gravestone was destroyed at the Jewish Cemetery of Wattenscheid during the Nazi reign…\n- …and a new one (with his wife’s name included) was placed at the “Denkmal der Jüdischen Gemeinde Dortmund” .\nDavid Jany has started the paperwork to sponsor a Stolperstein in my great- grandfather’s name. The brick with the brass plate bearing his name should be embedded into the sidewalk in front of their last home next year (2018). (Maybe I can take my granddaughter to the ceremony…?)\nStolpersteine helping to tell stories and helping us not to forget."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:2e3d2692-df8b-459e-8aec-6a8a0bdcbfc2>","<urn:uuid:82fb6fcd-591e-4689-9efd-fcf69fa4ceaa>"],"error":null}
{"question":"I'm working on a research paper about educational flexibility and structure. How does Montessori education balance student freedom with academic requirements, and what evidence exists about its effectiveness across different student populations?","answer":"Montessori education maintains a careful balance between freedom and structure. While students have the liberty to move about and select their work, the environment is actually highly structured with specifically constructed materials and consciously chosen classroom arrangements. The method has proven successful across all socioeconomic levels, including gifted children and those with developmental delays or disabilities. Research published in Science showed that Montessori students achieved equal or superior academic results compared to traditional school students, despite not having regular tests and grades. The system emphasizes freedom within limits, allowing children to work independently or collaboratively while still mastering essential skills and knowledge. This approach particularly benefits students who might struggle in traditional settings - some children with behavioral and learning issues in conventional classrooms show improvement in Montessori environments, where they can engage in self-directed learning at their own pace.","context":["Montessori Education: `better than traditional schooling,’\n– Daily Telegraph Sept 29th 2006\nBy Roger Highfield Science Editor\nThe century-old Montessori education method is more successful than traditional teaching methods, according to research published today. The study, which attempts to put education on a scientific basis by comparing children at a private inner-city Montessori school with those who attended traditional schools, suggests that Montessori education equips children with better social and academic skills.\nMontessori education was pioneered by Dr Maria Montessori, who used it in a slum in Rome a century ago. The new assessment of its effectiveness is published today in the journal Science. The method is based on observing young children and learning about their characteristics and needs. It puts children of various ages in the same classroom, and features special educational materials, student-chosen work, a collaborative environment with student mentors, absence of tests, and instruction in social skills. In America alone, more than 5,000 schools use the Montessori method, and researchers focused on one in Milwaukee, Wisconsin, that serves children in an inner-city area.\nFor the study, 59 students from families of similar income levels enrolled at the Montessori school while 53 students enrolled in other schools using traditional methods. Children were evaluated at the end of the two most widely used levels of Montessori education: primary (3 to 6year-olds) and elementary (6 to 12-year-olds) for their cognitive and academic skills, and for their social and behavioural skills.\nEvaluating Montessori Education\nAngeline Lillard and Nicole Else-Quest in ‘Science’\nAn analysis of students’ academic and social scores compares a Montessori school with other elementary school education programs.\nFor a few measures there were no differences but in many “we found significant advantages for the Montessori students in these tests for both age groups”, one of the authors Prof Angeline Lillard, of the University of Virginia, said. “Particularly remarkable are the positive social effects of Montessori education. Typically the home environment overwhelms all other influences in that area.” Among the five-year-olds, Montessori students proved to be significantly better prepared for elementary school in reading and mathematics than the non-Montessori children. They also tested better on “executive function”, the ability to adapt to changing and more complex problems, an indicator of future school and life success.\nMontessori children also displayed better abilities on the social and behavioural tests, with five-year-olds demonstrating a greater sense of justice and fairness. And on the playground they were much more likely to engage in “emotionally positive play” with peers, and less likely to engage in rough play. Among the 12-year-olds, the Montessori children, in cognitive and academic measures, produced essays that were rated as “significantly more creative and as using significantly more sophisticated sentence structures”. The Montessori and non-Montessori students scored similarly on spelling, punctuation and grammar, and there was not much difference in academic skills related treading and mathematics. This parity occurred despite the Montessori children not being regularly tested and graded.\nIn social and behavioural measures, 12-year-old Montessori students were more likely to choose “positive assertive responses” for dealing with unpleasant social situations, such as having someone jump a queue. They also indicated a “greater sense of community” at school and felt that students helped, cared for and respected each other. The authors concluded that, “…when strictly implemented, Montessori education fosters social and academic skills that are equal or superior to those fostered by a pool of other types of schools”.\nProf Lillard plans to continue the research by tracking students from both groups. But the method is not without criticism. Some complain that it is “too free” while others see the Montessori principle of “freedom within limits” as stifling creativity. Some see Montessori as elitist while others question its teaching priorities, decry children spending time on such menial tasks as washing tables or arranging flowers, or the lack of homework making it difficult for parents to get a clear picture on how well or poorly a student was doing.","Frequently Asked Questions about Montessori\nHow is Montessori different from other schools?\nMontessori schools are based on a teaching method developed by Dr. Maria Montessori in the 1950s. A physician specializing in both pediatrics and psychiatry, Dr. Montessori believed that every child has incredible potential for learning and she designed a classroom to maximize the experience specifically for them – from the materials to the size and scale of the classroom itself. Such materials in each classroom are specifically constructed and their location consciously chosen.\n“Ultimately, what sets Montessori children apart is not their brilliance… but their understanding of what they’ve learned, the joy on which they learn it and the ability to tackle future challenges.” – Edward Fidellow\nMontessori programs are created to address the developmental characteristics normal to children in that stage:\n- Classes are organized to encompass a three-year age span, which allows younger students the stimulation of the older children, who in turn benefit from serving as role models.\n- With two-thirds of the class returning each year, the classroom culture remains quite stable.\n- Working in the same class for three years allows students to develop a strong sense of community with classmates and teachers.\nWhy do Montessori schools ask younger children to attend five days a week?\nTwo and three day programs are often attractive to parents who do not need full-time care; however, five day programs create the consistency that is so important to young children and which is essential to developing strong Montessori programs. Since the primary goal of Montessori involves creating a culture of consistency, order, and empowerment, most Montessori schools will expect children to attend five days a week.\nIs Montessori unstructured?\nAt first, Montessori may look unstructured to some people, but it is actually quite structured at every level. Just because the Montessori program is highly individualized doesn’t mean that students can do whatever they like. Like all children, Montessori students live within a cultural context that involves the mastery of skills and knowledge that are considered essential.\nWhy is there so much emphasis on freedom and independence in Montessori?\nChildren learn best by doing. Montessori children are free to move about, working alone or with others at will. Freedom is critical as children begin to explore. Our goal is less to teach them facts and concepts, but rather to fall in love with the process of focusing their complete attention on something and mastering its challenge with enthusiasm. The prepared environment of the Montessori class is a learning laboratory in which children are allowed the liberty to explore, discover, and select their own work. Work assigned by adults rarely results in such enthusiasm and interest and independence as does work that children freely choose for themselves.\n* Questions and answers (in part) excerpted from The Montessori Way by Tim Seldin and Paul Epstein\nIs Montessori for all children?\nThe Montessori system has been used successfully with children from all socio-economic levels, representing those in regular classes as well as gifted children, children with developmental delays, and children with emotional and physical disabilities. There is no one school that is right for all children, and certainly there are some children who may do better in a smaller classroom setting with a more teacher-directed program that offers fewer choices and more consistent external structure. Children who are easily over stimulated, or those who tend to be overaggressive, may be examples of children who might not adapt as easily to a Montessori program. However, some children who have behavior and learning issues in a traditional setting do not have those issues at HIlltop. In our Montessori environment with freedom of movement and learning at ones own pace, students are engaged and motivated in a more productive and enjoyable way. Please call and set up a visit to see our classrooms in action."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:77635ada-03cc-4012-910b-bf395ce13376>","<urn:uuid:550c249b-5b41-405a-9d7d-eb8acd549fd6>"],"error":null}
{"question":"As a medical device manufacturer, I'm curious: how does silicone's versatility in medical applications compare with its waste management requirements in the healthcare industry?","answer":"Silicone is highly versatile in medical applications - it's used for precision O-ring seals, splash covers, dust covers, and implantable wiper seals. It can be sterilized through various methods (steam, ETO, Gamma) and can be pigmented for easier identification during assembly. Regarding waste management, the requirements depend on whether the waste is classified as hazardous or non-hazardous. A 'Cradle-to-Grave' approach makes waste generators responsible for proper disposal in compliance with both federal and state regulations. While federal law primarily governs hazardous waste through the Resource Conservation and Recovery Act (RCRA), states may impose stricter requirements. Documentation of proper disposal through tracking systems like Bills of Lading or Hazardous Waste Manifests is essential for compliance and audit purposes.","context":["Silicone rubber is a very versatile material, especially when considering its uses in the medical industry. Silicone is used to create a variety of items – precision O-ring seals, splash and dust covers, implantable wiper seals, and more. Silicone holds up well to many forms of sterilization, including steam, ETO, and Gamma.\nWhile many silicone parts are unpigmented (naturally clear silicone), most customers choose to have the material pigmented. This allows for easier identification and inspection of parts during assembly. Colored silicone is also used to create a particular “look” on the overall finished product, which is possible because silicone is available in a wide variety of standard and custom-matched colors.\nThe common practice of coloring silicone has led to a frequent question: does adding pigment to the silicone affect its physical properties? It is a valid concern. Recently, a small study was conducted to proactively alleviate some of that concern.\nTypically, most silicone is pigmented with colorant in the range of 1% to 3%. For our comparison, we evaluated the same batch of liquid silicone over the range of 0% to 4% color at 1% intervals. All tests slabs were cured for 20 minutes at 350°F and then post-cured for four hours at 400°F. All testing was performed on 70 durometer Shore A liquid silicone rubber with the addition of a standard cobalt blue pigment.\nThe first property we checked is the same one most people review when beginning to look at physical properties: durometer. For this test, a handheld gauge based on the ASTM D2240 Shore A scale for three stacked tensile test specimens was used and repeated three times for each of the five test mixes.\nFigure 1: Shore A hardness vs. color percentage\nTensile and Elongation\nSince durometer is a non-destructive test, we could then use those specimens to run three tensile tests to the ASTM D412 standard. In our setup, we used an MTS QTest 10 to pull the specimens in tension at a speed of 20 inches per minutes.\nFigure 2: Tensile Strength (psi) vs. Color percentage and Elongation vs. Color Percentage\nThe next property tested was tear resistance. Silicone applications are generally somewhat sensitive to tearing. Any degradation should be more apparent here. The same MTS QTest 10 machine was used to run the ASTM D624 Tear resistance test, as well as Die B, which has a small slit that cuts into the test specimen to create a stress concentration at the point of most geometric stress.\nFigure 3: Tear resistance (ppi) vs. Color percentage\nThe last test is one that is considered crucial to any sealing application: compression set. Compression set is important because it is an accelerated aging test that helps quantify the material’s ability to resist permanent deformation.\nWe used the plied disk method of ASTM D395 to conduct a compression set test. This requires three stacks, cut from the slabs, of three disks. The heights of the stacks are then measured so that they can be compressed 25% in a fixture. In this instance, they were put in an oven for 22 hours at 175°C. The stacks were then removed from the fixture and allowed to sit for 30 minutes before a final measurement was taken.\nComparing the initial heights before testing to the compressed heights after testing determines the final percent loss in height (permanent deformation) over that compression period, or the ‘set’ that the material took.\nFigure 4: Compression Set (%) vs. Color percentage\nIt is apparent from the graphs there are no significant changes to the materials physical properties over the typical pigmentation range of silicone rubber. As long as the silicone is within the normal range of colorant addition, then there should not be any significant physical property variation outside the normal variation of silicone rubber. Therefore, during process validations and risk assessment, color variation can be considered to have low severity.\nUltimately, it is the customer’s responsibility to ensure correct material choice. Contact our team of Apple Rubber engineers or scientists — we will be happy to discuss your application and recommend a material. We have years of experience designing and developing rubber parts whose functions vary from mundane hose washers to seemingly magical medical devices.","/ IN THIS BLOG\nIs a medical waste certificate of destruction required every time your facility or business disposes of medical waste? It depends.\nIf you generate medical waste – such as hazardous or infectious waste that requires special treatment before disposal, be aware of your legal obligations. It’s always a good idea to develop and implement best practices for medical waste disposal in order to avoid issues of non-compliance. That means paying attention to documentation that proves you’re following the rules. Case in point - the certificate of destruction.\nWritten documentation or evidence that medical waste has been legally and correctly disposed of could be necessary in an audit situation. Therefore, it's important to know what's required of you and then follow those rules to the letter. Not doing so could result in fines—or worse.\nYour first step is to identify which types of medical waste require special handling or disposal processes, such as hazardous waste.\n01 / Hazardous Waste versus Non-hazardous Waste\nWhether or not you legally need a certificate of destruction depends on what type of waste you're disposing of. If, for example, you're disposing of hazardous waste, verify regulations regarding disposal. Rules for hazardous waste management and disposal are under the purview of the Resource Conservation and Recovery Act (RCRA). Turn to guidelines for hazardous waste in the Federal Code of Regulations, Title 40 Parts 260 through 273.\nIn addition to federal guidelines, each state also has their own regulations regarding medical waste disposal, so make sure that you carefully review those rules to ensure compliance. The ‘Cradle-to-Grave’ approach is used by both federal and state agencies when it comes to waste generators, regardless of type of waste. Essentially, this approach makes it the responsibility of the waste generator to ensure that waste is managed and disposed of properly and in compliance with all state and federal laws. It is your responsibility, and any issues of non-compliance (and resulting fines) will fall on your shoulders.\n02 / Who Requires Medical Waste Disposal Documents?\nMedical waste disposal documents prove that you have followed the rules. For this reason, it is always recommended that you obtain documents or certificates that verify that your medical waste has been properly disposed of. The hazardous waste management or disposal facility engaging in any destruction processes should send all proper documentation to you within forty-five days.\nIt is standard practice for the waste management or disposal facility to send such documents directly to the generator (medical practice, hospital, business, industrial facility) of the hazardous waste—even if that business is using the waste management company to help facilitate proper waste transport and disposal.\nFor non-hazardous waste, this documentation is not legally required. However, that being said, if an auditor ever requests a documents to verify shipment and compliant destruction or disposal of any waste for any given reason, it's always better to be safe than sorry in those situations.\nTo that end, make sure any waste management company that you deal with is knowledgeable and reputable enough to properly keep these records. That way, if you ever need them in the future, they'll be readily available to you.\n03 / Destruction of Medical Waste - State versus Federal Laws\nEvery state also makes their rules for medical waste disposal, and they can be stricter than those of the federal government. While every state must follow the regulations of the federal government, it’s also important to keep in mind that you also need to follow those of your state. You cannot pick and choose. You must follow both to maintain compliance.\nState regulations for medical waste management, including hazardous and/or infectious waste management and disposal are often found within the state’s Department of Natural Resources. They primarily adopt the regulations of the federal Environmental Protection Agency (EPA).\nHowever, the state you're operating in (and any state where you waste will be transported through) matters as well. Any given state is merely required to meet the minimum federal standards. As mentioned, that means that any state is free to enforce stricter laws.\nSouth Carolina, for example, has more stringent requirements than many other states. It requires written proof of destruction with every shipment - regardless of the waste type. Don't be surprised if a state even has requirements as strict as needing the name of the individual actually operating the medical waste autoclave!\nTo ensure your business is in full compliance during every step of the medical waste management process, it’s important to stay up-to-date on both the state and federal guidelines. This can be time-consuming and challenging, as laws are consistently changing. However, if you're at all unsure about federal or state regulations, don’t guess. Get in touch with an experienced and reputable medical waste or hazardous waste management company such as MCF Environmental Services.\nCompliance is key to avoiding not only safety issues but poor medical waste management practices that can result in massive fines by both federal and state governmental agencies.\n04 / Proving Medical Waste has been Disposed of Properly\nOne of the surest ways to ensure that your medical waste has been disposed of properly (and in proving it), is through tracking. While the federal government (EPA) no longer has the authority to track medical waste, state environmental programs do.\nMost require a Bill of Lading for transport of non-hazardous or non-regulated waste and a Hazardous Waste Manifest for transport of hazardous/infectious waste. These are the best solutions to track your medical waste. Know the difference of details required on a bill of lading versus a hazardous waste manifest.\nIn this way, the medical waste generator is assured that waste has been managed properly and compliantly, and will have the documents to prove it.\n05 / Turn to the professionals for paperwork compliance\nThe best approach to proper medical waste disposal protocol is to be proactive. Ensure you're doing everything right before you're confronted with the possibility of an audit. One of the easiest ways to accomplish this is to seek out the advice and/or services of an experienced and professional waste management company that knows not only federal law but state regulations when it comes to medical waste tracking and disposal processes.\nWhen looking for help, keep the following points in mind:\nDepending on your location and the type of waste, a certificate of destruction might not be necessary. That being said, you want to work with a company that can easily and readily supply the necessary paperwork should you need it at any point in the future.\nBe wary of any company that doesn't keep proper documentation on file. This could mean big problems for you during an audit.\nWork with a company that's knowledgeable about the full range of paperwork. A bill of lading, for example, is going to show material was hauled away. It won't show what specific facility destroyed what specific shipment on what specific day.\nGetting dinged on or even failing a waste audit has real financial consequences in addition to damage to reputation. A facility that engages in non-compliant medical waste management processes may be liable for both federal and state fines and penalties as well as having funds cut or withdrawn. If deliberate, repeated, or egregious non-compliance is determined, a business, healthcare provider, or facility may lose licensing or even risk a facility shut-down.\nFor more information about medical waste documents that you may require for your medical waste management and disposal processes, contact one of our knowledgeable representatives for information or guidance. MCF Environmental Services is a waste management Atlanta-based business with over thirty years of industry experience."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:02238051-c8b4-4d4b-8027-7d6835670605>","<urn:uuid:5a582b8e-41f0-416c-841c-24a1acfeaefa>"],"error":null}
{"question":"Which is more space-efficient for residential installation: a geothermal heat pump or an air source heat pump?","answer":"Air source heat pumps are more space-efficient for residential installation, as they only require a small external unit measuring approximately 1m x 1m x 30cm mounted outside the property. In contrast, geothermal heat pumps need extensive space for either horizontal loops requiring 400-600 feet of piping per ton in 4-7 feet deep trenches, or vertical boreholes 150-400 feet deep, making them more challenging for properties with limited space.","context":["A Geothermal Heat pump is an electrically powered system that taps the stored energy from the earth. These systems are Energy Efficient, Cost Effective, Reliable, Quiet, Safe, Clean, and Environmentally Friendly. Geothermal heat pumps use the earth's constant temperature to provide heating, cooling, and hot water for homes and commercial buildings.\nThe geothermal heat pump, also known as the ground source heat pump, is a highly efficient renewable energy technology that is gaining wide acceptance for both residential and commercial applications. Geothermal heat pumps are used for space heating and cooling, as well as water heating.\nThe greatest advantage of these systems is that they work by concentrating naturally existing heat, rather than producing heat through combustion of fossil fuels. Installing a Geothermal system not only saves you money on your energy bills, but it also contributes to the efforts of reducing our overall fossil fuel consumption.\nThis type of design is cost effective on smaller projects or where there is sufficient space for the loop. Trenches, four to seven feet deep, are created and a series of parallel plastic pipes are laid inside of them. These loops are manifolded and connected to the heat pump. The fluid is then circulated, absorbing or rejecting heat to the earth depending on the mode of operation. A typical horizontal loop will be 400 to 600 feet long for each ton of heating and cooling but will vary according to the soil type and the layout of the piping.\nThis type is used mainly in commercial buildings where space is limited. Vertical holes 150 to 400 feet deep are drilled in the ground, and a single loop of pipe with a U-tube at the bottom is installed. The borehole is then sealed with grout to ensure good contact with the soil. The vertical ground loops are then connected to a horizontal underground header pipe that carried fluid to the unit. The earth's temperature is more stable farther below the surface which is an advantage for the system. Vertical ground loop fields may be located under the house and garden lots. The life expectancy is in excess of 50 years.\nThis type of design is economical when a project is located near a body of water. Fluid circulates through polyethylene piping in a closed system, just as it does through ground loops but in this case underwater. The pipes may be coiled in a slinky to fit more surfaces into a given amount of space. The lake needs to be a minimum size and depth depending on the load. Lake loops have no adverse impact on the aquatic system.\nThis type of design is only possible if there is sufficient ground water available in a well, lake or river in the area. The water must be of good quality. Local codes may limit the use of this system in certain areas. The system is open which means that water is pumped directly into the geothermal unit and then discharged either into a return well or a body of water. The water quality remains unaffected.","An Air Source Heat Pump (ASHP) captures heat from the air circulating outside your property and boosts the temperature to provide heat and hot water. It is a very effective, low carbon way of heating your home and can be combined with a number of renewable and energy efficient technologies to deliver a truly sustainable home or commercial premises.\nAn ASHP can be installed in new and existing properties and the Government’s Renewable Heat Incentive provides cash payments for all eligible installations, which makes the investment even more attractive.\nIn warmer months ASHPs can operate as air conditioning units, meaning that you benefit from your heating system all year round, whatever the weather!\nHow air source works\nAn Air Source Heat Pump uses the same technology as your kitchen fridge to move heat from one place to another.\nAn ASHP uses the temperature of the air circulating outside to warm a liquid refrigerant.\nThe heat pump turns the refrigerant into a gas, compresses it and then condenses back into a liquid again which generates heat energy and the cycle starts again.\nWhat makes heat pumps sustainable?\nThe temperature of the air outside is determined by the sun and local weather conditions and is therefore a renewable source of heat.\nUsing this renewable energy to help heat your home is more sustainable than fossil fuel alternatives.\nHeat pumps are electric, but by using heat from the air they use less electricity to produce heat than traditional electric heating systems.\nCombining your ASHP with your own solar energy increases their sustainability and further reduces carbon emissions.\nAre air source heat pumps efficient?\nThe efficiency and performance of an ASHP is measured using Coefficient of Performance (COP) and this varies by model and brand.\nA number of factors including the insulation levels that your property has, your existing heating system and whether you are using under floor heating versus radiators will determine the overall performance of your ASHP.\nGovernment incentive schemes to encourage investment in low carbon technology can currently cover the cost of installation and ongoing maintenance costs are lower than with traditional heating systems.\nGeo Green Power have extensive experience and will ensure you understand the options available.\nLearn more about how ASHPs work\nIf you choose Air to Air your ASHP will provide hot air to heat your property and you will need an alternative system to provide hot water.\nIf you choose Air to Water then your ASHP will provide hot water to heat your property and provide hot water for use within your home.\nIt’s important to consider your requirements and any unique aspects of your property and location to create the best solution.\nWhen designed and configured correctly your ASHP does not require a back-up heating option.\nHeat pumps are designed to maintain properties at a consistent temperature. This is when they run at their optimum efficiency and are most effective.\nASHPs can be combined with over-sized radiators or underfloor heating. Underfloor heating is often more efficient, but great results can be achieved with other forms of heating.\nWe will ensure that we recommend the right option for you and your property\nASHPs work all year round and in temperatures down to -15 degrees.\nThey are more efficient when the outside temperatures are higher, however in the UK the difference between seasons is very minimal.\nAll air that is above the temperature of absolute zero has heat energy!\nIn recent years ASHP technology has improved considerably. The cost of ownership has come down and the units have become quieter and more efficient.\nRequest a call to discuss your project\nWhat is the installation process?\nASHPs are fitted to the outside of your property and are about 1m wide, 1m high and around 30cm deep.\nNo ground works are required.\nDepending on your existing system, your ASHP may easily link to your water tank and heating with limited additional works. You may need to replace your radiators for larger units.\nA domestic installation in a single property is usually completed and commissioned within 5 working days.\nLearn more about installation\nASHPs have very few moving parts and require less ongoing maintenance than traditional heating systems. We recommend an annual service to ensure that your system is running at its optimum efficiently.\nYou do not require planning permission for an ASHP.\nYour ASHP is positioned outside your property and is best in a sheltered location which has good air flow. Most properties can easily accommodate an ASHP in a location that is both effective and convenient.\nASHPs work best when they are combined with other energy saving technology such as insulation and double glazing. Running your heat pump to maintain your property at a consistent temperature is also advisable.\nChanging from Oil Fired Heating to Air Source\nWhen Paul took on a 1970s property he wanted to move away from oil fired heating. He carefully compared the cost of replacing the old oil boiler with an up to date condensing oil boiler or an Air Source Heat Pump system and found that the heat pump option was the right choice for him.\nCosts & Expected Returns\nWhat is the installation cost for air source?\nOur ASHP installations start from around £12,000\nHow much do ASHPs cost to run?\nIt’s very difficult to estimate the running costs of your installation as it will depend on the size of the space you want to heat, the temperature you are running the system at and a number of other factors such as insultation and whether you are using underfloor heating or radiators.\nWe will be happy to provide an estimate of your running costs if you would like to discuss your project.\nWhat will my return on investment be?\nThe return on investment that you receive will be determined by the type of fuel you are looking to replace and the cost of running your system. Higher returns can be achieved when switching from oil or LPG heating systems.\nThe current Renewable Heat Incentive provides payments that, in a number of cases, can cover the cost of the installation.\nHeat pumps require less maintenance and repair than most traditional heating systems, which also makes them an attractive investment.\nGet in touch to discuss your ASHP project\nJames oversees all of our air source heat pump installations. Get in touch today to organise a no-obligation consultation about your next project.\nWhat is the RHI and are Grants available for ASHPs?\nThe current Government scheme is the Renewable Heat Incentive (RHI).\nThe domestic scheme provides payments for 7 years and applies to eligible heating systems that are used for one dwelling\nYour system will have an Energy Performance Certificate (EPC) and a Seasonal Performance Factor and these two values will be used to calculate your payments based on your usage and the current tariff.\nEstimated output: 12,000kWh per annum\nEstimated RHI income: £900.00 per annum (7 year contract)\nEstimated CO2 savings: 4 tonnes per annum\nASHPs in Commercial & Domestic Buildings\nASHPs in commercial buildings\nASHPs can easily be installed in commercial premises to provide heating and air conditioning and are particularly efficient when running to provide a consistent temperature for offices and workspaces.\nASPHs in domestic houses\nASHPs provide a low carbon, energy efficient way of heating homes. The systems are low maintenance and combining them with other energy efficient technology helps to ensure that they perform at their best.\nASHPs in New Builds vs Existing Properties\nASHPs installation in new builds\nThe construction of a new property is the perfect opportunity to consider and implement energy efficient, sustainable technology to ensure that the finished result is as cost effective and environmentally friendly as possible.\nThe UK Government is due to bring in the Future Homes Standard in 2025 which will prohibit any new build property from having a fossil fuel heating system.\nIncorporating low carbon, renewable heating into new build projects will ensure that you remain ahead of legislation.\nASHPs installation in existing and historic buildings\nASHPs can be retrofitted into existing properties to replace oil, LPG and mains gas heating systems. The return on investment will vary depending on the existing heating system and other factors such as the property’s energy ratings.\nAir Source vs Alternative Heating Solutions\nComparing ASHPs to existing gas and oil systems\nAn ASHP provides an energy efficient, low carbon option when compared to gas, oil or LPG heating systems. It delivers a significant reduction in CO2 emissions and has lower ongoing maintenance costs.\nThe costs of installation may be higher than traditional fossil fuel systems, but this is offset considerably by the RHI payments you will receive.\nRunning costs are usually comparable to mains gas systems, and significantly cheaper than oil or LPG.\nAir source heat pump or ground source\nAn ASHP installation is often cheaper and less intrusive than a GSHP installation and is ideal for smaller properties and those with limited outside space.\nIf you have a large property to heat, and enough outside space a GSHP will outperform an ASHP. A GSHP system can collect more renewable heat from the earth and therefore requires less electricity to produce the same heat output.\nAir source heat pump or solar panels\nWhen considering an ASHP or solar panels, the best choice for you will depend on your heating requirements, budget and energy usage and we will be happy to discuss your options with you.\nIt may be most cost effective for you to consider a scheme that incorporates both. Using solar power to run your heat pump could deliver the greatest cost efficiency and deliver the most sustainable scheme.\nFind out more about renewable energy…"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:b8d93472-5fca-48b9-b15e-785dc34b60b4>","<urn:uuid:416a8aee-e3a5-4e75-a89b-505befe84c0e>"],"error":null}
{"question":"How do the approaches to gamification in software testing and academic courses differ in terms of their competitive elements and potential risks?","answer":"In software testing, competitive game elements were found to be potentially destructive over time, even though initially productive. The testing teams discovered that cooperative gameplay worked better for sustained productivity, as competition often caused some people to shut down and led to system gaming rather than focusing on value. In academic settings, competitive elements like leaderboards were similarly found to be problematic - research showed they could actually demotivate students and lower academic results. While testing environments developed alternatives within cooperative structures, academic courses addressed this through a 'short and long game theory' approach, where short-term engagement (like daily journals and problem-based learning) was balanced with long-term participation elements (such as experience points ledgers and formative assessments) to maintain motivation throughout the semester.","context":["Gamification and Software Testing\nI haven’t spoken about this project publicly because we never got to a public release. Software testing tools represent a tiny market, so they are incredibly difficult to fund. Some of you have asked me about gamification tools with testing, so I thought I would share this brain dump.\nA few years ago, I was asked to help a development team that had significant regulatory issues, and frequently accrued testing debt. The product owner’s solution was to periodically have “testing sprints” where other team members helped the over burdened test teams catch up. There was just one problem: the developers HATED helping out with 2 weeks of testing, so I was asked to do what I could to help.\nA couple of the senior architects at this company were very interested in Session Tester and asked me why I had put game mechanics in a testing tool. I didn’t really realize at the time I had put game mechanics in, I was just trying to make something useful and engaging for people. So I started talking with them more about game design, and they encouraged me to look into MMOs and co-operative games. The team played games together a great deal, so I learned about the games they enjoyed and tried to incorporate mechanics\nI set up a game-influenced process to help structure testing for the developers, and taught them the basics of SBTM. They LOVED it, and started having fun little side contests to try to find bugs in each other’s code. In fact, they were enjoying testing so much, they would complain about having to go back to coding to fix bugs. They didn’t want to do it full time, but a two week testing sprint under a gamified, co-operative model with some structure (and no horrible boring test cases) really did the trick.\nEventually, I worked with some of the team members with a side-project, and the team lead proposed creating a tool to capture what I had implemented. This was actually extremely difficult. We started with what had been done with Session Tester, and went far beyond that, looking at a full stack testing productivity tool. One of the key aspects of our approach that differed from the traditional ET and scripted testing approaches was the test quest. As I was designing this test tool, I stumbled on Jane McGonigal’s work and found it really inspiring. She was also a big proponent of the quest as a model for getting things done in the real world. Also, we were very careful in how we measured testing progress. Bug counts are easily gamed and have a lot of chance. I have worked in departments that measured on bug counts in the past, and they are depressing if you are working on a mature product while your coworkers are working on a buggy version 1.0.\nOne thing Cem Kaner taught me was to reward testers based on approach rather than easily counted results, because they can’t control how many bugs there may or may not be in a system. So we set up a system around test quests. Also, many people find pure exploratory testing (ET) too free form and it doesn’t provide a sense of completion the way scripted test case management tools do. And when you are in a regulatory environment, you can’t do ET all the time, and test cases are too onerous and narrow focused. We were doing something else that wasn’t pure ET and it wasn’t traditional scripted testing. It turns out test quest was a perfect repository for everything that we needed to be done. Also, you didn’t finish the quest until you cleaned up data, entered bugs and other things people might find unpleasant after a test session or two. There is more here on quests: Test Quests – Gamification Applied to Software Test Execution\nAs I point out in that post, Chore Wars is interesting, but it was challenging for sustained testing because of different personalities and motivations of different people. So we used some ideas from ARGs to sprinkle within our process rather than use it as a foundation. Certain gamer types are attracted to things like Chore Wars, but others are turned off by them, so you have to be careful with a productivity tool.\nWe set up a reward system that reminded people to do a more thorough job. Was there a risk assessment? Were there coverage outlines? Session sheets? How were they filled out? Were they complete? What about bug reports? Were they complete and clear? I fought with the architects over having a leaderboard, but eventually I relented and we reached a compromise. Superstar testers can dominate a system like this, causing others to feel demoralized and not want to try anymore. We decided to overcome that by looking at chance events, which are a huge part of what makes games fun, so no one could stay and dominate the testing leaderboard, they would get knocked to the bottom randomly and would have to work their way back up. Unfortunately, we ran into regulatory issues with the leaderboard – while we forbade the practice of ranking employees based on the tool, this sort of thing can run afoul of labor laws in some countries, so we were working on alternatives but ran out of resources before we could get it completed.\nSocial aspects of gaming are a massive part of online games in particular, but board games are more fun with more people too. We set up a communication system similar to a company IRC system we had developed in the past. We also designed a way to ask for help and for senior testers to provide mentoring, and like MMOs, we rewarded people who worked together more than if they worked alone. Like developer tools, we set up flags for review to help get more eyes on a problem.\nWe also set up a voting system so testers could nominate each other for best bug, or best bug report, best bug video, and encouraged sharing bug stories and technical information with each other within the tool.\nAn important design aspect was interoperability with other tools, so we designed testing products to be easily exported so they could be incorporated with tools people already use. Rather than try to compete or replace, we wanted to complement what testers were already doing in many organizations, and have an alternative to the tired and outdated test case management systems. However, if you had one of those systems, we wanted to work with it, rather than against it.\nUnfortunately, we ran out of resources and weren’t able to get the tool off the ground. It had the basics of Session Tester embedded in it, with improvements and a lot of game approaches mixed in with testing fundamentals.\nWe learned three lessons with all of this:\n- Co-operative game play works well for productivity over a sustained period of time, while competitive game play can be initially productive, but over time it can be destructive. Competition is something that has to be developed within a co-operative structure with a lot of care. Many people shut down when others get competitive, and rewarding for things like bugs found, or bugs fixed causes people to game the system, rather than focus on value.\n- Each team is different, and there are different personalities and player types. You have to design accordingly and make implementations customizable and flexible. If you design too narrowly, the software testing game will no longer be relevant. If design is more flexible and customizable from the beginning, the tool has a much better chance of sustained use, even if the early champions move on to other companies. I’ve had people ask me for simple approaches and get disappointed when I don’t have a pat answer on how to gamify their testing team approach without observing and working with them first. There is no simple approach that fits all.\n- Designing a good productivity tool is very difficult, and game approaches are much more complex than you might anticipate. There were unintended consequences when using certain approaches, and we really had to take different personality and player styles into account. (There are also labour and other game-related laws to explore.) Thin layer gamification (points, badges, leaderboards) had limited value over time and only appealed to a narrow group of people.\nIf some of you are looking at gamification and testing productivity, I hope you find some of these ideas useful. If you are interested in some of the approaches we used, these Gamification Inspiration Cards are a good place to start.","This is a guest post by Dr. Szymon Machajewski, Blackboard MVP, faculty with the School of Computing & Information Systems at Grand Valley State University.\nGamification: the use of game design elements in nongame contexts (Deterding, 2012)\nGamification is a buzzword that sometimes receives a skeptical welcome in higher education. Some may wonder whether playing games has any place in a college level class, and the research results are mixed. Some researchers found that applying leaderboards to a college course can actually demotivate students and lower academic results (Hanus & Fox, 2015). Gartner warned that as much as 80% of gamification projects are likely to fail (Gartner, 2012). So why bother with a pedagogy that may be risky to adopt and at the same time represents controversial characteristics of childish play and perceived lack of seriousness?\nIn my view, the risk is worthwhile. Lack of engagement in traditional courses, especially introductory college courses, discourages students and faculty. This lack of engagement can perhaps be due to what has been called ‘dysfunctional illusions of rigor.’ Craig E. Nelson from Indiana University coined the term to reflect on research-debunked traditional views of college instruction. Some examples include: “1. Hard courses weed out weak students. When students fail it is primarily due to inability, weak preparation, or lack of effort. 2. Traditional methods of instruction offer effective ways of teaching content to undergraduates. Modes that pamper students teach less.”\nRather than ‘pampering’ students, active learning has proven to make a significant difference in academic results and student engagement. In addition, research published by the American Psychological Association (APA) found that play is good for learning and it holds a large potential for teaching new forms of thought and behavior. In that same study, playing digital games was found to boost learning, health, and social skills.\nAt Grand Valley State University, I have conducted research on gamification which has led to the development of a “short and long game theory” (Machajewski, 2017). The application of gamification in academic courses requires the gameful design of individual lecture periods, as well as the gameful design of the semester-long student journey.\nSome examples of short game elements include:\n- Daily Journal\n- One Minute Paper\n- Problem Based Learning\n- Quizlet Live\nExamples of long game elements include:\n- Exam Peace of Mind Points PofM\n- Formative Assessments\n- My Progress instead of My Grades\n- XP Ledger – MyGame\nWhile active learning fits well in the “short game” design, something else is needed to make the course “long game” worth playing for students. We have implemented the “long game” design in our Introduction to Computing course at Grand Valley State University, which earned exemplary course status in the 2017 Blackboard ECP program. The key to the “long game” is the adoption of an experience points (XP) ledger. This instrument allows students to earn XP during lectures, homework assignments, hands-on practice sessions and track the total across the length of the course.\nSchool grades used to serve as the “long game” strategy. However, today they are more likely to demotivate students than encourage them to conduct deeper exploration or to appreciate the subject matter. Grades have become a high stakes extrinsic reward. Just as money—in the research of Daniel Pink—is a poor motivator of knowledge workers, grades are a poor motivator of intellectual performance for students.\nStudents are knowledge workers and require creative thinking. To motivate knowledge workers, we need to appeal to autonomy, mastery, and purpose. These three characteristics are based on the self-determination theory (Ryan & Deci, 2000). Not only do we care about the intellectual performance of students during class, but we also want students to grow their affinity for the subject matter. In other words, we want to avoid the sentiment: “I got an A in the class, but I hate math, and I hope I will never have to use it again in my life.”\nFor gamification to be successful in academic courses, you need to have exemplary instructional design, which is why the value of the Blackboard ECP program is directly related to gamification. Clarifying instructions for students, setting attainable course and learning objectives, and chunking content to encourage progress are all efforts to turn a course into well-designed work, which is one of the definitions for a game.\nA focus on student experience creates the common ground between instructional design and gamification. Best practices of instructional design recommend active learning as the most effective way to engage students in the classroom. While tutoring may increase grades by two standard deviations (Bloom, 1984), active learning could increase academic performance by 0.47 standard deviations (Ruiz-Primo, Briggs, Iverson, Talbot, & Shepard, 2011). Some students are 150% more likely to fail in courses dominated by traditional lectures over courses with active learning strategies (Freeman et al., 2014).\nMy gamification research (Machajewski, 2017) recommends Quizlet Live and Kahoot! as active learning platforms with a competitive edge. The “short game” in courses depends on classroom, hybrid, or online deliveries. However, the “long game” approach is just as important to consider. Opportunities for improvement can be found in mitigating student exam anxiety, sufficient encouragement of failure in practicing of hands-on material, and adopting intermediate due dates to ensure an ongoing growth of mastery.\nThe specific technologies used in my Introduction to Computing course were presented in New Orleans at the BbWorld 2017 conference and can be reviewed in the conference publication. My theory of the “short and long game” in academic courses is being developed on the background of case studies and emerging gamification tools. Some examples of the case studies include a Germanic Studies course and STEM introductory college course.\nGamification in an academic course can be considered a complex system of many tools within two main categories: short-term engagement and long-term participation. Adoption of just a few of them separately, such as a leaderboard, badges, a points system, or classroom response system may not lead to an intrinsically satisfying experience and expected high levels of motivation. This follows the Anna Karenina principle (Bornmann & Marx, 2012) present in complex systems.\nConversely, gamification studies showing negative results should not reflect a general lack of applicability of gameful design in academic courses. A consideration of both the short game and the long game will help faculty experience more benefits of gameful design. In turn, the reciprocal quality of engagement is likely to affect the faculty and the students.\nBloom, B. S. (1984). The 2 Sigma Problem: The Search for Methods of Group Instruction as Effective as One-to-One Tutoring. Educational Researcher, 13(6), 4. doi:10.3102/0013189X013006004\nBornmann, L., & Marx, W. (2012). The Anna Karenina principle: A way of thinking about success in science. Journal Of The American Society For Information Science & Technology, 63(10), 2037-2051. doi:10.1002/asi.22661\nBurke, B. (2014). Gartner Redefines Gamification. Retrieved from http://blogs.gartner.com/brian_burke/2014/04/04/gartner-redefines-gamification/\nDeterding, S., (2012). Gamification: designing for motivation. Interactions 19, 14–17.\nFreeman, S., Eddy, S. L., McDonough, M., Smith, M. K., Okorafor, N., Jordt, H., and Wenderoth, M. P., (2014). Active learning increases student performance in science, engineering, and mathematics. Proceedings of the National Academy of Sciences (PNAS), 111(23),8410-8415.\nHanus, M., & Fox, J. (2015). Assessing the effects of gamification in the classroom: A longitudinal study on intrinsic motivation, social comparison, satisfaction, effort, and academic performance. Computers & Education, 80152–161.\nMachajewski, S. (2017). Application of Gamification in a College STEM Introductory Course: A Case Study. Retrieved from https://eric.ed.gov/?id=ED574876\nRyan, R., & Deci, E. (2000). Self-determination theory and the facilitation of intrinsic motivation, social development, and well-being. American Psychologist, 55, 68–78.\nRuiz-Primo, M.A., Briggs, D., Iverson, H., Talbot, R., Shepard, L.A. (2011). Impact of undergraduate science course innovations on learning. Science 331, 1269–1270."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:1b8f02a7-bcf0-4397-8a6b-92851b2d2f10>","<urn:uuid:db208e75-c6e0-4f02-aea4-44c708cac664>"],"error":null}
{"question":"What construction timelines are mentioned for PNE Amphitheatre and Dickies Arena?","answer":"The PNE Amphitheatre is scheduled to begin construction in 2024 with expected completion in 2026. The Dickies Arena has already been completed and opened on October 26 in Fort Worth, with events already scheduled including monster truck shows, basketball tournaments, and concerts.","context":["The new PNE Amphitheatre has taken another step towards construction, with Vancouver City Council voting to approve an increase in loan financing for the project.\nAccording to Vancouver Mayor Ken Sim, “The PNE is home to some of the best and most iconic summer experiences our city offers.”\n“Once completed, the new Amphitheatre will be a world-leading performance facility.”\nThe 10,000-seat arena at Hastings Park is expected to make a significant splash in the city’s cultural scene.\nThe increase in loan financing reflects updated design plans for the Amphitheatre.\nThe additional investment will allow the PNE to repay its loan to the City of Vancouver in a shorter timeframe on account of expected increases in revenue.\n“This decision is a win for Vancouver taxpayers, a win for Vancouver’s arts and culture sector, and a win for the future of the PNE,” said Councillor Sarah Kirby-Yung, PNE Chair.\n“There is an unmistakable magic and excitement that comes with watching performances at the PNE.”\n“This new investment will make the PNE Amphitheatre a truly world-class venue.”\nLocated in a vibrant area and known as ‘the heart of the park,’ the venue is expected to attract both residents and tourists.\nFeaturing a striking mass timber curved roof, construction of this open-air amphitheatre is scheduled to begin in 2024, with an expected completion in 2026 and an estimated cost of $65 million.\nShelley Frost, the President and CEO of the Pacific National Exhibition, expressed her enthusiasm for the project, stating, “The arena will be a jewel in Vancouver’s crown of spectacular venues.”\n“The project will highlight British Columbia building products and engineering while adhering to the highest environmental sustainability standards.”\n“We believe there will not be another venue like this anywhere in Canada.”\nIn October 2021, the British Columbia provincial government released its 2030 carbon roadmap – ‘CleanBC.’\nThe policy supports lowering carbon emissions by 40% over the next seven years – with the government specifically highlighting the role that low embodied timbers can play in achieving this goal.\nThe venue will offer various amenities, including VIP suites, lounge spaces, common areas, and catering options. It will also feature accessible washrooms, merchandise sales, and food and drink concessions to accommodate all attendees.\nVancouver-based Revery Architecture designed the venue’s unique roof structure, which, when completed, will be one of the most extended clear-span roofs in the world.\nVenelin Kokalov of Revery Architecture explained their approach:\n“Our architectural solution was an elegant structure covering the arena, gracefully landing on three points into the landscape.”\n“This gesture will frame vistas to the mountains and the surrounding context, creating transparency at the human level and an intimate atmosphere under the warmth of the wood.”\nMass-timber structure specialists, Fast + Epp Structural Engineers, handled the roof’s engineering.\nFast + Epp is at the forefront of the Canadian push towards mass timber adoption, and in 2018 developed a guide for mass timber application.\nTheir design consists of six-barrel vaulted segments intersecting at diagonal planes, creating a clear span of 105 meters from one buttress tip to the other.\nAccording to Robert Jackson, a partner at Fast + Epp Structural Engineers, the form is inspired by the shell of the 1956 CNIT building in Paris.\nAs a hub for diverse performances and events, the theatre is expected to host 75 concerts annually, including community arts and culture shows, commercial productions, corporate events, and the popular PNE Summer Night Concerts.","New Dickies Arena Is Designed for an Extensive Range of Audio\nFort Worth’s 14,000-seat venue will host events from rodeos to rim shots (basketball and music)\nDickies Arena, which opened on Oct. 26 in Fort Worth, was designed to be a truly multi-use venue. The 14,000-seater features AV systems designed by consultant WJHW, an L-Acoustics PA system installed by integrator Electro-Acoustics, and a 56-ft.-wide by 35-ft.-high Mitsubishi centerhung videoboard from ANC Sports. These allow it to accommodate an extremely wide variety of events.\nAlready on the schedule are the Hot Wheels Monster Truck event this month, the NCAA Men’s Basketball first and second rounds (scheduled for 2022), the NCAA Women’s Gymnastics Championships and American Athletic Conference Men’s Basketball Championships (both next year), and concerts by Twenty-One Pilots, the Black Keys, and George Strait (all in the venue’s first two months of operation).\nBut Dickies Arena — a public-private partnership between the City of Fort Worth, Tarrant County, the State of Texas, and a group of private-sector participants — is also designed, to a large extent, around its anchor client: the Fort Worth Stock Show & Rodeo, which runs each year in January and February.\nSound System To Satisfy the Rodeo\n“This is intended to be a truly multipurpose venue,” explains Dickies Arena Assistant GM Bill Shaw, “but we also built it around our main tenant, which does 25 performances in 23 days starting every January, so we wanted them to be especially happy with the venue.”\nThe decision process around the venue’s sound system, for instance, illustrates that.\n“We brought [the tenant] in while we were under construction and did a virtual walk-through,” he explains. “Right away, they told us that, based on the sound system originally specified, they’d need to bring in supplemental sound equipment. That was very disappointing to hear from our main client.” The comment ultimately led to the installation of a L-Acoustics system that Shaw describes as “a significantly increased investment.”\nOther ways that Dickies Arena accommodates livestock-based sports events is with a substantially larger floorplan. At 250 ft. long by 125 ft. wide, the arena floor far larger than the typical 94- x 50-ft. basketball arena and even the conventional 200- x 85-ft. hockey rink. The larger floor area mimics that of the nearby Will Rogers Memorial Center, which had hosted the Fort Worth Stock Show & Rodeo since 1944.\n“The size of the floor was important to them; it was the starting point for us for design,” Shaw explains. “It also helps us with events such as gymnastics.”\nComplementing that are retractable seating stands, from Irwin Seating, which allow up to 3,000 seats to be pulled out for concerts and other events and pushed back for rodeos and convention-type occasions. That, in turn, requires the sound system to be refocused each time seating is reconfigured, to keep the sound on the seats and away from reflective surfaces. That’s accomplished by raising or lowering the rigging ring that the speaker hangs are attached to.\nOther facets serving the flexibility of the venue include placing VIP seating areas and event PA and broadcast announcers on a platform atop the bucking chutes — which hold the bulls that some consider the true stars of the show at rodeos — opposite the timed-event chutes used for such events as calf roping.\nAll these design aspects were communicated to key national broadcasters, including ESPN and CBS Sports; the latter’s KRLD-AM is the CBS Radio affiliate for the rodeo broadcast. In addition, Shaw says, both fiber and triax cabling have been installed throughout the venue to accommodate both national and high school broadcasters, and SMPTE connections have been run from the venue control room, used by CBS Sports when necessary, to the production-truck docks.\n“We won the NCAA Men’s Basketball events early on,” Shaw notes. “That caused us to gear up from the beginning for national broadcasters.”\nAV Gear in the House\nThe venue’s AV is substantial, reflecting its multi-role mission and underscoring the kind of firepower even midsize venues need to be competitive.\nThe centerhung main videoboard comprises four 6-mm Mitsubishi displays measuring 25.2 ft. high x 37.8 ft. wide and wrapping into four 6-mm corner displays. It’s on a network that includes two 10.08- x 35.28-ft. 4-mm boards placed inside the center videoboard on the east and west sides and combining live video and replays for lower bowl seats and premium guests. A 360-degree, 16-mm fascia ribbon board wraps the upper bowl; a second set of 233.49- x 2.5-ft. fascia displays runs along the sidelines. And four courtside tables featuring 4-mm displays are used to create the courtside scoring-table LED.\nHanging from its own motorized truss encircling the interior of the venue are eight L-Acoustics K2 long-/mid-throw speakers atop four Kara short-throw boxes; four of these arrays are backed with two KS28 cardioid-configured subwoofers. Six ARCS A15 systems are paired with two AS Wide speakers to cover the floor area beneath the centerhung videoboard. The system is powered by 38 LA-12A amplified controllers and eight LA-4X amplified controllers, managed via an L-Acoustics P1 processor.\nDickies Arena is located in the middle of Fort Worth’s growing cultural district, which already has the Bass Performance Hall and the Kimbell Art Museum. “So the new arena also had to meet that world-class criteria,” says Chris Jordan, president/chief steward, Electro-Acoustics.\nIt also had to be as ecumenical in the range of sports it can host as in the genres of music that will be performed there.\n“We had to think about everything from basketball and hockey to rodeo and WWE wrestling, which is not something that most arenas have to consider,” says Shaw. Ball and stick sports, he notes, “aren’t the only games in town.”"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:b9bf90a5-7150-475b-bb0d-be5c44c16fb4>","<urn:uuid:972cea72-ee7d-444d-a862-9a4560bdc444>"],"error":null}
{"question":"What role do monitoring and control systems play in preventing disasters and ensuring operational success, comparing the Costa Concordia's safety systems with vertical farming's automated infrastructure?","answer":"The Costa Concordia's monitoring systems included a 'black box' voice recorder and automatic tracking systems that documented the ship's route and position, though these proved insufficient when critical warnings were ignored and incomplete information was relayed to authorities during the emergency. In contrast, vertical farming employs comprehensive automated control systems including computerized monitoring systems, remote control systems and software, automated racking and stacking systems, programmable LED lighting, and temperature control systems. These sophisticated control mechanisms help maintain optimal growing conditions and ensure efficient resource management, though they contribute to significant initial capital costs for establishing vertical farming operations.","context":["Costa Concordia: What happenedContinue reading the main story\nThirty-two people died after the Costa Concordia cruis ship ran aground with more than 4,000 passengers and crew on 13 January 2012, only hours after leaving the Italian port of Civitavecchia. The graphics and maps below reveal more about what happened.\nThe Costa Concordia left the Italian port of Civitavecchia at 19:18 local time (18:18 GMT).\nThe ship was heading out on a week-long cruise around the Mediterranean with 3,206 passengers and 1,023 crew onboard.\nAs it made its way north-west along the Italian coastline, Captain Francesco Schettino ordered the ship to be steered close to the island of Giglio as a \"salute\".\nAutomatic positioning data from Dutch firm QPS showed the ship's position as it approached land, and what happened next.\nItaly's Ministry of Infrastructure and Transport published a detailed timeline of events in its report into the accident (PDF), released in May 2013.\nOther details emerged at pre-trial hearings, including excerpts of the frantic conversations between the Captain and his crew in the aftermath of the accident, captured by the ship's \"black box\" voice recorder.\nNearing Giglio just after 21.30, the captain gave the helmsman coordinates, followed by the warning \"otherwise we go on the rocks\".\nMinutes later, at 21:45, the Costa Concordia hit a rocky outcrop while travelling at around 16 knots.\nThe ship was holed on the left-hand side, started taking on water and began to tilt. Engine rooms were flooded and power was lost.\nThe crew struggled to assess the situation and relayed incomplete information to the Italian authorities.\nAt 21:52 the chief engineer and electrical officer tried and failed to start the ship's emergency diesel generator.\nShortly afterwards, passengers were told that the ship was suffering a \"blackout\", but that the situation was under control. The same information was given to the harbour master at Civitavecchia.\nPositioning data shows that the Costa Concordia turned and began to drift back towards the island's port soon after 22:00 due to, investigators say, a combination of the wind and the rudder positioned to starboard (right).\nAs it drifted, the ship then began to list in the opposite direction, possibly caused by water in the damaged hull rushing to the far side during the turn.\nAt 22:12, the coastguard called the ship to say passengers were reporting problems to the local police, but the captain replied: \"We have a blackout and we are checking the conditions on board.\"\nAt 22:22 the captain gave orders to tell the coastguard that they had had a \"failure\" and needed help from tug boats. The radio operator did this and added that all the passengers had been given life jackets, none was injured and there was a gash in the left side of the ship.\nAt 22:33 the general emergency alarm was raised and passengers told to go to muster stations and await instructions.\nBy 22:48 the ship had settled on the rocky sea bed, tilted by more than 30 degrees. The captain finally gave the order to abandon ship at at 22:54.\nMost passengers escaped in lifeboats, but evacuation efforts were hampered by the angle of the tilting ship. The coastguard launched boats and helicopters to carry stranded passengers to safety.\nAt 23:19 Captain Schettino abandoned the bridge, leaving the second master to co-ordinate the evacuation.\nHowever by 23:32 the second master also left the bridge. Around 300 passengers and some crew were still on board.\nAt midnight dozens of passengers remained, many clinging to the exposed side of the ship.\nIn a conversation recorded at 00:42, a coastguard commander ordered the captain to get back on board. He did not, and went ashore.\nThe rescue continued over the weekend, with the ship's safety officer, Marrico Giampietroni, being discovered and evacuated with a broken leg at 12:00 on Sunday. A South Korean couple were also rescued.\nCapt Schettino was arrested and later went on trial, charged with multiple counts of manslaughter and abandoning ship.\nHe admitted making a navigational error, and told investigators he had \"ordered the turn too late\" as the ship sailed close to the island.\nThe ship's owners, Costa Cruises, said the captain had made an \"unapproved, unauthorised\" deviation in course, sailing too close to the island in order to show the ship to locals.Crash investigation\nAutomatic tracking systems show the route of the Costa Concordia until it ran aground on 13 January. Data from 14 August 2011 show the ship followed a similar course close to the shoreline, according to Lloyd's List Intelligence. On 6 January 2012, it passed through the same strait but sailed much further from the island.\nDivers searched the ship as it rested on the seabed in about 20m of water. The operation had to be suspended a number of times as the ship shifted position. The sea floor eventually drops to about 100m.\nBefore salvage work could begin, 2,400 tonnes of fuel had to be extracted from its tanks.\nThe Dutch salvage firm Smit brought a barge alongside the ship and divers installed external tanks to collect the diesel. More than 2,200 tonnes of fuel was eventually extracted, but the engineers were unable to remove all of it from some of the most inaccessible tanks.\nThe decision to salvage the ship, rather than break it up, was taken in May 2012, four months after the disaster.\nThe contract - awarded jointly to salvage companies Titan and Micoperi - was described as an unprecedented operation.\nThe ship was eventually refloated in July 2014 and taken to Genoa, where the scrapping operation is expected to take two years.","As urban populations continue to grow, entrepreneurs are going beyond traditional farming to find new ways to feed everyone while minimising the effect on our land and water resources. Vertical farming is one such method that has been used all around the world. Food crops may be conveniently farmed in urban settings using Vertical Farming by planting in vertically stacked layers to conserve space and require little energy and water for irrigation.\nVertical farming is the process of producing crops in layers that are vertically stacked. Controlled-environment agriculture, which tries to maximise plant development, and soil-less farming techniques such as hydroponics, aquaponics, and aeroponics, are frequently used.\nBuildings, shipping containers, tunnels, and abandoned mine shafts are among popular structures used to host vertical farming systems. There are approximately 30 hectares (74 acres) of functioning vertical farms around the globe as of 2020. Vertical farming, in conjunction with other cutting-edge technology such as customised LED lighting, has resulted in crop yields that are more than ten times greater than those obtained by standard agricultural methods.\nVertical farming is still in its early stages in India, but there are a few entrepreneurs and agri-tech enterprises aiming to revolutionise the area.\nVertical Farming Background and Concept\nGilbert Ellis Bailey originated the phrase “vertical farming” and published a book named “Vertical Farming” in 1915. William Frederick Gerick pioneered hydroponics at the University of California, Berkeley, in the early 1930s.\nke Olsson, a Swedish ecological farmer, devised a spiral-shaped rail system for growing plants in the 1980s and proposed vertical farming as a method of raising vegetables in cities.\nProfessor Dickson Despommier invented the concept of vertical farming in 1999. His idea was to grow food in urban areas, utilising less distance and saving time in transporting food produced in rural regions to cities.\nHe aimed to produce food in urban areas in order to have fresher goods available sooner and at a reduced cost. As a result, vertical farming is defined as the cultivation and production of crops/plants in vertically stacked layers and vertically inclined surfaces.\nThe plants are vertically piled in a tower-like form in the physical arrangement. This reduces the amount of space needed to cultivate plants. Following that, a combination of natural and artificial lighting is employed to ensure an ideal atmosphere for the plants’ effective growth. The third component is the plant’s growth medium. Aeroponic, hydroponic, or aquaponic growth media are employed instead of soil as the growing medium.\nAs the methodology gets more scientific, the process’s efficiency grows, and as a result, vertical farming becomes more sustainable, consuming 95 percent less water than previous agricultural methods.\nAlso Read, Oxagon: The World’s First Floating City in the World\nVertical Farming Techniques\nIt is a method of producing food in water without the use of soil by employing mineral fertiliser solutions.\nThe primary benefit of this strategy is that it lowers soil-related cultivation issues such as soil-borne insects, pests, and illnesses.\nAeroponics was inspired by NASA’s (National Aeronautical and Space Administration, USA) endeavour in the 1990s to develop an effective technique to grow plants in space. There is no growth medium in aeroponics, hence there are no containers for growing crops. Instead of water, mist or nutrient solutions are utilised in aeroponics. Because the plants are attached to a support and the roots are sprayed with nutritional solution, there is very little space, very little water, and no soil required.\nThe name aquaponics is derived from the combination of two words: aquaculture (fish farming) and hydroponics (the process of growing plants without soil in order to develop symbiotic interactions between the plants and the fish). The symbiosis is established by feeding nutrient-rich waste from fish tanks to hydroponic production beds called “fertigate.”\nIn turn, the hydroponic beds act as biofilters, removing gases, acids, and chemicals from the water, such as ammonia, nitrates, and phosphates. Furthermore, the gravel beds serve as a home for nitrifying bacteria, which aid in nutrient cycling and water filtering. As a result, the newly cleansed water may be recirculated back into the fish tanks.\nThe Benefits of Vertical Farming\nVertical farming offers various advantages, making it promising for agriculture’s future. The land need is fairly minimal, water usage is 80% less, water is recycled and stored, pesticides are not used, and in the case of high-tech farms, there is no true reliance on the weather.\nA vertical farm makes farming possible within the constraints of a metropolis. When the farms are close by, the food is delivered swiftly and is always fresh, as opposed to the chilled stuff commonly seen in stores. Transportation reduction minimises the cost of fossil fuels and the accompanying emissions, as well as transportation spoilage. Vertical farming, like anything else, has its limitations. The biggest issue is the initial capital expenses for building the vertical farming system.\nThere are further expenditures associated with building the structures as well as their automation, such as computerised and monitoring systems, remote control systems and software, automated racking and stacking systems, programmable LED lighting systems, temperature control systems, and so on."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"future"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:ac9aaf6b-6b63-49d1-8c17-ca80b0da01dd>","<urn:uuid:38abc93f-c15d-442d-abe3-4bcfbd141419>"],"error":null}
{"question":"I'm comparing conservation genetic approaches across marine and freshwater species. What are the key differences between how researchers investigate population structure in white marlin versus Indian freshwater fishes?","answer":"In white marlin research, scientists analyzed 24 molecular markers from samples across six geographic locations to determine if there was a single or multiple genetic stocks in the Atlantic Ocean. The research relied on recreational anglers to provide fin clip samples, requiring 30-50 samples per region for statistical analysis. In contrast, for Indian freshwater fishes, researchers focused on developing genetic resources for conservation management, studying signatures of natural selection, and examining functional biodiversity. They also investigated specific adaptational strategies, such as tolerance to hypoxia, high temperature and ammonia in species like Clarias batrachus and Heteropneustes fossilis.","context":["Dr Vindhya Mohindra, Principal Scientist and Head\nFish Conservation Division, National Bureau of Fish Genetic Resources (ICAR)\nMy research interests include conservation genetics and genomics in natural populations of fishes. In the course of twenty five years' research experience, I have developed genetic resources to generate the information to be utilised in conservation and management of natural resources of fishes of India. Additionally, I have studied signatures of natural selection and genomic diversity in important freshwater fish species to build genomic resources and to gain knowledge about functional biodiversity. I am also working on deciphering molecular basis of tolerance in fishes for abiotic stresses, like hypoxia, high temperature and high ammonia tolerance. At present, I have over hundred publications with an H-index of 18.\n- ICAR funded project under National Agricultural Science Fund (NASF): To elucidate the unique biochemical adaptational strategies that allow two air-breathing catfishes (Clarias batrachus and Heteropneustes fossilis) to survive in ammonia enriched toxic waste, under consortium mode as Consortium Co-Principal Investigator: 2018 to 2021.\n- National Innovations on Climate Resilient Agriculture (NICRA) competitive grant project: Understanding genomic mechanisms of thermal tolerance using golden mahseer, Tor putitora as model as Principal Investigator: 2018 to 2020.\n- ICAR sponsored project CRP-Genomics: De-novo genome sequencing of anadromous Indian shad, Tenualosa ilisha and Indian major carp, Catla catla (Principal Investigator) Phase I: 2015-2017, Phase II 2017-2020.\n- DBT-BBSRC-DFID Project: Poverty alleviation through prevention and future control of the two major socioeconomically-important diseases in Asian aquaculture as Co-Investigator 2016 to 2019.\n- ICAR funded project under National Agricultural Science Fund (NASF): Stock characterization, captive breeding, seed production and culture of hilsa (Tenualosa ilisha) under consortium mode as Consortium Co-Principal Investigator for NBFGR 2012 to 2017. Budget (NBFGR) INR 198.00 lakhs (USD 0.36 million) Institute's projects handled.\n- ICAR-Institute funded project: Signatures of natural selection and genomic diversity in important freshwater fish species, Tor putitora and Clarias magur Institute funded (As PI) (2014 to March 2019).\n- ICAR-Institute funded project: Outreach activity on fish genetic stocks-Phase II (Co-PI) 2015-2018.\n- ICAR-Institute funded project: Deciphering Aphanomyces invadans genome to understand its mechanism of infection in fishes (Co -PI) 2015-2018.\nResearch papers in Peered Reviewed Journals\n- Divya B.K., Vindhya Mohindra, Rajeev K. Singh, Prabhaker Yadav, Prachi Masih, J.K. Jena. Genes involved in growth, lipid metabolism and immune system revealed by muscle transcriptome profile of tropical shad, Tenualosa ilisha (Hamilton, 1822). Genes and Genomes.\n- Sangeeta Mandal, Kuldeep Kumar Lal, Rajeev Kumar Singh, Rama Shanker Sah, J. K. Jena, Vindhya Mohindra (2018) Comparative Length-Weight Relationship and Condition Factor of tropical shad, Tenualosa ilisha (Hamilton, 1822) from freshwater, estuarine and marine waters of India. Indian Journal of Fisheries 65(2): 33-41.\n- Bhaskar, R. and Vindhya Mohindra (2018) Variability in DNA COI sequences reveals new haplotypes in freshwater turtles from northern region of India. Mitochondrial DNA Part B 3:1, 317-323.\n- Maisnam Manorama; Mohindra, Vindhya; Chandra, Suresh; Lal, Kuldeep K; Singh, Rajeev K (2017). Characterization of threatened endemic fish Osteobrama belangeri (Valenciennes) and related species from North-East India based on morphological and molecular analysis. Mitochondrial DNA Part A 29(6):919-932.\n- Divya, B.K, Prabhaker Yadav, Prachi Masih, Rajeev K. Singh, Vindhya Mohindra (2017). In silico characterization of Myogenic Factor 6 transcript of Hilsa, Tenualosa ilisha and putative role of its SNPs with differential growth. MetaGene, 13:140–148.\n- Dwivedi, Arvind K, Braj Kishor Gupta, Rajeev K. Singh, Vindhya Mohindra, Suresh Chandra, Suresh Easawarn, Joykrushna Jena (2017). Cryptic diversity in the Indian clade of the catfish family Pangasiidae resolved by the description of a new species. Hydrobiologia 797 (1), 351–370.\n- Mohindra, Vindhya, Rajeev Kumar Singh, Ratnesh Kumar Tripathi, Kuldeep Kumar Lal and J. K. Jena (2017). Complete mitogenome of Indian mottled eel, Anguilla bengalensis bengalensis (Gray, 1831) through PacBio RSII sequencing (Mitogenome Announcement). Mitochondrial DNA Part A, 28(2): 238-239.\n- Lakra, W.S., Singh M., M. Goswami, A. Gopalakrishnan, K.K. Lal, V , Mohindra, UK , Sarkar, P.P., Punia, K.V., Singh, J.P., Bhatt, S., Ayyappan (2016). DNA barcoding Indian freshwater fishes. Mitochondrial DNA: The Journal of DNA Mapping, Sequencing, and Analysis Part A 27(6):4510-4517.\n- Mohindra, Vindhya, Ratnesh Kumar Tripathi, Akanksha Singh, Ruchi Patangia, Rajeev Kumar Singh, Kuldeep Kumar Lal and Joy Krushna Jena (2016). Hypoxic stress-responsive genes in air breathing catfish, Clarias magur (Hamilton 1822) and their possible physiological adaptive function. Fish and Shellfish Immunology 59, 46-56.\n- Lal, Kuldeep Kumar, Arvind K Dwivedi, Rajeev K. Singh, Vindhya Mohindra, Suresh Chandra, Braj Kishor Gupt, Smita Dhawan, Joykrushna Jena (2016). A new Bagrid catfish species, Rita bakaluae (Siluriformes: Bagridae), from the Godavari River basin, India. Hydrobiologia.\n- Mohindra, Vindhya, Ratnesh K. Tripathi, Prabhaker Yadav, Rajeev K. Singh and Kuldeep K. Lal (2015). Hypoxia induced altered expression of heat shock protein genes (Hsc71, Hsp90a and Hsp10) in Indian Catﬁsh, Clarias batrachus (Linnaeus, 1758) under oxidative stress. Molecular Biology Reports (2015) 42:1197–1209.\n- Mohindra, Vindhya, Rajeev K. Singh, Rajesh Kumar, R. S. Sah and Kuldeep K. Lal.(2015). Complete mitochondrial genome sequences of two endangered Indian catfish species, Clarias batrachus and Pangasius pangasius (Mitogenome Announcement). Mitochondrial DNA: The Journal of DNA Mapping, Sequencing, and Analysis 26(5): 678–679.\n- Lal, Kuldeep K., Braj K. Gupta, Peyush Punia, Vindhya Mohindra, Ved P. Saini, Arvind K. Dwivedi, Rajeev K. Singh, V. S. Basheer, Smita Dhawan, Rupesh K. Luhariya, and J. K. Jena (2015). Revision of gonius subgroup of the genus Labeo Cuvier, 1816 and confirmation of species status of Labeo rajasthanicus (Cypriniformes: Cyprinidae) with designation of a neotype. Indian Journal of Fisheries 62(4): 10-22.\n- Khare, Praveen, Vindhya Mohindra; Anindya S. Barman; Rajeev K. Singh; Kuldeep Kumar Lal (2014). Molecular Phylogenetic Evidence to Reconcile Taxonomic Instability in Mahseer Species (Pisces: Cyprinidae) of India. Organisms Diversity and Evolution 14 (3): 307-326.\n- Murray, H. M., S. P. Lall, R. Rajaselvam, L.A. Boutilier, R.M. Flight, B. Blanchard, S. Colombo, V. Mohindra, M. Yúfera and S.E. Douglas (2010). Effect of early introduction of microencapsulated diet to larval Atlantic halibut, Hippoglossus hippoglossus, L. assessed by microarray analysis. Marine Biotechnology 12: 214–229.\n- Suresh, V.R., A. M. Sajina, S. Dasgupta, D. De, D. N. Chattopadhyaya, B. K. Behra, Ritesh Rajan, Vindhya Mohindra and S. Bhattacharya (2017) Current Status of Knowledge on Hilsa, Publisher: ICAR - Central Inland Fisheries Research Institute, Barrackpore, Kolkatta 700 120, W.B. ISBN No. 81-85482-22-5.pp 108.\n- Jena, J. K. and Vindhya Mohindra (2013) Application of genomics for enhancing aquaculture productivity and production. Lead lecture: International Symposium on Genomics in Aquaculture, 22-23 January, 2013 at CIFA, Bhubneshwar, Orissa. pp 5-15.\n- Kumar, Ravindra, N S Nagpure, Vindhya Mohindra, B Kushwaha, Mahender Singh and J K Jena (2013) Fish genomics research in India: A way forward. Theme paper for Expert Consultation on Fish Genomics Research in India: A Way Forward, organized by National Bureau of Fish Genetic Resources, Lucknow on August 2, 2013.\nPresented at International Forum\n- Mohindra, Vindhya, Ratnesh K. Tripathi, Tanushree Dangi, Rajeev K Singh, Rakshit Chaudhary, Trivesh Mayekar, B. Kushwaha, Rajesh Kumar, Kuldeep K. Lal, J. K. Jena, T. Mohapatra (2017) Draft genome of the anadromous Indian shad, Tenualosa ilisha. Poster Presented at The International Conference on the Status of Plant & Animal Genome Research, PAG ASIA 2017. May 29-31, 2017, held at Seoul, South Korea. Abstract No. P0049.\nCanal Ring Road\nLucknow - 226 002\nMobile: +91 (0522) 2440145 extension no. 233\nFax: +91 (0522) 2442403\nExternal profile URL","Understanding billfish stock structure is fundamental to managing them sustainably. For example, biological or genetic evidence of multiple stocks in a given species might indicate that multiple reference points like maximum sustainable yield and different management approaches might be warranted. Unfortunately, actual stock status is poorly understood in many billfish species.\nNadya Mamoozadeh, a doctoral candidate at the Virginia Institute of Marine Science, is investigating the stock structure of striped and white marlin as part of her graduate research. The goal of her work is to reduce uncertainties currently associated with the management and assessment of billfish by providing information on genetic stock structure and connectivity among stocks, including the geographic location and number of stocks in an ocean basin, and the degree of genetic connectivity among stocks and between ocean basins.\nHer specific research questions are: What is the genetic stock structure of white marlin in the Atlantic Ocean? What is the genetic stock structure of striped marlin in the Pacific and Indian oceans? And finally, what is the genetic relationship of striped marlin and white marlin?\nTo accomplish this, Mamoozadeh has relied on recreational anglers to provide her with genetic samples from around the world. In doing so, she sent participating anglers sampling kits that provided materials to preserve tissue samples, mostly consisting of fin clips, which were returned to her at the Virginia Institute of Marine Science. The DNA from each sample is extracted in the lab, and then it undergoes a process to characterize large numbers of molecular markers, which are the specific locations in an individual’s DNA that vary among individuals and are useful for evaluating the presence of genetic stock structure. She needed between 30 and 50 samples from each region in order to achieve a statistical inference.\nSome of Mamoozadeh’s work on white marlin has recently passed peer review and has been published in the ICES Journal of Marine Science. She analyzed 24 molecular markers from 479 adult and 75 larval white marlin from six geographic locations. To cut to the chase, she didn’t observe any evidence to suggest that there is more than one genetic stock of white marlin in the Atlantic Ocean. However, she states that this is not necessarily a straightforward conclusion, as the lack of genetic structure could reflect several different scenarios that are important to consider.\nThe first is that no genetic stock structure exists, and this is consistent with a lack of biological stock structure. This would mean there is enough gene flow among white marlin in the Atlantic to prevent the accumulation of appreciable genetic and biological differences among groups of white marlin from different regions. The second would be that there is no genetic stock structure, but biological stocks do in fact exist. This would mean there is just enough gene flow among white marlin in the Atlantic to prevent the accumulation of an appreciable level of genetic differences, but not enough gene flow to prevent biological differences from developing that are specific to groups of white marlin from different regions.\nA third conclusion would be that both genetic and biological stock structure is present, but the molecular markers characterized in her study were not powerful enough to detect it. In this case, using really large numbers of genetic markers is necessary to detect genetic stock structure, which is one of Mamoozadeh’s next research steps.\nWhile her results support ICCAT’s management approach of a single Atlantic stock of white marlin, it is somewhat incongruent with rising catch rates of whites in the U.S. mid-Atlantic region that we discussed in the last issue, and the limited satellite tag data that shows cyclical movement confined to one region of the Atlantic. I’m particularly looking forward to Mamoozadeh’s next research question, which determines if morphologically similar white marlin represent a distinct species or rather an Atlantic population of striped marlin."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:bd216646-3716-413c-88f3-634548a23a48>","<urn:uuid:02db36ac-be7a-42c8-92c6-91f57fc0dde3>"],"error":null}
{"question":"I'm researching early commercial vessels on the Pearl River. What was the first documented steamboat operation on the upper Pearl River, and what were its operational details?","answer":"The first definite record of a steamer on the upper Pearl River was found in The Mississippian on December 4, 1835. Captain James Latham announced he would operate the steamboat Choctaw between New Orleans and Jackson, making return trips throughout the season. Later, in May 1838, the steamer Alice Maria was used to transport lumber to Jackson for building the first state capitol.","context":["The Pearl River has its beginning in the historic area of the Nanih Waiya Indian Mounds of Winston County. There, legend has it, is where the Great Spirit told the Choctaw Indians to make their home. The river then flows southward through central Mississippi bringing with it the vast potential of an abundant water supply. Continuing through Jackson, the Pearl flows through Georgetown, Monticello, Columbia and on past NASA Test Facilities before it empties into the Gulf of Mexico.\nThe Pearl was discovered by Beinville, a French explorer. He named the river Perle (later changed to Pearl) when he supposedly found an abundance of pearls on the banks of the river. These pearls were from oyster shells which the Indians had used to clean out their canoes. Today, however, no one knows where either the shells or the pearls are. Many people have explored the Pearl. These include Louis LeFleur as he traveled the river selecting a site for a fur trading post. Indians of Choctaw, Chickisaw and Biloxi rowed up and down the river in canoes.\nThe river history and its relationship to early settlement, development and present life of the river area is one to be reckoned with. Businesses have been dependent on it for growth. When the white man came, he found savage Indians near the river. The swamps have been the hunting ground for the Indian and the white man for many years. As the area is still timbered, game is still plentiful. There are many kinds of wildlife along the river including deer, beaver, mink, turkey and various types of duck. Thirty-nine species of snakes are found in the area including six poisonous types. Also found are many numerous species of fish.\nAlong the Pearl, there have been and always will be many prominent towns. A few of those were Philadelphia, Rockport, Oma, Pearlington, Logtown and Gainesville. Philadelphia was an outlet for keel boats. Edinburg and Carthage had ferry crossing and steamboat landings. Jackson, the capital of Mississippi, was the site of LeFleurís trading post. Georgetown had a boat landing. Rockport had a ferry. Because the Pearl was so wide, it made shipping easy. Pearlington, Logtown and Gainesville became important industrial communities because of the timber along the banks. After depletion of the timber, however, these old towns have faded away and today with the Mississippi Test Facilities of NASA, the towns of Gainesville and Logtown have been completely obliterated.\nThe water history of the Pearl has also been interesting. In March of 1902, Charlie Freeny had the following entries in his diary:\nIn January 1940, the river froze over from bank to bank in the Carthage area. The older residents said this was the first time they had seen it frozen here. In strong current, the ice did not extend completely across the river, but in eddy places it did. In Edinburg, the ice was so thick, the children enjoyed skating upon it.\nKeel boats were used before and after the [Civil] war. The first one was owned by Harvey Gill. He made one trip each month from Carthage to New Orleans. The speed was slow - four miles per hour down and two, up. In the 1860ís, Sam Salter and Cap Atkinson built a keel boat for W. A. Burnisde. It was placed on the waters of Lake Burnside which runs into the Pearl in Neshoba County. It was propelled by the hook and jam method. One crew in front had a hook which was thrown around trees or roots along the banks. When the hook was pulled, the boat was propelled forward. A second crew had long poles which was used to push the boat. The trip to Jackson from Philadelphia by keel boat took about fifteen days. This keel boat took was sunk by the Yankees in 1864. Later, practically all farmers owned a boat since these transported farm products only.\nFor a number of years, the Pearl was navigable as far up the river as Edinburg. Small boats, some propelled by steam, made irregular runs to Carthage in the 1870ís. They were used to carry small cargoes to Carthage from the larger towns down river.\nThe first definite record of a steamer on the upper Pearl was found in The Mississippian on December 4, 1835. In the paper, Captain James Latham informed the public he intended to run the steamboat Choctaw from New Orleans to Jackson and return throughout the season.\nIn May of 1838, lumber was brought to Jackson on the steamer Alice Maria to build the first state capitol.\nIn 1840, there was a floating grocery store on the river. It consisted of the steamer Geneva and the barge Leman. For sale on his barge were 200 sacks of salt, fifty-three barrels of whiskey, fourteen of molasses, thirteen of sugar, fifteen bags of coffee, gunpowder and shot. Captain Winslow was willing to trade his supplies for cotton and deerskins.\nOn December 26, 1843, Marcus Hilzheim announced that he had made arrangements to run a small steamer between Hinds and Neshoba counties. On December 27, 1844, Mr. Hilzheim and A. E. Haynes announced that they would have a small steamer on the river in January. It would run from Carthage via Jackson to New Orleans. Mr. Haynes promised free passage to persons sending freight on the first trip. Thereafter, passage would be governed by the customerís freight. Cotton to New Orleans cost $1.50 a bale. To bring freight back, a dry barrel was fifty cents, 100 pounds was $2.50. General passage to New Orleans was ten dollars.\nOn January 13, 1848, Captain D. W. Boss advertised the \"Pearl River Steam Packet,\" the new steamboat Caroline. After the Caroline, no other boat was recorded on the upper Pearl until December 14, 1858. This was when Turner Ellis and J. H. Ledbetter announced that the Ranger would leave six days later for New Orleans. They said she would load there for all points between the mouth of the Pearl and Carthage. Thereafter, they stated that it would run between Jackson and Carthage. The Ranger was commanded by Ed Hart. It left New Orleans on January 7, 1859, and reached Jackson on the thirtieth. On February 22, the boat burned three miles above Grantís Mill. The boat and freight of 415 bales of cotton was lost. The boat carried no insurance and was an estimated loss of 25,000 dollars.\nAfter the Civil War, the first steamer on the Pearl was the Steadman, which ran around the 1870ís. Next on the river was the Oliver Clifton built by James Hamilton. In the 1880ís, it made weekly trips between Jackson and Carthage. It would leave Jackson every Saturday and would arrive in Carthage every Tuesday. The next day it would leave and would arrive again in Jackson on Friday.\nIn 1885, the O. R. Singleton was built. It ran from Edinburg to Jackson and in 1892 was bought by a group of Carthage merchants. Loaded with seventy-five tons, it took a week for the boat to go to Jackson and come back.\nThe first gasoline boat to serve the people of Leake County was Captain Henry Caldwellís Belle of the Bends. The late Mr. Perce Phillips, a worker on the boat said the following:\nThe boat sank when it hit a cypress snag at Buzzardís Bend. The motor, however, was salvaged and was used in the Juanita and still later in the Caldwell No. 3.\nRiver transportation was so important that the United States Government kept snagboats on the river to keep it free. The first was the Black Warrior. The last was the Pearl, which operated as far up the river as Rockport. In 1903, Dan Walker was the captain of the Pearl. The boat was thirty feet wide and about eighty feet long. It had a cabin for the workers with a private bunk and storage area for each man. it also had a steam powered crane which was used to pull objects out of the water and place them on the banks. Anything the crane could not move was dynamited out.\nIn 1844, it was announced that with about 75,000 dollars, the Pearl might be made navigable for small steamboats from the Gulf to Jackson three-fourths of the year. Also during winter and spring, this could extend 100 miles up river.\nIn 1948, the Jackson Chamber of Commerce saw the possibility of building a dam across the Pearl to provide a water supply and a recreation area.\nIn 1958, the Mississippi State Legislature created the Pearl River Valley Water Supply District and empowered them to construct and operate a reservoir on the Pearl River. Its construction was finished in 1962, and was named in honor of the former Governor Ross R. Barnett. The main dam is earth sixty-four feet high and three and one-half miles long. It has ten concrete spillways which have a total discharge of 180,000 cubic feet per second. The reservoir is a fifty square mile lake beginning just north of Jackson and ending six miles south of Carthage.\nPresently being completed on the Pearl, are many water parks. These parks will provide recreation and pleasure for local residents. Among their many features are boat ramps, picnic areas, camping spots and in some even tennis courts. These parks will be located in Philadelphia, Edinburg, Carthage, Riverside, Georgetown, Atwood, Columbia, Summit, Bogue Chitto, Walker Bridge and Walkiah.\nYes, from Nanih Waiya to NASA or from Philadelphia to the Gulf of Mexico, the Pearl River contains much history and many stories. It has fulfilled its purpose in the past and will continue to do so as it makes history.\n\"Bulldozer Canít Budge It - Observers Think Object is Old Anchor for River Ferry,\" Jackson Daily News (September 2, 1953).\nHarry and Donna Caldwell. Pearl River (June 11, 1975).\n\"Chamber Eyeing Building of Dam at Pearl River,\" Jackson Daily News (November 10, 1948).\nDearman, Mildred. \"Early Leake Procedures Unique,\" The Carthaginian (July 17, 1975), 2.\nDearman, Mildred. \"Sixty Years of the Good Life,\" The Carthaginian (March 11, 1971), 1a, 3b, 6b.\nFreeny, Charlie B. Diary excerpts. March 1902.\n\"Keel Boats of the Pearl,\" Neshoba County Microfilm, National Archives, Jackson, Mississippi.\nMcCraw, Edythe W. \"The Mighty Pearl,\" Mississippi News and Views (January 1963), 14.\n\"Navigation of Pearl River,\" The Mississippian (November 20, 1844).\n\"Pearl River Boatway Map Edition Two.\" (Pearl River Basin Development District, 1975).\n\"Pearl River Freezes Over,\" The Carthaginian, LXIX (February 1, 1940), 1.\nThigpen, S. G. Pearl River Highway to Glory Land (Kingsport, Tennessee: Kingsport Press, Inc., 1965), p. ix.\nThigpen, S. G. \"íSnag Boatí Kept Pearl River Safe.\" Jackson Daily News (December 1, 1971), 14a.\nWells, Frank C. and Humphreys, Jr., Canoy P. \"Pearl River Boatway Map from Ross Barnett Reservoir to Jackson.\" (U. S. Department of the Interior, 1974).\nReturn to Leake County Page\nBruce Reeves / 3337 McGowen St / Houston TX 77004\n©1997-2020 Bruce Reeves"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:0827b462-b187-4fc9-a5a5-a626f8651e07>"],"error":null}
{"question":"What were the challenges of the French Revolutionary Calendar's design, and what were Napoleon's major mistakes during his reign?","answer":"The French Revolutionary Calendar faced several design challenges - its terminology was only appropriate for France's climate and not the Southern Hemisphere, it failed to replace Christian terminology despite attempting to create a new cult, and its decimal-based ten-day weeks were unpopular with workers who preferred the traditional seven-day week. The calendar's implementation was also complicated by ambiguous rules for leap years. As for Napoleon, he made three major costly mistakes that led to his downfall: the Continental System, the Peninsular War, and the Invasion of Russia. These mistakes, combined with the 1812 disastrous French invasion of Russia, ultimately led to his abdication and exile.","context":["French Revolutionary Calendar\nConverting dates since 1792\n|Crane Brinton's discussion of the calendar||The names of the months|\nWhy Conversion Programs Produce Varying Results\nThere are several internet programs which convert dates between the Gregorian and the French Revolutionary (Republican) calendars. These programs usually provide for the conversion not only of dates during the period when the French calendar was in use but also of dates subsequent to Napoleon's discontinuation of the calendar on Januray 1, 1806. Conversion of dates between 1792 and the end of 1805 is of utility to historians and genealogists. Conversion of subsequent dates is mostly of interest to hobbyists and francophiles (though the French Revolutionary Calendar was briefly revived by the Paris Commune in May 1871*).\nAlthough the new calendar was not adopted until October 1793, the first day of the first year was retrospectively set for the day the Republic had been proclaimed, which happened to coincide with the autumnnal equinox in 1792: thus September 22, 1792 = 1 Vendémiaire I. A normal French calendar year of 365 days was to consist of twelve 30-day months followed by 5 \"complementary\" days, the \"sans-culottides,\" at the end of the year, i.e. in late September.\nThe original scheme was to make every year start on the day of the autumnal equinox, as observed and calculated in Paris. To accomplish this, a leapday would be inserted as a sixth \"complementary\" day if it was needed to make the following year start on the autumnal equinox. This resulted in leap years in years III, VII and XI, with leap years projected for the years XV and XX. So leap years would be determined by an astronomical observation and, unlike the Gregorian calendar, would not occur at regular four year intervals. Note the five year interval between XV and XX.\nHowever, the French declaration establishing the Revolutionary Calendar was itself somewhat ambiguous, even contradictory, in laying out a method for inserting leapdays in perpetuity, and a variety of proposed alternative methods arose. For example, the head of the commission which proposed the Calendar, Charles-Gilbert Romme**, subsequently suggested that leap year calculation be simplified by borrowing from the Gregorian calendar the 4-100-400 rule***, replacing astronomical observation with the simplicity of arithmetic and yielding a more regular cycle. Some of these alternatives are well discussed in Wikipedia.\nConversion of dates since January 1, 1806, is made problematic (and essentially speculative) by these differing methods. It is the differences among these methods which cause most of the apparently inconsistent results given by the programs on the internet.\nThere is another source of divergence, however: namely a defect in the implementation of the\ncentury years when using the Date(mm/dd/yyyy) function (although it gets 1900 and years divisible by 400 right).\nThis defect exists even in the current version (6.0.4). The converter on this page avoids the problem with a small\nworkaround, kindly provided by calendar maven Steve Morse.\n* May 6th to May 23rd: 16 Floréal to 3 Prairial LXXIX.\nLinks to conversion programs with particular features\n|Stephen Morse's on-line program\nAllows the user to select any of three defined methods for handling leap years. Allows conversions both to and from the French calendar.\n|José Luis Martin Mas's Dashboard Widget for the Mac\nConverts (only) the current date to Revolutionary Calendar format, using the 4-128 rule for leap year calculations, i.e. a leap year is a year divisible by 4, unless divisible by 128.\n|José Luis Martin Mas's iPhone/iPad app: \"Calendrier\"\nThe user can choose between the Equinoctal rule and the Romme rule for leap year handling. ($1.99)\nThe names of the months\nCrane Brinton on the French Revolutionary calendar\nThe culmination...of revolutionary propaganda [was] its new calendar. Almanacs had been from the beginning of the Revolution a favorite and successful method of spreading the word. Collot d'Herbois himself had won, with his Almanach du Père Gérard, a prize offered by the Paris Jacobins for a work to spread the new ideas in simple language.\nBut for the Jacobins of 1794 it was not enough to print good republican moral counsels, after the manner of Franklin, at the appropriate dates and seasons. The whole calendar must be made over. The existing calendar perpetuated the frauds of the Christian church (Jesus himself was probably a good sans-culotte; all the nonsense stemmed from Paul), and was highly irrational and inconvenient.\nThe new calendar, based on a report of Fabre d'Églantine, was adopted by the Convention in October, 1793. By it the year began on September 22 of the old calendar, and was divided into twelve months of thirty days each, leaving five days (six in leap years) over at the end of the last month. These five or six days were to be known as the Sans-culottides, and were to be a series of national holidays. Each month was divided into three weeks, called décades, the last day of each décade being set aside as a day of rest corresponding to the old Sunday.\nThe months were grouped into four sets of three, by seasons, and given \"natural\" names, some of which are rather attractive--vendémiaire, brumaire, frimaire (autumn); nivôse, pluviôse, ventôse (winter); germinal, floréal, prairial (spring); messidor, thermidor, fructidor (summer). The days of the décade were named arithmetically--primidi, duodi, on to décadi. In place of the old saints' days, each day was dedicated to a suitable fruit, vegetable, animal, agricultural implement.\nThe Sans-culottides were dedicated, the first to Genius, the second to Labor, the third to Noble Actions, the fourth to Awards, and the fifth to Opinion. This last was to be a sort of intellectual saturnalia, an opportunity for all citizens to say and write what they liked about any public man, without fear of the law of libel. The sixth Sans-culottide of leap years was dedicated to the Revolution, and was to be an especially solemn and grand affair. The republican era was to date from the declaration of the republic in September, 1792. When the calendar came into use, the year I had already elapsed.\nIn spite of its symmetry and its poetic months of budding and of mist, the new calendar was not a success, and Napoleon abandoned it....Workingmen preferred one day's rest in seven to one in ten; its terminology, appropriate to the climate of France, was singularly inappropriate to that of the Southern Hemisphere; it embodied a new cult, and that cult, though it profoundly influenced Christians then and since, failed completely to supplant Christian terminology. The calendar and its fate form in many ways a neat summary of Jacobin history.\n--from A Decade of Revolution, 1789-1799 (1934)\nvisits since 1998:","Napoleon played a key role in the French Revolution (1789–99), served as first consul\n- 1 What was Napoleon's impact?\n- 2 What was Napoleon's most significant reform?\n- 3 What was Napoleon's most significant accomplishment and what did it involve?\n- 4 What were Napoleon's greatest accomplishments?\n- 5 What was the long term significance of Napoleon for Europe?\n- 6 What were Napoleon's 3 biggest mistakes?\n- 7 What did Napoleon do that was good?\n- 8 What are five major accomplishments of Napoleon?\n- 9 What are three of Napoleon's biggest accomplishments?\n- 10 What were Napoleon's 4 reforms?\n- 11 What are the important reforms of Napoleon?\n- 12 Why Napoleon is a hero?\n- 13 What were Napoleon's ideas about slavery?\n- 14 Why did Napoleon hide his hand?\n- 15 What was Napoleon's legacy?\n- 16 How did Napoleon lose power?\n- 17 How did Napoleon get defeated?\n- 18 What was the impact of Napoleon and his wars on Europe?\n- 19 Why is Napoleon known as the modernization of Europe?\n- 20 What was the lasting impact of the Napoleonic Code?\n- 21 What were Napoleon's accomplishments during peacetime?\n- 22 What made Napoleon a genius?\n- 23 What was the significance of the Battle of Waterloo?\n- 24 What was Napoleon's largest ally?\n- 25 How did Napoleon change the government?\n- 26 What were three important reforms Napoleon made as emperor?\n- 27 Why was Napoleon a hero essay?\nWhat was Napoleon's impact?\nHe had the laws of post-revolutionary France brought together into a coherent whole. This Civil Code removed the privileges of the aristocracy, ensured property rights, and created greater equality. In doing so, it shifted the focus of the law to benefiting the middle class.\nWhat was Napoleon's most significant reform?\nReforms in Education: Although he did not create a system of mass education, education was more available to the middle class than it ever had been before. At a meeting in 1807 he declared: Of all our institutions public education is the most important.\nWhat was Napoleon's most significant accomplishment and what did it involve?Undoubtedly, Napoleon’s greatest achievement was the spreading of French Revolutionary ideas across Europe and ultimately the world, which would lead to the revolutions of 1830, 1848, and other efforts by the masses to achieve true libertie, egalite, et fraternitie.\nWhat were Napoleon's greatest accomplishments?\nNapoleon instituted reforms in post-revolutionary France, starting with a complete overhaul of military training. He also centralized the government, reorganized the banking and educational systems, supported the arts, and improved relations between France and the pope.\nWhat was the long term significance of Napoleon for Europe?\nHe worked hard to create a unified Italy, Poland, and Germany. Napoleon was also responsible for sweeping away many of the old regimes and promoting the ideals of equality and European solidarity. Sure, the old regimes still had some life in them when Napoleon left the scene, but things were never really the same.\nWhat were Napoleon's 3 biggest mistakes?\nNapoleon made three costly mistakes that led to his downfall. The first mistake was The Continental system. The second mistake was The Peninsular War. The third mistake was The Invasion of Russia.\nWhat did Napoleon do that was good?Napoleon Bonaparte was a French military general, the first emperor of France and one of the world’s greatest military leaders. Napoleon revolutionized military organization and training, sponsored the Napoleonic Code, reorganized education and established the long-lived Concordat with the papacy.\nWhat are five major accomplishments of Napoleon?\n- National Bank. In 1800, he established the Bank of France which stabilized the economy by issuing paper money that was backed by a large gold reserve.\n- Louisiana Purchase. …\n- Conquers Europe. …\n- Meritocracy. …\n- Education Reforms. …\n- Concordat of 1801. …\n- Napoleonic Code.\nHis strong rapport with his troops, his organizational talents, and his creativity all played significant roles. However, the secret to Napoleon’s success was his ability to focus on a single objective. On the battlefield, Napoleon would concentrate his forces to deliver a decisive blow.Article first time published on askingthelot.com/what-was-napoleons-significance/\nWhat are three of Napoleon's biggest accomplishments?\n- #1 He demonstrated exceptional military skills during the Siege of Toulon. …\n- #2 He won the Battle of 13 Vendémiaire to end the threat to the Revolutionary Government. …\n- #3 He led the French to victory in the Italian Campaign against the First Coalition.\nWhat were Napoleon's 4 reforms?\n- Napoleon’s First Reform (Paying off the Debt) -Set prices of goods (bread) …\n- Napoleon’s Second Reform (Made peace with the Catholic Church) -Napoleon made a deal with the Pope. …\n- Napoleon’s Third Reform (Gave more rights to the people) -Émigrés: Can return to Paris if they take an oath of loyalty.\nWhat are the important reforms of Napoleon?\nTwo reforms inroduced by Napoleaon Bonaparte were: He abolished privileges based on birth, established equality before law and secured the right to property. He introduced uniform systems of weights and measures.\nWhy Napoleon is a hero?\nNapoleon was not only a great leader, he also was a military genius. As a military genius, Napoleon won many battles to expand France and was always welcomed back to France as a hero. His use of strategic warfare throughout many battles allowed him to be seen as a hero not only in France but all of Europe.\nWhat were Napoleon's ideas about slavery?\nNapoleon argued he was “maintaining” slavery, since its formal abolition had not actually been realized. He hoped to encourage the return of French settlers to the colonies, believing they were better able than the blacks to defend French interests against the British.\nWhy did Napoleon hide his hand?\nIt has been said that he hid his hand within the fabric of his clothing because the fibers irritated his skin and brought him discomfort. Another perspective holds that he was cradling his stomach to calm it, perhaps showing the early signs of a cancer that would kill him later in life.\nWhat was Napoleon's legacy?\nConsidered to be his greatest legacy, Napoleon’s Civil Code assured the spread of the ideals of the French Revolution long after the end of his rule. But, it was through the image he presented of himself that the people of Europe found a symbol of revolutionary change.\nHow did Napoleon lose power?\nAfter seizing political power in France in a 1799 coup d’état, he crowned himself emperor in 1804. … However, after a disastrous French invasion of Russia in 1812, Napoleon abdicated the throne two years later and was exiled to the island of Elba.\nHow did Napoleon get defeated?\nThe Waterloo Campaign (June 15 – July 8, 1815) was fought between the French Army of the North and two Seventh Coalition armies, an Anglo-allied army and a Prussian army, that defeated Napoleon in the decisive Battle of Waterloo, forced him to abdicate for the second time, and ended the Napoleonic Era.\nWhat was the impact of Napoleon and his wars on Europe?\nNapoleon’s conquests cemented the spread of French revolutionary legislation to much of western Europe. The powers of the Roman Catholic church, guilds, and manorial aristocracy came under the gun. The old regime was dead in Belgium, western Germany, and northern Italy.\nWhy is Napoleon known as the modernization of Europe?\nNapoleon saw his role as a moderniser of Europe. He introduced many laws such as the protection of private property and a uniform system of weights and measures provided by the decimal system.\nWhat was the lasting impact of the Napoleonic Code?\nIt codified several branches of law, including commercial and criminal law, and divided civil law into categories of property and family. The Napoleonic Code made the authority of men over their families stronger, deprived women of any individual rights, and reduced the rights of illegitimate children.\nWhat were Napoleon's accomplishments during peacetime?\nQ. What was Napoleon able to accomplish during peacetime? He set up government-run public schools. He set up a comprehensive system of laws.\nWhat made Napoleon a genius?\nNapoleon was a military genius in the strategic and tactical handling of armies and although he provided no large scale reforms of armies, or their equipment and techniques, he excelled at the refinement of an art that already existed.\nWhat was the significance of the Battle of Waterloo?\nThe Battle of Waterloo brought an end to the Napoleonic Wars once and for all, finally thwarting Napoleon’s efforts to dominate Europe and bringing about the end of a 15-year period marked by near constant warring.\nWhat was Napoleon's largest ally?\nNapoleon, who considered Russia a natural ally since it had no territorial conflicts with France, soon moved to teach Alexander a lesson. In 1812 the French emperor raised a massive army of troops from all over Europe, the first of which entered Russia on June 24.\nHow did Napoleon change the government?\nNapoleon had a powerful intellect and worked at a feverish pace. Starting in 1800 he reformed the chaotic Financial system by borrowing money to deal with short term expenses and creating a tax system that indirectly favored the elite. He also hired tax collectors to insure that the taxes made it to the Government.\nWhat were three important reforms Napoleon made as emperor?\nNapoleon served as first consul of France from 1799 to 1804. In that time, Napoleon reformed the French educational system, developed a civil code (the Napoleonic Code), and negotiated the Concordat of 1801. He also initiated the Napoleonic Wars (c.\nWhy was Napoleon a hero essay?\nNapoleon was not only an excellent leader but also a dominate war leader. Napoleon is a hero because he gave people rights and freedoms, and formed an amazing educational system. Along with his excellent skills with militarism, he became one of the greatest French leaders and heroes."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:847211f0-681c-4bae-8dee-58b62da0233a>","<urn:uuid:a3b452ad-18e6-4f7b-9720-5370a6da252b>"],"error":null}
{"question":"What failure recovery mechanisms exist for resource managers, and how does modern security verify system access?","answer":"Resource managers implement failure recovery through durable storage and transaction reconstruction. When a durable resource manager fails, it can restart by reconstructing the committed state using prepare information written during the prepare phase, and then commits or aborts transactions accordingly. Only transactions that were prepared or committed prior to failure survive. For system access verification, Zero Trust security employs a comprehensive approach including multi-factor authentication, biometric authentication, and device integrity verification before granting access. This verification process ensures protection against unauthorized access and sophisticated cyber attacks, especially in modern cloud-based environments.","context":["Implementing a Resource Manager\nEach resource used in a transaction is managed by a resource manager, whose actions are coordinated by a transaction manager. Resource managers work in cooperation with the transaction manager to provide the application with a guarantee of atomicity and isolation. Microsoft SQL Server, durable message queues, in-memory hash tables are all examples of resource managers.\nA resource manager manages either durable or volatile data. The durability (or conversely the volatility) of a resource manager refers to whether the resource manager supports failure recovery. If a resource manager supports failure recovery, it persists data to durable storage during Phase1 (prepare) such that if the resource manager goes down, it can re-enlist in the transaction upon recovery and perform the proper actions based on the notifications received from the transaction manager. In general, volatile resource managers manage volatile resources such as an in-memory data structure (for example, an in-memory transacted-hashtable), and durable resource managers manage resources that have a more persistent backing store (for example, a database whose backing store is disk).\nIn order for a resource to participate in a transaction, it must enlist in the transaction. The Transaction class defines a set of methods whose names begin with Enlist that provide this functionality. The different Enlist methods correspond to the different types of enlistment that a resource manager may have. Specifically, you use the EnlistVolatile methods for volatile resources, and the EnlistDurable method for durable resources. For simplicity, after deciding whether to use the EnlistDurable or EnlistVolatile method based on your resource's durability support, you should enlist your resource to participate in Two Phase Commit (2PC) by implementing the IEnlistmentNotification interface for your resource manager. For more information on 2PC, see Committing a Transaction in Single-Phase and Multi-Phase.\nBy enlisting, the resource manager ensures that it gets callbacks from the transaction manager when the transaction commits or aborts. There is one instance of IEnlistmentNotification per enlistment. Typically, there is one enlistment per transaction, but a resource manager can choose to enlist multiple times in the same transaction.\nAfter enlistment, the resource manager responds to the transaction's requests. A durable resource manager stores enough information to allow it to either undo or redo the transaction's work on resources it manages. There are many ways to do this; keeping versions of data or keeping a log of the changes are two common techniques.\nWhen the application commits the transaction, the transaction manager initiates the two-phase commit protocol. The transaction manager first asks each enlisted resource manager if it is prepared to commit the transaction. The resource manager must prepare to commit—it readies itself to either commit or abort the transaction.\nDuring the prepare phase, the durable resource manager records the old and new data in stable storage so that the resource manager can recover it even if the system fails. If the resource manager can prepare, it informs the transaction manager its vote on whether to commit or abort the transaction. If any resource manager reports a failure to prepare, the transaction manager sends a rollback command to each resource manager and indicates the failure of the commit to the application.\nOnce prepared, a resource manager must wait until it gets a commit or abort callback from the transaction manager in phase 2. Typically, the entire prepare and commit protocol completes in a fraction of a second. If there are system or communication failures, the commit or abort notification may not arrive for minutes or hours. During this period, the resource manager is in doubt about the outcome of the transaction. It does not know whether the transaction committed or aborted. While the resource manager is in doubt about a transaction, it keeps the data modified by keeping the transaction locked, thereby isolating these changes from any other transactions.\nWhen a resource manager fails, all of its enlisted transactions are aborted except for those that are prepared or committed prior to the failure. When a durable resource manager restarts, it reconstructs the committed state of the resources it manages by retrieving the prepare information written in the prepare phase, and commits or aborts these transactions accordingly.\nIn summary, the two-phase commit protocol and the resource managers combine to make transactions atomic and durable.\nThe Transaction class also provides the EnlistPromotableSinglePhase method to enlist a Promotable Single Phase Enlistment (PSPE). This allows a durable resource manager (RM) to host and \"own\" a transaction that can later be escalated to be managed by the MSDTC if necessary. For more information on this, see Optimization using Single Phase Commit and Promotable Single Phase Notification.\nIn This Section\nThe steps generally followed by a resource manager are outlined in the following topics.\nDescribes how a durable or volatile resource can enlist in a transaction.\nDescribes how a resource manager responds to commit notification and prepare the commit.\nDescribes how a durable resource manager recovers from failure.\nDescribes how the three levels of trust for System.Transactions restrict access on the types of resources that System.Transactions exposes.\nDescribes optimization practices available to implementations of resource managers.","What Is Zero Trust Security?\nZero Trust security is a high-standard security model designed to address today’s cyber concerns. This model focuses on verifying the identity of users and devices before granting access to sensitive data and systems rather than assuming access requests are automatically valid. In addition, Zero Trust security uses cloud-based literacy to protect against cyber-attacks and data leaks that traditional security measures such as firewalls and intrusion detection systems cannot. This makes it the ideal security model for database management systems.\nThe concept of Zero Trust security originated in the early 2000s as a response to the growing threat of cyber attacks and the increased use of mobile devices and cloud-based services. Traditional security models rely on a “perimeter” approach in which a firewall protects the network from external threats, and intrusion detection systems are used to detect and respond to attacks that make it past the firewall. However, it has become increasingly difficult to define and protect a network perimeter with the recent rise in remote personnel, hybrid cloud environments, and ransomware threats. Zero Trust security addresses these issues with a security system that is as mobile as the modern world.\nAs noted before, one of the key principles of Zero Trust security is that all devices and users must be verified before being granted access to sensitive data and systems. This verification process can include a combination of multi-factor authentication, fingerprinting, and behavioral analysis. For example, a user might be required to enter a password, use a biometric authentication method, and prove that their device has not been compromised before being granted access to a sensitive system. This ensures that the company’s security is not compromised, no matter when or where staff must access databases.\nBenefits of Zero Trust Security For Your Database Management Systems\nOne of the main benefits of Zero Trust security is that it is designed to protect against advanced persistent threats (APTs) and other sophisticated cyber attacks. APTs are attacks designed to evade traditional security measures and remain undetected for a prolonged period. By filtering all devices and users through strict verification protocols, Zero Trust security can detect and respond to these types of attacks more effectively.\nAnother benefit of Zero Trust security is that it is designed to work in a cloud-based environment. As more and more companies move their data and systems to the cloud, protecting them using traditional security models has become increasingly challenging. Zero Trust security is designed to work in a cloud-based environment by verifying the identity regardless of the source of an inquiry.\nIn the context of database management systems, Zero Trust security is paramount because it can help protect sensitive data from unauthorized access and breaches. Since database management systems are often used to store sensitive information such as financial data, personal information, and confidential business information, security is of the utmost importance. By implementing Zero Trust security, companies can be sure that this sensitive information is safeguarded.\nAdditionally, by verifying all users and devices before allowing a query to be executed, Zero Trust security can also help protect against attacks that exploit vulnerabilities in the database management system itself. This makes it possible for the security system to detect and respond to any attempts to exploit SQL injection vulnerabilities before they happen.\nZero Trust security is a modern security model that can help protect sensitive data and systems from cyber attacks and breaches. By setting up integral verification systems, Zero Trust security can detect and respond to advanced persistent threats and other sophisticated cyber attacks more effectively. With Zero Trust security, database management systems safeguard against unauthorized access and attacks that exploit vulnerabilities in the system itself, ensuring that secure information stays that way.\nFor more questions or more information, please contact us."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:f2d16331-3706-482a-aa31-f2c76721d099>","<urn:uuid:e3dcca23-5a3b-4527-aa7c-eac015e4f848>"],"error":null}
{"question":"What organizational tools are recommended to help students keep track of their school materials?","answer":"Students should use two folders: one for work to TAKE TO SCHOOL and one for materials to TAKE HOME. These folders should be reviewed daily both at home and at school. Additionally, calendars, schedules, and logbooks should be used to record assignments or daily events. Teachers should provide schedules of tests, reports, and assignments to parents several weeks before due dates.","context":["To develop better memory skills you must teach a child how to pay careful focused attention and how to engage actively in thinking about what he is learning.\nUse everyday activities\n1. Establish regular locations for essential items. Key, wallet, shoes, backpack etc. should always be stored in the same location. Practice a daily routine for putting items away that is initially supervised and then can be completed with greater independence.\n2. To remember recent events, have your child repeat out loud what just happened.\n3. To remember important information, the following may be helpful:\n- repeat out loud the main points\n- make up rhymes, acronyms, or letter associations\n- relate the new information to something familiar\n- relate the information to a personal experience\nRhyme: Shirley has curly hair.\nLetter Association: Remember the names of the seven continents by their beginning letters, 6A’s (Asia, Africa, Antarctica, Australia, Americas – North and South) and one E (Europe).\nMnemonic: Remember Dad (Divide)– Mother (Multiply) – Sister (Subtract) – Brother (Bring Down) as an aid to recall the steps for long division.\n4. When asking your child to recall information, remind him of the situation in which the information was first learned.\n5. Repeat back a short statement of the relevant information. For example, “Okay, so you said we are going at 8:00? And that is on Tuesday, right?” Repeating the most important information allows your student to “tag” this information for his memory and also assures the other person in the conversation that he did in fact pick out and note the important information.\nChange the environment:\n1. Focus your student’s attention on specific information: “I’m going to read a story and ask WHO is in the story.”\n2. Underline or highlight key words in a passage for emphasis and to help with recall.\n3. Include pictures or visual cues with verbal information.\n4. Provide a daily contact person to review your student’s schedule, materials needed, and assignments, both before and after school.\n5. Use a “back-and-forth” notebook to convey necessary information to teachers, and parents, which your child may forget.\n6. Provide your student with two folders, one for work to TAKE TO SCHOOL, and one for materials to TAKE HOME. Review each folder with your student both at home and at school, daily.\n7. Use calendars, schedules and a logbook to record assignments or daily events.\n8. Provide a printed or pictured schedule of daily activities, locations, and materials needed for each class or activity.\n9. Use aids such as:\n- responsibility chart\n- alarm watch\n- tape recorder,\n- spell checker\n10. Teachers should provide a schedule of tests, reports, and assignments to parents several weeks prior to due dates.\n11. It is going to be difficult for your child to demonstrate all that he knows about a topic on tests that rely on free recall of information. It will be more helpful to allow him to use aids such as a vocabulary list, open book and open notes test formats, and test questions in a multiple choice or matching format.\n12. It is easier to remember information if the student has some familiarity with the general content. Inform parents about upcoming content across classes so that they are able to provide background and activities at home that relate to the topic.\nTeach new skills:\n1. The current set of recommendations addresses the later stage, please review the attention cluster for specific recommendations regarding attention.\n2. Teach your student to form a mental picture of information that is presented orally.\n3. Develop a “procedures” or “how to” notebook so that your student can function as independently as possible, when carrying out routine tasks. For example, list mnemonics, steps for computer operation, use of microwave, etc.\n4. Teach paraphrasing skills. The ability to recite information in one’s own words is a powerful memory aid.\n5. Teach your student not to rely exclusively on mental memory. For example, when carrying a number in an addition problem your child should always write it down, above the column, rather than trying to remember it.\n6. Teach your student to use a mini-tape recorder to record assignments, appointments, and announcements in class and important summary information from conversations. She should review and transfer this information onto a written calendar daily.\nThe BrainSTARS manual was written by a team of professionals who have worked for many years with children and young adults who have brain injury. We wrote it because pediatric brain injury is very confusing for parents and teachers — and you are the most important people in the recovery of your child. It is important that a child's parents and teachers are well-educated so that they can work well together to provide the best chance for a child's recovery. Our goal is to make sure that every child has a safety net of support and understanding underneath him as he makes the leap back into life following a brain injury.\nFrom BrainSTARS, Brain Injury: Strategies for Teams And Re-education for Students, © 2002 Jeanne Dise-Lewis, PhD. Used with permission. The manual is available in English and Spanish. For more information or to order copies, call 720.777.5470 or email@example.com. A short video on how to use the BrainSTARS manual is available at www.youtube.com/BrainSTARSprogram."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:f0d45258-a188-4a0a-b1e7-8dd084635064>"],"error":null}
{"question":"As a database administrator, I'm curious about how parallel database updates work and what their limitations are. Could you explain both the process and constraints?","answer":"Parallel database updates work by assigning unique Process IDs to different processes from a predefined set (e.g., 00-99), ensuring no two parallel processes use the same identifier. Each process can only update records associated with its Process ID, allowing multiple processes to work simultaneously without hindering each other. However, there are key limitations: the process only works for Graphs (Jobflow and Profiler jobs cannot run in parallel), and neighboring components must have the same cardinality in their node allocation. Additionally, node allocation behind ParallelGather and ParallelMerge must have cardinality 1, and components in front of ParallelPartition must also have cardinality 1.","context":["|Publication number||US7437355 B2|\n|Application number||US 10/874,171|\n|Publication date||Oct 14, 2008|\n|Filing date||Jun 24, 2004|\n|Priority date||Jun 24, 2004|\n|Also published as||US20050289094|\n|Publication number||10874171, 874171, US 7437355 B2, US 7437355B2, US-B2-7437355, US7437355 B2, US7437355B2|\n|Original Assignee||Sap Ag|\n|Export Citation||BiBTeX, EndNote, RefMan|\n|Patent Citations (7), Referenced by (4), Classifications (14), Legal Events (4)|\n|External Links: USPTO, USPTO Assignment, Espacenet|\n1. Field of the Invention\nEmbodiments of the present invention generally relate to computers. More particularly, embodiments relate to methods and systems for parallel updates of computer databases.\n2. Background of the Invention\nIt is generally known that data may be stored in a tabular form. Conceptually, tables are comprised of multiple records. In a two-dimensional table, each record is associated with one row. Each row may comprise one or more cells. Columns in the table each represent a specific field with the same meaning for all records. As used herein, tables are equivalent to database tables and accordingly, rows may be referred to alternatively as “rows” or “records,” or “database records.” Columns may be referred to alternatively as “columns” or “fields.” For purposes of description herein, a first row in a table is called a “header row.” The cells of the header row may contain a description or identification of the content of the cells below it.\nThe data stored in tables may be provided as input to numerous reports. Some reports, for example, may summarize the data stored in the many fields in a table. The reports may be useful for such things as tracking the objectives of a business or the availability of commodities. Examples of business objectives may include the total amount of costs attributed to a cost center in a certain month or over a certain period, the amount spent on servicing multiple accounts, or the year to date income from all sources to a cost center. Examples of availability of a commodity may include the number of toy rockets available for purchase from all stores in a nationwide chain of stores. These examples are meant to be illustrative and not limiting.\nGenerally, and for purposes of description herein, data in a table may be accessed either by an individual process or by a mass process. An individual process may be a dialog session in which an individual user gains access to the table for purposes that may include generating reports or updating the table. For example, the user may have an invoice verification application where the user may post an invoice for a given cost center. Such an application will typically require the entry of cost center identification information and invoice amount into a dialog box on a computer interface unit, such as a keyboard and video terminal. The entry will result in a change in value of a cell of a record, associated with the cost center. A mass process, on the other hand, may be exemplified as a background process run for a multitude of entries. For example, a mass process may post debit memos to the table for all customers. In mass processes, one may receive the information via an electronic file. This file may be processed in the background. In fact, more than one file may be received and may run in parallel. Each file in a mass process file may contain several hundred or several thousand postings for execution.\nIt is conceivable that mass processes running in parallel may be accomplishing the same task (e.g., posting invoices or payments in parallel). In one example, when a first process accesses the table to change a specific record in the table, the first process may be given exclusive control of the specific record; other processes will not be able to edit the specific record while it is under the exclusive control of the first process. Only after the first process has relinquished its exclusive control of the record will other processes be able to again edit this record. In a system used by more than one user or accessed by more than one process, the ability of multiple users or processes to gain access to any given record is problematic and may cause a bottleneck if the multiple users or processes attempt to access or use the same record contemporaneously. A typical solution may involve a database administration system, which may form a single queue for the multiple parallel processes, where each of the multiple parallel processes waits in the queue for its turn to serially update (e.g., sum or input data to) the subject record.\nThe various advantages of embodiments of the present invention will become apparent to one skilled in the art by reading the following specification and appended claims, and by referencing the following drawings.\nIn an embodiment, the invention provides for methods, systems, and computer readable storage medium for performing database updates by parallel processes. In an embodiment, the invention provides for a method for assigning identifiers to a plurality of processes, wherein the identifiers are selected from a predefined set of identifiers, and where no two processes running in parallel use the same identifier. In an embodiment, the invention provides for receiving a first request from a first process, having assigned thereto a first identifier, for updating a first record in a set of records stored in a database; determining if the first record is associated with the first identifier in the database; and updating the first record if the first record is associated with the first identifier in the database.\nThe problem of multiple parallel users or processes being excluded from simultaneous access to a given record in a table may be solved by adding a field to the table and by modifying an overall process control to account for the added field. One method of changing the design of the table, in accordance with an embodiment of the invention herein, may be to add a field to a table to demarcate individual records for use only by a given process. Such a field may be referred to herein as a process identification (“PROCESS ID”) field. The process identification field may allow a plurality of processes to operate on separate records while serving to prohibit various parallel processes from updating the same record. Furthermore, in accordance with an embodiment of the invention disclosed herein, a process may dynamically update the table by adding another row to the table if a row did not previously exist that could be demarcated for use by the process.\nTo compare and contrast the basic table of\nBy way of example, let it be assumed that two processes are to be executed, a first process will add 5 to Cost Center 1 and a second process will add 10 to Cost Center 1. A method that makes use of the basic table of\nIn contradistinction, in accordance with an embodiment of the invention, a method may be executed that allows multiple processes to access different records for the same cost center simultaneously, eliminating the series bottleneck associated with the prior described method. By way of example, using the expanded table of\nFurther, now assume that a new process, process n+1, has begun. The n+1th process may attempt to add a value of 15 to the amount field of Cost Center 1. The n+1th process may attempt to identify a set of records associated with PROCESS ID n+1, but, in the table as illustrated in\nThe expanded table 200 could be shared among any number of multiple processes. Each process may add as many records to the table as it may require. For reporting, the records that share identical first fields but whose PROCESS ID fields differ in value (e.g., records of rows 214 and 230) may then be merged and their values summed together. For example, records 220 and 221 in the third field 210 may be summed together because their associated records 216, 217, respectively, in the first field 202 are identical, while their associated records 218, 219, respectively, in the second field 206 differ.\nThe method of\nIt will be noted that the probability of a record being used by a second process, as evaluated at 308,\nIt is noted that a method to update values (e.g., amounts, quantities) that are assigned to an object (e.g., cost center, general ledger account, material) that allows parallel running processes to update the values of the same object(s) without hindering one another may include an assigning of a PROCESS ID to every running process taken from a predefined set of PROCESS IDs (e.g. 00, 01, . . . 99) in a way that no two processes running in parallel will use the same PROCESS ID. In one embodiment, processes that are finished may release their PROCESS ID. The PROCESS ID thus becomes available for another process that requires one. Those of skill in the art will know of methods in data processing which can ensure the required unique allocation of PROCESS Ids, accordingly, such methods are not described herein.\nIt is also noted that a method to update values that are assigned to an object that allows parallel running processes to update the values of the same object(s) without hindering one another may involve the use of a heretofore unknown structure of database records, where the values assigned to the objects are stored. The new structure may differ from the known structures that are technically designed to store all the relevant data (e.g., object key(s) and object value(s)) in so far, that a unique process identification field (e.g., PROCESS-ID) is added. This field may be used as a key field to distinguish records.\nIt is further noted that in an embodiment, any update or insert of records to the database table, that is executed by a process, will affect only records where the unique process identifier field includes the value assigned to the process (e.g., a first process may have a value of 00 assigned to it).\nIn accordance with one embodiment, if more than a foreseen number of PROCESS IDs are required at the same time (e.g., one-hundred-and-one PROCESS IDs are required, but the foreseen set is 00, 01, . . . , 99), the first PROCESS ID (here 00) may be taken for the one hundred and first process, accepting, that two or more processes now work with the same Process ID.\nIn accordance with an embodiment, any retrieval function (e.g., a function that may determine a total value assigned to a cost center, general ledger account, material, etc.) may read all records assigned to the object. All these records contain the same object key (e.g., identification of the cost center, general ledger account, material, etc.) but different values for the PROCESS IDs. The total value assigned to the object may be calculated as the sum of all records retrieved.\nIn an embodiment, the values to be used in the PROCESS ID field for mass processes and the values to be used for individual processes may be pre-defined in the system, taking into account the following:\nA table with four fields plus a PROCESS ID and a summary field was created in an SAP system using a DB2 7.1.0 database. A test program generated 1,000 updates for each of five summary records. Each of the five summary records was thus changed 1,000 times, resulting in 5,000 updates per process.\nThe program was started simultaneously in two, three, and four parallel processes. In the first test sequence, the updates took place without different PROCESS IDs. In the second test sequence, they took place with different PROCESS IDs. The results are shown in Table A, below.\nDuration in Seconds\nDuration in Seconds\nSame PROCESS ID\nDifferent PROCESS ID\nThe measurement values indicate that processing times tend not to increase with additional number of processes when multiple PROCESS IDs are used. Processing times, however, do tend to increase with additional number of processes when the same PROCESS ID is used.\nIf these figures are extrapolated to an example with only four parallel processes in which 2,000,000 updates have to take place (for example, 400,000 documents, each with five posting items), 500,000 updates are allotted to each process, for which around 6,200 seconds are required if the PROCESS ID is the same and 2,200 seconds if the PROCESS IDs are different. The duration of the updates is thus reduced from around one hour and 43 minutes to 37 minutes.\nEffect with Read Accesses\nA differentiation between mass accesses and accesses to specific objects may be made (e.g., an object would be one specific cost center or one specific general ledger account). For example with a general ledger summary table, mass accesses (when all or a large proportion of entries are read) are required for preparing balance sheets. Accesses to specific objects are required, for example, if the overall status of an account needs to be displayed.\nTypically, mass accesses are only required periodically (e.g., during closing operations) and do not cause performance problems because the number of summary records is considerably lower than the number of individual records from which the summaries are created.\nMeasurements described above were made for accesses to specific objects: records from five PROCESS IDs were read instead of a single record. The measurement results did not show any significant differences in the time taken to access the database. Reading the five records was never over 20% slower than reading one record.\nThose skilled in the art can appreciate from the foregoing description that the broad techniques of the embodiments of the present invention can be implemented in a variety of forms. Therefore, while the embodiments of this invention have been described in connection with particular examples thereof, the true scope of the embodiments of the invention should not be so limited since other modifications will become apparent to the skilled practitioner upon a study of the drawings, specification, and following claims.\n|Cited Patent||Filing date||Publication date||Applicant||Title|\n|US5664129 *||Aug 2, 1995||Sep 2, 1997||Hitachi, Ltd.||Visual programming method|\n|US6457021 *||Aug 18, 1998||Sep 24, 2002||Microsoft Corporation||In-memory database system|\n|US6714948 *||Mar 16, 2000||Mar 30, 2004||Charles Schwab & Co., Inc.||Method and system for rapidly generating identifiers for records of a database|\n|US7047337 *||Apr 24, 2003||May 16, 2006||International Business Machines Corporation||Concurrent access of shared resources utilizing tracking of request reception and completion order|\n|US7093761 *||Dec 10, 2003||Aug 22, 2006||E2Interactive, Inc.||System and method for distributing stored-value cards|\n|US20040010502 *||Jul 12, 2002||Jan 15, 2004||Bomfim Joanes Depaula||In-memory database for high performance, parallel transaction processing|\n|US20050125406 *||Dec 3, 2003||Jun 9, 2005||Miroslav Cina||Database access with multilevel lock|\n|Citing Patent||Filing date||Publication date||Applicant||Title|\n|US8407183 *||Dec 21, 2007||Mar 26, 2013||Sap Ag||Business intelligence data extraction on demand|\n|US20090164486 *||Dec 21, 2007||Jun 25, 2009||Gabi Foeldesi||Business intelligence data extraction on demand|\n|US20100049715 *||Aug 20, 2008||Feb 25, 2010||Yahoo! Inc.||Controlled parallel propagation of view table updates in distributed database systems|\n|US20130151398 *||Dec 7, 2012||Jun 13, 2013||Dun & Bradstreet Business Information Solutions, Ltd.||Portfolio risk manager|\n|U.S. Classification||1/1, 707/E17.044, 707/E17.005, 707/E17.007, 715/763, 707/999.003, 707/999.2, 707/999.1|\n|International Classification||G06F7/00, G06F17/30|\n|Cooperative Classification||Y10S707/99933, G06F17/30286|\n|European Classification||G06F17/30S, G06F17/30C|\n|Sep 15, 2004||AS||Assignment|\nOwner name: SAP AKTIENGESELLSCHAFT, GERMANY\nFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:PLATE, KLAUS;REEL/FRAME:015787/0117\nEffective date: 20040811\n|Apr 4, 2012||FPAY||Fee payment|\nYear of fee payment: 4\n|Aug 26, 2014||AS||Assignment|\nOwner name: SAP SE, GERMANY\nFree format text: CHANGE OF NAME;ASSIGNOR:SAP AG;REEL/FRAME:033625/0334\nEffective date: 20140707\n|Mar 24, 2016||FPAY||Fee payment|\nYear of fee payment: 8","Parallel Data Processing\n|Node Allocation Limitations|\n|Sandboxes in Cluster|\n|Using a Sandbox Resource as a Component Data Source|\nThis type of scalability is currently only available for Graphs. Jobflow and Profiler jobs cannot run in parallel.\nWhen a transformation is processed in parallel, the whole graph (or its parts) runs in parallel on multiple Cluster nodes having each node process just a part of the data. The data may be split (partitioned) before the graph execution or by the graph itself on the fly. The resulting data may be stored in partitions or gathered and stored as one group of data.\nIdeally, the more nodes we have in a Cluster, the more data can be processed in a specified time. However, if there is a single data source which cannot be read by multiple readers in parallel, the speed of further data transformation is limited. In such cases, parallel data processing is not beneficial since the transformation would have to wait for input data.\nEach graph executed in a Clustered environment is automatically subjected to a transformation analysis. The object of this analysis is to find a graph allocation. The graph allocation is a set of instructions defining how the transformation should be executed:\nFirst of all, the analysis finds an allocation for individual components. The component allocation is a set of Cluster nodes where the component should be running. There are several ways how the component allocation can be specified (see the following section), but a component can be requested to run in multiple instances. In the next step an optimal graph decomposition is decided to ensure all component allocation will be satisfied, and the number of remote edges between graph instances is minimized.\nResulted analysis shows how many instances (workers) of the graph need to be executed, on which Cluster nodes they will be running and which components will be present in them. In other words, one executed graph can run in many instances, each instance can be processed on an arbitrary Cluster node and each contains only convenient components.\nFigure 42.1. Component allocations example\nThis figure shows a sample graph with components with various allocations.\nFlatFileWriter: node1, node2 and node3\nParallelPartition can change cardinality of allocation of two interconnected components (detailed description of Cluster partitioning and gathering follows this section).\nVisualization of the transformation analysis is shown in the following figure:\nFigure 42.2. Graph decomposition based on component allocations\nThree workers (graphs) will be executed, each on a different Cluster node. Worker on Cluster node1 contains FlatFileReader and first of three instances of the FlatFileWriter component. Both components are connected by remote edges with components which are running on node2. The worker running on node3 contains FlatFileWriter fed by data remotely transferred from ParallelPartitioner running on node2.\nAllocation of a single component can be derived in several ways (list is ordered according to priority):\nExplicit definition - all components have a common attribute Allocation:\nFigure 42.3. Component allocation dialog\nThree different approaches are available for explicit allocation definition:\nAllocation based on the number of workers - the component will be executed in requested instances on some Cluster nodes which are preferred by CloverDX Cluster. Server can use a build-in load balancing algorithm to ensure the fastest data processing.\nAllocation based on reference on a partitioned sandbox - component allocation corresponds with locations of given partitioned sandbox. Each partitioned sandbox has a list of locations, each bound to a specific Cluster node. Thus allocation would be equivalent to the list of locations. For more information, see Partitioned sandbox in Sandboxes in Cluster.\nAllocation defined by a list of Cluster node identifiers (a single Cluster node can be used more times)\nReference to a partitioned sandbox FlatFileReader, FlatFileWriter and ParallelReader components derive their allocation from the\nfileURLattribute. In case the URL refers to a file in a partitioned sandbox, the component allocation is automatically derived from locations of the partitioned sandbox. So in case you manipulate with one of these components with a file in partitioned sandbox, a suitable allocation is used automatically.\nAdoption from neighbor components By default, allocation is inherited from neighbor components. Components on the left side have a higher priority. Cluster partitioners and Cluster gathers are nature bounds for recursive allocation inheritance.\nAs mentioned before, data may be partitioned and gathered in several ways:\nPartitioning/gathering \"on the fly\"\nThere are six special components to consider: ParallelPartition, ParallelLoadBalancingPartition, ParallelSimpleCopy, ParallelSimpleGather, ParallelMerge, ParallelRepartition. They work similarly to their non-Cluster variations, but their splitting or gathering nature is used to change data flow allocation, so they may be used to change distribution of the data among workers.\nParallelPartition and ParallelLoadBalancingPartition work similar to a common partitioner - they change the data allocation from 1 to N. The component preceding the ParallelPartitioner runs on just one node, whereas the component behind the ParallelPartitioner runs in parallel according to node allocation.\nParallelSimpleCopy can be used in similar locations. This component does not distribute the data records, but copies them to all output workers.\nParallelSimpleGather and ParallelMerge work in the opposite way. They change the data allocation from N to 1. The component preceding the gather/merge runs in parallel while the component behind the gather runs on just one node.\nPartitioning/gathering data by external tools\nPartitioning data on the fly may in some cases be an unnecessary bottleneck. Splitting data using low-level tools can be much better for scalability. The optimal case being that each running worker reads data from an independent data source, so there is no need for a ParallelPartitioner component, and the graph runs in parallel from the beginning.\nOr the whole graph may run in parallel, however the results would be partitioned.\nNode Allocation Limitations\nAs described above, each component may have its own node allocation specified, which may result in conflicts.\nNode allocation of neighboring components must have the same cardinality.\nIt does not have to be the same allocation, but the cardinality must be the same. For example, there is a graph with 2 components: DataGenerator and Trash.\nDataGenerator allocated on NodeA sending data to Trash allocated on NodeB works.\nDataGenerator allocated on NodeA sending data to Trash allocated on NodeA and NodeB fails.\nNode allocation behind the ParallelGather and ParallelMerge must have cardinality 1.\nIt may be of any allocation, but the cardinality must be just 1.\nNode allocation of components in front of the ParallelPartition, ParallelLoadBalancingPartition and ParallelSimpleCopy must have cardinality 1.\nSandboxes in Cluster\nThere are three sandbox types in total - shared sandboxes, and partitioned and local sandboxes (introduced in 3.0) which are vital for parallel data processing.\nThis sandbox type is intended for data, which is accessible only by certain Cluster nodes. It may include massive input/output files. The purpose being, that any Cluster node may access content of this type of sandbox, but only one has local (fast) access, and this node must be up and running to provide data. The graph may use resources from multiple sandboxes which are physically stored on different nodes, since Cluster nodes are able to create network streams transparently as if the resources were a local file. For details, see Using a Sandbox Resource as a Component Data Source.\nDo not use a local sandbox for common project data (graphs, metadata, connections, lookups, properties files, etc.). It would cause odd behavior. Use shared sandboxes instead.\nFigure 42.5. Dialog form for creating a new local sandbox\nThe sandbox location path is pre-filled with the\nwhich, by default, points to\nThe placeholder can be configured as any other CloverDX configuration property.\nThis type of sandbox is an abstract wrapper for physical locations existing typically on different Cluster nodes. However, there may be multiple locations on the same node. A partitioned sandbox has two purposes related to parallel data processing:\nnode allocation specification\nLocations of a partitioned sandbox define the workers which will run the graph or its parts. Each physical location causes a single worker to run without the need to store any data on its location. In other words, it tells the CloverDX Server: to execute this part of the graph in parallel on these nodes.\nstorage for part of the data\nDuring parallel data processing, each physical location contains only part of the data. Typically, input data is split in more input files, so each file is put into a different location and each worker processes its own file.\nFigure 42.6. Dialog form for creating a new partitioned sandbox\nAs you can see on the screenshot above, for a partitioned sandbox, you can specify one or more physical locations on different Cluster nodes.\nThe sandbox location path is pre-filled with the\nwhich, by default, points to\nsandboxes.home.partitioned config property may be configured\nas any other CloverDX Server configuration property.\nNote that the directory must be readable/writable for the user running JVM process.\nDo not use a partitioned sandbox for common project data (graphs, metadata, connections, lookups, properties files, etc.). It would cause odd behavior. Use shared sandboxes instead.\nUsing a Sandbox Resource as a Component Data Source\nA sandbox resource, whether it is a shared, local or partitioned sandbox (or ordinary sandbox on standalone server), is specified in the graph under the fileURL attributes as a so called sandbox URL like this:\ndata is a code for the sandbox and\nis the path to the resource from the sandbox root.\nThe URL is evaluated by CloverDX Server during job execution,\nand a component (reader or writer) obtains the opened stream from the Server.\nThis may be a stream to a local file or to some other remote resource.\nSo a job does not have to run on the node which has local access to the resource.\nThere may be more sandbox resources used in the job, and each of them may be on a different node.\nThe sandbox URL has a specific use for parallel data processing. When the sandbox URL with the resource in a partitioned sandbox is used, that part of the graph/phase runs in parallel, according to the node allocation specified by the list of partitioned sandbox locations. Thus, each worker has it is own local sandbox resource. CloverDX Server evaluates the sandbox URL on each worker and provides an open stream to a local resource to the component.\nThe sandbox URL may be used on the standalone Server as well. It is suitable for graphs referencing resources from different sandboxes. It may be metadata, lookup definition or input/output data. A referenced sandbox must be accessible for the user who executes the graph."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:8ed9467b-5192-4fda-9d40-6ff8a7f5d81c>","<urn:uuid:7a297dfa-a3aa-4b07-b3ba-34e5b9fb5d42>"],"error":null}
{"question":"How has Gilbert Hill in Mumbai evolved over time, and what challenges does this ancient geological formation face in its urban setting?","answer":"Gilbert Hill is a 197-foot tall monolithic black basalt formation located in Mumbai, Maharashtra, featuring sheer vertical faces and precise vertical rock columns. This 65-million-year-old formation is often compared to a smaller version of Devils Tower. As Mumbai has expanded, the hill has become surrounded by urban development, which has led to significant challenges. Years of quarrying around its base have degraded its natural appearance and created increasing safety hazards as the basalt columns lose their ground-level support. While the hill's summit still offers remarkable views over Mumbai's densely packed roofs, its structural integrity has been compromised by human activity.","context":["Columnar basalt formations like Devils Tower and the Giant’s Causeway have amazed and mystified humans from time immemorial. We now know that these spectacular geological wonders formed when extruded molten lava cooled, crystallized and cracked along precise angles. When exposed en masse, these magnificent symmetrical pillars look anything but natural… but they are: SUPER natural!\nDevils Tower, Wyoming, USA\nDevils Tower, located in northeastern Wyoming, was designated America’s first national monument by President Theodore Roosevelt in 1906. Geologists are not completely sure how the 1,267 ft (386 m) tall tower came to be, with most theories centering on it being the core, or plug, of an ancient volcano whose outer layers have eroded away. Though the columnar basalt on the formation’s exterior is eroding, so is the softer sandstone surrounding its base.\nDevils Tower was known of and venerated by several Native American tribes. One legend concerning the formation involves 7 young girls pursued by a bear. When the ground rose, carrying them upward and out of the creature’s reach, the bear frantically scratched and clawed at the rock until it died of exhaustion. Devils Tower burst into modern pop culture consciousness with the release of the 1977 film Close Encounters of the Third Kind. The Tower first appears as a kitchen construction made of mashed potatoes and then, later on, as the contact point between humans and a more-advanced alien race.\nHexagon Pools, Golan Heights, Israel/Syria\nThe Hexagon Pools and their related watercourses are fed by cold, clear water draining off the nearby hills of the Golan Heights. Surprisingly to many who expect the region to have a classic Middle eastern desert environment, the area is well-watered and the rocks are mainly of volcanic origin.\nSheets of columnar basalt tinted a burnished gray hue hang alongside the Hexagon Pools – it’s the polygonal sections of rock that give the pools their name. The rock formations have been acted on by shifting subterranean fault lines over a very long time, which accounts for their unusual twisted appearance.\nCyclopean Isles, Italy\nLocated off the southeastern coast of Sicily in the Mediterranean Sea, the Cyclopean Isles are a small group of volcanic islands associated with nearby Mount Etna. The islands are mainly made up of black basalt that, under the influence of water and weathering, has evolved into a wide variety of otherworldly formations including vertical columns and horizontal polygonal mosaics.\nThe Cyclopean Isles feature prominently in The Odyssey, the ancient Greek poet Homer’s tale of the warrior king Odysseus (Ulysses) and his lyric journey home from the city of Troy. Odysseus and his crew were captured and imprisoned by a monstrous, one-eyed Cyclops when they landed on one of the islands, only escaping when Odysseus blinded the Cyclops while he slept. In his rage, the pain-crazed Cyclops wildly threw huge boulders in the direction of Odysseus’s sailing ship.\nJusangjeolli, Jeju Island, South Korea\nJusangjeolli is a huge formation of columnar basalt extending along a 3.5km () stretch of the Jungmun and Daepo seashore in Seogwipo, Jeju Island.” In some places, sheer cliffs made up of vertical basalt columns rise up to 20m (60 ft) above the beach. Often compared to the Giant’s Causeway in Northern Ireland, Jusangjeolli also juts out into the ocean and owes its unique character to age-old forces acting at the interface between sea and land.\nExposure to the elements over untold millions of years has left its mark on the once sharply delineated columns. In some areas the columns have partially separated into individual spires; closer to the shore crashing waves have softened and rounded their contours to the point where they resemble man-made walls similar to those constructed by the Incas.\nFingal’s Cave, Staffa, Scotland\nFingal’s Cave is an enormous sea grotto located on the rugged coast of Staffa, Scotland. The island is uninhabited and is graced with a host of natural geologic wonders formed from the same eruption of black basalt that composes the Giant’s Causeway. Show Caves of the World notes the dimensions of Fingal’s Cave as being 85 m (279 ft) deep and 23 m (75 ft) high.\nDescribed by visitors as a “truly natural cathedral complete with basaltic organ pipes”, Fingal’s Cave is endowed with unusual acoustic properties that distort and magnify the sounds of crashing waves. Scottish novelist Sir Walter Scott visited Fingal’s Cave and described it as “one of the most extraordinary places I ever beheld. It exceeded, in my mind, every description I had heard of it… eternally swept by a deep and swelling sea, and paved, as it were, with ruddy marble, it baffles all description.”\nGarni Gorge, Armenia\nGarni Gorge is located 23 km (14.3 miles) east of Yerevan, the capitol of the country of Armenia. Due to the fact that it’s an inland canyon and not a seaside cliff formation, Garni Gorge offers a rare opportunity to view vast expanses of well-preserved columnar basalt on both sides of the onlooker. A notable landmark dating far back into prehistory, visitors to Garni Gorge can visit a restored 1st century AD Hellenistic temple situated on a promontory overlooking the canyon’s depths.\nGarni Gorge’s most famous feature is the “Symphony of the Stones”, a frozen cascade of basaltic “organ pipes” likened to a hanging garden due to erosion and undercutting of the valley floor. It’s a brave tourist who attempts to take photos or film video from beneath thousands of tons of suspended stone!\nGilbert Hill, Mumbai, India\nGilbert Hill is a 197 ft (60 m) tall monolithic black basalt extrusion located in the outskirts of Mumbai in India’s state of Maharashtra. With its sheer vertical face and precisely etched vertical rock columns, the 65 million year old formation is said by some to resemble a much smaller version of Devils Tower.\nUnlike most other famous large formations of columnar basalt, Gilbert Hill finds itself located in an urban setting as the city of Mumbai (formerly Bombay) has expanded around it. Though its summit provides a remarkable view over the densely packed roofs of Mumbai, years of quarrying around the formation’s base has both degraded the Gilbert Hill’s original, natural appearance and has created an increasingly dangerous hazard as the basalt columns lose their ground-level support.\nDevils Postpile, California, USA\nDevils Postpile is located near Mammoth Mountain in eastern California near the border with Nevada. Along with 101-ft high Rainbow Falls, this unusually symmetrical formation of columnar basalt is included in Devils Postpile National Monument, established in 1911 by a presidential proclamation. The order to create the park was in response to a dam-building proposal that would have seen Devils Postpile blasted into oblivion. The basalt formation offers visitors easy access to both the face and the top – the latter looking much like a man-made parquet floor.\nThe lava flow that created Devils Postpile is relatively young, geologically speaking, being between 100,000 and 700,000 years old. Surface topography at the time of the eruption prevented the lava from spreading out and as a consequence, the original 400-ft (122 m) thick layer of basalt cooled slowly and evenly. It’s thought that this slow cooling allowed the basalt to form very long and uniform columns, most of which are six-sided in cross section.\nSvartifoss, Skaftafell National Park, Iceland\nSvartifoss (“Black Falls”, in Icelandic) is located in Iceland’s Skaftafell National Park. This rare and striking example of a columnar basalt formation combined with a 12 meter (39 ft) high waterfall can be appreciated in all 4 seasons. The undercut structure of the columns accentuates their visual similarity to traditional church organ pipes.\nThe basalt columns that make up the escarpment over which the falls flows are virtually unweathered and display straight, sharp edges that advertise their crystalline structure. This is due to the more rapid erosion caused by the constant, fast-flowing falls combined with Iceland’s perpetual freeze-thaw cycle.\nGiant’s Causeway, County Antrim, Northern Ireland\nThe Giant’s Causeway is a spectacular assemblage of around 40,000 black basalt columns, weathered and eroded to varying degrees by the harsh seaside environment of County Antrim in Northern Ireland. The columns are up to 60 million years old, and the passage of time has acted to form a series of terraces leading down – and into – the frigid North Channel of the Irish Sea.\nThe Giant’s Causeway has another claim to fame, one which has contributed to the site being declared a World Heritage Site by UNESCO (1986) and a National Nature Reserve (1987) by the Department of the Environment for Northern Ireland. It seems that the Girona, a straggler ship of the Spanish Armada was shipwrecked just offshore of the Giant’s Causeway in late October of 1588. The ship was carrying over 1,000 sailors from other sunk or shipwrecked Spanish ships, along with their valuables, in addition to her own crew of about 300 – it’s estimated less than 10 survived.\nIt’s somewhat surprising that the Giant’s Causeway was little known in learned geological circles until the last years of the 17th century. Part of this ignorance has to do with the formation’s isolated location, and anecdotal accounts of the features size and grandeur were deemed too grandiose to be true. Thanks to the wonders of modern photographic technology, the world can see that reports of the Giant’s Causeway were anything BUT an exaggeration!\nThe otherworldly beauty of columnar basalt formations like the Giant’s Causeway lends itself to most any artistic endeavor, including music. Those familiar with Led Zeppelin’s fifth album Houses of the Holy, released in March of 1973, may have wondered where on Earth the bizarre landscape on the album cover was located. Well, now you know… Rock On!!"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:db0353e1-9b06-42dd-84a3-967322c62ad5>"],"error":null}
{"question":"How should farm trial data be evaluated for yield assessment, and what factors beyond yield should be considered in variety selection?","answer":"Farm trial data evaluation requires combining scouting notes on pest/disease pressure, photos, field history, soil test results, and fertility applications, along with weather and satellite imagery data for a complete picture. It's important to note that trials aren't solely about yield - some practices may increase profit by decreasing input costs rather than increasing yield. For variety selection, after identifying varieties with high yield potential, secondary characteristics must be considered including test weight, disease resistance, lodging, height, maturity, and end-use quality goals. The decision is also complicated by factors like cropping system needs and the importance of spreading out flowering and maturity dates to reduce risk.","context":["Evaluating Your Farm Trial Results\nHow to measure the information you’ve collected from your on-farm trial and what to do with it.\nIn our series on running your own farm trials, we're looking now at what to do with the information you gather during your trial. For the most part, the clearest conclusion to the question you started with at the beginning of your farm trial will come at harvest, so it is important to make sure you do your part to get the highest quality data.\nMake sure your combine is clean, properly calibrated and in its best working order. Don’t let your trial be the place you decide to change all your settings.\nCheck your yield monitors before harvest.\nYour yield monitors actually have mechanical components that can become worn or broken. During your pre-harvest check on your combine, check the four main components of the yield monitor:\nYield Sensor: Check for wear and/or alignment.\nMoisture Sensor: Check for old crop residue in or on the sensor as well as the auger system for testing moisture which can blow small fuses.\nDisplay: Verify Grower, Farm and Field structure is set up, and Variety Tracking information is set up and turned on.\nReceiver: Check for solid GPS connections once outside with a clear view of the sky.\nBe conscious of your harvest speed.\nIf you want your data to give you an accurate portrayal of how your new processes are performing, you should give them every opportunity to show what they can do. And hurrying through a plot won’t give you that picture.\nKeep an eye on the endrows.\nThe endrows of your test plots can skew your measurements due to outside factors, giving you a false indicator. If any part of your trial could overlap a buffer row, make sure it affects both treatments equally.\nMeasure your harvest area before you harvest each plot.\nIf you have a well-calibrated yield monitor, your machine should take care of this for you. As an alternative, you can calculate yield by measuring weight with a weigh wagon or truck scales, and then area harvested with the formula below.\nCheck your work.\nYour yield monitor will provide you with moisture readings, but if you don’t have a monitor, take samples to your local elevator and have it tested there. When FBN presents data, we work with moisture-adjusted yields.\nRemember: You should always calibrate your yield monitors for accurate benchmarking and better insights on your own operation.\nReview and Evaluate\nIt’s great to have all the info you gathered from your farm trial during the season. But now you have to know what to do with it! Take yourself back to the start of the process and revisit what question you were trying to answer, and what steps you were taking to get your answer.\nThen combine all your scouting notes on pest or disease pressure, photos, any unique field history or features, soil test results and fertility applications in FBN, where you can see a full picture of your observations combined with weather, satellite imagery and yield.\nAll of these pieces will help ensure that you have a detailed look at your fields, as well as your overall operational competitiveness and performance.\nRemember, Trials Aren't Entirely About Yield\nNot all questions you may ask in the planning stage of your on-farm trials should be expected to result in a single answer around higher yields. Not every practice you trial may increase yield; some practices may increase your profit by decreasing input costs.\nSources: https://blog-crop-news.extension.umn.edu/2017/05/on-farm-research-trial-demonstrates.html https://cropwatch.unl.edu/on-farm-research https://cropwatch.unl.edu/2017/field-studies-setting-trial https://extension.umn.edu/crop-production/how-do-research-your-farm http://www.omafra.gov.on.ca/english/crops/field/news/croptalk/2015/ct-0315a5.htm http://corn.agronomy.wisc.edu/Management/L016.aspx http://extension.uga.edu/publications/detail.html?number=B1177&title=Designing%20Research%20and%20Demonstration%20Tests%20for%20Farmers%27%20Fields#Selecting https://www.canr.msu.edu/news/on_farm_crop_tests_can_be_powerful_tools_for_individual_farmers https://www.iasoybeans.com/upl/downloads/library/guide-to-replicated-strip-trials.pdf","Selecting an appropriate, high-yielding variety is one of the most important management decisions that producers make. Yield potential is clearly important, but the decision is complicated by such factors as the cropping system, the need for disease resistance, end-use quality goals, year-to-year climatic variation, and the need to select multiple varieties in order to reduce risk by spreading out flowering and maturity dates.\nChoosing varieties based on these many variables dictates that a significant amount of information be available about each and every one. This information is available in summaries of variety performance trials. Proper use of variety test performance data is the first step in making these important variety decisions. But in order to make proper use of the data, an understanding of the types of variety trials that are most commonly conducted and the pros and cons of each is needed.\nTypes of Variety Tests\n“This variety yielded better on my farm than this other one.” or “This variety performed well for me last year.”\nThere are many, many factors that could cause either of these statements to be true: year-to-year weather variation, yield potential differences in and between fields, planting date differences, etc.\nYearly or farm-specific evidence involves a single observation with no direct comparison to other varieties. This information is not without value, however. Most scientifically sound variety trials are designed and sited to avoid major same-soil variability along with pest or previous crop management problems that can and usually will be encountered on an individual’s farm. So, these types of observations about variety performance should be used in conjunction with other variety tests when making a final decision.\nSplit-field comparisons are planned so that two (or sometimes more) varieties are planted side by side in the same or adjacent fields. These tests allow for a “real world” assessment using farm equipment and large areas that should represent the range of soils and crop management scenarios that would normally be seen. The variability in this type of test can also lead to either inaccurate comparisons or difficulty in making comparisons.\nFor example, in the fields shown in figure 1A, it is apparent that there are soybean growth differences between the two sides of this field, which are separated by a line. Those differences could be due to soils or they might be due to historical management practices. There’s a farmstead within the field planted to variety A. If, for example, this is an animal operation, guess which field might have received the bulk of the manure.\nIn figure 1B, where varieties A and B are planted side by side in the same field, there is similar variation in crop growth. If the entire area for each variety is harvested and compared on the basis of yield, is the difference due to variety or to field variation? The danger of this type of test is that the comparison may favor field variability rather than variety differences.\nIt is true that a yield monitor that logs in yield data as the crop is harvested can give some good information about how a variety changes with soil type, but there is still only one side-by-side comparison of varieties.\nOn-Farm Tests With Multiple Varieties\nUnreplicated Strip Plots\nAn example of unreplicated strip-plot trials is shown in figures 2A-B. Varieties are planted in an adjacent pattern, usually the length of the field. The width of each strip for a variety may be a whole planter-/drill-width or a portion of the total. The term “unreplicated” indicates that each variety is planted once at each site. Studies that are “replicated” have each variety planted multiple (generally three or more) times in each field. Unreplicated trials can be a good source of information because they combine the use of farmer equipment and relatively large plot size with the testing of a number of varieties in the same field. The major problem, again, is field variability. It is apparent from figure 2 that there are more problem areas with reduced soybean growth in the part of the field where varieties 1-15 are planted.\nOne way to avoid this complication is to use one variety as a repeating check (the “C” in figure 2). With these checks, we can partially measure the variability as we move across the field. This “check” plot design allows for an adjustment of the yields based on the proximity of a variety to the nearest check plot.\nAnother partial solution to the inequities with an unreplicated strip-plots design is to replicate the same experiment over multiple farms. If all varieties are included at each location, then the locations can serve as replications and the data can be analyzed for statistical differences.\nIn Virginia, our Extension agents work together annually to establish a number of on-farm variety strip plots using this methodology. Many companies conduct similar trials that include primarily their own varieties, with a few representative varieties from their competitors. There’s nothing wrong with this type of test as long as the competing varieties are those that are known to yield well and to be widely used.\nStrip-plot trial data can serve to validate other on-farm tests and replicated small-plot experiments. But there is still the problem of spatial variability that could — as in the example used — misrepresent the varieties, so careful consideration of the results is important. In Virginia on-farm publications, Extension agents often provide useful insight and observations about the trials.\nHead-to-head comparisons are often conducted by splitting the planter or drill with one variety in each half (figure 3). Because the two varieties always occur in the same pattern, the randomization scheme does not necessarily meet scientific requirements. This is not a major issue, especially if at least four or more replications are used. “Randomization” means that each variety is placed in the field at random and that each has an equal probability of occurring next to another. When we split the planter, we know, for example, that variety 1 will always be to the left of variety 2.\nFinally, the use of a yield-monitor-equipped combine in fields such as this one can provide a great deal of beneficial information. There are actually three distinct soil types in the field in figure 3, where yield typically varies by as much as two-fold. Therefore, yield results from this test would tell the producer not only how the two varieties compare on average, but also how they compare in soils that range from low to high productivity.\nOfficial Variety Tests\nAgronomists from the state land-grant universities usually conduct official variety tests. The tests usually feature a large number of varieties planted in small plots multiple times (replicated) at each site.\nThis type of study employs a valid statistical design that likely makes it the only legitimate method for testing a large number of varieties in the same environment. Field sites are selected so that the soil has minimum variability. Experimental protocol ensures that soil fertility is adequate, pests are controlled, and the cultural practices used are optimum for maximizing yield potential. The goal is to minimize any potential nonsoil- and nonweather-related stresses in order to meet the primary objective of comparing yield potential among the varieties.\nThere are, however, also potential problems with official variety test trials. The first is associated with the small plot size necessary to stay within a uniform soil area. Because the harvest area is small, any error is magnified. For example, harvesting one extra foot from a plot that was supposed to be 17-feet long can result in a 6 percent error unless an adjustment for plot size is made for this particular plot when the data is reviewed and analyzed.\nAnother potential issue arises when not all varieties are tested at all locations. This can occur because of errors or purposefully when varieties are entered only at some locations. For example, if a certain variety is left out of the highest-yielding location, the average yield of that variety over all of the locations would be lower. Yet, that particular variety could be among the top yielders in the other locations.\nA partial solution can be to use relative yields. A “relative yield” is that variety’s yield at one location, divided by the average yield of all varieties at that location. This value is then usually multiplied by 100 to give a percentage. Using this method, a variety that yields the test average will have a relative yield of 100 and a variety that yields 10 percent above the average is given a relative yield value of 110.\nInterpreting Data From Variety Trials\nThere are two important points to understand when using the data from any variety trial:\n- The highest-yielding variety from a single location will rarely be the highest-yielding variety the following year at that location.\n- The highest-yielding variety selected from multiple location averages will usually be a variety that will yield among the best the following year at any location.\nFor example, table 1 shows relative yields of soybean varieties over three locations and three years of testing. The first two columns identify the location and the number of years used to choose the highest-yielding variety, respectively. The third column is the relative yield of this highest-yielding variety at a particular location based on one, two, or three years of data. The variety chosen with only one year of data was not always (and not likely) the highest-yielding variety chosen when using two or three years of data. The next columns (4-7) show relative yield of that variety at each of the three testing locations and the average over locations in the following year.\nLet’s look at an example. If we chose a variety with only one year of data at the Suffolk location (relative yield = 114), the next year that variety had a relative yield in the Suffolk test of 100, meaning its performance was as good as the average for all entries in the test. Even with two or three years of data for this variety at Suffolk, its performance the following year at Suffolk was not good. However, using the multi-location average data resulted in much greater predictability of performance the following year. In fact, using the multilocation data to choose a variety always gave at least 100 percent of the test average the following year. In fact, when all locations and years were considered, this variety had an average relative yield of 105 percent.\nVarieties that have performed well under diverse conditions, as evidenced over locations and years, are likely to perform well again. Depending on a grower’s location, it should be kept in mind that additional performance data about the same variety may be available from other state or private variety testing programs.\nGrowers and advisers should utilize multilocation and multiyear performance data to develop a list of the best-suited varieties. Then, once several varieties are selected, performance of those varieties in the closest regional test or from on-farm testing should be considered.\nAfter identifying a group of varieties with high grain-yield potential and stable performance across sites and years, the variety selection process should continue with consideration given to secondary characteristics such as test weight, disease resistance, lodging, height, maturity, and end-use quality.\n|Table 1. Example of Virginia soybean variety relative yield performance (as a percentage)|\nover multiple sites and years.\n|Relative yield for|\nin data set\n|Relative yield for highest-yielding|\nvariety the following year at each location\nVirginia Cooperative Extension materials are available for public use, reprint, or citation without further permission, provided the use includes credit to the author and to Virginia Cooperative Extension, Virginia Tech, and Virginia State University.\nIssued in furtherance of Cooperative Extension work, Virginia Polytechnic Institute and State University, Virginia State University, and the U.S. Department of Agriculture cooperating. Edwin J. Jones, Director, Virginia Cooperative Extension, Virginia Tech, Blacksburg; M. Ray McKinnie, Administrator, 1890 Extension Program, Virginia State University, Petersburg.\nJuly 29, 2011"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:c1494e2c-3880-4fc2-b309-8b209d192982>","<urn:uuid:00f3d952-639e-4708-b750-f72efb7e1fe6>"],"error":null}
{"question":"How can I effectively manage my browser's cookie settings while ensuring essential website functionality isn't compromised? I've noticed some sites don't work properly when I block all cookies.","answer":"You can control and manage cookies through your browser settings, but removing or disabling cookies may impact your user experience and some site functionality. To manage cookies, you can visit http://www.aboutcookies.org/ which provides comprehensive information for various browsers, including details on how to delete cookies from your computer. While you can usually change your browser to not accept cookies or to indicate when a cookie is being sent, if you don't accept cookies, you may not be able to use some portions of services.","context":["What is a cookie?\nA cookie is a small piece of data that a website asks your browser to store on your computer or mobile device. The cookie allows the website to “remember” your actions and/or preferences over time. Most Internet browsers support cookies: however, users can set their browsers to decline certain types of cookies or specific cookies. Furthermore, users can delete cookies at any time.\nDo we share any data with third parties?\nWe use third-party website infrastructure suppliers to help us build and manage our website. Cookie data is stored securely on these suppliers’ servers. Our service providers have been carefully selected by us, are contractually bound by our instructions, and will not process your data for any other purposes than intended.\nWe may use them as a way of measuring our service and to work with marketing partners to improve our service. You can usually change your browser so that it will not accept cookies or to indicate when a cookie is being sent. However, if you do not accept cookies, you may not be able to use some portions of our Service. To assist you, we have provided an explanation of the types of cookies we use below.\nThey have been categorized by its purpose:\nWhat are session cookies?\nThese are cookies that last for a session. A session starts when you launch a Site for example and ends when you leave the website or close your browser window. Session cookies contain information that is stored in a temporary memory location which is deleted after the session ends. Unlike other cookies, session cookies are never stored on your device. Therefore, they are also known as transient cookies, non-persistent cookies, or temporary cookies.\nWhat are preference cookies?\nWe use Preference Cookies to remember your preferences and various settings where the technical storage or access is necessary for the legitimate purpose of storing preferences.\nWhat are security cookies?\nSecurity Cookies are a type of HTTP cookie that have secure attribute set, which limits the scope of the cookie to “secure” channels. When a cookie has the secure attribute, the user agent will include the cookie in an HTTP request only if the request is transmitted over a secure channel (typically HTTPS).\nWhat are advertising cookies?\nAdvertising Cookies are specifically designed to gather information from you on your device to display advertisements to you based on topics that interest you.\nWe work with approved partners to deliver relevant content to you and measure the effectiveness of these advertisements.\nHow are third party cookies used?\nThird-party cookies are cookies that are stored under a different domain than what you are currently visiting. They are mostly used to track users between websites and display more relevant ads between websites. Another good example is a support chat functionality provided by a third party service. We use third party cookies to give you an enhanced experience and to power third party functionality such as chat windows.\nHow can I control cookies?\nYou can control and manage cookies in various ways. Please keep in mind, removing or disabling cookies may impact your user experience and parts of the Site may no longer be fully functional.\nYou may change your browser settings to manage and control cookies. You can visit http://www.aboutcookies.org/ which contains comprehensive information on how to do this on a wide variety of browsers. This site includes details on how to delete cookies from your computer as well as more general information about cookies.\nChanges to the Cookies Policy"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:1b83c78e-0351-4f07-ae60-6017c241485e>"],"error":null}
{"question":"When monitoring weather phenomena, what are the key differences between how traditional lightning data was collected from space versus the capabilities of NOAA's new GOES-16 satellite's GLM instrument compared to the IPHC's approach to data collection?","answer":"The GOES-16 satellite's Geostationary Lightning Mapper (GLM) represents a dramatic improvement over previous space-based lightning monitoring, collecting more data in its first week than all previous space-based lightning data combined. It can continuously monitor lightning storms from a fixed geostationary position 22,300 miles above Earth, capturing data at 500 frames per second across the Western Hemisphere. In contrast, the IPHC conducts annual standardized fishery-independent longline surveys and samples commercial fishery catch at major ports for their data collection. While both approaches aim to improve predictions and understanding of their respective phenomena, the GLM provides continuous real-time monitoring while the IPHC relies on periodic physical sampling and surveys.","context":["By Ian Stewart, Quantitative Scientist, International Pacific Halibut Commission\nEdited by Patrick Cooney and Steve Midway, The Fisheries Blog\nWhen you think of weather and climate, it is probably about how it will impact your clothes choice for the day, your plans for the weekend, or the potential need for an umbrella. A question to ask yourself: Is the weather forecast always perfect, or is there uncertainty in the forecaster’s prediction?\nJust like you, fisheries scientists also think about weather and climate, but rather than clothes choices, they are usually considering the potential response of fish and ecosystems to global climate change or periodic weather fluctuations, like El Niño and the Pacific Decadal Oscillation. Just as weather forecasts have a level of uncertainty, so too do fisheries forecasts. To better understand the uncertainty of fish population responses to weather and climate fluctuations, fisheries scientists are considering methods used by climate researchers that study and incorporate uncertainty into their forecasts.\nFor instance, hurricane forecasters frequently rely on ensemble modelling and base their predictions on a collection of plausible models or model conditions rather than basing their predictions on a single model. The International Pacific Halibut Commission (IPHC) now uses this collection of models approach with its annual stock assessment when providing scientific advice for U.S. and Canadian halibut catch limits.\nThe IPHC has been a historical leader in developing and testing new scientific methods for stock assessment and fisheries management, from William Thompson’s early work on fishery yield in the 1930s, through migratory catch-at-age models and an extensive tagging program in the early 2000s. This has been in large part out of necessity – Pacific halibut have a complex life history and a complex fishery that create formidable modelling challenges.\nThe complexities of Pacific halibut life history include high rates of geographic dispersal, seasonal spawning migrations, extensive variability in size-at-age among individuals, sexes, cohorts, and time periods, and periodic recruitment success correlated with environmental conditions. The complex fishery involves commercial, subsistence users, a significant recreational sector, as well as non-target fisheries that capture halibut in the process of harvesting other species.\nTo address the complexity of the fish and fishery, the IPHC conducts an annual standardized fishery-independent longline survey, and samples the commercial fishery catch in most major ports of delivery. These data provide clear information on stock and biological trends, but they still do not tell exactly how many halibut are swimming in the Pacific Ocean. To estimate population sizes, stock assessments are needed, which means fitting models to the available data. The IPHC has done this each year since the 1970s, and after thirty years of hard labor, there is still no perfect Pacific halibut stock assessment model. Each year refinements have been made to address shortcomings of the previous analyses. And each year new issues appear, creating instability in stock size estimates, and ultimately in the stakeholders’ confidence that the science is reliable. This is where fisheries scientists have something to learn from hurricane forecasters.\nThe public easily appreciates the risks associated with where and when a hurricane will strike land. A simple prediction is not sufficient for cities to plan response and evacuations; this requires an assessment of both likely outcomes as well as a clear expression of the uncertainty around those predictions. Hurricane forecasters have therefore employed ensemble modelling to include uncertainty in model structure, parameter values, initial conditions and other factors not easily captured by a single “best” prediction model.\nTo break the paradigm of endlessly searching for the single best fishery stock assessment model, we at the IPHC needed only to admit that it doesn’t exist. Annual halibut assessments now include a growing ensemble of models – different hypotheses about how the world works, each plausible given the available data, but potentially yielding different estimates of stock size. These alternate models are integrated in a risk assessment decision table containing a collection of metrics representing conservation and fishery risks associated with different future catch levels.\nBut what happens when you admit you don’t know exactly how many fish are out there? The response to the IPHC’s ensemble approach has been very positive: scientists understand that it removes some of the subjectivity from the annual stock assessment, and stakeholders recognize that the uncertainty is now better represented – they have known all along that each year’s single “best” assessment model wasn’t perfect! After 91 years, the IPHC is still finding new tools to address old questions, and our stakeholders have more realistic information about the stock, and our uncertainty in it, than ever before.\nTo learn more about the International Pacific Halibut Commission, please visit http://www.iphc.int, http://www.facebook.com/InternationalPacificHalibutCommission, and http://www.twitter.com/IPHCinfo.\nTo contact the author email email@example.com.\nClark, W. G., and S. R. Hare. 2002. Effects of Climate and Stock Size on Recruitment and Growth of Pacific Halibut. North American Journal of Fisheries Management 22:852-862.\nDeriso, R. B., T. J. Quinn, II, and P. R. Neal. 1985. Catch-age analysis with auxiliary information. Canadian Journal of Fisheries and Aquatic Sciences 42:815-824.\nHamill, T. M., M. J. Brennan, B. Brown, M. DeMaria, E. N. Rappaport, and Z. Toth. 2012. NOAA’s Future Ensemble-Based Hurricane Forecast Products. Bulletin of the American Meteorological Society 93:209-220.\nPatterson, K., R. Cook, C. Darby, S. Gavaris, L. Kell, P. Lewy, B. Mesnil, A. E. Punt, V. Restrepo, D. W. Skagen, and G. Stefansson. 2001. Estimating uncertainty in fish stock assessment and forecasting. Fish and Fisheries 2:125-157.\nStewart, I. J., and S. J. D. Martell. 2013. A historical review of selectivity approaches and retrospective patterns in the Pacific halibut stock assessment. Fisheries Research.\nStewart, I. J., and S. Martell. 2014. Assessment of the Pacific halibut stock at the end of 2013. In IPHC Report of Assessment and Research Activities 2013. p. 169-196.\nThompson, W. F., and F. H. Bell. 1934. Biological statistics of the Pacific halibut fishery (2) Effect of changes in intensity upon total yield and yield per unit of gear. In International Pacific Halibut Commission Scientific Report No. 8. 47 p.\nWebster, R. A., W. G. Clark, B. M. Leaman, and J. E. Forsberg. 2013. Pacific halibut on the move: a renewed understanding of adult migration from a coastwide tagging study. Canadian Journal of Fisheries and Aquatic Sciences 70 (4):642-653.","New NOAA satellite tracks lightning in real time from space\nNOAA's GOES-16 weather satellite released the first images from a new instrument that tracks lightning storms on Earth.\n—The National Oceanic and Atmospheric Administration hopes a new satellite in orbit will enlighten our understanding of a flashy weather phenomenon and lead to better forecasts of severe storms.\nNOAA released on Monday the first images captured by the GOES-16 satellite, snapped on Valentine's Day from 22,300 miles above Earth. The images and video show lightning flashes across the Western Hemisphere over the course of an hour.\nThe new instrument, one of several aboard the satellite, marks a leap forward in monitoring and understanding lightning storms. In its first week in orbit, the instrument – the Geostationary Lightning Mapper, or GLM – recorded more lightning data than all previous data captured about the weather event from space combined.\nNOAA hopes that more data will lead to better storm predictions.\n\"As you can imagine, we are pretty excited here at NOAA Satellites,\" spokeswoman Connie Barclay told NPR in an email. \"Lightning strikes the US on average of 25 million times each year, and kills on average 49 people in the US each year.\"\nThe lightning detector is in geostationary orbit – it remains in the same location relative to the ground below it – allowing it to continuously track lightning storms.\nIt works by looking for flashes anywhere in the Western Hemisphere, “so forecasters know when a storm is forming, intensifying, and becoming more dangerous,” explains NOAA.\n“Rapid increases of lightning are a signal that a storm is strengthening quickly and could produce severe weather,” the agency said in a press release. \"When combined with radar and other satellite data, GLM data may help forecasters anticipate severe weather and issue flood and flash flood warnings sooner.\"\nIn addition, the instrument will help identify lightning-sparked wildfires in dry areas like the American West, which should lead to faster response times from fire crews.\nThe first images from the GLM reveal lightning flashes from the Gulf of Mexico to the southern coast of South America. In the image, brighter colors indicate more lightning energy (or more kilowatt-hours of total optical emissions) recorded.\nNOAA also released a video that showed images of lightning storms developing over southeast Texas, as NPR’s Rebecca Hersher reported. Tornadoes from that storm system destroyed homes near Houston, wrote Ms. Hersher.\nUnlike traditional time-lapse animations that appear jerky because the images are presented more quickly than they were gathered, this video is a slower version of what the satellite sees, brought down from the satellite's 500 frames per second to a more human 25 frames per second.\nNOAA also expects the instrument to provide better forecasting of lightning storms over oceans, benefiting those traveling in the air or water, as well as better predictions of in-cloud lightning, a precursor to lightning strikes that make landfall.\nThe GOES-16 satellite was launched in November with the GLM aboard, to the excitement of the weather community.\n\"For weather forecasters, GOES-R will be similar to going from a black-and-white TV to super-high-definition TV,\" said Stephen Volz, assistant administrator for NOAA's Satellite and Information Services division, using another name for the satellite.\n\"For the American public, that will mean faster, more accurate weather forecasts and warnings,\" he said, as well as \"more lives saved and better environmental intelligence for state and local officials and all decision makers.\"\nIn addition to the lightning monitor, the satellite is outfitted with five other imaging and data-collection instruments. A Harris Corp. onboard camera can photograph inside the eye of a hurricane, a new perspective that promises forecasters the ability to measure the intensity and timeline of storms, The Christian Science Monitor previously reported. Another instrument monitors solar flares and space weather and fluctuations in radiation levels they cause.\n“It’s a big deal,\" Fred Johnson, a National Weather Service meteorologist in Melbourne, Fla., told USA Today. \"It’s a big upgrade from what we’ve had in the past. This should save lives and property.\""],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:5c5ac414-106f-48ed-91e9-c9ff3b88f4f2>","<urn:uuid:169abaae-fb3a-41be-b58f-06a48976f818>"],"error":null}
{"question":"I'm writing a paper about cross-cultural education. How do the benefits of studying in different communities within Belgium compare to the advantages of long-term orientation in immigrant students?","answer":"Studying in different Belgian communities and having long-term orientation as an immigrant student offer distinct educational advantages. Students studying across Belgian communities gain 'international' experience within the country's borders, integrate into different languages and cultures, and adapt to different surroundings and mentalities. In contrast, immigrant students with long-term orientation demonstrate progressive academic improvement, showing better performance in standardized tests, higher likelihood of taking advanced classes, fewer disciplinary problems, and better graduation rates. Their success is particularly influenced by cultural values emphasizing delayed gratification, with support from parents who actively choose better schools and advocate for gifted programs.","context":["Paola Giuliano's study finds the benefit to students increases over time\nNature, nurture or ... culture? There's long been a strong correlation between a student's educational success and her family's socioeconomic status. But mining the potential causes — parental education, income and wealth — yields, at most, moderate effects. A new study points to another possible explanation: the capacity of these students to defer gratification, what the behavioral literature calls \"long-term orientation,\" and the role of students' home countries in instilling that trait.\nIn a working paper, Northwestern University's David Figlio, UCLA Anderson's Paola Giuliano, the American Institutes for Research's Umut Özek and Northwestern's Paola Sapienza examined education and birth records from the state of Florida to study the educational achievement of immigrant students living in the United States. They paired that with country-level data on long-term orientation from social psychologist Geert Hofstede, who first noticed country-specific differences while surveying cross-cultural groups as a manager with IBM. Hofstede later expanded that research and developed his cultural dimensions theory (long-term orientation is one of six dimensions) to explain how culture affects behavior.\nControlling for both school quality and individual characteristics, the researchers found that students from countries emphasizing the value of delayed gratification did better than students from countries without that emphasis. In Hofstede's research, East Asian countries, most notably China and Hong Kong, top the list. European countries get moderate scores, while Anglo, African and Latin American countries score lower. In the study by Figlio, Giuliano, Özek and Sapienza, South Korea scores highest and Puerto Rico scores lowest.\nThe benefits are long-lasting, and even get stronger over time. The study includes both first-generation and second-generation immigrants. Students whose home countries value delayed gratification scored higher on third-grade standardized tests in both math and reading, and continued to score higher in subsequent grades. But they didn't do better only in comparison to others — they also did better relative to themselves. This is significant because it's unusual for students, in general, to show major relative improvement between the third and eighth grades. In this study, the higher a student's capacity for delayed gratification, the more likely the student was to improve over time.\nThese students also had fewer absences and fewer disciplinary problems. They were less likely to repeat a grade and more likely to graduate from high school in four years. They were also more likely to take advanced-level classes while in high school, and to choose scientific subjects for those classes.\nBy focusing on children rather than young adults, the researchers identified two other key factors: the role of parents in transmitting cultural values, and the role of social learning in reinforcing those values. Parents from countries with a long-term orientation were more likely to choose better schools and to advocate, where appropriate, for their kids to join gifted programs. Once in school, these students did even better if they were surrounded by other immigrants speaking the same language.\nFiglio, Giuliano, Özek and Sapienza chose to study immigrants in part because non-native people tend to maintain a strong cultural connection with their home countries. And since many immigrants come from lower-income backgrounds, it's far easier to see whether cultural factors — versus economics and privilege — account for differences. Florida offered a particularly rich data set because its foreign-born population is large — more than 4 million people — and incredibly diverse, representing Hispanic, Asian, European and African countries, as well as non-Hispanic Caribbean countries.","International orientation is an important basic competence for students, as the organisations by which you may be employed as a graduate are often active internationally. This is why Hasselt University encourages its students to spend time abroad, and we have noticed that this allows our students to develop an open, global vision and international competencies (including independence, self-confidence, improved language, communicative and social skills and a broader knowledge of your own field). In short, international experience is an important advantage in the globalised labour market. Of course, you can pursue an internship abroad either during or after your studies at Hasselt University.\nWould you like to say goodbye to Hasselt University's lecture halls and experience studying abroad? Some programmes offer the possibility of an internship abroad.\nYou can apply through your student file (exchange section), where you can also consult destinations that are available for your programme. The deadline for application is 20 February.\nThere are various scholarships with which you can (partially) finance your stay abroad\nErasmus+ is a programme of the European Commission. One of the main objectives of Erasmus is to stimulate the mobility of students and lectors and professors within higher education between participating countries.\nThe European Commission granted the Erasmus Charter for Higher Education to both Hasselt University and tUL for the complete Erasmus+ programme period 2021 - 2027. The Charter and the Erasmus Policy Statement can be consulted below:\nAll faculties of Hasselt University offer their students the opportunity to go on an Erasmus exchange for a study and/or internship stay abroad. This involves staying for a minimum of three (study) months and a maximum of one complete academic year in one of the participating countries (28 EU member states + Norway, Lichtenstein, Iceland, Turkey and the Former Yugoslav Republic of Macedonia (FYROM)). For more information about the possibilities within your programme, please contact the internationalisation coordinator of your programme.\nThe internationalisation coordinator of your programme is - in consultation with the programmes - in charge of the selection of the candidates and will rank the selected candidates. This ranking determines the order in which the Erasmus scholarships are awarded.\nMore information about Erasmus+ and Erasmus is available on the website of the European Commission. Do you still have questions? Please contact the international office (firstname.lastname@example.org).\nErasmus Belgica is a partnership between the Flemish Community, the French Community and the German-speaking Community aimed at improving the mobility of students and teachers in higher education between the Communities of Belgium. The initiative was founded with assistance from the Prince Philippe foundation.\nAn international experience is often a fundamental requirement of employers for candidates for a job. Due to its special structure, Belgium offers students the unique advantage of being able to gain \"international\" experience within the borders of the country. The participating universities and university colleges are required to conclude an agreement with the partners of the other Communities.\nThe goal is to provide students the opportunity to spend part of their programme at a university or university college in another Community. This gives them a chance to integrate into the language and culture of another Community of Belgium and to adapt to different surroundings and a different mentality.\nGenerally speaking, the programme is conducted similarly to the European Erasmus programme. For more information regarding the conditions, nomination process, manuals, etc., please visit the Erasmus page.\nContact Laurien Stuvers of the international office for more information.\nThe Priority Country Programme aims to promote the exchange of students between the community of Flanders and the following priority countries: Brazil, Chile, Japan, Mexico, Morocco, Russia, South Africa, Turkey and the United States of America.\nThe ASEM Work Placement Programme (ASEM WPP) promotes the exchange of talent between Europe and Asia by offering scholarships to higher education students for an internship in a number of Asian countries.\nThe government of Flanders awards grants for student mobilities to countries outside Europe that are not among the Erasmus+ programme countries. This scholarship programme aims to encourage mobilities for studies, internships or thesis research in countries outside Europe, complementarily to those included in the Erasmus + programme.\nVLIR-UOS and Hasselt University award travel scholarships on an annual basis to students who would like to travel to a developing country for an internship or to perform (thesis) research. This scholarship programme aims to give students the opportunity to acquire field experience within an academic context.\nTo be eligible, students must find a Flemish and a local promoter who are willing to support the (research) project. The home institution must also thoroughly prepare the student with practical and substantive information.\nAn international internship is an opportunity to find your bearings on the international job market, supplement your academic knowledge with practical experience, improve your language skills and to get to know the work ethic of the host country.\nMost international organisations offer the opportunity to do an internship with them"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:606fce58-d207-467f-8b48-a598d14894b4>","<urn:uuid:bd1fa6f0-1da0-47f8-ad9c-ba75b6c2490e>"],"error":null}
{"question":"What blood pressure level is considered a hypertensive crisis requiring urgent medical attention?","answer":"A blood pressure reading above 180/120 mm Hg indicates a hypertensive crisis and requires urgent treatment even if there are no accompanying symptoms.","context":["- Does 140 80 require medication?\n- Does anxiety cause high blood pressure?\n- Does treated high blood pressure shorten your life?\n- What foods are bad for high blood pressure?\n- Is 160 blood pressure high?\n- What if my blood pressure is 160 90?\n- Which fruit is best for high blood pressure?\n- Can drinking too much water cause high blood pressure?\n- Is Egg good for high blood pressure?\n- What is considered a dangerous blood pressure?\n- How can I quickly lower my blood pressure?\n- Does drinking water help reduce blood pressure?\n- What is a good blood pressure by age?\n- How do you feel when you have high blood pressure?\n- What is considered stroke level high blood pressure?\n- What is the best drink for high blood pressure?\n- Why is my blood pressure suddenly high?\n- When should someone go to the hospital for high blood pressure?\n- How do I lower my blood pressure 160 100?\n- What are the 5 warning signs of a stroke?\n- Should I be worried if my blood pressure is 150 100?\nDoes 140 80 require medication?\n120 to 129/less than 80 (Elevated): You probably don’t need medication.\n130/80 to 139/89 (stage 1 hypertension): You might need medication.\n140/90 or higher (stage 2 hypertension): You probably need medication..\nDoes anxiety cause high blood pressure?\nAnxiety doesn’t cause long-term high blood pressure (hypertension). But episodes of anxiety can cause dramatic, temporary spikes in your blood pressure.\nDoes treated high blood pressure shorten your life?\nIn fact, those in the study who took medicine to lower their blood pressure for more than four years reduced their risk of dying from cardiovascular disease over a 20-year period, the researchers found. “For the first time, we prove that treating high blood pressure prolongs life,” said lead researcher Dr.\nWhat foods are bad for high blood pressure?\nWhat foods are high in sodium?Processed foods such as lunch meats, sausage, bacon, and ham.Canned soups, bouillon, dried soup mixes.Deli meats.Condiments (catsup, soy sauce, salad dressings).Frozen and boxed mixes for potatoes, rice, and pasta.Snack foods (pretzels, popcorn, peanuts, chips).More items…•\nIs 160 blood pressure high?\nNormal pressure is 120/80 or lower. Your blood pressure is considered high (stage 1) if it reads 140/90. Stage 2 high blood pressure is 160/100 or higher. If you get a blood pressure reading of 180/110 or higher more than once, seek medical treatment right away.\nWhat if my blood pressure is 160 90?\nnormal: less than 120/80 mm Hg. pre-hypertensive: systolic between 120-139 or diastolic between 80-89. stage 1 hypertension: systolic 140-159 or diastolic 90-99. stage 2 hypertension: systolic 160 or higher or diastolic 100 or higher.\nWhich fruit is best for high blood pressure?\nBerries, especially blueberries, are rich in natural compounds called flavonoids. One study found that consuming these compounds might prevent hypertension and help lower blood pressure. Blueberries, raspberries, and strawberries are easy to add to your diet.\nCan drinking too much water cause high blood pressure?\nThe National Academy of Sciences recommends drinking when thirsty rather than consuming a specific number of glasses daily. It is unlikely that drinking water raises blood pressure. A healthy body regulates fluids and electrolytes quickly.\nIs Egg good for high blood pressure?\nEggs are also a well-known source of protein which is perfect for breakfast. Egg whites are especially good for high blood pressure. You can prepare scrambled eggs and add some vegetables to it.\nWhat is considered a dangerous blood pressure?\nDanger zone A blood pressure reading above 180/120 mm Hg indicates a serious health problem. The AHA refers to these high measurements as a “hypertensive crisis.” Blood pressure in this range requires urgent treatment even if there are no accompanying symptoms.\nHow can I quickly lower my blood pressure?\nHere are some simple recommendations:Exercise most days of the week. Exercise is the most effective way to lower your blood pressure. … Consume a low-sodium diet. Too much sodium (or salt) causes blood pressure to rise. … Limit alcohol intake to no more than 1 to 2 drinks per day. … Make stress reduction a priority.\nDoes drinking water help reduce blood pressure?\nThe answer is water, which is why when it comes to blood pressure health, no other beverage beats it. If you’re looking to up the benefits, studies have shown that adding minerals such as magnesium and calcium to water can further aid in lowering blood pressure.\nWhat is a good blood pressure by age?\nWhat Should Blood Pressure be According to Age?Approx. Ideal BP According to Age ChartAgeFemaleMale19-24120/79120/7925-29120/80121/8030-35122/81123/8223 more rows•Nov 19, 2017\nHow do you feel when you have high blood pressure?\nIn some cases, people with high blood pressure may have a pounding feeling in their head or chest, a feeling of lightheadedness or dizziness, or other signs. Without symptoms, people with high blood pressure may go years without knowing they have the condition.\nWhat is considered stroke level high blood pressure?\nA hypertensive crisis is a severe increase in blood pressure that can lead to a stroke. Extremely high blood pressure — a top number (systolic pressure) of 180 millimeters of mercury (mm Hg) or higher or a bottom number (diastolic pressure) of 120 mm Hg or higher — can damage blood vessels.\nWhat is the best drink for high blood pressure?\n7 Drinks for Lowering Blood PressureTomato juice. Growing evidence suggests that drinking one glass of tomato juice per day may promote heart health. … Beet juice. … Prune juice. … Pomegranate juice. … Berry juice. … Skim milk. … Tea.\nWhy is my blood pressure suddenly high?\nCommon causes of high blood pressure spikes These spikes, which typically last only a short period of time, are also known as sudden high blood pressure. These are some possible causes: Caffeine. Certain medications (such as nonsteroidal anti-inflammatory drugs) or combinations of medications.\nWhen should someone go to the hospital for high blood pressure?\nSeek emergency care if your blood pressure reading is 180/110 or higher and you have any of the following symptoms, which may be signs of organ damage: Chest pain. Shortness of breath. Numbness or weakness.\nHow do I lower my blood pressure 160 100?\nStop high blood pressure and stop your blood pressure from increasingBe physically active for 30 to 60 minutes on most days of the week. … Eat a lot of fresh fruits and vegetables, low-fat dairy products and other foods low in fat and salt. … Eat less salt in your diet.More items…\nWhat are the 5 warning signs of a stroke?\n5 Classic Warning Signs of StrokeWeakness or numbness in the face, arm or leg, usually on just one side.Difficulty speaking or understanding language.Decreased or blurred vision in one or both eyes.Unexplained loss of balance or dizziness.Severe headache with no known cause.\nShould I be worried if my blood pressure is 150 100?\nDepending on the exact classification used, pressures around 140-150/90-100 would be called mild hypertension. Pressures around 150-170/100-110 would be called moderate, and pressures higher, e.g. 200/120 would be considered fairly severe."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:3ff51b10-ce45-46c3-8a05-7007cfe26917>"],"error":null}
{"question":"I'm studying traditional versus modern approaches in marine restoration. Can you explain how ūd instruments are used in Yemeni music preservation, and how coral transplantation techniques help preserve marine ecosystems?","answer":"In Yemeni music, the ūd plays a crucial role in preserving traditional forms, particularly in the firtāsh - an instrumental introduction to singing that originated from Turkish influence but became fully Yemeni. The ūd is the main instrument used to perform firtāsh as an introduction to the qawma. As for coral preservation, transplantation is used to provide immediate coral coverage to damaged reefs, with key objectives including improving reef quality, biodiversity, and topographic complexity. This technique can accelerate reef recovery after ship groundings, replace corals killed by pollutants, and help save threatened coral communities. However, transplantation must be preceded by indirect action to address root causes of damage, or it will prove ineffective.","context":["The Arab Music Archiving and Research foundation (AMAR), in collaboration with the Sharjah Art Foundation (SAF), presents “Durūb al-Nagham”.\nWelcome to a new episode of “Durūb al-Nagham”.\nToday, we will resume our discussion with Prof. Jean Lambert, a music researcher specializing in Music in Yemen who has published numerous books and research reports on this subject.\nIn the two previous episodes, we talked a lot about the History, the music theory, and the cultural forms.\nLet us talk now about the musical forms, regardless of the poetry that will be sung… Let us talk about the form as a musical form. What are the forms existing in the Literary or Classical Yemeni Music? Did instrumental music play a significant role?\nThe musical forms are built mostly upon the rhythm, i.e. the rhythms I mentioned: the das‘a, the wusṭa and the sāri‘. There is also the wusṭa kawkadāniyya that includes a type of 3-pulse’ rhythm, and the saja‘ –a simple 2-pulse rhythm that accompanies the extended melodies–, along with some rhythms that came from other regions in Yemen, such as the sharḥ –a 2-3 pulse’ rhythm including a variety of rhythms (Polyrhythmic). These are the major forms in Sanaa singing, along with the muṭawwal –a performance without a set rhythm, i.e. mursal.\nInstrumental music is not widespread in Yemen where it exists under one form whose origin is said to be Turkish, called firtāsh: it is a muwaqqa‘ section, yet allowing variation and improvisation by the artist.\nWe will now listen to a firtāsh –called Turkish firtāsh by Yemenis, which indicates its Ottoman origin. Yet, it has become a 100% Yemeni form with all its characteristics –by Mr. Ḥasan al-‘Ajamī who specializes in this form.\nIt is played as an instrumental introduction to the singing.\nThe firtāsh is played as an introduction to the qawma.\nIt is played by only one instrument, the ‘ūd.\nIt is played by the ‘ūd and also by a percussion instrument called the ṣaḥn, the brass ṣaḥn or the ṣaḥn mīmyā, a plate used as an instrument, similar to the far-east gong –brass plate sometimes played with a wooden or iron stick. Yemenis play it in a very distinctive manner: the plate is balanced on both thumbs and played with the other fingers, a very difficult technique because it is difficult for the instrumentalist to reach a balance.\nThey hold it with both the right and the left thumbs and play it with the rest of the fingers.\nYes, the ṣaḥn is balanced on both thumbs and thus can produce, beautifully, echo and resonance to its full capacity. The ṣaḥn may accompany the voice alone because it is considered to be an instrument in its full right. Yet the one who sings accompanied by a ṣaḥn must enrich the melody with some layālī, humming, and such scanned words, in order to replace the ‘ūd, because someone must sing the melody in the absence of a ‘ūd.\nWe will listen to an example of ṣaḥn playing by artist Muḥammad al-Khamīsī, may he rest in peace, who specialized in this instrument and also sang beautiful qaṣīda accompanied by the brass ṣaḥn.\nWe talked a lot about the qawma. Do we have a recording of a beautiful qawma to end today’s beautiful episode?\nYes. I suggest we listen to a qawma by Yaḥya al-Nūnū, God grant him long life, a capable Sanaa artist who lives in Sanaa, recorded at the Maison des Cultures du Monde in Paris, and which he starts with a firtāsh followed by qaṣīda “Bi-ism al-Lāh mawlānā ibtadaynā”. He starts it with a das‘a followed by a wusṭa, then a sāri‘.\nWith this qawma performed by Mr. Yaḥya al-Nūnū we reach the end of our discussion on Music in Yemen…\nWe will meet again in a new episode.\nWe thank Prof. Jean Lambert.","Melissa and friends walk the streets of Bocas, Panama.\nCoral reef accretion continues to be defeated by an influx of disturbances that pose a serious threat to marine ecosystems. Anthropogenic impacts by the human race have administered blows of colossal degrees on reefs across the globe. Pressures caused by anchor damage, vessel groundings, blast fishing, coastal development, recreational activities, coral mining, dredging and numerous other damaging activities have created grave disturbances in coral reef environments. This has greatly hindered reefs’ abilities to recover naturally and so has birthed a new recovery effort. Coral reef restoration has become paramount in the struggle to keep reef ecosystems a thriving bionetwork of the oceans but there is still much more to be determined about the benefits of certain methods. Still in its infancy, restoration techniques are only just beginning to situate themselves among the scientific community. Very little has been decided on these new practices and methods and there is widespread debate on what are the most beneficial choices for reefs, their inhabitants, and their surrounding environments. Numerous techniques and processes are currently being discussed and put to the test including indirect action, reef repair such as triage, restoring structural integrity and restoring topographic complexity, transplantation, artificial reefs, and the use of reef gardening and seeding.\nWhen discussing the revival of coral reef communities two terms have come into use – restoration and rehabilitation. While they may at first seem to be interchangeable a clear distinction has been made between these terms in the scientific community. Restoration is “…used to indicate human intervention that is designed to accelerate the recovery of damaged habitats, or to bring ecosystems back to as close an approximation as possible of their pre-disturbance states” (Yap 841). Rehabilitation, on the other hand, refers to “the act of partially or, more rarely, fully replacing structural or functional characteristics of an ecosystem that have been diminished or lost, or the substitution of alternative qualities or characteristics that those originally present with the provision that they have more social, economic or ecological value than existed in the disturbed or degraded state” (842). The former is the obvious preferred definition for what may be done for coral reefs and so sets up the stage for what are considered to be the most beneficial restoration techniques.\nIndirect action is considered to be the simplest and most necessary technique to be applied to coral reef restoration efforts. This method revolves around eliminating the very root causes that have pushed reefs to their damaged positions. Being the least invasive of techniques, it requires solely that the source of disturbances, usually anthropogenic, be removed to rebuild reef health. Some irritations that may be eliminated include frequent vessel groundings, nutrient loading, causes of water pollution, and sedimentation runoff. Without the removal of these and many other disturbances coral reef environments, even with the addition of several other costly restoration actions, will never be able to fully recover and will only continue down their road of destruction (Precht 40-41). Indirect action should always be the first step in restoring reefs to their original splendor.\nThe second critical method to be considered is that of reef repair. This approach is another fundamental step in restoring reefs and includes “…emergency triage, restoring the structural integrity of the reef framework, and/or restoring topographic complexity” (Precht 41). These restoration techniques aid in reducing further damage after its initial cause and enhance the natural recruitment process. Most if not all restoration projects have included some form of reef repair in their framework.\nAs mentioned above triage is a key practice in reef repair. This is a term that involves a variety of restoration activities that all revolve around the stabilization and return to the needs of the current reef environment. Technique components involved in this method are “…careful vessel salvage following a grounding event, stabilization or removal of loose sediment and/or coral rubble, removal of debris (foreign objects), and the recovery, storage and/or reattachment of dislodged corals, sponges, and other reef biota” (Precht 41). Vessel salvage will usually include removing objects from deck to increase buoyancy and make removal easier. The stabilization and removal of rubble and debris is imperative to a reef community because storms can easily create a resuspension of these particles seriously affecting the environment. Lift bags, lift vacuums, clam dredges and suction dredges are all put to use in the removal of rubble from a reef area. Numerous adhesive materials can be used in the stabilization practice including epoxy, limestone boulders, and concrete mats. Dislodged corals have a chance for survival if attached soon after a disturbance. Attachment methods include using epoxy, cement, expansion anchors, threaded rods, wires, nails, bamboo skewers, and plastic wire ties (Precht 41). All of these processes further aid in reducing the possibility of secondary damage from a disturbance.\nOne of the most important features for the flourishing of reef health is its structural integrity. Most often this type of damage is caused by vessel groundings and occurrences of blast fishing. It is of the utmost importance that careful consideration be taken when choosing a substrate material to be used in restoration. An artificial substrate’s purpose is determined by “1) Its structural characteristics (composition, surface, design, and stability) [and] 2) The environmental characteristics (temperature, light, sediment, surrounding biota, hydrodynamics, depth, and temporal effects)” (Precht 43). Structural integrity repair is restoration of the usually unseen cornerstones of the reef.\nAnother result of frequent vessel groundings, blast fishing, coral mining, and other structurally damaging activities is the loss of topographic complexity. The reestablishment of topographic integrity is vital to the survival of the current collection of plants and animals living in and amongst a reef. The structural features of a reef environment “…greatly affect[s] the species diversity, density and size distribution of both invertebrates and fish…” (Perkol-Finkel, Shashar, Benayahu 122). If the correct form of substrate is not chosen for the repair of a reef structure a whole new reef may come into existence depending on what fish and coral are recruited to the new form of grounding. Differing substrates and varying vertical/horizontal frameworks will attract very different species. Limestone and concrete are the most common and appropriate materials for the restoration of topographic complexity (Precht 43). The method most usually involves placing large boulders or concrete mats over demolished rubble sights or atop sunk or grounded vessels to add suitable stability.\nTransplantation has become one of the most popular methods in coral reef restoration. It is seen as the most effective way of providing immediate coral coverage to a damaged reef. The primary objectives of coral transplantation are to improve reef quality, biodiversity, and topographic complexity. Reasons for the transplantation of corals to a site may include a need to:\n1.accelerate reef recovery after ship groundings\n2.replace corals killed by sewage, thermal effluents, or other pollutants\n3.save coral communities or locally rare species threatened by pollution, land reclamation or pier construction\n4.accelerate recovery of reefs after damage by Crown-of-Thorns starfish or red tides\n5.aid recover of reefs following dynamite fishing or coral quarrying\n6.mitigate damage caused by tourists engaged in water-based recreational activities\n7.enhance the attractiveness of underwater habitat in tourism areas (Edwards, Clark 475)\nThere have been numerous advantages and disadvantages found within this method that has spawned intense debate among scientists on the true profit that transplantation may contain for coral reefs. According to scientist Helen Yap there are three areas that need to be well understood before transplantation should take place – 1) knowledge of coral species being used 2) ecological requirements of corals and 3) particular local ecological conditions to which they will be exposed (Yap 1). Some of the most popularly accepted advantages as stated in Edwards’ and Clark’s paper “Coral Transplantation: A useful tool or misguided meddling?” are:\n1.Immediate increase in coral cover and diversity\n2.Increased recruitment of coral larvae as a result of the presence of transplants\n3.Survival of locally rare and threatened coral species when primary habitat is destroyed\n4.Reintroduction of corals to areas which are larval supply limited or have very high post-settlement mortality\n5.Improved aesthetics of areas frequented by tourists\n6.Instant increase in rugosity and shelter for herbivores in bare areas (476)\nDisadvantages that are just as quickly merited are:\n1.Loss of coral colonies from donor reef areas\n2.Higher mortality rates of transplanted corals\n3.Reduced growth rates of transplanted corals\n4.Loss of transplanted colonies as a result of wave action (attachment failure)\n5.Reduced fecundity of transplanted colonies due to stress of transplantation\n6.Raised public expectations followed by disillusionment when transplants suffer high mortality (476)\nRegardless of these lists it has become increasingly obvious that if indirect action is not first taken and all chronic problems cleared up transplantation will prove to be totally fruitless (Yap 1). In many cases transplantation is carried out before the necessary preliminary actions are taken. Politicians especially look to transplantation as a way to show immediate results for the public who then see that years later the transplants have not survived and loose faith in the benefits of other restoration methods.\nAn alternative to minimize impacts to donor coral environments can be found in Coral gardening and coral seeding. Coral gardening is the mariculture of corals for use in coral restoration that avoid great amounts of damage and adverse affects on donor populations while providing possible genetic diversity to damaged areas. Coral seeding is also a method of enhancing recruitment for restoration. It involves collecting larvae or coral spawn that has been cultured in the laboratory and then deposited on reef substrate (Precht 45).\nAnother alternative to natural reef recovery comes in the form of artificial reefs which are “…man-made objects deployed on the seabed for the purpose of influencing physical, biological, or socio-economical processes related to living marine resources” (Einbinder, Perelberg, Ben-Shaprut, Foucart, Shashar 111). These mock reefs are deployed in areas with large amounts of coral destruction and structure loss and may be used to retard the stress on natural reefs in the vicinity (Lan, Hsui 663). These reefs have two ways of arriving at their destinations, either by accident such as sunken vessels and other various submerged rubble or by pre-thought placement. The latter of these is greatly preferred by scientists across the board however debate has also risen on whether these artificial alternatives may actually have a negative affect on its surrounding environments. At the moment few criteria exist for deploying artificial reefs and most of the decisions are based on personal judgment calls (Lan, Hsui 664).\nStructural features have proven to be a huge determining factor in the success of artificial reefs. A study was done on how well an artificial reef can mimic its adjacent natural reef communities. It was made painfully obvious that even after centuries if the structural features of the artificial reef are not almost identical to that of the natural then their communities will remain distinct. Impacts include attracting animals away from close natural reefs and bringing new varieties of plants and animals to an area that were not able to be supported before. The latter part, however, may not always be included as a negative impact (Perkol, Finkel, Shashar, Benayahu 2).\nScientists have recently become interested in looking at the affects of artificial reefs on neighboring environments. A study done by exposing heaps of magroalgae to natural grazing areas at various distances from artificial reefs has shown that these false reefs change grazing patterns of ocean animals. This increase in grazing may have many significant affects on the environment such as an increase in fish abundance, an increase in fish variety (depending on the structural features of the artificial reefs), a decrease in the amount of grazing food, and an increase in predation (Einbinder, Perelberg, Ben-Shaprut, Foucart, Shashar 111).\nAnother important debate sprouted by the increased use of artificial reefs in restoration is called the attraction-production hypothesis. This poses the question of “…whether artificial reefs merely attract resources away from the natural surroundings, thus depleting [them, or] whether they are capable of producing a biomass that would have otherwise been lost without the artificial reef” (Perkol-Finkel, Benayahu 25). The possible benefits of such changes remains controversial making its continued study of utmost importance. A closer look needs be taken in regards to an artificial reefs secondary effects on its surrounding environment and especially upon neighboring natural reefs.\nThe need for well researched and practiced methods for coral reef restoration has never been more critical. Coral environments around the globe are literally dying out due to an increased human involvement. Various methods are being tested but there are still masses of practices to be learned and put to use. Restoration techniques are vital not only for the reefs they are meant to revive but for the secondary affects reef loss has on all aspects of life. Coral reef restoration may very well be our last chance to conserve this important bionetwork.\nEdwards, Alasdair, and Clark. \"Coral Transplantation: A Useful Management Tool or Misguided Meddling?.\" Marine Pollution Bulletin 37(1998): 474-487.\nEinbinder, Shai, Perelberg, Ben-Shaprut, Foucart, and Shashar. \"Effects on artificial reefs on fish grazing in their vicinity: Evidence from algae presentation experiments.\" Marine Environmental research. 61(2006): 110-119.\nLan, Chun-Hsiung, and Hsui. \"The deployment of artificial reef ecosystem: Modelling, simulation and application.\" Simulation Modelling Practice and Theory 14(2006): 663- 675.\nPerkol-Finkel, S., and Benayahu. \"Differential recruitment of benthic communities on neighboring artificial and natural reefs.\" Journal of Experiemental Marine Biology and Ecology 340(2007): 25-39.\nPerkol-Finkel, S., Shashar, and Benayahu. \"Can artificial reefs mimic natural reef communities? The roles of structural features and age.\" Marine Environmental Research 61(2006): 121- 135.\nPrecht, William. Coral Reef Restoration Handbook. 1st. Boca Raton: CRC Press, 2006.\nYap, Helen. \"The case for restoration of tropical coastal ecosystems.\" Ocean and Coastal Mangement 43(2000): 841-851.\nYap, Helen. \"Coral reef \"restoration\" and coral transplantation.\" Marine Pollution Bulletin 46(2003): 529.\nReturn to Topic Menu\nWe also have a GUIDE for depositing articles, images, data, etc in your research folders.\nArticle complete. Click HERE to return to the Pre-Course Presentation Outline and Paper Posting Menu. Or, you can return to the course syllabus\nWEATHER & EARTH SCIENCE RESOURCES\nOTHER ACADEMIC COURSES, STUDENT RESEARCH, OTHER STUFF\nTEACHING TOOLS & OTHER STUFF\nIt is 5:35:01 PM on Sunday, September 27, 2020. Last Update: Wednesday, May 7, 2014"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:be487db5-a6c3-4996-809e-799b9aa4daa6>","<urn:uuid:eba8ba60-bdb2-43c4-a698-11d41fa2e1a4>"],"error":null}
{"question":"How many hours of supervised driving practice do teen drivers typically need?","answer":"Teen drivers should receive at least 40 hours of additional behind-the-wheel instruction from a licensed adult, under various driving conditions including nighttime and inclement weather. Most states legally require between 40 to 50 hours of supervised driving. This practice is crucial since three out of four serious teen driver crashes are due to inexperience.","context":["Learning how to drive is a major milestone in anyone’s life. If your son or daughter is getting behind the wheel for the first time, you’re probably a very proud parent. And you know it’s important to teach your teen that driving is a privilege – not a right.\nDriving is a complex task that involves mastery of multiple skills. It requires a full understanding of the rules of the road and how to adjust to different driving situations and circumstances. Over time, your son or daughter may be able to predict actions other drivers may take.\nEnrolling your teen in an accredited driver’s education program can help improve their confidence and skill level – and can give you peace of mind when they’re on the road.\nIs driver’s ed required for your child?\nIn many states, teens aren’t required to complete a driver’s ed course to obtain their driver’s license. New York, for instance, doesn’t require driver’s ed; California and Texas do.\nGeorgia requires drivers to complete a certified driver’s education course to receive their license at 16. Otherwise, they must wait until 17. The law, Joshua’s Law, was passed after teenager Joshua Brown died following a car accident on a wet road.\nSince laws regarding driving and driver’s licensing vary by state, check your state’s department or bureau of motor vehicles website to find out about specific requirements.\nIs driver’s ed worth it?\nDriver’s education courses can offer very real benefits:\nYour child can become more confident.\nWhen your teen successfully completes a driver’s ed course, they may feel more self-assured about driving because they’ve learned best practices from a professional training instructor.\nYour child will learn about the responsibilities of driving.\nAfter completing a driver education class, your child will have a better understanding of the responsibilities all drivers share.\nYour child can become a safer driver.\nIn addition to teaching driving etiquette, certified driver’s ed courses address safety. Young drivers are taught the rules of the road and the importance of avoiding distractions while driving. Instruction may also include safety tips specific to your area – like tips about driving in different weather conditions and navigating busy roads.\nIs driver’s ed alone enough?\nWhile enrolling your child in a certified driver’s ed course is an important first start, it should only supplement the other driving lessons they receive. Teens are new to driving and they need as much experience as possible. Three out of four serious teen driver crashes are due to inexperience, according to the Children’s Hospital of Philadelphia Research Institute.\nWe recommend that your teen driver receives at least 40 hours of additional behind-the-wheel instruction from a licensed adult, preferably under a variety of different driving conditions, including nighttime and inclement weather driving. Most states require 40 to 50 hours of supervised driving by law, so check your state’s bureau of motor vehicles website for more information.\nTeens often emulate how their parents drive, so be sure to set a great example. It’s likely that your teen will make a few mistakes. You should always remain calm and provide clear instructions ahead of time.\nDuring this exciting journey, check out our teen driving resources for more useful information."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:00321df4-7697-4c18-a45a-870709e38195>"],"error":null}
{"question":"What are the overdose risks and fatal outcomes associated with Nucynta versus Tramadol?","answer":"Both medications can cause fatal overdoses but present different risks. Nucynta overdose symptoms include slow breathing and heart rate, severe drowsiness, muscle weakness, cold and clammy skin, pinpoint pupils, and fainting, with explicit warning that overdose can be fatal especially in children or those without prescription. For Tramadol, overdose symptoms include abnormally low blood pressure, slow heart rate, and sweaty/clammy skin. The broader context of opioid overdoses shows their severity - in 2017 alone, 47,600 people died from opioid overdoses in the US, with over 399,000 opioid-related deaths occurring between 1999-2017.","context":["Generic Name: tapentadol (ta PEN ta dol)\nBrand Names: Nucynta, Nucynta ER\nWhat is Nucynta?\nNucynta (tapentadol) is an opioid pain medication. An opioid is sometimes called a narcotic.\nNucynta is used to treat moderate to severe pain.\nThe extended-release form of tapentadol (Nucynta ER) is for around-the-clock treatment of pain that is not controlled by other medicines. It is not for use on an as-needed basis for pain.\nNucynta may also be used for purposes not listed in this medication guide.\nYou should not use Nucynta if you have severe breathing problems, or a bowel obstruction called paralytic ileus.\nDo not use Nucynta if you have used a MAO inhibitor in the past 14 days. A dangerous drug interaction could occur. MAO inhibitors include isocarboxazid, linezolid, methylene blue injection, phenelzine, rasagiline, selegiline, or tranylcypromine.\nTapentadol can slow or stop your breathing, especially when you start using this medicine or whenever your dose is changed. Never take this medicine in larger amounts, or for longer than prescribed. Do not crush, break, or open an extended-release tablet. Swallow it whole to avoid exposure to a potentially fatal dose.\nTapentadol may be habit-forming, even at regular doses. Take this medicine exactly as prescribed by your doctor. Never share the medicine with another person. MISUSE OF NARCOTIC PAIN MEDICATION CAN CAUSE ADDICTION, OVERDOSE, OR DEATH, especially in a child or other person using the medicine without a prescription.\nTell your doctor if you are pregnant. Nucynta may cause life-threatening withdrawal symptoms in a newborn if the mother has taken this medicine during pregnancy.\nDo not drink alcohol. Dangerous side effects or death could occur when alcohol is combined with tapentadol.\nBefore taking this medicine\nYou should not use Nucynta if you are allergic to tapentadol, or if you have:\nsevere asthma or breathing problems; or\na bowel obstruction called paralytic ileus.\nDo not use Nucynta if you have taken a MAO inhibitor in the past 14 days. A dangerous drug interaction could occur. MAO inhibitors include isocarboxazid, linezolid, methylene blue injection, phenelzine, rasagiline, selegiline, and tranylcypromine.\nTapentadol may be habit forming. Never share Nucynta with another person, especially someone with a history of drug abuse or addiction. Keep the medication in a place where others cannot get to it. Selling or giving away Nucynta to any other person is against the law\nSome medicines can interact with tapentadol and cause a serious condition called serotonin syndrome. Be sure your doctor knows if you also take medicine for depression, mental illness, Parkinson's disease, migraine headaches, serious infections, or prevention of nausea and vomiting. Ask your doctor before making any changes in how or when you take your medications.\nTo make sure this medicine is safe for you, tell your doctor if you have:\nany type of breathing problem or lung disease;\na history of head injury, brain tumor, or seizures;\na history of drug abuse, alcohol addiction, or mental illness;\nliver or kidney disease;\nproblems with your gallbladder, pancreas, or thyroid; or\nif you use a sedative like Valium (diazepam, alprazolam, lorazepam, Ativan, Klonopin, Restoril, Tranxene, Versed, Xanax, and others).\nNucynta is more likely to cause breathing problems in older adults and people who are severely ill, malnourished, or otherwise debilitated.\nIt is not known whether Nucynta will harm an unborn baby. If you use tapentadol while you are pregnant, your baby could become dependent on the drug. This can cause life-threatening withdrawal symptoms in the baby after it is born. Babies born dependent on habit-forming medicine may need medical treatment for several weeks. Tell your doctor if you are pregnant or plan to become pregnant.\nIt is not known whether tapentadol passes into breast milk or if it could harm a nursing baby. You should not breast-feed while using this medicine.\nDo not give Nucynta to a child.\nHow should I take Nucynta?\nTake Nucynta exactly as prescribed. Follow all directions on your prescription label. Nucynta can slow or stop your breathing, especially when you start using this medicine or whenever your dose is changed. Never take Nucynta in larger amounts, or for longer than prescribed. Tell your doctor if the medicine seems to stop working as well in relieving your pain.\nTapentadol may be habit-forming, even at regular doses. Take this medicine exactly as prescribed by your doctor. MISUSE OF NARCOTIC PAIN MEDICATION CAN CAUSE ADDICTION, OVERDOSE, OR DEATH.\nTake this medicine with a full glass of water. Nucynta can be taken with or without food.\nStop taking all other around-the-clock narcotic pain medications when you start taking Nucynta ER extended-release tablets.\nDo not crush, break, or open an extended-release tablet. Swallow the tablet whole to avoid exposure to a potentially fatal dose.\nNucynta can cause constipation. Talk to your doctor before using a laxative or stool softener to treat or prevent this side effect.\nWhile using Nucynta, you may need frequent blood tests.\nDo not stop using this medicine suddenly, or you could have unpleasant withdrawal symptoms. Ask your doctor how to safely stop using tapentadol.\nNever crush or break a Nucynta tablet to inhale the powder or mix it into a liquid to inject the drug into your vein. This practice has resulted in death.\nStore at room temperature away from moisture and heat.\nKeep track of the amount of medicine used from each new bottle. Nucynta is a drug of abuse and you should be aware if anyone is using your medicine improperly or without a prescription.\nDo not keep leftover Nucynta tablets. Ask your pharmacist where to locate a drug take-back disposal program. If there is no take-back program, flush any unused tablets down the toilet. Disposal of medicines by flushing is recommended to reduce the danger of accidental overdose causing death. This advice applies to a very small number of medicines only. The FDA, working with the manufacturer, has determined this method to be the most appropriate route of disposal and presents the least risk to human safety.\nSee also: Dosage Information (in more detail)\nWhat happens if I miss a dose?\nSince Nucynta is used for pain, you are not likely to miss a dose. If you do miss a dose, take the medicine as soon as you remember. Skip the missed dose if it is almost time for your next scheduled dose.\nDo not take extra medicine to make up a missed dose. Do not take more than your prescribed dose in a 24-hour period.\nWhat happens if I overdose?\nSeek emergency medical attention or call the Poison Help line at 1-800-222-1222. A tapentadol overdose can be fatal, especially in a child or other person using the medicine without a prescription. Overdose symptoms may include slow breathing and heart rate, severe drowsiness, muscle weakness, cold and clammy skin, pinpoint pupils, and fainting.\nWhat should I avoid?\nDo not drink alcohol. Dangerous side effects or death can occur when alcohol is combined with tapentadol.\nThis medication may impair your thinking or reactions. Avoid driving or operating machinery until you know how Nucynta will affect you. Dizziness or severe drowsiness can cause falls or other accidents.\nNucynta side effects\nGet emergency medical help if you have any of these signs of an allergic reaction to Nucynta: hives; difficult breathing; swelling of your face, lips, tongue, or throat.\nLike other narcotic medicines, Nucynta can slow your breathing. Death may occur if breathing becomes too weak.\nCall your doctor at once if you have:\nweak or shallow breathing, weak pulse, slow heartbeat;\na light-headed feeling, like you might pass out;\nsevere drowsiness or dizziness, confusion, problems with speech or balance;\ninfertility, missed menstrual periods;\nimpotence, sexual problems, loss of interest in sex; or\nlow cortisol levels - nausea, vomiting, loss of appetite, dizziness, worsening tiredness or weakness.\nSeek medical attention right away if you have symptoms of serotonin syndrome, such as: agitation, hallucinations, fever, sweating, shivering, fast heart rate, muscle stiffness, twitching, loss of coordination, nausea, vomiting, or diarrhea.\nMorphine and naltrexone is more likely to cause breathing problems in older adults and people who are severely ill, malnourished, or otherwise debilitated.\nCommon Nucynta side effects may include:\nconstipation, mild nausea, stomach pain;\nheadache, tired feeling; or\nmild drowsiness or dizziness.\nThis is not a complete list of side effects and others may occur. Call your doctor for medical advice about side effects. You may report side effects to FDA at 1-800-FDA-1088.\nSee also: Side effects (in more detail)\nNucynta dosing information\nUsual Adult Dose of Nucynta for Pain:\n50 mg, 75 mg, or 100 mg orally every 4 to 6 hours depending upon pain intensity, with or without food.\nOn the first day of dosing, the second dose may be administered as soon as one hour after the first dose, if adequate pain relief is not attained with the first dose. Subsequent dosing is 50 mg, 75 mg, or 100 mg every 4 to 6 hours and should be adjusted to maintain adequate analgesia with acceptable tolerability. Daily doses greater than 700 mg on the first day of therapy and 600 mg on subsequent days have not been studied and are, therefore, not recommended.\nOpioid naive: Initial: 50 mg twice daily (recommended interval: 12 hours); titrate in increments of 50 mg no more frequently than twice daily every 3 days to effective dose (therapeutic range: 100 to 250 mg twice daily) (maximum dose: 500 mg/day)\nOpioid experienced: Initial: 50 mg titrated to an effective dose; titrate in increments of 50 mg no more frequently than twice daily every 3 days (therapeutic range: 100 to 250 mg twice daily) (maximum dose: 500 mg/day). Note: No adequate data on converting patients from other opioids to Nucynta ER extended release.\nConversion from immediate release to extended release: Convert using same total daily dose but divide into two equal doses and administer twice daily (recommended interval: 12 hours) (maximum dose: 500 mg/day).\nWhat other drugs will affect Nucynta?\nNarcotic (opioid) medication can interact with many other drugs and cause dangerous side effects or death. Be sure your doctor knows if you also use:\nother narcotic medications - opioid pain medicine or prescription cough medicine;\ndrugs that make you sleepy or slow your breathing - a sleeping pill, muscle relaxer, sedative, tranquilizer, or antipsychotic medicine; or\ndrugs that affect serotonin levels in your body - medicine for depression, Parkinson's disease, migraine headaches, serious infections, or prevention of nausea and vomiting.\nThis list is not complete. Other drugs may interact with tapentadol, including prescription and over-the-counter medicines, vitamins, and herbal products. Not all possible interactions are listed in this medication guide.\nMore about Nucynta (tapentadol)\n- Side Effects\n- During Pregnancy\n- Dosage Information\n- Drug Images\n- Drug Interactions\n- Support Group\n- Pricing & Coupons\n- En Español\n- 202 Reviews – Add your own review/rating\n- Drug class: narcotic analgesics\nRelated treatment guides\nWhere can I get more information?\n- Your pharmacist can provide more information about Nucynta.\n- Remember, keep this and all other medicines out of the reach of children, never share your medicines with others, and use Nucynta only for the indication prescribed.\n- Disclaimer: Every effort has been made to ensure that the information provided by Cerner Multum, Inc. ('Multum') is accurate, up-to-date, and complete, but no guarantee is made to that effect. Drug information contained herein may be time sensitive. Multum information has been compiled for use by healthcare practitioners and consumers in the United States and therefore Multum does not warrant that uses outside of the United States are appropriate, unless specifically indicated otherwise. Multum's drug information does not endorse drugs, diagnose patients or recommend therapy. Multum's drug information is an informational resource designed to assist licensed healthcare practitioners in caring for their patients and/or to serve consumers viewing this service as a supplement to, and not a substitute for, the expertise, skill, knowledge and judgment of healthcare practitioners. The absence of a warning for a given drug or drug combination in no way should be construed to indicate that the drug or drug combination is safe, effective or appropriate for any given patient. Multum does not assume any responsibility for any aspect of healthcare administered with the aid of information Multum provides. The information contained herein is not intended to cover all possible uses, directions, precautions, warnings, drug interactions, allergic reactions, or adverse effects. If you have questions about the drugs you are taking, check with your doctor, nurse or pharmacist.\nCopyright 1996-2017 Cerner Multum, Inc. Version: 6.07. Revision Date: 2016-09-30, 10:59:56 AM.","Tramadol Use and Abuse\nTramadol Use and Abuse\nWhat is Tramadol?\nTramadol is an opioid pain reliever (OPR) that belongs to the same family as fentanyl and oxycodone. Tramadol, however, ranks far lower in strength than fentanyl or oxycodone. Apart from reducing pain, it is also known as a mood enhancer. This medication works best in cases where the patient is suffering from moderate to severe pain. It is also considered for use in people who are recuperating after surgery.\nTramadol is categorized as a Schedule IV drug because of a lower threat perception for inducing abuse in users. People commonly underestimate its addictive properties because it is weaker than other opioid medications. Since it is an opioid, however, the danger of addiction is still present and looms large on regular users of the drug.\nIs Tramadol Dangerous?\nTramadol usage is always accompanied by the possible threat of adverse reactions, even when used with caution and under medical supervision. While typical reactions of its usage range from nausea to dizziness, its abuse can lead to more intense reactions. The medications usage, in combination with other habit-forming substances, can sometimes lead to fatal consequences.\nPeople suffering from addiction or misusing the medication usually procure tramadol illegally from the unregulated markets and street vendors. Some common street names of the drug include Chill Pills, Trammies, and Ultras (a take on the generic brand).\nSince Tramadol is among the least potent of opioid painkillers, there is a common misconception that it does not lead to addiction. It is precisely this misinformation that has led many people on the path to addiction and substance abuse.\nIf a person is using this drug without obtaining a prescription, this is considered tramadol abuse. Similarly, those who are steadily increasing their dosage or continue to use it even after they are no longer prescribed it are also signs of a substance addiction. The same notion applies when using it in combination with other habit-forming drugs or substances.\nThat’s why identifying the signs of abuse at the earliest stage is essential to prevent the onset of addiction.\nThe Scope of Tramadol Abuse in America\nIn 2017, 47,600 people in the United States died from drug overdoses involving opioids.1\nIn 2017, 43,036 people in the United States died from unintentional drug overdoses involving opioids.2\nA total of 2,586 people died of drug overdoses of undetermined intent involving opioids.3\nIn 2016, an estimated 91,840 hospitalizations occurred for opioid-related poisonings in the United States.4\nPrescriptions for Tramadol increased 88 percent from 23.3 million in 2008 to 43.8 million in 2013.5\nIn 2011, Tramadol was ranked ninth in the list of narcotic pain relievers secured in law enforcement operations and analyzed by federal, state, and local forensic laboratories.6\nMore than 399,000 people died from overdoses involving any opioid, including prescription and illicit opioids from 1999-2017.7\nUnfortunately, and ironically, many people who faithfully follow their physicians’ prescriptions are at risk of suffering addiction.\nFrequent and prolonged usage of this drug has been known to lead to drug tolerance in many people. As a result, people must take larger and larger dosages to experience the drug’s effects. In addition to developing tolerance, Tramadol users can expect to suffer from withdrawal symptoms when use suddenly stops. Typically, withdrawal is known to cause symptoms such as irritability, depression, and flu-like symptoms.\nIf a person exhibits tolerance to the drug and also suffers from withdrawal symptoms when they stop using, they are most likely struggling with Tramadol dependence and addiction. Some of the telltale signs of substance addiction are frequent cravings, uninhibited use, and relationship problems that stem from drug use.\nDoctors usually prescribe Tramadol because of its relatively lower potential to cause addiction as compared to many of the other opioid painkillers. This point is also underlined by the fact that it is classified as a schedule IV substance, while the majority of opioid painkillers are listed under schedule II of the Controlled Substances Act.\nMore often than not, the feeling of calmness and euphoria that this drug generates in users is the main reason for its abuse. Those who abuse it usually do so in an attempt to feel unnaturally relaxed and carefree. People suffering from severe pain are also likely to take higher doses of the drug, thus exposing them to greater threats of dangerous side effects that could include seizures and respiratory depression.\nTramadol is a central nervous system (CNS) depressant and thus tends to effect lung and heart function negatively. Those who take very large doses of this drug (much higher than what would be prescribed) might stop breathing altogether and may experience a fatal overdose.\nSide Effects of Tramadol Abuse\nSome of the typical signs and side effects of Tramadol abuse are:\nVomiting coupled with nausea\nPinpoint pupils (pupils are very small)\nChanges in appetite\nTramadol abuse can potentially result in negative reactions of extreme nature, such as seizures. Seizures usually occur when larger dosages are consumed (averaging 400 mg or more per day), over extended time frames. Seizures also manifest more often when this medication and antidepressants are taken together.\nOverdose and Withdrawal\nSome of the symptoms associated with Tramadol overdose are:\nAbnormally low blood pressure\nSlow heart rate\nSweaty or clammy skin\nTramadol Withdrawal Symptoms\nOne of the problems linked to extended usage or high doses of Tramadol use is the possibility of suffering from withdrawal symptoms. When a person with tramadol dependence stops taking it, the body struggles to perform normally. As a result, the very effects that the drug was prescribed to heal might reappear.\nPsychological Symptoms of Tramadol Withdrawal include:\nPhysical symptoms of Tramadol Withdrawal include:\nEarly and Late Withdrawal Symptoms\nOpioid withdrawal is characterized by two main phases: early and late withdrawal. The early withdrawal phase starts when the drug leaves the bloodstream while the late withdrawal phase happens sometime later. Depending upon factors such as intensity and duration of addiction or substance abuse, opioid withdrawal symptoms vary according to the phase in which the patient is currently passing through.\nEarly Opioid Withdrawal Symptoms Include:\nLate Opioid Withdrawal Symptoms Include:\nThe onset of opioid withdrawal symptoms typically begins within about 12 hours of taking the last dose. According to figures released by the Drug Enforcement Administration (DEA), approximately 90% of people experience traditional withdrawal symptoms when stopping use. The remaining 10% might experience symptoms such as severe confusion, extreme paranoia, panic attacks, anxiety, hallucinations, and even tingling or numbness in their extremities.\nWhen people experience intense drug cravings, it is usually an indication that a detox or rehabilitation center would be beneficial as a first point of treatment. Tramadol detox gives the body a chance to regain its original healthy condition without suffering the discomforts associated with the withdrawal symptoms. Rehabilitation equips people with the strategies required to tackle chemical cravings and triggers that encourage the use of drugs.\nWhen administered under medical supervision in a safe and conducive environment, there are significantly higher chances of the patient successfully completing addiction therapy than without formal treatment.\n- Scholl L, Seth P, Kariisa M, Wilson N, Baldwin G. Drug and Opioid-Involved Overdose Deaths – United States, 2013-2017. WR Morb Mortal Wkly Rep. ePub: 21 December 2018"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:f1a92185-f0a4-444c-97c4-40cb2941ffba>","<urn:uuid:bbefd1de-f178-4e22-95cd-b00fd95e45a0>"],"error":null}