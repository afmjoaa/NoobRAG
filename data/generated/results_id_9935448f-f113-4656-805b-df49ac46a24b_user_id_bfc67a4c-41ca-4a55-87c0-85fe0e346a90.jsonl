{"question":"What are the recommended reading practices for 4th grade students comparing to Accelerated Reader guidelines - how much daily reading is advised?","answer":"While Accelerated Reader (AR) specifically recommends 20-45 minutes of daily reading for optimal progress, fourth grade reading practices are more focused on content type, with Common Core State Standards prescribing that 50% of reading material should be nonfiction. Both approaches emphasize structured reading development, with AR focusing on reading within a student's zone of proximal development (ZPD) for comprehension improvement, while fourth grade standards particularly emphasize compare and contrast skills across both fiction and nonfiction works.","context":["Fourth grade is a significant juncture for readers because the Common Core State Standards prescribes that 50% of reading material should be nonfiction. One of the critical skills on Common Core-aligned end of year assessments is compare and contrast. By the end of the fourth grade, students need experience in comparison for both fiction and nonfiction works. Practice in comparison not only improves a student’s close reading abilities, but also enables educators to gauge student comprehension and interpretation.\nIn honor of Parrots Over Puerto Rico winning the 2014 Robert F. Siebert Medal for the most distinguished informational book for children published in 2013, I am comparing Parrots Over Puerto Rico with Puffling Patrol. This book comparison is useful because the books tackle similar central ideas, yet have very different text structures and presentations of information.\nI have created sample questions to teach towards and check mastery of each of the three Common Core categories. These are by no means the only questions to ask in each category, but these provide an overview of the progression in question complexity and mastery of the texts.\nBy creating a range of compare and contrast questions across the standards, we are able to differentiate for students within a class, provide extension opportunities for ready learners, or move the whole class from literal- to higher-level thinking over the course of several lessons.\nParrots Over Puerto Rico (level: Q)\nPuffling Patrol (level: R)\nWhy: I have chosen these two texts because they share the same genre and examine similar subject matter (endangered bird species) that pushes students beyond their personal experiences, while presenting the information in a different format, point of view, and structure.\nKey Ideas and Details:\n- What causes Puerto Rican parrots and Westman Islands puffins to become endangered? What role do humans play in the declining numbers of each species?\n- How are the scientists and volunteers in both books similar or different? What actions does each team take to protect the parrots or puffins? How does each of their actions impact the success of protecting these endangered species? How do both groups demonstrate creativity and persistence? Study how the authors each describe the actions of the scientists and volunteers. Do the authors want you to aspire to be like these scientists or not? What makes you think so?\n- Compare the food webs of the pufflings and parrots. What do their diets consist of and what predators do they have? What in their food webs affects their chances of survival? How does each book explore the interdependence of nature?\n- Compare the environments of the two species. What is the geography of the Westman Islands and Puerto Rico? How does each book explore the impact of geography and climate on the pufflings’ and parrots’ survival?\n- Compare the purpose, or mission, of the Puffling Patrol and the Puerto Rican Parrot Recovery Program. How are their approaches to protecting the two species similar or different? What unique challenges does each program have? Compare the relative success of their approaches.\n- Look back in the text for evidence of the parrots’ and pufflings’ behavioral and physical adaptations. What do they need to be able to do in order to survive in their respective environments? How is each species physically adapted to survive in its unique habitat? What adaptations is each group of scientists focusing on to improve each species’ chance of survival?\n- What is the central idea(s) in both books? How are these central ideas similar or different from each other?\n- What role do the authors of Puffling Patrol and Parrots Over Puerto Rico want humans to play in their environment? What evidence in each book demonstrates the authors’ views on sustainability?\nCraft and Structure:\n- How is the information in Parrots Over Puerto Rico arranged? What evidence do you have that each book is organized by chronology, comparison, description, classification, cause/effect, or problem/solution? How is the information in Puffling Patrol presented similarly or differently to Parrots Over Puerto Rico?\n- How does each book use primary and secondary source information? Where did each book get its information? How do you know what part is primary and what part is secondary source information?\n- Why would the authors of Puffling Patrol choose to convey information through a narrative style? What effect does that have on readers about the subject? How is this presentation of facts different from Parrots Over Puerto Rico? What effect does chronology/sequence of events have on presenting information in Parrots Over Puerto Rico?\n- Betsy and Ted Lewin of Puffling Patrol choose to narrate the story themselves and name the children who are active in the story. In contrast, Susan L. Roth and Cindy Trumbore, the authors of Parrots Over Puerto Rico, choose NOT to name the scientists. Why might Parrot Over Puerto Rico authors want to withhold the identity of the scientists? How might this choice change what you focus on as you read? What does this choice suggest about the authors’ perspective?\n- What evidence do you have that these are nonfiction books? What text features does each book have? What subgenre of nonfiction is each book?\n- Why might the authors of Puffling Patrol choose to name the book about the program protecting the pufflings, while the authors of Parrots Over Puerto Rico choose to title the books about the parrots, not the program? What does this suggest about the focus and central ideas of each book? What does this choice demonstrate about the authors’ perspectives?\nIntegration of Knowledge and Ideas:\n- At the end of each book, there is additional background information. How do these text features contribute to the central ideas of each book? How do they extend understanding on the subject matter?\n- Read a magazine excerpt from the Audubon Magazine about another endangered bird species, the Florida grasshopper. Compare the central ideas, purposes, text features, and text structure of these three texts.\n- The authors of Parrots Over Puerto Rico and Puffling Patrol both chose NOT to use photographs for their nonfiction books. Instead, illustrators created images with paint or collage. What does this suggest about the focus and central ideas of each book? What does this choice demonstrate about the authors’ perspectives?\n- Both books present cases of humans saving an endangered (bird) species. If you were to write a report on another endangered species, what central ideas, text features, and characteristics should the report include? Why?\nWhat have you found successful in teaching how to compare and contrast? Share with us at email@example.com!\nFurther reading on teaching literacy in Fourth Grade:\n- Using Dual Language And Bilingual Books In Third And Fourth Grade\n- What Does Close Reading Look Like In Fourth Grade?\n- Integrating Reading, Writing, Speaking & Listening In Grades 4-5\nFurther reading on Compare and Contrast series:\n- How To Compare And Contrast With The Common Core In Kindergarten\n- How To Compare And Contrast With The Common Core In Second Grade\n- How To Compare And Contrast With The Common Core In Third Grade\n- How To Compare And Contrast With The Common Core In Fifth Grade\nJill Eisenberg, our Resident Literacy Expert, began her career teaching English as a Foreign Language to second through sixth graders in Yilan, Taiwan as a Fulbright Fellow. She went on to become a literacy teacher for third grade in San Jose, CA as a Teach for America corps member. She is certified in Project Glad instruction to promote English language acquisition and academic achievement. In her column she offers teaching and literacy tips for educators.","Accelerated Reader (AR) is a reading program designed specifically to enhance and develop students reading skills. AR works by assessing the students reading age and producing their zone of proximal development (ZPD). The ZPD allows students to read books at their optimal learning level and does this by offering each student their personalised book levels. As long as a student is reading a book within this range, their ZPD, they will make significant progress towards developing a stronger level of comprehension and fluency in what they are reading.\nFor more information, Renaissance Learning who run AR, have a fantastic website: www.renlearn.co.uk\n- How are reading ages assessed?\nEach student takes an online reading assessment which consists of 34 multiple choice questions. Each answer given dictates the level of the next question which results in a very specific and tailored reading age for every student. The questions assess their ability to de-code words and their level of comprehension.\n- What are book levels and why do the students have to choose books within their levels?\nEach reading assessment that a student completes gives us their zone of proximal development (ZPD). This is the level in which they will make most progress. The book levels issued are specific to each student’s results, and by reading within these levels they will make the most effective progress. Each student has a book level range, for example, 3.5 – 5.5. Books at the lower end of the range should be comfortable for the student to read and they should achieve 100% on these books early on. As they move up through the levels the books will become more challenging for them and may take longer to read. A book at the top of their range should not be attempted until they have shown competency throughout the lower levels. Students may be able to de-code words in higher level books, however these will be challenging comprehensively and will slow progression.\n- How often should a student read their AR book?\nAR recommends that students read their AR book between 20-45 minutes a day to ensure they make significant progress.\n- What are book quizzes?\nEach book has a points value attached. These are formulated by AR and are based on the difficulty of the book, for example, sentence length and level of vocabulary used. Longer books are not necessarily worth more points! When a student finishes their book, they log onto their AR account and take a short quiz assessing their level of understanding of the book. The base pass rate is 60%. 80% is the optimum score for showing that a student has adequately understood the book while being suitably challenged. If a student is regularly achieving 100% they will need to be choosing books higher up their levels to continue being challenged.\n- How are the targets set and measured?\nAfter each reading assessment, AR automatically generates a target for each student. This is a points target based on the points value of each book. The target is set using the students ZPD, the number of weeks in the term and the requirement of the student reading for 20 minutes each day.\n- To achieve points towards their target, a student must first pass their book quiz by achieving 60% or more. If they achieve 100% on a book quiz, they receive the full points value for that book. This is then proportioned down to 60%. For example, if a book is worth 1pt they would achieve the following:\n- 100% pass rate = 1pt\n- 80% pass rate = 0.8pt\n- 60% pass arte = 0.6pt\n- These points are added to the students target automatically and will calculate what percentage of their target they have achieved overall.\n- Can I read a book from home?\nYes! AR is a national program and is not limited to the books in our library. If you have a book at home you would like to read for AR, please bring it in and I will check the level and points value. If the book is not in your level, please continue to read it at home secondary to reading your AR book for 20 minutes each day.\nBook levels can be checked here: http://www.arbookfind.co.uk\nThe list below is split into interest levels (MY/UY) with the book levels in brackets.\nInterest Level: Middle/Middle Years Plus (MY/MY+)\nDon’t Do It! (2.5) – Helen Orme\nPoint Blanc: The Graphic Novel (2.6) – Anthony Johnston\nNight of the Scrawler (2.7) – Michael Dahl\nDeath Match (3.0) – Andy Croft\nSkeleton Key: The Graphic Novel (3.0) – Anthony Johnston\nGoosebumps (3.2 – 4.6) – R.L. Stine\nA Greyhound of a Girl (3.3) – Roddy Doyle\nFootball Academy series (3.4 – 3.7) – Tom Palmer\nTom Gates series (3.8 – 4.4) – L Pichon\nThe 13-Storey Treehouse (3.9) Andy Griffiths (part of the Treehouse series)\nThe Private Blog of Joe Cowley (4.1)/Return of the Geek (4.2) – Ben Davis\nThe Crossover (4.3) – Kwame Alexander\nThe Midnight Gang (4.4) – David Walliams\nThe Unforgotten Coat (4.4) – Frank Cottrell Boyce\nRiver of Ink: Genesis (4.5) – Helen Dennis\nCandyfloss (4.5) – Jacqueline Wilson\nWonder (4.8) – R.J Palacio\nDropzone (4.8) – Andy McNab\nShadow (4.9) – Michael Morpurgo\nMatilda (5.0) – Roald Dahl\nScorpia (5.0) – Anthony Horowitz (part of the Alex Rider series)\nSands of the Scorpion (5.1) – Bear Grylls\nSwallows and Amazons (5.1) – Arthur Ransom\nPrivate Peaceful (5.2) – Michael Morpurgo\nGrandpa’s Great Escape (5.2) – David Walliams\nThe Hunger Games series (5.3) – Suzanne Collins\nThe Chronicles of Narnia (5.4 – 5.9) – C.S Lewis\nJourney to the River Sea (5.6) – Eva Ibbotson\nSurvival (5.7) – Chris Ryan (part of the Alpha Force Series)\nHow to Train Your Dragon series (5.7 – 6.9) – Cressida Cowell\nMurder Most Unladylike (5.8) – Robin Stevens\nI Am Malala (5.9) – Malala Yousafzai\nThe Victory Dogs (6.0) – Megan Rix\nMary Poppins (6.1) – P.L Travers\nGoth Girls Series (6.5 – 6.6) – Chris Riddell\nThe Escape (6.6) – Robert Muchamore (part of the Henderson Boys series)\nA Christmas Carol (6.7) – Charles Dickens\nLittle Women (7.5) – Louisa May Alcott\nPeter Pan (7.7) – James M Barrie\nInterest Level: Upper Years (UY)\nThe Recruit (4.7) – Robert Muchamore (part of the Cherub series)\nThere Will Be Lies (4.8) – Nick Lake\nThe Divergent Series (4.8 – 5.7) – Veronica Roth\nOne (5.2) – Sarah Crossan (Carnegie Prize winner 2016)\nThe Fault in our Stars (5.5) – John Green\nAbout a Boy (5.5) – Nick Hornby\nTo Kill a Mockingbird (5.6) – Harper Lee\nThe Boy in the Striped Pyjamas (5.8) – John Boyne\nI know why the Caged Bird Sings (6.7) – Maya Angelou (Autobiography)\nThe Great Gatsby (7.3) – F Scott Fitzgerald\nJane Eyre (7.9) – Charlotte Bronte\n1984 (8.9) – George Orwell\nGreat Expectations (9.2) – Charles Dickens\nA Tale of Two Cities (9.7) – Charles Dickens\nPride and Prejudice (12.0) – Jane Austen\nBlue Peter Book Awards\nRooftoppers (3.5) – Katherine Rundell – Winner 2014\nTom Gates Genius Ideas (mostly) (4.0) – Liz Pichon – Winner 2013\nThe Astounding Broccoli Boy (4.5) – Frank Cottrell Boyce – Runner up 2016\nThe Nowhere Emporium (5.0) – Ross MacKenzie – Winner 2016\nThe Spy who Loved School Dinners (5.3) – Pamela Butchart – Winner 2015\nThe Boy who Sailed an Ocean in an Armchair (5.4) – Lara Williamson – Runner up 2016\nFor further ideas on books to read, please visit www.booktrust.org.uk which has an array of resources and book ideas for teens and young adults."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:efaca4b5-9756-4349-afff-43d3d72e0b93>","<urn:uuid:78e37354-f383-44a7-b3d0-83a59fcc330d>"],"error":null}
{"question":"What are the legal risks for employers in garnishment cases, and how do major companies like Amazon and UPS address delivery vehicle emissions compliance?","answer":"In garnishment cases, employers face significant legal risks if they don't handle withholdings properly. They can become liable for the employee's entire debt if they fail to follow garnishment procedures correctly, as demonstrated in the Premiere Property Services case where True North Painting became liable after incorrectly paying funds to a defendant. Regarding emissions compliance, major delivery companies like Amazon and UPS are addressing this through EV adoption. They're making significant investments in electric vehicles, with Amazon ordering 100,000 electric delivery trucks and UPS ordering 10,000 electric trucks, partly to comply with anti-idling laws in cities like New York and Philadelphia where companies can be fined for idling delivery vehicles.","context":["The Michigan Court of Appeals has published another employment related case, this time about the whacky world of garnishments. I say whacky because an employer can incur the employee’s debt if the garnishment withholding is not handled properly.\nThat’s what happened in Premiere Property Services, Inc v Crater. Let’s take a look so that you don’t make the same mistake.\nThe plaintiff, Premiere Property Services, served a “writ of periodic garnishment” on garnishee defendant True North Painting in order to satisfy a judgment it had obtained against the defendants, Matthew Crater, Fresh Outlook Painting and Better Brush Painting.\nBefore we get too deep into the case details, let’s take a step back and see how this is supposed to work for those who aren’t familiar with the process.\nPremiere Property would have filed a previous lawsuit against Matthew Crater, Fresh Outlook and Better Brush and succeeded in obtaining a judgment in that litigation. But obtaining a judgment is not the end of the story. The plaintiff must collect on the judgment. Sometimes a defendant will just pay the plaintiff the amount of the judgment (or negotiate a lower amount so that the plaintiff doesn’t have to jump hurdles or defend the judgment on an appeal, etc.).\nBut, if the defendant does not pay, the plaintiff takes action such as placing a lien on property or garnishing the earnings of the defendant. That’s how “garnishee defendant” True North Painting became involved, and it did not end well for the company.\nThe process is described in the Michigan Court Rules, Subchapter 3.100 and is regulated by statute, MCL 600.4001, et seq. But, generally, a “periodic” garnishment is required when the total judgment cannot be collected all at once. This is often the case when the garnishment is against an employee’s paycheck because a formula limits the amount employers can withhold from each paycheck. Let’s follow the employer/employee example to keep it simple.\nWhen served with a writ of garnishment, the garnishee (employer) is required to make certain disclosures about the amounts it owes (wages) to the original defendant (employee). Based on the disclosures and the formula, the amount of the periodic garnishment (money to be withheld from the employee’s paycheck) is established and paid “periodically” by the employer to the original plaintiff. But, if the garnishee/employer fails to follow the process correctly, it (instead of the employee) can end up owing the amount of the judgment to the plaintiff.\nLet’s go back to our case and pick up where plaintiff Premiere served garnishee True North Painting with the writ of garnishment. True North acknowledged in its disclosures that the three defendants, Crater, Fresh Outlook and Better Brush, were its subcontractors and that it owed money to them.\nHowever, instead of withholding the full amount of the garnishment, it only withheld 25% of it ($7,610.62) for the plaintiff and paid the remaining 75% ($22,746.64) directly to Crater. Apparently, True North was under the mistaken belief that the amount was payable to Crater as “wages” owed to him as an employee. Crater then filed for bankruptcy. This placed a stay on the proceedings.\nSadly, several trees likely died as a result of the court filings that followed. Because the other two defendants did not file for bankruptcy, Premiere argued that it could still garnish the $22,746.64 from True North to Better Brush.\nIn the end, the lower court ruled that it would be inappropriate to order True North to pay plaintiff the $22,746.64 it had already paid to Crater since it could seek payment directly from Crater in the bankruptcy court (at pennies on the dollar).\nThe Michigan Court of Appeals saw things differently. Ignoring the bankruptcy, it found that True North was liable because it had disregarded the writ of garnishment by paying Crater. A writ of periodic garnishment continues until the original judgment is satisfied.\nA garnishee is liable for the amount of the garnishment to the extent it owes a debt to the defendant(s). The lower court found that once Crater had been paid, there was no further debt owed to the defendants and, therefore, nothing from which True North could withhold. But this is the same as saying that “a garnishee who violates a writ of garnishment by making payment directly to the defendant cannot be held liable because it is no longer in possession of an obligation owed to the defendant” and is erroneous circular reasoning.\n“Garnishees’ duties and obligations under the rules and their potential liability to the plaintiffs attach at the time they are properly served the writ. They then become responsible for the timely performance of the specific duties imposed…at the risk of default judgment against them which may be executed against their own funds or property…” Chayka v Brown, 92 Mich App 360 (1979).\nWhile it’s true that a garnishee’s liability is determined by the obligation it owes to the defendant (often the paycheck, but in this case the contract amount), the lower court was simply wrong by concluding that a garnishee can “escape all liability by turning over property and paying obligations to the defendant in violation of the writ.” The appellate court said this would be an “absurd” result.\nThe fact that True North had made a good faith mistake and failed to accurately understand it legal responsibilities does not deny plaintiff its recovery. “Allowing a garnishee to transfer garnished funds to a defendant without risk of liability could greatly frustrate” a key mechanism for prevailing parties to enforce their judgments.\nWhile the bankruptcy applied to the garnishment directed at True North as against Crater, Better Brush and Fresh Outlook were not in bankruptcy. Thus, the case was remanded to the lower court to determine whether the payments made to Crater were actually owed to Better Brush or Fresh Outlook and, therefore, unaffected by the bankruptcy.\nYou can see that an “inadvertent noncompliance” with the writ of garnishment can be a costly error. It is crucial to get it right!\n- Of Counsel\nAn of counsel attorney in the firm’s Detroit office, Claudia D. Orr exclusively represents and advises employers and management in employment and labor law matters.\nMs. Orr has an ever-growing practice in Alternative Dispute ...\nAdd a comment\n- Employment Liability\n- Labor Law\n- Employment Discrimination\n- Human Resources\n- Equal Employment Opportunity Commission (EEOC)\n- Wage & Hour\n- Employment Agreement\n- Department of Labor (DOL)\n- Fair Labor Standards Act (FLSA)\n- Regulatory Law\n- OSHA Issues\n- Department of Justice\n- Family Medical Leave Act (FMLA)\n- Americans With Disabilities Act (ADA)\n- Hostile Work Environment\n- National Labor Relations Act\n- Title VII\n- Business Risk Management\n- Noncompete Agreements\n- Sick Leave\n- Medicare Issues\n- Workplace Harassment\n- Minimum Wage\n- National Labor Relations Board\n- Paid Medical Leave Act (PMLA)\n- Workers' Compensation\n- Transgender Issues\n- Unemployment Benefits\n- Whistleblower Protection Act\n- Sexual Harassment\n- Civil Rights\n- Social Media\n- Class Actions\n- Retail Liability\n- Emergency Information\n- Uniformed Services Employment and Reemployment Rights Act (USERRA)\n- Department of Education (DOE)\n- Tax Law\n- Title IX\n- Medical Marijuana\n- Right to Work\n- Health Insurance Portability and Accountability Act (HIPAA)\n- Union Organizing & Relations\n- DOJ Issues Guidance on ADA, Opioid Crisis Issues\n- Congress Passes Law, With Retroactive Effect, to Invalidate Forced Arbitration Provisions at the Employee’s Election\n- U.S. Supreme Court Temporarily Blocks Implementation of Vaccine Requirement for Large Businesses\n- Contractual Limitations Periods STILL Alive and Well... on job Applications!\n- Federal Appellate Court Takes Brakes Off COVID-19 Vaccine Mandate Carousel\n- Employers: Hang on for Another Spin Around the Vaccine Mandate Carousel\n- Stray Comments can Lead to Employer Liability\n- President Biden's COVID-19 Vaccine Mandates Face Uncertain Future\n- Employment Noncompete Agreements Enforced Badly\n- Feds Release New COVID-19 Vaccination Rules for Large Employers, Medicare/Medicaid Providers","Electric vehicles (EVs) hold a lot of promise for the private sector — especially as consumers, who are increasingly aware of the relationship between emissions and climate change, are starting to demand eco-friendly delivery options. EV adoption, however, has been slowed down by a few different challenges — the US’s poor EV charging infrastructure in particular.\nNow, however, we’re beginning to see signs that major businesses are willing to buy into EVs, despite potential road bumps.\nHere are the businesses that are leading the way when it comes to EV adoption.\nAmazon and UPS Lead Way on EV Adoption\nTwo delivery giants — Amazon and UPS — have begun to aggressively add EVs to their delivery fleets.\nEarlier this year in January, Amazon ordered 100,000 electric delivery trucks from EV manufacturer Rivian, as well as 10,000 electric delivery rickshaws for their operations in India. Then, around the end of the month, UPS announced that it had ordered 10,000 electric trucks from the UK-based manufacturer Arrival Ltd., and would soon be teaming up with self-driving car manufacturer Waymo for a pilot test of self-driving delivery vehicles.\nThe moves are part of broader pushes towards carbon neutrality and self-driving delivery by the two companies. Last year, Amazon announced the company’s plan to be 100 percent carbon-neutral by the year 2040. UPS already offers carbon-neutral and carbon-offset delivery options.\nThe moves also come as more cities around the U.S., including New York and Philadelphia., have begun to adopt anti-idling laws that allow the city to fine companies over idling delivery vehicles.\nSome cities have even developed apps that allow citizens to report idling vehicles based on that vehicle’s DOT number — making these policies even more costly for delivery companies. Because electric vehicles produce no emissions, they’re typically free from being fined — meaning savings for businesses that adopt EVs for city deliveries.\nThe announcements are both historic. While other companies have announced EV purchases — like Lyft, which plans to deploy 200 EVs in Denver as part of its rental vehicle program there — there’s been nothing near scale of these announced by Amazon and UPS.\nWhile neither UPS nor Amazon has plans to go fully electric any time soon, the purchases are a welcome sign for the EV industry. Coupled with similar positive signals from the individual consumer side of the industry, they likely demonstrate that despite early growing pains, EVs may be on track for widespread adoption in the near future.\nChallenges Facing Further EV Adoption\nHowever, there still remain significant barriers that may slow or prevent full EV adoption, primarily the weak EV charging infrastructure in the US and limited number of charging stations — although this, too, seems like it’s starting to change.\nChargePoint, in coalition with the National Association of Truck Stop Operators (NATSO) has formed the National Highway Charging Collaborative, which plans to install new charging stations at more than 4,000 highway-side locations in the U.S., in order to increase the availability of EV charging stations in rural areas.\nAt the same time, legislative support for stronger EV infrastructure is beginning to build. In February, Democratic lawmakers in the House of Representatives announced a new bill that would create a nationwide EV charging network within the next five years.\nUpgrades to existing infrastructure would likely encourage further adoption. They may also be especially beneficial for businesses like Amazon and UPS, as both companies regularly make deliveries to rural parts of the country — areas that don’t always have the charging infrastructure needed to support EVs.\nThe Future for EVs in Business\nEV adoption in the private sector, which has lagged in the past, seems to be accelerating. Two major delivery companies have now announced that they will be adding significant numbers of EVs to their delivery fleets, with more likely to come in the near future as both pursue low-carbon delivery options.\nWhile challenges remain that may slow down EV adoption — primarily the nation’s weak EV charging infrastructure — the purchases are likely a good sign for the industry and the future of EVs in the private sector."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:88839e0c-cf48-48b3-b2ca-f73a7f573fa2>","<urn:uuid:a3b2914f-bfcd-4107-a76d-db9ca5d880cd>"],"error":null}
{"question":"What are the cultural celebrations in Asia that involve lanterns, and how do tourism authorities plan to prevent visitor misconduct at these festivals?","answer":"In Asia, there are notable lantern-based celebrations including the Lantern Festival in Taiwan's Pingxi District where thousands of sky lanterns light up the sky, and Vietnam's Trung Thu festival where children dance with star lanterns made of bamboo and plastic that represent the moon. Regarding visitor conduct at such festivals, tourism research suggests that misbehavior can be reduced by increasing psychological closeness between tourists and locals. Destination marketers are advised to highlight similarities between visitors and local residents, promote a welcoming image, and remind tourists about the negative consequences of misconduct. Creating opportunities for healthy interactions between tourists and locals has been shown to improve visitor behavior.","context":["1. Carnival- Brazil\nBrazil’s most popular and festive holiday is Carnival. In fact, many people consider Carnival one of the world’s biggest celebrations. Each spring, the streets of Brazil’s largest city, Rio de Janeiro, come alive with wild parties, festivals and glamorous balls.\n2. Chinese New Year- Hong Kong\nChinese New Year is the longest and most important celebration in the Chinese calendar. During Chinese New Year celebrations, people wear red clothes, give children “lucky money” in red envelopes and set off firecrackers.\n3. Oktoberfest- Germany\nEvery year in Munich, people drink German beer and eat lots of Pretzeln (a special cake), weiss wurst bratwurst (white sausage), and sauerkraut. Oktoberfest is one of the most famous events in Germany and is the world’s largest fair, with more than 5 million people attending every year.\n4. Lantern Festival - Taiwan\nDuring the Lantern Festival, thousands of sky lanterns light over Pingxi District in Taiwan. In Yanshui District, the firecrackers ceremony of Wumiao Temple is also one of the important activities. The Tainan Yanshui Fireworks Display (“beehive of fireworks”) was originally celebrated to ward of evil and disease from the town. The Taipei Pingshi Sky Lanterns were released originally to let others know that the town was safe.\n5. Dia de los Muertos - Mexico\nDia de los Muertos is celebrated on November 1 in Mexico, Ecuador, Guatemala and other parts of Central and South America. People decorate altars in their homes and grave sites with food, candles, candy skulls and marigolds to welcome the souls back to earth.\n6. Hina Matsuri- Japan\nEach year, Japanese girls eagerly await the third of March, called Hina Matsuri, or Doll’s Festival. Girls display their most precious dolls on a seven-tiered platform in their home.\n7. Diwali- India\nDiwali, the Hindu festival of lights, is the best known of Hindu celebrations and certainly the brightest. Families get together and celebrate with gifts and feasts. Many families decorate their homes with flowers.\n8. Trung Thu- Vietnam\nTrung Thu is a mid-fall festival commemorating the moon at its brightest and most beautiful. The children wear colorful masks and dance in the streets with star lanterns that are illuminated by candles. The lanterns, which are made out of bamboo and plastic, represent the moon. Children also feast on moon cakes. Shaped like fish or flowers, the sweet cakes are filled with sugar and meat or eggs.\n9. Esala Perahera- Sri Lanka\nEvery July or August, thousands of Sri Lankans travel to the hill city of Kandy to watch dancers, acrobats, drummers, whip crackers, flame throwers and more than 100 elegantly decorated elephants parade through the streets during Esala Perahera. This is a 10-day festival in honor of the country’s most prized possession, the Tooth Relic of Lord Buddha.\n10. Çocuk Bayrami- Turkey\nOn April 23, Turkey celebrates Çocuk Bayrami. Turkish leader Mustafa Kemal Atatürk declared the holiday in 1920, as Turkey was becoming an independent nation after the fall of the Ottoman Empire. Every year on this day, children all over Turkey dress up in special outfits or the national costume for Çocuk Bayrami.","CUHK Business School Research Looks at How Tourism Destinations Can Reduce Visitor Misbehaviour\nHONG KONG SAR - Media OutReach - 25 March 2021 - It's been a bad year for tourism and a boom in the travel and hospitality sector would no doubt be good for the economy. However, it could also be a nightmare for people living and working amid the flux of transient tourists. It wasn't that long ago that we saw media reports of tourist misbehaviour from carving names on ancient Egyptian bas reliefs to chasing geishas down the streets in Kyoto. Around the world, countries are looking for ways to curb bad behaviour from tourists. Iceland, for instance, launched the Inspired by Iceland pledge in June 2017 that urges tourists to travel responsibly. Apart from implementing official rules, a recent research study reveals that poor tourist behaviour could be reduced by simply making the tourists feel closer to the locals.\nTo illustrate the magnitude of opportunity, international tourist arrivals fell by 72 percent in January to October over the same period in 2019, translating into a loss of US$935 billion – more than 10 times the loss the industry suffered during the 2009 global financial crisis. Hotels and the tourism industry are hoping for \"revenge travel\" to take place, a riff on the concept of \"revenge spending\" that describes shopping starved consumers overcompensating by splurging when the pandemic is over. But how can places that seek to cash in on the return of tourist dollars do so while avoiding a return to the old days of bad tourist behaviour?\nThis is the subject of a paper titled Tourist Misbehaviour: Psychological Closeness to Fellow Consumer and Informal Social Control. The study is the first to examine the psychological closeness between locals and tourists and how this \"psychological distance\" affects tourists' intention to misbehave. It was conducted by Lisa Wan, Associate Professor at the School of Hotel and Tourism Management and Department of Marketing at The Chinese University of Hong Kong (CUHK) Business School and her co-authors Prof. Michael Hui at University of Macau and Yao Qiu at CUHK Business School.\nPsychological closeness refers to one's feelings of attachment and connection towards other people. We tend to feel psychologically closer to someone whom we perceive as \"one of us\". People who belong to the same social group are likely to follow the same set of social or cultural norms because they care about the consequences of their behaviour due to the need for approval and belongingness. In-group members are also likely to impose informal social control on fellow members, such as showing angry looks or sharing comments on those who violate the social norms to reinforce the order of the group. However, the same social practice does not usually apply to out-group members.\nIn the tourism context, Prof. Wan and her co-authors explain that when people are travelling abroad, they feel less connected to locals than they would at home. In other words, tourists feel psychologically distant from locals. Since they do not perceive themselves as belong to the same \"group\", it is less likely for them to comply with local norms because they are less concerned about the consequences of their behaviour and they do not expect their actions to invoke informal social control from locals. Locals, on the other hand, consider tourists as outsiders and therefore may not always voice out their disapproval of tourist bad behaviour.\nIn addition, the more local residents suffer directly or indirectly from the bad behaviour of the tourists, the higher the likelihood they would impose informal social control on tourists who are violating the social norms. This is emphasised when locals feel victimised or responsible for protecting the interests of their community. For example, local residents in Sai Kung – a district in Hong Kong, protested after getting upset at the influx of tourists during the pandemic and the mounting garbage problem left behind. The researchers predict that tourists would have a higher intention to misbehave when they think their behaviours are less likely to affect local residents.\n\"It is essential for cities and destination organisations that rely heavily on tourism sector but suffer from the influx of tourists to grasp the underlying reasons or even the motivations for tourists' misbehaviour,\" Prof. Wan says.\nThe Importance of Psychological Distance\nProf. Wan and her co-authors conducted three experiments to test their hypotheses. In the first study, the participants – recruited from a local Hong Kong university, were asked to imagine themselves either as tourists in Bangkok or as local residents in Hong Kong. They were asked if they would eat on a public bus, something that is banned in many cities in the world (including Hong Kong). The results show that those who imagined themselves as tourists in Bangkok reported a higher intention of eating on the bus because they feel less close to fellow passengers on the bus and consequently, they perceive a significantly lower likelihood of disapproval from other passengers.\nA second study was conducted to test whether the degree to which local residents would suffer as a result of tourist misbehaviour could contribute to how likely this said misbehaviour would occur. A total of 200 American participants were asked to imagine themselves at a supermarket checkout before catching a train. They were surveyed on whether they would misuse the express checkout service (by having more items than the supermarket's limit for the lane) in order to catch the train, which they were about to be late for.\nAccording to the results, the participants who imagined themselves as tourists showed a higher intention to use the express checkout lane when they thought their actions would not significantly affect other customers at the supermarket. This shows that tourists would indeed consider how deeply it would affect local residents before committing misbehaviour.\nIn the third experiment, the researchers examined the notion of excitement and its relation to tourists' misbehaviour. The researchers theorised that excitement may lead to misjudgement on whether their behaviours are disapproved by locals. Over 200 students and staff members from a Hong Kong university were recruited for the study.\nThe participants were randomly told that they were tourists in Tokyo, tourists in Bangkok or non-tourists. They were asked whether they would occupy a priority seat on public transport. The results reveal that when the participants believe their actions were less likely to affect other passengers, those who imagined themselves as tourists in Bangkok were more likely to misbehave than those who imagined themselves as tourists in Tokyo. In addition, the participants who imagined themselves as tourists in Bangkok also perceive a significantly lower likelihood of disapproval from fellow passengers than they were tourists in Tokyo due to the difference in psychological distance.\nProf. Wan explains that since Hong Kong is known to be the \"most Japanese City\" outside Japan, it is understandable that Hong Kong people feel psychologically closer to locals in Tokyo than locals in Bangkok. Furthermore, the findings rule out the excitement factor because if it was indeed associated with tourist misbehaviour, then a higher intention to misbehave should be reported by both groups regardless of their travel destinations.\nImplications for Destination Marketers\nAs the study results show that tourists are less likely to misbehave if they believe their bad behaviours will cause significant harm to other consumers, the researchers suggest marketers and authorities in the tourism industry to remind tourists about the negative consequences of even seemingly minor misbehaviour and for tourists to constantly put themselves in the shoes of the locals. For example, hotel managers could design table decorations that remind tourists about the bad consequences of wasting food and the unnecessary burden on locals due to their wasteful behaviour.\n\"At the end of the day, the best advice is to remind visitors to 'do unto others as you would have them do unto you',\" says Prof. Wan. \"They need to know that their actions will cause the same level of nuisance and provoke a similar level of disapproval if they were done back home.\"\nOn the other hand, destination marketers should also remind the tourists that bad conduct may tarnish the image of their home country. Above all, marketers should try to increase the psychological closeness between tourists and local residents by highlighting their similarities, such as promoting a friendly and welcoming image of the destination and the local residents, thereby creating opportunities for healthy interactions between tourists and locals.\n\"For instance, Koreans and Americans both love eating friend chicken and drinking beer. Perhaps destination marketers can think of some promotional materials based on such similarity,\" says Prof. Wan. \"However, the easiest way to make their guests feel at home is always a warm smile.\"\nLisa C. Wan, Michael K. Hui and Yao (Chloe) Qiu (2021). \"Tourist misbehavior: Psychological closeness to fellow consumers and informal social control.\" Tourism Management, Volume 83, 2021, Article 104246. https://doi.org/10.1016/j.tourman.2020.104258\nThis article was first published in the China Business Knowledge (CBK) website by CUHK Business School: https://bit.ly/3tHsPDB.\nAbout CUHK Business School\nCUHK Business School comprises two schools – Accountancy and Hotel and Tourism Management – and four departments – Decision Sciences and Managerial Economics, Finance, Management and Marketing. Established in Hong Kong in 1963, it is the first business school to offer BBA, MBA and Executive MBA programmes in the region. Today, the School offers 10 undergraduate programmes and 18 graduate programmes including MBA, EMBA, Master, MSc, MPhil and Ph.D.\nIn the Financial Times Global MBA Ranking 2021, CUHK MBA is ranked 48th. In FT's 2020 Executive MBA ranking, CUHK EMBA is ranked 15th in the world. CUHK Business School has the largest number of business alumni (40,000+) among universities/business schools in Hong Kong – many of whom are key business leaders. The School currently has more than 4,800 undergraduate and postgraduate students and Professor Lin Zhou is the Dean of CUHK Business School.\nMore information is available at http://www.bschool.cuhk.edu.hk or by connecting with CUHK Business School on:"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:e148c025-3506-42f1-a92b-1b52c783b02f>","<urn:uuid:158f0163-1eaf-47c7-a3b3-15a91e2f342e>"],"error":null}
{"question":"What are Theodore von Kármán's major contributions to rocket technology development, and how do environmental changes affect NATO's military operations today?","answer":"Von Kármán made several pivotal contributions to rocket technology: he demonstrated the first stable, long-duration solid-propellant rocket engine since the invention of black powder rockets in China, developed the prototype for the jet-assisted takeoff (JATO) rocket that became the basis for modern long-range missiles, and developed spontaneously igniting liquid propellants used in Apollo missions. As for environmental impacts on NATO operations, they affect military forces in multiple ways: coastal installations become vulnerable to rising sea levels, floods and extreme temperatures impact military exercises and supply chains, and forces must increasingly conduct humanitarian assistance and disaster relief missions. Additionally, climatic changes are opening up new operational environments like the Arctic where military forces must adapt to challenging conditions.","context":["Theodore von Kármán\nTheodore von Kármán (1881-1963) was a Hungarian-American engineer and scientist, best known for his pioneering work in the use of mathematics and the basic sciences in aeronautics and astronautics. He served as the first Director of GALCIT from 1930-1949 and the first director of JPL from 1938-1944.\nWritten By: Frank J. Malina\nVon Kármán was the third of five children of Maurice and Helene von Kármán. His father, a professor at the University of Budapest and commissioner of the Ministry of Education, reformed the Hungarian secondary-school system and founded the Minta (Model) Gymnasium, which his son attended, as did the atomic physicists George de Hevesy and Leo Szilard. Von Kármán showed a natural mathematical facility at an early age and was well on his way to becoming a child prodigy when his father, fearing that he would become a mathematical freak, guided him toward engineering.\nOn completing his undergraduate studies in 1902 at the Royal Polytechnic University in Budapest, he decided to pursue his engineering career in the academic world, which would enable him to fulfill his wide scientific interests and to practice the art of teaching, which his father had inspired in him. In later years, he was delighted when engineers to whom he had imparted his scientific attitude and methodological approach acknowledged him as their teacher.\nBetween 1903 and 1906 he served on the faculty of the Polytechnic University and as consultant to the principal Hungarian engine manufacturer. The research that von Kármán conducted on the strength of materials prepared the way for important later contributions to the design of aircraft structures. He was awarded a two-year fellowship to the University of Göttingen, Germany, in order to obtain a doctor’s degree, but before completing it he went to the University of Paris. There, after an all-night party, a friend suggested that, instead of going to sleep, they watch the French aviation pioneer Henri Farman fly his machine. Farman successfully completed a 2-km (1.25-mile) course, unknowingly providing the inspiration for the young man who was to become a founder of the aeronautical and astronautical sciences.\nShortly thereafter, Ludwig Prandtl, a pioneer of modern fluid mechanics, invited von Kármán to return to Göttingen as his assistant on dirigible research and to complete his degree. The environment at the university was admirably suited to develop von Kármán’s talents. He responded, in particular, to the school of the eminent mathematician Felix Klein, which stressed the fullest use of mathematics and of the basic sciences in engineering to increase technological efficiency. In 1911 he made an analysis of the alternating double row of vortices behind a bluff body (one having a broad, flattened front) in a fluid stream, now famous as Kármán’s Vortex Street. The use of his analysis to explain the collapse, during high winds, of the Tacoma Narrows Bridge in the state of Washington, in the United States in 1940, is one of the most striking examples of its value.\nIn 1912, after a short stay at the College of Mining Engineering in Hungary, he became director of the Aeronautical Institute at Aachen (Aix-la-Chapelle) at the age of 31, remaining until 1930. In World War I he was called into military service and, while at the Military Aircraft Factory at Fischamend in Austria, led the development of the first helicopter tethered to the ground that was able to maintain hovering flight. After the war, as his international reputation grew, so did that of the institute. Students came from many countries, attracted by the intellectual and social atmosphere he had created. To help reestablish contacts and friendships broken by the war, he was instrumental in calling an international congress on aerodynamics and hydrodynamics at Innsbruck, Austria, in 1922. This meeting resulted in the formation of the International Applied Mechanics Congress Committee, which continues to organize quadrennial congresses, and gave birth, in 1946, to the International Union of Theoretical and Applied Mechanics, with von Kármán as honorary president.\nHe began traveling widely in the 1920s as a lecturer and consultant to industry. After his first visit to the United States in 1926, he was invited in 1930 to assume the direction of the Guggenheim Aeronautical Laboratory at the California Institute of Technology (GALCIT) and of the Guggenheim Airship Institute at Akron, Ohio. His love for Aachen made him hesitate, but the darkening shadow of German Nazism caused him to accept. He never regretted his decision. When President John F. Kennedy presented to him the first National Medal of Science in 1963, he “pledged his brain as long as it lasted” to the country of which he had become a citizen in 1936.\nShortly after his arrival at the California Institute of Technology, his laboratory became again a mecca of the world of the aeronautical sciences. Two years later he became a founder of the U.S. Institute of Aeronautical Sciences, consultant to various American industries and to the government. His personal scientific work continued unabated with important contributions to fluid mechanics, turbulence theory, supersonic flight, mathematics in engineering, aircraft structures, and wind erosion of soil.\nHis open-mindedness was well demonstrated by his involvement in the development of astronautics. In 1936, in spite of the general disbelief in academic circles in the possibilities of rocket propulsion and its applications, he supported the interest of a group of his students in the subject. Within two years the U.S. Army Air Corps sponsored a project at his laboratory on the use of rockets to provide superperformance for conventional aircraft—especially to reduce their distance of takeoff from the ground and from naval aircraft carriers. In 1940 von Kármán, together with Frank J. Malina, showed for the first time since the invention of the black-powder rocket in China in about the 10th century that it was possible to design a stable, long-duration, solid-propellant rocket engine. Shortly thereafter, the prototype of the famed jet-assisted takeoff (JATO) rocket was constructed. This became the prototype for rocket engines used in present-day long-range missiles, such as the Polaris, Minuteman, and Poseidon of the U.S. armed forces. In 1941 von Kármán participated in the founding of the Aerojet General Corporation, the first American manufacturer of liquid- and solid-propellant rocket engines. In 1944 he became the cofounder of the present NASA Jet Propulsion Laboratory at the California Institute of Technology when it undertook America’s first governmental long-range missile and space-exploration research program for the U.S. Ordnance Department.\nWhen he took leave from the institute in 1944 to establish in Washington, D.C., the Air Corps Scientific Advisory Group for General Henry H. Arnold, commander of the U.S. Army air forces in World War II, von Kármán could look back on his participation in a number of major contributions to rocket technology: America’s first assisted takeoff of aircraft with solid- and liquid-propellant rockets, flight of an aircraft with rocket propulsion alone, and development of spontaneously igniting liquid propellants of the kind that were to be used in the Apollo Command and Lunar Excursion modules some 25 years later.\nHis dedication to international scientific cooperation led him in 1947 to propose to the United Nations the establishment of an international research center for fluid and soil mechanics in the Middle East, which, though unfulfilled, contributed to the development by UNESCO of the Arid Zone Research Project in 1950. He conceived the idea of cooperation among aeronautical engineers of the member nations of the North Atlantic Treaty Organization (NATO) and, in 1951, obtained approval to launch the Advisory Group for Aeronautical Research and Development (AGARD), of which he was chairman until his death. In 1956 his efforts brought into being the International Council of the Aeronautical Sciences (ICAS) and, in 1960, the International Academy of Astronautics. One of the outstanding activities of the academy under his presidency was its sponsorship, in 1962, in Paris, of the First International Symposium on the Basic Environmental Problems of Man in Space, at which for the first time scientists from the United States and the Soviet Union, as well as other countries, exchanged information in this field. Between 1960 and 1963 he led NATO-sponsored studies on the interaction of science and technology.\nAn appreciation of von Kármán’s personality must also take account of his nonscientific talents. He was much interested in poetry and literature and could always supply a story appropriate to any occasion. When the atmosphere became charged with tension in a scientific meeting, he was able to restore balance by drawing on his collection of anecdotes. He had a fantastic capacity for work and left behind him wherever he went a trail of bits of paper covered with calculations. He was an optimist and believed in the future, despite the prevailing difficulties in the world.\nVon Kármán never married. His mother and his sister, Josephine, lived with him from 1923 onward in the Netherlands near Aachen and later in Pasadena, Calif. His sister was his manager and hostess until her death in 1951 in America. Brother and sister were devoted to each other, and her death plunged von Kármán into deep depression for several months, during which he was unable to work.\nVon Karman passed away on a trip to Europe in 1963. He is buried in Los Angeles at the Beth Olam Mausoleum at the Hollywood Forever Cemetery. His name is immortalized in the scientific community in a large number of concepts, some listed below, along with named laboratories at the California Institute of Technology, the Arnold Engineering Development Center of the U.S. Air Force at Tullahoma, TN, and the NATO institute for fluid dynamics at Saint-Genesius-Rode, Belguim which also bear his name. A crater on the Moon has carried his name since 1970.\n- Föppl–von Kármán equations (large deflection of elastic plates)\n- Born–von Karman boundary condition (in solid state physics)\n- Born–von Kármán lattice model (model for the lattice dynamics of a crystal)\n- Chaplygin–Kármán–Tsien approximation (potential flow)\n- Falkowich–Kármán equation (transonic flow)\n- von Kármán constant (wall turbulence)\n- Kármán line (aerodynamics/astronautics)\n- von Kármán–Gabrielli diagram (transportation)\n- Kármán–Howarth equation (turbulence)\n- k function\n- Kármán–Penner flux fraction (combustion)\n- Kármán–Nikuradse correlation (viscous flow; coauthored by Johann Nikuradse)\n- Kármán–Pohlhausen parameter (boundary layers)\n- Kármán–Treffz transformation (airfoil theory)\n- Prandtl–von Kármán law (velocity in open channel flow)\n- von Kármán integral equation (boundary layers)\n- von Kármán ogive (supersonic aerodynamics)\n- von Kármán vortex street (flow past cylinder)\n- von Kármán–Tsien compressibility correction\n- Vortex shedding\n- Von Kármán swirling flow","Image courtesy of Lisa Ferdinando/DVIDS.\nEnvironmental change1 is increasingly recognised as one of the major factors that will shape the global security environment. According to most experts, rising global temperatures will lead to rising sea levels and cause more extreme weather events, such as storms, flooding, droughts and wildfires.2 The firestorms that engulfed parts of Australia in late 2019 and early 2020, burning an area the size of Belgium and Denmark combined, and severely decimating that continent’s wildlife, were a stark reminder of the force of these changes.\nWhile the causal relationship between environmental change and conflict is difficult to establish, there have been arguably several conflicts where environmental change has acted as a trigger, notably Darfur and Somalia. Even the beginning of the Arab Spring has been related to environmental change: unrest erupted because of increasing food prices, which in turn were the result of several bad harvests attributed to climate change.3 In general, there is a widely held assumption that environmental change could lead to food and water shortages, pandemic diseases, mass migration, and humanitarian disasters.\nEnvironmental changes will also influence the way in which military forces conduct their missions. For example, the military could be called upon more often to provide humanitarian assistance and disaster relief. Other consequences for the military are the vulnerability of coastal installations to rising sea levels and the impact of floods, wildfires, and more extreme temperatures on military exercises and supply chains. Finally, as climatic changes open up regions hitherto largely closed to human activity (e.g. the Artic) the military will need to operate in these challenging environments.\nNATO is not the first responder to climate change. This role is played by other international bodies, in particular those who can set limits on CO2 emissions. However, as the premier transatlantic security and defence organization, NATO would seem an appropriate forum for discussing the security implications of environmental change. After all, NATO offers a seamless continuum of political consultation and decision-making, military planning and military implementation.\nIn addition, through its various committees and agencies NATO is covering many non-military, security-related subjects that range from intelligence sharing to civil emergency planning, and from environmental to medical issues. Moreover, as a democratic Alliance NATO is answerable to public concerns, in particular if these concerns have a genuine security dimension.\nNATO’s environmental security “acquis”\nIn NATO’s 2010 Strategic Concept, Allies for the first time acknowledged that “[k]ey environmental and resource constraints, including health risks, climate change, water scarcity and increasing energy needs will further shape the future security environment and have the potential to significantly affect NATO planning and operations”.4 This formulation already hinted at two distinct dimensions of environmental security: as a “threat multiplier” that generates new security challenges or aggravates existing ones, and as a phenomenon that impacts on the nature and conduct of NATO’s military operations.5 Both dimensions lead to different conclusions. While the first puts the onus on prevention and mitigation, the second puts the emphasis on (military) adaptation. Given NATO’s nature as a political-military alliance, its emphasis will inevitably lie on adaptation. However, as a closer look at NATO’s environmental security activities reveals, the Alliance’s agenda also encompasses some preventive elements. Three major areas can be identified: Strategic Awareness, the Military Dimension, and Cooperative Security.\nStrategic Analysis. The security implications of environmental change in terms of climate-induced geopolitical, economic and military shifts are already dealt with in Allied consultations and intelligence-sharing. Moreover, certain NATO Committees in the Civil Emergency Planning domain have been dealing with, inter alia, extreme weather conditions, pandemics, and disaster relief. Also, the Secretary General’s Policy Planning Unit and the Emerging Security Challenges Division are promoting a dialogue with climate experts. NATO’s Allied Command Transformation is conducting work related to the security implications of environmental change through its regular Strategic Foresight Analysis and the Framework for Future Alliance Operations. Finally, the NATO Strategic Direction-South Hub is also undertaking studies on environmental change, notably with a focus on the African continent.\nPolicies and Standards on Environmental Protection. Since the 1970s NATO developed environmental protection guidelines and standards, resulting in an overarching policy (MC469, agreed in 2003). The implementation of this policy is supported by a number of NATO standards, for example on waste management, water treatment and best practice for camps. Furthermore, NATO’s “Policy on Power Generation for Deployed Force Infrastructure” has a strong focus on saving fossil fuel. The rationale of these efforts is both military and political: mission success requires ensuring the public acceptance of deployed NATO forces in the host country. This requires these forces to demonstrate a genuine commitment to the well-being of the local population, including by protecting their environment.\nEducation, Training, and Exercises. The need to prepare for operations in a changing environment is also reflected in NATO’s education and training efforts. Some training tools already exist, for example the Norway-based NATO Centre of Excellence (CoE) for Cold Weather Operations. ln addition to CoEs, national education and training facilities as well as the NATO School in Oberammergau are conducting environmental protection courses that sensitise soldiers and civilians with the stringent requirements for operating deployed camps, protecting cultural property, etc. NATO’s education and training efforts are constantly being adapted to changing needs and requirements. If considered politically and/or militarily desirable, training courses could be devised that focus specifically on climate-related threats and responses.\nLikewise, NATO’s Euro-Atlantic Disaster Response Coordination Centre (EADRCC) is regularly conducting consequence management field exercises involving dozens of Allies and partner countries. The scenarios, some of which are based on environmental challenges faced by the host nations, are designed to strengthen the ability of teams from different nations to cooperate across a wide range of relief operations. These include urban search and rescue, emergency medical teams, as well as detection, protection and decontamination teams. Compared with traditional military exercises, they are smaller (up to 2,000 personnel) and performed in close cooperation with the United Nations Office for the Coordination of Humanitarian Affairs (UN OCHA), which retains the primary role in the coordination of international disaster relief operations.\nThe military dimension\nOperational Planning and Defence Procurement. As Allies acknowledged in the Strategic Concept, climate change and other developments could impact NATO’s operational planning. The Pentagon has repeatedly noted that rising sea levels may impact the execution of amphibious landings; changing temperatures lengthening the arid season could impact operation timing windows; and the increased frequency of extreme weather could limit surveillance and reconnaissance measures.6 Environmental change can also affect military bases (e.g. a rise in the sea level could render important hubs like Diego Garcia unusable). More hostile climatic conditions could also affect both the service life and the maintenance requirements of military equipment (for example, adding dust filters to protect the engines of military vehicles will limit their range and performance). Finally, changing climatic conditions could also lead some Allies to invest in a different force structure, e.g. emphasising helicopters, coast guard vessels and amphibious vehicles over other equipment.\nEnergy Efficiency in the Military. The high fuel demand of combat forces can diminish their performance, increase their vulnerability, and may require the diverting of combat forces to protect supply lines. Hence, in-creased energy efficiency could offer benefits in terms of combat power and agility. NATO’s own work in this regard focuses on reducing the consumption of fossil fuel in deployed force infrastructure (i.e. camps), resulting in more autonomy, a lesser logistical burden and a smaller environmental footprint.7 In early 2014, Allies agreed the “Green Defence Framework”, which sought to bring various internal work strands in NATO closer together in order to create synergies and improve NATO’s “green” profile.8 In addition, questions related to national energy efficiency measures have been included in NATO’s Defence Planning Capability Survey. All these measures are undertaken with military effectiveness in mind. However, there is widespread agreement that a reduced energy footprint of certain military activities offers the additional advantage of demonstrating to a broader public that the military is not indifferent to environmental concerns.\nConsequence Management. While humanitarian relief missions are not considered a core task of NATO, the Alliance has been involved in such missions on several occasions, starting in 1991 to protect the Kurdish population in Northern lraq (“Provide Comfort”); in the aftermath of a severe earthquake in Pakistan in 2005 (where NATO delivered almost 3,500 tons of relief supplies); and after Hurricane Katrina hit the US in 2005. The Euro-Atlantic Disaster Response Coordination Centre also coordinated national assistance on many other occasions (floods, mudflows, wildfires) in Allied and partner countries (e.g. Albania, Bulgaria, Turkey, Algeria, Moldova). Noticeably, thus far, the NATO Response Force, which had initially been conceived as the flagship of NATO’s military transformation, has mostly been deployed in humanitarian relief efforts.\nEnvironmental security as a partnership tool. The partnership dimensions of addressing environmental security risks are several-fold. First, as environmental change affects many partner countries, they will be interested in scientific cooperation with NATO on mitigation measures, but also on consequence management, training and education. Past examples of such scientific cooperation encompassed measures on flood control in Ukraine and preventing desertification in Egypt. Second, helping partner countries build the capacity and resilience to better manage environmental impacts could become a legitimate element in NATO’s Defence Capacity Building approach. Finally, as humanitarian relief oper-ations involve many other international organizations and NGOs, they provide a strong rationale for deepening NATO’s ties with all prospective actors, thereby underlining NATO’s inclusive and comprehensive approach to security.\nScientific Cooperation. NATO’s Science for Peace and Security (SPS) Programme has a long tradition of addressing environmental concerns. Its predecessor, the “Committee on the Challenges of Modern Society”, looked at the challenge of environmental degradation as far back as the late 1960s. Today, SPS brings together scientists from Allied and partner countries, has supported numerous workshops and multi-year projects linked to security issues arising from key environmental and resource constraints, as well as disaster forecasting and the prevention of natural catastrophes, and defence-related environmental issues. Allies’ interest in supporting projects on environmental security has waned in the recent past. Still, SPS remains an important resource for NATO to support workshops and training courses with military and civilian experts addressing geophysical, meteorological and atmospheric/ space-weather phenomena that have an effect on military capabilities.\nThe delicate path ahead\nAs this non-exhaustive list of NATO’s activities demonstrates, the Alliance is already coping with the consequences of environmental change on various levels. Hence, giving this dossier greater emphasis and visibility appears feasible. This could entail, first and foremost, revisiting the aforementioned “Green Defence Framework”, seeking to operationalise its various suggestions in areas such as energy efficiency measures and the sharing of best practices, and perhaps even considering “the applicability of ‘green’ standards and principles across the NATO HQ, NATO Command Structure and NATO Agencies, and […] the applicability of setting up ‘green’ accounting and benchmarks to measure progress”.9 Allies could also support more scientific research projects through NATO’s Science for Peace and Security Programme, seek to elevate the role of environmental security in its dialogue and cooperation with partner countries, enhance NATO’s presence at climate-related events, and initiate a more robust public diplomacy effort. However, simply producing more public statements on the importance of environmental security will not be enough. Successfully raising NATO’s visibility in this domain will depend on a number of important factors.\nFirst, as environmental change touches upon many Allied sensitivities, great care needs to be taken to establish and sustain consensus on this subject. One reason why the 2010 Strategic Concept did not lead to a stronger focus on environmental security was the hesitation of many Allies to become engaged in a subject that could upset the balance of interest in certain regions (e.g. the High North) or risks degenerating into a controversial debate about national environmental or energy policies. In the same vein, Allies’ interest in using the SPS Programme to support scientific projects related to environmental security proved rather uneven in the past, with some Allies questioning whether NATO was the appropriate framework for sponsoring such efforts. In short, without proper handling, these diverging views could quickly re-surface. Hence, all Allies must be reassured that a more visible NATO role in climate and environmental issues will not be detrimental to their national (security) interests.\nA plausible narrative\nSecond, given NATO’s nature as a political-military Alliance, for any NATO narrative on environmental security to be credible, it should be focussed predominantly on consequence management and less on prevention – where NATO’s role is comparatively small. While this may clash with the public sense of urgency about the need to slow down or even arrest environmental change, NATO’s major contribution to security remains in employing its military competence as a “force for good”, be it through deterring major war or offering humanitarian assistance after natural disasters. This contribution to international peace and stability is what makes NATO unique. Hence, any attempt to give NATO more visibility in the environmental field must take care not to send conflicting messages. With public expectations set on mitigation and prevention, Allies must not allow a view to take hold that they have concluded that mitigation efforts will fail and that NATO therefore had to prepare for the worst. At the same time, neither should they give the impression that a stronger focus by NATO on environmental security means moving away from its core business of military deterrence and defence.\nThird, given the highly emotional and at times outright apocalyptic nature of the current debate on environmental change, Allies must resist any temptation to cater to the wilder shores of this debate. After all, irrespective of public expectations regarding reduced greenhouse emissions, the military (notably air forces) will remain a major polluter.10 Stressing NATO’s “greening” efforts is a genuinely positive message, all the more so as it would tie in with the logic of other initiatives, such as the EU’s new “Green Deal”, which aims for the Union to become climate-neutral by 2050. However, its military focus will not allow NATO to formulate similarly ambitious goals. Militaries will have to focus on operational effectiveness, with environmental concerns playing a growing, yet secondary role. Overselling NATO’s contribution to environmental security would run the risk of breeding disappointment or even outright resentment, with hardcore climate activists denouncing such efforts as mere political window-dressing. In short, in discussing environmental security NATO must not be apologetic. Environmental and military security are both of existential importance; they must not be pitted against each other.\nConclusion: setting the stage for a more visible role in environmental security\nThe international discussion on environmental change is now becoming a legitimate part of the security debate. While NATO is not going to be a major factor in the environmental debate, its elevated role in international security demands that it be more than merely a dispassionate observer of that debate. If NATO can demonstrate that in implementing its core mission of deterrence and defence it is conscious of environmental concerns, and that its national militaries have understood the need to make their own contribution, the stage could be set for a more visible role in environmental security. Even if modest, such a more visible role in environmental security would help align NATO with a challenge that a growing number of people are regarding as a major security concern.\nThis article was published under a Creative Common “Attribution-Non Commercial-NoDerivs” Licence. (CC BY-NC-ND)\n1 Although the term “climate change” is more commonly used, it is politically charged, as it is closely connected with the question of whether it is a man-made phenomenon. Hence, this paper mostly uses the term “environmental change”, as it is also more comprehensive.\n2 See International Military Council on Climate and Security, The World Climate and Security Report 2020, February 2020.\n3 See S. Johnstone and J. Mazo, “Global Warming and the Arab Spring”, Survival, April/May 2011, pp.11-17. 4 NATO’s Strategic Concept, 2010, para.15.\n5 See also T. H. Lippert, NATO, climate change and international security: a risk governance approach, Palgrave MacMillan, 2019.\n6 See the numerous examples listed in: Office of the Under Secretary of Defense for Acquisition and Sustainment, “Report on Effects of a Changing Climate to the Department of Defense”, Washington, DC, January 2019.\n7 For a selection of documents and other information on Allies’ and NATO’s work on “smart energy” see the “Smart Energy LibGuide” at http://www.natolibguides.info/smartenergy\n8 Green Defence Framework, February 2014, http://www.natolibguides. info/ld.php?content_id=25285072. Given the novelty of some of the issues addressed in that paper, its rather hesitant tone should not come as a surprise. However, it was Russia’s annexation of Crimea in March 2014 and NATO’s subsequent re-emphasis on collective defence that prevented a more systematic pursuit of the paper’s various innovative elements.\n9 Green Defence Framework, 2014, para.10.\n10 For a typical example see T. Lorincz, “NATO is a threat to the climate”, Ricochet, 29 December 2019.\nAbout the Author\nMichael Rühle is Head, Energy Security Section, in the Emerging Security Challenges Division in NATO’s International Staff.\nFor more information on issues and events that shape our world, please visit the CSS website."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:e2247ea3-a55b-4449-a695-a0312101c657>","<urn:uuid:c393af76-3013-49d5-919b-39a6c9f5c59f>"],"error":null}
{"question":"With my family history of joint problems, I want to know: what's the key difference between how the vestibular system and sex hormones protect our joints?","answer":"The vestibular system and sex hormones protect joints through entirely different mechanisms. The vestibular system, located in the inner ear, works by using fluid-filled semicircular canals to detect head position changes and send signals to the brain, helping maintain proper body positioning and prevent harmful joint movements. In contrast, sex hormones (testosterone and estrogen) work at the cellular level to maintain joint health. Testosterone stimulates the growth and differentiation of cartilage stem cells, while estrogen increases proteoglycan production in chondrocytes and decreases cartilage damage. Both systems are essential, but while the vestibular system provides immediate physical protection through balance control, hormones provide ongoing biochemical protection through tissue maintenance and repair.","context":["A balance disorder is a condition that makes you feel dizzy or unsteady, creating the sensation of spinning or floating or moving. And while short or minor episodes of dizziness are common and no cause for concern, more extreme sensations of spinning (vertigo) or prolonged dizzy spells should be assessed.\nCoupled with dizziness, you may also experience other symptoms like nausea, variations in heart rate, anxiety, or panic. Again, if these episodes are especially extreme or extended, it’s a good idea to seek professional care.\nThe types and causes of balance disorders are numerous, but before we get to that, let’s quickly review how the body normally preserves its sense of balance.\nHow the body sustains its balance\nWe take our body’s facility to maintain balance for granted because it usually operates effortlessly behind-the-scenes. But when you think about it, maintaining balance is quite an impressive feat.\nEven in motion, your body is able to perceive its location in space and make modifications to hold your body upright, while calling for very little to any mindful regulation. Even if you close your eyes, and take away all visual cues, you can precisely sense the position of your head as you move it up or down, left or right.\nThat’s because your vestibular system—the collection of organs and structures in your inner ear—can detect any changes in your head position, transmitting nerve signals to inform your brain of the change.\nStructures in the inner ear known as semicircular canals possess three fluid-filled ducts placed at roughly right angles to each other. When you move your head, the fluid moves together with it, stimulating the nerve cells that send the information to your brain.\nThis, combined with visual cues and musculoskeletal sensory information, signals the brain to precise changes in head and body position.\nCommon balance disorders and causes\nBalance disorders are a consequence of a disturbance within the vestibular system or with the brain and its capability to ascertain and use the information.\nBalance disorders can consequently be caused by anything that disturbs the inner ear or brain. This list includes, but is not restricted to, medications, benign tumors, ear infections, head injuries, low blood pressure or other heart conditions, and some neurological conditions.\nCommon balance disorders include Meniere’s Disease, Benign Paroxysmal Positional Vertigo (BPPV), Labyrinthitis, Vestibular Neuronitis, along with many others. Each disorder has its own distinct causes and symptoms and can be diagnosed only by a professional.\nDiagnosis and treatment of balance disorders\nThe diagnosis and treatment of any balance disorder begins by ruling out any medical conditions or medications that might be creating the symptoms. You may be required to change medications or seek treatment for any underlying cardiovascular, neurological, or musculoskeletal condition.\nIf your balance problem is due to issues with the inner ear, such as with Meniere’s Disease, treatment may include nutritional and lifestyle changes, physical manipulations of the head, or medications to ease the symptoms. Your healthcare provider can provide additional information specific to your condition and symptoms.","Osteoarthritis: A Hormone Deficiency Disorder?\nAngela Cortal, ND\nOsteoarthritis (OA) is not only the most common form of arthritis; it is also one of the leading causes of disability among older adults and is the second-most rapidly rising disability-associated disorder. As a result, the “Global Burden of Disease Study 2013” reported OA as a leading cause of chronic pain in the United States and globally.1\nIn my 2016 NDNR article, “Halting Osteoarthritis in Its Tracks,”2 in addition to reviewing the epidemiology and pathophysiology of OA, I described how the condition had historically been thought of as a degenerative joint disease simply due to “wear and tear.” Our evolving view of OA includes an understanding that dysfunction in inflammation modulation and mechanical abnormalities (repetitive injuries, acute traumas, or inappropriate joint loading) are additional significant factors in its development.\nThis article seeks to elucidate the strong connection between sex hormone levels and OA, including risk of development, risk of worsening, and reversal of its pathophysiology.\nTestosterone & Osteoarthritis\nAs an anabolic hormone, adequate levels of circulating testosterone are known to be vitally important for development and maintenance of muscle mass, as well as for having systemic effects on inflammation, wound healing, and inflammatory responses.3 In a study evaluating all-cause chronic knee pain, testosterone levels were inversely associated with knee pain severity.4\nPhysiologic levels of testosterone, and its correlation to cartilage tissue health, has been a much less appreciated facet, so we will be looking at this in detail.\nAs of 2010, over 600 000 total knee replacements and 300 000 hip arthroplasties were performed annually in the United States,5 which makes them among the most commonly performed orthopedic procedures. In men, the likelihood of undergoing these procedures correlates with circulating levels of anabolic hormones.6 Specifically, those with the highest levels of androstenedione (a testosterone precursor) are at the lowest risk for both total knee replacement and hip arthroplasty. Similar observations have been noted for women: those with higher androstenedione levels have 30% fewer hip replacements.7\nA primary finding through the course of development of OA is intra-articular cartilage loss. When patients are categorized as having a severe case of OA, they are often told the joint is “bone on bone,” meaning that the cartilage has so thoroughly degraded, the approximating bones are now in contact. In studies tracking male patients over 2 years via MRI, those with the lowest testosterone levels were found to have accelerated knee cartilage loss (ie, more degradation).8 This finding remained even after controlling for age, body mass index, and bone mineral density – all common confounding factors in OA.\nTestosterone is important for everyone’s joints. Although OA is often thought of as a condition exclusive to older adults (due to age-related increased incidence), among women ages 24-45 who are diagnosed with OA, those with the lowest testosterone levels were found to experience a greater degree of arthritic pathology.9\nThese studies thus far indicate a strong correlation between low testosterone and more severe OA. However, correlation does not equal causation. What do interventional studies show?\nFirst, looking at cartilage cells in culture, we see that application of testosterone to severe osteoarthritic cells directly stimulates the growth and differentiation of cartilage stem cells (called chondrogenic progenitor cells).10 This positive effect was observed in both male and female cartilage cell samples, using physiologic dosages of testosterone – dosing that equates to pre-andropause levels in men, and premenopausal levels in women.\nTurning to animal models, studies have demonstrated accelerated development of OA in orchidectomized mice (ie, testicular removal, which causes hypogonadism and thus low testosterone). This intentionally created OA was then reversed to match cartilage levels of non-orchidectomized mice of similar age (the control group).11 This reversal was achieved by supplementation of dihydrotestosterone, a main testosterone metabolite, which exerts many of the secondary sex characteristics we typically attribute to testosterone.\nBased on these research findings, testosterone injections have even been successfully performed intra-articularly at the site of degenerative joint disease in human patients to stimulate immune processes and chondrocyte activity necessary to address the osteoarthritis process.12\nThe research clearly suggests that healthy testosterone levels are necessary for both osteoarthritis prevention and correction.\nEstrogen & Osteoarthritis\nEstrogen levels in women are also an important aspect of cartilage health, to the extent that some experts have proposed specific OA classification subtypes including estrogen-related OA: “type I OA, genetically determined; type II OA, estrogen hormone dependent; and type III OA, aging related.”13\nIn addition to the well-established positive effects of maintaining bone mineral density and muscle mass and function, estrogen has also been shown to have the following intra-articular effects14:\n- Increases proteoglycan production in chondrocytes\n- Decreases cartilage damage in animal models\n- Decreases anti-type II collagen serum levels in mice [type II collagen is the predominant type in cartilage]\nSimilar to the associations between testosterone levels and orthopedic procedure prevalence mentioned earlier in this article, women with higher estrogen levels have been shown to have a 30% reduced risk for total knee replacements.7\nOne study showed women with serum estradiol (E2) levels in the lowest tertile to have an almost 2-times greater risk of developing knee OA as compared to women whose E2 levels are in the middle tertitle.15 Another study confirmed that in women with knee OA, serum E2 correlates with synovial E2.16\nThe exact cellular mechanisms at play continue to unfold. Two major factors appear to be the estrogen-induced inhibition of matrix metalloproteinases (MMP) – a primary driver of collagen degradation17 – and estrogen stimulation of collagen growth and differentiation.18 In essence, estrogen appears to both halt processes that cause cartilage cell death and to activate those pathways that enhance new cell growth.\nEstrogen’s inhibition of MMP has been found to promote a substantial 50% reduction in the rate of cartilage destruction.19 Recent research, published in January of 2019, suggests that an additional physiologic effect of estrogen (specifically the metabolite, 2-hydroxyestrone) on cartilage degradation is via arachidonic acid pathway modulation, affecting activity of lipoxygenases and thus leukotriene production and effects.20\nEstrogen replacement used to address climacteric symptoms is known to be most effective when begun close to the perimenopausal timeframe, and the same is seen when estrogen is used to address OA-related damage.21 While the OA process can be decades in the making, arresting cartilage loss in perimenopause, before the decline in endogenous estrogen levels causes it to accelerate, is strongly recommended.\nThis article is not meant to suggest that hormone replacement therapies are indicated for every patient with osteoarthritis. My aim in sharing this information is to inspire you to consider sex hormone deficiencies as a possible contributory factor when working with your OA patients. Prompt identification and treatment of such deficiencies will go a long way toward improving health outcomes of your patients with OA. Reduced pain and improved physical function and quality of life for our patients are goals we can all get behind.\n- Moradi-Lakeh M, Forouzanfar MH, Vollset SE, et al. Burden of musculoskeletal disorders in the Eastern Mediterranean Region, 1990-2013: findings from the Global Burden of Disease Study 2013. Ann Rheum Dis. 2017;76(8):1365-1373.\n- Cortal A. Halting Osteoarthritis in Its Tracks. NDNR. 2016;12(9).\n- Adelson H, Moore T, Anderson P. Autologous Stem Cell Therapy: A Naturopathic Approach for the Treatment of Chronic Musculoskeletal Pain Conditions Part I of II. The Pain Practitioner. 2015;25(3):48-51.\n- Jin X, Wang BH, Wang X, et al. Associations between endogenous sex hormones and MRI structural changes in patients with symptomatic knee osteoarthritis. Osteoarthritis Cartilage. 2017;25(7):1100-1106.\n- Daigle ME, Weinstein AM, Katz JN, Losina E. The cost-effectiveness of total joint arthroplasty: a systematic review of published literature. Best Pract Res Clin Rheumatol. 2012;26(5):649-658.\n- Hussain SM, Cicuttini FM, Giles GG, et al. Relationship between circulating sex steroid hormone concentrations and incidence of total knee and hip arthroplasty due to osteoarthritis in men. Osteoarthritis Cartilage. 2016;24(8):1408-1412.\n- Hussain SM, Cicuttin FM, Bell RJ, et al. Incidence of total knee and hip replacement for osteoarthritis in relation to circulating sex steroid hormone concentrations in women. Arthritis Rheumatol. 2014;66(8):2144-2151.\n- Hanna F, Ebeling PR, Wang Y, et al. Factors influencing longitudinal change in knee cartilage volume measured from magnetic resonance imaging in healthy men. Ann Rheum Dis. 2005;64(7):1038-1042.\n- Sowers MF, Hochberg M, Crabbe JP, et al. Association of bone mineral density and sex hormone levels with osteoarthritis of the hand and knee in premenopausal women. Am J Epidemiol. 1996;143(1):38-47.\n- Koelling S, Miosge N. Sex differences in chondrogenic progenitor cells in late stages of osteoarthritis. Arthritis Rheum. 2010;62(4):1077-1087.\n- Ma HL, Blanchet TJ, Peluso D. Osteoarthritis severity is sex dependent in a surgical mouse model. Osteoarthritis Cartilage. 2007;15(6):695-700.\n- Ravin T. The use of testosterone and growth hormone for prolotherapy. J Prolotherapy. 2010;2(4):495-503.\n- Herrero-Beaumont G, Roman-Blas JA, Catañeda S, Jimenez SA. Primary osteoarthritis no longer primary: three subsets with distinct etiological, clinical, and therapeutic characteristics. Semin Arthritis Rheum. 2009;39(2):71-80.\n- Roman-Blas JA, Castañeda S. Largo R, Herrero-Beaumont G. Osteoarthritis associated with estrogen deficiency. Arthritis Res Ther. 2009;11(5):241.\n- Richette P, Laborde K, Boutron C, et al. Correlation between serum and synovial fluid estrogen concentrations: comment on the article by Sowers et al. Arthritis Rheum. 2007;56(2):698; author reply 698-699.\n- Sowers MR, McDonnell D, Jannausch M, et al. Estradiol and its metabolites and their association with knee osteoarthritis. Arthritis Rheum. 2006;54(8):2481-2487.\n- Claassen H, Steffen R, Hassenpflug J, et al. 17β-estradiol reduces expression of MMP-1, -3 and -13 in human primary articular chondrocytes from female patients cultured in a three dimensional alginate system. Cell Tissue Res. 2010;342(2):283-293.\n- Tanamas SK, Wijethilake P, Wluka AE, et al. Sex hormones and structural changes in osteoarthritis: a systematic review. Maturitas. 2011;69(2):141-156.\n- Karsdal MA, Bay-Jensen AC, Henriksen K, Christiansen C. The pathogenesis of osteoarthritis involves bone, cartilage and synovial inflammation: may estrogen be a magic bullet? Menopause Int. 2012;18(4):139-146.\n- Hafsi K, McKay J, Li J, et al. Nutritional, metabolic and genetic considerations to optimise regenerative medicine outcome for knee osteoarthritis. J Clin Orthop Trauma. 2019;10(1):2-8.\n- Nevitt MC, Cummings SR, Lane NE, et al. Association of estrogen replacement therapy with the risk of osteoarthritis of the hip in elderly white women. Study of Osteoporotic Fractures Research Group. Arch Intern Med. 1996;156(18):2073-2080.\nAngela Cortal, ND, graduated from NUNM in 2012. Her practice focuses on regenerative injection therapies (prolotherapy and platelet-rich plasma injections) and hormonal imbalances, particularly as they impact chronic musculoskeletal disorders. Dr Cortal’s private practice is based in Salem, OR, with a satellite practice in Portland (www.rosecityhealth.com). She also enjoys writing, speaking, and teaching on the topics of regenerative injection therapies and hormonal dysfunction. Dr Cortal is on the board of Santiam Community Health, a 501(c)(3) nonprofit which expands the reach of prolotherapy through mobile health “pop-up clinics” to those with medical and financial barriers to accessing traditional healthcare services (www.santiamhealth.org)."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:66b56018-ebd6-45a3-b19e-e85873d19236>","<urn:uuid:577f9c3a-29b4-48ea-8db4-ebc7fc024376>"],"error":null}
{"question":"Could you compare the earliest discoveries of oxygen as an element with the current understanding of its role in marine ecosystems and climate regulation?","answer":"The early discovery of oxygen involved multiple scientists in the 1770s, with Joseph Priestley isolating the gas (though defining it as 'dephlogisticated air'), Carl Wilhelm Scheele likely discovering it before Priestley, and Antoine Lavoisier finally identifying it as an element. Today, we understand oxygen's crucial role in marine ecosystems - it accounts for about 23% of the atmosphere's mass, makes up 86% of the oceans as H2O, and is produced by marine algae through photosynthesis. These algal blooms are responsible for half of all photosynthesis on Earth and play vital roles in the planet's carbon, oxygen, phosphate, nitrogen, and sulfur cycles.","context":["Advertise with us\nThe Chemistry Centre (venue hire)\nChemistry World Jobs\nChemistry World magazine\nChemNet student networking\nChemSpider Synthetic Pages\nConferences & events\nLibrary and Information Centre\nPrizes & awards\nSubmission system (journals)\nChemistry in its element: oxygen\nYou're listening to Chemistry in its element brought to you by\n, the magazine of the Royal Society of Chemistry.\nHello! And welcome to Chemistry in its element, where we take a look at the stories behind the elements that make up the world around us. I'm Chris Smith. This week, we are continuing our tour of the periodic table with a lung full of a gas that we can't do without. It protects us from solar radiation, it keeps us alive and by helping things to burn, it also keeps us warm. It is of course oxygen. And to tell its story, here's Mark Peplow.\nLittle did those humble cyanobacteria realize what they were doing when two and a half billion years ago, they started to build up their own reserves of energy-rich chemicals, by combining water and carbon dioxide. Powered by sunlight, they spent the next two billion years terraforming our entire planet with the waste products of their photosynthesis, a rather toxic gas called oxygen. In fact, those industrious bugs are ultimately responsible for the diversity of life, we see around us today.\nOxygen accounts for about 23% of the atmosphere's mass with pairs of oxygen atoms stuck together to make dioxygen molecules, but it's not just in the air, we breathe. Overall, it's the most abundant element on the earth's surface and the third most abundant in the universe after hydrogen and helium. Our planet's rocks are about 46% oxygen by weight, much of it in the form of silicon dioxide, which we know most commonly as sand. And many of the metals we mine from the Earth's crust are also found as their oxides, aluminium in bauxite or iron in hematite, while carbonates such as limestone are also largely made of oxygen and the oceans are of course about 86% oxygen, connected to hydrogen as good old H\nO, just about the most perfect solvent you can imagine for biochemistry.\nOxygen is also in virtually every molecule in your body including fats, carbohydrates and DNA. In particular, it's the atom that links together the phosphate groups in the energy-carrying molecule ATP. Oxygen is obviously pretty useful for keeping us going, but is also widely used in industry as an oxidant, where it can give up some of that solar energy captured by plant and those cyanobacteria. A stream of oxygen can push the temperature of a blast furnace over 2000 degrees and it allows an oxyacetylene torch to cut straight through metal. The space shuttle is carried into space on an incredible force produced when liquid oxygen and liquid hydrogen combine to make water.\nSo who first noticed this ubiquitous stuff? There's certainly some debate about who first identified oxygen as an element, partly because at the time the precise definition of an element still hadn't really been pinned down. English chemist, Joseph Priestley certainly isolated oxygen gas in the 1770s, although he tried to define it as dephlogisticated air. Phlogiston was then thought to be some kind of primordial substance that was the root cause of combustion. Swedish chemist, Carl Wilhelm Scheele was a fan of phlogiston too and probably discovered oxygen before Priestly did. But it was Antoine Lavoisier, sometimes called the father of modern chemistry, who was the first to truly identify oxygen as an element and in doing so, he really helped to firm up the definition that an element is something that cannot be broken down by any kind of chemical analysis. This also helped him to kill off the phlogiston theory, which was a crucial step in the evolution of chemistry.\nOxygen isn't only about the dioxygen molecules that sustain us. There is another form, trioxygen, also known as ozone and it's also pretty important in the upper reaches of the atmosphere, is responsible for filtering out harmful ultraviolet rays, but unfortunately, ozone is also pretty toxic. So it's bad news that tons of the gas are produced by the reactions between hydrocarbons and nitrogen oxides churned out by cars every day. If only we could transplant the stuff, straight up into the stratosphere! Now ozone is normally spread so thinly in the air, that you can't see its pale blue colour and oxygen gas is colourless unless you liquefy it, but there is one place where you can see the gas in all its glory. The aurora or polar lights, where particles from the solar wind slam into oxygen molecules in the upper atmosphere to produce the swirling green and red colours that have entranced humans for millennia.\nSo why life is a gas, that was Mark Peplow revealing the secrets of the element that we can't live without. Next time on Chemistry in its element, Johnny Ball joins us to tell the story of a chemical that's craved by Olympic athletes, makes good hi-five connectors and is also a favourite for fillings. And that's in teeth, not pies.\nToday one gram can be beaten into a square meter sheet just 230 atoms thick, one cubic centimetre would make a sheet 18 square meters, 1 gram could be drawn out to make 165 meters of wire just 1/200\nof a millimetre thick. The gold colour in Buckingham Palace fence is actually gold; gold covered because it lasts 30 years; whereas gold paint which actually contains no gold at all lasts in tip-top condition only a year or so.\nSo all that glitters isn't gold, but some is, and you can find out why on next week's Chemistry in its element. I'm Chris Smith, thanks for listening. See you next time.\nChemistry in its element is brought to you by the Royal Society of Chemistry and produced by\n. There's more information and other episodes of Chemistry in its element on our website at\nListen to the chemistry in its element podcast on\nNow Playing > Oxygen\nRead the transcript\niTunes User Subscription\nChemistry World magazine\nWorking for us\nPrivacy & cookies\n© Royal Society of Chemistry 2015\nRegistered charity number: 207890","Rx for the ocean\nThe largest bodies of water need our urgent attention\nThe Tara research vessel on which Weizmann research has been conducted\nTake a breath. Now take another breath. Every second breath you take comes from the ocean.\nCovering 72 percent of the Earth and supplying half its oxygen, the ocean is our planet’s life support system—regulating climate, and providing food for an infinite variety of species, including our own. And here’s something you might not think about: The ocean is home to the Earth’s largest mountain ranges and its deepest canyons, and at the same time is the world’s most diverse habitat, sheltering the plants and animals that make up 80 percent of life on Earth.\nHumans are deeply dependent on the ocean, but you might not think so based on human behavior. Unlike the 12 percent of our planet’s total land mass that is legally set aside for preservation, less than 2 percent of the ocean is similarly protected. Fishing has sent the majority of the ocean’s large predators into extinction. Pollution, which has already destroyed 20 percent of our coral reefs, is damaging niche ecosystems in ways we are only beginning to understand. And humankind has yet to mount a serious response to climate change—the documented rise in global temperatures that is causing the ocean to get warmer and more acidic. That’s despite increasingly dire projections that the rising water levels, increased storm activity, and flooding we are experiencing will only get worse in the coming years.\nIf we want to preserve our precious water ecosystems, we must first understand them. A range of research projects on the oceans and, in particular, the Mediterranean Sea, are now underway at the Weizmann Institute. These projects, carried out in the labs of the Department of Earth and Planetary Sciences, the Department of Plant and Environmental Science, and elsewhere on campus, are creating a more integrated understanding of the biological and geological processes that drive the complex chemical “cross-talk” between the sea and the sky.\n“Weizmann Institute teams have developed powerful techniques for studying the ocean’s ancient history, and are contributing up-to-the-minute analyses of how marine ecosystems are responding to the twin challenges of climate change and human-generated pollution,” says Prof. Ilan Koren, head of the de Botton Center for Marine Science, whose own research focuses on ocean-atmosphere interactions as well as cloud and rain physics. “By gathering fundamental data using a range of approaches based in biology, chemistry, physics, and materials science, our scientists are clarifying what’s going on beneath the waves. These findings may contribute to humankind’s ability to rehabilitate endangered ocean ecosystems, and protect the planet we all call home.”\nThe projects described below, and many others, have positioned the Weizmann Institute of Science as a leading center of basic and applied environmental research.\nA microplastic choke hold\nFar from land, the ocean and the air just above its surface looks pristine. However, work by three Weizmann Institute scientists—Prof. Assaf Vardi, Prof. Koren, and Prof. Yinon Rudich—has demonstrated that looks can be deceiving.\nThe scientists have been analyzing samples collected during successive voyages of the Tara Expedition, an environmental research vessel that serves as a floating laboratory for oceanography and climate experts from all over the world. Packed in dry ice and sent back to the Weizmann Institute, these samples have created the basis for in-depth study of aerosols—specs of matter that, in Weizmann labs, were revealed to contain high levels of microplastics, fragmentary remains of human-generated plastic pollution. These surprising findings, recently published in Nature Communications Earth and Environment, demonstrate how near-ocean emissions facilitate the transport of microplastics from the shoreline to the deep ocean, where they affect marine ecosystems, disrupt food chains, and—eventually, when transferred into the atmosphere—can even harm human health.\nThe life and death of algal blooms\nSingle-cell marine algae may be small, but altogether, they have a huge ecological impact. Prof. Vardi is an expert on algal blooms—colonies of algae that can stretch for thousands of miles over the ocean surface in layers up to 50 meters in depth, and can even be seen from space. These blooms are responsible for half of all the photosynthesis on Earth, and also play an important role in the planet’s carbon, oxygen, phosphate, nitrogen, and sulfur cycles.\nInfections by viruses can spread quickly among the algae, causing the death of these massive blooms in a matter of days. Prof. Vardi investigates the molecular basis of such infections, as well as the effect bloom demise has on carbon transfer—a critical factor in the maintenance of a stable climate.\nThe role of aerosols\nLiquid and solid particles called aerosols that hover over the surface of the ocean are among the most significant factors in climate research, but their chemical properties and physical dynamics are poorly understood.\nAerosols affect climate both directly, by scattering and absorbing sunlight, and indirectly, by their effects on clouds and incoming solar radiation. Using a combination of state-of-the-art chemistry and microbiology tools, Prof. Rudich is working to understand the role of oceanic aerosols, as part of the overall profile of emissions that contribute to air pollution, drive global climate change, and affect human health. His discoveries about atmospheric aerosols contribute to a scientifically based understanding of global climate dynamics, as well as these aerosols’ impact on terrestrial and marine ecosystems.\nThe ocean’s timeline\nTo understand modern-day climate change, we need to compare these dynamics to shifts in climate that occurred over the course of our planet’s history. Prof. Aldo Shemesh is clarifying the picture, using oxygen isotopes from sediments to reconstruct continental paleoclimate. He studies variations in such phenomena as surface water temperatures and rates of ice melting which, over time, impact concentrations of atmospheric carbon dioxide.\nProf. Shemesh is now developing two new approaches. First, he is analyzing oxygen isotopes left behind in the mineral skeletons of a family of marine protozoa called radiolaria. He is also examining the chemical content of Mediterranean reefs constructed out of marine mollusk shells. Such shells can be used to reconstruct past sea surface temperature, productivity, CO2 levels, and continental runoff. Together, these approaches provide an enriched timeline of the chemical changes that have occurred in the ocean, and will help to put present and projected climate trends into historical context.\nExtreme weather—the wave of the future?\nRising global temperatures are causing greater evaporation of water from the ocean, a dynamic that drives the formation of stronger and more frequent storms. Dr. Shira Raveh-Rubin studies the causes and dynamics of extreme weather. Her research focuses on the highly destructive cyclonic storms that often leave a trail of devastation, and which transport moisture and pollutants across long distances.\nWorking in collaboration with international teams, and using huge global meteorological datasets that contain observations collected over recent history, Dr. Raveh-Rubin hopes to develop new diagnostic tools for understanding specific features of weather systems. Her ultimate goal is to better identify the factors and mechanisms that determine the location, intensity, and frequency of extreme climate events such as intense droughts, floods, heavy precipitation, and dangerous storm systems. This in turn may lead to the development of early-warning measures that would improve our ability to prevent weather-related disasters.\nNot your grandmother’s Dead Sea\nThe time it takes to traverse exposed land to reach the water’s edge provides poignant evidence of the “retreat” of Israel’s Dead Sea from its previous wide expanse to its present, much-diminished dimensions. Dr. Yael Kiro is looking into the history of how this happened.\nA geochemist who uses chemical clues to read the environmental record preserved in groundwater, sediments, and rocks, Dr. Kiro has examined 1,500 feet of core samples from the bottom of the Dead Sea. An international study she co-led based on this data showed evidence of two “mega” droughts: one that began approximately 120,000 years ago when average global temperatures rose about four degrees Fahrenheit, and another about 10,000 years ago following the last ice age.\nWith current water levels in the Dead Sea dropping roughly one meter per year since the 1950s, and rainfall down about 10 percent on average, Dr. Kiro’s research suggests that the region could be headed for another serious dry spell. She is currently expanding her research to study the circulation of seawater and groundwater in aquifers beneath the coastline, including the Mediterranean.\nWith rising levels of greenhouse gas emissions from human activity, sea water absorbs more CO2 from the atmosphere. This lowers pH, causing a higher acid content in the ocean. One of the results of this ocean acidification is reduction in the concentration of calcium carbonate in marine ecosystems.\nDr. Assaf Gal studies how living organisms use minerals such as calcium carbonate to build structures ranging from the delicate shapes of single-celled marine algae to the bones of elephants. His work focuses on two groups of mineralizing marine algae called coccolithophores and diatoms, which live near the ocean’s surface, and produce their energy through photosynthesis. Dr. Gal discovered a previously overlooked organelle where calcium is stored in intracellular reservoirs, and also identified a number of macromolecular recognition mechanisms that control their crystallization. His work is clarifying the mechanisms of mineral deposition that drive the creation of calcium carbonate structures in marine organisms, and at the same time, is helping to characterize the role these organisms play in the overall balance of marine ecosystems.\nBacteria-algae ‘buddy system’\nMarine bacteria have been affecting the growth, development, and death of algae throughout the history of our planet. This is the research focus of Dr. Einat Segev. Geologists study algae because the chemistry of algae’s debris, deposited over millions of years, reflects the temperature of the ocean at the time that the algae were still alive. This makes algae a “paleo proxy” for climate scientists; they use the algal remains as a tool for estimating ancient conditions, and comparing them to current trends. In her postdoctoral research, Dr. Segev identified a symbiotic relationship between marine bacteria and algae that causes algae to “record” the wrong temperature.\nAfter creating a research model that accurately reflects algae-bacteria interactions, Dr. Segev is now studying how algal debris can be deciphered to reveal long-ago changes in ocean temperature. Research based on Dr. Segev’s model may contribute to our understanding of both historical climate trends, and the impact of climate change on microbial life at sea.\nProf. Assaf Vardi\nDr. Einat Segev\nProf. Ilan Koren\nDr. Yael Kiro\nDr. Assaf Gal"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:bd56d7fa-8a07-4bc6-bee2-30b83cdd4a74>","<urn:uuid:8f88086e-2897-4126-ba11-d901fad0399b>"],"error":null}
{"question":"Which former industrial site is older - the Bethlehem Steel plant or the Belgian textile factory building?","answer":"Both industrial sites began operations in the 1880s. The Belgian textile factory building dates from 1880, while the Bethlehem Steel's Bethlehem Plant has been in operation since the 1880s.","context":["LocationKapellestraat 3, 9950 Waarschoot, Belgium\nGeneral ContractorMark De Buck, Eeklo, Belgium\nGlass ProviderGama Glas, Bassevelde, Belgium\nFire PlaceStüv, Belgium\nThe building envelope ≠ the insulation envelope : the interspace between these two shells is ‘covered outdoor’ space.\nThe interspace allows/obliges inhabitants to use the space differently each season.\nIn summer and spring the interspace is occupied as living space (covered from rain) and in winter there’s only the ‘cocoon’ that needs to be heated.\nThe covered outdoor space also allows a flexible use in time which is a way of sustainable living / building.\nUnits can be added, removed or repositioned to create a different constellation and use of space.\nThe covered outdoor space allows the use of materials and building details in a way that wouldn’t be possible outdoors in all weather conditions.\nIn a ‘normal’ building the façade and roof must protect inside space from precipitation and at the same time insulate outdoor versus indoor. This implies a certain way of detailing the construction.\nIn the case of working in this covered space this is not the case, which makes construction details very interesting and unconventional.\nWe (my boyfriend and I) divided the factory of 800 m2 in two parts:\n- one side is where we construct our house.\n- the other side is workshop.\nIt is inside the workshop that we made the living units as a temporary living space meanwhile we are building our final house. The idea was to make a livable space but meanwhile not having to determinate to much and narrowing our possibilities for any future use of the workshop.\nThe light in the mobile units comes through the polycarbonate plates on the top. There are no windows in the walls so that when you close the door of the unit you appear to be in a cocoon. Possibly not inside the factory… (This was a psychological advantage because when you worked all day on het final house you want to relax without having to see it all the time…)\nOn the contrary office that I made in the workshop is definite and very open.\nPossibly I will make some additions to it in the future inside the workshop when my architecture office expands.\nINHABITANTS CREATE OPPORTUNITIES\nMy boyfriend and I are the inhabitants.\nHis specific demand for a workshop (to work on motorbikes, cars, etc. as a secondary activity) and my request for an architecture office space have been determinative in our search for a premises.\nWe are both very interested in (old) industrial architecture.\nOur choice of constructing our final house ourselves (long term and phased project) resulted in the fact that we needed a temporary house.\nAt the end of 2011, my office was the first thing that we added to the building. In May 2012 we moved to the temporary house.\nThe roofs and exterior walls were (some still are) in really bad shape when we bought the building. The walls have a lot of ‘rising humidity’ and some were almost falling apart.\nThat is a reason why we chose to make living units that stand alone, loose / independently from the exterior walls and roof. This way the exterior walls become ‘garden walls’ and the newly added spaces can have the comfort/convenience of a new modern space.\nThis allows us to phase the execution of the whole project.\nIn 2010 we were able to buy the building and the lot at the value of the patch.\nIn the eyes of the seller/ previous owner the building did not have any value. He even didn’t want to show us the inside because he thought it wasn’t relevant. It had to be demolished anyway!\nMy boyfriend an I immediately fell in love with the cast iron columns and the shed roofs with their northern rooflights. The building dates from 1880 and was the oldest building that made part of a textile factory that employed about 2000 people. Nowadays a lot of beautiful other buildings of the factory are demolished.\nIt was already clear to us, the first time we visited the space, that we would work with the existing space and characteristics.\nWitness of the rich history of the building are the many inscriptions of names and dates that we encountered on the columns, trusses and beams.\nThat is one of the reasons we didn’t want to sand-blast the whole interior and give it a new layer of paint.\nI think the building should be livable and adapted to modern comfort standards but also show it’s true age and patina.\nThe contrast between the new additions and old patina is very interesting.\nTo construct my office space I used KERTO-beams. This is a Scandinavian product, related to plywood, that is used as an alternative for wooden beams for example in a wooden floor.\nI used them in a rather unconventional way as a sequence of inverted U-shaped frames. In between them are bookshelves of Polish deal plywood and insulating glass panes.\nSeeing the KERTO’s on the head they seem really thin. Seeing them on the side, they have a certain mass. I like the different perception from a different angle.\nThe construction detail of the KERTO’s in combination with the ‘cold posted’ (siliconed) glass panes would never be possible / durable in an exterior environment. This is an example of a very specific construction detail for this (sheltered) environment.\nOn one side of the office space I used ‘hammered’ (ribbed) glass panes. This is a reference to the type of glass panes of the existing rooflights. They admit light but make a visual barrier between my part of the workshop and the part of my boyfriend.\nThe starting point of the project, the reuse of an old building, is a form of sustainable thinking.\nEvery addition to this existing building can be removed in the future, leaving the existing building and structure unharmed.\nLeaving certain spaces open and undefined allows us to adapt the building to functional changes and future ideas.\nAll the wood that we used for the new additions is FSC-labeled.\nI like the fact that the practical side of building had a repercussion on the form and size of things. We also did a lot of construction ourselves, which has practical consequences.\nMy boyfriend and I built the temporary mobile living units ourselves in two weekends.\nI designed them as a very basic, simple construction (on wheels!) that was easy and quickly to build.\nI opted to design these units using the standard measurements of an OSB-fiberboard. The advantage of doing so was that we didn’t have to saw the plates and didn’t have any wood waste.\nWe made the walls flat on the ground and then erected them and fastened them to the wooden floor on wheels. It like making a house of playing cards.\nThe total size of the units is also determined by the distance of the cast iron pillars in the workshop. It fits perfectly in between them so we can relocate the units any time we want.\nAs in any other architecture project the construction details of my office space are the result of several consultations with the contractor and many practical considerations.\nThe glass panes couldn’t be much larger then the ones on the front of the office space because the contractor had to carry them inside by hand.\nI try to imply the ‘big idea’ (of an intermediate space between inside and outside) in all of my projects as an architect, although I don’t always have the square meters (or surface) to do so in the way that we did here.\nI’m convinced that an intermediate space between inside and outside can be a very valuable addition to a ‘traditional’ house, even in a Belgian (rainy) climate!\nThis is already the case with a lot of Flemish houses and their typical ‘verandas’, although I’m convinced that there can be a more valuable solution than this.\nFor example architects Lacaton & Vassal use this idea of intermediate space between inside and outside to create very relevant architecture that focuses on the use of a house according to the seasons.","ISG Tecumseh Redevelopment Inc.\n(Formerly: Bethlehem Steel Corp.)\nOn This Page\nEPA Project Manager\nMs. Linda Matyskiela\nU.S. Environmental Protection Agency - Region III\n1650 Arch Street\nMail code: 3LC30\nPhiladelphia, PA 19103-2029\nPhone: (215) 814-3420\nRCRA Corrective Action activities at the Bethlehem Steel Works facility are being conducted as a joint lead by EPA and PADEP’s Land Recycling Program (Act 2).Environmental Investigation and clean-up are proceeding in concert with the redevelopment and sales of individual parcels of the property. Investigation and remediation, approved by both EPA and PADEP occurs before re-sale. As each of the parcels is sold, an environmental covenant is signed by the new and former owners, detailing the type of land-use and groundwater use controls appropriate for the parcel. This covenant runs with the land and binds each new owner to its restrictions.\nLocated on the south side of Bethlehem, PA, Bethlehem Steel’s Bethlehem Plant has been in operation since the 1880s. BSC closed its last operations in March, 1998. The site, part of which borders then Lehigh River, consists of approximately 1650 acres, on which are a coke production plant, steel iron making operations, finishing and forging operations, and a chemical plant. In May 2003, Bethlehem Steel Corporation went into bankruptcy and this Bethlehem Steel site was sold to an International Steel Group subsidiary, Tecumseh Redevelopment Corp. In May 2004, the Coke Works, Saucon, East Lehigh, and Greenway parcels were sold to Lehigh Valley Industrial Park, Inc, a local redevelopment firm and owners of several industrial parks. In 2004, Tecumseh Redevelopment sold Bethlehem Works to BethWorks Now. Tecumseh continued to own the 450 -Acre area. In 2004, a Dutch steelmaker, Mittal Steel, purchased ISG, including Tecumseh and the 450-Acre area. In December 2007, Majestic Realty Co. and Bethlehem Commerce Center Corp. purchased the 450-Acre area for warehousing.\nEPA, PADEP and Bethlehem Steel Corporation-Bethlehem Plant (BSC) formed a team in 1998 to proceed with the clean-up of the facility. The facility is subject to RCRA corrective action, however, BSC’s redevelopment plans hinged on obtaining a release from liability which is available for remediation performed under the Pennsylvania Act 2 Land Recycling Program. The redevelopment plan is designed to revitalize South Bethlehem, which has been in an economic slump since BSC started downsizing in the 1970's.\n- Some of the site’s key documents of interest are accessible below:\n- Environmental Indicator Determination - Human Exposures [PDF, 6 pages, 25 KB, About PDF]\n- Corrective Action Statement of Basis[PDF, 29 pages, 17.25 MB, About PDF]\n- PA DEP Environmental Covenant - Deed Restrictions Artsquest Parcel [PDF, 20 pages, 866 KB, About PDF]\n- PA DEP Environmental Covenant - Deed Restrictions Majestic Parcel [PDF, 13 pages, 922 KB, About PDF]\n- PA DEP Environmental Covenant - Deed Restrictions Lots 6 & 7 [PDF, 10 pages, 803 KB, About PDF]\n- PA DEP Environmental Covenant - Deed Restrictions Lot 29 [PDF, 9 pages, 638 KB, About PDF]\n- PA DEP Environmental Covenant - Deed Restrictions -LVIP -1235 Easton Rd[PDF, 20 pages, 880 KB, About PDF]\n- PA DEP Environmental Covenant - Deed Restrictions -LVIP -1245 Easton Rd[PDF, 11 pages, 626 KB, About PDF]\n- Documents and reports regarding this facility also can be reviewed in person at these locations:\nU.S. EPA Region III\nLand & Chemicals Division\n1650 Arch Street-11th Floor\nPhiladelphia, PA 19103\nCall for an appointment.\n- Submit a FOIA Request\nGet instructions on how to submit a FOIA request. Additional fee for requests over 100 pages.\n|ISG Tecumseh Redevelopment Inc. -Former Bethlehem Steel||ISG Tecumseh -Former Bethlehem Steel Geospatial PDF Map [PDF, 1211 KB, 1 page, About PDF]||ISG Tecumseh -Former Bethlehem Steel Geospatial Bethworks PDF Map [PDF, 1302 KB, 1 page, About PDF]|\nClick on a thumbnail to enlarge the photo or GeoSpatial PDF Map )\n- The EPA is dedicated to providing you with timely and accurate information about our work at this site. If you have any questions or concerns, please contact EPA Project Manager: Ms. Linda Matyskiela (215) 814-3420."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:5b381412-395b-4cda-96d9-c5b6eab134d3>","<urn:uuid:e6223633-dbe7-4333-b601-15cbec5409c7>"],"error":null}
{"question":"Who has conducted more research related to environmental sustainability: the bioremediation expert studying petroleum degradation or the professor developing sustainable concrete?","answer":"While both scientists work on environmental sustainability, Dr. Findlay's research focused on biodegradation of petroleum, coal tar hydrocarbons, and chlorinated solvents for environmental cleanup, including developing treatment processes for contaminated soil and groundwater. Prof. Bernal's sustainability work centers on developing low-carbon concretes, waste management and valorization in construction materials, and leading projects like the UKRI Interdisciplinary Circular Economy Centre For Mineral-based Construction Materials. Both contribute significantly to environmental sustainability but in different domains.","context":["Margaret Findlay Ph.D.\nDr. Findlay has been president of Bioremediation Consulting since its founding in 1991. She has over fifteen years experience in biodegradation research and direction of treatability laboratories, as well as in site remediation. Dr. Findlay currently specializes in design and execution of laboratory microcosm studies of ground water containing chlorinated solvents. As an editor of Bioremediation Journal, she maintains a broad overview of the field of environmental remediation.\nBiodegradation of Chlorinated Solvents. Dr. Findlay directed and participated in research projects funded by industrial clients, as well as by USEPA and National Science Foundation, on the biodegradation of chlorinated solvents, both by anaerobic reductive dechlorination and by aerobic degradation by methanotrophic bacteria. The work resulted in elucidation of biodegradation pathways, development of simplified laboratory treatability protocols, and conceptual designs for bioreactors and in situ groundwater treatment. She was co-author of the first paper on methanotrophic degradation of TCE and related ethenes, and a paper on a microcosm test method for evaluating natural attenuation of PCE by anaerobic bacteria.\nHydrocarbon Biodegradation. Dr. Findlay has conducted research and treatability studies on biodegradation of petroleum and coal tar hydrocarbons, designed soil treatment and compost processes for site remediation, and managed field monitoring programs. Dr Findlay worked with a major petroleum company to design a petroleum sludge composting project which received a commendation from the State of Utah. She also teamed with a major petroleum refinery to design and manage the soil treatment process for the first successful closure of a refinery landfarm. Dr. Findlay conducted bench scale research for the Petroleum Environmental Research Forum to determine the fate of petroleum spilled on soil. It was shown, using 13C, that during biodegradation a portion of the petroleum carbon is covalently incorporated into the soil organic matter. Biodegradation of 14C-benzo(a)pyrene to 14CO2 was also demonstrated.\nEducation, Affiliations and Employment\nPh.D. Chemistry. University of Illinois at Urbana, 1970. (minor in microbiology)\nB.A. Chemistry. Syracuse University, 1963. (minor in biology)\nBioremediation Journal (editorial board). American Chemical Society (member)\n1991 - present Bioremediation Consulting Inc, President\n1989 - 1991 Bioremediation Systems Division, ABB-Envir Services, Wakefield MA. Director of Treatability Laboratory.\n1984 - 1989 Bioremediation Systems Division, Cambridge Analytical. Director of Research and Development, Senior Scientist.\nFindlay, M. et al. (2002). “Optimizing Reductive Dechlorination in a Large-Scale In Situ Bioremediation System”. Third International Conference, Remediation of Chlorinated and Recalcitrant Compounds. Monterey, California, May 20-23, 2002.\nFindlay, M. and S. Fogel. (2000). “Reductive Dechlorination in High Sulfate Ground Water: A Microcosm Study. Second International Conference, Remediation of Chlorinated and Recalcitrant Compounds. Monterey, California, April-May, 2000.\nFindlay, M. and S. Fogel. (2000). Microcosm Test for Natural Attenuation of Chlorinated Solvents. Soil, Sediment & Groundwater. Feb/Mar p. 13.\n(Findlay)Fogel, M., A. Taddeo, and S. Fogel. 1986. Biodegradation of Chlorinated Ethenes by a Methane-Utilizing Mixed Culture. Applied and Environ. Microbiology. vol 51 p 720.\nFindlay, M., A.C. Leonard, S.Fogel, W.H. Mitchell. 1995. A laboratory biodegradation test to define in situ potential for PCE transformation. Third International In Situ and On Site Bioreclamation Symposium. Vol 3(8) Microbial Processes for Bioremediation, R.E.Hinchee, Vogel, C.M., and Brockman, F.J.,eds., 223-229. Battelle Press, Columbus OH.\nLeahy, M., M. Findlay, S. Fogel. 1989. Biodegradation of Chlorinated Aliphatics by a Methanotrophic Consortium in a Biological Reactor. HMCRI Superfund Conference, Washington, D.C.\nDooley, M., S. Fogel, and M. Findlay. l989. Sequential Anaerobic/Aerobic Biodegradation of Chlorinated Ethenes in an Aquifer Simulator. Am. Soc. for Microbiology. New Orleans.\nFogel, S, M. Findlay, J. Kerr, and S. McMillen. 1998. Incorporation of petroleum carbon into soil organic matter during biodegradation. Bioremediation Journal. 1 (3): 233.\nFindlay, M. and S. Fogel. 1997. Fate of petroleum carbon during biodegradation. Presented at the Fourth International In Situ and On-site Bioremediation Sympiosium, New Orleans, Apr 28. Battelle Press, Columbus, OH.\nKanaly, R. Bartha, R, Fogel, S, and M. Findlay. (1977). Biodegradation of 14C benzo(a)pyrene added in crude oil to uncontaminated soil. Appl. Environ. Microbiol. 63 (11): 4511.\nFindlay, M., S. Fogel, and A. Taddeo. 1995. \"Field Treatment of Coal Tar Contaminated Soil\" in Microbial Transformation and Degradation of Toxic Organic Chemicals. L. Young and C. Cerniglia, eds. John Wiley & Sons.\nNordrum, S., O. Fyock, & M. Findlay. 1992. Treatment of production tank bottom sludge by composting. Annual Technical Conference of the Society of Petroleum Engineers. Wash. DC.\nTaddeo, A., M. Findlay, M. Danna, S. Fogel. 1989. Field Demonstration of a Forced Aeration Composting Treatment for Coal Tar. HMCRI Superfund Conference, Dec., Washington, D.C.\nFogel, S., M. Findlay, C. Scholl, & M. Warminsky. 1995. Biodegradation and bioavailability of bis(2-ethylhexyl)phthalate in soil. in Third International In Situ and On Site Bioreclamation Symposium. Vol 3(7) Bioremediation of Recalcitrant Organics. R.E. Hinchee, Heoppel, R.E., and Anderson, D.B. , eds. 315-322. Battelle Press Columbus, OH.\nFindlay, M., S. Fogel, & J. Borovsky. 1993. Bioremediation of a Petroleum-Contaminated Soil from a Crank Case Oil Refining Site. in Hydrocarbon Contaminated Soils, Vol. 4. Amherst MA, E.J.Calabrese, et al, eds. Univ. Mass. Amherst MA.\nChardhuri, I., M. Findlay, et al. 1992. Use of Risk Assessment to Assess the Effectiveness of Bioremediation. in Hydrocarbon Contaminated Soils, Vol 3, U Mass, Amherst MA.\nFogel, S. and M. Findlay. 1988. Bioremediation Techniques for In-Situ Treatment of soil and Water. 2nd Annual Northeast Underground Storage Tank Management and Hydrocarbon Contaminants Clean-up Conference. Sturbridge, MA. April.\nFindlay, M. and S. Fogel. Crude oil biodegradation: Mass balance used to validate gravimetric and GC methods. ibid.","Prof. Susan A Bernal\n- Position: Professor in Structural Materials\n- Areas of expertise: Traditional and alternative cements; cement materials characterisation; wastes management and valorisation; durability of cement and concrete\n- Email: S.A.BernalLopez@leeds.ac.uk\n- Location: Civil Engineering Building, room 1.11\n- Website: LinkedIn | Googlescholar | Researchgate | ORCID\nSusan holds the Chair in Structural Materials in the School of Civil Engineering at University of Leeds, and a prestigious EPSRC Early Career Fellowship in Multi-scale Engineering of Sustainable Concretes. Previously she was University Academic Fellow in Cementitious Materials at Leeds (2018-2019), and Research Fellow in Cements in the world leading NucleUS Immobilisation Science Laboratory in Department of Materials Science and Engineering at The University of Sheffield (2012-2018). During that period she also held a one year appointment (2015-2016) as Lecturer in Concrete Technology in the Civil and Structural Engineering Department of this University. Prior to this, she was a Postdoctoral fellow (2009-2010) with Professor Jørgen Skibsted in the iNANO Instrument Center for Solid-State NMR Spectroscopy, Chemistry Department at Aarhus University, Denmark, and she then undertook a Research Fellow position (2010-2012) focusing on durability assessment of alkali-activated concretes, with Professor Jannie van Deventer and Professor John Provis in the Chemical Engineering Department at the University of Melbourne, Australia.\n- Recipient of the 2020 IOM3 Rosenhain Medal and Prize for distinguished achievements in any branch of materials science – read more here\n- Recipients of one of the 2018 ICE Advances in Cement Research Prize (Best paper in the journal) - see paper here\n- Recipient of one of the two 2016 RILEM Gustavo Colonnetti medals for outstanding scientific contributions to the field of construction materials and structures\n- Recipient of one of the 2016 Distinguished Graduate Awards, Faculty of Engineering, Universidad del Valle, Colombia, in recognition of outstanding scientific contributions promoting regional development.\n- Recipient of one of the Walter Mangold Trust Fund scholarship for studies in Australia, 2008. The programme provided funding for a 1-year occupational trainee position in The Department of Chemical and Biomolecular Engineering at The University of Melbourne, Australia.\n- Recipient of a COLCIENCIAS DEng Scholarship, 2005: Enhancement of Colombian Scientific Community through National Doctorate Programmes. (Fortalecimiento de la comunidad científica a través del programa de doctorados nacionales, Créditos condonables COLCIENCIAS).\n- Deputy-Chair of the RILEM technical committee TC 281 – CCC: Carbonation of concrete with supplementary cementitious materials, since 2017.\n- Member of the RILEM technical committee TC MPA: Mechanical properties of alkali-activated concretes, since 2019.\n- Member of the RILEM technical committee TC 283 – CAM: Chloride transport in alkali-activated materials, since 2018.\n- Member of the RILEM technical committee TC 282 – CCL: Calcined clays as supplementary cementitious materials, since 2018.\n- Member of the RILEM technical committee TC 267-TRM: Tests for reactivity of supplementary cementitious materials, since 2015.\n- Member of the RILEM technical committee TC 247-DTA: Durability testing of alkali-activated materials, since 2013-2018.\n- Member of the RILEM technical committee TC 238-SCM: Hydration and microstructure of concrete with supplementary cementitious materials, 2012-2015.\n- Member of the RILEM Technical committee TC 224-AAM: Alkali-activated materials, 2011-2012.\n- Reviewer for more than 50 international journals (in both Spanish and English) in the areas of materials science and engineering of infrastructure materials, cement and concrete chemistry and performance, nuclear waste management, and sustainability.\nCommisions of trust\n- Materials and Structures Group Director, School of Civil Engineering, University of Leeds. Since 2020\n- Member of the Management Committee of the Bragg Centre for Materials Research at University of Leeds. Since 2019\n- Member of the EPSRC Peer Review College. Since 2019\n- Associate Editor, Materials – Open Access Journal of Materials Science. Since 2018\n- Associate Editor, RILEM Technical Letters. Since 2016\n- Editorial Advisory Panel member, Proceedings of the Institute of Civil Engineers (ICE) Construction Materials. 2016 – 2020.\n- 2021 – 2024. UKRI Interdisciplinary Circular Economy Centre For Mineral-based Construction Materials. EPSRC grant (EP/V011820/1). U. Leeds investigators: L. Black, O. Iuorio, C. Velis & S.A. Bernal; in collaboration with colleagues from UCL, Loughborough U., Imperial College London, Lancaster U. & U. Sheffield.\n- 2021 – 2023. Transforming Foundation Industries – A Network plus towards value by innovation. EPSRC / ISCF grant (EP/V026402/1). Investigator: I. Reaney (U. Sheffield, Director), S.A. Bernal (U. Leeds, Co-Director), W. Sampson (U. Manchester) and C. Preydell-Pearce (Swansea U.)\n- 2021 – 2022. Enhancing analytical capabilities in soils for low-carbon technologies. White Rose Collaboration Fund. Investigators U. Leeds: A. Marsh (PI), H. Freeman, A. Brown; V. Leadley; S.A. Bernal; U. Sheffield: B. Walkley (Co-PI), M. Stennett, N. Hyatt; U. York: M. Hodson (Co-PI), R. Mills.\n- 2020 – 2022. WISE: Maximising waste resources utilisation in future infrastructure development. Royal Society – International Exchanges Cost Share (Argentina). UK Investigators: S.A. Bernal (UK-PI), L. Black, S. Adu-Amankwah, A. Marsh, J.P Gevaudan; Argentina Inverstigators: Y.A. Villagran-Zaccardi (Argentina PI), M.A. Sosa, C. Pico Cortes, L. Masselli\n- 2020 – 2023. CMMI-EPSRC RENACEM: Response to CO2 exposure of concrete with natural supplementary cementitious materials. NSF-EPSRC Lead Agency grant (EP/T008407/1 and 1903457). UK Investigators: S.A. Bernal (UK-PI), J.L. Provis, L. Black & P.A.M. Basheer; US Investigators: M. Juenger (US-PI) & L. Katz\n- 2019 - 2022. Design-for-manufacture of 3D concrete printed structural composites (DfM:3DCP). EPSRC grant (EP/S019650/1). Investigators: L. Susmel, R. de Borst, J.P. Provis & S.A. Bernal\n- 2019 - 2021. PERFoRM. Passive layer failure mechanisms for steel embedded in alkali-activated slag materials. Marie Sklodowska-Curie Individual Fellowship. Investigators: J.P. Gevaudan (Fellow), S.A. Bernal & P.A.M. Basheer (Supervisors)\n- 2019 – 2021. NOVA-VIDA: Novel approach for vital infrastructure post-disaster. British Academy – Infrastructure for wellbeing grant. Investigators: O. Iuorio, M. Janoschka, R. Romano & S.A. Bernal\n- 2018 - 2023. Multicale engineering of alkali-activated concretes for sustainable infrastructure. EPSRC Early Career Fellowship (EP/R001642/1). Investigator: S.A. Bernal\n- 2016 - 2019. Predicting long-term performance of cement disposal systems for radionuclide-loaded zeolite and titanate ion exchangers. EPSRC-MEXT grant within the scope of Phase 3 of the UK-Japan Civil Nuclear Research Programme (EP/P013171/1). Investigators: J.L. Provis, H. Kinoshita, S.A. Bernal.\n- 2016. REFURB. Resource futures for sustainable urbanisation. The University of Sheffield/ EPSRC GCRF (seeding fund). Investigators: S.A. Bernal, S. Marvin, M. Mayfield, D. Densley-Tingley, P. Styring, S. Schindler & A. While.\n- 2016-2019. Immobilisation of problematic wastes in geopolymer. Nuclear Decommissioning Authority (NDA) Bursary. Investigators: D. Geddes (PhD researcher), J.L. Provis & S.A. Bernal.\n- 2016. Novel lightweight plaques and panels based on geopolymer composites. The University of Sheffield IIKE funding. Knowledge exchange project in collaboration with Suministros de Colombia S.A (SUMICOL). Investigators: J.L. Provis, S.A. Bernal, M. Bach.\n- 2015. WASTECEM. Wastes as resources for producing eco-efficient cements. Newton Fund Fellowship (Brazil-UK), Royal Academy of Engineering. Investigators: J.L Provis, S.A. Bernal, E.D. Rodriguez & A.P. Kirchheim.\n- 2014-2017. Rational design of low-CO2 alkali-activated concretes for eco-efficiency and durability. EPSRC-NSFC grant for collaborative research between UK and China on Sustainable Materials for Infrastructure (EP/M003272/1). Investigators: J.L Provis, S.A. Bernal, M. Basheer, M. Soutsos, & S. Nanukuttan.\n- 2013. Bulk use of biomass and co-fired ash in novel binders. NERC catalyst grant (NE/K015680/1). Investigators: A. Heath, J.L. Provis, S.A. Bernal, H. Kinoshita, K.A. Paine, R.J. Ball, M.C. McManus.\nSusan and her team research centres on novel cements and concretes, particularly focusing on waste management and valorisation, multi-scale infrastructure materials characterisation, durability assessment, and improving sustainability of novel cement and concrete technologies. Specifically Susan focuses on:\n(i) Development, characterisation and exploitation of advanced and non-traditional cement and cement and concrete technologies, including:\n- Design of bespoke cements including geopolymers and alkali-activated materials for waste management and/or valorisation.\n- Determining the factors controlling phase assemblage evolution and stability of non-Portland cements, including application of thermodynamic modelling.\n- Mechanisms leading to structural modifications of Portland and non-Portland cements exposed to aggressive environments, emphasising chemical/physical binding of anions/cations and structural phase transformations.\n- Validation of testing methods for physicochemical characterisation of supplementary cementitious materials\n- Development and validation of standardised testing methods to determine performance of alkali-activated materials\n(ii) Evaluation of fit-for-purpose cementitious wasteforms for immobilisation of nuclear wastes, including:\n- Chemical/physical binding mechanisms of radionuclides in cements with different chemistries\n- Phase assemblage evolution and stability of Portland and non-Portland based cementitious wasteforms\n- Effect of radiation in cementitious systems\nCurrent team members\n- Rutendo Rusike (UG Laidlaw Scholar\n- Yuyan Huang (PhD researcher)\n- Thomas Hutchinson (EPSRC CDT GREEN PhD researcher)\n- Moro Sabtiwu (EPSRC CASE PhD researcher – Highways England)\n- Zengliang Yue (PhD researcher)\n- Heng Song (PhD researcher, co-supervised by Dr. Gehan Selim)\n- Dr Alastair Marsh (Research Fellow in Alkali-Activated Materials)\n- Dr Juan Pablo Gevaudan (Marie Sklodowska-Curie Individual Fellow in Corrosion of Steel in Alkali-Activated Materials)\n- Dr James Vigor (Research Fellow in Advanced Cementitious Materials Characterisation)\n- Dr Yuvaraj Dhandapani (Research Fellow in Durability of Cement and Concrete Materials)\nFormer team members\n- 2019 – 20 - Luke Atkinson (MSc)\n- 2019 – 20 - Moosa Awad (MSc)\n- 2019 – 20 – Dr Tao Yang (Visiting Research Fellow)\n- 2019 – 20 - Xiaowen Zhang (Visiting PhD researcher)\n- 2018 – 19 - Kate Button (MEng)\n- Doctor in Engineering, emphasis in Materials Engineering (Cum Laude) Universidad del Valle, Colombia\n- Materials Engineer (Cum Laude), Universidad del Valle, Colombia\n- Fellow of the Institute of Materials, Minerals and Mining, IOM3\n- Senior Member of International Union of Laboratories and Experts in Construction Materials, Systems and Structures, RILEM\n- CIVE5452 – Cement and Concrete Properties\nCurrent postgraduate researchers\n<li><a href=\"//phd.leeds.ac.uk/project/825-formation-mechanisms-of-corrosion-resistant-oxide-films-in-steel-reinforced-low-carbon-concretes\">Formation mechanisms of corrosion-resistant oxide films in steel reinforced low-carbon concretes</a></li>"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:e8579bb5-3e65-4b5f-be5f-2ef52da80580>","<urn:uuid:1f8c9d38-bfdb-46b8-84d8-c782dcc07aa8>"],"error":null}
{"question":"How do workplace sanitizers vs. sterilization methods compare in killing germs and bacteria? Need to understand the differences!","answer":"While hand sanitizers are effective in eliminating 99.9% of germs through a simple application process, sterilization methods are more comprehensive industrial processes. Moist heat sterilization kills microbes by coagulating their proteins and enzymes under steam pressure, while dry heat sterilization destroys microbes through oxidation of proteins at high temperatures. Sanitizers are for routine workplace hygiene, while sterilization is a more intensive process for thoroughly destroying all microorganisms on equipment and instruments.","context":["The workplace needs to be safe and hygienic for employees and visitors. An unsafe workplace can leave your business at risk of hazards, resulting in you facing personal injury claims from injured employees—or worse, property damages. On the other hand, an unclean workplace can lead to more sick days and hamper employee morale and productivity.\nWhile many business owners are quick to adopt stringent measures to forestall office theft, potential fire, and electrical hazard, some often ignore or play down the importance of workplace hygiene. Workplace hygiene is essential as it helps ensure that the workplace is healthier, more productive, and happier.\nIf your workplace lacks proper hygiene, here are some of the best tools to purchase.\n1. Hand Sanitizers\nDuring the peak of the coronavirus infection, the few operating businesses prioritized the use of hand sanitizers. These antiseptic products were quite handy, and even today, having them in your workplace can help prevent the spread of germs.\nMake sure you keep bottles of hand sanitizers in strategic locations in your workplace, like desks, lockers, or lunchrooms for application during the day. Most sanitizers are effective in eliminating 99.9% of germs. In addition, they are portable and quite affordable.\n2. Paper towels\nBathrooms are considered the epicenter of cross-contamination. However, to ensure that yours isn’t, you should take necessary measures to prevent the build of germs. One of the most simple and effective measures to adopt is the provision of industrial paper towels in your workplace. By installing industrial dispensers in the wash-up areas like the bathroom, you can prevent cross-contamination. Ensure you encourage employees and visitors to use them after using the bathroom. You can put up signs if it helps.\n3. Microfiber cloths\nRather than use your hands to wipe off little dust from your chair or table, why not get microfiber cloth? These clothes are handy for dusting and usual clean-up. They are also effective in picking up more dust and bacteria than other wipes, keeping your workplace clean. After use, you can wash them with the washing machine and reuse them when appropriate.\n4. Multi-Purpose cleaner\nDoes your workplace have a multi-purpose cleaner? A multi-purpose cleaner is a must-have for every workplace. This cleaning product can be used to wipe desks, workbenches, kitchens, and bathrooms each day.\nWith multi-purpose cleaners, you can eliminate bacteria from surfaces and keep them sparkling clean. In addition, if you observe any foul odor in your workplace, a multi-purpose cleaner can be used to get rid of it.\n5. Hand Soap\nHaving hand sanitizers in the workplace can rid your hands of germs. However, you’ll need to get some hand soaps to wash your hands and prevent the spread of germs.\nAlways make plans to refill your hand soaps, so it doesn’t run out. Also, ensure that your employees are well aware of its availability, so they use it to promote good workplace hygiene practices.\nPrevention is better than cure! It’ll cost you less to ensure that your workplace has the appropriate tools to keep it hygienic and more to keep up with the absence of one or more staff who took sick leaves due to an illness contracted in the office.","When the process of sterilization is carried out at high -pressure through water (steam) it is called as moist heat sterilization, on the other hand, dry heat sterilization is carried out at high temperature under dry condition.\nThe best way to kill microbes is through ‘heat‘, as it destroys their proteins as well the enzymes present in them. So sterilization (destroying or killing the microbes) process follows this principle of killing microbes, which can be either by giving wet (moist) heat or dry heat.\nAs their name suggests, both the methods have the different mechanism of working to sterilize the types of equipment and it depends on the type of equipment to be sterilized. So this article is to know about the few, though important differences between the moist heat sterilization and dry heat sterilization.\nContent: Moist Heat Sterilization Vs Dry Heat Sterilization\n|Basis for Comparison||Moist Heat Sterilization||Dry Heat Sterilization|\n|Meaning||Sterilization involving lower temperature and high-pressure of water (steam) is known as Moist Heat Sterilization.||Sterilization involving the dry air of higher temperature and for the longer time is known as Dry Heat Sterilization.|\n|Process requires||As the name says, it needs steam and water.||There is no use of steam and water.\n|Sterilization is done by||Coagulating protein of the microbes very effectively.||Oxidation of the protein and other chemical bonds present in microbes.\n|This process is performed under pressure.||It is performed on direct flame.|\n|Autoclaving and Boiling come under moist heat sterilization.||Incerination, Bunsen burner (flame), hot air oven and microwave comes under dry heat sterilization.|\n|Required time to complete the process||Moist heat sterilization takes less time.||This process takes more time comparatively.\nDefinition of Moist Heat Sterilization\nMoist heat sterilization has its importance for sterilizing instruments like custom trays. In this type of sterilization the autoclave is used, where the pressure of the steam with relatively high temperature (though lower than the dry heat sterilization) is used.\nIn this method, the moist heat (steam) sterilize the equipment by denaturating the enzymes and the structural proteins of the harmful microbes present on the equipment and thus killing them. The moist heat destroys the proteins of the microbes by coagulating them. The required timing is about 15 minutes with the temperature of 121°C.\nAdvantages of the Moist Heat Sterilization\n- Requires low temperature.\n- Less time to complete.\n- Easy to control and monitor.\n- Low cost, non-toxic.\nDisadvantages of the Moist Heat Sterilization\n- The heat-sensitive instrument cannot be sterilized.\n- After sterilization, the instrument remains wet, which may lead to rust.\n- Chances of getting instrument damaged, due to repeated exposure.\nDefinition of Dry Heat Sterilization\nDry heat sterilization is one of the old technique to sterilize the glassware and other equipment. In this method, heated air of high temperature and further the process is same as that of conduction method. Heat is absorbed from the surrounding area of the equipment and is moved forward to the next layer, and slowly the whole equipment gets heated and attains sterilization. The time may vary from 1 hour to 2 hours along with the temperature of the 160°C to 170°C respectively.\nThe temperature is higher than the moist heat sterilization process; there are more chances of the microbes to get the kill. Incineration, the high flame are the different types of processes performed under dry heat sterilization.\nThe oxidation process is involved to destroy the microbes. Though this method takes longer time than the moist heat sterilization as due to the absence of water, high energy is required to break the peptide bonds of the proteins present in the microbes.\nAdvantages of Dry Heat sterilization\n- Reliable, nontoxic.\n- Low cost and is easy to install.\n- As the instrument remains dry after sterilization, there are no chances of corrosion.\n- Not harmful to the environment.\nDisadvantages of the Dry Heat Sterilization\n- It requires more time for sterilization.\n- Exposure to the higher temperature may be harmful to the instrument.\nKey Differences Between Moist Heat Sterilization and Dry Heat Sterilization\nGiven below are the points which present the key differences between the Moist Heat and Dry Heat Sterilization:\n- Sterilization involving lower temperature and high-pressure of water (steam) is known as Moist Heat Sterilization, on the counterparts sterilization including the dry air of higher temperature and for the higher time is known as Dry Heat Sterilization.\n- In the moist heat sterilization coagulation of protein and enzymes of the microbes are done very effectively, it also takes less time whereas in dry heat oxidation denaturation or killing of microbes is done by the process of oxidation of the protein and other chemical bonds present in microbes, and it takes more time to complete.\n- Autoclaving and Boiling come under moist heat sterilization, while Incineration, Bunsen burner (flame), Hot air oven and Microwave comes under dry heat sterilization.\nFrom the above article, we conclude that both are the acceptable process and can be performed according to the requirement and the conditions of the equipment to be sterilized."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:f5458ec9-4861-4f76-9eb8-9e4e8f0fdcc4>","<urn:uuid:dd4142de-07ef-40a3-88a4-429ec55aec11>"],"error":null}
{"question":"What's the typical yield of sweet potatoes per 100 ft bed using regular slip method?","answer":"Using the regular slips method, the average yield is 500-600 lbs of sweet potatoes per 100 ft bed, with yields ranging from a poor 250 lbs to occasional successes of 1000 lbs per bed.","context":["The April issue of Growing for Market is out! For those of you growing sweet potatoes, Andrew Schwerin from NW Arkansas has written an interesting article. I’ve written about starting sweet potato slips before and I have a slideshow that includes three methods of starting your own slips. He and his wife Madeleine grow 1500 feet of sweet potatoes each year, a third of their growing area.\nI was interested to note their reasons for growing so many sweet potatoes (apart from the obvious fact that they can sell that many). Sweet potatoes are not a big moneymaker in terms of the space occupied. Here at Twin Oaks we pondered similar issues this winter when deciding which crops to grow. We worked down a list of 25 factors, deciding which were important to use. We chose our top handful of factors and then worked down a list of crops we might grow, awarding points (or not!) for each factor for each crop. This helped us narrow down what to focus on this year. And yes, we are growing sweet potatoes! I wrote about this in Growing for Market in February 2017.\nThese growers listed the following factors as their reasons to grow sweet potatoes:\nSweet potatoes produce well in our soil\nThey aren’t troubled by intense summer heat\nExtensive vines will smother most weeds\nFew pest or disease issues\nMost of the labor is in early October, between intensive harvests of summer and fall crops\nThey store long-term for steady sales through the winter\nThey like Beauregard, and wanted to try using the single node cutting method, as advocated by Anthony and Caroline Boutard – see my Sweet Potato slideshow for details. Initially they were excited about the single node cutting process, as their roots produced exponentially more growing shoots each week. OK, maybe exponential is a bit of an exaggeration, but it gives the sense of it. Because they had so much propagation material, they started making 5-node slips, rooting clusters of cuttings in pots of compost, 3 nodes in the soil for roots, 2 nodes above ground.\nSome of their single node cuttings failed to thrive, both in the trays and in the field, so they developed a 2-node cutting system instead, and also used their five-node slips. And so they had a trial of three sweet potato cutting methods, with plants in different 100 ft beds. In the past (using the regular slips method) they have averaged yields of 500-600 lbs of sweet potatoes per 100 ft bed, with a range from a poor 250 lbs to a few successes with 1000 lbs/bed. Of course, yield is not the only important feature of a market crop, although understandably it has a high profile for those growing 1500 row feet for sale.\nAt harvest, they found that their single-node sweet potato plants were producing a couple of hundred pounds per 100 ft bed. The 2-node beds produced about 500 pounds per bed, of relatively few, very large (6 – 20 lb) sweet potatoes. The plants with 3+ nodes in the soil gave more reasonable sized potatoes. They tend to get jumbos, so they have started planting closer (10″) to tackle this – not many customers want jumbos.\nIn his article in Growing for Market, Anthony Boutard pointed out that single-node cuttings do produce fewer tubers which are larger and better formed. This is a big advantage for growers in the north, but less so in the south. The Arkansas growers have found that the 2-node cuttings are even better at this tendency in their location, which is much further south than Anthony Boutard’s farm.\nOther articles in this month’s Growing for Market include Managing a cash crisis\nHow to climb out of the hole by Julia Shanks, the author of The Farmer’s\nOffice. Farmers deal with a very seasonal cash flow, and may well have gone into farming with good farming skills but not good business skills. Julia writes about four rules for getting out of a financial hole.\n- Quit digging (don’t incur any more inessential expenses).\n- Keep the dogs at bay (communicate with your creditors about how you plan to pay, and how you plan to keep producing the goods).\n- Climb out (increase revenue in as many ways as possible).\n- Get your head out of the sand (don’t panic, face realities, be proactive).\nShe goes on to list 10 ways to protect yourself from getting in such a hole again.\nSam Hitchcock Tilton has an article about cultivating with walk-behind tractors, ie, weeding and hoeing with special attachments. There are some amazing walk-behind\nweeding machines (manufactured and homemade) throughout the world. There\nis an entire style of vegetable farming and scale of tools that have been forgotten, in between tractor work and hand growing – the scale of the walk-behind tractor. The author explains how commercially available tools can be adapted to work with a BCS or a carefully used antique walk-behind tractor.\nMike Appel and Emily Oakley contributed Every farm is unique, define success your own way. Money is not the only measure. Quality of life, family time, and personal well-being are up there too, as are wider community achievements. Farming is equal parts job and lifestyle, and the authors recommend having a strategic plan for yourself and the farm, which you update every couple of years to pinpoint goals and the steps you need to take to reach them.\nLiz Martin writes about husk cherries (ground cherries) and how to improve production of them, to make a commercial crop viable. Who would have guessed that hillling the beds before planting can make harvest so much easier, because the fruits roll down the sides?\nJudson Reid and Cordelia Machanoff wrotea short piece: Fertility tips and foliar testing to maximize high tunnel crops, and Gretel Adams wrote about Scaling up the flower farm. Many of the ideas also apply to vegetable farms."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:a7e55a9c-89a1-46ff-a433-ee4d903b2013>"],"error":null}
{"question":"¿Por qué is the copy number effect in CRISPR screens important to understand compared to AAV vector design challenges?","answer":"The copy number effect in CRISPR screens creates significant false positives and negatives in identifying essential genes in cancer cells, due to DNA damage responses in amplified regions. This makes it crucial to validate findings using alternative methods like RNAi. AAV vector design challenges are fundamentally different, focusing on improving the viral capsid's ability to deliver genetic material effectively to target cells, which is being addressed through artificial intelligence platforms like CapsidMap to create better-performing vectors.","context":["Several CRISPR/Cas9 screens identifying essential genes in cancer cell lines have been performed to date (Shalem et al., 2014, Hart et al., 2015, Kiessling et al., 2016). These typically take the form of pooled screens where sgRNA libraries targeting all genes or subsets of genes are introduced in parallel into Cas9-expressing cells, at a single sgRNA per cell. The sgRNAs exert a negative or positive selection pressure on cells based on their impact on cell viability and proliferation. The most depleted or enriched sgRNA sequences are determined by next-generation sequencing, revealing relevant gene ‘hits’. Very similar to how pooled shRNA screens are performed.\nFrom these screens, several groups have observed a worrying phenomenon: CRISPR gRNAs targeting genomic regions of high copy number amplification showed a striking reduction in cell proliferation/survival. Dr William Hahn’s group at the Dana Farber Institute was one of the first to characterize this in a publication last year involving a CRISPR/Cas9 screen on 33 cancer cell lines looking for essential genes. In total, 123411 unique sgRNAs were used targeting 19050 genes (6 sgRNAs/gene), 1864 miRNAs and 1000 non-targeting negative control sgRNAs.\nWhat they discovered is a little worrying to say the least.\nThe figure shows two genomic regions in two different cell lines (SU86.86 and HT29). At genomic coordinates highlighted by the red box, 3 tracks are shown. Top, copy number from the Cancer Cell Line Encyclopaedia (CCLE) SNP arrays, red indicating above average ploidy and blue showing below; middle, CRISPR/Cas9 guide scores with purple trend line indicating the mean CRISPR guide score for each CN segment defined from the above track; bottom, RNAi gene-dependency scores. AKT2 and MYC, known driver oncogenes at these loci, respectively, are highlighted in orange. For RNAi data, shRNAs targeting AKT2 used in Project Achilles were not effective in suppressing AKT2 (hence the negative result).\n- A striking enrichment of negative CRISPR guide scores (i.e. sgRNAs that reduced cell proliferation/survival) for genes that reside in genomic regions of high copy-number amplification.\n- Genes identified in CRISPR that reduced survival, did not have the same effect when disrupted by RNAi in the same cell lines (this RNAi screen was done by the same group but published 2 years before).\n- This enrichment was seen also for unexpressed genes, i.e. genes not transcribed. Meaning the reduced survival was not due to loss-of-function of the targeted gene.\n- Even for regions with low absolute copy numbers, a significant reduction in survival was observed compared to non-targeting control sgRNAs. Furthermore, the effect was dose-dependent with greater copy number amplifications producing larger negative CRISPR guide scores.\nNotably, the correlation between copy number and genes that were scored high on essentiality was also observed when looking at data from other studies (Hart et al., 2015). The “copy number effect” would therefore produce a high number of false positives in CRISPR screens for essential genes in cancer cell lines. The graph above shows just how big an effect this is. Comparing genes identified as essential in a CRISPR screen vs RNAi screen, increasingly essential CRISPR-identified genes were more likely to reside on copy number amplifications (defined as having average sample ploidy > 2). This effect was notably absent for RNAi-derived essential genes.\nAside from false positives, the increased noise due to “copy number effects” also increases false negatives. MET, a gene identified by shRNA screens, for example, failed to be picked out by CRISPR screens as it is located on a chromosome 7 amplicon (7q31) in MKN45 cells (gastric cancer cell line) where all other gRNAs within that amplicon also scored as essential.\nThe authors go on to explore mechanisms behind the “copy number effect”. They found it was attributed to a DNA damage response stimulated by excessive cutting by Cas9. This response appeared p53-dependent and induced cell cycle arrest at the G2 phase, explaining the anti-proliferative effect. A similar response was seen for promiscuous sgRNAs that cut at multiple sites, with effects being more pronounced when cuts were spread over several chromosomes as opposed to a single chromosome.\nHow to manage this?\nSo far, most simply avoid analysing hits where sgRNAs lie at amplified regions or target multiple sites (Wang et al., 2017). However, these regions of copy number amplifications have been implicated in cancer and may contain relevant hits. Several computational methods have therefore recently been developed to correct for “the copy number effect”. Hahn’s group developed a computational algorithm called CERES based on data obtained from CRISPR sgRNA screens in 342 cancer cell lines representing 27 cell lineages.\nNovartis also developed a Local Drop Out (LDO) algorithm that corrects obtained data based on examining gRNAs scores at direct genomic neighbours. When multiple neighbouring genes show similar drop out scores, effects are assumed to be due to “copy number effects”. This method has the advantage of not requiring prior knowledge of copy number, however it does require a sufficient density of gRNAs to accurately capture “copy number effects”. They also had an alternative method, Generalized Additive Model (GAM) where copy number was taken into account.\nHow the CERES Model Works\nThe Results – copy number dependency is reduced while preserving essentiality of cancer-specific genes such as KRAS\nA step towards the right direction but the penetrance of this effect still raises some concerns:\n- Although false positives are reduced with these computational methods, it is difficult to recapture false negatives. This is dependent on the gRNA having a stronger phenotype compared to neighbouring gRNAs on the amplicon which is not always the case. The LDO method for example still failed to recapture MET.\n- Guide scores can vary with cell line, sgRNA and experimental conditions, making it difficult to apply the same counter-measures to every experiment.\n- Given multiple cut sites trigger the same effect, how do we ensure multiple sgRNAs when introduced into a cell are not inducing a similar response? This is difficult to control in pooled screens, and poses a limitation in multiplex screens. Synthetic lethality screens for example with sgRNAs targeting multiple genes, might be subject to a higher false positive rate.\n- With even diploid genes (copy number = 2) having statistically significant growth reduction compared to haploid gene loci, the challenge still remains to delineate a true loss-of-function over a non-specific cellular response.\n- Negative sgRNA controls have to be carefully selected. From the study, non-targeting controls had little impact on viability compared to most other sgRNAs. Controls targeting non-expressed genes or non-essential loci have been recommended as better controls.\n- Lastly, although this effect seems to apply mostly to cancer cell lines that undergo a high rate of gene amplifications, similar effects may extend to polyploid tissues such as the liver.\nHence as always gene function should be determined by a variety of methods. Using RNAi for example to affirm a CRISPR-knockout phenotype would add greater confidence to a hit. To avoid those RNAi-related false positives however, its probably best to use siPOOLs.\nSource of figures:\nAguirre, A. J., Meyers, R. M., Weir, B. A., Vazquez, F., Zhang, C.-Z., Ben-David, U., … Hahn, W. C. (2016). Genomic Copy Number Dictates a Gene-Independent Cell Response to CRISPR/Cas9 Targeting. Cancer Discovery, 6(8), 914 LP-929.\nMeyers, R. M., Bryan, J. G., McFarland, J. M., Weir, B. A., Sizemore, A. E., Xu, H., … Tsherniak, A. (2017). Computational correction of copy-number effect improves specificity of CRISPR-Cas9 essentiality screens in cancer cells. bioRxiv. Retrieved from http://biorxiv.org/content/early/2017/07/10/160861.abstract\nOther relevant sources:\nMunoz, D. M., Cassiani, P. J., Li, L., Billy, E., Korn, J. M., Jones, M. D., … Schlabach, M. R. (2016). CRISPR Screens Provide a Comprehensive Assessment of Cancer Vulnerabilities but Generate False-Positive Hits for Highly Amplified Genomic Regions. Cancer Discovery, 6(8), 900 LP-913. Retrieved from http://cancerdiscovery.aacrjournals.org/content/6/8/900.abstract\nde Weck, A., Golji, J., Jones, M. D., Korn, J. M., Billy, E., McDonald, E. R., … Kauffmann, A. (2017). Correction of copy number induced false positives in CRISPR screens. bioRxiv. Retrieved from http://biorxiv.org/content/early/2017/06/23/151985.abstract\nWant to receive regular blog updates? Sign up for our siTOOLs Newsletter:","Dyno Therapeutics Announces Ocular Collaboration with Novartis to Develop Improved Gene Therapies with AAV Vectors based on AI Technology\n– Collaboration focuses on gene therapies for serious eye diseases and leverages Dyno’s CapsidMap™ artificial intelligence platform to design AAV vectors –\n– Financial terms include upfront consideration, research funding, license fees, clinical and sales milestone payments, and royalties to develop AAV vectors to address an extensive range of ophthalmic gene therapies –\nCAMBRIDGE, Mass.–(BUSINESS WIRE)–Dyno Therapeutics, a biotechnology company applying artificial intelligence (AI) to gene therapy, today announced a collaboration with Novartis to develop improved Adeno-Associated Virus (AAV) vectors for research, development, and commercialization of gene therapies for ocular diseases. The partnership will allow the parties to utilize Dyno’s CapsidMap™ artificial intelligence platform along with Novartis expertise in gene therapy development and global commercialization to deliver innovative gene therapies to patients with serious diseases of the eye.\n“We are delighted to be collaborating with Novartis,” stated Eric D. Kelsic, Ph.D., CEO and Co‑founder of Dyno Therapeutics. “Many eye diseases are ideally suited to being treated with gene therapies, and more opportunities can be opened with new and improved AAV vectors. With their extensive ophthalmologic expertise, Novartis is an ideal partner to leverage Dyno’s platform to design AI-powered vectors to expand the impact of gene therapies for ocular diseases. This collaboration is a major step forward in our plan to realize the potential of Dyno’s CapsidMap platform for gene therapies to improve patient health.”\nUnder the terms of the agreement, Dyno will be responsible for using AI technology and its suite of machine learning and experimental tools for the design and discovery of novel AAV capsids, the cell-targeting protein shell of viral vectors, with improved functional properties for gene therapy. Novartis will be responsible for conducting preclinical, clinical, and commercialization activities for the gene therapy product candidates created with the novel AAV capsids. Dyno will receive upfront consideration plus committed research funding and license fees. Additionally, Dyno will be eligible to receive clinical, regulatory and sales milestone payments. Dyno will also receive royalties on worldwide net sales of any commercial products developed through the partnership.\nAbout CapsidMap™ for Designing AAV Gene Therapies\nBy designing capsids that confer improved functional properties to Adeno-Associated Virus (AAV) vectors, Dyno’s proprietary CapsidMap™ platform overcomes the limitations of today’s gene therapies on the market and in development. Today’s treatments are primarily confined to a small number of naturally occurring AAV vectors that are limited by delivery, immunity, packaging size, and manufacturing challenges. CapsidMap uses artificial intelligence (AI) technology for the design of novel capsids, the cell-targeting protein shell of viral vectors. The CapsidMap platform applies leading-edge DNA library synthesis and next generation DNA sequencing to measure in vivo gene delivery properties in high throughput. At the core of CapsidMap are advanced search algorithms leveraging machine learning and Dyno’s massive quantities of experimental data, that together build a comprehensive map of sequence space and thereby accelerate the discovery and optimization of synthetic AAV capsids.\nDyno’s technology platform builds on certain intellectual property developed in the lab of George Church, Ph.D., who is Robert Winthrop Professor of Genetics at Harvard Medical School (HMS), a Core Faculty member at Harvard’s Wyss Institute for Biologically Inspired Engineering, and a co-founder of Dyno. Several of the technical breakthroughs that enabled Dyno’s approach to optimize synthetic AAV capsid engineering were described in a November 2019 publication in the journal Science, based on work conducted by Dyno founders and members of the Church Lab at HMS and the Wyss Institute. Dyno has an exclusive option to enter into a license agreement with Harvard University for this technology.\nAbout Dyno Therapeutics\nDyno Therapeutics is a pioneer in applying artificial intelligence (AI) and quantitative high-throughput in vivo experimentation to gene therapy. The company’s proprietary CapsidMap™ platform is designed to rapidly discover and systematically optimize superior Adeno-Associated Virus (AAV) capsid vectors with delivery properties that significantly improve upon current approaches to gene therapy and expand the range of diseases treatable with gene therapies. Dyno was founded in 2018 by experienced biotech entrepreneurs and leading scientists in the fields of gene therapy and machine learning. The company is located in Cambridge, Massachusetts. Visit www.dynotx.com for additional information.\nThe Yates Network"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:670c8a95-767a-434b-88ac-6ee1806651f7>","<urn:uuid:e8a2b3fa-0072-4979-b1da-31d86273e7f3>"],"error":null}
{"question":"How does CRISPR compare to traditional genetic modification methods in terms of speed and accessibility, and what ethical concerns has it raised?","answer":"CRISPR is considered easier to use than traditional methods like Zinc Finger Nucleases (ZFNs) and TALENs, allowing for the creation of animal models of human diseases in months rather than the multiple years required by traditional methods. However, it has sparked serious ethical concerns, particularly when used in human embryos. These concerns led to calls from biologists, including CRISPR's inventor, for a worldwide ban on using the technique in ways that could be passed down to human offspring, especially due to fears it could be used to enhance qualities like intelligence or physical appearance in developing 'designer babies'.","context":["Five Russian couples who are deaf want to try the CRISPR gene-editing technique so they can have a biological child who can hear, biologist Denis Rebrikov has told New Scientist. He plans to apply to the relevant Russian authorities for permission in “a couple of weeks”.\nThe case for using CRISPR for this purpose is stronger than for trying to make children HIV-resistant, as attempted previously, but the risks still outweigh the benefits, say other researchers.\n“Rebrikov is definitely determined to do some germline gene editing, and I think we should take him very seriously,” says CRISPR expert Gaetan Burgio at the Australian National University. “But it’s too early, it’s too risky.”\nBoth would-be parents in each couple have a recessive form of deafness, meaning that all their children would normally inherit the same condition. While the vast majority of genetic diseases can be prevented by screening IVF embryos before implantation, with no need for gene-editing, this is not an option for these couples.\nThat is exactly why Rebrikov, at Russia’s largest fertility clinic, the Kulakov National Medical Research Center for Obstetrics, Gynecology and Perinatology in Moscow, has sought out these very unusual couples.\n“It is clear and understandable to ordinary people,” he says. “Each new baby for this pair would be deaf without gene mutation editing.”\nIn November, a biophysicist in China announced that he had secretly created the first-ever gene edited babies using CRISPR. He Jiankui tried – but probably failed – to induce mutations that protect against HIV by gene-editing IVF embryos from couples in which the man is HIV-positive.\nHis work has been condemned for many reasons, but one of the biggest is that there is no need to resort to such a risky experimental procedure to prevent these men infecting their children with HIV.\nIn June, Nature reported that Rebrikov also wanted to use CRISPR to create HIV-resistant babies – but this time for couples where the mother is HIV-positive. Unlike He, Rebrikov is openly discussing his plans in advance.\nAgain, though, experts have pointed out that there is no need for risky gene editing in this case.\nRecessive deafness gene\nNow Rebrikov has told New Scientist that he also wants to prevent children inheriting a form of deafness caused by mutations in the GJB2 gene. In western Siberia, many people have a missing DNA letter in position 35 of the GJB2 gene. Having one copy has no effect, but those who inherit this mutation from both parents never develop the ability to hear.\nRebrikov has found five couples in which both would-be parents are deaf because of this mutation and don’t want their children to be deaf too.\nSo he plans to use CRISPR to correct this mutation in IVF embryos from these couples. All these embryos will have the mutation in both copies of the GJB2 gene – correcting one copy using a method known as homology-directed repair will prevent deafness.\n“Technically, it is achievable,” says Burgio.\nIn November, scientists in the field released a statement saying that this kind of germline genome editing could be acceptable if the risks were addressed and if certain criteria are met. Those criteria include “a compelling medical need” and “an absence of reasonable alternatives”.\nPreserving deaf culture\nThe five couples don’t have any other choice if they want to have their own biological children who can hear. But not everyone will agree that there is a compelling need, because deafness isn’t a life-threatening disorder. In fact, some people who are deaf don’t consider it to be a disability and want their children to inherit the condition in order to preserve unique deaf cultures.\nAnd the risks of CRISPR certainly haven’t been addressed. There is no proven way to ensure that gene-edited children won’t have unintended mutations, or to ensure that every cell in the children’s bodies will have the corrected gene.\n“We continue to believe that proceeding with any clinical use of germline editing remains irresponsible at this time,” the November statement says.\n“The first human trials should start with embryos or infants with nothing to lose, with fatal conditions,” says bioethicist Julian Savulescu of the University of Oxford. “You should not be starting with an embryo which stands to lead a pretty normal life.”\nSo why isn’t Rebrikov trying to prevent more serious genetic disorders? The answer is that people with serious recessive disorders almost never find themselves in the same situation as the five couples. For instance, people with cystic fibrosis usually die young and are actively discouraged from meeting to avoid swapping the bacteria that infect their lungs.\nSavulescu thinks the first gene-editing trials should involve couples whose children could inherit fatal conditions such as Tay-Sachs, but who refuse to opt for screening IVF embryos because they are opposed to destroying embryos on religious grounds.\nIf germline gene-editing is one day shown to be safe, Savulescu thinks there would then be a moral imperative to use it to prevent conditions such as deafness.\nMore on these topics:","by Dinesh Puppala\nCRISPR Clustered Regularly Interspaced Short Palindromic Repeat\nCas9 CRISPR-associated nuclease 9\nCRISPR-Cas9 is a new genetic modification technique that is generating extraordinary buzz among life sciences researchers. The Institute of Genetic Medicine recently held a CRISPR-Cas9 symposium for the Johns Hopkins community to share knowledge and resources about this new technology in the field of genetics.\nGene editing is a type of genetic engineering technique in which DNA is inserted, replaced, or removed from a genome using artificially engineered nucleases. CRISPR-Cas9 isn’t the only technology capable of editing genes, but researchers consider it easier to use than other methods including Zinc Finger Nucleases (ZFNs), Transcription Activator-Like Effector Nucleases (TALENs), Meganucleases, and other tools. Recently, using CRISPR-Cas9 technology, Chinese scientists modified monkey genes that regulate metabolism, immune cells, stem cells, and sex. “CRISPR-Cas9 technique allows for the creation of animal models of human diseases in mere months, a process that typically takes multiple years with traditional methods”, said Tim Harris, Senior Vice President of Precision Medicine at Biogen Inc. There are a lot of model organisms that researchers use to study disease, notably mice. For some kinds of disease research, especially neuropsychiatric disorders, animals more closely related to us would be preferable. CRISPR has become a leading technique in research due to its ease, efficiency, and cost.\nThe possibilities for this technology include genetic medicine and genetically modified organisms of all forms: plants, animals, and microbes. However, CRISPR/Cas9 also comes with inevitable ethical and technical hurdles. Initially, the challenges included figuring out the best way to deliver the editing machinery to the cells it was targeting. Although CRISPR is faster and easier to use than other gene-editing techniques that doesn’t mean it’s more accurate. Off-target cuts to DNA can occur when the sequence is similar but not identical to the guide RNA. This could have unintended and potentially deleterious health consequences. This can also make biomedical research more complicated with unknown off-target effects, which are unwanted characteristics of unknown origin that can make interpreting results much more difficult. Ethical concerns were raised when CRISPR-Cas9 was used to repair the genes that cause beta thalassemia in 86 human embryos obtained from a fertilization clinic. The work raised fears that gene editing could be used to tweak babies in many ways before they were born. A group of biologists including the inventor of the CRISPR approach has called for a worldwide ban on the use of this technique in humans in any way that could be passed down to offspring, as the technique could be used to enhance qualities like intelligence or physical appearance in developing so-called “designer babies.”\nThe University of California, Berkeley and Massachusetts Institute of Technology (MIT) researchers have been arguing over the patent rights to the CRISPR-Cas9 technology. In April, lawyers of the UC Board of Regents asked the United States Patent and Trademark Office to consider again the patents that were given to the Broad Institute of MIT and Harvard. According to their legal team, the rights to the process belong to the team off Jennifer Doudna, Professor of Chemistry and Molecular and Cell Biology at the UC Berkeley.\n- Why Gene-Editing Technology Has Scientists Excited, WSJ June 1st 2015\n- MIT and UC Berkeley Researchers argue over Disputed Patent Rights to CRISPR-Cas9: Uncover Michigan\n- Johns Hopkins Tackles Genome Editing with CRISPR/Cas9 Symposium: Biomedical Odyssey"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:7bcb78c1-a8b0-43b8-b6ff-881e0ceb4115>","<urn:uuid:9ea62339-a108-4f47-8054-a83f6ae98ddc>"],"error":null}
{"question":"Is the future of agriculture moving towards lighter or heavier farming equipment?","answer":"The trend is clearly moving towards lighter farming equipment. While traditional tractors and combine harvesters can weigh up to 35 tons, new autonomous solutions like the AgBot tractors weigh only around 2.8 tons. This shift is environmentally significant since heavy machines damage soil by squashing it and forcing out air, making it difficult for plants to put down roots and draw up nutrients, while also leaving land prone to flooding. Research has shown that supersize tractors could be damaging up to a fifth of global land used to grow crops. Similarly, CloudGro systems emphasize lightweight solutions, with their systems engineered to be lightweight enough to retrofit on existing building roofs.","context":["INTERVIEW WITH JONATHAN LODGE, CEO AT CITY FARM SYSTEMS LTD\nQ. Can you start by giving us your City Farm Systems elevator pitch? What do you do and why do you do it?\nOur protected IP enables us to install the most sustainable, environmentally friendly and economically viable urban farms - growing at the point of need, on a just in time basis and avoiding the costs of distribution. Our CloudGro® systems offer CloudGrown® produce, grown to order, at the point of need with full provenance from Seed to Sale™.\nQ. In order to make a rooftop farm work, you need cheap buildings, insurance and the go ahead from health and safety. Did I miss anything? Were you able to get all these variables in place?\nYou raise an interesting point but the reality is no building in a busy city is cheap for any length of time, which is why we decided not to go down the route of installing urban farms inside a building. Our CloudGro® systems have been engineered to allow us to retrofit a system in hard to access spaces such as the roof of an existing retail warehouse or hotel building.\nWe quickly realised the best way to make a profit was to cut the cost of distribution, which means distributing the growing to the point of need. All heavily occupied buildings need feeding and pay to dump heat and CO2 at roof level. Our patents cover placing a lightweight, largely automated greenhouse on a modern steel framed roof. The increasing number of wild weather events has reinforced the need to grow delicate crops under cover. Commercial greenhouse growers will often run gas boilers all summer simply to create the CO2 their crops need - even if it means dumping the heat.\nSome have tried to bring the rural greenhouse business model to expensive city buildings in the vain hope that saving a few motorway miles will cover the rent, rates and massive energy bills. Distribution businesses talk of the final mile being the most expensive – in the case of fresh produce, this is caused by disproportionately large amounts of single use transit only packaging and city traffic delays.\nMy lightbulb moment came when I found myself stuck behind a supermarket lorry for several minutes, only yards from the store. Once inside I found the shelves bare.\nLorries are some of the most expensive vehicles on the road and this one was achieving nothing while the fresh produce it was carrying lost shelf life and the store was losing sales. Coming from a resource efficiency background this made no sense at all. The grower was paying to grow, pack and deliver the most delicate and perishable crops to a store or hotel where they were already paying to dump low level heat waste and CO2 and would now have to find a responsible way to dispose of the transit only packaging.\nRecognising that insurance and Health & Safety would not permit regular workforce access to the roof, we developed our automation systems to avoid this. Indoor space in a busy city costs 30-50,000 times as much as agricultural land. A supermarket expects to sell 3 times each week what a greenhouse can grow each year in the same area. One square metre of warehouse roof solar PV in the UK generates the equivalent of 9 litres of diesel whilst each square metre of a commercial greenhouse should be growing about £600 of produce at retail prices.\nQ. I’d like to talk a bit more about the technology you use. What goes into creating a 'self-sustaining growing system'? Would I be right in thinking that much of the technology used is applied to generate data to ensure the crops are growing?\nAutomating a rooftop greenhouse is essential to monitor growing crops. Field based agriculture has started to use GPS and drones. GPS doesn’t work for undercover farming and we need that space for crops rather than drones. We chose to move trays of growing crops around the volume and to use RFID tags to keep track of them. This allows us to use large amounts of simple mechanical elements to move the crops and bring them to our transfer corridors where we use a smaller amount of high technology to monitor and deliver nutrient rich water. Multi-spectral cameras can recognise nutrient deficiency, pests and diseases before the human eye can.\nCollecting data is essential to making all this work. Growing at the point of need allows a change of business model. Data is key to unlocking the full potential. Where others control their growing environment, our approach offers much more. Some promote the concept of traceability from ‘Farm to Fork’. We believe this is vague with little, if any, information about what happens on the farm and no control over what happens once the produce is sold – with much still to be done before it reaches a plate.\nWith our approach to data we collect and can offer CloudGrown® produce with full provenance from Seed to Sale™. As we need to work with the building occupier we can analyse their current sales data and contrast it with their ordering figures to create future ‘grow to order’ crop plans.\nMore recent plans regarding our approach to data focus on what we can offer for research. Our movement of crop trays and individual watering allows a very precise data path along the growing cycle. With some adjustment we can automate many laborious research tasks.\nQ. Can you talk to me about the ‘low impact production’ process you follow? What does that entail exactly and what are the benefits?\nOur ability to change the business model is what sets us apart. Growing at the point of need and having the ability to match harvesting to short term demand changes avoids waste. Others follow what we call the ‘Heathrow model’ – growing at absolute maximum capacity and then hoping to sell.\nThe lightbulb moment was all about the desire to avoid waste and unnecessary effort. Why pay to grow, pack and deliver what we know will be wasted? Shortening supply chains from a typical 3 days to a few minutes offers so much more.\nQ. You worked with a lot of different academics and specialists to develop your strategy. From what I understand you consulted with robotic engineers, architects and economists? What were the key points they highlighted and how much of that advice did you incorporate into your strategy?\nEarly on, we met with a few academics who highlighted points which became key drivers for our research and development. A professor of economics advised us to, ‘reduce the Minimum Efficient Scale’. For a long time, agriculture has depended on monoculture and the ability to fill a whole lorry to be able to make a profit. Different crops have been grown at scale on each farm or greenhouse. For horticulture, this created the demand for seasonal labour and contributed to the issues surrounding migrant labour. Retailers on the other hand want to sell fresh produce daily.\nAnother meeting was with one of the technical authors of the ISO paper on measuring greenhouse gases. With his background in agriculture he suggested we should ‘see green whenever we look down’. A lot of our development has followed this approach and we have used robotics, mechatronics engineers and data specialists to unlock its full potential.\nQ. I would like to wrap up by talking about urban agriculture in relation to climate change adaptation. In order to ensure food security in the face of climate change and expanding cities, a model such as this will need to be ‘industrialized’. If you were to design a factory or a city with urban agriculture in mind, where would you start and where is this being done?\nWe are confident that financiers will not allow the ‘food at any cost’ approach to continue for long. One key point often neglected is the significance of the changes in seasonal demand. In the UK, demand for salad crops halve in winter. If others cannot make a profit at the time of peak demand, how will they cope with competition? The figures will never stack up for them. Anyone expecting to grow on several levels inside a warehouse type building will depend on horrifically large amounts of energy. Frankly, they may as well light them with piles of burning banknotes.\nCurrent horticulture often relies on back breaking work in appalling conditions. When it comes to the industrialisation of urban farming, the Japanese would happily use robotics from start to finish. We took the view that, whilst this is entirely possible, it is only necessary when following a traditional business model.\nReducing the Minimum Efficiency Scale offers opportunities to create engaging tasks for a broad spectrum of age and ability. There is no need to work at scales that require sophisticated high-speed seed sowing or packaging machinery when you only need to produce a few dozen fresh crop trays each day.\nNot all school leavers are suited to academic life. Having said that even those with degrees can end up with roles no more fulfilling than flipping burgers or stacking supermarket shelves. By distributing the growing, a CloudGro® system offers the chance to work in ideal conditions with much less stress. Rather than unload a lorry, stack the shelf and dispose of the packaging, local workers can be a key part of the growing process, sowing seeds and harvesting fresh produce at floor level - a job they can take pride in.\nThere are a few who have applied the market garden or allotment style growing approach and brought it to small unused city spaces but you can’t feed a city using this approach. With monitoring, horticultural expertise and management delivered via the cloud and much of the crop care being automated, a CloudGro® system can achieve much more.\nNew urban communities are being designed without roads and will depend on the approach we offer. Local authorities are coming back to community housing. Our systems would make perfect sense to broaden local employment opportunities whilst growing food for the community. The general consensus is we can do better than simply picking low hanging fruit. At the moment, commercial sustainability is about little more than crawling around picking up windfall apples. When retailers are ready to break down their silo thinking, we will collaborate to pick the top-quality fruit hanging above our heads.","For generations, enormous tractors with ruddy-cheeked farmers at the wheel have been a familiar symbol of countryside and farming life.\nSoon, however, the sight of tractor and farmer hard at work together, preparing the land and harvesting the crops, may be as out of date as the horse-drawn plough from the age of Robert Burns.\nFor the age-old relationship of man, machine and land is set to be broken with the arrival in Scotland of the first fleet of robot tractors.\nDriverless ‘Agbot’ tractors, due to arrive in Scotland within weeks, have been described as a gamechanger for agriculture, with the ability to work solo, 24-hours a day and to precise standards – raising the potential that they can help solve a crippling labour shortage crisis which has left farmers and growers scrabbling for staff.\nBecause they are significantly lighter than a traditional tractor – and, as hybrid vehicles, use less diesel – they are also being touted as a greener option, offering a solution to soil compaction caused by huge vehicles which trample the land and which can lead to flooding, degradation and lower yields.\nWhile, by freeing up time that would normally be spent in the driver’s seat, the vehicles – programmable several months in advance – allow farm staff to concentrate on other areas of increasingly diversified businesses, such as running farm shops, tourist accommodation and food production.\nThe robot tractors are being offered to Scottish farmers and growers by Brechin-based precision farming specialists SoilEssentials, which first unveiled them at last year’s Royal Highland Show.\nThe first AgBots destined for farmers’ fields are now “two or three” weeks away from arriving in the UK with orders for the vehicles being taken for delivery in time for this year’s harvest.\nIt raises the prospect of the autonomous tractors being used on Scottish farms to harvest this year’s crops – without a farmer and his dog in sight.\nAlthough developed over the course of the past four years on the distinctly flat landscape of the Netherlands by farming technology company AgXeed, the diesel and electric hybrid vehicles are said to have been thoroughly tested to handle the rugged Scottish landscape during tests on the steep grassy slopes of an Angus hill farm.\nThe vehicles, which measure roughly 12ft long and 5ft high but will be much larger once they are fitted with additional equipment to carry out specific jobs such as drilling and cultivating, are said to have “sailed through” their task with ease.\nGraham Ralston, hardware director at SoilEssentials, said the driverless vehicles could solve a range of issues affecting modern agriculture, from labour shortage to addressing sustainability and environmental problems, and freeing up farmers’ time.\nAt a webinar this week to present the new tractors to farmers and growers, he said: “So many farmers in our region have chosen to diversify their businesses to remain viable, this can often mean time sitting in a tractor.\n“Although that can be quite enjoyable, it can be a frustrating consumption of time, and it’s not the most efficient place to run a business, especially when the requirements of that business can fall outside the normal workings of the farm and aren’t necessarily complementary with the standard farming calendar.”\nThe land the robots are to work on is initially mapped out using drones, and the vehicles programmed via a portal which allows farmers to plan how they want them to operate for months in advance.\nAdditional equipment can be fitted to the tractor which then can be set to work, travelling backwards and forwards, unmanned, for several hours at a time.\nRather than spend time in the tractor cab, farmers are more likely to be in front of computer screens or checking iPads.\nMr Ralston added: “We can see it working in different scenarios, carrying out repetitive jobs like grass mowing and working with small producers as part of a team, for example, as a drilling machine with someone rolling behind with seed.\n“We ran tests last summer using it to pull up some potato beds which takes skill to be right place before you start, to line up and a bit of moving around.\n“We planned it on the portal and let it go: we knew exactly how long that job would take it, there was no stopping to align the machine, no stopping for a chat.”\nA key advantage of the robots is their lightweight: they weigh around 2.8 tons, while modern combine harvesters, tractors and other farm machinery can weigh up to 35 tons. Heavy machines squash soil and force out air, making it difficult for plants to put down roots and draw up nutrients, and leaving land prone to flooding.\nResearch by scientists in Sweden last year suggested supersize tractors could be responsible for damaging up to a fifth of global land used to grow crops.\nDriverless tractors represent the next stage in an increasingly futuristic farming landscape which has seen the use of everything from drones to map fields, data analytics and AI to plot crop rotation, fertiliser and water, soil conditions, weather patterns and apps to keep track of cattle herds.\nIt has also seen a range of unmanned equipment such as automated milking machines in dairy sheds, semi-autonomous weeding machines that can identify and pluck out weeds, sorting and fruit picking machines and ‘smart’ watering systems that can identify which areas of land require water.\nGeorge Baikie, Head of Farms at Scotland’s Rural College (SRUC), said: “Robots will certainly have an increasingly bigger role to play on farms in the future, especially when it comes to more mundane tasks.\n“Robotics for milking cows, for example, is well established but robotic feeding, muck scraping and so on can all be easily automated using technologies similar to autonomous lawn mowers and vacuum cleaners.\n“Autonomous vehicle technologies will eventually be available for cars and lorries, so it’s also likely to be made available for tractors on more perfunctory tasks.\n“This will especially be the case in arable farming, where robotic vehicles will be able to cultivate, weed, inspect and even harvest crops in conditions where there is less need for complex decision-making during the process.”\nHe added: “Cutting standing wheat in good conditions is an easy task but the same task in a wet season, with lodged crops and weed admixtures, can test even the best operators.\n“Automation will have its place and, as technologies advance, this window of opportunity will expand to ever more diverse and complicated situations. The real challenge will be teaching machines how to make real-time agricultural decisions.\n“It’s a hugely exciting time to be in the industry and being able to work with this kind of technology is a big part of that.”\nAndrew Christie, Agronomist and Agri-Technologist at The James Hutton Institute said: “While robots seem like something out of a futuristic Sci-Fi movie, technology is always evolving and making life easier. However, like all new tools and technology, their popularity will entirely depend on how successful they are in practice.\n“A good example is GPS auto guidance, which has had huge uptake since the early 2000s. This technology has helped to reduce fatigue when working long hours, reducing the chance of operator error by allowing the driver to focus on implement controls and machine performance.\n“This results in far more efficient field work operations with greater accuracy in-field. Robot units are a next step in automating the tractor as an evolution of this technology – taking the operator out of the seat and allowing them to control the vehicle remotely.”"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:27f6be85-bd09-46f9-a881-f8ea741e43d6>","<urn:uuid:f0c622c1-ea8a-451f-b406-fcb1efb3c625>"],"error":null}
{"question":"How does fear manifest in people with amygdala damage, and what mental training techniques are used to manage fear in athletes?","answer":"People with bilateral amygdala damage, like patient S.M., are typically fearless when faced with snakes, spiders, or horror films. However, when exposed to carbon dioxide inhalation, they experience intense panic and fear, suggesting fear can be triggered through interoceptive pathways that bypass the amygdala. As for athletes, mental training techniques for managing fear and anxiety include emotional control methods, concentration techniques, and imagery work. The Sport Psych Handbook specifically addresses anxiety management, helping athletes maintain focus under pressure and transform from feeling 'pumped to panicked' through specialized psychological training.","context":["There’s a female patient, known in the research literature as S.M., who’s been dubbed the “woman with no fear”. She has severely damaged amygdala on either side of her brain and consequently is left unmoved by snakes, spiders, horror films, haunted houses and real-life knife attacks. She doesn’t even have a sense of personal space. But when S.M. inhaled carbon dioxide for a new study, she was far from calm. Within seconds, she groped desperately for the air mask and cried for help. After researchers removed the mask, S.M.’s entire body went rigid, her toes and fingers flexed taut, toward the ceiling. Her skin was flushed, her eyes wide like a scared animal. Thirty-seconds after the ordeal, she began to calm, finally releasing the experimenter’s hand. Later, she recalled the experience she’d had was of panic – “the number one, worst” feeling ever.\nThe amygdala is an almond-shaped structure found deep on either side of the brain. So many studies have shown it to be involved in learning and experiencing fear, it’s become shorthand to refer to it as the brain’s “fear centre”, even though it’s also involved in positive emotional processing. The apparent fearlessness of S.M., who lost her amygdala to Urbach-Wiethe disease (a rare genetic disorder), had previously supported this caricature. But now things have gotten a lot more complicated*. Carbon dioxide inhalation causes unpleasant suffocating feelings and it triggers panic in those susceptible to it. But if S.M. has no “fear centre”, how and why did she get so scared and panicky after inhaling?\nTo check this was no anomaly, Justin Feinstein at the University of Iowa and his colleagues also tested two other patients with bilateral amygdala damage (a pair of identical twins, A.M. and B.G.). These patients also had panic attacks and experienced fear when they inhaled carbon dioxide. They and S.M. said these fearful feelings were entirely novel to them. It’s notable too that this wasn’t a one off. The patients’ panic reaction occurred all over again during a repeat of the carbon dioxide procedure.\nAnd yet, when the researchers performed the carbon dioxide inhalation with 12 healthy controls, 9 of them exhibited no panic and they reported far less fear than the patients. Three of the controls did panic. The amount of subjective fear and panic reported by these panicky controls and the patients was equivalent, and their physiological signs were similar, such as raised heart-rate. The only difference between the patients and the panicky controls was that the former didn’t show any signs of anticipatory anxiety when they saw the apparatus being prepared.\nSo we have a situation where three out of three patients with bilateral amygdala damage (who are usually fearless) panicked and experienced more fear than nine of the twelve amygdala-intact controls. What’s going on? Feinstein and his team think that CO2 inhalation acts on “interoceptive” receptors that project directly to the brainstem and other sites that “underlie fear and panic”, whereas other fear-stimuli, like scary films or dangerous animals, are “exteroceptive in nature, mainly processed through visual and auditory pathways that project to the amygdala”. The patients were completely unfazed when the procedure was repeated with normal air, supporting the idea that the carbon dioxide played a specific role in provoking fear.\nThis still leaves the mystery of why the amygdala-damaged patients were more disturbed by the carbon dioxide inhalation than most of the controls. Here the researchers’ interpretation is bold. They suggest that an intact amygdala might normally serve to inhibit panic. This isn’t as revolutionary as it seems. Feinstein’s team point to a study from ten years ago that found patients with panic disorder had amygdala atrophy. So, if we consider this brain structure as the “centre” of anything, perhaps it should be as the centre of calm!\nThis new study also raises some deep, almost philosophical questions. If the amgydala-damaged patients usually live a life that’s entirely fear-free, how did they know to describe their feelings during the inhalation as extreme fear or panic? Can we be sure they really felt fear the way that the rest of us do? The researchers have previously reported that S.M. references fear and anxiety appropriately in conversation, and she can recognise fear in others, so perhaps she does have a solid understanding of the concept. Finally, what do these findings say about the neural correlates of the subjective sense of fear and panic? At the very least it seems the amygdala is not necessary for the conscious experience of fear.\nJustin S. Feinstein, Colin Buzza, Rene Hurlemann, Robin L. Follmer, Nader S. Dahdaleh, William H. Coryell, Michael J. Welsh, Daniel Tranel, and John A. Wemmie. (2013). Fear and panic in humans with bilateral amygdala damage. Nature Neuroscience, DOI: 10.1038/nn.3323\n*Please note, this isn’t the first documented case of fear in a patient with bilateral amygdala damage. In a 2002 diary study (pdf), one such patient reported normal daily experience of anxiety and fear, but impaired recognition of other people’s fearful expressions. S.M. (in the current study) is impaired at recognising fear in static images of faces but she can recognise fear in people’s voices and from dynamic displays of fearful faces. One obvious question mark over the 2002 study is the reliance on self-report of fear.","Athletes’ physiques and conditioning levels have advanced significantly in the past decade. Now, it’s time for their mental prowess to catch up.\nThe Sport Psych Handbook makes significant performance improvement possible through training the mind to parallel the body’s enhanced development. Experts covering each facet of the mental game present research-based, field-tested approaches to enhance individual and team performance. More specifically, this comprehensive guide will give athletes breakthrough methods to do the following:\nOptimize inner drive for training and competition\nMaintain emotional and mental control for unwavering focus\nImprove communication skills for better leadership and teamwork\nAvoid and overcome slumps, injuries, and other challenges to a successful season\nComplement your physical skills with sharper mental skills. Make The Sport Psych Handbook part of your training arsenal, and start performing to your potential.\nPart I. Inner Drive Chapter 1. Motivation: The Need to Achieve\nJohn F. Eliot Chapter 2. Goals: More Than Just the Score\nChris Harwood Chapter 3. Competitive Drive: Embracing Positive Rivalries\nCal Botterill Chapter 4. Overtraining: Balancing Practice and Performance\nPart II. Emotional and Mental Control Chapter 5. Anxiety: From Pumped to Panicked\nGloria Balague Chapter 6. Anger: How to Moderate Hot Buttons\nMitch Abrams and Bruce Hale Chapter 7. Concentration: Focus Under Pressure\nClark Perry Chapter 8. Imagery: Inner Theater Becomes Reality\nPart III. Interactive Skills Chapter 9. Leadership: Full Engagement for Success\nJim Loehr Chapter 10. Teamwork: For the Good of the Whole\nTracy L. Veach and Jerry R. May Chapter 11. Coaching: An Effective Communication System\nCharles J. Hardy, Kevin L. Burke, and R. Kelly Crace\nPart IV. Potential Pitfalls Chapter 12. Injuries: The Psychology of Recovery and Rehab\nCharles Brown Chapter 13. Eating Disorders: When Rations Become Irrational\nKaren D. Cogan Chapter 14. Substance Use: Chemical Roulette in Sport\nMark H. Anshel\nPart V. The Educated Consumer Chapter 15. Roles: The Sport Psychologist\nSean McCann Chapter 16. Qualifications: Education and Experience\nBradley Hack Chapter 17. Success in Sport Psych: Effective Sport Psychologists David Tod and Mark Andersen\nShane Murphy, PhD, is one of the nation's leading sport psychologists, with expertise in performance excellence, competitiveness, and teamwork. He is currently assistant professor at Western Connecticut State University. Murphy served as a sport psychologist to the U.S. Olympic team at the 1988 Summer Games in Seoul, the U.S. Olympic team at the 1992 Winter Games in Albertville, and the 1996 U.S. slalom canoe and kayak teams in Atlanta. In addition, he was sport psychology consultant to the U.S. snowboard team from 1999 to 2002. For seven years he worked for the United States Olympic Committee (USOC) as head of its sport psychology department and later as the associate director of its sport science and technology division.\nMurphy is a popular speaker and author of the best-selling book The Achievement Zone: An 8-Step Guide to Peak Performance in All Arenas of Life. He has appeared on many television and radio programs on ABC, CBS, MSNBC, CNN, and NPR. He also has had articles containing or referring to his work in USA Today, The Washington Post, The New York Times, Newsweek, and other major publications. He is a former president of the division of sport and exercise psychology of the American Psychological Association (APA) and is a fellow of the Association for the Advancement of Applied Sport Psychology (AAASP).\nMurphy lives in Trumbull, Connecticut, with his wife, Annemarie, and two children, Bryan and Theresa.\nABOUT THE CONTRIBUTORS John F. Eliot, PhD, is the director of sport psychology in the department of kinesiology at Rice University in Houston, Texas. His areas of specialization include performance enhancement, management consulting, and rehabilitation. Eliot has worked with a wide range of sports teams, including the Houston Astros, Philadelphia Eagles, San Antonio Spurs, and Chicago White Sox, along with numerous health care and business clients.\nChris Harwood, PhD, is a lecturer in applied sport psychology at Loughborough University, United Kingdom. He is a BASES-accredited and BOA- registered sport psychologist whose research focuses on achievement motivation in sport, social psychology of elite youth sport, and performance-enhancement interventions. Harwood has served as a consultant to the Nottingham Forest Soccer Club, Lawn Tennis Association, English Cricket Board, the Football Association, and the Youth Sport Trust, plus a variety of other national governing bodies in UK sport.\nCal Botterill, PhD, is a health and performance psychology professor at the University of Winnipeg, Canada. He has served as a consultant to seven Canadian Olympic teams, five NHL Hockey hockey teams—including the 1994 Stanley Cup champion the New York Rangers—and numerous corporations such as Coca-Cola and Great-West Life Assurance Company. Cal has authored hundreds of articles, books, videos, and international presentations.\nBruce D. Hale, PhD, is an associate professor of kinesiology at Penn State University, Berks-Lehigh Valley College in Reading, Pennsylvania. His areas of specialization include sport and exercise psychology and educational sport psychology. He is a fellow of the Association for the Advancement of Applied Sport Psychology (AAASP) and has been a performance-enhancement consultant to hundreds of college, professional, and elite national teams. Hale has worked with USA Wrestling, the British Biathlon, USRowing, TAC, and USA Rugby.\nMitch Abrams, PsyD, is president and founder of Learned Excellence for Athletes, a sport psychology consulting company; an adjunct faculty member of Fairleigh Dickinson University; and director of inpatient psychology at Northern State Prison in Newark, New Jersey. He specializes in developing programs for athletes to improve performance through emotion management. Abrams is a member of the American Psychological Association's (APA) division of exercise and sport psychology and the Association for the Advancement of Applied Sport Psychology (AAASP).\nJim Loehr, EdD, is chairman and CEO of LGE Performance Systems, a training company specializing in performance enhancement for professionals and athletes. He is a member of the American Psychological Association (APA), the Association for the Advancement of Applied Sport Psychology (AAASP), and the American College of Sports Medicine (ACSM). Loehr has worked with a wide range of teams—the United States Tennis Association (USTA), Women's Tennis Association (WTA), International Tennis Federation (ITF), and Intercollegiate Tennis Association (ITA)—as well as professional athletes from the NBA, NFL, NHL, PGA, and LPGA, wrote 13 books, received numerous awards for his contributions in sport psychology, and contributes regularly to a wide variety of sports and business publications.\nTracy L. Veach, EdD, is a professor of Psychiatry and Behavioral Sciences and Internal Medicine at the University of Nevada School of Medicine. He works with individual athletes and teams in performance enhancement and integrative health. He has consulted with a wide range of athletes at high school, collegiate, and national team levels in football, basketball, swimming, track and field, and equestrian sports. Veach also served as a sport consultant to a world-record-setting U.S. sports parachute team.\nCharles H. Brown, PhD, is director of FPS Performance, a Charlotte, North Carolina-based company specializing in performance enhancement of athletes, performing artists, and business professionals. He has worked with athletes at every level, specializing in the contextual factors that impact performance, athletes' relationships, and work-life balance of elite performers. Brown is a consultant certified by the Association for the Advancement of Applied Sport Psychology (AAASP), a member of the United States Olympic Committee (USOC) sport psychology registry, and a member of the National Register of Health Service Providers in Psychology.\nSean C. McCann, PhD, is the head of the United States Olympic Committee (USOC) sport psychology department. He works directly with teams and coaches at the U.S. Olympic Training Center and has traveled with the last five Olympic teams as a sport psychologist. McCann writes extensively about sport psychology in several outlets, including refereed journals, book chapters, columns, brochures, and workbooks for Olympic athletes and coaches.\nBradley Hack, PhD, is the director of sport psychology for the department of athletics at the University of North Carolina at Chapel Hill. He is an executive committee member of the American Psychological Association's (APA) division of exercise and sport psychology. In 2003 he was appointed chairman of the division's Presidential Task Force to establish national standards for sport psychology training, education, and experience for sport psychologists.\nMark B. Andersen, PhD, is an associate professor at the School of Human Movement, Recreation, and Performance at Victoria University in Melbourne, Australia. He specializes in supervision, injury, exercise, quality of life, and chronic disease. Andersen has worked with athletes from a wide range of sports, including the Arizona State University Intercollegiate Athletics, Victorian Diving Association, and various other diving, swimming, track and field, wrestling, and golf teams. Andersen has been on the editorial boards of The Sport Psychologist, Journal of Applied Sport Psychology, and the Journal of Sport & Exercise Psychology.\nDavid Tod is a PhD candidate at Victoria University in Melbourne, Australia, specializing in professional practice and performance enhancement. Tod has worked with the Northern Institute of the New Zealand Academy of Sport and with professional rugby unions and rugby league teams. He has also worked with athletes in numerous sports including international track and field, swimming, cricket, triathlon, cycling, and powerlifting.\nKirsten Peterson, PhD, is a member of the United States Olympic Committee's (USOC) sport psychology staff, providing counseling and performance enhancement services to athletes and coaches of numerous Olympic, Paralympic, and Pan American sports. She has traveled as part of the USOC sport psychology staff for three Olympic teams. Peterson is a licensed psychologist, a member of the American Psychological Association (APA), and is a certified consultant through the Association for the Advancement of Applied Sport Psychology (AAASP).\nJerry R. May, PhD, is a professor of Psychiatry and Behavioral Sciences at the University of Nevada School of Medicine. He developed and implemented the first official sport psychology program for the United States Olympic Committee (USOC) and has worked with such teams as the U.S. Alpine ski team (1980-92), U.S. national soccer team (1997-98), and the U.S. sailing team (1992-present), and athletes in sports ranging from soccer, golf, tennis, and basketball to skiing, sailing, biathlon, and powerlifting.\nClark Perry, PhD, is senior consultant psychologist at the Australian Institute of Sport in Canberra and managing director of PST Systems, a performance enhancement and training company. He is a world leader in sport psychology, leadership, and team development, having worked closely with some of the world's best athletes as a member of four Olympic Games, three Commonwealth Games, eight World Championships, five Pan Pacific Swimming Championships, and three Super 12 Rugby Finals. Perry has served as senior psychologist to the Australian Olympic Swim Team, Australian Cycling, Triathlon Australia, Australian Baseball, ACT Brumbies, and the Australian Wallabies.\nMark Anshel, PhD, is a professor at Middle Tennessee State University in the department of health, physical education, and recreation. His areas of research and writing have included coping with stressful events in sport and the use of steroids and other banned drugs among competitive athletes. He is a certified consultant for the Association for the Advancement of Applied Sport Psychology (AAASP).\nGloria Balague, PhD, directs the sport psychology services at the University of Illinois at Chicago Sports Medicine Center and Human Performance Lab. An internationally acclaimed sport psychology consultant and speaker, she most recently addressed the Sydney Olympic Scientific Congress in Australia. Balague is a member of the executive committee and president of the sport psychology division of the International Association of Applied Psychology.\nCharles J. Hardy, PhD, is the acting dean of the Jack N. Averitt College of Graduate Studies and a professor in the Juann-Ping Hsu School of Public Health at Georgia Southern University. He is a past president of the Association for the Advancement of Applied Sport Psychology (AAASP). His primary area of research has been in social influence processes in exercise and sport. He has extensive consulting experience at the youth sport, high school, university, Olympic, and professional levels.\nKaren D. Cogan, PhD, is a psychologist and assistant professor at the University of North Texas (UNT). She has been a consultant to the U.S. ski team at the Olympics, a staff psychologist with the Counseling and Testing Center at UNT, a faculty member of the Center for Sport Psychology, and author or coauthor of several publications, including Sport Psychology Library: Gymnastics. Cogan is a member of the Association for the Advancement of Applied Sport Psychology (AAASP) and the American Psychological Association (APA).\nKevin L. Burke, PhD, is an associate professor in the Jiann-Ping Hsu School of Public Health at Georgia Southern University. As a certified sport psychology consultant of the Association for the Advancement of Applied Sport Psychology (AAASP), Burke has advised professional, college, high school, and recreational athletes from various sports. He has also coauthored Sport Psychology Library: Basketball and Tennis and actively served on numerous editorial boards for journals and research committees within the field.\nR. Kelly Crace, PhD, is director of the counseling center and adjunct associate professor in psychology at the College of William & Mary. He is a licensed psychologist and a certified sport psychology consultant. Crace is co-developer of the Life Values Inventory, an empirically-based values clarification survey. His research and consultation focus on values and transition as applied to both individual and organizational development.\nCreated as a companion to the text, the Instructor Guide consists of the following:\nA sample syllabus – includes weekly reading assignments, suggested class grading scale based on percentages, and suggestions for written assignments (including paper topics, paper guidelines, and reference sources).\nExam study guides to accompany the Test Package exam questions. Like the book, the study guide is divided into parts 1-5, with each part containing 3-4 chapters.\nDelivered through Respondus, available online through the Murphy PSWS.\nConsists of 17 chapters of multiple choice and essay exam questions (288 questions total). IG study guide complements TP questions.\nA Respondus upgrade also offers the capability to create and manage exams that can be published directly to Blackboard, eCollege, WebCT, and other course management systems. Instructions for downloading a free version of Respondus and for acquiring the test package are at www.HumanKinetics.com/TheSportPsychHandbook.\n2006 • ISBN 0-99-004043-7\nDelivered through PowerPoint; available both online through the Murphy PSWS and on CD.\nMore than 125 color photos.\nThe presentation package has more than 350 slides of art and text from the book that can be used directly with PowerPoint to print transparencies or to make copies for distribution to students. Instructors can easily add, modify, or rearrange the order of the slides. Available at www.HumanKinetics.com/TheSportPsychHandbook."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:c6ac5fe7-94aa-4aa5-8210-ced13f33a1e2>","<urn:uuid:d40b2df1-6afb-4891-934c-c22cd8e5c554>"],"error":null}
{"question":"As an environmental researcher, I am curious about how mining exploration initiatives like the Coates Project balance economic potential with environmental impacts. What are the promising mineral prospects at Coates, and what environmental damage could result from mining there?","answer":"The Coates Project shows promising potential for Nickel, Copper and Platinum Group Elements (PGE) mineralization, particularly given its geological similarities to the successful Julimar project. The area contains interpreted mafic-ultramafic units that represent targets for Cu-Ni-Co-PGE mineralization, with previous sampling showing coincident base metal and PGE geochemical anomalies. However, mining operations typically cause significant environmental damage, including erosion, loss of biodiversity, contamination of soil and water, deforestation for debris storage, and permanent landscape destruction. The mining process can leave behind huge holes, damage natural beauty, and potentially create lifeless wastelands if not properly managed. Water quality can be severely impacted through both surface and groundwater contamination.","context":["- Project Highlights\n- Exploration Potential\n- Proposed Exploration Programme and Budget\n- Geological Setting\n- Previous Exploration\n- Recent Exploration\nThe recent discovery of Nickel (Ni) Copper (Cu) and Platinum Group Elements (PGE) in sulphide mineralisation at Chalice Gold Mines’ Julimar project (announced in March 2020), as well as indications of other instances of similar mineralisation discovered by other explorers within the Jimperding Metamorphic Belt, serve to highlight the great potential of this geological terrain\n- Project comprises 1 granted Exploration Licence and 1 EL Application; and 1 retention licence, with a combined area of 48m2\n- Located 60km northeast of Perth\n- Covers the northern half of the Coates Mafic Intrusive\nCoates Project overlain on the regional aeromagnetic image (left) and simplified geology plan (right).\nThe coincidence of base metal and PGE geochemical anomalies from the BRL vacuum drilling with the Coates Mafic Complex is most encouraging from an exploration point of view. By analogy the mineralisation at Chalice’s Gonneville Prospect is characterised by a similar Cu-Ni-Co-PGE elemental association with a mafic intrusive complex.\nThere are a number of interpreted mafic-ultramafic units that have been identified in the region, all of which show geophysical similarities with the Julimar Complex and consequently all represent targets for Cu-Ni-Co-PGE mineralisation.\nPROPOSED EXPLORATION PROGRAMME AND BUDGET\nOutcrop in the area is poor and interpretation of the mafic-ultramafic layered intrusive complexes is based on the regional aeromagnetic data. Following the experience of Chalice, the targets within the interpreted mafic-ultramafic complexes are identified by airborne electromagnetic (EM) surveys then followed-up by drilling.\nThe land title situation in the project area contains both freehold private land and conservation areas. Statutory approvals for ground access within the granted tenements will be necessary prior to commencing field work that involves ground disturbance activities.\nInitially helicopter-borne EM surveys such as VTEM can be flown to identify targets for follow-up work. These surveys are usually sufficiently sensitive to pinpoint anomalies to the extent that further ground-based EM is not required. The previous analysis of bottom-of-hole vacuum drill samples has been shown to be effective and this can be extended over the project area here access has been obtained.\nThe Coates Ni-Cu-Co-PGE Project, with an area of 47.3 km2, is located in the northern part of the South West Greenstone terrane of the Yilgarn Craton. The Project covers the Coates Mafic Complex within the Jimperding Metamorphic Belt and comprises various gneisses and banded-iron formation, interleaved with ultramafic intrusive units that have been repeatedly deformed, metamorphosed and intruded by granite.\nThe regional geological setting of the Coates Ni-Cu-Co-PGE Project has been highlighted by the Julimar Project being explored by Chalice. Chalice commenced a greenfield exploration program in mid-2019 for high-grade Ni-Cu-Co-PGE mineralisation in the region and interpreted the presence of an unrecognised layered mafic-ultramafic intrusive complex at Julimar based on high resolution regional magnetics. Follow-up exploration identified the Julimar (mafic-ultramafic layered intrusive) Complex that extends over a strike length of approximately 26 km, which has been confirmed by drilling to be highly prospective for Ni, Cu and PGEs.\nRC drilling resulted in the discovery of high-grade Ni-Cu-Co-PGE mineralisation at the newly named Gonneville Intrusion. A recent airborne EM survey identified three new large EM anomalies (Hartog, Baudin and Jansz) located north of the Gonneville Intrusion .\nThe style of sulphide mineralisation intersected consists of massive, matrix, stringer and disseminated sulphides typical of metamorphosed and structurally overprinted magmatic Ni sulphide deposits. It is understood that the discrete high-grade Cu-Ni-Co-PGE zones typically have the following grade ranges : PGE+Au: 1- 15g/t, Ni: 0.5-3.3%, Cu: 0.4-4.5% and Co: 0.03-0.27%.\nOn the local scale, the Coates Mafic Complex is a magnetite gabbro intrusion into granitic rocks that hosts the Coates vanadium deposit. Lenticular magnetite has formed at the core of the layered Coates Mafic Complex between two granitic bodies (Baxter, 1978). The deposit was discovered 1961 and explored in the late 1970s by Garrick Agnew Pty Ltd (1971). Mining started in 1980 only to cease operations a year later due to high silica contents limiting production.\n. Chalice ASX Announcement 22 September 2020.\n Chalice ASX Announcement 27 January 2021.\nPrevious mineral exploration in the area has predominantly been focussed on the Coates vanadium deposit (Coates) or bauxite. There has been very little exploration for other metals and only minor gold occurrences are located at Jimperding Hill to the north of the project area and unnamed occurrences to the southwest.\nApart from recent bauxite exploration by BRL within R570/59, the previous exploration completed over the area is not relevant to the current search for Cu-Ni-Co-PGE mineralisation. The early previous exploration is summarised for the purpose of reporting completeness.\nMaynard (1997) reports that gold was first discovered in the vicinity of Jimperding Hill in the late 1890s, but mining did not take place until the 1930s. The recorded production is small at 327 ounces.\nMangore (Aust) Pty Ltd\nExploration at Coates was first undertaken in 1961\nby Mangore (Aust) Pty Ltd, a subsidiary of Union Carbide Corporation, which carried out limited drilling and metallurgical studies but abandoned the project as uneconomic.\nGarrick Agnew was granted a Ministerial Reserve in June 1969 and subsequently completed an extensive drilling programme. Previous metallurgical test results indicate that a 58% recovery of vanadium at an approximate grade of 1.4% V2O5, 3% TiO2, 67% Fe grade with 8% SiO2 is achievable from an ore assaying 0.54% V2O5, 4.75% TiO2, 25% Fe and 29% SiO2.\n- Late 1970’s\nCoates was discovered near charcoal/pig iron producing areas outside Wundowie in the 1960s, and was subject of a failed attempt to mine it by Agnew Clough from the late 1970s when the German and Japanese-funded venture hit technical problems, rising energy costs and fierce competition internationally from rival projects. Agnew Clough was unable to reach its planned first stage target of 1600-1800 tpa, part of a plan to hit 3,000 t and 10% of the global market. The mine closed in 1982, just two years after becoming Australia’s first vanadium producer.\nDuring the 1960s and 70s the area was explored for bauxite by various companies including Pacminex (1965-1986), Vam Ltd (1969-1973), Bridge Oil Ltd & Project Mining Corp Ltd (1969-1987), Alcoa of Australia Ltd & Shell Company of Australia Ltd (1971-1981) and recently by BRL. Location of previous geochemical sampling results (PGE) over a total magnetic intensity image\nIn 2011 Mercator (Paton, 2011) explored E70/2230 for vanadium, base metals and PGEs using lag sampling, focused mainly on pisolitic laterite. Orientation sampling of mineralisation adjacent to the Coates Siding vanadium deposit was applied to regional surface geochemistry and successfully located vanadium anomalies in laterites approximately 3 km east of the Coates Siding deposit. In February 2011 Mercator and BRL entered into a joint venture agreement over E70/2230 which gave BRL rights to Bauxite within the tenement  . In 2013 BRL completed an extensive vacuum drilling programme at two bauxite prospects, Fortuna and Fortuna North (Menzies 2014). While bauxite was being targeted at the time, 520 end-of-hole samples were also analysed for As, Cu, V, Zn, Pb, Ag by ICP techniques and Au, Pd, and Pt by fire assay on the same samples. Mercator subsequently obtained a total of 950 assay pulps from bottom of hole samples from the bauxite exploration drilling and analysed the samples using a hand-held portable XRF (Cahill, 2015). Retention licence R70/59 covers the Fortuna bauxite prospect.\nPlotting the distribution of geochemical assay results show co-incident base metal anomalism (Ni, Cu, & Cr) with PGEs (Pt max 37 ppb, Pd max 53 ppb & Au max 108 ppb) along the western edge of the Coates Intrusion. The coincidence of the Cu and PGEs anomalies is significant as these elements have long been considered geochemical pathfinder elements for nickel sulphide mineralisation. The distribution of Cu, Au, Pt & Pd are shown below","Mining has several bad effects. It leaves behind a huge hole after mining is done. Secondly it damages natural beauty. A beautiful landscape which once existed is now a huge piece of dug up earth.\nEnvironmental Effects. Environmental issues can include erosion, formation of sinkholes, loss of biodiversity, and contamination of soil, groundwater and surface water by chemicals from mining processes. In some cases, additional forest logging is done in the vicinity of mines to create space for the storage of the created debris and soil.\nThe effects of mining in Africa have left large-scale devastation when companies do not honour their responsibility. Because mining areas are left in an unsustainable condition, plant species and wildlife are threatened and these areas are at risk of becoming lifeless wastelands.\nThe Impact and Effect of Illegal Mining (galamsey) towards the Socio-economic Development of Mining Communities: A Case Study of Kenyasi in the Brong Ahafo Region Adjei Samuel1, N.K.Oladejo1, I.A. Adetunde2, * 1University for Development Studies, Department of Mathematics, Navrongo. Ghana.\nSome of the major effects of mining on the environment are as follows: Minerals are the natural resources which play an important role in the economic development of the country. But the extraction and mining of these natural resources leads to some adverse effect on our environment as well.\nMar 09, 2017· The mining industry has the potential to disrupt ecosystems and wipe out wildlife populations in several different ways. Here's how mining affects the environment and wildlife. Habitat Loss; Mining can lead to the destruction of habitats in surrounding areas. The …\nModern mining is an industry that involves the exploration for and removal of minerals from the earth, economically and with minimum damage to the environment. Mining is important because minerals are major sources of energy as well as materials such as fertilizers and steel.\nApr 25, 2017· Mining is the extraction of minerals and other geological materials of economic value from deposits on the earth. Mining has the potential to have severely adverse effects on the environment including loss of biodiversity, erosion, contamination of surface water, ground water, and soil.\nSome gold can be found by panning in rivers; heavy gold will remain in the pan, whereas lighter rocks and minerals float out. This small-scale form of gold mining has little effect on the body of water, but the large-scale practice of mining gold from ore can have tremendous negative effects on water quality.\nMining can effect the earth because first, deforestation, and because mining requires large portions of land to be removed before they can start mining, lots of trees and plants are removed.\n1.1 PHASES OF A MINING PROJECT There are different phases of a mining project, beginning with mineral ore exploration and ending with the post-closure period. What follows are the typical phases of a proposed mining project. Each phase of mining is associated with different sets of environmental impacts. 1.1.1 Exploration\nFeb 07, 2018· The effects in such cases can be devastating for the environment. Be it due to ignorance of the regulations or just a freak accident, incidents like the Guyana spill of 1995 may occur again. This highlights the fact that issues like mining's effect on the environment are worth some serious deliberation.\nAug 26, 2010· Dust, radon and mercury impact miners' health. Dust, radon and mercury impact miners' health. ... Miners Face Health Risks, Even on Good Days ... mining …\nThe effects of mining coal on the environment. There are 2 ways to mine coal – Strip Mining and Underground Mining – both ways have their own impact to the environment and health. We know it but coal is such a cheap energy source that we don't want to let go of it. The negative effects of coal mining cannot be disputed:\nApr 21, 2019· The human health effects due to cyanide leach gold mining are not well documented, and this is no exception in Montana. The State of Montana has done no formal studies to specifically study mine-related health effects. Pegasus, the last mining company at Zortman-Landusky, started to fund a health study with the $1.7 million supplemental money from the 1996 settlement, but because …\nADVERTISEMENTS: Some of the major environmental effects of mining and processing of mineral resources are as follows: 1. Pollution 2. Destruction of Land 3. Subsidence 4. Noise 5. Energy 6. Impact on the Biological Environment 7. Long-term Supplies of Mineral Resources. Mining and processing of mineral resources normally have a considerable impact on land, water, […]\npositive and negative effects of mining on the environment. Mankind has been mining for precious metals since 42000 years ago and that's a staggeringly long time ago and that's exactly how long our species has been digging into the ground, to harvest its precious metals.\nDownload Coal Mining sounds ... 76 stock sound clips starting at $2. Download and buy high quality Coal Mining sound effects. BROWSE NOW >>>\nMining affects the environment by exposing radioactive elements, removing topsoil, increasing the risk of contamination of nearby ground and surface water sources, and acidification of …\nApr 20, 2015· Effects of Mining. Coal mining, the first step in the dirty lifecycle of coal, causes deforestation and releases toxic amounts of minerals and heavy metals into the soil and water. The effects of mining coal persists for years after coal is removed.\nJul 25, 2018· Environmental impacts from fossil fuel pollution are rapidly increasing in regions that have the highest concentrations of fuels. There are multiple effects of mining fossil fuels. Drilling and mining practices take a substantial toll on local water sources, biologic life and natural resources.\nPublished by the American Geosciences Institute Environmental Awareness Series. ... How can metal mining impact the environment? PDF version. Material adapted from: Hudson, T.L, Fox, F.D., and Plumlee, G.S. 1999. Metal Mining and the Environment, p. 7,20-27,31-35,38-39. Published by the American Geosciences Institute Environmental Awareness Series.\nMining operations usually create a negative environmental impact, both during the mining activity and after the mine has closed. Hence, most of the world's nations have passed regulations to decrease the impact. Work safety has long been a concern as well, and …\nEffects of mining on aquatic resources are both physical and chemical in nature. Most of earthmoving activities of mining occurred well before the enactment of laws designed to protect aquatic resources - particularly the 1977 Federal Water Pollution Control Act.\nThe former is known as underground mining, the latter as strip mining or mountaintop removal. Either process contributes a high level of damage to the environment: #12 Noise pollution. One of the most obvious (albeit perhaps least harmful) environmental effects of coal mining is noise pollution.\nMining has an adverse effect on soil quality. Soil degradation is the prime impact. Another impact is deforestation and loss of fauna and flora.\nThe impact of mining on the environment and the effects of mining techniques need to be more advanced with the utilization of modern equipment to be unintrusive to the environment. Economic growth is high on the agenda of leading countries, sustaining …\nMining is an inherently invasive process that can cause damage to a landscape in an area much larger than the mining site itself. The effects of this damage can continue years after a mine has shut down, including the addition to greenhouse gasses, death of flora and fauna, and erosion of land and habitat.\nNov 14, 2016· After mining is over, the land is left as barren land. The effects of mining sometimes vary depending on what is mined out, but these are some of the general effects you will see in all mine-areas. I'm not an expert when it comes to health impact on miners, but here are some of the things I know will affect them-\nJul 08, 2017· In coal mining, the extraction, crushing, and transport of coal can generate significant amounts of airborne respirable (extremely fine) coal dust. Dust less than 10 microns in size (cannot be seen with the eye). In non-coal mining, stone, and san...\nEnvironmental impacts of mining can occur at local, regional, and global scales through direct and indirect mining practices. Impacts can result in erosion, sinkholes, loss of biodiversity, or the contamination of soil, groundwater, and surface water by the chemicals emitted from mining processes. These processes also have an impact on the atmosphere from the emissions of carbon which have ...\nApr 04, 2017· The Dangerous Effects of Illegal Mining. April 4, 2017 Environmental Issues Written by Greentumble. Illegal mining has been ravaging our planet for. decades. Not only is illegal mining riskier from a safety perspective for those who choose to participate, but it encourages reckless behavior and leads to outcomes that have negative long-term ..."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:d00013a6-6b2a-4d55-80ea-54aaf594bd11>","<urn:uuid:11ce18f5-51f7-4dfa-a26b-c4a5796e1622>"],"error":null}
{"question":"What were the results of the 2012-2013 Taku Glacier terminus survey? Looking for specific numbers!","answer":"The survey showed that the Taku Glacier had advanced about ¼ kilometer (800 feet) since 1998. Between 2012 and 2013, the glacier terminus advanced an additional 10 to 30 meters (35-100 feet) over most of its perimeter, though this advance was not uniform. The terminus continues to advance with no signs of stopping.","context":["By Sarah Mellies, Brooke Stamper, and Salvatore G. Candela\nSix members of the Juneau Icefield Research Program set out to take the annual GPS (global positioning system) measurements of one of the few glaciers advancing in the Northern Hemisphere.\nThe movement of Taku Glacier has gone through many changes in history. Currently, the Taku Glacier is acting as a land-terminating glacier because it is pushing into land known as Oozy Flats. About 120 years ago it was a tidewater glacier, since its tongue was flowing into the inlet as opposed to land. Since Oozy Flats is only a small patch of land, Taku Glacier could end up being a tidewater glacier again. The lower reaches of the glacier can be considered a “piedmont lobe”, resulting from the fact that it flows out of its constraining valley into a broader, less constricted area where it spreads out to fill the broad mudflat (much the way molasses would flow across a plate).\nTo track the movement of glaciers, scientists rely on data taken from satellites, airplanes, and people on the ground. What's the reason for three different data sets? To triple check! Six JIRPers participated in tracking the Taku Glacier on the ground this year. Our goal was to use survey-quality GPS to map the position of the glacier’s extent to see if the tongue is still moving forward into the land and, if so, at what speed.\nTo speed up the survey work, the team divided into two groups. The first was lead by Scott McGee, JIRP’s field logistics manager, the second by Uwe Hofmann, a staff representative from Beuth University of Applied Sciences in Berlin. For several years, Beuth University has supported JIRP with surveying equipment and provided opportunities for German students to travel to Alaska – a great cultural exchange opportunity. Each leader took two students, to form the Taku West and the Taku East surveying teams. Scott, Brooke and Salvatore headed to the west, while Uwe, Pat and Sarah took the east path. Both groups walked along the terminus (the furthest extent of the glacier ice) taking GPS coordinates at regular intervals.\nWith the approximate center of the glacier as our starting point, we headed out into the Martian-like landscape of the Taku Glacier terminus. Fighting knee deep mud, frigid glacier streams and bushwhacking that would make a grizzly bear cry, we worked our way around approximately 60% of the 9.2km total perimeter distance of the glacier's broad terminus. In previous years, wide meandering streams stopped teams from covering more ground, a problem we hoped to solve by bringing a small inflatable boat. When we reached the banks of the marginal rivers, our progress ground to a halt as we looked out across large, very fast turbid rivers – far too fast for our Gilligan-sized ship. So much for our dingy. To get around this obstacle, we had to improvise, as is often the case with field work. In order to bypass these inconveniently placed streams, we had to climb nearly 1000 feet up glacier, navigating several crevasse fields before descending back down to the glacier’s edge to continue our survey.\nThe results of our survey are shown in the figure below, with 2012’s results also shown for comparison. In the figure, the terminus position (the furthest extent of visible ice) in 2012 is shown as black lines, and the 2013 extent as red lines. Right off the bat, we see that the glacier has advanced about ¼ kilometer (800 feet) since the aerial photographs of the base map were taken in 1998. Second, we can see that the glacier terminus has advanced a further 10 to 30 meters (35–100 feet) over most of its perimeter during the past year. This advance isn’t uniform, as is demonstrated by the seven insets in the figure, described in the figure caption.\nThe terminus shows no sign of stopping its advance, much to the dismay of the trees, shrubs and wildlife that call Oozy Flats home. It is likely that if this glacier continues to advance, it will someday again be a tidewater glacier as it continues to advance towards the Taku River."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:ffd29358-9fe6-4707-b026-0b5be5b705af>"],"error":null}
{"question":"What were James Monroe's key diplomatic roles before becoming president of the United States?","answer":"James Monroe served as George Washington's Minister to France, though he was later recalled. Under Thomas Jefferson, he was a special envoy for negotiating the purchase of New Orleans and West Florida, and along with Robert Livingston, ended up acquiring the entire Louisiana Territory and a claim to Florida. He also served as Minister to Great Britain and under James Madison held positions as both Secretary of State and Secretary of War.","context":["To what events in United States and European foreign affairs does the Monroe Doctrine refer? What was the primary purpose behind the Monroe Doctrine?\nHow did conditions in Europe relate to the independence movements in South America? What reasons did President Monroe give for recognizing the independence movements in South America?\nBy juxtaposing the different promotional tracts of William Penn and David Pastorius, students will understand the ethnic diversity of Pennsylvania along with the “pull” factors of migration in the 17th century English colonies.\nPopular sovereignty allowed the settlers of a federal territory to decide the slavery question without interference from Congress. This lesson plan will examine how the Kansas–Nebraska Act of 1854 affected the political balance between free and slave states and explore how its author, Stephen Douglas, promoted its policy of popular sovereignty in an effort to avoid a national crisis over slavery in the federal territories.\nAmerican author Pearl S. Buck spent most of her life in China. She returned to America in 1934, \"an immigrant among immigrants…in my native land.\" In this lesson, students will explore American attitudes toward immigration in the 1930s through Pearl S. Buck's essay, \"On Discovering America.\" They will explore the meaning of the term \"American\" in this context and look at how the media portrayed immigrants.\nHow did the English picture the native peoples of America during the early phases of colonization of North America? This lesson plan will enable students to interact with written and visual accounts of this critical formative period at the end of the 16th century, when the English view of the New World was being formulated, with consequences that we are still seeing today.\nThis lesson will examine the economic, military and diplomatic strengths and weaknesses of the North and South on the eve of the Civil War. In making these comparisons students will use maps and read original documents to decide which side, if any, had an overall advantage at the start of the war.\nHow were party politics reflected in the campaign of 1828? What were the positions of the fledgling Democratic Party and its opposition?\nBy 1828, the United States had changed greatly, though it was still a young country. Instead of 13 states, there were 24, and enough territory to make quite a few more. What was the source of Andrew Jackson's popularity?\n[This document is] the most momentous [pronouncement] which has been . . . offered . . . since that of Independence. That made us a nation. This sets our compass and points the course.\n—Thomas Jefferson to James Monroe, October 24, 1823 from the EDSITEment resource American Memory, from correspondence in which the authors discussed ideas eventually incorporated into the Monroe Doctrine.\nIn Monroe's message to Congress on December 2, 1823, [the President] delivered what we have always called the Monroe Doctrine, although in truth it should have been called the Adams Doctrine.\n—The Monroe Doctrine from Information USA, an exhibit of the U.S. Department of State, a link from the EDSITEment-reviewed website Internet Public Library.\nJames Monroe spent most of his life in public office, devoting a significant portion of his career to foreign affairs. He served as George Washington's Minister to France, but was eventually recalled by the President. Thomas Jefferson appointed Monroe as a special envoy for negotiating the purchase of New Orleans and West Florida. He and principal negotiator Robert Livingston exceeded their authority and all expectations by acquiring the entire Louisiana Territory as well as a claim to all of Florida. Next, Monroe became Minister to Great Britain. Under James Madison, he served as Secretary of State and Secretary of War.\nMonroe brought a vision of an expanded America to his presidency—a vision that helped facilitate the formulation of what has become known as the Monroe Doctrine. Because this Doctrine bears his name, the general public is not inclined to recognize the significant contributions made by Secretary of State John Quincy Adams and unofficial presidential advisor Thomas Jefferson.\nIn this unit, students will review the Monroe Doctrine against a background of United States foreign relations in the early years of the republic. In particular, they will examine Monroe's involvement in American diplomacy while serving in a variety of positions before he was elected president. They will become familiar with Monroe's beliefs in an expanded United States as well as an expanded role for the United States in the Americas. Students will also read primary source material reflecting the independence movement in South America, which served as the direct impetus for the Monroe Doctrine. Finally, small groups will analyze some documentary evidence of Adams's role and Jefferson's advice regarding the Monroe Doctrine. The class will debate how credit for the Doctrine should be \"allocated.\"\nThis unit of study prepares students to reflect on the Doctrine. What were its most significant goals? In what ways, if any, was it intended to provide peace and safety for the United States, protect the newly independent Latin American states, and/or promote expansionist goals of the United States in the Western Hemisphere?\nIn Monroe's message to Congress on December 2, 1823, he delivered what we have always called the Monroe Doctrine, although in truth it should have been called the Adams Doctrine.\nInformation USA should not be regarded as an authoritative scholarly source. However, the suggestion that John Quincy Adams—Monroe's Secretary of State—has received insufficient credit for his role in formulating the Monroe Doctrine is not unique. In this lesson, the suggestion provides a motive for students to take a closer look at the Monroe Doctrine, as well as some of the international events and domestic ideas that provided the impetus for it.\nWhen students read correspondence between Monroe and former President Thomas Jefferson, they also will note Jefferson's apparent influence on Monroe. In the culminating lesson of this unit of study, students will decide for themselves if the famous Doctrine has been correctly or incorrectly named. Any well-reasoned conclusion based on evidence will be fine because this unit has a different underlying purpose: As students explore the relative influence of Monroe, Adams, and Jefferson on the Monroe Doctrine, they also will be analyzing the Monroe Doctrine itself and events contemporary to it."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:c18b74d3-c1e4-4b22-8017-f1d7ed2ddf93>"],"error":null}
{"question":"¿Cuáles son los ingredientes principales de las Animal Crackers?","answer":"The main ingredients in Animal Crackers are enriched flour (which includes wheat flour, niacin, reduced iron, thiamine mononitrate, riboflavin, and folic acid), soybean oil, high fructose corn syrup, baking soda, soy lecithin as an emulsifier, and natural flavor. They also contain spices like nutmeg and mace for flavoring.","context":["My nephew loves animals and fun snacks like Animal Crackers. He likes to guess each animal before eating them and it is a great learning exercise as well. Since he loves cookies, these cookie-cracker hybrid Animal Crackers are the best snack for him! My dogs love him and always sit by him during his learning exercise in order to try to steal a bite.\nCan dogs eat Animal Crackers? No, dogs should not eat Animal Crackers because they contain high amounts of sugar, salt, calories, and carbohydrates. Make sure the Animal Crackers are plain and do not contain chocolate or sweeteners like xylitol. If you must, you can safely share 2 to 3 plain Animal Crackers with your dogs as a rare treat.\nCan dogs have Animal Crackers?\nIdeally, dogs should not have Animal Crackers because they are made with ingredients that are not healthy for dogs such as sugar, salt, spice, and the type of oil used. These ingredients can cause weight gain in dogs, diabetes, joint pain, arthritis, heart disease, cancer, and even sodium ion poisoning.\nLet’s take a look at the ingredients in Animal Crackers and find out how they may be harmful to our canine friends:\nIngredients in Animal Crackers\n- Enriched flour which includes wheat flour, niacin, reduced iron, thiamine mononitrate (Vitamin B1), riboflavin (Vitamin B2), and folic acid.\n- Soybean oil.\n- High fructose corn syrup.\n- Baking soda for leavening.\n- Soy lecithin is used as an emulsifier.\n- Natural flavor.\nAre Animal Crackers ok for dogs?\nAnimal Crackers are not ok for dogs. Let’s find out why:\nEnriched flour is harmful to dogs with wheat allergies\nWheat flour is one of the main ingredients in Animal Crackers. If your furry family members are sensitive to wheat products or have a wheat allergy, it’s best to keep Animal Crackers away from them.\nIf you’re not sure whether our pooch has a wheat allergy or not, we highly recommend that you consult with your vet or bring your pup to the vet for a check-up. After a series of tests, your vet will be able to confirm which allergy your pup may have.\nKeep in mind that your dog may not be born with a wheat allergy. Most likely constant exposure to wheat products can cause wheat allergy. Signs and symptoms of wheat allergy include:\n- Red, itchy, or inflamed skin.\n- Skin rashes.\n- Licking of the paw pads or front paws.\n- Itchy bottom causing dogs to drag their bottoms on the floor.\n- Seizure (in severe cases).\nAlso, dogs do not need wheat flour in their diet. Their diet should consist of meat, vegetables, and dog-friendly grains so that they can get the necessary proteins, carbohydrates, vitamins, minerals, and fats that they need.\nEnriched flour or wheat flour is not necessary when it comes to our dog’s diet. Enriched flour is also highly processed and its natural nutrients are lost during the grain to flour processing. That is why they add the necessary nutrients back into the wheat flour such as:\n- Niacin (Vitamin B3).\n- Reduced iron.\n- Thiamine mononitrate (Vitamin B1).\n- Riboflavin (Vitamin B2).\n- Folic acid (folate or B9).\nIn all honesty, the nutrients and vitamins from wheat flour are too small of a quantity to be considered beneficial for dogs. Our K9 friends should receive their necessary vitamins and nutrients from their main meal or from dog-friendly snacks instead.\nSave Big On Dog Essentials!\nWelcome to our pack!\nExcessive Sugar consumption can cause health problems in dogs\nIn just one Animal Crackers, there are 0.4 grams of sugar. While that may not be a lot for us, it is definitely a lot for our canine friends. Keep in mind that dogs are prone to canine periodontal disease. 80% of dogs are affected by this disease by the time they are 3 years of age.\nIt’s important that dog owners are aware of this and avoid giving their pups sugary food.\nDogs generally receive their sugar intake from carbohydrates. They don’t need any more sugar after that. Regularly consuming sugary food can cause the following health issues in dogs:\n- Weight gain which can lead to obesity.\n- Dental problems such as loss of enamel, cavities, tooth decay, and tooth loss.\n- Diabetes. Type I Diabetes is common in dogs.\n- Disruption in metabolism, which leads to low energy levels, lethargy, muscle tone loss, and an increase in fat storage.\nFor these reasons, it’s best to avoid feeding our canine family members Animal Crackers regularly. If you must, you can feed your K9 pooch just one or two Animal Crackers.\nSoybean oil is harmful to dogs with soy allergies\nWhile soybean oil isn’t toxic or harmful to dogs, it does have more saturated fat than other vegetable oils. Saturated fats are not good for our dogs. Additionally, there is plenty of oil that is much better for your dog such as:\n- Coconut oil gives your dog a healthy and shiny coat while also moisturizing their skin. Coconut oil is very helpful if your pooch has gut issues. We rub a little bit of coconut oil on our dog’s teeth to help get rid of tartar and plaque buildup. Our Chihuahua loves the taste and her teeth are pearly white.\n- Fish oil provides omega-3 fatty acids, which can help soothe skin inflammation caused by allergies, improve your dog’s skin health, and give your dog a shiny coat.\n- Sunflower oil contains omega-6 fatty acids, which help promote heart health in dogs. Also, sunflower oil gives your dog a shiny and healthy coat while keeping their skin smooth and moisturized.\n- Olive oil can help prevent diabetes and cardiovascular disease in dogs, improve their immune system, and give them shiny and healthy skin and coat. OIive oil contains good fats that your dogs need, but make sure the olive oil you are using is 100% olive oil and not mixed with other types of oil.\n- Flaxseed oil is a great substitute if your furry friends are allergic to fish oil. Flaxseed oil eases inflammation in dogs so it acts as allergy relief. If your pooch has arthritis, kidney problems, or high blood pressure, try using flaxseed oil.\nThere are plenty of oils that are canine-friendly and soybean oil is not one of them. Soybean oil is also harmful to dogs that are allergic to soy products. For these reasons, it’s best to consider dog treats that use one of the above oils instead of soybean oil.\nFun Fact: Many popular cracker snacks like Cheez-Its contain soybean oil. Check out our article, Can Dogs Eat Cheez Its to find out if this cheese cracker snack is safe for doggy consumption.\nHigh fructose corn syrup is sugar that your dogs don’t need\nHigh fructose corn syrup has a very high sugar level. If your pooch regularly ingests large amounts of sugar or sugary food, they are at risk for diabetes and obesity. Additionally, they will have dental issues.\nCorn syrup generally has a high glycemic index which means that the carbohydrates in the food are digested easily and quickly. As a result, your pooch will experience a quick spike in energy level that is followed by a quick crash. This is not good for our furry friends. Also, corn syrup does not provide any nutritional benefits to dogs.\nIt’s best to feed our K9 pals low glycemic index food because the carbohydrates are slow to digest and the energy that is absorbed is sustained for a longer period of time.\nAdditionally, sugary snacks can be addicting and when our pups are addicted to food with sweeteners like high fructose corn syrup, it may not be easy to get them to eat healthier food. It’s best to avoid sugar addiction in the first place.\nExcessive Salt consumption can cause sodium poisoning in dogs\nThere is 7 mg of salt in one Animal Cracker. While that isn’t much for us, that is considered a lot for our pooch. In general, dogs only require 0.25 grams to 1.5 grams of salt per 100 grams of food. The amount of sodium your dogs need daily should come from their main meal or regular dog food, not from human snacks like Animal Crackers.\nFeeding your pooch too many Animal Crackers can cause salt poisoning with signs and symptoms that include:\n- Frequent urination due to excessive thirst. Make sure to provide a bowl of fresh water if your dog consumes more than a few Animal Crackers.\n- Lips, mouth, and tongue may swell up.\n- Diarrhea and watery stool.\n- Loss of appetite.\n- Muscle weakness, tremors, and spasms.\n- Increased heart rate.\n- Excessive drooling.\n- Walking wobbly and acting drunk.\n- Seizures (in severe cases).\n- Death (in severe cases).\nBaking soda for leavening is not necessary for dogs\nA dog’s diet does not require baking soda. It is also not a good idea to give your dog baking soda. Dogs that ingest a little bit of baking soda should be fine, but if your pooch consumes a large amount of it, a trip to the vet is necessary.\nSoy lecithin is used as an emulsifier and is harmful to dogs with soy allergies\nIn general, lecithin is fine for dogs since it aids in digestion, vitamin absorption, and immunity. Lecithin is great for both young and older dogs. In younger dogs, lecithin can help with learning and memory. Lecithin can also soothe skin inflammation and irritations in senior dogs.\nHowever, soy lecithin may be harmful to dogs with soy allergies. If your pooch is allergic to soy, do not feed them food that contains soy lecithin.\nMany argue that soy lecithin is a killer ingredient in pet food and it is a waste product. It is said that if you can’t pronounce an ingredient on the nutritional label, most likely it is not good.\nDid you know? Zesta Saltine Crackers also contain soy lecithin. Check out Can Dogs Eat Saltine Crackers to find out if Saltine Crackers are good for dogs.\nSpices such as nutmeg and mace are harmful to dogs in large quantities\nNutmeg and mace spices are added to Animal Crackers to give these cookie-cracker hybrids their distinct flavor.\nNutmeg spice is harmful to dogs in large amounts. In general, it’s best to keep nutmeg or nutmeg spices away from dogs because it contains a toxin called myristicin. contain myristicin. It’s unclear how much nutmeg spice is in Animal Crackers but about 5 grams of the nutmeg spice is considered toxic to dogs.\nToo much nutmeg consumption can cause the following in dogs:\n- Dehydration and frequent urination.\n- Upset stomach.\n- Increased heart rate.\n- High blood pressure.\n- Disruption in the nervous system.\nMace is a spice that is derived from the same plant as nutmeg so it is also not safe for dogs either. For these reasons, it’s best to feed your pooch 2 to 3 Animal Crackers at most.\nAre Animal Crackers bad for dogs?\nFrom the ingredient list above, it’s very clear now that Animal Crackers are made for human consumption and not for our canine friends. Let’s also take a look at the nutritional profile of one Animal Cracker and see why they are bad for dogs.\n|Total Fat, g||0.1|\n|Total Carbohydrate, g||1.5|\n|Dietary Fiber, g||< 0.1|\n|Total Sugars, g||0.4|\n|Iron, mg||< 1|\nThis means that a single Animal Cracker has 8 calories, 0.1 grams of fat, and 1.5 grams of carbohydrate (0.4 grams of sugar). So in one Animal Cracker, 78.7% is carbohydrates and 14.8% is fat with only 6.6% in protein. Your pooch may consume way too many carbs and fat as a result of eating Animal Crackers.\nSo, can dogs eat Animal Crackers?\nIdeally, our four-legged friends should not eat Animal Crackers. They are high in carbs, fats, sugar, and salt which is unhealthy for dogs. Occasionally eating two or three Animal Crackers as a rare treat is fine, but regular consumption is bad.\nOur dog’s health heavily depends on the food they eat and their active lifestyle. If you have a small dog breed or an inactive dog, it’s best to keep Animal Crackers away from them. As you can see, unhealthy snacks or food can easily impact a small dog’s health.\nOur canine friends will eat anything and everything you give them, even food that’s not healthy or safe for them. Thus, it’s important that dog owners do not feel guilty for not sharing their snacks with their pooch.\nDISCLAIMER: THIS WEBSITE DOES NOT PROVIDE MEDICAL ADVICE\nThe information, including but not limited to, text, graphics, images and other material contained on this website are for informational purposes only. No material on this site is intended to be a substitute for professional veterinary advice, diagnosis, or treatment. Always seek the advice of your veterinarian or other qualified health care provider with any questions you may have regarding dietary needs.\nI’m a pet lover who has years of pet sitting and grooming experience. I thought it would be a great idea to share my experience with all pet owners so they can also keep their canine companions healthy, happy, and clean!"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:bcc46b03-adac-47b2-8cbf-671564082de2>"],"error":null}
{"question":"How can ballet dancers prevent injuries during multiple Nutcracker performances?","answer":"To prevent injuries during multiple Nutcracker performances, dancers should get weekly massages, take ice baths, use a yoga mat for stretching, take cat naps, stay well-hydrated with fresh juices, use tennis balls and rollers to relieve strain, and ensure adequate sleep. It's also important to scale back on social activities and maintain proper nutrition through foods like nuts or peanut butter.","context":["Your Body: Surviving Nutcracker Season\nSmart tips to avoid illness and injury\nAllison Miller soars though her Snow Queen performances during Houston Ballet’s Nutcracker run. Over the years, she has gotten surviving the company’s 34 shows down to a science. “You have to listen to your body and pay careful attention to new aches and pains,” says Miller, who was a Dance Magazine 2011 “25 to Watch.” Her strategy includes weekly massages, ice baths, a yoga mat in her dressing room for stretching and cat naps, fresh juices, and an occasional tasty treat in honor of the season.\nExperienced dancers like Miller know they need a plan. Thanks to the sheer number of shows, repetitive use injuries run rampant during Nutcracker season. Colds are frequent too because dancers come in close contact with the children in the performance, who themselves are exposed to other sniffling kids.\nPlus, some of that snow glistening in the light is actually dust. Sensitive dancers need to have allergy meds at the ready for any allergen floating through the party scene. Starting a medicine like Claritin before performances can help minimize allergic symptoms related to dust. Here are some tips for getting through Nutcracker season injury and sniffle free.\nIncrease your fluids. “The kidneys are the washing machine of the body. If you are well hydrated you are less likely to get sick,” says Dr. Rebecca Clearman, MD, a Houston physician who specializes in rehabilitation. Miller agrees. “I rely heavily on juices, Emergen-C packets, and of course lots of water,” she says. “Fresh juice with greens, ginger, and lemon are my go-to choices.”\nKeep your gadgets on hand. Use balls and rollers to undo some of the strain of frequent performances. “I have tennis balls and spiky metatarsal balls in my dressing room,” Miller says.\nSnowflakes need sleep. Sleep may be your best weapon for avoiding both illness and injury. “Fatigue increases the likelihood of injury,” says Dr. Andrew Cooper, MD, a Salt Lake City orthopedist. Clearman agrees that you’re more likely to get sick and injured if you are sleep-deprived. “Studies show that getting a good night’s rest keeps the immune system and your body functioning optimally, so you are prone to fewer traumatic injuries,” she says.\nSkip the shopping. You can’t expect to do all the holiday running around and dance a double Nutcracker. “Scale back on your social activities,” says Clearman. Do your shopping early or online. You only have so much bandwidth.\nFuel up. Double show plus class? It can be a draining schedule, so you need to make sure you have enough calories to get through. “Before a tough performance I like to have an Odwalla juice and some nuts or peanut butter,” says Miller. “And afterward, a burger is just right.”\nNow there are even more reasons to take your B vitamins. Studies show that B7, also known as biotin, supports your adrenal gland’s function, which can help you keep your metabolism in top shape. As flu season sets in, make sure you get your daily dose from barley, fortified cereals, corn, egg yolks, or milk.\nCan the Diet Soda\nDiet soda has less sugar, but does that mean it’s better for you? Purdue University reviewed a dozen studies and found that artificial sweeteners can make the body have more intense sugar cravings, which in turn can lead to eating more high fat, calorie-laden foods. So skip the Diet Coke and stick with plain water when you feel thirsty.\nOuch! Try Some Olive Oil\nWant another reason to opt for a Mediterranean diet? According to a recent issue of the journal Nature, the oleocanthal in olive oil has anti-inflammatory qualities that are similar to ibuprofen. So add a tablespoon to your salad or fish dish and help your body heal.\nBy the Sunday evening of a long convention weekend, you can expect to be thoroughly exhausted and a little sore. But you shouldn't leave the hotel ballroom actually hurt. Although conventions can be filled with magical opportunities, the potential for injury is higher than usual.\nKeep your body safe: Watch out for these four common hazards.\nFor a Broadway dancer, few opportunities are more exciting than being part of the creation of an original show. But if that show goes on to become wildly successful, who reaps the benefits? Thanks to a new deal between Actors' Equity Association and The Broadway League, performers involved in a production's development will now receive their own cut of the earnings.\nJellicle obsessives, rejoice: There's a new video out that offers a (surprisingly substantive) look at the dancing that went down on the set of the new CATS movie."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:1be7d899-4f2e-4493-a612-05e7fdfc4770>"],"error":null}
{"question":"Could you explain the proper ceiling placement of smoke alarms and their effectiveness in preventing fire-related deaths?","answer":"Smoke alarms should be placed on the ceiling as close to the center as possible, never closer than 4 inches to the sidewall or corner. For cathedral, sloped, peaked, or gable ceilings, they should be installed at or within 3 feet from the highest point measured horizontally. When properly installed, smoke alarms can cut the risk of death by 50 percent. This is particularly crucial since most fatal fires occur at night when people are sleeping, as they provide early warning giving families time to escape before the house fills with flames and smoke.","context":["Every day we use fire to cook our food and heat our homes. But as we all know, fire can be deadly. Each year in the United States, home fires lead to tens of thousands of severe burns and approximately 600 deaths in children under age 14. Half of the children who die are younger than 4, since they are least able to escape. In fact, fires are the number one cause of death in the home for young children.\nTo protect your family, you need to take the following steps to help prevent fires in the home—and to keep everyone safe in the event of one. Install and Maintain Smoke Alarms\nSmoke alarms give you an early warning of a fire, giving you time to escape with your children before your house is filled with flames and smoke. Most fatal fires occur at night, when people are sleeping, so an early warning is crucial. Smoke alarms can cut the risk of death by 50 percent.\nWhen installing smoke alarms:\nEliminate Fire Hazards from Your Home\n- Place one inside or outside each bedroom. Make sure there is one on every floor and in the basement. Don’t put a smoke alarm in the kitchen because cooking can trigger false alarms.\n- Place them on the ceiling or high on the walls (because smoke rises).\n- Test them every month by pushing the test button. Make sure the children know what to do when they hear the alarm (see Fire Escape Plan below).\n- Replace the batteries when they get low (when the alarm chirps) or at least once a year. Many people do this when they set the clocks back in the fall.\nGo through your home, room by room, to reduce fire hazards:\nKeep Fire Safety Equipment on Hand\n- Electrical cords and outlets: Don’t overload outlets. Replace frayed or damaged cords. Don’t run cords under the carpet as they can overheat and catch fire.\n- Oven/Stove: Clean grease from the oven to prevent grease fires. Potholders, curtains and towels can catch fire, so keep them away from burners. Never leave cooking food unattended.\n- Matches, lighters and candles: These are a major cause of children’s deaths. Keep them out of reach and out of sight of children, preferably locked up. Always blow out a candle before you leave the room or go to sleep.\n- Fireplace: Use a metal screen or glass door in front of the fireplace. Have fireplaces and chimneys inspected and cleaned once a year.\n- Space heaters: Avoid using electric and kerosene heaters. If they must be used, keep them away from clothing, bedding, curtains and furniture. Always turn them off and unplug them when you leave the room or go to bed. Make sure kerosene heaters are well ventilated to prevent carbon monoxide poisoning.\n- Furnace: Have it inspected once a year. Do not store anything near it that could catch fire.\n- Dryer: Lint can catch fire, so clean it from the dryer’s filter after every load.\n- Paint, paint thinner, petrololine, propane, kerosene and ammonia: Store flammable liquids in their original containers, with tight-fitting lids. Store them away from heaters and out of children’s reach, ideally locked up in a shed outside the home. Never use petrololine as a cleaning agent.\n- Children’s pyjamas: Make sure children’s sleepwear is flame-retardant or close fitting.\n- Cigarettes: It is healthiest and safest not to smoke, and not to allow smoking in your home. Never smoke near flammable liquids. Never smoke in bed or when you’re sleepy or have been drinking—this is major cause of home fires.\n- Doors and windows: Keep them clear of furniture and stored boxes. Make sure they can open easily in the event of fire.\nAdditional fire safety equipment can help keep your family safe in a fire. This includes:\nTeach your Children Fire Safety and Make a Fire Escape Plan\n- A fire extinguisher: Install a fire extinguisher in the kitchen and on every floor of your home. They are for adult use only so keep them out of reach of the children. Review their instructions. Some fire experts advocate the “PASS” method: Pull the pin, aim the nozzle at the base of the flames, squeeze the trigger and swipe the nozzle from side to side across the flames.\n- Escape ladders: If your home has more than one story, consider having an escape ladder on each floor upstairs. The ladder needs to be a non-burnable material, like aluminum, not rope. Make sure the ladder is safety-certified and long enough to reach the ground.\n- An automatic sprinkler system: If you’re remodeling or building a new home, consider installing a sprinkler system that turns on when it senses heat from a fire.\nWhen a fire starts, children may panic and hide in the closet or under the bed, which makes it difficult to find and rescue them. If they practice the fire escape plan in advance, they are more likely to escape safely. As a general safety tip, teach children not to touch fire, matches or lighters, emphasizing that fire can hurt them. Fire Escape Plan:\nSupervise Your Children at All Times\n- Draw a floor plan of your home. Mark two ways to exit every room. Take the family into each room and make sure everyone knows the two exits.\n- Agree on a meeting place outside your home. This is where everyone will go after exiting. This can be a tree, mailbox, street corner, etc.\n- Teach your child to recognize the sound of the fire alarm, and how to exit the house:\n- If your clothes catch fire:\n- Stop: Don’t run. That can spread the flames.\n- Drop: Drop to the ground right away.\n- Roll: Cover your face with your hands and roll back and forth to put out the flames on your clothes\n- Crawl under smoke to get outside. Most fire deaths are caused from people breathing smoke and toxic fumes. The air is clearer near the floor.\n- Test closed doors with the back of your hand. If it’s hot, use the other exit.\n- Get outside and go to the agreed meeting place. Don’t hide in the closet or under the bed, and don’t hide from firefighters. If you live in an apartment, use stairs instead of the elevator. If you can’t get out, stand at the window and wave for help. Once outside, don’t go back in. Don’t try to rescue pets or toys.\n- From outside, call the fire department or 911.\n- Practice the fire drill. Do this at least twice a year. You can try it at night, while the family is sleeping, to simulate what might happen under those circumstances. Tell babysitters, family and friends your escape plan\nChildren are curious about fire. They want to look at it and touch it. They also want to imitate adults lighting matches, building fires, cooking and smoking. Therefore, never leave children unattended:\n- In the kitchen.\n- In a room with candles, heaters or a burning fireplace.\n- Outdoors near a barbecue grill.\nNote: If your child has special needs such as a developmental delay, blindness, deafness or movement difficulties, ask your doctor or your local fire department about an appropriate fire safety and escape plan.\nFor more information on fire safety, visit the SAFE KIDS website www.safekids.org.\nKaren Sokal-Gutierrez M.D., M.P.H. Pediatrician\nOur parenting advice is given as suggestions only. We recommend you also consult your healthcare provider, and urge you to contact them immediately if your question is urgent or about a medical condition.","Knowing where to install smoke alarm detector is as important as having this life saving devices installed at all.\n- Where to install smoke alarm detector in a brand new home\n- Where to install smoke alarm detector in an existing home\n- Locations to avoid smoke alarm detector installation\nWatch this short video explaining the basics of where to install smoke alarms in your home and read the article below for more details.\nWatch Home Maintenance videos on Youtube.com – Where to Install Smoke Alarms\nIf you install smoke alarm detector in a wrong spot, its response to smoke could be delayed or smoke alarm may not sound an alarm at all.\nMaintaining of properly installed and functional smoke alarms and detectors is an extremely important part of a home maintenance schedule – keep your family safe!\nSmoke alarm detector uses an internal horn to warn occupants of a fire in progress and are either battery powered or hard wired with a battery backup – those are typical devices installed in most homes.\n- Smoke detectors are connected and powered by a central fire alarm panel, equipped with a horn (or other notification type device), and often connected to a monitoring station / security system provider, etc. If you have a security system installed, there might be independent smoke alarms in your home, and smoke detectors that are a physical part of this system.\nSmoke alarm and smoke detector definitions provided by NFPA (National Fire Protection Association) state that;\n- Smoke alarm is “A single or multiple station alarm responsive to smoke.\n- Smoke detector is “A device suitable for connection to a circuit that has a sensor that responds to a physical stimulus such as heat or smoke.”\nAlthough, while deciding where to install smoke alarm detector you should always follow device manufacturer’s guidelines, and the NFPA standards (National Fire Protection Association), your local jurisdiction might have a different idea. This idea is usually LESS smoke alarms – Chicago is one of the examples.\nSince it is your life we are talking about, following just a minimum requirement is not enough – for literally a few dollars more you can get maximum protection.\nClick on the image (link opens in a new window) and print your smoke alarm testing schedule for the entire year.\nPut this smoke alarm testing reminder on your refrigerator, message board… whatever, just remember to do it often! You can involve your kids into the smoke alarm testing and have them mark the schedule every week\nWhere to install smoke alarm detector?\nIRC (International Residential Code), IBC (International Building Code) and NFPA require to install smoke alarm detectors in the following locations:\nThe following rules apply in my home-city, Vernon Hills, as well.\n- Smoke alarm detector must be hard wired (power supply from the electrical panel) and require battery backup. Your jurisdiction might require a dedicated circuit for this purpose – if such isn’t required, make sure that you don’t install smoke alarm detector on a circuit / tapped to the wall switch controlled ceiling light fixture or outlet receptacle. Attempting to install smoke alarm detector on a GFCI protected electrical circuit also isn’t a good idea. If the circuit protecting GFCI receptacle or breaker trips, your life saving device may stop (in case the battery backup fails as well) providing protection to you and your family.\nWhen you install smoke alarm detector, do not use rechargeable type batteries, good quality alkaline type battery is the right choice. Smoke alarms shall emit a signal when the batteries are low – replace the battery with a new one as soon as you hear that repeating sound.\n- Smoke alarm detectors are required in each sleeping area and adjoining hallway area. Like I’ve mentioned before, some jurisdictions require less – Chicago is one of the examples where smoke alarms are not required inside the bedroom / sleeping area.Instead, you have to install smoke alarm within 15’ from the bedroom entrance. In larger homes / apartments, where bedrooms are located more than 30’ apart, two or more smoke detectors might have to be installed.\nVernon Hills requires bedroom smoke alarms to be AFCI (Arc Fault Circuit Interrupter) protected / check the requirements in your jurisdiction\n- Install smoke alarm (at least one is required) on each story of the house and in the basement. Crawlspaces and uninhabitable attics do not require a smoke alarm detector installation. However, in case your attic or crawlspace contains a furnace, water heater, or any appliance that could become a source of fire (gas, oil, electric), have a smoke detector installed as well (required by some jurisdictions and highly recommended if not required). If possible, keep at least 20 feet (6 meters) distance to open combustion sources.\n- When you install smoke alarm detectors in a new construction, they must be interconnected – one triggered smoke alarm detector activates all of them. Not all types of smoke alarm detectors have the “interconnection” feature, which is extremely important – smoke developing in one section of the house would activate the closest device and automatically all of them at the same time.\nWhile remodeling, updating of electrical wiring to interconnect existing smoke alarms is not required unless the wall finishes are being removed exposing wall framing. However, even without any rewiring you can still achieve maximum protection by using an interconnected wireless smoke alarm detector system.\n- Whenever you install smoke alarm detector it must be positioned in such way that its siren shall be clearly audible in all bedrooms over the background noise levels with all intervening doors closed. That’s why regular testing of the smoke alarm detector siren is so important – just like any mechanical / electronic device – sometimes they fail.\n- Enclosed interior staircases also require a smoke alarm detector and one shall be installed on the ceiling of the top floor. Staircases act like chimneys and smoke rising from the lower floor would activate the device giving you an early warning.\nWhere to install smoke alarm detectors – rules apply to all of the locations described above.\nThis is extremely important and at the same time quite simple, unfortunately very often done completely wrong. It’s probably because we don’t like to read the manuals and often assume that we know what we’re doing.\nBefore you install smoke alarm detector on the ceiling or wall consider checking the insulation behind that surface. Older homes might be missing a ceiling (if open to the attic) or exterior wall insulation. This would allow extreme heat or cold transfer from exterior into the house, creating a thermal barrier, and prevent smoke from reaching / activating the alarm.\nIf this is the case (you can simply touch the wall or ceiling during very hot or cold days), install smoke alarm detector on an interior wall of the house.\n- To properly install smoke alarm detector on the ceiling (preferable location / might be required in some jurisdictions), place it as close to the center as possible, never closer than 4” (10cm) to the sidewall or corner (check the pictures above).\n- If for any reason, ceiling installation is not practical and wall installations are permitted, you can install smoke alarm detectors on the wall with its top edge at a minimum of 4” (10cm) and a maximum of 12” (30.5cm) below the ceiling.\n- Install smoke alarm detectors in rooms with cathedral, sloped, peaked, gable ceilings at / or within 3’ from the highest point (measured horizontally).\n- Install smoke alarm detectors in each section of the room / area that has been divided by a partial wall. The wall might be coming down from the ceiling (at least 24”) or up from the floor.\n- Install smoke alarm detectors on a tray-shaped ceiling (also called coffered ceiling) on the highest portion of the ceiling or on the sloped portion of the ceiling within 12″ (30.5cm) vertically down from the highest point.\nWhere to install smoke alarm detectors – smoke alarm locations to avoid in order to minimize possibility of false alarms :\n1. Do not install smoke alarm detectors in areas where combustion particles are present (Combustion Particles – the by-products of burning process)\n- Poorly ventilated kitchens\n- Close to furnaces and water heaters – recommendation is to keep it at least 20′ away from any open combustion source. I wouldn’t 100% agree with this one, because presence of those particles would be a sign of combustion process problems…\nVernon Hills, IL. requires smoke alarm / detector to be installed in a room containing your furnace and in many cases keeping the 20′ distance may not be possible.\nIf you decided to install smoke alarm detectors in any of those areas, photoelectric type might be less annoying.\n2. Damp or very humid areas such as bathrooms. The humidity levels after taking a hot shower could result in a false alarm.\n3. You should not install smoke alarm detectors within 3’ from the forced air heating and / or cooling system air supply vents, in a direct airflow area, close to the whole house fan locations. High air flow could blow smoke or shift it away from the detector preventing it from responding properly or at all.\n4. Near fluorescent lights, where electronic “noise” may cause nuisance alarms.\n5. Don not install smoke alarm detectors in dusty areas where particles of dust could cause smoke alarm detector failure or false alarm\n6. In areas where air temperature may fall below 40°F (4°C) or rise above 100°F (38°C)\n7. Do not install smoke alarm detectors in areas near the doors and windows\nWhere to Install Smoke Alarm detectors – State by State Residential Requirements\nIf you got that far, I hope you have more than a general idea on “where to install smoke alarm detectors” in your home, and what is the optimal smoke alarm detector location, last thing to remember is smoke alarm maintenance."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:0dc312b9-a463-4094-a742-86734c6143d9>","<urn:uuid:8d4e7fa9-e349-4e3c-9dd9-6e93f455015e>"],"error":null}
{"question":"As a digital publishing researcher, I'm curious: what are the unique features of Inkless Editions' digital platform for scholarly texts, and how does its business model compare to traditional scientific publishers?","answer":"Inkless Editions provides innovative digital access to historical documents through features like full manuscript images, transcripts, collations, and scholarly analyses - particularly for works like Canterbury Tales and Armenian texts. They offer dynamic editions accessible via computers and mobile devices, with features like audio performances and glosses. Unlike traditional scientific publishers who report 40% profit margins and charge both for access and article preparation, Inkless operates independently without grant funding and focuses on bringing historical documents directly to readers through digital technology. This represents a different approach from conventional scientific publishing where content is acquired for free but access is heavily monetized.","context":["- Old English\nParliament Rolls for ten Kings of England,\nfrom Edward 1 to Henry VII (1272-1504)\nEight million words of text, translation\nand scholarly introductions\nEvery stitch of the Bayeux Tapestry\nFull facsimiles of three versions\nTranslations of many primary texts\nImages of related objects\nFull scholarly commentary on every inch\nPrue Shaw's edition of the Commedia\nImages, transcripts of seven key manuscripts\nFull word by word collation\nAnalysis of textual relations\nImages and transcripts of all pages of the\noldest manuscript of the Canterbury Tales\nQuire, collation views, scholarly analyses\nView manuscript structure, inks, conjugates.\nDynamic edition of a core section of\nÆlfric’s Catholic Homilies\nImages, transcripts and comparisons of 27 manuscripts for eight homilies\nCritical resources for study of Armenian\nBase lexicon and four full dictionaries\nArmenian and Greek/Armenian texts\nAdditional lexicons, wordlists, analyses\nFull scholarly descriptions, with images\nof all 88 manuscripts and incunables\nof Chaucer's Canterbury Tales.\nBased on new examination of every copy\nPrue Shaw's edition of the Monarchia\nImages and transcripts of all 20 manuscripts\nFull word by word collation\nAnalysis of textual relations\nFull images and transcripts of every page of\nWilliam Caxton's two editions of Geoffrey\nChaucer's Canterbury Tales. Collations, analyses, visualizations, introductions.\nFull transcripts, collations, analyses of every\npage and word of the tales of the Reeve and\nCook from Chaucer's Canterbury Tales.\nCollations, analyses, visualizations, introductions.\nGround-breaking app for mobile phone,\ntablet, and internet. Full audio performance,\nnotes, glosses, Terry Jones' translation, newly-edited text, manuscript images. Designed for the reader encountering Chaucer for the first time.\n- Why Inkless\n- Publication model\n- For scholars\n- For readers\nOur founder speaks: I started Inkless Editions, the imprint of Scholarly Digital Editions, to use digital technology to present fundamental documents from the past in challenging new forms. Inkless aims to give leading scholars the chance to provide innovative access to their original research on the documents. As a publisher, Inkless (like its parent, Scholarly Digital Editions) is apart from any institution, and has never received grant or any other funding.\nThe dream of digital technology is to bring the past direct to your eyes, through your computer or mobile device. As a publisher, Inkless is independent of traditional university structures, with their elaborate controls. My hope is that it will open digital space to allow readers and scholars to meet in new understandings of the past.\nPeter RobinsonFounder, Director, Editor\nAs a small boy in the Australian outback with nothing but sheep and dogs for miles around, I found the past in books. Each month a book club sent several new books to my mother: she read some, I read every one, as I travelled the world page by page.\nSchool, university, museums, and travel took me deeper into history. Computers and the internet arrived while I was doing my doctorate. I was at Oxford, the epicenter of computing and the humanities, and saw how digital methods could change what we see in the past, and how we see it. The screen is the museum and the library, but always open and open to all. At the same time, I was wading into the worlds of editorial theory and practice. From Jerome McGann and Peter Shillingburg and others I learnt that the past is not just to be seen: it is to be understood, to be argued about, to inspire.\nMy first work with digital editions was in the 1990s, as I and Cambridge University Press together worked out how to make these wonderful new objects. I started Scholarly Digital Editions -- now Inkless Editions -- to help us decode the past into our own lives. It is the site I would like to have discovered when I was a child.\nBarbara BordalejoPublishing Director\nWhat are the chances that a Latin American girl, first-generation university student, would end up specializing in medieval English literature and teaching in Anglophone universities?\nMy parents are artists, and when I was three-years-old, they packed up the apartment they had just bought and moved us from Buenos Aires to Caracas, where I grew up among journalists, filmmakers and public intellectuals. When I was ten, we closed up our place in Caracas and headed to Europe, where we spent a year visiting museums, churches, and pinacothecas. After skipping 5th grade, I continued my education in Venezuela and then in north America, obtaining multiple degrees, including two master's degrees in succession and two doctorates simultaneously.\nMy fascination with manuscripts and editions started almost from the very beginning of my studies. I have been designing transcription and encoding systems for primary textual sources since then, as well as creating digital editions.\nAfter a pursuit of academic degrees that, apart from Frances McSparran's courses, probably challenged me too little to be of significant help, my intellectual work began to come into focus when I started studying the theories of Mikhail Bakhtin in the mid 1980s. Perhaps it is not entirely coincidental that a volume I edited, addressing the relevance of those ideas to medieval literature, came out in 1996, the same year that I first encountered the newly published Wife of Bath's Prologue on CD-ROM. The ensuing dialogues, literal and figurative, with friends, with teachers, with other scholars, with intramural colleagues, with students, occasionally with members of my family—all those dialogues about texts, about editorial processes and the problems of editorial theory (and theories), and about scribal work and its value to editors, have been my primary interest ever since, leading to my work on the Reeve and Cook's Tales for Inkless Editions.\nInkless Editions is an imprint of Scholarly Digital Editions LLC\nScholarly Digital Editions is an LLC registered in Delaware, United States.","Scientific publishing has long been a licence to print money. Scientists need joumals in which to publish\ntheir research, so they will supply the articles without monetary reward. Other scientists perform the specialised work of peer review also for free, because it is a central element in the acquisition of status and the production of scientific knowledge.\nWith the content of papers secured for free, the publisher needs only fnd a market for its journal. Until this century, university libraries were not very price sensitive. Scientific publishers routinely report profit margins approaching 40% on their operations, at a time when the rest of the publishing industry is in an existential crisis.\nThe Dutch giant Elsevier, which claims to publish 25% of the scientific papers produced in the world,made profits of more than ￡900m last year, while UK universities alone spent more than ￡210m in 2016 toenable researchers to access their own publicly funded research; both figures seem to rise unstoppably despite increasingly desperate efforts to change them.\nThe most drastic, and thoroughly illegal, reaction has been the emergence of Sci-Hub, a kind of global photocopier for scientific papers, set up in 2012, which now claims to offer access to every paywalled article published since 2015. The success of Sci-Hub, which relies on researchers passing on copies they have themselves legally accessed, shows the legal ecosystem has lost legitimacy among its users and must be transformed so that it works for all participants.\nIn Britain the move towards open access publishing has been driven by funding bodies. In some ways it has been very successful. More than half of all British scientific research is now published under open access terms: either freely available from the moment of publication, or paywalled for a year or more so that the publishers can make a profit before being placed on general release.\nYet the new system has not worked out any cheaper for the universities. Publishers have responded to the demand that they make their product free to readers by charging their writers fees to cover the costs of preparing an article. These range from around ￡500 to $5,000. A report last year pointed out that the costs both of subscriptions and of these’’article preparation costs’’ had been steadily rising at a rate above inflation. In some ways the scientific publishing model resembles the economy of the social internet: labour is provided free in exchange for the hope of status, while huge profits are made by a few big firms who run the market places. In both cases, we need a rebalancing of power.\n26. Scientific publishing is seen as“a licence to print money\" partly because________\n[A] its funding has enjoyed a steady increase .\n[B] its marketing strategy has been successful.\n[C] its payment for peer review is reduced.\n[D] its content acquisition costs nothing.\n27. According to Paragraphs 2 and 3, scientific publishers Elsevier have________\n[A] thrived mainly on university libraries.\n[B] gone through an existential crisis.\n[C] revived the publishing industry.\n[D] financed researchers generously.\n28. How does the author feel about the success of Sci-Hub?\n29. It can be learned from Paragraphs 5 and 6 that open access terms________\n[A]allow publishers some room to make money.\n[B] render publishing much easier for scientists.\n[C] reduce the cost of publication substantially.\n[D] free universities from financial burdens.\n30. Which of the following characterises the scientific publishing model?\n[A] Trial subscription is offered.\n[B] Labour triumphs over status.\n[C] Costs are well controlled.\nD] The few feed on the many."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:4a9f37fa-ec79-42ee-9ea8-da5c92ddb587>","<urn:uuid:df703869-2562-4a23-8d64-d025b13feba9>"],"error":null}
{"question":"How do modern radio telescopes achieve high resolution imaging, and what everyday materials can be used to build basic radio telescopes?","answer":"Modern radio telescopes achieve high resolution through radio interferometry and aperture synthesis, using large arrays of antennas like the Very Large Array with its twenty-seven 25-meter antennas. For basic radio telescopes, everyday materials can be repurposed - satellite TV dishes can be converted using cast-off Dish Network antennas, and simple equipment like satellite finders connected to an Arduino can be used to detect and map radio signals.","context":["When the news broke recently that communications had finally been re-established with Voyager 2, I felt a momentary surge of panic. I’ve literally been following the Voyager missions since the twin space probes launched back in 1977, and I’ve been dreading the inevitable day when the last little bit of plutonium in their radioisotope thermal generators decays to the point that they’re no longer able to talk to us, and they go silent in the abyss of interstellar space. According to these headlines, Voyager 2 had stopped communicating for eight months — could this be a quick nap before the final sleep?\nThankfully, no. It turns out that the recent blackout to our most distant outpost of human engineering was completely expected, and completely Earth-side. Upgrades and maintenance were performed on the Deep Space Network antennas that are needed to talk to Voyager. But that left me with a question: What about the rest of the DSN? Could they have not picked up the slack and kept us in touch with Voyager as it sails through interstellar space? The answer to that is an interesting combination of RF engineering and orbital dynamics.\nContinue reading “After Eight-Month Break, Deep Space Network Reconnects With Voyager 2”\nJoin us on Wednesday, February 12 at noon Pacific for the DIY Radio Telescopes Hack Chat with James Aguirre!\nFor most of history, astronomers were privy to the goings-on in the universe only in a very narrow slice of the electromagnetic spectrum. We had no idea that a vibrant and wondrous picture was being painted up and down the wavelengths, a portrait in radio waves of everything from nearly the moment of creation to the movement of galaxies. And all it took to listen in was an antenna and a radio receiver.\nOver the years, radio telescopes have gotten more and more sophisticated and sensitive, and consequently bigger and bigger. We’re even to the point where one radio telescope often won’t cut it, and astronomers build arrays of telescopes spread over miles and miles, some with antennas that move around on rails. In the search for signals, radio astronomy has become the very definition of “Big Science.”\nBut radio astronomy doesn’t have to be big to be useful. James Aguirre, an astronomer at the University of Pennsylvania, spends his days (and nights) studying the radio universe with those big instruments. But he’s also passionate about down-scaling things and teaching everyone that small radio telescopes can be built on the cheap. His Mini Radio Telescope project uses a cast-off satellite TV dish and a couple of hundred bucks worth of readily available gear to scan the skies for all sorts of interesting phenomena.\nDr. Aguirre will join us on the Hack Chat to discuss all things radio astronomy, and how you can get in on the radio action on the cheap. Chances are good your junk pile — or your neighbor’s roof — has everything you need, and you might be surprised how approachable and engaging DIY radio astronomy can be.\nOur Hack Chats are live community events in the Hackaday.io Hack Chat group messaging. This week we’ll be sitting down on Wednesday, February 12 at 12:00 PM Pacific time. If time zones have got you down, we have a handy time zone converter.\nClick that speech bubble to the right, and you’ll be taken directly to the Hack Chat group on Hackaday.io. You don’t have to wait until Wednesday; join whenever you want and you can see what the community is talking about. Continue reading “DIY Radio Telescopes Hack Chat”\nThere’s something iconic about dish antennas. Chances are it’s the antenna that non-antenna people think about when they picture an antenna. And for many applications, the directionality and gain of a dish can really help reach out and touch someone. So if you’re looking to tap into a distant WiFi network, this umbrella-turned-dish antenna might be just the thing to build.\nStretching the limits of WiFi connections seems to be a focus of [andrew mcneil]’s builds, at least to judge by his YouTube channel. This portable, foldable dish is intended to increase the performance of one of his cantennas, a simple home-brew WiFi antenna that uses food cans as directional waveguides. The dish is built from the skeleton of an umbrella-style photographer’s flash reflector; he chose this over a discount-store rain umbrella because the reflector has an actual parabolic shape. The reflective material was stripped off and used as a template to cut new gores of metal window screen material. It’s considerably stiffer than the reflector fabric, but it stretches taut between the ribs and can still fold up, at least sort of. An arm was fashioned from dowels to position the cantenna feed-horn at the focus of the reflector; not much detail is given on the cantenna itself, but we assume it’s similar in design to cantennas we’ve featured before.\n[andrew] hasn’t done rigorous testing yet, but a quick 360° scan from inside his shop showed dozens of WiFi signals, most with really good signals. We’ll be interested to see just how much this reflector increases the cantenna’s performance.\nContinue reading “Umbrella And Tin Cans Turned Into WiFi Dish Antenna”\nHumanity has been a spacefaring species for barely sixty years now. In that brief time, we’ve fairly mastered the business of putting objects into orbit around the Earth, and done so with such gusto that a cloud of both useful and useless objects now surrounds us. Communicating with satellites in Earth orbit is almost trivial; your phone is probably listening to at least half a dozen geosynchronous GPS birds right now, and any ham radio operator can chat with the astronauts aboard the ISS with nothing more that a $30 handy-talkie and a homemade antenna.\nBut once our spacecraft get much beyond geosynchronous orbit, communications get a little dicier. The inverse square law and the limited power budget available to most interplanetary craft exact a toll on how much RF energy can be sent back home. And yet the science of these missions demands a reliable connection with enough bandwidth to both control the spacecraft and to retrieve its precious cargo of data. That requires a powerful radio network with some mighty big ears, but as we’ll see, NASA isn’t the only one listening to what’s happening out in deep space. Continue reading “Serious DX: The Deep Space Network”\nHave you got a spare Dish Network antenna lying about? They’re not too hard to come by, either curbside on bulk waste day or perhaps even on Freecycle. If you can lay hands on one, you might want to try this fun radio telescope build.\nNow, don’t expect much from [Justin]’s minimalist build. After all, you’ll be starting with a rather small dish and an LNB for the Ku band, so you won’t be doing serious radio astronomy. In fact, the BOM doesn’t include a fancy receiver – just a hacked satellite finder. The idea is to just get a reading of the relative “brightness” of a radio source without trying to demodulate the signal. To that end, the signal driving the piezo buzzer in the sat finder is fed into an Arduino through a preamp. The Arduino also controls stepper motors for the dish’s azimuth and elevation control, which lets it sweep the sky and build up a map of signal intensity. The result is a clear band of bright spots representing the geosynchronous satellites visible from [Justin]’s location in Brazil.\nModifications are definitely on the docket for [Justin], including better equipment that will allow him to image the galactic center. There may be some pointers for him in our coverage of a tiny SDR-based radio telescope, or from this custom receiver that can listen to Jupiter.\nContinue reading “See Satellites With A Simple Radio Telescope”\nParabolic reflectors are pretty handy devices. Whether you’re building a microwave antenna or a long-distance directional microphone, suitable commercial dishes aren’t that hard to come by. But a big, shiny mirror for your solar death-ray needs is another matter, which is where this pressure-formed space blanket mirror might come in handy.\nPressure-forming was a great choice for [NighthawkInLight]’s mirror. We’ve covered pressure-formed plastic domes before, and this process is similar. A sheet of PVC with a recessed air fitting forms the platen. The metallized Mylar space blanket, stretched across a wooden frame to pull out the wrinkles and folds, is applied to a circle of epoxy on the platen. After curing, a few puffs with a bicycle tire pump forms the curve and stretches the film even smoother. [NighthawkInLight]’s first attempt at supporting the film with spray foam insulation was a bust, but the later attempt with fiberglass mesh worked great. A little edge support for the resulting shiny taco shell and the mirror was capable of the required degree of destructive potential.\nWe doubt this process can be optimized enough to produce astronomy-grade mirrors for visible light, but it still has a lot of potential applications. Maybe a fiberglass radio astronomy dish could be pressure-formed directly with a rig like this?\nMany Ham Radio operators in the United States participate in Field Day. This is an annual exercise where radio operators are encouraged to set up stations in conditions that might occur after a natural disaster. Usually, this means taking over some park or camp site, bringing generators, portable equipment, and making it all work for the weekend before you tear it back down.\nIt isn’t much of a Field Day without electricity. That’s why most stations use a generator, solar cells, or even batteries. Today, though, you probably need an Internet-connected computer to do logging and other features. [HamRadioConcepts] has a video (see below) that shows how they grabbed Internet from a distance for their Field Day site.\nContinue reading “Ham Radio WiFi”","Whats is Radio Astronomy?\nIntroduction: What is radio astronomy [reference: pictortelescope.com]\nRadio astronomy is a subfield of astronomy that studies celestial objects at radio frequencies. Unlike traditional telescopes that observe the sky in the visible part of the electromagnetic spectrum (400-700 nm), radio telescopes observe the sky in wavelengths ranging from centimeters to a few meters.\nA diagram of the electromagnetic spectrum, showing various properties across the range of frequencies and wavelengths.\nBut why should we observe in radio wavelengths?\nThere are several reasons to observe the sky at radio frequencies. The most common reason radio astronomers point radio telescopes to the sky is to study sources that produce radio emissions: radiation that is invisible to the human eye, but is capable of providing compelling information to astronomers & astrophysicists.\nA significant advantage radio astronomers have over IR, UV and high-energy astronomers is the atmospheric window: our atmosphere is completely transparent to radio waves, so we don’t need to send large radio antennas to space (like we usually have to do with IR/UV/high-energy astronomy satellites) in order to efficiently expose our instruments to the sky.\nA diagram showing the amount of absorption of each wavelength of light by the Earth’s atmosphere, highlighting the atmospheric windows.\nIt is also worth noting that radio observations can be carried out during both daytime and nighttime, and even under poor weather conditions (e.g. clouds)!\nLast but not least, we can utilize the techniques of radio interferometry and aperture synthesis using large arrays of antennas to achieve extremely high (angular) resolution observations, which is how the first image of a black hole became possible (the combination of several radio telescopes around the world allowed the Event Horizon Telescope team to create a virtual telescope the size of the Earth)!\nThe Very Large Array (VLA) is one of the most sophisticated radio telescope arrays in the world, consisting of twenty-seven 25-meter radio antennas that can function as a high-resolution radio interferometer.\nSo… What can I observe with a radio telescope?\nThere are plenty of sources in the sky that present radio emissions, including galaxies, supernova remnants, nebulae, radio galaxies, quasars, pulsars, masers and more. Since the radio telescope you’ll be using is primarily used to detect the neutral hydrogen concentration of the spiral arms of the galaxy, we will focus our attention on the hydrogen line.\nThe 21-centimeter hydrogen line\nThe hydrogen line refers to the electromagnetic radiation spectral line that is created by a change in the energy state of neutral hydrogen atoms. This electromagnetic radiation is at the approximate frequency of 1420.4 MHz, which is equivalent to the wavelength of approximately 21 cm ( λ =c/v). This wavelength falls within the radio region of the electromagnetic spectrum, and it is frequently observed by radio astronomers.\nThe exact mechanism under which hydrogen atoms emit electromagnetic radiation with a wavelength of 21 cm is slightly complicated.\nWhy observe at radio wavelengths?\nThere are several reasons to observe at radio wavelengths. Below we will show you the advantages and disadvantages:\nRadio astronomy can be done from the earth without being too much affected by the weather (although the quality of the observations is better with good weather)! However, there is now also a radio antenna in space, to further improve the resolution of the observations.\nDay and night\nRadio telescopes observe day and night (although for some observations the influence of the sun is negative!)\nAbout 90 % of the visible matter in the Universe is Hydrogen (wavelength: 21.106114 cm). With a radio telescope, we can study the most abundant element in the Universe.\nRadio waves are not affected by absorption. Optical waves are absorbed by e.g. dust clouds that are floating between the stars (like a sort of interstellar fog ). Radio telescopes see straight through these dust clouds.\nOn the negative side, to get good quality images that show all the details of the celestial objects it is more complicated than, e.g. at optical wavelengths. This has to do with long wavelengths of radio waves. To get good angular resolution requires large telescopes.\nA complicated procedure is required to produce the images of the observed objects (in other words, the observer does not see the images straight away). This procedure uses very powerful computers and it is necessary because of the way the observations are done."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:77c78fdf-f686-44f4-b266-b47eac39916a>","<urn:uuid:c0cec02f-19fe-4092-af2b-f3bbee760a0f>"],"error":null}
{"question":"Which international artists participated in the 'Poetics and Politics of Data' exhibition?","answer":"The group exhibition featured works by Christopher Baker (USA), Aram Bartholl (D), Paolo Cirio (IT), R. Luke DuBois (USA), Ellie Harrison (GB), Marc Lee (CH), Rafael Lozano-Hemmer (MEX/CAN), Bernd Hopfengärtner (D) & Ludwig Zeller (D/CH), Kristin Lucas (USA), Moniker (NL), Jennifer Lyn Morone (USA), RYBN (FR), and Erica Scourti (GR/GB).","context":["29.05.2015 - 30.08.2015\nThe cooperation project 'Poetics and Politics of Data' is supported by the Swiss Arts Council Pro Helvetia as part of its “Digital Culture” initiative.\nThe exhibition is also supported by:\nThe exhibition „Poetics and Politics of Data“ addresses the paradigm of a data-driven society and reflects life in an increasingly datified world. In visionary future scenarios, scientists enthuse over a world in which algorithms take over managing processes, envisioning a highly sensory and datafied space for us to live in, a world in which our desires and activities are anticipated, long before we carry them out. „Big Data“ is the keyword to this new era in which the power of data induces a radical transformation of a society whose actions and production of knowledge rely increasingly on the accumulation and evaluation of data.\n“Poetics and Politics of Data” shows artistic works that approach the phenomena of Big Data and data mining, visualizing the continuous bitstream in various ways while referring to the political and social implications that come with a world that is controlled by data – from the processes of self-optimization to economical aspects and questions concerning the use and evaluation of this data. Who has access to our data? In what ways is it possible to extract useful information and find “valuable” and applicable correlations from the immense pool of data?\nThe exhibition introduces critically subversive approaches and interventions in networked spaces that make use of the potential of a virtual community and reflect personal performance in social networks. It focuses on aspects of surveillance strategies, data mining, privacy, post-privacy and digital autobiography acted out in social networks. Amid the constantly growing, infinite ocean of data, artists question the meaning and position of the individual in a technologically networked society and – thanks to their resistance and sense of independence – offer various alternatives to a normative world of data.\nFrom computer-mediated installations to data visualizations, they address these questions through different media in order to not only generate a new approach to complex data structure, but to create a poetic immersive space of data.\n“Poetics and Politics of Data” is an interdisciplinary project between HeK (House of Electronic Arts Basel), the Institute of Experimental Design and Media Cultures of the University of Applied Sciences and Arts Northwestern Switzerland FHNW, the Centre for Technology Assessment TA-SWISS and Opendata.ch, the Swiss chapter of the Open Knowledge Foundation, presenting an exhibition about artistic approaches to big amounts of data. Artistic strategies and concepts of data usage, -interpretation and -criticism will be on display, discussing the potential and dangers of Big Data and data mining.\nGroup exhibition with works by:\nChristopher Baker (USA), Aram Bartholl (D), Paolo Cirio (IT), R. Luke DuBois (USA), Ellie Harrison (GB), Marc Lee (CH), Rafael Lozano-Hemmer (MEX/CAN), Bernd Hopfengärtner (D) & Ludwig Zeller (D/CH), Kristin Lucas (USA), Moniker (NL), Jennifer Lyn Morone (USA), RYBN (FR), Erica Scourti (GR/GB)\nAs part of the scenography of the exhibition, the design research group “Inhabiting & Interfacing the Cloud(s)” has been asked to create an infrastructure that will also present works by ECAL (Media & Interaction Design), HEAD – Genève (Media Design), EPFL Lausanne (ALICE) and fabric | ch."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:75eead67-3069-4744-8a74-e369ba8fc630>"],"error":null}
{"question":"How has the effectiveness of electric toothbrushes evolved historically, and what are current dentist recommendations for toothbrush replacement and maintenance?","answer":"Historical analysis from 1966 to 2004 showed that most power toothbrushes were no more effective than manual ones, with only rotation oscillation designs showing consistent superiority in plaque removal and gum health. Currently, dentists recommend replacing toothbrushes every three to four months to ensure bristle effectiveness and minimize bacterial accumulation. The average lifespan of an electric toothbrush is around 5 years, with manufacturers typically offering 2-year warranties. For optimal maintenance, dentists recommend using soft-bristled brushes and brushing for a minimum of two minutes, twice daily.","context":["A furrowed brow, a tense look, a fresh acne breakout — you can often tell on sight when someone’s under pressure. If you could look into a stressed-out person’s mouth, you might learn even more of their story.\nStress and Your Mouth: What’s the Connection?\nMore and more researchers have been studying the link between stress and gum disease. When you’re anxious or depressed, your body produces more of the hormone cortisol. This compound harms your teeth and gums, contributing to the risk for periodontal (gum) disease.\nThere’s also evidence that stress and depression impair your immune system,causing chronic infection throughout your body — including in your mouth — more likely. In addition, hard times lead to bad-for-your-teeth habits. These include smoking, drinking alcohol, and skipping your nightly brushing and flossing.\nThe following dental conditions also have been linked to stress, depression, or anxiety:\n- Burning mouth syndrome. This a painful condition that sufferers describe as a scalding feeling in the tongue, lips, and roof of the mouth.\n- Canker sores. Small, painful ulcers develop inside the mouth. Doctors aren’t sure what causes canker sores, but they are thought to appear more often when the individual is stressed or very tired.\n- Cold sores. These fluid-filled blisters are caused by the herpes virus. If you’re infected, you’ll often experience an outbreak in response to being upset.\n- Bruxism. People who grind their teeth (a problem called bruxism) tend to do it more when under stress. Grinding can wear and chip teeth and put pressure on jaw muscles and joints.\nWays to Relieve the Pressure\nDon’t let your mouth take the brunt of your stress. Try positive stress-reducing techniques instead. Here are some strategies:\n- Change your outlook. Some things, like the weather, are out of your hands and for that reason are not worth getting worked up about. Try to see other life events as positive challenges rather than threats.\n- Keep your body healthy. Eat a nutritious, well-balanced diet and get enough sleep. And exercise on a regular basis. Not only will you get in shape and feel better overall, you’ll release anxiety and produce mood-boosting brain chemicals.\n- Practice relaxation techniques. These include meditation, stretching, and deep breathing and progressive relaxation of muscle groups.\nElectric toothbrushes have been widely touted, and indeed they can be equally effective as — or in some cases even more than — manual brushes when used consistently. A variety of studies have been done comparing different power toothbrushes, and while there is agreement that power brushes are safe, results in plaque-removing capabilities of the various devices have varied.\nUnderstanding the Research\nAn analysis conducted by an independent nonprofit organization, Reference by Harvard University. compared various types of electric toothbrushes. Researchers systematically sorted through the data from studies done from 1966 to 2004 that compared power brushes’ effectiveness at removing plaque, maintaining gum health, and removing stains, as well as their dependability and adverse effects. The power brushes were divided into seven groups based on how they worked.\nThey found that most of the power toothbrushes were no more effective than manual toothbrushes. Just one type of brush — the rotation oscillation design (where the brush heads rotate in one direction and then the other) — was consistently better at removing plaque and reducing gingivitis (gum inflammation) than a manual toothbrush. Examples of the rotation oscillation action include brushes in the Braun Oral-B Triumph and Professional Care product lines.\nWho Needs an Electric Brush?\nAn electric toothbrush can be particularly helpful for people who have trouble reaching all corners of their mouth. For example, power brushes are useful for people with braces, parents brushing their young children’s teeth, and individuals with mental or physical disabilities that impair dexterity. The thicker handle on power models also is a plus for some older people and people with arthritis who have difficulty grasping the thinner shaft of a manual brush.\nBut ultimately the best brush may simply be the one you feel most comfortable with. If you have questions, bring your toothbrush to your next dental visit so your dentist can examine it. While you’re at it, demonstrate your brushing technique so your dentist or hygienist can make sure you are brushing correctly.\nEven if you had cavities when you were young, your child doesn’t have to develop them. Find out how to protect kids’ teeth from tooth decay. Practices like using fluoride to strengthen teeth and eating a balanced diet can set your child up for a healthy smile for life.\nContinue reading Kids’ Teeth And Cavity Protection\nDigital Dentistry is one of the most latest technique apart from routine dental procedures. It involve use of digital technology like Cad Cam, software assisted dental procedures, digital impressions, digital radiography, digitally designed surgical stent for dental implants, digital dental photography, also digitally designed stents for orthodontics, making of digitally developed models for dental crowns, bridge & restorations.\nContinue reading Digital Dentistry","- 1 Do dentists recommend Oral-B or Sonicare?\n- 2 Which toothbrush do dentists recommend?\n- 3 What is the best electric toothbrush 2021?\n- 4 Which electric toothbrush do dentists recommend UK?\n- 5 How often should you replace your toothbrush?\n- 6 How can I remove tartar from my teeth without going to the dentist?\n- 7 Is it worth buying electric toothbrush?\n- 8 Is it better to use soft or medium toothbrush?\n- 9 Do electric toothbrushes damage teeth?\n- 10 Which toothbrush removes the most plaque?\n- 11 What is the best budget electric toothbrush?\n- 12 Is the Oral-B iO worth it?\n- 13 What is the lifespan of an electric toothbrush?\n- 14 Is an electric toothbrush better for receding gums?\n- 15 Which Oral-B is the best?\nDo dentists recommend Oral-B or Sonicare?\nFor the most part, though, the dentists we spoke with still recommend classics like Philips Sonicare and Oral-B because of the years of scientific research behind them. That’s not to say the shiny new toothbrush you bought from an Instagram ad won’t clean your teeth.\nWhich toothbrush do dentists recommend?\nDentists recommend brushing teeth for a minimum of two minutes, twice a day, with a soft-bristled toothbrush. The right toothbrush is a matter of personal preference and financial considerations. A classic, manual brush costs a few dollars. An electric toothbrush can cost over $100.\nWhat is the best electric toothbrush 2021?\nBest electric toothbrush for 2021\n- Best overall electric toothbrush. Hum by Colgate.\n- Best simple electric toothbrush. Quip.\n- Best electric toothbrush with water flosser. Waterpik Complete Care 5.0 Water Flosser.\n- Best high-end electric toothbrush. Oral-B iO.\n- Best electric toothbrush for sensitive gums.\nWhich electric toothbrush do dentists recommend UK?\nOral-B is widely recommended by dentists in the UK, so whichever of their brushes you choose you can be quite confident it is doing a good job. Different types of brush heads are designed for whitening, sensitive gums, and use with braces. 3\nHow often should you replace your toothbrush?\n“The average person should be swapping out for a new toothbrush every three to four months,” explains Dr. Sienna Palmer, DDS, dentist at Meridien Dental in Santa Monica, CA. “This is recommended to ensure that the bristles are still effective and bacteria accumulation on the toothbrush is minimal.”\nHow can I remove tartar from my teeth without going to the dentist?\nStart by mixing white vinegar in a glass of warm saltwater. This solution can then be gargled once a day to aide in the removal of tartar that has formed on the region between the teeth and gums. The mixture should be made from two tablespoons of white vinegar into a cup of warm water with dissolved salt.\nIs it worth buying electric toothbrush?\nA review of studies showed that, in general, electric toothbrushes do decrease more plaque and gingivitis than manual toothbrushes. After three months of use, plaque was reduced by 21 percent and gingivitis by 11 percent. Oscillating (rotating) toothbrushes seem to work better than just vibrating toothbrushes.\nIs it better to use soft or medium toothbrush?\nThe medium brush removed more plaque than the soft toothbrush from the premolar area. Both toothbrushes removed more plaque in premolar areas than the molar or anterior areas. It means that a soft bristle toothbrush will clean your teeth as well as a medium bristled one, but with less damage to the gum tissue.\nDo electric toothbrushes damage teeth?\nUsed properly, an electric toothbrush should not hurt your gums or enamel but instead promote overall oral health. Many people are guilty of brushing too hard, which can, over time, cause irreversible damage to tooth enamel and can cause receding gums, which is also irreversible.\nWhich toothbrush removes the most plaque?\nIn fact, Oral-B Electric Toothbrushes deliver up to 48,800 movements per minute, while a regular manual toothbrush can only deliver 300 to 600 movements per minute. The clinically proven superior technology of an electric toothbrush removes 100% more plaque than a regular manual toothbrush.\nWhat is the best budget electric toothbrush?\nThe best electric toothbrushes to buy\n- Colgate ProClinical 250+ Rechargeable Electric Toothbrush: The best cheap electric toothbrush.\n- Oral-B Pro 2 2500N: The best-value budget toothbrush.\n- Philips Sonicare DailyClean 3100: The best budget sonic toothbrush.\n- Ordo Sonic+: A serious challenger to the big name brands.\nIs the Oral-B iO worth it?\nIt Was Worth It. The iO smart toothbrush delivers that straight-from-the-dentist clean feeling. The Takeaway: A powerful new motor and upgraded features make the new iO Series 8 Oral-B’s most effective smart toothbrush yet.\nWhat is the lifespan of an electric toothbrush?\nThe average life span of an electric toothbrush is around 5 years. Manufacturers normally offer a 2 year warranty should the brush fail sooner. But some brushes will last a lot longer — we know of people still using electric toothbrushes that are 10 years old.\nIs an electric toothbrush better for receding gums?\nIf you have receding gums, your dentist may recommend an electric toothbrush to help treat the condition and reduce the risk of dental damage. Electric toothbrushes clean your teeth while minimizing the excess pressure some people place on their gums.\nWhich Oral-B is the best?\nThe best electric toothbrush The Oral-B Pro 1000 has the most important features for the lowest price: a two-minute timer, an easy brushing process, and compatibility with the largest range of brush heads."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:34d4714b-acd5-4615-ac52-8199fdef4290>","<urn:uuid:a248a385-7d21-42b0-8519-c2b7dec4be2a>"],"error":null}
{"question":"How does time allocation compare between franchise labor dispute hearings and Supreme Court oral arguments?","answer":"In Supreme Court oral arguments, each case is strictly allocated one hour total, with 30 minutes per side, and arguments are heard Monday through Wednesday during specified weeks. In contrast, the documents don't specify exact time allocations for franchise labor disputes, though they do show these cases can extend over longer periods, as evidenced by the Oakland franchise case which went through multiple stages including settlement with the franchisee before continuing against the parent company. While Supreme Court oral arguments follow a precise schedule and timing protocol, franchise labor disputes appear to follow more variable timeframes through the regular court system.","context":["The term “ostensible agency” refers to the relationship that exists between two parties and that leads a person to believe that the first is an agent of the second, or vice versa. For example, ostensible agency would apply to a hospital worker who is employed by an outside contractor.\nFor all intents and purposes, patients may believe that worker to be a direct employee of the hospital but, in fact, he is an “ostensible agent” who is employed by a third party. This can create confusion in a malpractice action when a patient sues the hospital for the actions of such a worker. To explore this concept, consider the following ostensible agency definition.\nDefinition of Ostensible Agency\n- A relationship that exists between two parties that would lead a person to believe that one party is employed by the other.\n1720-1730 Latin ostēns(us)\nEven if an employer hires an independent contractor, the employer can be responsible for the actions of that contractor. This is known as “ostensible agency,” or “apparent authority.” When an employer does something to suggest that the independent contractor actually works for him, or fails to correct this assumption, it can be held liable for the independent contractor’s actions.\nWhen a person is hired as an independent contractor, an agency relationship develops. For example, ostensible agency may exist when a homeowner hires a carpet installer to install carpet in his home. While the homeowner hires Company A, Company A may then hire an outside contractor to actually do the work. This contractor may then show up to install the carpet without specifically informing the homeowner that he is not an employee of Company A. This is known as apparent authority.\nShould something go wrong with the installation, causing the homeowner to eventually file a civil lawsuit, it is likely he would sue Company A. Because it is the contractor himself who made the mistake, the company may want to push the responsibility off onto him, but ostensible agency holds that, because nobody told the homeowner the installer wasn’t an employee of the company, it may be held liable anyway.\nAmy and Aaron are remodeling their home, and the visit a discount flooring outlet to purchase hardwood flooring. The company offers free installation if the purchase a certain amount of flooring, so they decide to have the wood installed in three rooms.\nOn the day of installation, two workers show up in an unmarked work truck, and get to work. When Amy and Aaron get a good look at their floors that evening, they realize there are gaps between planks and mis-cuts, and the installers have left a mess of sawdust, floor glue, and packaging in their laundry room.\nWhen Amy contacts the flooring outlet demanding to have the problems corrected, they refer her to an independent contractor they hired to do the installation. Unfortunately, the contractor refuses to return her calls, so the couple files a civil lawsuit against the flooring outlet. In this case, neither the outlet or the installer advised the couple that the installer wasn’t employed by the outlet.\nIn this example, ostensible agency makes it likely that the outlet will be held responsible for correcting the problems. It may, alternatively, be ordered to reimburse Amy and Aaron for the cost to have someone else re-do the installation, as well as for any additional flooring that may need to be purchased.\nOstensible Agency in the Medical Field\nOstensible agency in the medical field is a type of vicarious liability. In this case, a healthcare organization – such as a hospital – can be held liable for the negligence of any healthcare provider under its control. In order for this type of ostensible agency to apply, something must have occurred to lead the patient to reasonably believe that the healthcare provider was actually employed by the organization. This belief by the patient may be what determines the organization’s liability, whether or not an actual employment relationship exists.\nOstensible Agency Example in Personal Injury Lawsuit\nAn example of ostensible agency being challenged in a court of law involved a personal injury case from 1981. Here, John Lynn Stephens was seriously injured when his motorcycle went out of control. Stephens had recently had a new tire put on the bike by L&A Tire Company. Stephens sued L&A Tire Company, among others, for damages related to personal injuries, claiming that a washer had been trapped between the tube and the inside of the tire. As a result, Stephens claimed, a leak was created which led to the rapid deflation of the tire that caused Stephens’ accident.\nStephens based his complaint on theories of negligence, product liability, and warranty. Conoco, Inc. was joined in the action as a co-defendant. Stephens claimed that it was the Conoco sign that he saw displayed at the service station that gave him the confidence to select L&A for his repair service. He believed that he could rely on the services he would receive, considering how well known the Conoco brand was. Conoco filed a motion for summary judgment, claiming that it had no control over how the L&A Service Station operated, and that the employees who performed the mounting of the tire were agents and employees of L&A, not Conoco.\nFurther, Conoco claimed that the only item it furnished to the owner of the service station was a Conoco sign. It did not distribute or sell Dunlop tires – like the one that Stephens had purchased – to the service station, defective or otherwise. The district court sustained Conoco’s motion and dismissed the case against them, finding that the facts presented by Conoco could not be disputed. Said the Court:\n“The existence of actual authority between principal and agent is not a pre-requisite to establishing apparent authority. Apparent authority results from a manifestation by the principal to a third person that another is his agent. The manifestation may be made directly to a third person or to the community by signs or by advertising.”\n“Apparent authority exists only to the extent that it is reasonable for the third person dealing with the agent to believe that the agent is authorized.”\nThe Court held that Stephens’ merely saying that he believed the repair services offered by L&A were authorized by Conoco was not enough to oppose Conoco’s motion for summary judgment and save the case from dismissal.\nCourt Ruling on Ostensible Agency in a Franchise\nIn the fast food industry, it is common for ginormous companies to sell franchises. In this case, the parent company furnishes the brand name – complete with universally-recognized logo and advertising slogans – the menu, and required operational standards. The franchise owner takes care of everything else, including hiring employees, training them, and firing them when they don’t live up to standards.\nIn 2014, three fast food workers in Oakland, California filed a civil lawsuit against their employer, a family-owned company operating eight franchise restaurants, for a host of labor code violations. These workers also named the fast food parent company in their lawsuit.\nEventually, the workers settled their case with the franchisee (the family-owned company), but continued with their lawsuit against the parent company, turning it into a class-action lawsuit on behalf of more than 1,200 employees. In order to be successful in suing the parent company under an ostensible agency theory, the workers must prove:\n- The workers reasonably believed that the franchisee had the authority to act on the company’s behalf\n- The workers’ belief was fostered by something the parent company did, or failed to do\n- The workers didn’t negligently rely on the franchisee’s apparent authority\nThe judge determined that there was not enough information to support a class action suit, but the individual workers – and the original group of three plaintiffs – could continue in their original claims against the parent company. The parent company asked the court to dismiss all claims against it, claiming that it had no direct control of the workers’ employment, and therefore it could not be held liable for wage and labor code violations of the franchisee.\nThe court decided that it was possible that the franchisee created the impression that it was acting as an agent of the parent company. If the employees relied on that mistaken understanding, ostensible agency may leave the parent company holding the bag. California law defines an employer as anyone who “directly or indirectly, or through an agency or any other person, employs or exercises control over the wages, hours, or working conditions of any person.”\nUltimately, the federal court granted the parent company a summary judgment, releasing franchisors from wage and employment claims under the ostensible agency theory. Essentially, as franchisors have no actual control over the employees, they cannot be considered joint employers.\nRelated Legal Terms and Issues\n- Liability – Responsibility for payment of damages, or for other court-imposed penalties in a civil lawsuit.\n- Malpractice – A failure to exercise professional duty in rendering professional services.\n- Negligence – Failure to exercise a degree of care that would be taken by another reasonable person in the same circumstances.\n- Objection – The action of challenging or disagreeing with a statement made in court or at a deposition.\n- Summary Judgment – A final decision on the case, handed down by the judge on the basis of the statements and evidence presented, without a full trial.\n- Sustain – A judge’s agreement that an objection is valid.","Supreme Court Procedures\nArticle III, Section 1 of the Constitution establishes the Supreme Court of the United States. Currently, there are nine Justices on the Court. Before taking office, each Justice must be appointed by the President and confirmed by the Senate. Justices hold office during good behavior, typically, for life.\nThe Constitution states that the Supreme Court has both original and appellate jurisdiction. Original jurisdiction means that the Supreme Court is the first, and only, Court to hear a case. The Constitution limits original jurisdiction cases to those involving disputes between the states or disputes arising among ambassadors and other high-ranking ministers. Appellate jurisdiction means that the Court has the authority to review the decisions of lower courts. Most of the cases the Supreme Court hears are appeals from lower courts.\nWrits of Certiorari\nParties who are not satisfied with the decision of a lower court must petition the U.S. Supreme Court to hear their case. The primary means to petition the court for review is to ask it to grant a writ of certiorari. This is a request that the Supreme Court order a lower court to send up the record of the case for review. The Court usually is not under any obligation to hear these cases, and it usually only does so if the case could have national significance, might harmonize conflicting decisions in the federal Circuit courts, and/or could have precedential value. In fact, the Court accepts 100-150 of the more than 7,000 cases that it is asked to review each year. Typically, the Court hears cases that have been decided in either an appropriate U.S. Court of Appeals or the highest Court in a given state (if the state court decided a Constitutional issue).\nThe Supreme Court has its own set of rules. According to these rules, four of the nine Justices must vote to accept a case. Five of the nine Justices must vote in order to grant a stay, e.g., a stay of execution in a death penalty case. Under certain instances, one Justice may grant a stay pending review by the entire Court.\nEach Justice is permitted to have between three and four law clerks per Court term. These are individuals who, fairly recently, graduated from law school, typically, at the top of their class from the best schools. Often, they have served a year or more as a law clerk for a federal judge. Among other things, they do legal research that assists Justices in deciding what cases to accept; help to prepare questions that the Justice may ask during oral arguments; and assist with the drafting of opinions.\nWhile it is the prerogative of every Justice to read each petition for certiorari himself/herself, many participate in what is informally known as the \"cert pool.\" As petitions for certiorari come in on a weekly basis, they are divided among the participating Justices. The participating Justices divide their petitions among their law clerks. The law clerks, in turn, read the petitions assigned to them, write a brief memorandum about the case, and make a recommendation as to whether the case should be accepted or not. The Justice provides these memoranda and recommendations to the other Justices at a Justices' Conference.\nIf the Justices decide to accept a case (grant a petition for certiorari), the case is placed on the docket. According to the Supreme Court's rules, the petitioner has a certain amount of time to write a brief, not to exceed 50 pages, putting forth his/her legal case concerning the issue on which the Court granted review. After the petitioner's brief has been filed, the other party, known as the respondent, is given a certain amount of time to file a respondent's brief. This brief is also not to exceed 50 pages.\nAfter the initial petitions have been filed, the petitioner and respondent are permitted to file briefs of a shorter length that respond to the other party's respective position. If not directly involved in the case, the U.S. Government, represented by the Solicitor General, can file a brief on behalf of the government. With the permission of the Court, groups that do not have a direct stake in the outcome of the case, but are nevertheless interested in it, may file what is known as an amicus curiae (Latin for \"friend of the court\") brief providing their own arguments and recommendations for how the case should be decided.\nBy law, the U.S. Supreme Court's term begins on the first Monday in October and goes through the Sunday before the first Monday in October of the following year. The Court is, typically, in recess from late June/early July until the first Monday in October.\nThe Court hears oral arguments in cases from October through April. From October through December, arguments are heard during the first two weeks of each month. From January through April, arguments are heard on the last two weeks of each month. During each two-week session, oral arguments are heard on Mondays, Tuesdays, and Wednesdays only (unless the Court directs otherwise).\nOral arguments are open to the public. Typically, two cases are heard each day, beginning at 10 a.m. Each case is allotted an hour for arguments. During this time, lawyers for each party have a half hour to make their best legal case to the Justices. Most of this time, however, is spent answering the Justices' questions. The Justices tend to view oral arguments not as a forum for the lawyers to rehash the merits of the case as found in their briefs, but for answering any questions that the Justices may have developed while reading their briefs.\nThe Solicitor General usually argues cases in which the U.S. Government is a party. If the U.S. Government is not a party, the Solicitor still may be allotted time to express the government's interests in the case.\nDuring oral arguments, each side has approximately 30 minutes to present its case, however, attorneys are not required to use the entire time. The petitioner argues first, then the respondent. If the petitioner reserves time for rebuttal, the petitioner speaks last. After the Court is seated, the Chief Justice acknowledges counsel for the petitioner, who already is standing at the podium. The attorney then begins: \"Mr. Chief Justice, and may it please the Court . . . .\"\nOnly the Chief Justice is addressed as Mr. Chief Justice. Others are addressed as \"Justice Scalia,\" \"Justice Ginsburg,\" or \"Your Honor.\" The title \"Judge\" is not used for Supreme Court Justices.\nCourtroom/Classroom Simulations: Modifications of Procedure\nJustices, typically, ask questions throughout each presentation. However, in courtroom or classroom simulations, to put student attorneys at ease, student Justices do not ask questions for the first two minutes of each side's argument. When the student Marshal holds up a five-minute warning card, the student attorney at the podium should conclude his/her argument and be ready to end when the Marshal holds up the STOP card.\nBefore leaving the podium after making the initial presentation, counsel for the petitioner may reserve some time for rebuttal after the respondent's counsel has presented. The petitioner — not the Court — is responsible for keeping track of the time remaining for rebuttal. In typical program simulations, more than one student attorney argues each side. In that instance, they should inform the student Marshal before the court session begins how they wish to divide their time. Usually, the first student attorney to speak also handles the rebuttal.\nWhen oral arguments are concluded, the Justices have to decide the case. They do so at what is known as the Justices' Conference. Two Conferences are held per week when Court is in session, on Wednesday and Friday afternoons. The Justices vote on cases heard on Mondays and Tuesdays of a given week at their Wednesday afternoon Conference. The Justices vote on cases heard on Wednesday at their Friday afternoon Conference. When Court is not in session, usually only a Friday Conference is held.\nBefore going into the Conference, the Justices frequently discuss the relevant cases with their law clerks, seeking to get different perspectives on the case. At the end of these sessions, sometimes the Justices have a fairly good idea of how they will vote in the case; other times, they are still uncommitted.\nAccording to Supreme Court protocol, only the Justices are allowed in the Conference room at this time—no police, law clerks, secretaries, etc. The Chief Justice calls the session to order and, as a sign of the collegial nature of the institution, all the Justices shake hands. The first order of business, typically, is to discuss the week's petitions for certiorari, i.e., deciding which cases to accept or reject.\nAfter the petitions for certiorari are dealt with, the Justices begin to discuss the cases that were heard since their last Conference. According to Supreme Court protocol, all Justices have an opportunity to state their views on the case and raise any questions or concerns they may have. Each Justice speaks without interruptions from the others. The Chief Justice makes the first statement, then each Justice speaks in descending order of seniority, ending with the most junior justice—the one who has served on the court for the fewest years.\nWhen each Justice is finished speaking, the Chief Justice casts the first vote, and then each Justice in descending order of seniority does likewise until the most junior justice casts the last vote. After the votes have been tallied, the Chief Justice, or the most senior Justice in the majority if the Chief Justice is in the dissent, assigns a Justice in the majority to write the opinion of the Court. The most senior justice in the dissent can assign a dissenting Justice to write the dissenting opinion.\nIf a Justice agrees with the outcome of the case, but not the majority's rationale for it, that Justice may write a concurring opinion. Any Justice may write a separate dissenting opinion. When there is a tie vote, the decision of the lower Court stands. This can happen if, for some reason, any of the nine Justices is not participating in a case (e.g., a seat is vacant or a Justice has had to recuse).\nAll opinions of the Court are, typically, handed down by the last day of the Court's term (the day in late June/early July when the Court recesses for the summer). With the exception of this deadline, there are no rules concerning when decisions must be released. Typically, decisions that are unanimous are released sooner than those that have concurring and dissenting opinions. While some unanimous decisions are handed down as early as December, some controversial opinions, even if heard in October, may not be handed down until the last day of the term.\nA majority of Justices must agree to all of the contents of the Court's opinion before it is publicly delivered. Justices do this by \"signing onto\" the opinion. The Justice in charge of writing the opinion must be careful to take into consideration the comments and concerns of the others who voted in the majority. If this does not happen, there may not be enough Justices to maintain the majority. On rare occasions in close cases, a dissenting opinion later becomes the majority opinion because one or more Justices switch their votes after reading the drafts of the majority and dissenting opinions. No opinion is considered the official opinion of the Court until it is delivered in open Court (or at least made available to the public).\nOn days when the Court is hearing oral arguments, decisions may be handed down before the arguments are heard. During the months of May and June, the Court meets at 10 a.m. every Monday to release opinions. During the last week of the term, additional days may be designated as \"opinion days.\"\nFor those interested in learning more about how the Supreme Court operates, please visit the Supreme Court of the United States."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:2d6a1b78-4d65-4ed9-ad9c-579f94c6fd28>","<urn:uuid:975b5a2c-5b15-47d3-a756-151aeba65998>"],"error":null}
{"question":"How do color creation methods differ between CMYK printing and RGB displays, and what primary colors do they use?","answer":"In CMYK printing, colors are created through subtractive mixing using Cyan, Magenta, Yellow, and Key (black) inks. Cyan + yellow creates green, magenta + yellow creates red, and cyan + magenta creates blue. Black is typically printed separately to avoid damaging paper with excess ink. In RGB displays, colors are created through additive mixing of Red, Green, and Blue light. Red and green combine to yield yellow, green and blue create cyan, and blue and red produce magenta. The key difference is that RGB mixing of all colors produces white light, while CMYK mixing of all inks produces black.","context":["The CMYK color model is a lesser-known color model. You may have never heard of it before, even though you’ve most certainly benefited from it.\nIn this post, we’ll cover when the CMYK color wheel is used, the CMYK primary colors, and what it means to be a subtractive color model.\nYou’ll walk away with a high-level understanding of when the CMYK color model is relevant and how to interact with it – all invaluable skills if you’re involved in the world of digital color.\nWhat is a Color Model?\nA color model (also known as a color system) is a process for creating more colors starting from a few primary colors.\nColor models explain how color works in different situations and applications. Our goal for this post is to explain how this works in the CMYK color model.\nWhat is the CMYK Color Model?\nThe CMYK color model is often also referred to as “process color.” It was first popularized at the turn of the 20th century when printers found that starting with magenta, cyan, and yellow inks provided the largest possible set of unique colors when used in print media.\nIf you’re wondering why CMYK has four letters in its acronym (instead of three, like the RGB and RYB models), it’s because of the addition of key (black) into the mix.\nThis is because black ink is usually used in printing even though black (or at least a dark muddy color due to technical limitations) can technically be created by blending cyan, magenta, and yellow together.\nAlthough it’s possible, using pure black pigmented ink simply creates a blacker black, but it avoids soaking the paper with ink, which would damage it. Blending three colors of ink to create black weakens the paper since it saturates the fibers so much.\nIn additive color models, such as the RGB model, white is the “additive” combination of all primary colored lights, black is the absence of light. In the CMYK model, it is the opposite: white is the natural color of the paper or other background, and black results from a full combination of colored inks.\nDid you know? RGB is the color space for digital images, while CMYK is the color space for printed materials.\nCMYK Colors Explained\nSurfaces appear to be certain colors because of the wavelengths of light they absorb and reflect.\nObjects that look red look that way because they absorb all wavelengths except red ones. Objects that look white reflect all wavelengths of light.\nObjects that look black absorb all color wavelengths (which is why you don’t want to wear black color or any dark colors in the summer if you want to stay cool). The CMYK color model works by blending pigments that reflect the desired wavelengths.\nThis is known as subtractive color because the pigments “subtract” specific colors from white paper. Specific color wavelengths are absorbed by pigment, changing what color is reflected back to be perceived by our eyes.\nWhite light without red leaves cyan, white light without blue leaves yellow, and white light without green leaves magenta.\nThe CMYK color model is used in digital printing. Anytime you’re translating an image from a digital computer screen to a printed product on the page, you’re using the CMYK model.\nBecause mixing ink works differently than mixing pigments or light, the CMYK color model has a different set of primary colors than the RYB or RGB color models.\nCMYK Primary Colors\nKey (black) deserves an honorable mention in this context because most printers use it, essentially, as another primary color.\n- Magenta + yellow = red\n- Cyan + yellow = green\n- Cyan + magenta = blue\nTheoretically, you could create black by combining all the primary colors. But in practice, printers generally use key (black) ink to do it instead, since using that much ink can actually damage the paper in the printing process.\nAll digital printers use the CMYK color model because it’s the most effective way to blend pigments for printing.\nWhen working on creating printed materials, you can tell the printer the exact density of ink required to create the desired colors, making a seamless transition from design to the printed final product, from business cards to magazines and beyond.\nHowever, many programs (like Adobe Photoshop) will display in RGB color spaces, be sure to change the color profile of the file you want to print to achieve your desired results.\nYou’re probably wondering why printers can’t just use the RYB color model, since they’re both subtractive models.\nThe reason is that the CMYK model is able to bridge the gap between RGB color mode (such as a digital design of a logo) and the printed page in a way the RYB model simply can’t.\nAt the end of the day, understanding the different color models is essential to successful graphic design. This way, you can optimize each stage of the design process.","Using the correct color mode in your design is essential to getting your message across. Learn the distinctions between RGB and CMYK color profiles with this simple overview.\nCover image via Rostislav_Sedlacek.\nThere can be a lot of confusion behind RGB and CMYK color modes. If you’re a designer, it’s crucial to differentiate the color profiles since designing in the wrong mode can affect the tone and message of your document. In this article, we’ll decipher the characteristics of each profile and when to use RGB and CMYK in your designs.\nThe RGB color profile consists of Red, Green, and Blue hues that combine to create extensive variations of colors. This color mode exists exclusively in screen displays, such as in computer monitors, mobile, and television screens. Even though RGB is present across all electronic devices, the color elements vary across systems and models. An image you see on an iMac may display differently than the same image on a Dell desktop computer.\nInstead of utilizing ink to produce hues, the RGB profile partakes in additive processes to produce color by blending light. This is the exact opposite of subtractive color processes, such as mixing paints or dyes. The presence of all RGB primaries at full intensity yields white, while the absence of color produces black. When you turn a monitor or screen off, you see an absence of RGB color, resulting in black. The color displays on your screen result from the presence of those RGB base hues.\nIn Adobe Illustrator, you can see how mixing RGB primaries at different intensities can vary the color output. When isolated, the primary hues generate vibrant shades. Red and green combine to yield yellow, green and blue mix to create cyan, and blue and red produce magenta.\nRGB produces a large array of colors; this color profile features a larger gamut, or color range, than CMYK. Above, you can see how Illustrator provides a gamut warning for RGB colors that can’t be successfully translated into CMYK. This occurs when an RGB color exists outside of CMYK printing abilities. Instead, the color will convert to a CMYK equivalent, which could affect the tones of your design. When printing a design produced in RGB color profiles, the print output will be different from the online preview.\nWhen to Use RGB Color Mode\nTo avoid color profile complications, always set a document in RGB when designing for social media, digital design, or online advertisements. Set any design or image that will exist on a screen or monitor as RGB. If you design in CMYK for an online-only design, you’ll limit your color options due to CMYK’s limited gamut range.\nThe CMYK color profile contains Cyan, Magenta, Yellow, and Key (Black) that combine to produce a range of hues. This four-color process works for any type of printer. When zoomed in on printed images you can see the four-color dots that layer to create different hues and gradations. Dots per inch result from printing and involve the CMYK color profiles. Although all printers produce prints in CMYK, the end result may vary among different styles and models of printers.\nIn RGB color spaces, all primaries combine to produce white with additive color processing. CMYK modes combine with subtractive color processes, meaning all primaries mask to yield to a blackish hue. This process is similar to when you mixed paints and dyes as a kid to make that unsightly dark color. As inks and dyes are layered upon each other, they subtract from the white of the paper.\nNotice how the CMYK sliders differ from the RGB sliders. Zero intensity of all primaries produces black in RGB color profiles; in CMYK color profiles, the absence of color produces white. Cyan and magenta create a blue, magenta and yellow create red, and yellow and cyan produce green. The combination of cyan, magenta, and yellow develop an off-black shade. Key, or black, comes in to add shades to each primary due to the primaries’ inability to combine and create black.\nWhen to Use CMYK Color Mode\nReserve CMYK for printed designs. Set products such as business cards, flyers, posters, and packaging in the CMYK color profile, and do yourself and the print shop a favor by always setting your document to CMYK before exporting or printing. Doing so prevents unbalanced colors that can result from RGB colors translated into CMYK equivalents.\nChanging Color Modes in Creative Cloud\nMost Adobe Creative Cloud programs will default to RGB color modes. You can easily adjust modes in the New Document panel under the Color Mode dropdown, or within the program itself. Not all programs support RGB to CMYK conversions, but with Illustrator, Photoshop, and InDesign, you can switch in just one click.\nIn Adobe Illustrator, you can switch colors modes by navigating to File > Document Color Mode > RGB Color or CMYK Color. This will affect the output of your vector shapes when shifting between color modes.\nWithin Photoshop, you can change color profiles by going to Image > Mode > CMYK or RGB Color.\nNavigate to the Color Panel within the software. If the panel isn’t visible, hit F6 to bring it up. Hit the dropdown symbol and select CMYK or RGB.\nLooking to expand your knowledge on essential design terminology? Check out these educational posts:"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:8fbd0cd0-24b5-401d-9670-4a8e72066913>","<urn:uuid:24845e0f-9c7f-46e4-81ac-3a59c9a12fef>"],"error":null}
{"question":"rights Dalits vs forest protection - which came first historically?","answer":"Forest protection laws emerged much earlier than Dalit rights advocacy. The first laws protecting forests were decreed in Ur, Mesopotamia around 2700 BCE, while Dalit rights activism only emerged in the late 1990s when groups began transforming caste-based discrimination into an international human rights issue.","context":["The International Struggle for New Human Rights\nPublication Year: 2010\nIn recent years, aggrieved groups around the world have routinely portrayed themselves as victims of human rights abuses. Physically and mentally disabled people, indigenous peoples, AIDS patients, and many others have chosen to protect and promote their interests by advancing new human rights norms before the United Nations and other international bodies. Often, these claims have met strong resistance from governments and corporations. More surprisingly, even apparent allies, such as Amnesty International, Human Rights Watch, and other nongovernmental organizations, have voiced misgivings, arguing that rights \"proliferation\" will weaken efforts to protect their traditional concerns: civil and political rights.\nWhy are certain global problems recognized as human rights issues while others are not? How do local activists transform long-standing problems into universal rights claims? When and why do human rights groups, governments, and international organizations endorse new rights? The International Struggle for New Human Rights is the first book to address these issues.\nFocusing on activists who advance new rights, the book introduces a framework for understanding critical strategies and conflicts involved in the struggle to persuade the human rights movement to move beyond traditional problems and embrace pressing new ones.\nEssays in the volume consider rights activism by such groups as the South Asian Dalits, sexual minorities, and children of wartime rape victims, while others explore new issues such as health rights, economic rights, and the right to water. Examining both the successes and failures of such campaigns, The International Struggle for New Human Rights will be a key resource not only for scholars but also for those on the front lines of human rights work.\nPublished by: University of Pennsylvania Press\n1. Introduction: Fighting for New Rights\nWhy does the international human rights movement recognize certain issues, but not others, as rights violations? How do some aggrieved groups transform their troubles into internationally acknowledged human rights concerns, whereas other groups fail when they attempt todo so? Asking these questions has practical implications for victims of...\n2. Orphaned Again? Children Born of Wartime Rape as a Non-Issue for the Human Rights Movement\nIn November 2005, representatives of the United Nations Children’s Fund (UNICEF) country offices and a variety of independent experts met at UNICEF Headquarters in New York to discuss and assess the protection needs of children born as a result of wartime rape in conflict zones. Such children, it was clear to humanitarian practitioners, were at...\n3. “Dalit Rights Are Human Rights”: Untouchables, NGOs, and the Indian State\nThis chapter explores recent mobilization by groups representing India’s Dalits (Untouchables) aimed at transforming age-old caste-based discrimination into an international human rights issue.1 Until the late 1990s, the daily violence, exclusion, and humiliation suffered by millions of Dalits were not treated as human rights issues by UN organs...\n4. Applying the Gatekeeper Model of Human Rights Activism: The U.S.-Based Movement for LGBT Rights\nIn recent years, many groups representing people who are lesbian, gay, bisexual, and transgendered (LGBT) have framed their grievances as international human rights claims. In so doing, many of these advocates have fought to add their cause to the human rights movement and to place a new right to sexuality on the international agenda. This chapter...\n5. From Resistance to Receptivity: Transforming the HIV/AIDS Crisis into a Human Rights Issue\nThe Joint United Nations Programme on HIV/AIDS (UNAIDS) estimates that 33 million people worldwide are HIV-positive.1 The AIDS epidemic presents one of the greatest challenges to public health systems around the world, straining national budgets and medical expertise worldwide. Not only is AIDS incurable, but it also disproportionately...\n6. Disability Rights and the Human Rights Mainstream: Reluctant Gate-Crashers?\nIn the 1970s, international human rights monitors loudly condemned the Soviet Union’s internment of political dissidents in “psychiatric hospitals.” Nongovernmental organizations (NGOs) such as Helsinki Watch and Amnesty International (AI), along with local activists such as Andrei Sakharov and Yuri Orlov, publicized the horrors of these detentions:...\n7. New Rights for Private Wrongs: Female Genital Mutilation and Global Framing Dialogues\nInternational human rights conventions and foundational documents do not mention a number of long-standing cultural practices that affect organizations have not typically included these issues in their international campaigns. In the past generation, however, a transnational coalition politicized a number of these traditional practices, including...\n8. Economic Rights and Extreme Poverty: Moving toward Subsistence\nEconomic and social rights have long been part of the human rights movement. Indeed, the Universal Declaration of Human Rights (UDHR) gives them equal status with civil and political rights.1 In practice, however, for most of the period since the UDHR’s adoption by the UN General Assembly in 1948, economic and social rights have held a secondary...\n9. Local Claims, International Standards, and the Human Right to Water\nIn recent years, conflicts over water have become increasingly common around the world. In many of these disputes, the “right to water” has been a central claim. In Cochabamba, Bolivia, activists invoked water rights in 2000 to oppose privatization of municipal water services, becoming a global cause célèbre. In India since the late 1990s, farmers...\nList of Contributors\nPage Count: 208\nPublication Year: 2010\nSeries Title: Pennsylvania Studies in Human Rights\nSeries Editor Byline: Bert B. Lockwood, Jr., Series Editor See more Books in this Series\nMUSE Marc Record: Download for The International Struggle for New Human Rights","400,000 years before current epoch (BCE) — Controlled use of fire by early humans began, according to many researchers.\n7,000 BCE — Emergence of Catal Huyuk, Jarmo and Alosh cultures in the Middle East. The destruction of lush forests may have given rise to myths about the Garden of Eden. (O’Brien, 1985) Also see K.J.W. Oosthoek, The role of wood in world history.\nc.7,000 BCE — Lead castings in Anatolia (Turkey).\n6,000 BCE — Deforestation leads to collapse of communities in southern Israel / Jordan. (Grove, 1995).\n6,000 BCE — An agricultural crisis leads to more herding of goats and sheep, and partly as a result, the discovery of cheese, according to Paul Kindstedt, author of the book Cheese. This was important because human adults, at the time, were lactose intolerant, and cheese was the only way an adult could eat dairy products. Human evolution has since eliminated lactose intolerance in most people.\n2700 BCE — Epic of Gilgamesh describes vast tracts of cedar forests in what is now southern Iraq. Gilgamesh defies the gods and cuts down the forest, and in return the gods say they will curse Sumeria with fire (or possibly drought). By 2100 BCE, soil erosion and salt buildup have devastated agriculture. One Sumerian wrote that the “earth turned white.” Civilization moved north to Babylonia and Assyria. Again, deforestation becomes a factor in the rise and subsequent fall of these civilizations. (Perlin, 1991).\n2700 BC — Some of the first laws protecting the remaining forests are decreed in Ur, Messopotamia. (Grove, 1995).\n2600 BCE — Large scale commercial timbering of cedars in Phoenicia (Lebanon) for export to Egypt and Sumeria. Similar commercial timbering in South India.\n2500 BCE — First Water War, Mesopotamia — King of Lagash diverts water to deprive rival Sumerian kingdom of Umma of water in the “Gu’edena” (Edge of Paradise) region in present-day Iraq. The story of this first conflict was engraved on clay cylinders and somehow managed to survive 43 centuries before it ended up in a French museum. The story shows the back-and-forth nature of early warfare, as well as the role of water as both source of conflict and a target for conflict.\nThe war began with a dispute over how much water the city of Lagash should take from common irrigation canals. Apparently the two cities had an agreement about how the system should work, but and at some point, Umma destroyed monuments and boundary markers set out by the city of Lagash. The ruler of Umma “was as puffed up as the mountains,” one Lagesh inscription says. He “wickedly flooded the diked and irrigated field; he commanded that the boundary canal … be ruined.” So the armies of Lagesh killed 60 of the Umma soldiers and “left their bones on the plain.” Then the ruler of Lagesh “restored their canal to its place according to the righteous word of Enlil … Their canal which he had constructed from the river Tigris to the great river, the protecting structure, its foundation he had made of stone.” (Hatami & Gleick, 1994).\n2500 BCE — Mohenjo Darro civilization of Indus River valley achieves high level of public health and citywide sanitation.\n2000 BCE — Lead used as pigment in Crete.\n1720 BCE — Abi-Eshuh, grandson of Hammurabi and ruler of Sumer, dams the Tigris River in an attempt to prevent the retreat of rebels from Babylon. (Hatami & Gleick, 1994).\n1500 BCE — Soil erosion is both a consequence of growth and a cause of collapse of Central American city -states. (Grove, 1995).\n1300 BCE — Hebrew law as proclaimed by Moses includes provisions for humane slaughter and care of work animals. (M. Clifton, 2007)\n1450 BCE — Minoan civilization in the Mediterranean declines, but scholars are divided on the cause. Possibly a volcanic eruption was the source of the catastrophe. On the other hand, gradual deforestation may have led to materials shortages in manufacturing and shipping. Loss of timber and subsequent deterioration of its land was probably a factor in the decline of Minoan power in the late Bronze Age, according to John Perlin in A Forest Journey.\n1200 BCE — Troy dominates trade between the Aegean and Black seas due to its position on the north coast of what is now Turkey. Deforestation and soil erosion moves the coastline north over the ages. The ancient city is rediscovered in 1870 when Heinrich Schliemann accounts for the build-up of the coast over the centuries.\nEarliest ideas about nature, East and West\nShort excerpt from Rabindranath Tagore’s SĀDHANĀ, (See Gutenberg.org for full text)\nThe civilisation of ancient Greece was nurtured within city walls. In fact, all the modern civilisations have their cradles of brick and mortar.\nThese walls leave their mark deep in the minds of men. They set up a principle of “divide and rule” in our mental outlook, which begets in us a habit of securing all our conquests by fortifying them and separating them from one another. We divide nation and nation, knowledge and knowledge, man and nature. It breeds in us a strong suspicion of whatever is beyond the barriers we have built, and everything has to fight hard for its entrance into our recognition.\nWhen the first Aryan invaders appeared in India it was a vast land of forests, and the new-comers rapidly took advantage of them. These forests afforded them shelter from the fierce heat of the sun and the ravages of tropical storms, pastures for cattle, fuel for sacrificial fire, and materials for building cottages. And the different Aryan clans with their patriarchal heads settled in the different forest tracts which had some special advantage of natural protection, and food and water in plenty.\nThus in India it was in the forests that our civilisation had its birth, and it took a distinct character from this origin and environment. It was surrounded by the vast life of nature, was fed and clothed by her, and had the closest and most constant intercourse with her varying aspects…\nThe west seems to take a pride in thinking that it is subduing nature; as if we are living in a hostile world where we have to wrest everything we want from an unwilling and alien arrangement of things. This sentiment is the product of the city-wall habit and training of mind. For in the city life man naturally directs the concentrated light of his mental vision upon his own life and works, and this creates an artificial dissociation between himself and the Universal Nature within whose bosom he lies.\nBut in India the point of view was different; it included the world with the man as one great truth. India put all her emphasis on the harmony that exists between the individual and the universal. She felt we could have no communication whatever with our surroundings if they were absolutely foreign to us. Man’s complaint against nature is that he has to acquire most of his necessaries by his own efforts. Yes, but his efforts are not in vain; he is reaping success every day, and that shows there is a rational connection between him and nature, for we never can make anything our own except that which is truly related to us…\nThe first invasion of India has its exact parallel in the invasion of America by the European settlers. They also were confronted with primeval forests and a fierce struggle with aboriginal races. But this struggle between man and man, and man and nature lasted till the very end; they never came to any terms. In India the forests which were the habitation of the barbarians became the sanctuary of sages, but in America these great living cathedrals of nature had no deeper significance to man. The brought wealth and power to him, and perhaps at times they ministered to his enjoyment of beauty, and inspired a solitary poet. They never acquired a sacred association in the hearts of men as the site of some great spiritual reconcilement where man’s soul has its meeting-place with the soul of the world…\nOther Recommended sources:\nForest History: Looking at Pre-History\nClarence Glacken, Traces on the Rhodian Shore (1967) — “Glacken showed how past generations contemplated and interpreted the mutual relations between nature and human cultures. ‘After more than a decade of research, Glacken concluded that there had been three major ideas in the history of Western environmental thought: the idea of a divinely designed earth (both ecological theory and the intelligent design argument are direct descendants), the idea of environmental influence on people (similar to the environmental determinism popular in early anthropology), and the idea of human influence on the environment.” (Wikipedia)\nHatami, H., and Gleick, P. 1994. Chronology of conflict over water in the legends, myths, and history of the ancient Middle East. In: Water, war, and peace in the Middle East. Environment 36(3).\nJ. Donald Hughes, Pan’s Travail: Environmental Problems of the Ancient Greeks and Romans. Baltimore: Johns Hopkins, 1996.\nJohn Perlin, A Forest Journey.Countryman Press, 2005.\nWater Conflict Chronology List, Worldwater.org."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:802625ca-f73a-4221-a3f2-b1fce9b739c4>","<urn:uuid:e7f31684-a368-4380-add8-6dbb288bc481>"],"error":null}
{"question":"What key differences exist between standard photography and astrophotography that make space photos more challenging?","answer":"Astrophotography is more difficult than standard photography because it requires accounting for several unique factors: lighting, shadows, atmospheric changes, and the distance of heavenly objects. Additionally, astrophotography requires long exposure times, and historically, this has led to issues such as blurry images and problems with telescopes losing power, focus, or direction during extended photo shoots.","context":["Author : Lowell Bradford\nAstrophotography is the process of taking photos of objects in space. Whether photographing a celestial object visible with the naked eye, such as the moon or a group of stars, or astronomers photographing space with the Hubble telescope, photographing space is astrophotography. The practice of astrophotography does not date as far back as some other sciences, simply because it depends on photography. Photography didn’t become a viable invention until the early 19th century. The first case of astrophotography took place in 1840 when the moon was photographed for the first time. Over a century later, astrophotography is the method used to capture the world’s most phenomenal images of space. Astrophotography is available to everyone. From professional astronomers to the backyard skygazer, all can enjoy the wonders of astrophotography.\nOverview of Astrophotography\nThere are several methods used in astrophotography including various techniques, camera and video equipment, and various telescopes. Understanding the type of equipment used in astrophotography will help ensure you select the best methods for taking photos. The first thing to understand is that astrophotography is different from standard photography. Those accustomed to taking standard photos may find photographing celestial objects is more difficult than anticipated. Lighting, shadows, atmospheric changes, and the distance of heavenly objects must be taken into account to ensure the best photos are captured.\nTelescopes should be adjusted to accommodate for the rotation of the earth. This is accomplished by setting equipment to rotate in an opposite manner from earth. Telescope mounts are an important device used in astrophotography that ensures photos captured are precisely timed and accurately track heavenly objects. Preventing tracking errors is vital to successful astrophotography and there are modern-day breakthroughs in computer science that helps make that happen.\nAstrophotography is one of the oldest forms of science-based photography. Beginning in the 19th century, early astrophotography consisted of photographing the moon, stars, eclipses, and nebulas. The procedure requires long exposure and early images were known for being quite blurry. It was not uncommon for telescopes to lose power, focus, or direction during a prolonged photo shoot.\nThe earliest astrophotography photos captured were of the moon, followed by the sun and stars. It wasn’t until the early 20th century that astrophotography began to become an important scientific research method. The invention of refracting telescopes enabled more powerful imaging to be captured. As the 20th century progressed, new telescopes such as the Hale, Samuel Oschin, and Hubble revolutionized the art of professional astrophotography. By the late 20th century, new telescopes and equipment allowed for some of the most awe-inspiring space photos to be captured for the first time in history.\nWhile astrophotography is an important tool used in scientific applications, it is also a popular hobby. Advances in video equipment, standard, and digital cameras have enabled the backyard enthusiast to capture amazing photos. Computer software makes it possible to take photos that appear as though a professional took them. From the novice to the advanced hobbyist, everyone can enjoy the art and science of astrophotography.\nA variety of media devices and equipment setups are used. Examples include video and webcams, CCD, over-the-counter cameras, single lens and digital single lens cameras. Computer software may be used to adjust cameras and telescopes to zoom in on certain objects then take pictures. Photos taken may be edited in image processing software, for a clearer picture. Please consult the following links for additional information on astrophotography."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:98662f17-7243-44e6-8b93-f249dc488e6a>"],"error":null}
{"question":"How do the extreme surface conditions compare between Venus and Mars - which one is more intense in terms of temperature and pressure?","answer":"Venus has significantly more extreme surface conditions than Mars. Venus has the strongest greenhouse effect in the solar system with an average daily temperature of 462°C (864°F), making it even hotter than Mercury despite being further from the sun. The atmospheric pressure on Venus's surface is 92 times that of Earth, equivalent to being 1km deep in Earth's ocean. Mars, in contrast, has much colder conditions than Earth and has such low atmospheric pressure that water cannot exist in liquid state on its surface.","context":["Venus is the second planet from the sun in the Solar System. It is named after the roman goddess of love and beauty. Venus is the second brightest object in the night sky after the moon and has been known for centuries as the Morning or the Evening star. Venus is similar in size to Earth and is the closest planet to Earth. It completely rotates the sun in 224.7 Earth days. Let’s find out some more fun facts about Venus.\nInteresting facts about Venus\n- Venus is most visible in the sky shortly before sunrise and shortly after sunset and it is at this time that Venus is at its brightest. Venus is so bright that on a moonless night the light from the planet will cast a shadow.\n- Venus is a terrestrial planet meaning that it has a rocky body and is similar in size, mass and gravity to the Earth.\n- Venus outer atmosphere is made up mostly of carbon dioxide and an opaque layer of sulfuric cloud which gives the planet a silver appearance.\n- Venus has the strongest greenhouse effect of all the planets of the solar system and has the hottest surface temperatures. The average daily temperature on Venus is 462 °C (864 °F). This makes the surface hotter than mercury which is twice as close to the sun.\n- Due to the windy nature of the planet the surface temperature of Venus does not differ much between the day and night.\n- Venus has a double atmospheric vortex located at its south pole. This is similar to a hurricane except at the atmospheric level.\n- The atmospheric pressure on the surface of Venus is 92 times that of earth. This is equivalent to the pressure found at a depth of 1 kilometer under the ocean on Earth.\n- Venus is a very volcanically active planet with much of the surface being regenerated by volcanic eruptions. It is desert like and consists of large slabs of rock. 80% of the surface of Venus is volcanic plains. Venus has 167 volcanoes that are over 100km in diameter.\n- There are two continents one in the northern hemisphere and one in the southern hemisphere. The continent in the northern hemisphere, Ishtar Terra, is named after the Babylonian goddess of love Ishtar. The southern continent Aphrodite Terra is named after Aphrodite, the Greek goddess of love. This continent is the bigger of the two and is roughly the size of South America. Ishtar terra is approximately the size of Australia.\n- On the planet Venus it never rains, but research has shown lightning and thunder do occur.\n- Venus is only tilted on its axis 3 degrees compared to earths 23, meaning it experiences little variation in temperature during the seasons.\n- Venus only rotates on its axis every 243 earth days. This is the slowest of any major planet in the solar system. This rotation has slowed down by 6.5 minutes every day since the Magellan spacecraft visited it 16 years ago.\n- Venus rotates in a clockwise direction which it the opposite direction to every other planet in the solar system.\n- Venus has no natural satellites or moons; this is due to strong solar tides which would push any natural satellites out of the planets orbit.\n- Venus’ orbit of the sun is the closest to being completely circular with a variation of only 0.01.\n- Venus has an ozone layer which exists high above the surface in the planet’s atmosphere.","michigander See Bro! Look over there, I told you, if we waited long enough, another one of these scooters would show up!! That one's probably better than this one I'm riding. Hop on up there and we'll see if we can find another one of those shiny thingies we've been dragging behind this slow POS!\nRegister to post comments and participate in contests.\nThis contest is fueled by the following news: Spirit, NASA's robotic rover which is not on the surface of Mars, has extended its mechanical arms for the first time. It is now taking extraordinarily close photos of the rush colored soil of Mars. Mars is 300 million miles from Earth.\nRed Planet: Mars\nIn our solar system, planet Mars is located at fourth position from the Sun and according to size it is the seventh largest planet. Planet Mars received its name from the Roman god of war, corresponding to the one called as Ares in Greek mythology. Mars is sometimes also referred as the ‘Red Planet' because of its reddish surface, which can be attributed to iron oxide.\nMars belongs to the family of terrestrial planets with an atmosphere that has very low-density. Features of the topography of Mars surface can be compared with the lunar craters and volcanoes, valleys, deserts and polar ice caps like the ones seen on Earth. Extinct volcano Mount Olympus on Mars is the highest mountain in our solar system, and Mariner Valley on the planet is the largest canyon. In addition, three articles were published in the journal Nature in 2008 that put forward the evidence that Mars has an impact crater on its northern hemisphere that is the largest known in the solar system. Its length is 10,600 km and width is 8500 km, and it is roughly four times larger than the earlier known largest impact crater, also located on Mars near its south pole. In addition to the similarity of its surface topography, rotation intervals and changing of seasons on Mars are also similar to Earth, but its climate is much colder and drier than on Earth.\nUntil the first fly-by mission to Mars ‘Mariner 4' in 1965, many researchers were of the view that its surface is water in liquid state. This opinion was based on the observations of periodic variations in light and dark areas, particularly in the polar latitudes, which looked like continents and oceans. Dark fossae on Martian surface were interpreted by some experts as irrigation canals for liquid water. Later it was discovered that it was just an optical illusion and there were no such depressions.\nBecause of low-pressure on the surface of Mars, water can not exist in liquid state there, but it is quite possible that conditions were different in the past, and therefore the existence of elementary life on the planet can not be ruled out. On July 31, 2008 NASA's Phoenix lander discovered water in the form of ice on Mars.\nCurrently (as of February 2009), there are three operational spacecraft engaged in the research of Mars: ‘Mars Odyssey', ‘Mars Express' and ‘Mars Reconnaissance Orbiter', and the number is more than on any other planet except Earth. Mars at the moment also has two exploring rovers viz. Spirit and Opportunity. Mars also has on its surface many inactive landers and rovers that have already completed their mission. Geological data, collected by all these missions, suggest that a large part of Martian surface was covered with water in the past. Studies over the past decade have also revealed geyser like activity at some places on the surface of Mars. Based on the observations from NASA's ‘Mars Global Surveyor' some parts on the southern polar cap of Mars have been gradually receding.\nMars has two natural satellites, Phobos and Deimos (in Greek they mean ‘panic/fear' and ‘terror/dread' - the names of two sons of Ares that accompanied him to battle), which are relatively small and irregular in shape. These may be the asteroids, captured by the gravitational field of Mars, and similar to Asteroid 5261 Eureka from the Trojan group.\nMars can be seen from Earth with the naked eye. Its apparent magnitude is about 2.91 m (at closest approach to Earth), and its brightness is exceeded only by Jupiter, Venus, Moon and Sun.\nMinimum distance of Mars from Earth is 55.75 million miles (when Earth comes exactly between the Sun and Mars), and the maximum is about 401 million km (when the Sun is exactly between Earth and Mars). Average distance from Mars to Sun is 228 million km (1.52 AU), its orbital period around the sun equals 687 Earth days. The orbit of Mars has quite noticeable eccentricity (0.0934), that is why its distance from the Sun varies from 206.6 to 249.2 million km. Inclination of Martian orbit is equal to 1.85 °.\nMars is almost half the size of Earth and its equatorial radius is 3396.9 km (53.2% that of the Earth). The surface area of Mars is approximately equal to the land area on Earth. Sufficiently fast rotation of the planet has led to significant flattening which is evident from the fact that polar radius of Mars is about 21 km less than the equatorial one. Mass of the planet is 6,418 × 1023 kg (11% of Earth's mass). Gravitational acceleration at its equator is equal to 3.711 m / s² (0.378 that of the Earth), orbital velocity is 3.6 km / sec and its escape velocity is 5.027 km / sec."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:694943a3-44c0-4559-9964-a64fbbd6d527>","<urn:uuid:3d8b08ea-95f5-4dfc-a56e-c64bacd7860d>"],"error":null}
{"question":"Could you explain how concrete's physical properties compare to water in terms of particle formation and density?","answer":"Concrete and water have different physical properties. Concrete is formed by coalescence of separate particles into a solid mass, creating an artificial stone-like material through mixing cement with aggregates and water. In contrast, water has a specific density of 62.4 pounds per cubic foot at typical temperatures, and serves as the baseline for density comparisons through specific gravity measurements. Unlike concrete's solid mass formation, water remains in liquid state and is used as the reference point (1.0) for specific gravity calculations.","context":["CONCRETEcon•crete (kon′krēt, kong′-, kon krēt′, kong- for 1–10, 11, 14, 15; kon krēt′, kong- for 12, 13),USA pronunciation adj., n., v., -cret•ed, -cret•ing.\n- constituting an actual thing or instance;\nreal: a concrete proof of his sincerity.\n- pertaining to or concerned with realities or actual instances rather than abstractions;\nparticular (opposed to general): concrete ideas.\n- representing or applied to an actual substance or thing, as opposed to an abstract quality: The words \"cat,'' \"water,'' and \"teacher'' are concrete, whereas the words \"truth,'' \"excellence,'' and \"adulthood'' are abstract.\n- made of concrete: a concrete pavement.\n- formed by coalescence of separate particles into a mass;\nunited in a coagulated, condensed, or solid mass or state.\n- an artificial, stonelike material used for various structural purposes, made by mixing cement and various aggregates, as sand, pebbles, gravel, or shale, with water and allowing the mixture to harden. Cf. reinforced concrete.\n- any of various other artificial building or paving materials, as those containing tar.\n- a concrete idea or term;\na word or notion having an actual or existent thing or instance as its referent.\n- a mass formed by coalescence or concretion of particles of matter.\n- set or cast in concrete, to put (something) in final form;\nfinalize so as to prevent change or reversal: The basic agreement sets in concrete certain policies.\n- to treat or lay with concrete: to concrete a sidewalk.\n- to form into a mass by coalescence of particles;\n- to make real, tangible, or particular.\n- to coalesce into a mass;\n- to use or apply concrete.\nINin (in),USA pronunciation prep., adv., adj., n., v., inned, in•ning.\n- (used to indicate inclusion within space, a place, or limits): walking in the park.\n- (used to indicate inclusion within something abstract or immaterial): in politics; in the autumn.\n- (used to indicate inclusion within or occurrence during a period or limit of time): in ancient times; a task done in ten minutes.\n- (used to indicate limitation or qualification, as of situation, condition, relation, manner, action, etc.): to speak in a whisper; to be similar in appearance.\n- (used to indicate means): sketched in ink; spoken in French.\n- (used to indicate motion or direction from outside to a point within) into: Let's go in the house.\n- (used to indicate transition from one state to another): to break in half.\n- (used to indicate object or purpose): speaking in honor of the event.\n- in that, because;\ninasmuch as: In that you won't have time for supper, let me give you something now.\n- in or into some place, position, state, relation, etc.: Please come in.\n- on the inside;\n- in one's house or office.\n- in office or power.\n- in possession or occupancy.\n- having the turn to play, as in a game.\n- [Baseball.](of an infielder or outfielder) in a position closer to home plate than usual;\nshort: The third baseman played in, expecting a bunt.\n- on good terms;\nin favor: He's in with his boss, but he doubts it will last.\n- in vogue;\nin style: He says straw hats will be in this year.\n- in season: Watermelons will soon be in.\n- be in for, to be bound to undergo something, esp. a disagreeable experience: We are in for a long speech.\n- in for it, [Slang.]about to suffer chastisement or unpleasant consequences, esp. of one's own actions or omissions: I forgot our anniversary again, and I'll be in for it now.Also,[Brit.,] for it.\n- in with, on friendly terms with;\nfamiliar or associating with: They are in with all the important people.\n- located or situated within;\ninternal: the in part of a mechanism.\n- in favor with advanced or sophisticated people;\nstylish: the in place to dine; Her new novel is the in book to read this summer.\n- comprehensible only to a special or ultrasophisticated group: an in joke.\nincluded in a favored group.\ninbound: an in train.\n- being in power, authority, control, etc.: a member of the in party.\n- playing the last nine holes of an eighteen-hole golf course (opposed to out): His in score on the second round was 34.\n- Usually, ins. persons in office or political power (distinguished from outs).\n- a member of the political party in power: The election made him an in.\n- pull or influence;\na social advantage or connection: He's got an in with the senator.\n- (in tennis, squash, handball, etc.) a return or service that lands within the in-bounds limits of a court or section of a court (opposed to out).\nv.t. Brit. [Dial.]\n- to enclose.\nMODERNmod•ern (mod′ərn),USA pronunciation adj.\n- of or pertaining to present and recent time;\nnot ancient or remote: modern city life.\n- characteristic of present and recent time;\nnot antiquated or obsolete: modern viewpoints.\n- of or pertaining to the historical period following the Middle Ages: modern European history.\n- of, pertaining to, or characteristic of contemporary styles of art, literature, music, etc., that reject traditionally accepted or sanctioned forms and emphasize individual experimentation and sensibility.\n- (cap.) new (def. 12).\n- [Typography.]noting or descriptive of a font of numerals in which the body aligns on the baseline, as 1234567890. Cf. old style (def. 3).\n- a person of modern times.\n- a person whose views and tastes are modern.\n- [Print.]a type style differentiated from old style by heavy vertical strokes and straight serifs.\nHello peoples, this photo is about ROBATA 72 CONCRETE IN NOUGAT – RAVINE MODERN (awesome Linear Fire Pit #12). This picture is a image/jpeg and the resolution of this photo is 983 x 656. This picture's file size is just 91 KB. Wether You decided to download This image to Your PC, you might Click here. You may too download more pictures by clicking the image below or see more at this post: Linear Fire Pit.\nThe house frequently has its figure. Also with cottages or the bungalow can be found in britain. Don't want to modify the building's framework is toomuch, ROBATA 72 CONCRETE IN NOUGAT – RAVINE MODERN (awesome Linear Fire Pit #12) types compete with classic pad.\nNever asked a result, lovely! So that you can take care of the identity of the building, Kitchen's artist Alex Saint Structure incorporating a kitchen layout in addition to the key building. The result? Stunning! Yes, Chelshire was located in by a cottage, the united kingdom may be the building involved.\nIf you just like the setting of the home that is cozy as well as peaceful with a minor antique sense with possibly a great choice for you. To obtain this style you utilize a wooden ground and can make cheap kitchen units an election that have pattern includes a sample. Using pastel shades brown with variations of lumber and bright hues is likely to make supper within the kitchen together with your household may experience warmer.\nThe bungalow was built-in the 18th-century and is now past the stage of renovation. As opposed to looking to imitate the cottage's design, Alex Saint decided to develop one more home design that can lessen the entire lodge's structural change and protect the type with this residence.\nDesire to deliver the setting is comfortable and hot, the furniture has a delicate bright color as his finishing. Storage that is much and contemporary equipment can be beautiful home design suits this one. Similarly with up lighting to illuminate the area through the night.\nA cube's kitchen style in the type. The usage of glass here is supposed to have the ability to control the heat. While summer occurs, glass can be exposed to provide fresh-air into the area. Surfaces utilising the same product with the outside patio for there to be always a frequent bond involving the Linear Fire Pit with new home.","Density and Specific Gravity\nweight per unit volume. There are 62.4 pounds of water per\n1 cubic foot. This means that a tank of volume 1 cubic foot,\nfilled with water, would contain 62.4 pounds of water. The\ndensity of all substances depends on the temperature. For\nexample, at 32°F the density of water is 62.41 pounds/ft3;\nat 62°F the density of water is 62.36 pounds/ft3.\nFor most practical purposes the density of water can be considered\nequal to 62.4 pounds/ft3. The following table lists\nweight-volume relationships of water in different units:\nSpecific gravity is defined as the ratio of the density of a substance to the density of water. Written mathematically:\nThe density of lead is 708 pounds per cubic foot.\nWhat is the specific gravity of lead?\nSolution: Using the above equation,\nSubstances with higher specific gravity will settle more quickly than substances with lower specific gravity A grit chamber is designed to remove particles with larger specific gravity than a settling tank.\nThe strengths of pollutants or substances in water are expressed in a number of ways:\nMilligrams per liter is a weight to volume ratio. A suspended solids concentration of 200 mg/L means that one liter of sampled water will contain 200 milligrams of suspended solids.\nTo say that 200 mg/L equals 200 ppm would mean that\none liter of sampled water weighs one million milligrams.\nThen, 200 mg/L would be rewritten 200 mg per million milligrams\nof sampled water which is a parts per million weight to weight ratio.\nThe Weight-Volume Water Relationship indicates that one liter of\nwater weighs 1000 grams which is equal to one million milligrams.\nTherefore if one liter of water weighs one million milligrams then\n200 ppm = 200 mg/L. However, one liter of pure water weighs\none million milligrams. If the water contains pollutants,\none liter will no longer by exactly equal to one million milligrams.\nThen 200 mg/L will no longer be exactly equal to 200 ppm.\nAnother way of stating this is, if the specific gravity of a water\nsample is not equal to one, then ppm is not exactly equal to milligrams\nOne liter of raw sewage was found to weigh 1002\ngrams. (1,002,000 mg). What is the specific gravity of this\nWe previously used 62.4 lb/ft3 as the density of water. Density, however, may also be expressed in metric units, which the table indicates as 1000 grams/liter, so that,\nIn most cases, the specific gravity of sewage will be very close to 1,000 or 1 liter will weigh approximately 1000 grams and for this reason parts per million can be considered equivalent to milligrams per liter. Because of this discrepancy, however, the use of milligrams per liter is now the more favorable expression of concentrations. The use of just milligrams per liter eliminates any opportunity for confusion.\nExpression of concentrations in terms of percent is generally used in cases where the concentrations of pollutants are high. In sludge analyses, for example, results are expressed in terms of percent solids, percent moisture, percent volatile matter, and percent ash. Percent is a weight to weight ratio. A 5 percent sludge solids concentration indicates that there are 5 parts of solids per 100 parts of sludge, or 5 pounds of solids per 100 pounds of sludge.\nPercent calculations are made by using the following equation:\nA 56.3 g sludge sample contained 2.7 g of dry solids.\nWhat is the percent concentration of solids?"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:249d9c2c-3a4d-4393-970a-eacbbc299d5d>","<urn:uuid:dedf66f1-b6c7-41dd-ba58-39d65123b299>"],"error":null}
{"question":"What are the practical requirements for commercial imports, and what sustainable alternatives are being developed for maritime shipping?","answer":"Commercial imports require several practical steps: obtaining an EORI number for EU trade, applying for customs credit to avoid additional charges, arranging appropriate packaging, securing transit insurance, and organizing warehouse handling. For sustainable shipping alternatives, the industry is exploring various clean fuel options including biofuels, synthetic fuels like methanol, hydrogen, and ammonia, as well as electric propulsion systems. However, each alternative has challenges - hydrogen requires costly storage and immature technology, biodiesel faces production volume limitations, methanol is space-intensive and expensive, while electric vessels are currently limited to short distances of around 95 kilometers due to battery technology constraints.","context":["Are you about to start importing goods commercially from countries outside the EU? There are a number of things to think about before beginning.\n1. Find out what regulations and customs duties apply\nCheck whether you need a license to import the goods. Otherwise, you may have problems when they arrive at the border. Also, see what the customs duties are, so that you include them in your financial calculations.\n2. Apply for an EORI number\nTo import goods into Sweden, Denmark and Finland, you need an EORI number. EORI numbers are used when doing business with a customs authority of any EU Member State. To obtain your EORI number, apply to Finnish Customs.\n3. Think about whether you need customs credit\nIf you intend to import regularly, it is advisable to apply for customs credit. Tullverket will then send an invoice for duty and VAT on imports directly to you. If you do not have customs credit, the forwarder will have to pay VAT and duty and then invoice it on; you will then incur a payment handling charge. If you have your own credit license, you avoid having to pay these charges.\n4. Specify who is to pay the cost of shipping\nAre you to bear the cost of shipping all the way? Or is the supplier to pay a share of the shipping costs in the country of dispatch, for example, collection, terminal fees and other charges? The terms and conditions of delivery should be clearly detailed in the agreement with the supplier of the products you are to import. Obviously, it's convenient to have shipping organized end-to-end. The drawback is that you have no control over method of transport, the logistics supplier used and how long it will take to get the goods to you. The supplier may not choose the most cost-efficient method either. So you should think about how much control you want over the shipping process.\n5. Decide how the goods are to be packed\nGood packing is the be-all and end-all where delivering goods is concerned, whether the goods are to be sent inside the country, or come from the other side of the world. If sea transport is used, it is important for a dehumidifying agent to be packed inside the boxes if the product is sensitive to damp. Discuss with the product supplier how the goods are to be packed and specify this in your agreement if appropriate.\n6. Obtain quotations from logistics suppliers\nThe mode of transport chosen will be based entirely on the size of shipment, the value of the goods and the lead time that your shipment allows. In addition, different methods of s\nhipping may suit different types of product. Some may have to be air-freighted, while others can go by sea and take a month to arrive. In many cases, a combination of transport modes is a good solution. When obtaining quotations from logistics suppliers, it is also important to ask whether all costs are included or whether any additional expense will be involved.\n7. Consider transit insurance\nCheck whether you have, or need, transit insurance. Insurance is not a must-have. But remember that all carriers and forwarders only bear limited liability for your goods under NSAB 2000 (the General Conditions of the Nordic Association of Freight Forwarders). According to these conditions, the financial liability of the forwarder is calculated solely by weight, not value. The most common solution is to take out transit insurance to supplement business insurance. The logistics supplier may also sell transit insurance for a particular delivery or for all deliveries under its auspices.\n8. How should your goods be looked after once they arrive in the country?\nWhen your goods arrive in the country, someone has to look after them, address them and make sure they are sent on. You can either do it yourself, or you can engage a third-party logistics (TPL) supplier to do it for you. If you don't want to pay duty and import VAT before the goods leave the warehouse, you must engage a logistics supplier with its own customs warehouse. A customs warehouse requires a special license, so most importers procure this as a service.","Date: 26 May 2020\nA future where shipping no longer imperils people, oceans and the climate is one mariners dream of. But what does a transition to sustainable shipping really mean?\nShipping is the lifeblood of the world economy. A highly efficient mode of transportation, it enables more than 80 per cent of global trade, moving everything from smartphones to clothes to cars across the planet.\nBut here is the catch: The industry pumps around 3 per cent of the world's annual greenhouse gas emissions into the earth's atmosphere, fuelling climate change. If shipping were a country, it would be the sixth-largest emitter globally.\nWith global trade as well as awareness over the industry's environmental impacts on the rise, the maritime sector has come under tremendous pressure to devise sustainable ways to transport people and goods.\nHeeding growing calls for change, the International Maritime Organisation (IMO) two years ago established a target of halving the sector's carbon-dioxide emissions by 2050 relative to 2008 levels.\nChange is not new to the sector. Over the centuries, technological progress has enabled seafarers to transition to ever more efficient energy sources. Coal-fired steam engines largely replaced paddles and sails two centuries ago, only to see oil take over a century later.\nTechnology is poised to play a key role yet again as the industry sets out on its decarbonisation journey, and the solutions needed are already out there today.\nBut despite the waves that clean fuels, green propulsion systems and energy-efficient vessel designs are making, shipping's path towards sustainability is far from clear-cut. With consensus lacking on how the maritime industry will evolve in the years to come, it is headed for uncharted waters.\nThe shipping sector is beginning to grasp that its carbon footprint is a challenge unlikely to be tackled through innovation alone, because it ultimately hinges on more than individual industry players switching to climate-friendly technologies.\nSo, what exactly does a transition to sustainable shipping mean?\nSustainable shipping puzzle\nToday, fossil fuels power virtually all commercial shipping activities. A staggering 82 per cent of shipping's energy needs are met by heavy fuel oil, with marine gas and diesel oil making up the remaining 18 per cent.\nAs the sector races to switch from climate-disrupting fuels to cleaner alternatives, many industry players tout liquefied natural gas (LNG) as a bridge fuel that could help shrink shipping's climate impacts while enabling ship owners to comply with the IMO's new sulphur cap. However, LNG is still a fossil fuel. Not only are the emissions cuts it can deliver insufficient, but the technology also grapples with leakages of the climate super-pollutant methane.\nAs such, the industry has pinned its hopes on truly clean fuels to replace polluting ones. Potential candidates include various biofuels and synthetic fuels generated from clean energy, such as methanol, hydrogen and ammonia. All come with their strengths and weaknesses.\nHydrogen, for instance, if generated from renewables, eliminates carbon and sulphur emissions when combusted or used in fuel cells. But the technology needed for hydrogen applications onboard ships has yet to come of age. Its volumetric energy density is lower than that of fossil fuels, and its storage requirements are costly.\nOther clean alternatives present their own challenges. Biodiesel may be the readiest green fuel, but it is difficult to raise its production volume to levels needed to meet global demand. While methanol is easy to transport and handle, it is space-hungry and toxic, and its costs are currently inhibitive. Ammonia emits no carbon or sulphur when used in fuel cells and does not have special storage needs, but it is costly to produce as well as toxic.\nElectric propulsion has been proposed as yet another important piece of the sustainable shipping puzzle, but it too faces barriers. Infrastructure is still in its infancy, as few ports provide charging facilities, and with the current state of battery technology, electric vessels are generally unable to travel more than around 95 kilometres.\nThis means big battery-powered bulk and container carriers are currently out of the question, which is important, as 85 per cent of shipping's carbon dioxide emissions come from large vessels. While the energy density of batteries is forecast to increase, experts do not expect electric solutions to become commercially viable for bigger long-haul ships for many years to come.\nTo overcome such obstacles, the industry is exploring ways to pairing electric drives with other technologies, such as hydrogen-powered fuel cells, in hybrid systems. But, either way, without adopting clean fuels, the sector will not achieve the emissions reductions needed, according to a new report by Singapore's Nanyang Technological University and the Singapore Maritime Institute.\nYet switching to clean fuels is not the only way to address shipping's climate impacts, and the current absence of market-ready alternatives to fossil fuels only underscores the necessity not only to deploy energy-saving hull and propeller designs but also to rethink how vessels are operated.\nWe have spent too long turning this into an engineering problem.Dr Tristan Smith, energy institute, University College London (UCL)\n\"What the industry needs is a focus on operational carbon intensity, which measures the actual emissions produced for the amount of transport work that individual vessels do. We have spent too long turning this into an engineering problem, which distracts people from the fact that what matters is what gets emitted,\" said Dr Tristan Smith of the University College London's (UCL) Energy Institute.\nSaving fuel can be as simple as slowing down. But although the idea of slow steaming has been around for years, few have embraced it—and the global economy's dizzying pace is to blame, said Sanjay Kuttan, executive director of the Singapore Maritime Institute.\n\"The problem always is that buyers want their products today, so they require ships to travel at breakneck speeds. But the higher the speed is, the more fuel you burn. If everyone slowed down a bit, the industry could achieve significant emissions cuts with current technologies, clever designs and intelligent operations,\" he told Eco-Business.\nUnravelling the challenge\nSustainable shipping may be difficult to pin down, but industry players would say the transition is even harder to pull off. It is easy to see why: port infrastructure around the planet needs adjusting, tens of thousands of ships must be replaced or retrofitted, and the world needs to ramp up the production volume of whichever clean fuel option it goes for. All this calls for a concerted global effort.\nBut what happens when no one knows which solution is the best way forward?\nIt is a big decision to invest significant capital in a project not knowing whether the technology will be mature a few years down the road.Matthew Forrest, director, shipping and offshore, BNP Paribas\nUntil industry players know for certain that a particular technology will catch on, they will remain squeamish about investing in it. And as they bide their time, the switch becomes a chicken-and-egg problem, said Simon Bennett, general manager of sustainable development at the China Navigation Company & Swire Pacific Offshore Operations.\nEach solution requires dedicated infrastructure, yet without enough demand for clean fuels or battery power from ships already using them, port operators are reluctant to build the logistics and storage facilities needed.\nConversely, without these supporting services, ship owners will not be able to make the switch. They cannot adopt hydrogen if ports along their routes lack bunkering facilities for the fuel. An electric vessel that cannot plug into the power grid onshore gets stranded.\n\"It is a big decision to invest significant capital in a project not knowing whether the technology will be mature a few years down the road, so people are cautious because they do not want to risk ending up with stranded assets,\" said Matthew Forrest, director of shipping and offshore at French banking group BNP Paribas.\nFurther complicating the issue is the different political agendas of countries due to their diverse geographies, climate conditions and resource endowments. Given its tremendous capacity to generate synthetic fuels from solar energy, Australia, for instance, may have a different vision for the industry than states well positioned to produce biofuels.\nThe problem with many different clean fuel and propulsion alternatives is that countries will struggle to scale up technologies to the levels needed to bring costs down, said Bennett.\n\"It is going to be difficult to get a consensus to deliver the same spread of infrastructure that we have for oil and gas at the moment. We are a long way away from the silver bullet to replace fossil fuels,\" he observed.\nWhat the switch to sustainable seafaring needs most, it appears, is better collaboration among industry players to align their decarbonisation strategies. \"It is really about how the sector comes together and decides which way it should move,\" Kuttan told Eco-Business.\nThat doesn't mean every single ship must be powered by the same technology. Forrest said: \"For larger ocean-going vessels that are trading globally, you need a common fuel source that they can refuel wherever they are. The infrastructure must be available to support these ships. For more regional trade, you could have more regional solutions that suit regional demands.\"\nWith its island-dotted seas, Southeast Asia, for instance, could be a prime location for battery-powered sailing, observed Soren Kvorning, president for Asia-Pacific at engineering firm Danfoss, which electrifies ferries around the world.\n\"There are a lot of ferries and smaller vessels travelling on short distances from island to island in the region, and on fixed routes. They could already be fully electric by now,\" he said.\nOver the next five years, global trade volume could grow at an annual rate of 3.8 per cent. Under business-as-usual, the sector's greenhouse gas emissions could grow between 50 per cent and 250 per cent by 2050.\nWhile the market will ultimately select the cheapest and most technologically mature sustainable shipping solutions available, the current reluctance to invest in them necessitates stronger regulatory intervention.\nSmith told Eco-Business: \"The IMO's initial strategy of greenhouse gas reductions was progress, as there was no such signpost before. But … the strategy does not elucidate whether the shift will happen overnight on 31 December 2049, or gradually.\"\nThere must be a detailed timeframe that stipulates how quickly the sector's fuel consumption must shift away from fossil sources in the coming decades, he said. \"No one has clarity at the moment. The IMO needs to show that it is continuing in the direction laid out in the initial strategy.\""],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:2c525b18-9cb6-4cf5-8758-d30528e4e2dc>","<urn:uuid:78073cbe-4f48-4fda-92a2-1414ee8daaf4>"],"error":null}
{"question":"What are the three specific tests included in PVEL's energy forecast protocol for solar panels?","answer":"The three tests in PVEL's energy forecast protocol are: 1) PAN file testing, which measures panel output at 22 different temperatures and irradiances, 2) Light-induced degradation (LID) testing, and 3) Incidence Angle Modifier (IAM) testing, which measures output at ten different incidence angles from zero to 90 degrees.","context":["PV module test facilities generally begin with procedures certifying that modules meet IEC or UL standards. PV Evolution Labs (PVEL), a small facility that began doing module evaluation in 2010, has been referring manufacturers seeking certification to other labs.\n“We have the capability and will start offering the service soon,” explained PVEL CEO Jenya Meydbray, “but it is not a strategic focus.”\nPVEL’s focus, Meydbray said, is the downstream PV market. “The needs of bankers and developers are different than those of manufacturers. They want more than just raw data test reports. They need higher-level advisory services.”\nAdvisory services are a PVEL differentiator, Meydbray said, setting it apart from more established testing labs.\nPVEL’s testing may be different from the bankability testing described by Renewable Energy Testing Center and Intertek representatives in this series (see previous articles Testing and Ranking Solar Module Quality and What Do Solar Module Test Procedures Prove?).\n“If bankability is the ability to get institutionally financed,” Meydbray said, “that is what we provide. But it is not enough to provide a third-party test report. PVEL services start with the test report to the manufacturer, they don’t end there.”\nWith its diverse relationships on the “buy side,” Meydbray said, “we can help established manufacturers access buyers. If they do well on our extended reliability testing, we can introduce them around, circulate their test reports, and support getting their product allocated.”\nThere are two key sets of PVEL test protocols, Meydbray said. The first set has to do with energy forecasting.\nAfter a developer’s extensive -- and expensive -- effort to make a project happen, Meydbray explained, the majority of the return comes from a “development fee” when the project goes on-line. “That fee is determined by its forecasted lifetime output in kilowatt-hours made by an independent third-party engineering firm like Black & Veatch or BEW on behalf of the bank.”\nA per-kilowatt-hour price determines the potential cash flow, the bank’s loan and the developer’s fee, Meydbray said. “Developers live and die on that fee. A 1 percent higher energy forecast can increase a developer’s profitability by 10 percent.”\nPVEL’s first protocol characterizes panel performance in varying environmental conditions and therefore provides some certainty around the energy forecast. “[Just] 1 percent can mean millions of dollars. We can do these tests for around $15,000, depending on the details. It becomes a no-brainer.”\nThe first of the three tests in the energy forecast protocol is a PAN file.\n“It is a software file used to project energy and cash flow,” Meydbray said. “It contains raw data from twenty-two varying temperatures and irradiances. Black & Veatch, with whom we have a strategic partnership, turns that raw data, using a software platform called PV Syst, into PAN file coefficients that characterize the panel’s output for energy forecasting.”\nThe second test is a third-party report on the light-induced degradation (LID) of a panel. “That can be a half percent to 3 percent of output, and every fraction of a percent really matters. The test is easy and not expensive.”\nA third measurement done by PVEL is called the Incidence Angle Modifier (IAM). “A solar panel’s ratings are done with light at a normal incident angle, but in the field, a panel is hit by light from many angles, depending on the time of day, the season and the installation,” Meydbray explained. “We characterize performance as the light angle changes.”\nThe IAM measures output at ten different incidence angles from zero to 90 degrees, Meydbray said. The method was developed by PVEL’s David King when he was a Director at Sandia Labs. It is performed at PV USA, PVEL’s outdoor solar testing facility. “PV USA was built in 1986,” Meydbray said. “PVEL took it over in early 2011.”\nThe first three tests are focused on increasing the accuracy of the energy forecast and characterizing the product’s out-of-the-box performance, Meydbray said. “But of course, the product does not perform over 25 years in the same way. It degrades. That’s what most labs are looking at: accelerated lifetime testing.”\nProduct certification “does not catch most panel problems seen in the field because it is performed before high volume manufacturing ramps,” Meydbray said. Most field problems are due to manufacturing quality control failures. “Once you start producing 10,000 modules per day, you start to get deviations in the process that impact product quality.”\nCurrent UL and IEC certification requirements are limited and are done before manufacturing ramps up. “This regulatory regime doesn’t provide the protection investors need. That is the gap we are filling.”\nPVEL has developed statistical models for randomly sampling modules, Meydbray explained. “If you want 100 percent certainty that all the modules are defect-free, you have to test every one. Nobody will do that because it is too expensive and takes too long. We have a statistical model that lets project owners decide their risk tolerance.”\nThe decision, Meydbray said, “has implications on cost of capital, insurance premiums, residual value, resale value -- real commercial things that influence the success and profitability of a project.”\nThe PVEL statistical model allows developers to choose a level of certainty and a cost. “The cost for 99 percent certainty,\" Meydbray said, is usually “between $0.005 cents per watt and $0.007 cents per watt.”\nThe cost for the first set of three tests is $15,000 to $25,000, depending on variables, Meydbray added.\nPVEL and PV consultant SolarBuyer are developing an Approved Vendor Program “that puts this all together,” Meydbray said."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:885e3329-08e9-479f-81ad-01fe4bd06347>"],"error":null}
{"question":"How do type I and type II muscle fibers differ in their response to various training intensities?","answer":"Type I (slow-twitch) fibers are smaller, weaker, but more fatigue-resistant compared to Type II (fast-twitch) fibers. While it was traditionally thought that Type II fibers had superior growth potential and responded better to high-intensity training (>50%-1RM), recent evidence shows that low-intensity training to failure can produce comparable whole-muscle hypertrophy. Type I fibers can actually grow at rates similar to Type II fibers during low-intensity training, as demonstrated by studies showing bodybuilders having larger Type I than II fibers, while powerlifters and Olympic lifters have larger Type II than I fibers.","context":["Drop Set Science\nHow’s it going Train Loco readers! Today we have an awesome guest blog by Dan Ogborn, one of the industry’s top experts on the effects of strength training on skeletal muscle. We’ve read Dan’s blogs before and we are big fans of his work because he uses science to back up his claims, so we asked him to spread some knowledge with the Train Loco readers so you all could get into the weight room and apply drop sets to your training protocols and further your gains. Enjoy!\n-Eric and Chris\nThe dropset goes by many names, and opinions on their use in hypertrophy training are just as plentiful. To some, it’s a gateway to pain and overtraining, an unnecessary technique reserved only for those “advanced” enough to handle it. To others it’s the quickest ticket to slabs of muscle. Despite the widespread use and abuse of dropsets in training, and the variety of methods for their use, little is known as to whether or not dropsets are actually effective, and if they are, how they work from a physiological perspective.\nThe Science of Drop Sets\nUnfortunately the scientific community hasn’t given much thought to the dropset, and I’m not sure this technique will see much time in the lab in the future either. Two studies from Goto et al. (1,2) have suggested that dropsets may confer a slight hypertrophic advantage. In the first (2), the inclusion of a lower load set (50%-1RM) immediately after a high-load set (90%-1RM) resulted in a greater acute growth hormone (GH) response and post-exercise muscle swelling which the authors cautiously theorized could enhance the adaptation to training. The significance of the post-exercise GH spike is unclear (3), especially in the context of recent evidence that post-exercise fluctuations in systemic hormones (like GH), do not necessarily impact or correlate with the ultimate hypertrophic response (4-8).\nIn the second study, Goto et al. (1) trained groups of men in what they considered to be a hypertrophy-type training program (variable %-1RM, minimal rest) for six weeks, then split the groups into a pure strength group (5 sets at 90%-1RM, 3 min rest) and a strength group that performed a final dropset for their last set (5 sets at 90%-1RM, drop to 50%-1RM on last set), over an additional four weeks. While both groups increased muscle size roughly 5% in the initial hypertrophy phase, the addition of the dropset in the subsequent training phase did not result in a statistically significant difference in growth between the groups (2% vs -1%, p=0.08). While this is a very specific experiment that may not reflect what happens in most hypertrophy training programs, it doesn’t provide the most compelling case that dropsets confer a hypertrophic advantage.\nDropsets are actually difficult to study, and in Goto et al’s (1,2) work there’s a major limitation. Scientific studies require strict controls in order to determine how individual variables contribute to the result of the experiment. In the case of the dropset it’s hard to control for the volume of exercise or work performed when you add on additional dropsets between the experimental groups. It may be that the dropset advantage in these studies (1,2) is simply because they had higher training volumes than without dropsets, that may contribute to increased muscle growth.\nSo it seems at this point we have many anecdotal accounts and articles singing the dropset praises, however the scientific evidence (1,2), while showing the potential for modest benefit at best, can’t give us a solid conclusion. While the data may not be there yet, there is a nice physiological rationale that can be used to justify using dropsets in your hypertrophy programming based on some recent evidence on the effects of various training intensities on muscle growth.\nA Tale of Two Fibers\nWithout going into too much detail, skeletal muscle is mainly composed of two fiber-types, the slow-twitch type I and fast-twitch type II fibers. As their “slow-twitch” name alludes, the type I fibers are often smaller, weaker, and have lower power outputs than their type II relatives but are highly fatigue-resistant (9,10). In the hypertrophy world, it is often thought that type II fibers have superior growth potential to type I fibers (11), and that training programs should target type II fibers to maximize muscle growth by utilizing high training intensities (12). This assumption is not necessarily invalid, and work from Fry (11) has shown that when the results of multiple training studies are pooled, type II fibers respond well to high intensity (>50%-1RM) training, and to a greater extent than the type I fibers. At lower training intensities (<50%-1RM), type I fibers may grow more than type IIs, but the peak growth rates are still lower than at high training intensities. Unfortunately, the majority of training studies utilize higher training intensities (>50%-1RM) (11,13), so it’s hard to conclusively say that type II fibers always outgrow the type I fibers, or that this response is specific to higher training intensities. In fact, recent evidence has shown that low-intensity training can stimulate muscle protein synthesis (14) and produce comparable whole-muscle hypertrophy to high intensity training when sets are taken to failure (15,16). Even more interesting, in this dataset the type I fiber growth rate with low-intensity training rivaled that of the type II fibers with high-intensity (15), suggesting that we may have misjudged the ability of our type I fibers to grow. The fact that bodybuilders also have larger type I than II fibers (11), and powerlifters and olympic lifters have larger type II than I fibers, adds additional, albeit indirect support, to this recent data on type I fiber and whole muscle hypertrophy when training to failure regardless of load (15,16).\nSo how does this impact your training? If you’re focused on packing on more muscle mass, you need to ensure you provide a training stimulus to promote the growth of both type I and II fibers. Focusing solely on load (high intensity) may be great for type II fiber growth but may leave your type I fibers under-stimulated and craving some time under tension. On the opposite end of the spectrum, focusing on fatigue and time under tension with light loads, which can possibly still hit your type II fibers (17), may not be optimal for type II fiber growth (18). Since the scientific literature on the topic is still undecided (19) you’ll need to take an approach that maximizes type II fiber recruitment (high training intensity, high effort, and/or high fatigue/failure) and satisfies the type I fibers. Not a small order!\nWhat Does This Have to do With Dropsets?\nUltimately our understanding of muscle growth and fiber-type specific hypertrophy is not at the level where we should be making black-and-white statements to training exclusively at high or lower intensities to maximize mass. There’s a multitude of ways to address the needs of your type I and II fibers across a strength training program, but the dropset allows us to get the best of both worlds in a single, but painful, set.\nWhile there are many ways to structure a dropset, the traditional dropset allows us to maximize both load and time under tension. In the first portion of the set, load can be emphasized, even as high as a 3-5RM, to demand high levels of muscle activation so you’ll have to recruit your full complement of motor units and tap into your fast-twitch fibers. As you work through each subsequent drop in weight, you’ll be providing greater time under tension than if you stopped after one set, giving the type I fibers the stimulus they need to get growing. If things couldn’t get any better, the increased fatigue will act as a safe-guard to ensure you really hit those type II fibers, as fatigue can actually make it easier to recruit your type II fibers at a weight where their use wouldn’t be normally be required (17).\nMaximizing muscle growth is a balancing act of training variables, but simple techniques used judiciously in a well thought out training program, consistently executed is the path to pounds of extra muscle. While the research hasn’t given us conclusive insight into the use of dropsets for hypertrophy training, based on our understanding of muscle physiology it’s likely that dropsets are still a viable and valuable component of your training toolbox.\nNeed Help Putting Together a Flexible Training Program\nIf you want personalized help with us programming a flexible and individualized training program, you can work directly with us. Click HERE to set up your FREE phone consultation.\n- Goto K, Nagasawa M, Yanagisawa O, Kizuka T, Ishii N, Takamatsu K. Muscular adaptations to combinations of high- and low-intensity resistance exercises. J Strength Cond Res. 2004 Nov;18(4):730–7.\n- Goto K, Sato K, Takamatsu K. A single set of low intensity resistance exercise immediately following high intensity resistance exercise stimulates growth hormone secretion in men. J Sports Med Phys Fitness. 2003 Jun;43(2):243–9.\n- West DWD, Phillips SM. Associations of exercise-induced hormone profiles and gains in strength and hypertrophy in a large cohort after weight training. Eur J Appl Physiol. 2012 Jul;112(7):2693–702.\n- Schroeder ET, Villanueva M, West DDW, Phillips SM. Are Acute Post–Resistance Exercise Increases in Testosterone, Growth Hormone, and IGF-1 Necessary to Stimulate Skeletal Muscle Anabolism and Hypertrophy? Med Sci Sports Exerc. 2013 Nov;45(11):2044–51.\n- West DWD, Burd NA, Tang JE, Moore DR, Staples AW, Holwerda AM, et al. Elevations in ostensibly anabolic hormones with resistance exercise enhance neither training-induced muscle hypertrophy nor strength of the elbow flexors. J Appl Physiol. 2010 Jan;108(1):60–7.\n- Mitchell CJ, Churchward-Venne TA, Bellamy L, Parise G, Baker SK, Phillips SM. Muscular and Systemic Correlates of Resistance Training-Induced Muscle Hypertrophy. PLoS ONE. 2013;8(10):e78636.\n- West DWD, Kujbida GW, Moore DR, Atherton P, Burd NA, Padzik JP, et al. Resistance exercise-induced increases in putative anabolic hormones do not enhance muscle protein synthesis or intracellular signalling in young men. J Physiol (Lond). 2009 Nov 1;587(Pt 21):5239–47.\n- Wilkinson SB, Tarnopolsky MA, Grant EJ, Correia CE, Phillips SM. Hypertrophy with unilateral resistance exercise occurs without increases in endogenous anabolic hormone concentration. Eur J Appl Physiol. 2006 Dec;98(6):546–55.\n- Burke RE, Levine DN, Zajac FE. Mammalian motor units: physiological-histochemical correlation in three types in cat gastrocnemius. Science. 1971 Nov 12;174(4010):709–12.\n- Burke RE, Levine DN, Tsairis P, Zajac FE. Physiological types and histochemical profiles in motor units of the cat gastrocnemius. J Physiol (Lond). 1973 Nov;234(3):723–48.\n- Fry AC. The role of resistance exercise intensity on muscle fibre adaptations. Sports Med. 2004;34(10):663–79.\n- Thibaudeau C. High Threshold Muscle Building. F. Lepine Publishing; 2007.\n- Wernbom M, Augustsson J, Thomeé R. The influence of frequency, intensity, volume and mode of strength training on whole muscle cross-sectional area in humans. Sports Med. 2007;37(3):225–64.\n- Burd NA, West DWD, Staples AW, Atherton PJ, Baker JM, Moore DR, et al. Low-load high volume resistance exercise stimulates muscle protein synthesis more than high-load low volume resistance exercise in young men. PLoS ONE. 2010;5(8):e12033.\n- Mitchell CJ, Churchward-Venne TA, West DWD, Burd NA, Breen L, Baker SK, et al. Resistance exercise load does not determine training-mediated hypertrophic gains in young men. J Appl Physiol. 2012 Jul;113(1):71–7.\n- Ogasawara R, Loenneke JP, Thiebaud RS, Abe T. Low-load bench press training to fatigue results in muscle hypertrophy similar to high-load bench press training. International Journal of Clinical Medicine. 2013 Feb;4:114–21.\n- De Luca CJ, LeFever RS, McCue MP, Xenakis AP. Behaviour of human motor units in different muscles during linearly varying contractions. J Physiol (Lond). 1982 Aug;329:113–28.\n- Campos GER, Luecke TJ, Wendeln HK, Toma K, Hagerman FC, Murray TF, et al. Muscular adaptations in response to three different resistance-training regimens: specificity of repetition maximum training zones. Eur J Appl Physiol. 2002 Nov;88(1-2):50–60.\n- Schoenfeld BJ. Is There a Minimum Intensity Threshold for Resistance Training-Induced Hypertrophic Adaptations? Sports Med. 2013 Aug 19.\nAbout Dan Ogborn:\nDan Ogborn PhD, CSCS. Dan recently completed his PhD in medical sciences specializing in the molecular adaptation of skeletal muscle to strength training. He has previously completed a MSc (kinesiology) and has been involved in the fitness industry in various capacities for the past 14 years. He is currently a post-dorctoral fellow at McMaster University and is completing a degree in physiotherapy.\nVisit his website\nFollow Dan on Twitter"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:a6fa8284-4981-404d-aa7a-b081cd48e6a4>"],"error":null}
{"question":"How do hypotrophic scars differ from keloids in terms of tissue formation and appearance?","answer":"Hypotrophic scars and keloids represent opposite extremes in tissue formation during healing. Hypotrophic (atrophic) scars are characterized by deficient formation of connective tissue, resulting in sunken scars that are deeper than the surrounding skin, appearing flat and in the form of small dimples. The wound heals poorly because there isn't enough formation of new connective tissue fibers. In contrast, keloids are caused by overproduction of connective tissue, where fibroblasts continue producing collagen even after the wound is healed. Keloids appear as thick, curved, often very red growths that extend beyond the original injury area into healthy skin tissue. The collagen in keloids is deposited randomly and can become so tightly packed that it causes pain and itching. They are particularly common in people of color and may run in families.","context":["A scar is a fibre-rich replacement tissue formed by connective tissue cells during wound healing. Scarring is a natural process that occurs when injured tissue heals. A scar occurs when not only the top layer of skin (epidermis) but also deeper layers of skin are injured. Scars are final but not immutable. Just as different as the types of scars are their healing and care processes.\nHow is a scar created?\nScar formation is the final step in wound healing, which is divided into three stages: Inflammation, repair and reconstruction phase. External scars usually develop after injuries to deeper layers of the skin. A small cut or superficial laceration often only injures the top layer of skin (epidermis). In this case, starting from the lowest, so-called basal cell layer of the epidermis, a new, intact skin layer forms, which closes the resulting wound.\nWhat types of scars are there?\nDepending on the type of injury, the condition of the wound edges, wound hygiene, but also on age and genetic factors, different visible scars can remain after an injury. A distinction is made between the following types of grain:\n- Hypertrophic scars\n- Keloid scars or proliferating scars\n- Hypotrophic (atrophic) / sunken scars\n- Sclerotic scars\n- Unstable scars\nHypotrophic scars (atrophic)\nHypotrophic scars are caused by the deficient formation of connective tissue. The term “atrophy” denotes a regression. In atrophic scars, the wound closes after healing, but the skin does not form enough new connective tissue to fill the wound. This creates sunken scars, the bottom of which is deeper than the surrounding skin. These scars are therefore usually found flat and in the form of small dimples. The wound tends to heal poorly because the formation of new connective tissue fibres is not sufficient. The sunken scar is therefore below the level of the skin, which is especially the case with acne scars. Stretch marks in bodybuilders or stretch marks in pregnancy are also atrophic scars.\nHypertrophic scars are raised scars which, in contrast to hypotrophic scars, are caused by an excessive formation of connective tissue. They are often the result of an injury that was stressed too early or an ongoing infection. Hypertrophic scars thus often develop when the wound has to close against strong tensile forces during healing and is thus exposed to constant movement or stress. The constant tension causes more blood vessels and connective tissue to form, which often makes hypertrophic scars appear very bulging and very red. They can also be itchy and sometimes painful. However, they remain limited to the original injury area, but protrude above the skin level. The scar growths develop directly within the first few weeks after the injury and not at a later time.\nKeloid scars or keloids are proliferating scars that form some time after wound closure due to an overproduction of connective tissue. Keloids are thick, curved, often very red, appear darker than the surrounding tissue and extend beyond the original area of injury. They are therefore often called “wild flesh” because they are not limited to the area of the wound, but often extend tumour-like to the neighbouring, healthy skin tissue. This also often causes itching. Especially young women and people with dark skin are affected by keloids. They only occur in pigmented skin and often develop on shoulders or earlobes. Burn wounds also often form keloids.\nSclerotic scars form after extensive injuries such as burns. They arise from the formation of new hard and dry tissue. During healing, the tissue contracts strongly, which is why the skin becomes hard and inelastic. Sclerotic scars also tend to shrink. If they lie over joints, they can cause contractures or immobilisation, which is why surgical correction is often advisable.\nUnstable scars are usually the result of poorly functioning wound healing. It often affects areas of the body where the skin is exposed to frequent tension. This includes, for example, the skin on the joints. Typical symptoms of unstable scars are, among other things, ulcers and tears that form again and again. It is imperative that this type of scar is treated professionally, because there is a risk that a malignant carcinoma will develop.\nWhat should absolutely be avoided in order for a scar to heal well?\nWound and scar healing depends on a wide variety of factors and can therefore vary from person to person. In order for a scar to heal well, contamination of the wound area, tension and irritation caused by tight-fitting clothing, and re-injury should be avoided. In addition, care should initially be taken to ensure that the scar is not exposed to strong temperature stimuli or extreme sunlight.\nWhen are the sauna, steam bath and swimming pool allowed again?\nAs a general rule, you should not go into the sauna, steam bath or swimming pool again until the wound has completely healed. In the case of scars after surgical procedures, this means that there are no more foreign bodies such as sutures or staples in the scar and that it has healed completely smoothly and free of crusts. If this is not the case, you run the risk of catching a serious infection.\nWhen can the scar be exposed to direct UV radiation or the solarium?\nSince the scar tissue has no pigment-forming cells, it cannot protect itself from UV rays by tanning. Scar tissue stands out from tanned skin in a radiant white and suffers from UV damage in the process. Therefore, direct sunlight as well as visits to sunbeds should be avoided for six months to a year after the wound has healed and a high sun protection with at least SPF 50 should also be applied to the scar afterwards.\nIn order to support the healing process of a scar, it is recommended to care for the scar properly, depending on its type and characteristics. This is because scar care can help the scar become paler, smoother and also flatter by stimulating blood circulation. Even older scars can still respond to scar care, provided the right product is used. Thus, proper scar care can prevent pathological scars from forming, such as hypertrophic scars or keloids. Unfortunately, scar care takes a lot of patience and is not done in just a few days. For special care products to work, they should normally be used for several months.\nWhen should scar treatment be started?\nIn order for scar care to have the best effect after an operation or injury, it should be started as early as possible. In the early stages, the scar is still in the process of remodelling and therefore more receptive to treatment. As soon as the wound is closed or the doctor has removed the stitches, you can start caring for the fresh scar. However, this should always be discussed with the attending physician beforehand.\nHow does scar care work?\nA wound that is not yet completely closed is best treated with healing ointments to support wound healing. Once the wound is completely closed, special products can be used for scar care. For example, silicone-containing gels and plasters are suitable for this. In addition, it is partly possible to support the process of scar formation and healing with massages. The scar should be stressed as little as possible. Regular massages and care with ointments and oils can prevent dehydration, redness and growths.\nHow long should the scar be tended for?\nBasically, the earlier you start to care for the scar, the better. Fresh scars should be cared for at least eight weeks, older scars even up to six months. The healing time of an injury and the duration of care for the resulting scar depend on the extent and severity of the injury. If only the epidermis is injured, the skin can renew itself on its own within 28 days. In most cases, no scar remains and the duration of care is correspondingly shorter.\nWhich products can be used?\nAs soon as the wound edges are completely closed and the stitches of surgical scars are removed, the scar should be applied with special creams and gels once or twice a day. Products with active ingredients such as heparin, dexpanthenol and allantoin, as well as silicone-based products, are particularly effective. They help reduce redness, soften scar tissue, relieve pain and itching, reduce inflammation and moisturise. In addition, when used correctly, you can reduce/prevent the growth of excess connective tissue of the scar. All products have moisture and fat as ingredients in common – this is the most important thing for the scar – regardless of manufacturer or marketing.\nOintments consist of a single-phase base, do not contain water and normally do not need to be preserved. Cream, on the other hand, is a multi-phase system that always contains emulsifiers and must be preserved. There are both lipophilic (fat-loving) and hydrophilic (water-loving) creams. Gels consist of a liquid phase that is thickened with a gelling agent. This creates a three-dimensional gel structure. Both should be applied to the affected skin area over several weeks, at least twice a day, and massaged in gently. Scar gels are suitable for both fresh and older scars. The attending physician can best explain which scar care is most suitable for the individual situation.\nWhat is a scar plaster?\nScar plasters are made of a special material that is breathable and permeable to water vapour. This creates a skin climate in the scar area that stimulates the metabolic processes and promotes cell formation.\nIn addition, scar plasters exert a slight pressure on the wound, which helps keep the tissue under the plaster supple. This makes the scar flatter, lighter and more elastic. Scar plasters thus protect against excessive scarring and are therefore popular for treating hypertrophic (bulging) scars. They can also lighten older scars and reduce excess scarring to a certain extent.\nThey should only be applied after wound healing is complete and are stuck directly over the scar. To achieve their full effect, the patches should remain on the scar for at least twelve hours. Treatment with scar plasters usually lasts two to three months. As they do not contain any active pharmaceutical ingredients, there are usually no side effects and they are therefore also very suitable for people with allergies, pregnant women as well as children and people with sensitive skin. However, caution is advised with allergies to silicone.\nCan home remedies for scar care also be used?\nHerbal skin oils are also suitable for scar care, as they have valuable ingredients that can have a positive effect on the healing process of the scar. Above all, vitamin E and vitamins of the A group are contained in almost all natural oils. They support the skin in regeneration and increase its elasticity. St. John’s wort, grape seed, marigold, almond or jojoba oil are particularly good for scar care. Such oils stimulate blood circulation and make the scar fade. In addition, they keep the skin elastic and supple and partly also have an anti-inflammatory effect. When using vegetable oils, however, it is also important to avoid direct sunlight, as otherwise unsightly discoloration can occur in the scar area.\nHow can scars be corrected?\nThere are many ways to correct scars. In many cases, the treating doctor uses several procedures to correct the scar. In addition to natural remedies and medical methods for scar care and correction, surgical procedures can also help to correct or even completely remove the scar. The doctor must always decide individually which method is best suited for the respective scar. Surgical correction is necessary for some more severe scars. Other scars can be corrected with laser treatment, chemical peeling or hyaluronic acid/platelet-rich plasma (PRP). For more information about the correction of scars, we recommend that you take a look at our page in the Gynaesthetic section.","Keloids are a very common type of scar that can be unsightly, painful and itchy. Anyone can develop keloids, but people of color develop them 15 times more frequently than others. Keloids may also run in families. They commonly occur on the ears but are also seen on the upper chest, back and arms. These scars are often a result of an injury or acne scaring, and can be very difficult to treat.\nWhat causes keloids?\nThroughout evolution, our body has developed an exquisite system to repair injured skin. If you think about it, our skin is all that protects our tissues and organs from injuries, and our immune systems from bacteria and infections. As a result, any time the integrity of the skin is interrupted, a rapid and elegant system springs into action to repair the defect (hole/cut/injury).\nSpecial cells called fibroblasts start producing collagen and other substances to fill in the defect. This process essentially “patches the hole.”\nUnfortunately, for some people, the fibroblasts don’t get the message that their work is done, so they keep making more and more collagen. This excess collagen turns into a scar that can become very large, and may even spread to normal skin outside of the injury area, making it very unsightly. The collagen can also get packed so tightly that it presses on nerves, causing extreme pain and itching.\nSome patients mistakenly think that the keloid formation (in a surgical scar) is the result of something the doctor did wrong, but a keloid is really a result of a person’s genetics and their body’s healing response.\nHow are keloids diagnosed?\nKeloids can be diagnosed by a physician or dermatologist. They occur in areas of injury or from acne scars on the chest, shoulders, upper arms and back. Scars called hypertrophic scars can be confused with keloids, but they generally stay at the site of the injury, and their collagen is parallel to the surface of the skin.\nKeloids, on the other hand, spread to normal skin around the injury and the collagen is deposited in a very random manner. Thus, a biopsy may be required for the precise diagnosis.\nCan keloids be prevented?\nIf you have a history of keloids, take careful measures to avoid getting injured or cut. Also be sure to have a plan with your dermatologist to treat any cuts or injuries right away to prevent them from turning into large, unsightly, painful keloids.\nHow are keloids treated?\nIt is important to realize that a scar can never be changed back into normal skin. With the correct treatment, however, keloids can be minimized and managed. When I treat keloids, my goal is to reduce the symptoms of pain and itching, flatten the scar, and even out the dark skin color.\nThe treatment of keloids involves two processes: 1) Stop the fibroblasts from producing keloid-causing collagen and other substances; and 2) Help enzymes in the skin break down the tightly packed collagen in the keloid to make it softer and flatter.\nI use a combination of anti-inflammatory injections, topical silicone preparations, pressure ear-rings, laser treatments and, if needed, surgical excision to effectively manage and treat keloids. I design a specific program for every patient that will serve them best.\nAction steps for anyone with unwanted keloids\nIf you have a keloid, you do not have to suffer through the pain and itching. Although difficult to treat, keloids can be managed effectively by an experienced physician or dermatologist.\nIt is very easy to surgically remove keloids, but it is hard to keep them from coming back. Keloids on the ears, however, are an exception — they are the only area on the body that is not a certain predictor of developing keloids elsewhere.\nA proper treatment program for keloids means sustained, repeated visits to your doctor for several treatments over six or more months. Without this sustained approach, success in treating keloids will be minimal at best. With your doctor, it is important to develop a plan and stick to it.\nAs my grandfather always said, “Plan your work and work your plan.”\nCharles E. Crutchfield III, MD is a board-certified dermatologist and Clinical Professor of Dermatology at the University of Minnesota Medical School. He also has a private practice in Eagan, MN. He has been selected as one of the top 10 dermatologists in the U.S. by Black Enterprise magazine and one of the top 21 African American physicians in the U.S. by the Atlanta Post. Dr. Crutchfield is an active member of the Minnesota Association of Black Physicians, MABP.org."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:6bd91de3-bebf-497c-81e2-17bd6c24cae3>","<urn:uuid:6e3fe3b5-67bb-4114-a9f6-796a996dc663>"],"error":null}
{"question":"I need to know when to replace my bicycle chain. What are the recommended replacement intervals, and what methods can I use to check if my chain needs replacing?","answer":"As a general rule, you should replace your bike chain every 1,000 miles (1,600 kilometers) or after 2,000-3,000 miles of riding. To check if replacement is needed, you can use a chain checker tool, or measure the distance between chain pins with a tape measure - if the distance between two pins measures more than 12 1/8 inches (original length should be exactly 12 inches), it's time for replacement. Other signs that indicate need for replacement include noise during riding, feeling of looseness, skipping, or if more than half of the chain links are worn when checked with a chain checker tool.","context":["If you're looking for a new bike chain, you're probably wondering how much it's going to cost. The good news is that bike chains are relatively inexpensive, and you can usually find one to fit your budget. But how much does a new bike chain cost? Here's a look at some of the factors that will influence the price.\n1. How much does a new bike chain cost?\nThe cost of a new bike chain will vary depending on the type of bike and the quality of the chain. Generally, a new bike chain will cost between $20 and $100.\n2. How often should I replace my bike chain?\nThere is no definitive answer to this question as it depends on a number of factors, including how often you ride your bike, what type of terrain you ride on, and how well you maintain your bike. However, as a general rule of thumb, you should replace your bike chain every 1,000 miles (1,600 kilometers).\n3. How do I know when my bike chain needs to be replaced?\nThe best way to know if your bike chain needs to be replaced is to check it regularly for wear and tear. If you see any links that are bent, broken, or excessively worn, then it's time for a new chain.\n4. What are the signs that my bike chain needs to be replaced?\nIf your bike chain is making noise, feels loose, or is skipping, then it is time to replace it.\n5. How can I tell if my bike chain is worn out?\nIf your bike chain is worn out, it will likely feel loose and may make noise when you ride. You can also check the wear on your chain by measuring the distance between the links. If the links are more than 1/16 of an inch apart, your chain is probably too worn to use.\n6. How do I measure my bike chain to see if it needs to be replaced?\nThe best way to measure your bike chain is to use a chain checker tool. This tool will tell you how many links are in your chain and how many are worn. If your chain has more than half of its links worn, it needs to be replaced.\n7. What are the consequences of riding with a worn out bike chain?\nIf you ride with a worn out bike chain, the consequences can be catastrophic. The chain could break while you are riding, causing you to lose control and possibly crash. Even if the chain doesn't break, it can cause your bike to malfunction and make it difficult to ride.\n8. How do I replace my bike chain?\nTo replace your bike chain, you'll need a chain tool, a new chain, and a little bit of patience. First, use the chain tool to remove the old chain. Next, thread the new chain through the derailleurs and around the sprockets. Finally, use the chain tool to connect the two ends of the chain.\n9. What tools do I need to replace my bike chain?\nIn order to replace your bike chain, you will need a chain breaker tool and a new chain. You may also need a chain whip if your bike has a cassette.\n10. How long does it take to replace a bike chain?\nIt usually takes around 15-20 minutes to replace a bike chain.\n11. Do I need to take my bike to a shop to have the chain replaced?\nYou can replace your bike chain at home with a few simple tools. You'll need a chain breaker and a new chain. Plus, you'll need a few rags to clean up the old chain grease.\n12. Can I replace my bike chain myself?\nYes, you can replace your bike chain yourself. You will need a few tools, including a chain tool, to remove the old chain and install the new one. The process is not difficult, but it is important to follow the instructions carefully to avoid damaging your bike.\n13. What are the steps to replacing a bike chain?\n1. Unhook the old chain from the front derailleur. You may need a chain tool to push the pin out of the old chain. 2. Unhook the old chain from the rear derailleur. 3. Take off the old chain. 4. Feed the new chain through the front derailleur. 5. Feed the new chain around the rear sprocket. 6. Join the two ends of the chain using a chain tool. 7. Adjust the chain tension using the rear derailleur.\n14. What are some tips for replacing a bike chain?\n1. Get a new chain that is the same length as your old one. 2. Unhook the old chain from the derailleur and the cogset. 3. Remove the old chain from the bicycle. 4. Install the new chain on the bicycle. 5. Hook the new chain onto the derailleur. 6. Hook the new chain onto the cogset. 7. Adjust the derailleur so that the new chain is tight. 8. Ride your bicycle and enjoy the new chain!\n15. How can I make my bike chain last longer?\nThere are a few things you can do to make your bike chain last longer: 1. Keep your chain clean and well-lubricated. This will help reduce friction and wear. 2. Don't shift gears while pedaling. This puts unnecessary strain on the chain. 3. Replace your chain regularly. Depending on how often you ride, a new chain may be needed every few months to a year.\n16. What are some common causes of worn out bike chains?\nThe most common cause of a worn out bike chain is simply wear and tear. Over time, the chain will begin to stretch and the teeth on the sprockets will begin to wear down, causing the chain to skip and jump. This can be exacerbated by riding in wet or dirty conditions, as well as by shifting poorly. Another common cause of chain wear is misalignment, which can be caused by a bent derailleur hanger or by an incorrect installation.\nThe cost of a new bike chain will vary depending on the type and size of the chain. A basic chain will start at around $15, while a more specialized chain can cost up to $100.","Your frame and wheels might get all the attention on the weekly group ride, but the true hero of your bike is the chain. Here’s all you need to know to keep it running smoothly, plus five reasons you should keep it in top shape.\n1. It’ll Make You Faster\nIf you’re getting edged at the town-limits sign on your weekly group ride or coming up just short at the finish of your local crit, don’t reach for the EPO. There’s a cheaper, more ethical solution.\nA clean, properly lubed chain will save about 10 watts over a poorly maintained chain, according to Jason Smith of Friction Facts, a Colorado-based research firm. For the average rider, that accounts for about 4 percent of lost power. If you’re only racing to the midride coffee stop, think of it this way: You’ll finish the ride fresher, having exerted less energy, if you lube the chain.\n2. You’ll Save Money\nA clean chain won’t just prevent you from wasting energy. It’ll also save you from wasting money. Dirty chains can damage drivetrain components as the dirt and grime slowly wear away cogs and chainrings.\n“Would you rather pay $30 for a new chain now or $400 for a new drivetrain later?” asks Eric Fostvedt, mechanic for the professional Axeon Cycling Team.\n3. It’s Easy\nEvery mechanic and rider has his or her favorite way to clean a chain. Park Tool master mechanic Calvin Jones uses, of course, Park Tool’s Chain Gang filled with degreaser.\nIf you don’t want to buy a special piece of equipment, try using a thick-bristled paintbrush to coat the chain and cogs with degreaser. Richard Belson, a mechanics instructor at the United Bicycle Institute, suggests dousing the chain in a self-cleaning lube, such as White Lightning Clean Ride. Just be sure to wipe off all the excess lubricant with a rag afterward. Too much lube can be a bad thing, especially when it’s picking up road grime.\n4. You Don’t Have to Do It (That) Often\nMost experts agree you should replace your chain after 2,000 to 3,000 miles of riding. The easiest way to see if you need a replacement: Use a chain checker, such as this one from Park Tool.\nBarring that, break out your tape measure. On a new chain, you should be able to measure exactly 12 inches between two chain pins. If the distance measures as little as an eighth of an inch more than that, replace the chain. You’ll be amazed at how much smoother your pedal stroke will get.\n5. You Can Get Creative with Lube\nPro tip: Some of the most effective bike-chain lubricants aren’t actually bike-chain lubricants. Plain old food-grade paraffin canning wax is a great way to keep your chain rolling fast and smooth, but it requires taking the chain off your bike and dipping it into the melted wax. The effect lasts quite a while, but few riders want to deal with the hassle.\nAnother excellent but unorthodox product is olive oil, says Smith. On the downside, it’ll last only about 50 miles or so. If you’re riding a century, be sure to save some of your salad dressing from the lunch stop for the ride home.\nIf a road rider wants a fast, simple liquid lube for clean conditions, Smith recommends Squirt, Rock ’n’ Roll Gold, or Silca NFS Pro. For mountain biking and cyclocross riding in wet or muddy conditions, try a thicker lube, such as Finish Line Wet, White Lightning Wet, or Phil Wood Tenacious Oil. Smith suggests keeping three to four different types of lube in your tool kit so you’ll be ready for nearly every weather and road condition you encounter.\nSubscribe to Outside\nSave 72% and Get the Special Women's Issue!"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:3186f64e-5e0f-4a42-8d11-9284eb03f67b>","<urn:uuid:0c87edc1-90ac-4398-848f-328e1d212694>"],"error":null}
{"question":"How do the selection processes at the African Human Rights Moot Competition and the Mill Valley Film Festival compare in terms of identifying top talent?","answer":"The African Human Rights Moot Competition and Mill Valley Film Festival employ different selection approaches to identify excellence in their respective fields. The Moot Competition uses a hierarchical selection process where law students must first compete within their universities before the winners can represent their schools and countries at the prestigious pan-African level. Meanwhile, the Mill Valley Film Festival curates its selection by focusing on high-quality independent and world cinema, attracting established talent including renowned filmmakers and actors like Robin Williams, Helen Mirren, and Bradley Cooper. While the Moot Competition builds talent through academic competition and legal expertise, MVFF identifies talent through artistic merit and cultural impact, welcoming over 200 filmmakers annually and helping to launch new films and create awards season buzz.","context":["Shameela Seedat, the director of the South African-Finnish co-production African Moot, visited Helsinki during the Love & Anarchy festival in September. Academy of Moving People & Images and Art School Maa hosted a TransBorder Conversation with her during the visit. Finnish Film Foundation has supported both the film and the TransBorder Conversations project.\nText: Sharron Todd\nPhoto: Maria Rannanheimo\n“Growing up, I was particularly inspired by family members who’d actively joined the political struggle and were engaged in some form of resistance to the system of racial inequality in our country at the time. Siblings of both my father and mother had been imprisoned, banned, or tortured by the Apartheid government,” says filmmaker, Shameela Seedat, about a long dark period in South African history that only formerly ended in the 1990s. “In some ways, this has inspired my filmmaking in contemporary times, as I am drawn to characters who are ready to speak and ready to act, while all showing their human vulnerabilities.”1\nApartheid translates to “apartness” or “separateness” in the language of Afrikaans. It was a system of legislation that upheld segregationist policies against non-white citizens in South Africa, a group that represents the majority of the country. This legislation was constructed by an all-white government and any resistance to it was met with inhumane physical and psychological treatment by authorities, sometimes ending in death.\nAcross the pond in the early 1950s, as the U.S. began a shift toward ending the nearly identical policies against black Americans, known as the Jim Crow Laws, Apartheid was just beginning. Under this divisive and oppressive legislation, non-white citizens were immediately forced to live separately from their white counterparts and were also forced to use separate public facilities. Interracial marriage was made illegal. Citizens were assigned hierarchical identities attached to their race, with the darkest among them positioned as the most disadvantaged.\nAs a teenager, Shameela found it difficult to not have a political mindset. She was surrounded by people who sought out compassion in law and fought for the equal treatment of the disenfranchised. She joined some of the mass protest activities in the 1980s.\n“When I was about 10 years old, my parents took me to see a play in my neighborhood about Steve Biko – a black consciousness intellectual and anti-Apartheid activist, who had been brutally killed in police detention at the young age of 29. This struck a very deep chord in me,” she explains.\nThis type of conviction runs deep in her family. Her aunts and uncles were known activists, with one such aunt being Fatima Meer, one of the most prominent women political leaders in South African history. Despite repeated imprisonment and assassination attempts on both her and her husband’s lives, Fatima refused to be silenced. Among a long history of notable contributions to fighting racial and social inequalities in South Africa, she aligned herself with the Black Consciousness Movement in the 1970s (which was led by Steve Biko) and also co-founded the Black Women’s Federation with Winnie Mandela. She also wrote the first authorized biography of Nelson Mandela, Higher Than Hope (1988).\nThe Population Registration Act of 1950 provided an underpinning for Apartheid legislation by classifying South Africans by their race: Bantu (black Africans), colored (mixed race), and white. A fourth category was later added to classify Indians and Pakistanis. Shameela’s grandparents immigrated to South Africa in the 1880s from India and her grandfather later started a small newspaper for people of Indian descent living on the east coast of Africa. During her childhood, she was forced to live in neighborhoods and attend schools restricted to only Indians.\nWhen asked how she identified under these conditions, she replied, “When I was younger, I did not want to engage with my Indian heritage too much, but rather opted to assert a ‘black’ identity in solidarity with all oppressed people in South Africa, and also to present a united force against white domination. This was not unusual for many people in my generation of Indian descent, mainly because a cornerstone of Apartheid was to rigidly and brutally fix racial and ethnic identities onto us, and then to place oppressed people in hierarchies in a typical ‘divide and rule’ strategy.”\nColorism was not only a byproduct of this legislation, but it was the backbone of its founding. Shameela came from a middle-class Indian family that experienced racial hardships and political backfire from the Apartheid regime, but the experiences of black Africans were significantly cruel and inequitable in comparison. Shameela says, “I must point out that we were ultimately so much better off than many black people in our country who during Apartheid, lost lives, homes, and also basic educational opportunities that made it incapable of entering into the mainstream of our society.”\nOne of the objectives of this systemic strategy was to create division among non-white citizens by weaponizing race and color while attaching hierarchy to both. However, this attempt to divide only produced the opposite effect on Shameela and her family as it became one of their major sources of motivation to actively dismantle Apartheid ideologies and systems.\nCombining a passion for law and filmmaking\nIt is not surprising that a lineage of human rights advocacy, and more poignantly, an almost inherited compassion for the human condition, is reflected in the films Shameela makes. Her first feature-length documentary, Whispering Truth to Power, follows South African anti-corruption pioneer Thuli Madonsela as she tackles political giants in pursuit of justice and social equity.\nShameela’s latest film is African Moot, a feature documentary that was co-produced in South Africa and Finland. The film premiered in May 2022 and has been making its way through the film festival circuit- winning Best African Documentary at the Zimbabwe International Film Festival and recently nominated for Best Documentary at the African Movie Academy Awards (October 2022). Much like her first film, African Moot is centered around human rights laws and the stories of those who endeavor to eradicate long-endured social and political injustices.\nBefore becoming a filmmaker, Shameela spent many years as a human rights lawyer. She studied law at the University of Cape Town and earned her master’s degree in constitutional law at Columbia University in New York City. She worked in law research advocacy roles at the constitutional court of South Africa, UNIFEM in New York, and a Cape Town-based democracy think tank, IDASA. After more than 12 years, Shameela decided to pursue a passion that had laid dormant for quite some time. Buried underneath legal paperwork and years of activism was a love for storytelling.\n“I always knew I wanted to make films. I was just waiting for the story that meant something to me and that I could throw myself into for long periods of time,” she says. “When Apartheid ended and South Africa’s democratic era began, it seemed logical for me to study and work in law – but over time I realized that it was really in documentary filmmaking where my big passion really lies. Hence, law-themed documentaries!”\nShameela’s background in law created a symbiotic pairing with her filmmaking. She was able to develop meaningful relationships with the characters in her film African Moot as she could empathize with them as law students. The term “moot” is defined as a simulated court case. Moot competitions have been held in different capital cities across Africa every year since 1992 and boast the largest pan-African gathering of law students and human rights practitioners on the continent. Top law students must first compete against each other within their universities. Then only the winners are selected to represent their schools and countries at the prestigious African Human Rights Moot Competition. In this film, the competition is being held in Gaborone, Botswana, and students are challenged with confronting this year’s complex subject: “The Right of Refugees on the Continent.”\nThe film follows teams from Kenya, Egypt, Uganda, and South Africa. These young aspiring lawyers diligently prepare to act as both prosecution and defense in fictional human rights cases. Each student is determined to win and bring the coveted trophy home. There is a cadence of tension throughout the film as we’re given a glimpse into the brilliant minds of those who are committed to shaping the future of Africa. We see them through moments of frustration and triumph as they navigate through mammoth tasks and legal texts hoping to strengthen their arguments. As we watch students wrestle with the complexities of migration facing their own countries, we also witness a story of friendship, conflict, hope, and self-evolution.\nThe film also follows some students on their intimate journeys of acceptance and identity in a still-conservative-leaning African culture. Edward is the gay son of Ugandan refugees, and he struggles to find his footing in a space where he is not fully welcomed. He carries this burden as he competes in the same trenches as other students at the moot. Edward’s story and those of other students are woven into the film’s larger commentary on refugee rights. These moments throughout the film give us welcomed visibility into the personal struggles and wins of the characters.\nThough African Moot is centered around a high-stakes law competition, it is a human story that captures the vulnerability and strengths of a young African generation. Now more than ever, it is critical that we hear the voices of this particular group as they will control the future trajectory of Africa’s internal progress and its global positioning. As the world quickly evolves into a technology-driven ecosystem faced with sociopolitical complexities that are largely foreign to older generations, impactful change will rely on the unique perspectives and experiences of emerging generations such as the characters in this film.\nCo-producing with Finland\nAfrican Moot was one of 25 films that were selected for the Generation Africa project. This pan-African project was created by STEPS, a South African non-profit media company that connects filmmakers, producers, broadcasters, and funders that are all driven by social change through documentary film. Generation Africa is the fourth collection of films produced by STEPS and focuses on migration through the lens of African youth and their lived experiences.\nAmong the 180 submitted films, African Moot was selected and Shameela and her team were connected to an international community of filmmakers, co-producers, and broadcasters. As a result, she and her partner at Undercurrent Films and Television, Francois Verster, were connected to Juha Löppönen from Finland-based Tuffi Films, and shortly after, they forged a co-production relationship. They garnered support from the Finnish Film Foundation and AVEK (The Promotion Centre for Audiovisual Culture), which played a pivotal role in both funding and sourcing Finnish co-production talent and opportunities.\nShe says, “We had a very positive experience working with Tuffi Films in Finland, who came on board once we had a rough cut. Juha Löppönen was our primary producer and was always very respectful of our vision and creative ideas for the film. Through the co-production, we worked with an incredible team of Finnish creatives including editor Okku Nuutilainen and sound designer, Tuomas Skopa, and others who helped to shape the final film in very significant ways.”\nIn September 2022, Shameela was reunited with her Finnish collaborators at the Helsinki International Film Festival – Love & Anarchy (HIFF) where African Moot was being shown. Following the film screening, Shameela participated in a TransBorder Conversations2 panel discussion hosted by HIFF, the Academy of Moving People & Images (AMPI), and Art School Maa about co-production and distribution matters in the context of the Generation Africa project. She was joined by Juha Löppönen from Tuffi Films and Erkko Lyytinen from the Finnish broadcasting company, Yle. They discussed the co-production process, the future of film, and the impact of diverse storytelling across broader audiences.\nIn recent years, there have been ongoing conversations in Finland about a lack of diverse stories in film and media. Yet a growing portion of the Finnish population is of BIPOC (Black, Indigenous, and people of color) descent and other non-white groups. As such, there has been an active social push from advocates and artists alike for fair representation and inclusion of diverse experiences across all media platforms. Though far from the finish line, progress has been made in this space with active participation from not only the Finnish arts and film community and local BIPOC organizations but key decision-makers in the media.\nYle broadcasting company is one such example as they teamed up with STEPS on the Generation Africa project in a 4-film commitment deal to broaden the reach for African filmmakers and their stories. African Moot is one of the films being distributed by Yle through this collaboration and can be viewed on Yle Areena.\nAfter premiering at the Hot Docs International Film Festival in May, African Moot has been screened at DOK.fest München, Sydney Film Festival, Black Filmmakers Film Festival, and many others. Despite the recognition, Shameela remains humble about her work. She says, “While I don’t feel that I myself am advancing human rights as such – in my filmmaking, I am definitely drawn to people who are passionate about a cause, and who confront the often messy and contradictory worlds in which we all operate.” She hopes that people will watch her film and not only gain perspectives on pressing political issues but into the vulnerable traits that make humans human.\nShameela discusses future projects and a desire to create a film that is personal to her, one that is about her aunt, Fatima Meer. “Fatima always spoke out fearlessly right up until her last days. Her ’growing up’ story is a fascinating one in itself, and I believe that her work and ideas continue to cast light on current predicaments. It would be a film made in collaboration with her daughters, who obviously have unique insights.” With a hint of pride in her voice, she says, “Fatima Meer was a writer, artist, sociologist, filmmaker, and prominent activist – but also a formidable matriarch, playing a distinct role in the world of our family.”\nShameela’s compassion and contributions to social change are undoubtedly influenced by a rich family history of human rights advocacy. Through activism and the arts, Shameela and her family have brought to light racial inequities plaguing South Africans and have fought for the rights of the underserved while leveraging their resources for those without. Most would agree that they are the best examples of a global citizen, one that endeavors to improve the communities they live in and the world around them. As author and activist James Baldwin once stated, “the precise role of the artist, then, is to illuminate that darkness, blaze roads through that vast forest, so that we will not, in all our doing, lose sight of its purpose, which is, after all, to make the world a more human dwelling place.” 3\n- In-person, Zoom, and email conversations between Shameela Seedat and Sharron Todd between September and October 2022.\n- From Transborder Conversations panel discussion organized by AMPI & Maa & HIFF, moderated by writer and filmmaker Sharron Todd with filmmaker Shameela Seedat, producer Juho Löppönen, and commissioner Erkko Lyytinen as panelists (HIFF, Kino Regina, September 19, 2022).\n- Baldwin, James, short essay The Creative Process (Creative America, Ridge Press, 1962)\nSharron Todd is an African-Asian American writer, filmmaker, entrepreneur, and currently a marketing executive.\nThe article is published in cooperation with Moving People & Images Journal.","Mill Valley Film Festival\nThe Mill Valley Film Festival is an annual American film festival founded in 1977.\n|Location||Mill Valley, California, United States|\n|Hosted by||California Film Institute|\nIn October 1977, Mark Fishkin and fellow film buffs Rita Cahill and Lois Cole organized a three-day film festival. It featured three film tributes, Francis Ford Coppola's The Rain People and George Lucas' Filmmaker. \"We did a very innovative program that I would not be embarrassed to repeat today,\" Fishkin said. The first official festival took place in August 1978.\nAbout the FestivalEdit\nPresented by the California Film Institute, the Mill Valley Film Festival takes place in early October. With a reputation for launching new films and creating awards season buzz, MVFF has earned a reputation as a 'filmmakers' festival\" by celebrating the best in American independent and world cinema alongside high profile and prestigious award contenders.\nOver its nearly 40-year history, MVFF has attracted a strong roster of talent, including Robin Williams, Jim Jarmusch, Kevin Smith, Jon Voight, Roberto Benigni, Alfre Woodard, Gael García Bernal, Helen Mirren, Steve McQueen, Annette Bening, Glenn Close, James Franco, Edward James Olmos, Jared Leto, Lily Taylor, Mike Leigh, Ben Stiller, Carey Mulligan, Mira Nair, Dustin Hoffman, Geoffrey Rush, Marcel Ophuls, Jane Russell, Les Blank, Barbet Schroeder, James Woods, Sissy Spacek, Jonathan Winters, Robert Altman, Nicholas Ray, Roger Corman, Jeanne Moreau, Karen Black, Barry Levinson, Sarah Silverman, Costa-Gavros, Jan Troell, William H. Macy, Milos Forman, Dianne Weist, Edward Norton, Uma Thurman, Alejandro González Iñárritu, Alan Arkin, Amanda Plummer, Darren Aronofsky, Laura Linney, Gena Rowlands, Albert Maysles, Donald Sutherland, John Sayles, Bradley Cooper, Jeff Daniels, Jean-Pierre Jeunet, Helena Bonham Carter, Derek Jacobi, Ismail Merchant, Carroll Baker, Malcolm McDowell, Joan Allen, Dick Cavett, Hilary Swank, Jason Reitman, John Hawkes, Laura Dern, Elle Fanning, Felicity Huffman, Clive Owen, Eddie Redmayne, Forest Whitaker, Tim Robbins, Billy Bob Thornton, Sir Ian McKellen, Woody Harrelson, Harry Dean Stanton, John Walsh, Waldo Salt, Jennifer Jason Leigh, Ang Lee, Emma Stone, Ryan Gosling, Todd Haynes, Holly Hunter, Sean Penn, Danny Huston, Joe Wright, Peter Marshall, Dee Rees, Greta Gerwig, Margot Robbie, Allison Janney, Richard Linklater, Aaron Sorkin, Simon Curtis, Kristin Scott Thomas, Andrew Garfield, Connie Nielson, Catherine Hardwicke, Sean Baker, Jason Clarke, Sophie Nelisse, Lou Diamond Phillips, and Brie Larson.\nEach year, the 11-day festival welcomes more than 200 filmmakers and 60,000 attendees from around the world. Festival Sections include the World Cinema, US Cinema, Valley of the Docs, filmHOOD, 5@5 (shorts programs) and MVFF Music. Festival Initiatives include Mind the Gap: Women | Film | Tech, Viva El Cine! and Active Cinema. The festival also features tributes and spotlights to acclaimed filmmakers, screenwriters and actors. Screenings are usually held at the Christopher B Smith Rafael Film Center in San Rafael, the Sequoia Theatre in Mill Valley, the Cinema in Corte Madera, and the Lark Theatre in Larkspur.\nThe MVFF Audience Favorite 2017\nAudience Favorite - US Cinema, Gold Award\nAudience Favorite - US Cinema, Silver Award\nAudience Favorite - US Cinema Indie, Gold Award\nQUEST, Santiago Rizzo\nAudience Favorite - US Cinema Indie, Silver Award\nAudience Favorite - World Cinema, Gold Award\nAudience Favorite - World Cinema, Silver Award\nTHE INVISIBLES, Claus Räfle\nAudience Favorite - World Cinema Indie, Gold Award\nAN ACT OF DEFIANCE, Jean van de Velde\nAudience Favorite - World Cinema Indie, Silver Award\nVALLEY OF THE DOCS\nAudience Favorite - Valley of the Docs, Gold Award\nKIM SWIMS, Kate Webber\nAudience Favorite - Valley of the Docs, Silver Award\nCITY OF JOY, Madeleine Gavin\nMIND THE GAP\nAudience Favorite – Mind the Gap, Gold Award\n- Cahill, Greg (October 1–7, 1998). \"A Life in Film: Mark Fishkin ushers in Mill Valley Film Fest\". Sonoma County Independent. Metro Publishing. Retrieved 2013-01-26.\n- \"Mill Valley Film Festival 1978\" (Press release). Mill Valley, CA. 1978.\n- Zinko, Carolyne (October 9, 2004). \"Mill Valley Film Festival skips prizes, keeps focus on celluloid\". San Francisco Chronicle. Retrieved 2013-01-20.\n- Manos, Gus (October 10, 2008). \"Joe Wright at MVFF Insight Event\". CineSource Magazine. Retrieved 2013-01-24.\n- Vigil, Delfin (September 13, 2006). \"Mill Valley Film Festival looks to be fit for royalty\". San Francisco Chronicle. Retrieved 2013-01-20.\n- \"Mill Valley Film Festival: The United States' Largest Fall Celebration of Indie and International Films Wraps One of its Most Successful Years\". Mvff.org. Archived from the original on 2008-08-07. Retrieved 2013-01-24.\n- Welte, Jim (August 5, 2011). \"MVFF Unveils Children's FilmFest Lineup\". Mill Valley Patch. Retrieved 2013-01-24.\n- Manos, Gus (November 27, 2008). \"MVFF Tribute to Eric Roth\". CineSource Magazine. Retrieved 2013-01-24.\n- \"Mill Valley Film Festival announces SHANGHAI CALLING Northern California premiere!\". Shanghaicalling.com. September 13, 2012. Retrieved 2013-01-20.\n- \"Mill Valley Film Festival\". www.mvff.com. Retrieved 2019-02-15."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:9a9feb46-fddb-4f19-b538-b0005c792354>","<urn:uuid:feb68442-6757-4fa1-90d9-a71d43bfde93>"],"error":null}
{"question":"What makes Mariana Trench special compared to other oceanic trenches in terms of its physical characteristics?","answer":"The Mariana Trench is the deepest ocean trench in the world, reaching 10,911 meters (about 35,798 ft) below sea level at its deepest point (the Challenger Deep). The extreme pressure at this depth is about 1,086 bars, which is more than 1,000 times the standard atmospheric pressure at sea level. It is also distinctive for having serpentinite mud volcanoes in its nonaccretionary forearc, formed when fluids from the downgoing plate interact with cold mantle lithosphere.","context":["1803: Location Reviews\nTitle text: Google and Yelp keep deleting my scathing reviews of the Mariana Trench, the Chernobyl reactor core, the jet stream, and the equator.\nMany online advertising services and social media networks (like Google and Yelp, both mentioned in the title text, and for instance Facebook) allow users to leave reviews of stores, businesses and locations. For various reasons these sites often find themselves with pages dedicated to, as Randall puts it, \"places that really don't need reviews\" such as municipal works installations, government property, and natural landmarks. This naturally attracts both clueless people and lots of self-styled comedians leaving less-than-helpful comments on such review pages.\nRandall is just poking fun at this phenomenon by inventing possible reviews for the (fictional) location Canyon River Nuclear Launch Facility, depicted with a Google Maps-styled map page along with a series of so-called reviews. (There does exist a Canyon River located in Ontario/Canada and one in Washington/USA (the latter is a significant tributary to the Satsop River). Canada does not maintain nuclear weapons since 1984, so the launch site should be located in Washington).\nSee explanations for the 11 visible (out of 22) reviews in the table below. Of course those responsible for such a facility with nuclear missiles would not like the attention they would be getting in this way, especially not when one of the comments mentions a hole in the fence... Although this comic makes a joke about reviews it has chosen a very dangerous facility to joke about.\nIn the title text Randall mentions that both Google and Yelp keep deleting his scathing reviews of several locations like the above. The questions is if they would have done it if they had not been so harsh... While Canyon River Nuclear Launch Facility appears not to exist, the places/phenomena he lists in the title text certainly do, and are places that you either cannot or would not normally visit as destinations. Here below each \"location\" is explained. That the deletion of such reviews is real has been proven by this comic, as it also happened for those that (of course) posted these reviews on Google maps as a response to this comic.\nMariana Trench is the deepest area of the world's oceans, about 10,994 meters (36,070 ft) deep, located between Japan and Australia. The pressure in the Mariana Trench is about 1,086 bars, more than 1,000 times the standard atmospheric pressure of about 1 bar at sea level. Despite this enormous pressure some organisms live in the Mariana Trench. Humans can reach the ground only by special deep-sea submarines, like Jacques Piccard did in 1960 with the Bathyscaphe Trieste. See reviews for the Mariana Trench at Google Maps and Facebook.\nThe Chernobyl reactor core is the most dangerous part of the Chernobyl Nuclear Power Plant. It is located in the North of Ukraine. In the reactor No. 4 there was a nuclear disaster that happened on 26 April 1986. It caused devastating damage and massive radioactive contamination. There is still a Chernobyl Exclusion Zone 30 kilometers around the power plant. See reviews for the Chernobyl power plant at Google Maps and Facebook.\nJet streams are a meteorologic phenomenon about 9 to 16 kilometers above the ground. A stream consists of air currents with speeds from 92 km/h (50 kn; 57 mph) to over 398 km/h (215 kn; 247 mph). Such jet streams are routinely used for reducing fuel usage for long distance plane travels. As it is a ribbon rather than a point, it could not have a single point on the map. Also, the jet stream fluctuates north and south; so even if it could be pinpointed, the location would be constantly changing.\nThe equator is, as with the jet streams, not a singular place but a circumference around the Earth. Reviewing the equator as a singular location is rather pointless, though there is a whole range of specific (and interesting) locations around the equator, with countries with tropical rainforest climate, which many people from European and North American countries struggle with. That said, most of the equator goes over water.\nIn the table the rating is given with the review. After that an explanation both of the rating and of the review is given. Notice that any or all of the reviews could be sarcastic or \"trolling\", as is fairly typical on the internet, especially for reviews given for such a location as this one. This table assumes all the reviews are played straight.\n|★★★★★||Greatest country on earth||A patriotic review (5/5), though provides no information on the actual nuclear site. The location is in the \"greatest country\", although this makes fun of people who go too specific, because all places in that country could be rated like this. This comment resembles what an extremely patriotic person would say. They believe that the United States is the greatest country on Earth, and therefore are pleased that the country is being protected by nuclear weapons.|\n|★★☆☆☆||Looks cool but you can't get in||This reviewer, although initially positive, attempts to highlight what they perceive as a major flaw with the site: namely, that it is off-limits to unauthorized personnel and heavily-guarded, so it's impossible to actually go inside (thus only 2/5 stars). This is typical of a nuclear facility, but this kind of review could also be seen for a fancy restaurant that needs very early pre-booking.|\n|★☆☆☆☆||What is this store||Reviewer really, really has no idea what this facility actually is, mistaking it for a store, and thus giving it only 1 star.|\n|★★★★☆||My cousin worked here||If true, this review is a serious security risk (e.g. kidnapping the reviewer to extort information from his cousin). The comment may also just be a way for the reviewer to pretend he knows someone who works in the higher levels of the government. Usually this kind of comment together with a four star rating is to signal that you know more about the location than a regular reviewer does. Of course you could then also be perceived as partial.|\n|★★☆☆☆||Waitstaff heavily armed and very rude||This review mistakes the facility's security guards for a restaurant's waitstaff. Since the guards are protecting some of the most dangerous weapons in existence, and would not let unknown outsiders into the facility, it follows that the guards would be heavily armed, and quite rude to those who sought entry without proper permission. Thus they earn the place only 2 stars.|\n|★☆☆☆☆||Stop doing chemtrails||This reviewer believes in the chemtrail conspiracy theory and is urging the government to cease spreading the chemtrails. Believing this place has something to do with it of course leads to only one star. This conspiracy was earlier mentioned both in 966: Jet Fuel and 1677: Contrails, and later in 2654: Chemtrails|\n|★☆☆☆☆||This place is a symptom of the military-industrial complex strangling our democracy and...(read full review-1184 words)||A slightly tongue-in-cheek reference to essays against 'The Military-Industrial complex' and how they are often copy-pasted by people who don't really understand them in inappropriate places. Or just to people who rant far beyond anything that people would ever read, except if they are already agreeing with the writer. Of course such an activist would only give one star.|\n|★★★★☆||Anyone else notice the hole in the west fence?||The adventurer's travel guide to government installations... Posting a comment like this would (at best) bring the hole to the attention of the site staff to be repaired and (at worst) bring the writer unwelcome attention from the authorities for publicizing a security vulnerability at a missile site. This might also be a reference to Richard Feynman's account of finding a hole in the fence surrounding the Los Alamos facility during the Manhattan Project. Using the hole to get in, this reviewer had an excellent time and gives 4/5 stars.|\n|★★★★★||Whoa, missiles!||The writer is impressed and apparently surprised to discover that the site has missiles. Seems like the reviewer just loves anything to do with missiles and hands out five stars. This may also be a reference to the \"Whoa, technology!\" meme, which originated when YandereDev, a Youtuber, uttered the phrase in one of his videos.|\n|★★★☆☆||Good idea but confusing web site. How do I preorder?||This reviewer thinks that one can order a nuclear missile launch here, but can't find a preorder form on the website. He loves the idea but since he cannot find out how to order there are only 3/5 stars. In reality, the decision to launch nuclear missiles often rests with the heads of state or government, and outside persons are not allowed to control them.|\n|★☆☆☆☆||Please don't launch these||A plea to the facility owners not to launch the nuclear missiles, due to their deleterious effects on human life. See more on this under Politics below.|\n- [Inside the main panel there is a frame with a Google location map with the typical red pin stuck in the center of the map inside a large gray region of the map. A river goes from the north through the gray region and out to the west. East and south of the river some roads and other items are shown, several of them also outside the gray region. The red pin is stuck next to a corner in one of the roads.]\n- [Below the map is the name of the location at the red pin, and below that there are three lines of unreadable text:]\n- Canyon River Nuclear Launch Facility\n- [Below that there is broken line with text in the break, and below that follows 11 reviews with yellow stars to the left. The stars are either just outlines or colored completely, with the left one always being filled:]\n- Reviews (22)\n- [5 of 5 stars filled] Greatest country on earth\n- [2 of 5 stars filled] Looks cool but you can't get in\n- [1 of 5 stars filled] What is this store\n- [4 of 5 stars filled] My cousin worked here\n- [2 of 5 stars filled] Waitstaff heavily armed and very rude\n- [1 of 5 stars filled] Stop doing chemtrails\n- [1 of 5 stars filled] This place is a symptom of the military-industrial complex strangling our democracy and...(read full review-1184 words)\n- [4 of 5 stars filled] Anyone else notice the hole in the west fence?\n- [5 of 5 stars filled] Whoa, missiles!\n- [3 of 5 stars filled] Good idea but confusing web site. How do I preorder?\n- [1 of 5 stars filled] Please don't launch these\n- [Caption below the panel:]\n- I love finding reviews of places that really don't need to have reviews.\n- Only 11 of the 22 reviews posted are shown. For those 11 the average star rating is 2.6/5 stars. All five possible ratings are represented at least once.\n- For a few days after the release of this comic the Google Search results for \"Canyon River Nuclear Launch Facility\" briefly showed the facility was located at 43.428445, -101.124018 in Blackpipe Township, Mellette County, South Dakota and it included the reviews shown in the comic and more.\n- Randall's statement about Google deleting these kinds of reviews turned out to be true as they were quickly deleted, but not before someone made this screenshot of one of the other reviews.\nadd a comment! ⋅ add a topic (use sparingly)! ⋅ refresh comments!","Trenches define one of the most important natural boundaries on the Earth’s solid surface, that between two lithospheric plates. There are three types of lithospheric plate boundaries: divergent (where lithosphere and oceanic crust is created at mid-ocean ridges), convergent (where one lithospheric plate sinks beneath another and returns to the mantle), and transform (where two lithospheric plates slide past each other). Trenches are the spectacular and distinctive morphological features of plate boundaries. Plates move together along convergent plate boundaries at convergence rates that vary from a few millimeters to ten or more centimeters per year. A trench marks the position at which the flexed, subducting slab begins to descend beneath another lithospheric slab. Trenches are generally parallel to a volcanic island arc, and about 200 km from a volcanic arc. Oceanic trenches typically extend 3 to 4 km (1.9 to 2.5 mi) below the level of the surrounding oceanic floor. The deepest ocean depth to be sounded is in the Challenger Deep of the Mariana Trench at a depth of 10,911 m (35,798 ft) below sea level. Oceanic lithosphere disappears into trenches at a global rate of about a tenth of a square meter per second.\nThere are about 50,000 km of convergent plate margins, mostly around the Pacific Ocean – the reason for the reference “Pacific-type” margin - but they are also in the eastern Indian Ocean, with relatively short convergent margin segments in the Atlantic Ocean and in the Mediterranean Sea. Trenches are sometimes buried and lack bathymetric expression, but the fundamental structures that these represent mean that the great name should also be applied here. This applies to Cascadia, Makran, southern Lesser Antilles, and Calabrian trenches. Trenches along with volcanic arcs and zones of earthquakes that dip under the volcanic arc as deeply as 700 km are diagnostic of convergent plate boundaries and their deeper manifestations, subduction zones. Trenches are related to but distinguished from continental collision zones (like that between India and Asia to form the Himalaya), where continental crust enters the subduction zone. When buoyant continental crust enters a trench, subduction eventually stops and the convergent plate margin becomes a collision zone. Features analogous to trenches are associated with collisions zones; these are sediment-filled foredeeps referred to as peripheral foreland basins, such as that which the Ganges River and Tigris-Euphrates rivers flow along.\nDuring the 1920’s and 1930’s, Felix Andries Vening Meinesz developed a unique gravimeter that could measure gravity in the stable environment of a submarine and used it to measure gravity over trenches. His measurements revealed that trenches are sites of downwelling in the solid Earth. The concept of downwelling at trenches was characterized by Griggs in 1939 as the tectogene hypothesis, for which he developed an analogue model using a pair of rotating drums. World War II in the Pacific led to great improvements of bathymetry in especially the western and northern Pacific, and the linear nature of these deeps became clear. The rapid growth of deep sea research efforts, especially the widespread use of echosounders in the 1950’s and 1960’s confirmed the morphological utility of the term. The important trenches were identified, sampled, and their greatest depths sonically plumbed. The heroic phase of trench exploration culminated in the 1960 descent of the Bathyscaphe \"Trieste\", which set an unbeatable world record by diving to the bottom of the Challenger Deep. Following Robert S. Dietz’ and Harry Hess’ articulation of the seafloor spreading hypothesis in the early 1960’s and the plate tectonic revolution in the late 1960’s the term ‘trench’ has been redefined with plate tectonic as well as bathymetric connotations!\nTrenches are centerpieces of the distinctive physiography of a convergent plate margin. Transects across trenches yield asymmetric profiles, with relatively gentle (~5°) outer (seaward) slope and a steeper (~10-16°) inner (landward) slope. This asymmetry is due to the fact that the outer slope is defined by the top of the downgoing plate, which must bend as it starts its descent. The great thickness of the lithosphere requires that this bending be gentle. As the subducting plate approaches the trench, it is first bent upwards to form the outer trench swell, then descends to form the outer trench slope. The outer trench slope is disrupted by a set of subparallel normal faults which staircase the seafloor down to the trench. The plate boundary is defined by the trench axis itself. Beneath the inner trench wall, the two plates slide past each other along the subduction decollement, the seafloor intersection of which defines the trench location. The overriding plate contains volcanic arc (generally) and a forearc. The volcanic arc is caused by physical and chemical interactions between the subducted plate at depth and asthenospheric mantle associated with the overriding plate. The forearc lies between the trench and the volcanic arc. Forearcs have the lowest heatflow from the interior Earth because there is no asthenosphere (convecting mantle) between the forearc lithosphere and the cold subducting plate.\nThe inner trench wall marks the edge of the overriding plate and the outermost forearc. The forearc consists of igneous and metamorphic crust, and this crust acts as buttress to a growing accretionary prism (sediments scraped off the downgoing plate onto the inner trench wall, depending on how much sediment is supplied to the trench). If the flux of sediments is high, material will be transferred from the subducting plate to the overriding plate. In this case an accretionary prism grows and the location of the trench migrates progressively away from the volcanic arc over the life of the convergent margin. Convergent margins with growing accretionary prisms are called accretionary convergent margins and make up nearly half of all convergent margins. If the sediment flux is low, material will be transferred from the overriding plate to the subducting plate by a process of tectonic ablation known as subduction erosion and carried down the subduction zone. Forearcs undergoing subduction erosion typically expose igneous rocks. In this case, the location of the trench will migrate towards the magmatic arc over the life of the convergent margin. Convergent margins experiencing subduction erosion are called nonaccretionary convergent margins and comprise more than half of convergent plate boundaries. This is an oversimplification, because different parts of a convergent margin can experience sediment accretion and subduction erosion over its life.\nThe asymmetric profile across a trench reflects fundamental differences in materials and tectonic evolution. The outer trench wall and outer swell comprise seafloor that takes a few million years to move from where subduction-related deformation begins near the outer trench swell until sinking beneath the trench. In contrast, the inner trench wall is deformed by plate interactions for the entire life of the convergent margin. The forearc is continuously subjected to subduction-related earthquakes. This protracted deformation and shaking ensures that the inner trench slope is controlled by the angle of repose of whatever material it is composed of. Because they are composed of igneous rocks instead of deformed sediments, non-accretionary trenches have steeper inner walls than accretionary trenches.\nThere an evolution in trench morphology can be expected as oceans close and continents converge. While the ocean is wide, the trench may be far away from continental sources of sediment and so may be deep. As the continents approach each other, the trench may become filled with continental sediments and become shallower. A simple way to approximate when the transition from subduction to collision has occurred is when the plate boundary previously marked by a trench is filled enough to rise above sealevel.\nThe slope of the inner trench slope of an accretionary convergent margin reflects continuous adjustments to the thickness and width of the accretionary prism. The prism maintains a ‘critical taper’, established in conformance with Mohr-Coulomb Theory for the pertinent materials. A package of sediments scraped off the downgoing lithospheric plate will deform until it and the accretionary prism that it has been added to attain a critical taper (constant slope) geometry. Once critical taper is attained, the wedge slides stably along its basal decollement. Strain rate and hydrologic properties strongly influence the strength of the accretionary prism and thus the angle of critical taper. Fluid pore pressures modify rock strength and are important controls of critical taper angle. Low permeability and rapid convergence may result in pore pressures that exceed lithostatic pressure and a relatively weak accretionary prism with a shallowly tapered geometry, whereas high permeability and slow convergence result in lower pore pressure, stronger prisms, and steeper geometry.\nThe Hellenic trench system is unusual because this convergent margin subducts evaporites. The slope of the surface of the southern flank of the Mediterranean Ridge (its accretionary prism) is low, about 1°, which indicates very low shear stress on the decollement at the base of the wedge. Evaporites influence the critical taper of the accretionary complex, as their mechanical properties differ from those of siliciclastic sediments, and because of their effect upon fluid flow and fluid pressure, which control effective stress. In the 1970s, the linear deeps of the Hellenic trench south of Crete were interpreted to be similar to trenches at other subduction zones, but with the realization that the Mediterranean Ridge is an accretionary complex, it became apparent that the Hellenic trench is actually a starved forearc basin, and that the plate boundary lies south of the Mediterranean Ridge.\nChemosynthetic communities thrive where cold fluids seep out of the forearc. Cold seep communities have been discovered in inner trench slopes down to depths of 6000 m in the western Pacific, especially around Japan, in the Eastern Pacific along North, Central and South America coasts from the Aleutian to the Peru-Chile trenches, on the Barbados prism, in the Mediterranean, and in the Indian Ocean along the Makran and Sunda convergent margins. These communities receive much less attention than the chemosynthetic communities associated with hydrothermal vents. Chemosynthetic communities are located in a variety of geological settings: above over-pressured sediments in accretionary prisms where fluids are expelled through mud volcanoes or ridges (Barbados, Nankai and Cascadia); along active erosive margins with faults; and along escarpments caused by debris slides (Japan trench, Peruvian margin). Surface seeps may be linked to massive hydrate deposits and destabilization (e.g. Cascadia margin). High concentrations of methane and sulfide in the fluids escaping from the seafloor are the principal energy sources for chemosynthesis.\nIgneous basement of a nonaccretionary forearc may be continuously exposed by subduction erosion. This transfers material from the forearc to the subducting plate and can be accomplished by frontal erosion or basal erosion. Frontal erosion is most active in the wake of seamounts being subducted beneath the forearc. Subduction of large edifices (seamount tunneling) oversteepens the forearc, causing mass failures that carry debris towards and ultimately into the trench. This debris may be deposited in graben of the downgoing plate and subducted with it. In contrast, structures resulting from subduction erosion of the base of the forearc are difficult to recognize from seismic reflection profiles, so the possibility of basal erosion is difficult to confirm. Subduction erosion may also diminish a once-robust accretionary prism if the flux of sediments to the trench diminishes.\nNonaccretionary forearcs may also be the site of serpentine mud volcanoes. These form where fluids released from the downgoing plate percolate upwards and interact with cold mantle lithosphere of the forearc. Mantle peridotite is hydrated into serpentinite, which is much less dense than peridotite and so will rise diapirically when there is an opportunity to do so. Some nonaccretionary forearcs are subjected to strong extensional stresses, for example the Marianas, and this allows buoyant serpentinite to rise to the seafloor where they form serpentinite mud volcanoes. Chemosynthetic communities are also found on non-accretionary margins such as the Marianas, where they thrive on vents associated with serpentinite mud volcanoes.\nThere are several factors that control the depth of trenches. The most important control is the supply of sediment, which fills the trench so that there is no bathymetric expression. It is therefore not surprising that the deepest trenches (deeper than 8,000 m) are all nonaccretionary. In contrast, all trenches with growing accretionary prisms are shallower than 8000 m. A second order control on trench depth is the age of the lithosphere at the time of subduction. Because oceanic lithosphere cools and thickens as it ages, it subsides. The older the seafloor, the deeper it lies and this determines a minimum depth from which seafloor begins its descent. This obvious correlation can be removed by looking at the relative depth, the difference between regional seafloor depth and maximum trench depth. Relative depth may be controlled by the age of the lithosphere at the trench, the convergence rate, and the dip of the subducted slab at intermediate depths. Finally, narrow slabs can sink and roll back more rapidly than broad plates, because it is easier for underlying asthenosphere to flow around the edges of the sinking plate. Such slabs may have steep dips at relatively shallow depths and so may be associated with unusually deep trenches, such as the Challenger Deep.\n|Mariana Trench||Pacific Ocean||10,911m (32,733)|\n|Tonga Trench||Pacific Ocean||10,882m (32,646)|\n|Kuril Trench||Pacific Ocean||10,542m (31,626)|\n|Philippine Trench||Pacific Ocean||10,540m|\n|Kermadec Trench||Pacific Ocean||10,047m|\n|Izu-Bonin Trench (Izu-Ogasawara Trench)||Pacific Ocean||9,780 m|\n|Japan Trench||Pacific Ocean||9,000m|\n|Puerto Rico Trench||Atlantic Ocean||8,605 m|\n|Peru-Chile Trench or Atacama Trench||Pacific Ocean||8,065 m|\n|Aleutian Trench||West of Alaska|\n|Bougainville Trench||South of New Guinea|\n|Cayman Trench||Western Caribbean Sea|\n|Cedros Trench (inactive)||Pacific coast of Baja California|\n|Hikurangi Trench||East of New Zealand|\n|Japan Trench||Northeast Japan|\n|Kuril-Kamchatka Trench||Near Kuril islands|\n|Mariana Trench (deepest known part of the oceans)||Western Pacific ocean; east of Mariana Islands|\n|Middle America Trench|\n|New Hebrides Trench||West of New Caledonia|\n|Puerto Rico Trench (deepest known part of the Atlantic Ocean)||Boundary of Caribbean Sea and Atlantic ocean|\n|Peru-Chile Trench||Eastern Pacific ocean; off coast of Peru & Chile|\n|Philippine Trench||East of Philippine Islands|\n|Ryukyu Trench||Eastern edge of Japan's Ryukyu Islands|\n|South Sandwich Trench|\n|Sunda Arc and Java Trench|\n|Tonga Trench||North-east of Australia|\n|Yap Trench||Western Pacific ocean; between Palau Islands and Mariana Trench|\n|Intermontane Trench||Western North America; between Intermontane Islands and North America|\n|Insular Trench||Western North America; between Insular Islands and Intermontane Islands|\n|Farallon Trench||Western North America|\n|Tethyan Trench||South of Turkey, Iran, Tibet and Southeast Asia|"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:5fcf9c38-8e6f-4179-957e-be9e5f140f51>","<urn:uuid:d936b3af-fff6-427a-bb02-458add29833e>"],"error":null}
{"question":"As someone developing a website, I'm curious about how user paths and navigation systems compare when it comes to improving user experience. What are the key differences between these approaches?","answer":"User paths and navigation systems take different approaches to improving user experience. User paths are analytical tools focused on user behavior and are designed to funnel users toward specific business goals like making purchases or signing up for newsletters. They are planned alongside information architecture and aim to make journeys as short and simple as possible. Navigation systems, on the other hand, are more structured elements that must be consistent throughout the website and include different types like global navigation (appearing on every page), local navigation (within specific sections), and utility links (in headers or footers). While user paths focus on achieving business objectives, navigation systems emphasize usability principles like being easy to learn, providing feedback, and using clear labels based on user terminology.","context":["White background, black letters and rectangles in different shades of grey – that doesn’t sound like the most compelling website design, does it? It is, however, how most designs we do in our studio come to be. Before a design is taken over by a graphic artist who spices it up with visual effects, it is first planned and designed like an architectural blueprint. This phase of the design process is called wireframing.\nSome business owners think that wireframes are just sketches of the final website design and are not worth the investment. They couldn’t be more wrong – I dare to say that most of the actual designing is done at this very stage of the work process. During this time – away from colors, typography, icons, and illustrations – assumptions are made, business goals are set, and tools to achieve them are picked.\nWireframes and mockups play an important part while designing a website or e-commerce store. They are not alone, though – other useful tools include information architecture, user paths, and detailed analysis. Their place in the process is constant and always precedes visual design – wireframes are the only way to properly plan work at later stages, guarantee consistency among other parts of the projects, and avoid serious problems (described later in this article).\nWhere are wireframes in the process?\n- Analysis (investigating the competition, client’s needs, propositions, and business goals)\n- Information architecture\n- User paths\n- Discussion, improvements, direction consolidation\n- Wireframing (iterations)\n- Visual design\n- Implementation (coding)\nBetter be prepared\nIt lies in every business owner’s best interest to provide all possible information when starting a new project. The more details, the easier it is for a UX designer to understand business specifics. Unique features, business goals, conversion goals, sales tools – the more they know, the more efficient will the new website or store be.\nThere are countless ways to design a proper user experience on the web. Store or website can be simple and straightforward. Or on the contrary – mysterious and extremely engaging. While designing the experience, one must keep in mind business and image goals and look out for various traps. It is easy to make a website mockup that will just be, tick a box on the list and push the project forward. Unfortunately, not paying attention to wireframes may cause additional problems that will snowball into serious issues at later stages.\nIf a wireframe is not sufficiently considered and detailed, visual design based on it will be implemented with all lacks and deficiencies that will become apparent too late. Making changes to already implemented site or store causes difficulty and additional costs. It is reasonable for a company to trust the specialists and spend more time polishing up the wireframes, dispelling any doubts, and setting a common direction for the project.\nInformation architecture, simply\nJust like visual design follows wireframing, information architecture and user paths precede website mockups. It is a very important stage of work when the UX designer makes decisions about site structure (map) and different connections between its elements.\nInformation architecture is also a great support for developers before they start the implementation. For large and complicated projects like extensive websites and apps, solid structure with well-described functionality will help them plan and estimate their work. Information architecture makes things easier but doesn’t guarantee anything – client’s corrections, adding features, and testing also have a great impact on project completion time. Solid but flexible information architecture allows the project to grow while maintaining full control over its scope and quality of final result.\nThere’s a reason why the word „architecture” is hidden in the name of this phase. Just like architecture, UX design revolves around users, their needs, goals, and expectations. It considers real-world issues and solves them within constraints. When an architect decides to pick cheaper build materials to fit within their client’s budget, a UX designer collects plugins and ready-made solutions that will decrease overall costs of the project and advance the implementation. While working on a website, designer specifies the number of pages, sets goals for each of them, and accentuates the connections between the elements. He’s driven by his will to design an intuitive and logical experience but keeps his client’s business goals in mind at all stages of the work.\nBusiness goals can be very different:\n- increase number of pageviews\n- put greater emphasis on branding\n- boost communication\n- optimize sales and conversion solutions\n- improve level of customer support\n- reach new and existing clients\n- shorten the time required to complete a certain task\n- improve usability and accessibility\n- redesign the application or service\nThere are many ways to achieve these goals. One of the most helpful ones is called user paths. It is a highly analytical tool that focuses on user’s behavior on the site. It helps discover pages that do not have many views or interrupt a path important for the site’s business goals (for example buying a product or signing up for newsletter). User paths are planned alongside the information architecture – with the right mixture of page structure, connections between them and layout, users can be funneled into the product page and encouraged to make a purchase.\nUser paths play one more important role: they guarantee intuitiveness and consistency for the site as a whole. It is good practice to make them as short and simple as possible, allowing the user to reach key parts of the site in just a few clicks. While planning user paths, UX designer is guided by their comfort and pleasant experience.\nFor SmartYou, we visualized website structure as a tree built of small blocks. The most important pages have been highlighted using a different colors and showed an essential, most suitable for our goals user path. The visualization helped the client see the big picture of the overall user experience on the site and pinpoint moments that required additional work or redesign. It also gave him a better view of the project which, from the initial 1-3 pages, turned into complex conversion-lead website.\nSimulating the experience\nFinished and accepted information architecture lays a good groundwork for the wireframes. At this stage the designer is familiar with overall site structure and knows all key features. In spite of appearances, his next step is not just making a black-and-white sketch of the site but rather give shape to its content. A wireframe is a mold for the substance.\nIt’s good to trust a specialist. Their knowledge, experience, and skills will help you get better results. Business owners who try to „design” their new website on their own, rarely take into account various functional, usability, and technology requirements (not all solutions can be implemented within the given budget). Furthermore, the first version of a wireframe rarely is similar to the last, while working on a prototype requires many iterations and changes. Even after a wireframe is accepted, when new information come up or the final results requires making changes to existing solutions, visual design will play the part of a wireframes. As you can see, it’s important for a UX designer to work closely with visual artist – he or she often supervises all work on the project.\nGreat wireframes are based on ready contents. When information architecture described the project’s structure and logic, texts and images are parts of direct communication that revolves around the single most important goal of wireframes: functionality. It consists of layout and content presentation on the site. A blog offers an entirely different experience from an e-commerce store, while a landing page requires other level of immersion from a complex application built in the browser. Thanks to texts, even if not in their final form, a designer can properly showcase on the site and assign to pre-planned user paths and scenarios.\nIt’s quite easy to find wireframe examples on the web. Usually they are just black letters and rectangles in different shades of grey scattered against a white background. There are a few reasons why they look that way:\n- wireframes focus on specifics. Most of them do not include icons (if so, they’re usually just examples) and the designer rarely pays attention to spacing and grid. The goal of the wireframes is to work a functional and usable solution to a problem.\n- wireframes are often expanded and must be flexible. More complex visual effects would slow down the iteration pace that often includes tens of different designs. Simple appearance allows the wireframes to be modular and expandable.\n- simple style is a way to show information hierarchy. Thanks to different shades of grey across various elements on the site, it is easy to set the hierarchy and rhythm for the design. The darker the color, the most important an element is.\n- wireframes are used for testing. Many of the mid- and hi-fi wireframes can be used for user testing. Usually they need to be turned into interactive prototypes that come with clickable buttons and links. User testing is used to verify initial functional assumptions and overall intuitiveness of a site or app. In our studio, we create prototypes in Adobe Xd that can export them as videos or clickable mockups in the browser.\nWriting on a wall\nWireframes simulate user experience on the site and help the designer put in his or hers shoes. From their point of view, the designer can spot any shortcoming and mistakes that, when discovered during the visual design or implementation stage, would put the project and relationship with the client to a test. Therefore, wireframes offer an insight into the final feel of the site and make it easier to solve any problems at an early stage.\nIn black and white, they show how the client’s new website or app will look in the future. It highlights the navigation and layout in an almost final form. They are the first touchpoint between a client and his projects – even the first iterations of wireframes can give the vibe of the design and help the client imagine the final effect. Prototypes become a topic for discussion that often leads to interesting insights and corrections to earlier propositions.\nWhile moving an element up or down is not an issue, often some changes are strategic. Clicking through the prototype can offer a fresh look on project. Wireframes highlight all shortcoming in communication and weak points on the site. Most frequent changes include:\n- number of options and links in the navigation\n- showcasing call-to-action buttons\n- level of complexity among products or services\n- length or size of texts\n- priority given to important elements\n- visibility of key elements above the fold\nOn the other hand, user testing can tell us that:\n- they’re not sure what this button does\n- this „Buy now” button is not clear and they almost missed it\n- contact form has too many fields\n- there’re too many elements on the product page and they feel overwhelmed\n- that icon is too small and they can barely see it\n- this copy says too much about the company but not enough about the product\nSounds familiar? If so and you’ve noticed a recent drop in your site’s statistics, consider getting a UX audit and making some changes. Our studio provides such service where we analyze all aspects of user experience on the site: structure, information architecture, usability, appearance, and communication (copywriting, branding). The results are presented in the form of a detailed report that includes conclusions and improvement suggestions.\nSolving important problems during the wireframing stage helps save time, money, and energy. If we would just proceed to visual design and coding after getting information architecture accepted, making any changes would leave an unpleasant mark on the budget and fluidity of the project.\nTo sum up (in a few words)\nIt’s good to end such a long text with a short and meaty summary. Now you know that:\n- Wireframe is more than just a sketch. It is placed somewhere between strategy and planning and visual design and implementation.\n- A well-designed wireframe can simulate the final product and solve any issues before they become real.\n- It is also a great basis for user testing and a chance to objectively assess project progress.\n- Information architecture is a plan for the general structure and logic behind all pages on the site. It precedes wireframing and helps lay the functional groundwork for all later work.\n- User paths are created alongside information architecture and can be used to meet business goals – the right order and layout of pages within the site can encourage purchase, sign up for a newsletter or take action.\nMaking a website is not that much different from building a house. Both projects require groups of specialists and major financial investments. Like a building that needs meticulous architectural plans, implementing a website is always preceded by information structure, wireframes, and prototype. These tools help save time and money and avoid many unexpected problems in the future.\nIn our studio wireframes and prototypes are useful instruments for working on different projects – be it a low-key software house or a complex comparison tools for thousands of products.","When it comes to the user experience it’s not the technical infrastructure or the storage system we are interested in. We focus on the structure of the information we share with the user and how this content is organized, labeled and visualized.\nWe define what data should be displayed and how the different pages of information is linked to each other.\nThe easiest ways to begin this process is to define a site map; This allows us to group pages together and helps determine the best structure to present them.\n“It’s very important to understands how people actually use content and how the structure should function to support that.”\nOne of the challenges for the UX designer is to find the right set of relationships between items and categories. This provides menus, links, buttons and other navigational structures to deliver a good user experience.\nSome of the techniques that can be used for this part are card sorting, user surveys, together with other user research.\nSometimes there’s the tricky problem to incorporate these in to existing taxonomies, already defined for other systems.\nThis may involve a careful examination of the existing taxonomy to ensure that menus, boxes, links, etc. can fully represent each taxonomy clearly without squashing data or omitting it entirely.\nTeamwork gives an advantages\nWhen a Back-End Developer and a UX-designer works together they have the ability to match technical/system limitations to the user’s needs and achieve a “best fit” between these two. All for an improved user experience if the perfect experience isn’t quite feasible.\nSteps to developing an intuitive information architecture:\n- Find out what the mission or purpose is: why will people come to use this?\n- Determine the immediate and long-range goals: are they different?\n- Pinpoint the intended audiences and conduct a requirements analysis for each group.\n- Collect content and develop a content inventory.\n- Determine the organizational structure\n- Create an outline of the project, which can include:\n- Content Inventory: a hierarchical view of the content, typically in a spreadsheet format, which briefly describes the content that should appear on each page and indicates where pages belong in terms of global and local navigation.\n- Site Maps: visual diagrams that reflect site navigation and main content areas. They are usually constructed to look like flowcharts and show how users will navigate from one section to another.\n- Create a visual blueprint, which can include:\n- Wireframes: rough illustrations of page content and structure, which may also indicate how users will interact. These diagrams get handed off to a visual designer, who will establish page layout and visual design. Wireframes are useful for communicating early design ideas and inform the designer and the client of exactly what information, links, content, promotional space, and navigation will be on every page. Wireframes may illustrate design priorities in cases where various types of information appear to be competing.\n- Define the navigation systems:\n- Global navigation: Global navigation is the primary means of navigation. Global navigation links appear on every page, typically as a menu.\n- Local navigation: Local links may appear as text links within the content of a page or as a submenu for a section.\n- Utility links: Utility links appear in the header or footer of every page. These may include infrequently used links.\n- Conduct user research:\n- Once you have a draft navigation structure, conduct appropriate usability research to collect feedback from the target audience. Methods may include: Card Sorting, Cognitive Walkthroughs, prototyping, Contextual Task Analyses, and Usability Testing.\nTips for creating usable navigational systems:\n- Be easy to learn.\n- Be consistent throughout the website.\n- Provide feedback, e.g. such as the use of breadcrumbs to indicate how to navigate back to where the user started.\n- Use the minimum number of clicks to arrive at the next destination.\n- Use clear and intuitive labels, based on the user’s perspective and terminology.\n- Support user tasks.\n- Have each link be distinct from other links.\n- Group navigation into logical units.\n- Avoid making the user scroll to get to important navigation or submit buttons.\n- Not disable the browser’s back button.\nSource: Own experience and Interaction Design Foundation, https://www.interaction-design.org"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:75f23002-410e-432e-8291-675bbf01a2a0>","<urn:uuid:db60a004-8de2-414d-85ae-787149a801b5>"],"error":null}
{"question":"How does Odysseus' journey home in Homer's epic compare to Aeneas' voyage in Virgil's work - what key challenges did each hero face?","answer":"Both heroes faced significant challenges in their journeys. Odysseus spent 10 years trying to return home to Ithaca after the Trojan War, facing struggles, loss, heartache and pain along the way. His journey was cursed by the sea god Poseidon. Meanwhile, Aeneas escaped from the sacking of Troy and made his way to Italy, where he faced difficulties including being driven by a storm to Carthage. There, Queen Dido fell in love with him, but Jupiter recalled him to his duty, leading to Dido's suicide when he departed. Aeneas then had to journey through the Underworld and ultimately face war with Turnus over Lavinia's hand.","context":["A journey home in the odyssey by homer\nThe odyssey home background information map/story summaries homer's biography works cited odysseus' journey, in numerical order click to read story . This particular journey, as detailed in homer’s the odyssey, is one of struggle, loss, heartache, pain, growth and triumph it is comprised of many steps that odysseus has to overcome and battle through in order to achieve his final goal of reaching his home and his loved ones. Free summary and analysis of the events in homer's the odyssey that won't make you snore we promise skip to navigation home / literature / the odyssey /. The odyssey by homer searchable etext and his long journey home after the fall of troy it takes odysseus ten years to reach ithaca after the ten-year trojan .\nThis particular journey, as detailed in homer's the odyssey, is one of struggle, loss, heartache, pain, growth and triumph it is comprised of many steps that odysseus has to overcome and battle through in order to achieve his final goal of reaching his home and his loved ones. The goal of the authors of this guide is not to get you to think like them, but to give you access to a great story we believe that stories are natural to human beings and that, therefore, we ought to approach them like humans naturally approach stories. The odyssey is told in a flashback form and recounts the story of the greek hero odysseus (known as ulysses in roman myths) and his ten-year journey home after the end of the trojan war today, we use the term odyssey to refer to any epic journey, and we see the influence of the odyssey on western culture in many cultural forms — novels, film .\nScholars believe the odyssey was composed near the end of the 8th century bc, somewhere in ionia, the greek coastal region of anatolia the poem mainly focuses on the greek hero odysseus (known as ulysses in roman myths), king of ithaca, and his journey home after the fall of troy. Homer the odyssey translated by robert fagles book i work out his journey home so odysseus can return lord poseidon, i trust, will let his anger go. Love making maps on tripline please help fund future development by supporting us on patreon for as little as $1/month it only takes a minute. Everything you ever wanted to know about quotes about the odyssey, the odyssey by homer home these shrink from making the journey to the house of her father . Homer’s odyssey and the odyssey of our lives homer’s odyssey is a magnificent mythological tale this work was presumably created after his encounter with goddess athena although odysseus’ journey is filled with unrealistic adventures and mythical powers, some principles behind this story can relate to our everyday lives.\nUnlike most editing & proofreading services, we edit for everything: grammar, spelling, punctuation, idea flow, sentence structure, & more get started now. - the journey of odysseus in homer's epic the odyssey, the hero odysseus attempts to complete his journey home from troy on his way home, however, he angers the sea god, posiedon, who curses him to travel for ten years on the sea, to loose all his men, and to return on a stranger's ship. The odyssey is a major ancient greek epic poem it was written by homer , as a sequel to the iliad in its origin it was an oral epic poem from mycenaean greece , about the 11th century bc. Here are examples of some of the most famous quotes from homer's the odyssey, from what war is odysseus attempting to return home the peloponnesian war.\nThese scenes are told by the poet to represent an important transition in odysseus' journey: being concealed to returning home the feminine in homer's odyssey. In homer's odyssey, odysseus' first stop after leaving troy is the land of the cicones after initially defeating the cicones in battle, the cicones regroup and attack odysseus and his men, who . The odyssey is an epic journey, but the word journey must be broadly understood the epic focuses, of course, on odysseus’s nostos (“return home” or “homeward voyage”), a journey whose details a greek audience would already know because of their rich oral mythic tradition. The odyssey of homer is a greek epic poem that tells of the return journey of odysseus to the island of ithaca from the war at troy, which homer addressed in the iliadin the greek tradition, the . The odyssey study guide contains a biography of homer, literature essays, a complete e-text, quiz questions, major themes, characters, and a full summary and analysis about the odyssey the odyssey summary.\nA journey home in the odyssey by homer\nA short summary of homer's the odyssey this free synopsis covers all the crucial plot points of the odyssey athena also prepares him for a great journey to . Odyssey: odyssey, epic poem in 24 books traditionally attributed to the ancient greek poet homer the poem is the story of odysseus, king of ithaca, who wanders for 10 years (although the action of the poem covers only the final six weeks) trying to get home after the trojan war. In the epic poem the odyssey, homer tells of the journey of odysseus in his journey home, odysseus’s character flaws and strengths are revealed through it all, odysseus is able to overcome his obstacles and prove his leadership abilities. Homer brings out a number of themes in his classic work, the odyssey, but one highly notable theme is the nobility of the desire to be at home the odyssey is by and large an account of one man’s desperate effort to return home.\n- The odyssey quotes (showing 1-30 of 152) “of all creatures that breathe and move upon the earth, nothing is bred that is weaker than man” ― homer , the odyssey.\n- Ancient greece - homer - the odyssey (epic poem, greek, c 725 bce, 12,110 lines) and his long journey home to ithaca following the fall of troy his adventure .\n- About the odyssey the odyssey is literature’s grandest evocation of every man’s journey through lifein the myths and legends that are retold here, the energy and poetry of homer’s original is captured in a bold, contemporary idiom, giving us an edition of the odyssey that is a joy to listen to, worth savoring treasuring for its sheer lyrical mastery.\nThe legend of homer's odyssey may be true after all: archaeologists find the palace where it all began by mail online reporter the story of the king of ithaca's ten-year journey home is an . The odyssey by homer is an epic poem that has survived thousands of years it is the story of odysseus, the crafty king of ithaca, whose trojan horse idea helped win the war with troy.","Publius Vergilius Maro (October 15, 70 B.C.E. – 19 B.C.E.), known in English as Virgil or Vergil, is a Latin poet, the author of the Eclogues, the Georgics and the Aeneid, the latter an epic poem of twelve books that became the Roman Empire's national epic. Virgil, along with his predecessor Homer and his successor Dante, would form the three main pillars of epic poetry. His Aeneid is still one of the most widely influential poems ever written. Little is known of Virgil's life or beliefs, but in his poetry he shines as a brilliant master of dactylic hexameter, a verse form first used by the ancient Greeks which would fall almost entirely into disuse with Virgil's death and the gradual decline of the Roman Empire. It is largely thanks to Virgil that, in a time when knowledge of Greek was rapidly eroding, much of Greece's culture was able to find its way into the literature of early modern Europe.\nVirgil was born in the village of Andes, near Mantua in Cisalpine Gaul (Gaul south of the Alps; present-day northern Italy). Virgil was of non-Roman Italian ancestry. He was educated in Cremona, Milan, and finally Rome, where he came under the tutelage of the Epicurean Siro, whose philosophy would be evident in Virgil's early works, but would later wane as Virgil turned to the sterner philosophy of Stoicism. It was during this time in Rome that Virgil began to compose his first poems in imitation of the ancient Greeks.\nA group of minor poems written during this time, sometimes referred to as the Appendix Vergiliana, have survived, but scholars largely consider the attribution to Virgil spurious. One such work, the Catalepton, consists of fourteen short poems, some of which may be Virgil's and some of which may be the work of later poets; another of the more significant pieces from this early collection is a short narrative poem titled the Culex, or mosquito, which was attributed to Virgil as early as the first century C.E. and may, in fact, be a legitimate Virgil poem.\nIn 42 B.C.E., after the defeat of Julius Caesar's assassins, Brutus and Cassius, the demobilized soldiers of the victors settled on expropriated land and Virgil's estate near Mantua was confiscated. Virgil explores the various emotions surrounding these appropriations and other aspects of rural life in the Eclogues, his earliest major poem, which addresses the wonders (and troubles) of the Roman Empire and spends a particularly great deal of attention to exhorting the ideal of pastoral life. Much of the poem consists of shepherds and farmers wandering a beautiful countryside and describing in awe the wonders of nature. Although it was common in earlier centuries for readers to find pieces of Virgil's own autobiography in the Eclogues, scholars today largely reject the effort to seek to identify him with characters in his poetry.\nVirgil soon became part of the circle of Maecenas, Octavian's capable agent d'affaires, who sought to counter sympathy for Mark Antony among the leading families by rallying Roman literary figures to Octavian's side. After the Eclogues were completed, Virgil spent the years 37 B.C.E. – 29 B.C.E. working on the Georgics (\"On Farming\"), which was written in honor of Maecenas, and is the source of the expression tempus fugit (\"time flies\").\nThe Georgics have largely been neglected by modern readers, in favor either of the earlier romanticism of the Eclogues or the epic grandeur of the Aeneid. Yet in ancient times the Georgics were esteemed by some poets and writers to be Virgil's greatest achievement. It is easy to understand the poem's rather unusual place in Virgil's corpus: the Georgics, although a poem, is written as a series of instructions on farming, ploughing, bee-keeping, and other agricultural activities (and, indeed, contains a good deal of useful advice on all these occupations.) However, in the ancient tradition of instructional poetry, the prosaic instructions are metaphors for such universal themes as the sowing, ploughing and tending of the soil of the soul. For reasons of its prosaicness, the Georgics are sometimes acknowledged as Virgil's most subtly charming work, and the poem has seen the occasional resurgence of interest. Thoreau, for instance, cited it as his favorite work of literature, and helped to re-introduce the poem to an American audience.\nHowever, it would not be until 31 B.C.E., after Octavian had defeated Antony at the Battle of Actium and was crowned as the new emperor Augustus after a long period of strife, that Virgil would begin to compose his most famous work. It would be a poem which the new emperor enjoined Virgil to write in praise of his rule and to justify his right to rule the empire, but would become the most enduring work of all Roman literature.\nVirgil responded to this request with the Aeneid epic poem that would forever immortalize his fame. The composition would take up his last ten years. The first six books of the epic tell how the Trojan hero Aeneas escapes from the sacking of Troy and makes his way to Italy. On the voyage, a storm drives him to the coast of Carthage, where the queen, Dido, welcomes him, and under the influence of the gods falls deeply in love with him. Jupiter recalls Aeneas to his duty, however, and he slips away from Carthage, leaving Dido to commit suicide, cursing Aeneas as revenge. On reaching Cumae, in Italy, Aeneas consults the Cumaean Sibyl, who conducts him through the Underworld and reveals his destiny to him. Aeneas is reborn as the creator of Imperial Rome.\nThe first six books (of \"first writing\") are modeled on Homer's Odyssey, but the last six are the Roman answer to the Iliad. Aeneas is betrothed to Lavinia, daughter of King Latinus, but Lavinia had already been promised to Turnus, the king of the Rutulians, who is roused to war by the Fury Allecto. The Aeneid ends with a duel between Aeneas and Turnus. Aeneas defeats and kills Turnus, spurning his plea for mercy.\nWhile still working on the poem, Virgil traveled with Augustus to Greece. There, Virgil caught a fever and died in Brundisium harbor, leaving the Aeneid unfinished. Augustus ordered Virgil's literary executors, Lucius Varius Rufus and Plotius Tucca, to disregard Virgil's own wish that the poem be burned, instead ordering it published with as few editorial changes as possible. As a result, the text of the Aeneid that exists may contain faults that Virgil was planning to correct before publication. However, the only obvious imperfections are a few lines of verse that are metrically unfinished (i.e., not a complete line of dactylic hexameter). Other alleged \"imperfections\" are subject to scholarly debate.\nIncomplete or not, the Aeneid was immediately recognized as a masterpiece. It proclaimed the imperial mission of the Roman Empire, but at the same time depicted the casualties and grief of Rome's expansion into a continent-straddling empire. Dido and Turnus, who are both casualties of Rome's destiny, are more attractive figures than Aeneas, whose single-minded devotion to his goal may seem almost repellent to the modern reader. However, at the time Aeneas was considered to exemplify virtue and pietas, a term related to the English \"piety\" which conveys a complex sense of being duty-bound and respectful of one's gods, one's family and one's homeland. Despite his pietas Aeneas struggles between doing what he wants to do as a man, and doing what he must as a virtuous hero. Aeneas' inner turmoil and shortcomings make him a more realistic character than the heroes of older poems, such as Odysseus, and in this sense Virgil paved the way for later Roman poets like Seneca who would establish the groundwork of classical realism.\nLater views of Virgil\nEven as the Roman world collapsed, literate men acknowledged Virgil as a master poet, although they ceased to read him. Gregory of Tours who read Virgil and some other Latin poets, nevertheless cautioned, \"We ought not to relate their lying fables, lest we fall under sentence of eternal death.\" Surviving medieval collections of manuscripts containing Virgil's works include the Vergilius Augusteus, the Vergilius Vaticanus and the Vergilius Romanus.\nVirgil is still considered one of the greatest of the Latin poets, and the Aeneid is an integral part of the canon for classical studies programs.\nIn the Middle Ages, Virgil was considered a herald of Christianity for his Eclogue 4 verses (Template:Perseus) concerning the birth of a boy, which were re-read to prophesy Jesus' nativity. The poem may actually refer to the pregnancy of Octavian's wife Scribonia, who in fact gave birth to a girl.\nAlso during the Middle Ages, as Virgil developed into a kind of magus or wizard, manuscripts of the Aeneid were used for divinatory bibliomancy, called the Sortes Virgilianae, in which a line would be selected at random and interpreted in the context of a current situation (Compare the ancient Chinese I Ching). The Old Testament was sometimes used for similar arcane purposes. Even in the Welsh mythof Taliesin, the goddess Cerridwen is reading from the \"Book of Pheryllt\"—that is, Virgil.\nDante mentioned him twice\n- in De vulgari eloquentia , along with Ovid, Lucan and Statius as one of the four regulati poetae (ii, vi, 7)\n- in Inferno ranks him side by side with Homer, Horace, Ovid and Lucan (Inferno IV, 88).\nMore recently, professor Jean-Yves Maleuvre has proposed that Virgil wrote the Aeneid using a \"double writing\" system, in which the first, superficial writing was intended for national audience and Augustus' needs, while the second one, deeper and hidden, unnoticed prior to Maleuvre's discovery, reflected Virgil's true point of view and his true historical reconstruction of the past. Maleuvre also believes that Augustus had Virgil murdered. Maleuvre's ideas have not met with general acceptance.\nThe tomb known as \"Virgil's tomb\" is found at the entrance of an ancient Roman tunnel (also known as \"grotta vecchia\") in the Parco di Virgilio in Piedigrotta, a district two miles from old Naples, near the Mergellina harbor, on the road heading north along the coast to Pozzuoli. The site called Parco Virgiliano is some distance further north along the coast. While Virgil was already the object of literary admiration and veneration before his death, in the following centuries his name became associated with miraculous powers, his tomb the destination of pilgrimages and pagan veneration. The poet himself was said to have created the cave with the fierce power of his intense gaze.\nIt is said that the Chiesa della Santa Maria di Piedigrotta was erected by Church authorities to neutralize this pagan adoration and \"Christianize\" the site. The tomb, however, is a tourist attraction, and still sports a tripod burner originally dedicated to Apollo, bearing witness to the pagan beliefs held by Virgil.\nVirgil's name in English\nIn the Middle Ages \"Vergilius\" was frequently spelled \"Virgilius.\" There are two explanations commonly given for the alteration in the spelling of Virgil's name. One explanation is based on a false etymology associated with the word virgo (maiden in Latin) due to Virgil's excessively \"maiden\"-like (parthenias or παρθηνιας in Greek) modesty. Alternatively, some argue that \"Vergilius\" was altered to \"Virgilius\" by analogy with the Latin virga (wand) due to the magical or prophetic powers attributed to Virgil in the Middle Ages. In an attempt to reconcile his pagan background with the high regard in which his Medieval scholars held him, it was posited that some of his works metaphorically foretold the coming of Christ, hence making him a prophet of sorts. Some scholars defend this view today, such as Richard F. Thomas.\nIn Norman schools (following the French practice), the habit was to anglicize Latin names by dropping their Latin endings, hence \"Virgil.\"\nIn the nineteenth century, some German-trained classicists in the United States suggested modification to \"Vergil,\" as it is closer to his original name, and is also the traditional German spelling. Modern usage permits both, though the Oxford Style Manual recommends Vergilius to avoid confusion with the eighth-century Irish grammarian Virgilius Maro Grammaticus.\nSome post-Renaissance writers liked to affect the sobriquet \"The Swan of Mantua.\"\nList of works\nDates are approximate.\n- (50 B.C.E.) Appendix Vergiliana\n- (37 B.C.E.) Eclogues (or \"Bucolics\"), 10 books\n- (29 B.C.E.) Georgics (or \"On Farming\"), 4 books\n- (19 B.C.E.) Aeneid, 12 books\n- Collected Works\n- Suetonius: The Life of Virgil, an English translation.\n- Vita Vergiliana, Aelius Donatus' Life of Virgil in the original Latin.\n- Virgil.org: Aelius Donatus' Life of Virgil translated into English by David Wilson-Okamura\n- Project Gutenberg edition of Vergil—A Biography, by Tenney Frank.\n- Virgil in Late Antiquity, the Middle Ages, and the Renaissance: an Online Bibliography\n- Virgilmurder (Jean-Yves Maleuvre's website setting forth his theory that Virgil was murdered by Augustus)\n- The Secret History of Virgil, containing a selection on the magical legends and tall tales that circulated about Virgil in the Middle Ages.\nNew World Encyclopedia writers and editors rewrote and completed the Wikipedia article in accordance with New World Encyclopedia standards. This article abides by terms of the Creative Commons CC-by-sa 3.0 License (CC-by-sa), which may be used and disseminated with proper attribution. Credit is due under the terms of this license that can reference both the New World Encyclopedia contributors and the selfless volunteer contributors of the Wikimedia Foundation. To cite this article click here for a list of acceptable citing formats.The history of earlier contributions by wikipedians is accessible to researchers here:\nNote: Some restrictions may apply to use of individual images which are separately licensed."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:9894382e-e644-4dd7-afe3-7d27c3f43df8>","<urn:uuid:d2cb4f2e-e886-4f84-9cdf-6b1394af1ce6>"],"error":null}
{"question":"What is the recommended method for preparing beans in a slow cooker, and what are the key nutrients they provide for immune health?","answer":"Green beans and raw kidney beans must be thoroughly boiled at 212°F before adding to a slow cooker, as they contain Phytohaemagglutinin, a potentially toxic protein. In terms of immune health benefits, beans are valuable sources of protein, which forms immune cells and transporters, and they also contain zinc, which is critical for wound healing and immune system function. They're also part of a healthy diet that can help maintain immune function, along with other plant-based protein sources like peas and soy products.","context":["Homemade chicken noodle soup made in the crock pot for a set-it-and-forget-it easy dinner. You can evan prep all the ingredients ahead of time and store them in the freezer to pull out on a day where you forgot to plan dinner. Just make sure you thaw the ingredients before adding it to the slow cooker to prevent it from staying at an unsafe temperature for too long.\n8 ounces whole-wheat egg noodles or other whole-wheat noodles\n3 pounds bone-in chicken breast, skin removed\n2 cups chopped onion\n1 cup chopped carrot\n1 cup chopped celery\n2 sprigs thyme\n8 cups low-sodium chicken broth\n2 teaspoons kosher salt\n2 cups frozen peas\n¼ cup chopped fresh dill, plus more for garnish\n2 tablespoons lemon juice\nThe adolescent dancer faces unique challenges due to physical and emotional changes that occur during pubertal development. Rapid growth periods can lead to reduced strength, impaired balance, and decreased flexibility, which can alter technical ability and increase the risk of injury.\nGrowth spurts in dancers usually occur between the ages 11-15 in girls and 13-17 in boys, and can last up to two years (IADMS 2000). As height increases, weight gain also occurs. A girl’s menstrual cycle begins during these growth phases and is essential for formation of bone. The pressure to stay thin during periods of weight gain in addition to being unaware of/ignoring nutritional needs results in an energy deficit and increases the likelihood of irregular periods (Delegate 2018). Bones grow at a faster rate than muscles and tendons, and limbs grow at a faster rate than the trunk. This affects strength, flexibility, and balance control in dancers. These changes can make movement feel awkward and may affect your ability to perform at the level that you are used to. Don’t be discouraged, these changes are temporary!\nThe injury rate increases by 35% as dancers reach ages 14-16. Body regions most commonly affected are the foot/ankle, lumbar spine, hips, and knees (Steinberg 2012, Delegate 2018).\nCommon injury types in adolescents:\nREDUCING INJURY RISK DURING GROWTH CHANGES:\n1)Education Committee (Kathryn Daniels, Chair). International Association for Dance Medicine & Science. November 2000 https://www.iadms.org/page/1\n2) Delegete, A. Health Considerations for the Adolescent Dancer. A webinar through the Harkness Center for Dance Injuries. Accessed September 23, 2018.\n3) Steinberg, N., Siev-Ner, I., Peleg, S., Dar, G., Masharawi, Y., Zeev, A., & Hershkovitz, I. (2012). Extrinsic and intrinsic risk factors associated with injuries in young dancers aged 8–16 years. Journal of sports sciences, 30(5), 485-495.\n4) Steinberg, N., Siev-Ner, I., Peleg, S., Dar, G., Masharawi, Y., Zeev, A., & Hershkovitz, I. (2013). Injuries in female dancers aged 8 to 16 years. Journal of athletic training, 48(1), 118-123.\nThe immune system provides protection from seasonal illness such as the common cold as well as other health problems including arthritis, allergies, abnormal cell development and cancers. Dancers are exposed to physical stress from training, which increases susceptibility to illness. Additionally, working in close proximity with other dancers increases exposure to infection. Nutrition plays an important role in maintaining immune function to protect against infection. Learn how to boost your immunity by including these nutrients in your eating plan.\nProteins form many immune cells and transporters. Try to consume a variety of protein foods including seafood, lean meat, poultry, eggs, beans and peas, soy products and unsalted nuts and seeds.\nVitamin A helps regulate immune function and protects from infections by maintaining healthy tissues in skin, mouth, stomach, intestines and respiratory system. Vitamin A is found in foods such as sweet potatoes, carrots, kale, spinach, red bell peppers, apricots, eggs or foods labeled \"vitamin A fortified,\" such as cereal or dairy foods.\nVitamin E works as an antioxidant to neutralize free radicals. Include vitamin E in your diet with fortified cereals, sunflower seeds, almonds, vegetable oils (such as sunflower or safflower oil), hazelnuts and peanut butter.\nVitamin C protects stimulates the formation of antibodies, which are necessary to fight infection. Citrus fruits such as oranges, grapefruit and tangerines, red bell pepper, papaya, strawberries, and tomato juice are good sources of vitamin C.\nZinc is critical for wound healing and aids the immune system. This mineral can be found in lean meat, poultry, seafood, milk, whole grain products, beans, seeds and nuts.\nOther nutrients, including vitamin B6, folate, selenium, iron, as well as prebiotics and probiotics, may also influence immune response.\nAcademy of Nutrition and Dietetics (2017). Protect Your Health with Immune-Boosting Nutrition. Retrieved at eatright.org.","Slow cooking brings out the best flavors of the food. But the low temperature of 165 to 200 degrees Fahrenheit requires cooking time to be considerably longer than traditional roasting or cooking at higher temperatures. A good quality slow cooker is what you need for this purpose, but: how safe is a slow cooker?\nThe convenience of slow cooking does not require permanent attention, there is no stirring necessary and food does not burn in the pot. For most dishes, especially soups and stews, you only have to put all the ingredients together into the cooker, set it and return at the end of the day, to a ready slowly cooked tasty dish.\nHow safe is a slow cooker?\nSome people wonder about the thought of how safe is a slow cooker when leaving it unattended for many hours cooking food in the kitchen. But this is exactly what slow cookers are made for. They do not create as much heat as an open flame or an oven.\n- Make sure you buy a quality slow cooker made by a reputable manufacturer that has product safety certificates.\n- Use the item only as instructed in the manual.\n- Observe common safety precautions like placing the slow cooker on top of a heat-resistant surface away from flammable items.\nSlow cooking is safe as long as you follow the safe cooking instructions. Read the manual of the slow cooker and familiarize yourself with the respective model.\nFoodborne diseases can occur when certain foods such as seafood, poultry or meat are cooked incorrectly. Too low cooking temperatures or too short cooking times may not be sufficient to kill off pathogens. The result could be food poisoning with symptoms like diarrhea and vomiting or worse. In this case, consult a doctor!\nSlow cookers heat food to a temperature of 165 to 200 degrees Fahrenheit. This will slowly simmer the food at just below the boiling point, but at a hot enough temperature to kill off bacteria and germs as recommended by the FDA Safe minimum internal temperatures.\nSo the slow cookers’ lengthy cooking time together with the heat inside the pot will destroy disease-causing bacteria. It is a safe way to prepare food and the results are healthy tasty dishes.\nSome foods’ different characteristics may require special cooking, so it is advisable to learn about the food you are going to cook and how you should safely prepare each ingredient.\nSome green beans and raw kidney beans contain Phytohaemagglutinin, a kind of Protein that can be toxic to humans. Phytohaemagglutinin breaks down at a boiling temperature of 212 F.\nBoil beans thoroughly before putting them into a slow cooker!\nHow to safely slow cook?\n- Always thaw meat before starting to slow cook, don’t use the slow cooker for thawing.\n- Avoid putting raw meat into the slow cooker and have it sit for delayed timer cooking.\n- Don’t open the lid to check on the food too often, every time the lid is opened add some cooking time.\n- Limit the keep warm function to a maximum of 3 to 4 hours. After that put leftover food into the fridge.\n- Do not use your slow cooker for warming up dishes.\nKitchen Hygiene and Safety\n- Handling food carefully can reduce the risk of foodborne illness.\n- Germs such as Salmonella, Listeria, Campylobacter, or other microorganisms are spread when beef, pork, poultry, and seafood are not stored properly.\n- Particularly small children, pregnant women, the elderly, and people with a weakened immune system may experience health complications such as sour stomach and intestinal symptoms as a result.\n- To enjoy fish, poultry, meat, and egg dishes without fear, a few simple rules should be considered when buying, storing, and preparing food.\nMainly raw or inadequately cooked meat and poultry dishes, seafood, raw eggs, and delicatessen salads can become a health risk when not handled properly and could provide an ideal breeding ground for microorganisms. However, germs can also be transferred from plant foods, for example from ground-grown vegetables, salads or berries.\nPurchase and storage\nThe cold chain must not be interrupted by shopping for perishable food. These products should quickly go into an icebox and at home into the fridge or freezer. Store delicate foods such as ground beef at a temperature below 39° Fahrenheit in the refrigerator and consume before the expiry date. Raw, unheated food should be stored separately from already prepared or cooked food.\nA temperature below 39°F is best to discourage bacterial growth in refrigerated foods.\nFrozen meat, fish, or poultry that is left in the refrigerator to thaw, should be put in a strainer. This prevents the food to be in contact with the thawing water which is a breeding ground for germs. Care should also be taken to ensure the thawing water does not get in contact with other food.\nMinced meat, pork, poultry, and fish must be thoroughly cooked until the meat is no longer red, raw or bloody inside. Food containing raw eggs or mayonnaise, as well as cold dishes, should be left in the refrigerator for a maximum of 24 hours after preparation.\nKitchen and workplace hygiene\nAvoid Cross-Contamination; When handling any kind of food, washing hands often is the most important between workflows. Work surfaces, cutting boards, and utensils that were in contact with poultry, meat, and seafood should be cleaned immediately with hot water and dishwashing detergent. The best is to use separate cutting boards and plates.\nCloths, dishwashing brushes, and kitchen sponges should be changed regularly and cleaning aids should be washed with hot water. To sanitize kitchen utensils it is best to soak them at a temperature of 170 degrees Fahrenheit for at least 30 seconds.\nFive basic hygienic rules\nThe World Health Organization (WHO) advises Five Keys to Safe Food\n- Keep hands and work surfaces clean in the kitchen\n- Separate raw and cooked food\n- Cook food carefully\n- Store food at “safe” temperatures\n- Use clean water and clean initial products"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:85785016-bfef-4557-a453-501ded74570b>","<urn:uuid:e2c82672-7ad7-48dd-8ce8-6ce03d611bb0>"],"error":null}
{"question":"what background Ryan Edwards have? im student looking for creative career inspiration","answer":"Ryan Edwards has a degree in performance from Berklee College of Music, completed an apprenticeship in traditional music, dance and instrument-building in Guinea, West Africa, and has worked with public and performing artists in New England. As a creative professional, he manages multiple roles including marketing, finance, grant writing, human resources, and maintaining time for creative development of new works.","context":["Interactive Sculpture at Lamont Gallery\nOn Friday, November 17, 2017 the Lamont Gallery welcomed Masary Studios’ Sound Sculpture for a special one day, community participatory exhibition: Illuminating Bodies. This event coincided with the opening reception for the Possible Subject Positions exhibition.\nThe day started just like any other – until Ryan Edwards, One of the artistic directors of Masary Studios and co-creator of the Sound Sculpture arrived with a van full of 17” cubes made of poly-ethylene plastic and a plethora of electronic gear for this interactive piece. After unloading the van Ryan began setting up the cubes and equipment all the while good-naturedly explaining the process and the piece to curious gallery visitors.\nOf course, we also took the opportunity to have some fun with the blocks as well!\nLunch and Learn\nThe gallery hosted a luncheon lecture with Ryan so our community could learn more about the piece as well as the other work that Masary Studios has done. The audience was made up of a wonderful mix of PEA students, faculty, and staff as well as members from the local community and Merill Comeau, one of the artists exhibiting in Possible Subject Positions. The conversation was largely directed by some excellent student questions. One of our student gallery proctors, Wendi Yan, introduced Ryan and talked about her experience seeing one of Masary Studios’ pieces during the Illuminous festival in Boston, Massachusetts.\nRyan talked about the path that brought him to where he is now, which includes a degree in performance from Berklee College of Music, an apprenticeship in traditional music, dance and instrument-building in Guinea, West Africa, and many opportunities working with public and performing artists in New England.\nRyan also shared with us the realties and rewards of making a living doing his work and what he has learned from other artists working in the field. According to Ryan, one important skill required for successfully working in a creative career is the ability to “wear many hats.” Some of the “hats” that Ryan wears as an artist and performer include: marketing, finance, submitting grant proposals, acting as a human resources director to hire people for specific aspects of projects, and most importantly carving out time to work on the creative process of developing new works.\nOne of the projects, in addition to the Sound Sculpture, that Ryan shared with the group was Waking the Monster, a piece that transformed a Boston icon: Fenway Parks’ Green Monster. Ryan explained how the space was transformed with video projections, lights, performers and six musical compositions that turned the “Monster” as an instrument. This led to a discussion about the importance of public art and interactive art in community spaces and how art can help boost the economies of towns and cities as well as enrich the lives of the individuals experiencing it.\nThe last question of the luncheon was asked by Wendi – our student proctor who introduced Ryan – she asked if he could do any project, and money and time was not an object, what would his next project be. Ryan became pretty excited and went on the describe creating a large-scale Sound Sculpture performance the size of a football field with 200 blocks and a large group of dancers from local dance companies performing a choreographed routine with the blocks. By the end of the luncheon we all decided that we would go see this project when it becomes a reality!\nIlluminated Bodies and the Sound Sculpture in Action\nAfter the luncheon, the Sound Sculpture was ready for our community to interact with it. The 25 blocks were in the center of the gallery and the lights were dimmed.\nRyan Edwards explains the details of how this interactive piece works:\n“Sound Sculpture is an interactive sound and light set of building blocks that produce sound…. Each block has a positioning tag inside, a battery, a router, and an LED light array.\nParticipants place the blocks throughout a designated area. When activated, the\ncontrolling software sends a ‘bang’ to the blocks in sequence – from one end of\nthe area to the other. In this way ‘musical time’ is viewable as the bricks light\nsequentially and make their corresponding sound. Music! Colored Light!\nAs participants change the order and orientation of the blocks, the musical\ncomposition changes accordingly. Certain pre-set programs might correlate\nmusical pitch to position as well.\nEach ‘bang’ or instance where a block lights up and triggers a sound is also an\nevent in color.”\nThis event drew many students, PEA employees, and children of all ages and adults from the Seacoast community to the Lamont Gallery. Ryan was on hand all day to talk with visitors about the technical aspects of the piece.\nThe Sound Sculpture was in constant action as participants stacked and moved the blocks created musical and visual compositions. By the end of the evening people were building forts, towers, and even thrones with the blocks. There was also a platform reminiscent of a disco-era dance floor created by a few of our smallest participants.\nWe also couldn’t help but notice the interesting interplay between these blocks and the shapes and colors of some of the pieces in the Possible Subject Positions exhibition.\nWe are grateful to Ryan Edwards for sharing the Sound Sculpture and his time and knowledge with our community.\n– Stacey Durand, Lamont Gallery Manager"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:8049791c-e09b-445d-99d8-12bd63ffbbc7>"],"error":null}
{"question":"电力系统中的中央控制和分散控制之间如何权衡，以及这对系统安全性有什么影响？","answer":"The power system faces a fundamental trade-off between centralization and decentralization of control. While centralized control through regional transmission organizations increases efficiency and improves customer service, it can be exploited by terrorists as a vulnerability. The optimal approach requires bridging simultaneous top-down and bottom-up decision-making in real time - maintaining fast-acting and totally distributed control at the local level while ensuring coordination at the mid-level. This is reflected in the state estimation algorithms, where decoupled equations allow for separate estimation of voltage angles and magnitudes, but complete decoupling may not yield optimal solutions. The system must balance the benefits of wider grid integration with the increased complexity and security risks it brings.","context":["Static State Estimation in Power Systems:\nStatic State Estimation in Power Systems – As noted earlier, for a system with N buses, the state vector x may be defined as the 2N — 1 vector of the N — 1 voltage angles δ2, …, δN and the N voltage magnitudes V1, V2, …, Vn. The load flow data, depending on type of bus, are generally corrupted by noise and the problem is that of processing an adequate set of available data in order to estimate the State Estimation vector. The readily available data may not provide enough redundancy (the large geographical area over which the system is spread often prohibits the telemetering of all the available measurements to the central computing station). The redundancy factor, defined as the ratio min should have a value in the range 1.5 to 2.8 in order that the computed value of the state may have the desired accuracy. It may be necessary to include the data for the power flows in both the directions of some of the tie lines in order to increase the redundancy factor. In fact, some `psuedo measurements’ which represent the computed values of such quantities as the active and reactive injections at some remote buses may also be included in the vector y (k).\nIt is thus apparent that the problem of estimation of the power system state is a non-linear problem and may be solved using either the batch processing or sequential processing formula. Also, if the system is assumed to have reached a steady-state condition, the voltage angles and magnitudes would remain more or less constant. To develop explicit solutions, it is necessary to start by noting the exact forms of the model equations for the components of the vector y (k).\nLet Pi and Qi denote the active and reactive power injections of ith bus. These are related to the components of the state vector through the following equations.\nwhere |Yij| represents the magnitude and θij represents the angle of the admittance of the line connecting the ith and jth buses. The active and reactive components of the power flow from the ith to the jth bus, on the other hand are given by the following relations.\nLet us assume that the vector y has the general form\nThe Jacobian H will then have the form\nwhere IN is the identity matrix of dimension N, H1 is the N x (N — 1) submatrix of the partial derivatives of the active power injections wrt δ′s, H2 is the N x N sub-matrix of the partial-derivatives of the active power injections wrt |V|s is and so on. Jacobian H will also be a sparse matrix since Y is a sparse matrix.\nTwo special cases of interest are those corresponding to the use of only the active and reactive injections and the use of only the active and reactive line flows in the vector y. In the first case, there are a total of 2N components of y compared to the 2N — 1 components of the state x. There is thus almost no redundancy of measurements. However, this case is very close to the case of load flow analysis and therefore provides a good measure of the relative strengths of the methods of load flow and Static State Estimation. In the second case, it is possible to ensure a good enough redundancy if there are enough tie lines in the system. One can obtain two measurements using two separate meters at the two ends of a single tie-line. Since these two data should have equal magnitudes but opposite signs, this arrangement also provides with a ready check of meter malfunctioning.\nThe Injections Only Algorithm:\nIn this case, the model equation has the form\nwith the components of the non-linear function given by\nThe elements of the sub-matrices H1, H2, H3 and H4 are then determined easily as follows.\nEquation (14.33) may be used to determine the Jacobian at any specified value of the system state vector. The injections only state estimation algorithm is then obtained directly. Since the problem is non-linear, it is convenient to employ the iterative algorithm given in Eq. (14.22).\nApplying the principle of decoupling, the submatrices H2 and H3 become null with the result that the linearized model equation may be approximated as:\nIf we partition the vectors y, x and r as\nthen, Eq. (14.34) may be rewritten in the decoupled form as the following two separate equations for the two partitioned components of the state vector\nBased on these two equations, we obtain the following nearly decoupled Static State Estimation algorithms.\nwhere the subscripts p and q are used to indicate the partitions of the weighting matrix W and the non-linear function h (.) which correspond to the vectors yp and yq respectively. As mentioned earlier, if the covariances Rp and Rq of the errors rp and rq are assumed known, one should select Wp= R-1p and Wq= R-1q.\nNote that Eqs. (14.37) and (14.38) are not truly decoupled because the partitions of the non-linear function depend on the estimate of the entire state vector. It may be possible to assume that νi (j) = 1 for all i and j while, Eq. (14.37) is being used in order to estimate the angle part of the state vector. Similarly one may assume δi(j) = 0° for all i and j while using Eq. (14.38) in order to estimate the voltage part of the state vector. Such approximations allow the two equations to be completely decoupled but may not yield very good solutions. A better way to decouple the two equations would be to use the load flow solutions for xν and xδ as their supposedly constant values in Eq. (14.37) and Eq. (14.38) respectively. There are several forms of fast decoupled estimation algorithms based on such considerations. A flow chart for one scheme of fast decoupled Static State Estimation in shown in Fig. 14.2.","Guaranteeing the Security of an Increasingly Stressed Grid\n- Written by Massoud Amin\nGrid communications and control systems are often thought to be much more securely firewalled than is actually the case. An effective remedial program begins with a thorough and clear-eyed assessment of all vulnerabilities; especially to intrusion via the Internet. Creation of an adequate security system requires a management culture that demands and respects attentiveness to security among all employees.\nBoth the importance and difficulty of protecting power systems have long been recognized, as noted in the January 2011 issue of this newsletter. The critical assets that must be accounted for include thousands of transformers, line reactors, series capacitors, and transmission lines. Physical protection of these widely diverse and dispersed assets is impractical and command control layers yield new benefits only if designed correctly and securely, posing additional challenges.\nAs a practical matter, electrical systems must be structured to withstand temporary loss of physical components, just as they must operate through spontaneous local outages; but that depends on defending control and communications against cyber attacks, be they terroristic, military, or criminal.\nAt the end of the 1990s, partly in reaction to cyber attacks in Brazil’s power system and partly in anticipation of Y2K, the Electric Power Research Institute launched a two-year Infrastructure Security Initiative. Before and after 9/11 (when I became responsible for research and development on infrastructure security at EPRI), we reached out to utilities and their vendors to share relevant information and develop action plans. We conducted \"red team\" studies of simulated cyber attacks on a variety of grid assets and developed protocols for secure communications between control centers, substations, and power plants.\nElectric power utilities typically own and operate at least parts of their own telecommunications system—oftentimes backbone fiber optic or microwave networks connecting major substations with spurs to smaller sites. We found that this can give rise to a false sense of security. PCs, for example, can be a vulnerable point when they link dedicated communications with operations control. And sometimes modem access could be readily attained using obvious passwords. Yet we saw reports and files claiming that systems were bulletproof.\nConsider the following conversation with two operators at a typical, multi-screen layout we had a couple of years ago concerning a five-year-old, gas-fired combined cycle power plant of 200–250 MW:\nDo you worry about cyber threats?\nNo, we are completely disconnected from the Internet.\nThat’s great! But this is a peaking unit. How do you know how much power to make?\nThe office receives an order from the ISO, then sends it over to us. We get the message here on this screen.\nIsn’t that message coming in over the Internet?\nSure, we can see all the ISO to company traffic… Oh! That’s not good, is it?\nAny telecommunications link that is even partly outside the control of the organization that owns and operates power plants, supervisory control and data acquisition (SCADA) systems, or energy management systems represents a potential pathway into the business operations of the company and a threat to the larger transmission grid. And as the number of documented intrusions and their level of sophistication continue to rise, it has become apparent that human response is often neither fast enough nor smart enough to effectively counter malicious code and denial-of-service attacks.\nA vivid example of just how insidious such attacks can be came last year with the Stuxnet malware, which evidently targeted Iran’s uranium enrichment operations. It did so by infiltrating standard SCADA and industrial control systems widely used in power systems and manufacturing facilities. Constructed in nested shells like a Russian doll, Stuxnet’s innermost \"payload\" ultimately took over logic controllers, so as to modify the way equipment ran, while remaining completely invisible to system operators. Control room screens would indicate normal operations, while machines spun wildly out of control and self-destructed.\nBefore Stuxnet had been fully analyzed, first by German cyber security experts, then by others who largely confirmed their conclusions, its outer shells had showed up in power systems and plants around the world. Though it appears to have done no damage (as intended), it showed just how vulnerable widely used control systems can be to highly sophisticated malware.\nLike any complex, dynamic infrastructure system, the electricity grid has many layers and is vulnerable to many different types of disturbances. While strong centralized control is essential to reliable operations, this requires multiple, high-data-rate, two-way communication links, a powerful central computing facility and an elaborate operations control center, all of which are especially vulnerable when they are needed most—during serious system stresses or power disruptions. For deeper protection, intelligent, distributed, secure control is also required, which would enable parts of the network to remain operational and even automatically reconfigure in the event of local failures or threats of failure.\nAccordingly, protection of the system requires a multi-layered effort. At the highest level of generality there needs to be a corporate culture that insists on adherence to procedures, visibly promotes better security, and sees that management is well informed. The security program must be up-to-date, complete, closely supervised, and must include vulnerability and risk assessments. Of course, employees must be screened and trained and emergency procedures must be rehearsed and drilled.\nPhysical assets need to be evaluated in terms of criticality and vulnerability to intrusion. IT security must include protection of wired and wireless networks and assessment of firewalls and process control systems, among other things. Very high-level mathematical modeling is required to guard against false data injection, detection of stealth attacks, risk estimation and impact analysis.\nAt the same time, as security programs are built and protections put into place, difficult choices will have to be made about how to handle a number of trade-offs.\n- Security versus efficiency. The specter of future sophisticated terrorist attacks raises a profound dilemma for the electric power industry, which must make the electricity infrastructure more secure, while being careful not to compromise productivity. Resolving this dilemma will require both short-term and long-term technology development and deployment that will affect fundamental power system characteristics.\n- Centralization versus decentralization of control. For several years, there has been a trend toward centralizing control of electric power systems. The emergence of regional transmission organizations, for example, promises to greatly increase efficiency and improve customer service. But we also know that terrorists can exploit the weaknesses of centralized control; therefore, smaller and local would seem to be the system configuration of choice. In fact, strength and resilience in the face of attack will increasingly require the ability to bridge simultaneous top-down and bottom-up decision-making in real time—fast-acting and totally distributed at the local level, coordinated at the mid-level.\n- Wider grid integration and increasing complexity. System integration helps move power more efficiently over long distances and provides redundancy to ensure reliable service but it also makes the system more complex and harder to operate. We will need new mathematical approaches to simplify the operation of complex power systems and make them more robust in the face of natural or manmade interruptions.\n- Dependence on Internet communications. Today’s power systems could not operate without tightly knit communications capabilities—ranging from high-speed data transfer among control centers to the interpretation of intermittent signals from remote sensors. But due to the vulnerability of Internet-linked communications, protecting the electricity supply system will require new technology to improve the security of power-system command, control and communications, including both hardware and software.\n- Investments in security. Although hardening some key components—such as power plants and critical substations—is highly desirable, providing comprehensive physical protection for all components is simply not feasible or economical. Dynamic, probabilistic risk assessments have provided strategic guidance on allocating security resources to greatest advantage.\nFortunately, the same core technologies that were developed to address other system vulnerabilities can strategically improve system security as well. But the electricity infrastructure will also require power system-specific advanced technology. Assuming that individual utilities are already taking prudent steps to improve their physical security, technology can help by increasing the inherent resilience and flexibility of power systems to withstand terrorist attacks as well as natural disasters."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:9d392b2a-2dc8-4ddb-8ac0-aff773b471cd>","<urn:uuid:5d68b378-aa0b-4ff0-bb01-acb547fa1db1>"],"error":null}
{"question":"I'm a teacher struggling with difficult students - how can I use self-management interventions to help them control their behavior?","answer":"Self-management interventions teach students to observe, record, and reinforce their own behavior through three main components: 1) Self-monitoring: Students become aware of their behavior and record data about it, which helps increase awareness of social skill issues and reduce targeted behaviors. 2) Self-evaluation: Students compare their behaviors against a preset standard - they must be taught how to compare monitored behaviors to criteria and evaluate their performance. Initially, teachers set daily goals and implement systematic reinforcement, gradually fading their involvement. 3) Self-reinforcement: Students receive preferred objects, privileges or positive reinforcement as their behavior improves, with teacher involvement decreasing as students become more independent in managing their behaviors.","context":["Students arrive at school often with more baggage than contained in the backpack. Feelings of worthlessness, low-self concept, inadequate coping mechanisms necessary to engage in socially appropriate ways; and being ill equipped to handle authority may lurk in the recess of the students mind.\nStudents arrive at school often with more baggage than contained in the backpack. Feelings of worthlessness, low-self concept, inadequate coping mechanisms necessary to engage in socially appropriate ways; and being ill equipped to handle authority may lurk in the recess of the students mind. Carrying such a heavy load may well cause the student to react socially inappropriately by acting out, refusing to follow the rules, becoming violent or withdrawing (Yell, Meadows, Drasgow, & Shriner, 2009). Students exhibiting such behaviors may be labeled emotionally behaviorally disordered (EBD) and require interventions for remediation. Under current federal law schools must use models with proven methods of successful outcomes (Department of Education, 2004). The cognitive behavioral approach (CBA) is one such proven model that provides procedures for intervention plans (Yell, et al., 2009). This paper serves to inform the reader regarding the procedures used in the CBA model as well as the components of cognitive behavioral intervention (CBI) and the interrelationship between thoughts; emotions and behaviors. Understanding the influence one component has on the others, the effectiveness and limitation of CBA and the value of social skill training will be included. Finally, important considerations for developing curriculum, approaches and assessments will be revealed.\nThe responsibility of schools is to educate children so they may be valued members in society equipped to work, recreate and contribute to society. When children attend school with a load of emotional stressors or lack of skills necessary to work and play well with others, he or she may need CBI to assist or change the way difficult situations are handled, and to promote a healthy learning environment. CBI’s primary goal is to teach students how to manage their academic and nonacademic behaviors (Yell, et al., 2009). Self-instruction (or management) and verbal mediation are two procedures used in CBI to control behavior. Self-management interventions teach the student how to observe record and reinforce their own behavior. Three components to self-management interventions are self-monitoring, self-evaluation and self reinforcement. Self-monitoring has been used to successfully improve social skills behaviors using two facets; first being aware of the behavior and then being able to record data on the behavior. By forcing the student to monitor their own behavior, student awareness around social skill issues increase and targeted behaviors are reduced (2009). Self-evaluation requires the student to compare his or her behaviors against a standard. Self-monitoring as prerequisite for this strategy and students must be taught how to “(a) compare the behaviors they monitored to a preset criterion, and (b) evaluate their performance” (p.152).Initially the teacher and student set daily goals and systematic reinforcement is implemented with fading by the teacher as the student becomes accurate and independent in self evaluation. Self-reinforcement is a traditional behavior modification program whereby the students gets a preferred object, privilege or some form of positive reinforcement as the student behavior improves. Teacher involvement should fade out as student achievement increases and the student becomes more able to manage his or her behaviors (2009). The operative word through the self-management interventions is self. Students learning how to recognize analyze and change behaviors themselves under the guidance or facilitation of educational professionals. Verbal Mediation is another technique empowering the student to change behavior.\nVerbal mediation–based interventions are basically positive self-talk phrases students are taught to overcome the negative deficient or maladaptive self- statements (2009) ruminating in his or her head. According to Yell et al. (2009) verbal –mediation based approaches include self-instructional training in which students are taught verbal prompts to help them make choices that affect non verbal behavior, problem-solving training which teaches the student to recognize, define, generate alternative solutions, evaluate those solutions and implement a plan to work through the conflict; anger-control training where students are taught how to give themselves self-instructions to facilitate the use of anger control procedures; and finally, the student monitors the solution to ensure quality results (2009).\nMonitoring behaviors for students with EBD is a complex and often difficult process. Emotions, thoughts and behaviors are components of the student’s personality that have been developed and nurtured in some form from inception. Rational emotive behavior therapy (REBT) zeros in on the cognitive and affective domains in order to teach students how to better manage thoughts, feelings and consequently behaviors. Yell et al. (2009) outline the how these attributes interrelate and influence each other. Often students think in the present and react rather than think through a difficult situation. In REBT the student is taught that his or her behavior is somewhat hinged on a culmination of environmental experiences that have lead to the present circumstance. Personal problems are the result of perceptions that have evolved from the thinking, feeling, and behaving that individuals have experienced throughout their lives “(p.148). Furthermore, individuals cannot control events, people will always have personal problems and the problem is not the event itself but rather the students perception of the event that contributes to the student feeling, emotions the by product of which is behavior. Sometimes the environmental or biological factors have influenced the student over time in ways that have reinforced harmful or dysfunctional thinking and cognitive distortions. This may result in irrational and exaggerated thinking on the part of the student leading to a belief that his or her life is ruined due to a crisis. Although irrational thinking promotes harmful emotions and dysfunctional behaviors the cause of the irrational thinking lies in the student adhering to a “set of rigid and absolute beliefs” (p. 148). REBT proponents believe students can change thinking and thought processes to be able to see life’s challenges as manageable and gain improved quality of life (2009). Additionally, social skills training is an adjunct means designed to assist students with maladaptive behaviors to “interact effectively with others and avoid or escape socially unacceptable behaviors exhibited by others” (p.156).\nSocial skills instruction does not enjoy a stellar track record for improving student social skills across domains. Several reasons have been reported with results describing an overall lack of understanding that curriculum must be relevant and meaningful to the student, that students may learn a skill but not be able to perform or generalize that skill across domains, social skill trainings typically were taught in isolated settings rather than with a comprehensive approach, many curricula were designed for a targeted population of students, but were delivered to the total EBD population with less than effective outcomes; and at times the targeted goals were selected inappropriately and did not serve the needs of the student which was to replace maladaptive behaviors with socially appropriate adaptive behaviors (2009). Although the limitations of afore mentioned social skills training may seem bleak and worrisome toward helping this population of struggling students much has been learned. Programs with curriculum aligned to meet the needs of these students now exist providing socially valid outcomes. Best practice criteria for students with EBD encourage social skills training to incorporate a comprehensive approach including: “(1) social skills assessments; (2) assumptions of social skills instruction; (3) structured learning approach; (4) teaching of pre-social skills; (5) social skills strategy instruction; (6) published social skills curricula” (p. 157). Completing Functional Behavioral Assessment (FBA) is the first step in generating a social skills training program for the student (Wheeler & Richey, 2010). The FBA consists of gathering information or data concerning the behavior needing attention. This can be done by structured interview or using a behavior rating scale. Next the teacher or other educational professionals will conduct behavioral observations looking for antecedents and consequences that are influencing behaviors as well as identifying any behavior patterns that have developed. Once this information has been accumulated across domains and carefully reviewed, the teacher will form a hypotheses related to the functions(s) of the behavior which may include a functional analysis with the aim of confirming the function of the behavior.\nPrinciples guiding the effective teaching of social skills require a direct, structured approach to teaching (CITEd research center, www.cited.org. 5/10.11). Maintaining a balance between structure and activity in a classroom can be challenging for the teacher when planning for and implementing social skills training. The Center for Development and Learning (CDL) offers a Best Practices list for the classroom teacher. The list invites teachers to check off his or her strengths, identifying five strengths and 5 weaknesses, and creating a plan to improve on his or her weak areas. The ten categories on the list include “classroom management, physical facilities, student voice and involvement, activities and assignments, language and communication, time allocations; time allocations are balanced between teacher-directed and student–directed work, subject specific lessons individual and small group” (p.1) intensive subject exploration; student work and assessment, teacher attitude and initiative toward students and toward self (n.d.). Armed with self directed knowledge about his or her own classroom management strong points and those targeted for improvement, the teacher is prepared to meet the challenges ahead. As there is not always the perfect program or curricula for each student, the teacher must teach to the student relying on relevant, contextual teaching strategies, using language that is meaningful to the student and cooperative learning opportunities (Yell et al. 2009).\nEvery day students walk through the doors of the school building and into the classroom a backpack slung over their shoulder. In front of the class stands a teacher willing and wanting to impart knowledge designed to assist the students in learning life’s lessons. The effective, intelligent and professional teacher is girded with the best curricula and the most winning attitude toward the individual potential of each and every student. Ready to tackle what ever is hiding or falling out of that backpack."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:873e460e-9782-49b2-a14f-49c7dc5f6f3c>"],"error":null}
{"question":"What historical events led to the fall of Kirtipur in 1767?","answer":"In 1767, Kirtipur fell to Prithvi Narayan Shah after two attacks and a six-month siege. Despite promises of amnesty upon surrender, Prithvi Shah ordered his troops to cut off the noses and lips of every man and boy in Kirtipur, sparing only those skilled in playing wind instruments. This atrocity was intended to demoralize opposition in the Kathmandu Valley, which fell within a year after Kirtipur's capture.","context":["Kirtipur is an ancient city of Nepal. Once-proud KIRTIPUR (“City of Glory”) occupies a long, low battleship of a ridge 5km southwest of Kathmandu. Commanding a panoramic – not to mention strategic – view of the valley, the well-preserved old town is vehicle-free and great for a morning or afternoon’s wandering. It also conceals, deep within its miniature maze of brick and stone streets, one of the best Newari restaurants in the valley.\nIt is one of the most famous and religious places to visit. Anyone who admires a location with rich history, good food and interesting structures will love for sure.\nEstablished as a western outpost of Patan in the twelfth century, Kirtipur had gained nominal independence by the time Prithvi Narayan Shah began his final conquest of the Kathmandu Valley in 1767. The Gorkha king considered the town to be the military linchpin of the valley and made its capture his first priority.\nAfter two separate attacks and a six-month siege, with no help forthcoming from Patan, Kirtipur surrendered on the understanding it would receive a total amnesty. Instead, in an atrocity intended to demoralize the remaining opposition in the valley, Prithvi Shah ordered his troops to cut off the noses and lips of every man and boy in Kirtipur.\nSupposedly, only men skilled in the playing of wind instruments were spared. The rest of the valley fell within a year.\nPlaces to visit in Kirtipur\n- Bagh Bhairab Temple\nA history attached with the temple. Hundreds of years old temple with good views of Kathmandu valley. One of the oldest temple of Kathmandu Valley, made in early 12th century is the master piece of the city in the Hill Top. So, this temple, one of the hidden jewel of Kathmandu valley.\n- The Uma Maheshwar Temple.\nThe Uma Maheshwar Temple is a three-storied pagoda style temple built in 1655 A.D. Lord Shiva and Parvati are the main deity in this temple with the images of other like Saraswati and Mahismardini. The doors, pillars and the beams demonstrate the art work of different gods and goddess. The locals call the temple Kwacho Dege and are one of the most important sites in Kirtipur. The temple lies on the top of the hill thus offers a beautiful view of the Kathmandu valley, several mountains like Langtang, Dorge Lakkpa, Chobhu Bhamure, and Gaurisankar.\n- Champa Devi Hill\nThe Champa Devi Hill is a magnificent hill at an elevation of 2,285 meters. The Hill is a famous hiking destination in Kirtipur especially during the New Year. At the top of the hill, there are Buddhist and Hindu shrines. It is one of the best, easy, short and enjoyable hiking destination which will take about 4 hours to reach the top. It will be a wonderful experience to walk between the pine forest in a well developed trial with the superb view of the Langtang, Gaurisankar, Jugal, Dorge Lakpa and many other mountains.\n- Shri Kirti Bihar\nThe Shri Kirti Bihar is situated at the bottom of Kirtipur was built in 1975. The main temple at the bottom is dedicated to the Lord Buddha and to its right. There is a small building which consists of several Buddhist paintings.\n- Chilancho Bahal, Kirtipur\nChilancho Bahal is an old Buddhist Stupa located in Kirtipur town around 8 kms from Kathmandu city. This is one of the stupas in Nepal where the brickwork is not covered with plaster. The atmosphere around the stupa is serene. It was supposedly built during the time of Ashoka, the emperor who converted to Buddhism.\nTaste Authentic Newari Food\nRestaurants in the area have not developed much yet, so you really should not expect anything to be open in the off-peak season. However, Newa Lahana serves some genuine Newari food, and visitors claim that one must try their authentic local cuisine when in Kirtipur.\nHow To Reach Kirtipur\nGetting to Kirtipur is fairly easy. Buses leave for the area from Ratna Bus Park in Kathmandu after every 30 minutes. The bus fare is NRS 25, and you should reach your destination in 25 minutes. Alternatively, you can hire a car from Thamel to Naya Bazaar and back to Thamel for NRS 2000. The journey by taxi would take around 15 minutes in all. Cycling to and back from the town is also a feasible option for the adventure lovers and fitness enthusiasts.\nKirtipur Day Tours and Sightseeing\nKirtipur Tour with Newari food tasting Trip\nKirtipur is one of several ancient kingdoms in the Kathmandu Valley that were once independent, but now make up part of greater Kathmandu. It’s a prime destination to learn about the Newar people, who are one of Nepal’s many distinct ethnic groups and the original inhabitants of the Kathmandu Valley. Come to Kirtipur for exquisite architecture, hilltop temples, and sweeping city views. Book half day sightseeing tours in Nepal."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:80fb3bb2-b6ef-45c3-bf45-973954081289>"],"error":null}
{"question":"¿Como documentalista, me interesa saber about Northern California's historical footage and scientific breakthroughs - what archives exist from the 1960s, and what agricultural experiments made history?","answer":"The San Francisco Bay Area Television Archive maintains thousands of hours of 1960s footage from major Bay Area broadcasters, including nine videos specifically about Sonoma County. These include documentaries like 'Portrait of Pleasure' (1963) and agricultural footage like 'The Story of the Grape' (1962). A significant agricultural breakthrough came later when Dr. Stephen Lindow developed the ice-minus strain of P. syringae bacteria through recombinant DNA technology. This became the first genetically modified organism released into the environment in 1987 in California, aimed at protecting crops from frost damage. Despite promising results in protecting strawberry and potato crops, the experiments were controversial and faced significant opposition from environmental groups.","context":["Want to remember (or discover) what Sonoma County looked like in the 1960s? You're in luck; there's an online archive of documentaries and news footage shot in that decade.\nSadly, it's hard to find unless you know where to look - since this material is not available on YouTube, Google does not list any of it in video search results, even if the exact title is entered. Overall there are thousands of hours in those archives and they are a treasure for anyone interested in what happened in Northern California in the latter part of the Twentieth Century. Here are discussed only those most relevant to Sonoma County, along with a bonus section looking at other obscure - and sometimes bizarre - historical videos per our corner of the woods.\nThe videos are part of the San Francisco Bay Area Television Archive, which sweeps together decades of leavings from major Bay Area TV broadcasters. Maintained by San Francisco State University, the older stuff offers a glimpse of a time we'll never see again - and by that I mean an era when local stations produced educational programs to show they were operating in the pubic's interest, an FCC requirement for having a commercial license. These programs usually aired during the off-off-hours on Sundays and were produced on a zero budget except for film and a little staff/studio time. Quality varies greatly, as one might expect; the footage is sometimes puzzling or as awful as an unedited home movie, but there are also treasures such as the 1953 interview with Frank Lloyd Wright, where he talks for half an hour about architecture and his proposed design for the \"Southern Crossing\" (a companion to the Bay Bridge which was never built).\nThere are nine videos in the archive related to Sonoma County. Here are summaries and links to most of them:\nPORTRAIT OF PLEASURE (1963) is a half-hour travelogue on Sonoma County. Like most of these amateurish documentaries, the script doesn't add much and is sometimes misleading or outright wrong - apparently Burbank's home and gardens are somewhere near Bodega. But, hey, we're here for the pictures, right? Highlights are the aerial shots near the beginning, scenes of downtown Sebastopol and views of vintage (not then!) cars on Hwy. 101. It was county fair season so there's a long segment which won't be very interesting unless there's someone you know slurping ice cream or watching the cattle judging. Unfortunately, that time wasted on boilerplate footage of carnival rides cuts into most coverage of Santa Rosa, which is limited to a few downtown scenes. The streets are surprisingly empty; the only person clearly seen is on Fourth Street - and surprise, it's a cop writing a parking ticket. Was ever thus.\nLUTHER BURBANK: A GARDENER OF EDEN (1967) is mostly a generic Burbank bio but it contains some footage I have never seen elsewhere, including film taken during his funeral. Other highlights include a few views of a Rose Parade (I'm guessing c. 1920) and a two minute interview with Judge Hilliard Comstock that begins at the 13:30 mark. Hilliard reminisces about working for him as a teenager and the time Burbank scolded him for eating on the job.\nTHE STORY OF THE GRAPE (1962) covers that year's grape harvest and crush in the manner of an industrial video along with some sketchy history of the Buena Vista winery. The best parts are the many aerial views of Sonoma Valley and a full segment on the 16th Valley of the Moon Vintage Festival, including the children's parade. There are some nice shots of the Sonoma Plaza and surrounding streets during the festival.\nHIGHWAY THROUGH TIME (1965) A drive up Highway 1; the Sonoma County stretch begins at 19:55.\nEARTHQUAKE IN SANTA ROSA A ten minute news segment from KPIX on the aftermath of the 1969 earthquake. Includes a short press conference by Police Chief Melvin (Dutch) Flohr, images of downtown and residential damage and the cleanup at the Fourth street Safeway.\nOther material in the archive includes an interview with a PG&E spokesman on their intent to build a nuclear power plant at Bodega Head (1963) and a story on Bobby Kennedy visiting a school on the Kashia Reservation (1968). There is also a Jack London documentary which is too dark to watch and has a particularly bad script.\nThese videos either were made before 1960 or come from sources other than the Bay Area Television Archive. Some of these I have found over the years, lost, then found again, so it saves me grief to offer them all in one place.\nTHE INNOCENT FAIR is all footage from the Panama-Pacific International Exposition of 1915. The film has a mysterious past; it was discovered at a Marin junk store in 1961 and aired with narration the following year on KPIX. There are several copies floating about the Internet, but this one is the best quality, even with the station logo in the corner. Skip to 13:30 for the shots of Burbank, Edison and Ford together.\nA VISIT WITH LUTHER BURBANK, THE GREAT AMERICAN NATURALIST (1917) is a short produced for the Ford Educational Weekly, a newsreel series the Ford Motor Company distributed free to nickelodeons and movie houses. Seen by an estimated 4-5 million people every week, these productions made the company the world's largest film distributor at the time. A much higher quality copy of this is available for purchase via shutterstock but lacks the title cards revealing its origin.\nTHE LARGEST OMELET IN THE WORLD (1930) According to the Argus-Courier of May 3, 1930, \"eleven Petaluma girls left here early Saturday morning for San Francisco to assist in the ceremonies of preparing the world's largest omelet, which was cooked in a 16-foot frying pan...those who made the trip were Vivian Foster, Hazel White, Janet Perry, Hazel McDonald, Mildred Keller, Lola Hames, Jeane Sweeney, Helen Kenneally, Nettie Rorden, Marie Hames and Dorothy Swan.\" Why they had to do calisthenics in the frying pan is anybody's guess.\nTHE LIBRARY STORY (1964?) The old Carnegie library was condemned in 1960 and Santa Rosa was in no hurry to pay for a new one, so for years the town's main library was on the second story above a saloon and a beauty shop on Exchange Avenue. This video, produced by the Friends of the Santa Rosa Public Library, was an effort to shame the town into building a replacement so the children didn't have to hustle past the \"Bambi Room\" bar to do their homework. It worked; voters approved a 1964 bond and the new main library building was opened in February, 1967. Thanks to architectural photographer Darren Bradley for an interesting blog post on this history and many fine pictures of the current building.\nJACK LONDON NEWSREEL (1915) Two minutes of Jack and Charmian supposedly filmed just three days before he died.\n????? (early 1960s??) This was apparently for an industrial video or commercial promoting the Italian Swiss Colony winery, but it's such a bizarre 55 minutes of random junk I would love to slip it to conspiracy theorists and watch them spin their wheels. Why all the ominous footage of the deep shadows in the redwoods at Armstrong Grove? Why shots of highway 101 south of Petaluma followed by views of empty beaches? Why do the people in the tasting room scenes apparently not know how to dress themselves - and why do they seemingly never swallow the wine they eagerly tipple? Why keep coming back to the ladies making straw chianti bottles? The film goes black for three minutes and when it returns...hand puppets!! There is a scene in a puppet store and another at the side of a puppet swimming pool. The puppets appear to be arguing. One sits in a puppet chair and dies, or at least falls unconscious. The footage immediately cuts to a real swimming pool, as the camera pans over to show barrels of (poisonous!) chlorine stacked at poolside. This. Explains. Everything.","To use all functions of this page, please activate cookies in your browser.\nWith an accout for my.bionity.com you can always see everything at a glance – and you can configure your own website and individual newsletter.\n- My watch list\n- My saved searches\n- My saved topics\n- My newsletter\nIce-minus bacteria is a nickname given to a variant of the common bacterium Pseudomonas syringae (P. syringae). This strain of P. syringae lacks the ability to produce a certain surface protein, usually found on wild-type \"ice-plus\" P. syringae. The \"ice-plus\" protein (Ina protein, \"Ice nucleation-active\" protein) found on the outer bacterial cell wall acts as the nucleating centers for ice crystals. This facilitates ice formation, hence the designation \"ice-plus.\" The ice-minus variant of P. syringae is a mutant, lacking the gene responsible for ice-nucleating surface protein production. This lack of surface protein provides a less favorable environment for ice formation. Both strains of P. syringae occur naturally, but recombinant DNA technology has allowed for the synthetic removal or alteration of specific genes, enabling the creation of the ice-minus strain.\nAdditional recommended knowledge\nTo systematically create the ice-minus strain of P. syringae, its ice-forming gene must be isolated, amplified, deactivated and reintroduced into P. syringae bacterium. The following steps are often used to isolate and generate ice-minus strains of P. syringae:\nThe success of the agricultural world is heavily dependent on the weather. Cold weather conditions are directly responsible for the appearance of frost on plants and most importantly, crops. In the United States alone, it has been estimated that frost accounts for approximately $1 billion in crop damage each year. As P. syringae commonly inhabits plant surfaces, its ice nucleating nature incites frost development, freezing the buds of the plant and destroying the occurring crop. The introduction of an ice-minus strain of P. syringae to the surface of plants would incur competition between the strains. Should the ice-minus strain win out, the ice nucleate provided by P. syringae would no longer be present, lowering the level of frost development on plant surfaces at normal water freezing temperature (0oC). Even if the ice-minus strain does not win out, the amount of ice nucleate present from ice-plus P. syringae would be reduced due to competition. Decreased levels of frost generation at normal water freezing temperature would translate into a lowered quantity of crops lost due to frost damage, rendering higher crop yields overall.\nDr. Hall Hoppe of the U.S. Department of Agriculture was the first to notice a connection between bacteria and frost damage. In 1961, Dr. Hoppe studied a corn fungus by grinding up infected leaves each season, then applying the powder to test corn for the following season to track the disease. A surprise frost occurred that year, leaving peculiar results. Only plants infected with the diseased powder incurred frost damage, leaving healthy plants unfrozen. This phenomenon would baffle scientists until graduate student Stephen Lindow of the University of Wisconsin-Madison found a bacterium in the dried leaf powder in the early 1970s. Dr. Lindow, now a plant pathologist at the University of California-Berkeley, found that when this particular bacterium was introduced to plants where it is originally absent, the plants became very vulnerable to frost damage. He would go on to identify the bacterium as P. syringae, investigate P. syringae's role in ice nucleation and in 1977, discover the mutant ice-minus strain. He was later successful at developing the ice-minus strain of P. syringae through recombinant DNA technology as well.\nIn 1983, Advanced Genetic Sciences (AGS) obtained U.S. government authorization to perform field tests with the ice-minus strain of P. syringae, but environmental groups and protestors delayed the field tests for four years with legal challenges. In 1987, the ice-minus strain of P. syringae became the first genetically modified organism (GMO) to be released into the environment. A strawberry field in California was spayed with the ice-minus strain of P. syringae just before a frost in 1987. The results were promising, showing lowered frost damage to the treated plants, but the data was in suspect as environment activists destroyed some of the plants. Dr. Lindow also conducted an experiment on a crop of potato seedlings sprayed with ice-minus P. syringae. He was successful in protecting the potato crop from frost damage with a strain of ice-minus P. syringae.\nAt the time of Dr. Lindow's work on ice-minus P. syringae, genetic engineering was considered to be very controversial. The controversy primarily revolved around fears of introducing new organisms that may permanently disrupt the ecosystem. The fear was that the introduction of ice-minus bacteria to the environment would eliminate bacterial and plant varieties. This was true in the case of the gypsy moth's accidental introduction into the U.S. Without a predator in the U.S., the gypsy moth is still causing overwhelming destruction to the hardwood forests of northeastern U.S.\n|This article is licensed under the GNU Free Documentation License. It uses material from the Wikipedia article \"Ice-minus_bacteria\". A list of authors is available in Wikipedia.|"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:fb386924-8bb1-4b47-b3fd-cf4cba928490>","<urn:uuid:e6028c42-49c5-44cf-abea-7632a84478ad>"],"error":null}
{"question":"How do desert springs in Egypt maintain constant water flow despite arid conditions, and what are the different geological mechanisms that create springs?","answer":"Desert springs in Egypt maintain constant water flow through fossil water reserves from ancient wet periods, specifically from 11,000 to 5,000 years ago when the Sahara was green. This water infiltrated during rainy periods and created huge underground reserves that now emerge through karst springs, particularly in the Galala mountain range. These springs, like those at St. Paul's and St. Anthony's monasteries, flow constantly throughout the year, independent of seasonal variations. As for geological mechanisms, springs can form in several ways: depression springs occur along valley bottoms, contact springs form when groundwater meets an impermeable rock layer on hillsides, fracture springs emerge where groundwater encounters rock cracks, artesian springs occur when groundwater pressure exceeds atmospheric pressure, and karst springs form when surface water infiltrates through networks of cracks and caves before resurfacing.","context":["The Gospel says: \"God has spoken many times and in many ways\" ... (Hebrews 1:1)\nAccording to the Christian faith, God is revealed in many ways, one through nature, expressing His will through the elected ...\nThose phenomena in which God has appeared, and we cannot explain, we call them miracles and those men who have been instruments of his revelation, we call them saints.\nAt the origin of the Christian monastic activity, throughout this earthly world, God has used the symbiosis between a holy man and what perhaps could be attributed to a miracle ... I mean life itself of St. Paul of Thebes, or the Hermit (228-342 AD), and a spring, just a trickle of water, which flows from the bowels of the earth in the arid Egyptian desert.\nChurch (up) and Monastery of Saint Paul (Egypt)\nEntry to the church in Saint Paul's, cave of Saint Anthony and a street in Saint Anthony's monastery (Egypt)\nRather, I am going to focus on describing a natural fact, this \"miracle of nature\" ..., as long as the scientific man of the century we live in is unable to explain it. Let's see:\nLet’s think of the hard, dry and inhospitable land of the Egyptian desert, among the fertile Nile Valley and the western shore of the Red Sea, actually, the eastern end of the very Sahara Desert, which extends into the Sinai, and after in the Arabian Peninsula. A territory in which it hardly ever rains, maybe once or twice a year, a few minutes, a few millimeters.\nWith this volume of rainfall, what water could there be in this space of hundreds or thousands of square kilometers? None, not superficial, of course not, nor a water table. And yet, at the foot of a mountain range near to the coast, several hundred meters above the sea, some small springs sprout with crystal clear water, drinkable and rich ... as fruitful as is the monastic history which originates in them.\nLet's say that St. Paul spent 60 of his more than 100 years long life as a hermit, praying and meditating, united to his spring, as San Anthony to his, and as with other foundations, it was their followers who years after their death gave place to the monasteries that bear their names.\nSpring of Saint Paul's monastery (Egypt)\nSaint Anthony was luckier…, his source produces one hundred cubic meters per day. Its water in addition, unlike the first, is blander, since the first is a little salty. In both cases, the flow is constant all year round, every day of the year. Important as is water, while the monastery of St. Paul is small and modest, San Antonio Monastery is bigger and more apparent, although both equally remote.\nMonastery of Saint Anthony (Egypt)\nWe know that the Nile, in its course towards the Mediterranean, transfers huge volumes of tropical rains water from the Central Africa highlands situated in the south. But the Nile is far, too far, and water that may infiltrate will never come to these springs, nor because of distance nor because of height. In addition, the flow of the Nile is not always constant, but fluctuates with its famous floods and then infiltration neither is, nor the result that this could lead to.\nWhat can then be the source of water that flows in these places ...? condensation, capillarity from the Red Sea, ... or quite simply a miracle ...?\nMonastery of Saint Paul (Egypt)\nAt this point, it could accept the divine origin of spring water, and what is even more miraculous ... that already in the third century, when there was barely anyone wandering around in the world, God guided the steps of Paul and Antonio to find these places, lost and hidden, and bring them to holiness ...\nBut I am a geologist, and curious by nature and profession, analytical and imaginative. Besides, I'm a lucky man: I do not watch TV, I have internet and I have traveled quite a lot... This leaves me precious time available and allows me to use my experience and my knowledge. So I think, I compare and if I find a better theory, I adopt it.\nMonastery of Saint Paul (Egypt)\nWell, in one of my countless trips to Algeria I found a fascinating book: \"Lost civilizations of the Sahara\". Today, satellite images have confirmed what was suspected for decades. Not so long ago in geological magnitudes, only a few thousand years ago, the territory of the Sahara was a green savannah with numerous lakes inhabited by all the wildlife that has been reflected in the paintings of the Algerian Tassili: hippos drinking water or wandering herds of elephants and giraffes, even astronauts ...\nActually, for several hundred thousand years, the Sahara territory has gone through several stages of ancient desertification and other intermediate flowering. The last time the Sahara was green probably covers the period from 11 thousand to 5 thousand years ago. Mentioned paintings date from more than 15 thousand.\nDuring the stages, let’s say green or wet, the immense surface of Sahara led to the infiltration of rainfall which resulted in huge reserves of groundwater. Reserves nonrenewable today. Fossil water. Waters which were rained down thousands of years ago ...\nWaters which are largely lost in underwater springs in the Mediterranean or the Red Seas. Some others, however, spring in the surface. They are karst springs. Caves have been inventoried and studied in Egypt and Libya. They are unbecoming of currently existing landscape and for their formation water flow was needed.\nSeen in a map and though from one to another monasteries there is a good roll to go with the car, the fact is that both are on opposite sides of the same Galala mountain range.\nIn this massif, in the hyper-arid eastern desert of Egypt, some significant karstic morphologies have been identified. Although small, the caves that sheltered San Pablo and San Antonio have a great geological and cultural significance. The fossil water comes from tertiary sandstones of the Eocene out through small dissolution passages, favored by fractures in the rock. An inexhaustible water for now and which as not dependent of any seasonality, it does not suffer noticeable fluctuations.\nThinking about the lives of Saint Anthony the Abbot or St. Paul of Thebes in the desert, immersed in a heat and unbearable thirst, I imagined a world of blinding light where absolute deprivation lead to a harrowing experience of opposing views of demons and God ...\nUnder such conditions life is simply not possible, it cannot last. The reality is that even in the Eastern Desert of Egypt, temperatures are bearable in the shade, either that of a cave or the thick walls of a monastery and its palm trees.\nMonastery of Saint Anthony (Egypt)\nHermits living in the wild desert, yes, but they established around areas of striking beauty, life and peace, protected from the elements. In both monasteries, the spring water has been channeled to create small oasis that although in the case of St. Paul is certainly very limited, in Saint Anthony keeps a beautiful garden.\nSaint Paul and Saint Anthony chose these locations and monasteries were here founded, because here was a source of water ... the only water in a large region ...\nChurch in Saint Paul's monastery and Spring of Saint Anthony (Egypt)","A spring may be the result of karst topography where surface water has infiltrated the Earth's surface (recharge area), becoming part of the area groundwater. The groundwater then travels through a network of cracks and fissures—openings ranging from intergranular spaces to large caves. The water eventually emerges from below the surface, in the form of a karst spring.\nThe forcing of the spring to the surface can be the result of a confined aquifer in which the recharge area of the spring water table rests at a higher elevation than that of the outlet. Spring water forced to the surface by elevated sources are artesian wells. This is possible even if the outlet is in the form of a 300-foot-deep (91 m) cave. In this case the cave is used like a hose by the higher elevated recharge area of groundwater to exit through the lower elevation opening.\nNon-artesian springs may simply flow from a higher elevation through the earth to a lower elevation and exit in the form of a spring, using the ground like a drainage pipe.\n- Depression spring, is a spring which occurs along a depression, such as the bottom of alluvial valleys, basins, or valleys made of highly permeable materials. \n- Contact Spring, which occurs along the side of a hill or mountain is created when the groundwater is underlaid by an impermeable layer of rock or soil known as an aquiclude or aquifuge \n- Fracture, joint, or tubular spring, occurs when groundwater running along an impermeable layer of rock meets a crack (fracture) or joint in the rock.\n- Artesian spring, is a spring that typically occurs at the lowest point in a given area. An artesian spring is created when the pressure for the groundwater becomes greater than the pressure from the atmosphere. In this case the water is pushed straight up out of the ground.\n- Wonky holes, which are freshwater submarine exit points for coral and sediment covered sediment filled old river channels.\n- Karst spring\n- Hot springs, which have a water temperature significantly higher than the mean air temperature of the surrounding area.\nSpring discharge, or resurgence, is determined by the spring's recharge basin. Factors that affect the recharge include the size of the area in which groundwater is captured, the amount of precipitation, the size of capture points, and the size of the spring outlet. Water may leak into the underground system from many sources including permeable earth, sinkholes, and losing streams. In some cases entire creeks seemingly disappear as the water sinks into the ground via the stream bed. Grand Gulf State Park in Missouri is an example of an entire creek vanishing into the groundwater system. The water emerges 9 miles (14 km) away, forming some of the discharge of Mammoth Spring in Arkansas. Human activity may also affect a spring's discharge—withdrawal of groundwater reduces the water pressure in an aquifer, decreasing the volume of flow.\nSprings are often classified by the volume of the water they discharge. The largest springs are called \"first-magnitude\", defined as springs that discharge water at a rate of at least 2800 liters or 100 cubic feet (2.8 m3) of water per second. Some locations contain many first-magnitude springs, such as Florida where there are at least 27 known to be that size; the Missouri and Arkansas Ozarks, which contain 10 known of first-magnitude; and 11 more in the Thousand Springs area along the Snake River in Idaho. The scale for spring flow is as follows:\n|Magnitude||Flow (ft3/s, gal/min, pint/min)||Flow (L/s)|\n|1st magnitude||> 100 ft3/s||2800 L/s|\n|2nd magnitude||10 to 100 ft3/s||280 to 2800 L/s|\n|3rd magnitude||1 to 10 ft3/s||28 to 280 L/s|\n|4th magnitude||100 US gal/min to 1 ft3/s (448 US gal/min)||6.3 to 28 L/s|\n|5th magnitude||10 to 100 gal/min||0.63 to 6.3 L/s|\n|6th magnitude||1 to 10 gal/min||63 to 630 mL/s|\n|7th magnitude||2 pint to 1 gal/min||8 to 63 mL/s|\n|8th magnitude||Less than 1 pint/min||8 mL/s|\n|0 magnitude||no flow (sites of past/historic flow)|\nMinerals become dissolved in the water as it moves through the underground rocks. This mineral content is referred to as total dissolved solids (TDS). This may give the water flavor and even carbon dioxide bubbles, depending on the nature of the geology through which it passes. This is why spring water is often bottled and sold as mineral water, although the term is often the subject of deceptive advertising. Mineral water contains no less than 250 parts per million (ppm) of tds. Springs that contain significant amounts of minerals are sometimes called 'mineral springs'. (Springs without such mineral content, meanwhile, are sometimes distinguished as 'sweet springs'.) Springs that contain large amounts of dissolved sodium salts, mostly sodium carbonate, are called 'soda springs'. Many resorts have developed around mineral springs and are known as spa towns. Mineral springs are alleged to have healing properties. Soaking in them is said to result in the absorption of the minerals from the water.\nIn parts of the United States a stream carrying the outflow of a spring to a nearby primary stream may be called a spring branch or run. Groundwater tends to maintain a relatively long-term average temperature of its aquifer; so flow from a spring may be cooler than other sources on a summer day, but remain unfrozen in the winter. The cool water of a spring and its branch may harbor species such as certain trout that are otherwise ill-suited to a warmer local climate.\nThis section needs expansion. You can help by adding to it. (October 2010)\nSprings have been used for a variety of human needs - including drinking water, domestic water supply, irrigation, mills, navigation, and electricity generation. Modern uses include recreational activities such as fishing, swimming, and floating; therapy; water for livestock; fish hatcheries; and supply for bottled mineral water or bottled spring water. Springs have taken on a kind of mythic quality in that some people believe, falsely, that springs are healthy sources of drinking water. They may or may not be. In order to know how to use a spring appropriately, whether for a mineral bath or drinking water you must perform a comprehensive water quality test. Springs that are managed as spas will already have such a test.\nSprings are often used as sources for bottled water. When purchasing bottled water labelled as spring water you can often find the water test for that spring on the website of the company selling it. Just because a spring is bottled and sold as spring water does not mean that it is contaminant free. It is best to search out a water test or purchase one yourself before making a habit of drinking regularly from any spring.\nThis section does not cite any sources. (February 2018) (Learn how and when to remove this template message)\nA sacred spring, or holy well, is a small body of water emerging from underground and revered either some religious context: Christian and/or pagan and/or other. The lore and mythology of ancient Greece was replete with sacred and storied springs—notably, the Corycian, Pierian and Castalian springs. In medieval Europe, pagan sacred sites frequently became Christianized as holy wells. The term \"holy well\" is commonly employed[by whom?] to refer to any water source of limited size (i.e. not a lake or river, but including pools and natural springs and seeps), which has some significance in local folklore. This can take the form of a particular name, an associated legend, the attribution of healing qualities to the water through the numinous presence of its guardian spirit or of a Christian saint, or a ceremony or ritual centred on the well site. Christian legends often recount how the action of a saint caused a spring's water to flow - a familiar theme especially in the hagiography of Celtic saints.\n- Baotu Spring, Jinan, Shandong, China (\"City of Springs\")\n- Gihon Spring, Jerusalem\n- Wolmyeongdong Spring, South Korea\n- Al-Hasa Springs, Saudi Arabia\n- Beitou District, Taiwan\n- Su'ao Cold Spring, Yilan, Taiwan\n- Vrelo Bosne, Ilidža, Bosnia\n- Farmakas, Troodos Mountains, Cyprus\n- Bath, England\n- Buxton, England\n- Harrogate, England\n- Kiikunlähde, Hollola, Finland\n- Fontaine de Vaucluse, France\n- 20-Pipe Well, Altleiningen, Germany\n- Aachtopf, Germany\n- Sachsenbrunnen, Bad Harzburg, Germany\n- Castalian Spring, Delphi, Greece\n- Pierian Spring, Pieria, Greece\n- Spring of Juturna, Roman Forum, Rome, Italy\n- Afyonkarahisar, Turkey\n- Termal, Yalova, Turkey\n- Ice River Spring, Nunavut, Canada\n- Bagby Hot Springs, Oregon\n- Barton Springs, Texas\n- Berkeley Springs, West Virginia\n- Bennett Spring, Missouri\n- Big Spring, Texas\n- Big Spring, Missouri\n- Big Springs, Idaho\n- Blue Spring, Florida\n- Comal Springs, Texas\n- Colorado Springs, Colorado\n- Giant Springs, Montana\n- Glenwood Springs, Colorado\n- Greer Spring, Missouri\n- Homosassa Springs, Florida\n- Idaho Springs, Colorado\n- Mammoth Spring, Arkansas\n- Maramec Spring, Missouri\n- Montezuma Well, Arizona\n- Niagara Springs, Idaho\n- Poland Spring, Maine\n- Kitch-iti-kipi, Michigan\n- Las Estacas, Morelos\n- Little Salt Spring, Florida\n- Radium Hot Springs, British Columbia\n- Rainbow Springs, Florida\n- San Marcos Springs, Texas\n- Sanlando Springs, Florida\n- Saratoga Springs, New York\n- Saratoga, Wyoming\n- Silver Springs, Florida\n- Steamboat Springs, Colorado\n- Wakulla Springs, Florida\n- Warm Springs, Georgia\n- Wekiwa Springs, Florida\n- Weeki Wachee Springs, Florida\n- Ojo Caliente Hot Springs, New Mexico\n- Hanmer Springs, Canterbury, New Zealand\n- Hot Water Beach, Waikato, New Zealand\n- Innot Hot Springs, Queensland, Australia\n- Paeroa, Waikato, New Zealand\n- Te Waikoropupu Springs, Tasman, New Zealand\n- Tjuwaliyn (Douglas) Hot Springs, Northern Territory, Australia\n- Body of water – Any significant accumulation of water, generally on a planet's surface\n- Chalk stream – Rivers that rise from springs in landscapes with chalk bedrock\n- Ciénega – A wetland system unique to the American Southwest\n- Drinking water – water safe for consumption\n- Fountain – Architecture which pours water into a basin or jets it into the air\n- Geyser – Hot spring characterized by intermittent discharge of water ejected turbulently and accompanied by steam\n- Holy well\n- Hot spring – Spring produced by the emergence of geothermally heated groundwater\n- Rhythmic spring, also known as Intermittent spring\n- Karst spring\n- Lake – large body of relatively still water\n- List of hot springs in Japan – Wikipedia list article\n- List of hot springs in the United States – Wikipedia list article – Wikipedia list article\n- List of hot springs in the world – Wikipedia list article\n- List of Ozark springs\n- List of spa towns\n- List of major springs in Florida – Wikipedia list article\n- Mineral water\n- Petroleum seep\n- Pond – A relatively small body of standing water\n- Spa town – Specialized resort town situated around a mineral spa\n- Spring creek\n- Spring supply\n- River source – Starting point of a river\n- Thousand Springs State Park\n- Water cycle – Continuous movement of water on, above and below the surface of the Earth\n- Well – Excavation or structure to provide access to groundwater\n- \"Springs - The Water Cycle, from USGS Water-Science School\". ga.water.usgs.gov. Archived from the original on 9 May 2009.\n- \"https://directives.sc.egov.usda.gov/OpenNonWebContent.aspx?content=26985.wba\". directives.sc.egov.usda.gov. Retrieved 29 September 2020. External link in\n- Westbrook, Cherie J. (2017). \"Heads Above Water: The Inside Story of the Edwards Aquifer Recovery Implementation Program by Robert L. Gulley\". Great Plains Research. 27 (2): 143. doi:10.1353/gpr.2017.0022. ISSN 2334-2463. S2CID 133972692.\n- Horstman, Mark (18 May 2006). \"Wonky Holes\". Catalyst transcript. Australian Broadcastiing Corporation. Retrieved 17 April 2019.\n- cite web=https://www.britannica.com/science/spring-water Britannica encyclopedia}\n- \"USGS Surface-Water Data for Missouri\". waterdata.usgs.gov.\n- Vineyard and Fender, 1982. p. 12\n- \"Archived copy\". Archived from the original on 12 December 2012. Retrieved 10 May 2009.CS1 maint: archived copy as title (link)\n- Stunning turquoise natural spring Kiikunlähde at Hollola\n- LaMoreaux, Philip E.; Tanner, Judy T, eds. (2001), Springs and bottled water of the world: Ancient history, source, occurrence, quality and use, Berlin, Heidelberg, New York: Springer-Verlag, ISBN 3-540-61841-4\n- Springs of Missouri, Vineyard and Feder, Missouri Department of Natural Resources, Division of Geology and Land Survey in cooperation with U.S. Geological Survey and Missouri Department of Conservation, 1982\n- Cohen, Stan (Revised 1981 edition), Springs of the Virginias: A Pictorial History, Charleston, West Virginia: Quarrier Press.\n|Wikimedia Commons has media related to Springs.|"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:cc7c34a5-2d9f-42a8-a51b-3fb7266d5068>","<urn:uuid:ed239ebd-3c41-40fb-a6ae-094b64daaeb9>"],"error":null}
{"question":"How do the causes of the 2020 Russia Norilsk disaster and the 1964 Prince William Sound catastrophe differ in terms of natural phenomena?","answer":"The 2020 Norilsk disaster and the 1964 Prince William Sound catastrophe were caused by different natural phenomena. The Norilsk disaster occurred when a storage tank collapsed due to melting permafrost, which had weakened its supports. This was attributed to unusually warm weather in the Arctic, potentially linked to global warming. In contrast, the 1964 Prince William Sound catastrophe was caused by the Good Friday Earthquake (also known as the Great Alaska Earthquake), which had a magnitude of 9.2 and generated a tsunami. This earthquake and subsequent tsunami resulted in 131 deaths and destroyed the town of Valdez along with killing several Chugach villagers in Chenega.","context":["Oil Spill in Russia Could Pollute the Arctic Ocean after Large Lake\nA catastrophic diesel oil spill in Russia has polluted a large freshwater lake in the region. Evidently, the huge spill poses a risk of spreading into the Arctic Ocean. Emergency response teams have been trying to contain the oil for a while, which had now traveled about 20 km (12 miles) north of Norilsk from a collapsed fuel tank.\nAccording to environmentalists and officials, this is the worst accident of its contemporary times in Russia’s Arctic region.\nThe Worst Disaster\nThe oil leaking began on May 29. So far 21,000 tonnes have contaminated the Ambarnaya River and surrounding terrain.\nInvestigators are of the belief that the storage tank near Norilsk sank owing to the melting permafrost, which deteriorated its supporters. The Arctic has had weeks of strangely warm weather, which could be a result of global warming.\nPermafrost is a term used for ground that is frozen continuously for two or more years. Nearly 55 percent of Russia’s terrain, predominantly Siberia, is permafrost and home to its main oils and gas fields. The power plant, where the accident occurred, is administered by Norilsk Nickel, the world’s leading nickel and palladium producer.\nThis oil spill in Russia has polluted the Lake Pyasino, which serves as the basin for the Pyasina River that flows to the Kara Sea, part of the Arctic Ocean. From the months of October to June, the river is usually ice-bound.\nGovernor of Krasnoyarsk region, Alexander Uss said,\nThe fuel has got into Lake Pyasino. This is a beautiful lake about 70km [45 miles] long. Naturally, it has both fish and a good biosphere. Now it’s important to prevent it from getting into the Pyasina river, which flows north. That should be possible.\nReportedly, the clean-up teams have removed as much as 23,000 cubic meters of polluted soil.\nVasily Yablokov of Greenpeace Russia said,\nThe pollution will have a negative effect on the water resources, on the animals that drink that water, on the plants growing on the banks.\nGreenpeace has compared this catastrophe to the 1989 Exxon Valdez disaster in Alaska.\nGreenpeace director of Russia Vladimir Chuprov said that it would be a disaster if 10,000 tonnes or more of fuel had reached the lake. He criticized the authorities for not giving more information about the extent of the spill.\nRussian prosecutors have ordered to check at “particularly dangerous installations” built on permafrost.\nDelayed Reports Angered Putin\nPresident Vladimir Putin has been furious as the reports were delayed over the collapse of the oil tank. He declared a state of emergency after 21,000 tonnes of diesel was leaked.\nMeanwhile, the director of the power plant has been taken into custody. The Russian Investigative Committee has launched a criminal case over pollution and alleged negligence.\nThere was another oil spill in Russia on May 29th. 20,000 tons of oil spilled into the Artic Circle due to a collapsed storage tank. Russia declared a state of emergency yesterday following the spill. My heart is heavy. pic.twitter.com/YX7xuFKVYC\n— Karlie Alexander (@KarlieAlexande7) June 5, 2020\nA 2017 report to the Arctic Council, an international forum which includes Russia, warned that global warming and melting ice would weaken the foundations in permafrost regions and they would no longer be able to support loads of power plants as they did in the 1980s.\nThe oil spill has turned the long stretches of the Ambarnaya River crimson.\nNorilsk Nickel revealed in a statement that the incident had been reported in a “timely and proper” way. The company has pledged to pay for the clean-up, which so far amounts up to $146 million.\nDisastrous Environmental Impacts\nNorilsk is famed as a pollution hotspot already, owing to the contamination from the industry that dominates the city. In 2016, Norilsk Nickel admitted that an accident at one of its plants was responsible for turning a nearby river red.\nThe recent oil spill has polluted the ground and waterways, activating a major clean-up effort. Officials in the Siberian region of Krasnoyarsk said that a high concentration of contaminated water had been discovered beyond floating barriers set in place to stop the fuel from spreading.\nUnited States Secretary of State Mike Pompeo announced on Twitter that the US “stands ready to assist Russia to mitigate this environmental disaster and offer our technical expertise.”\nSaddened to hear about the fuel spill in Norilsk, Russia. Despite our disagreements, the United States stands ready to assist Russia to mitigate this environmental disaster and offer our technical expertise.\n— Secretary Pompeo (@SecPompeo) June 6, 2020\nLooking toward the future, the company said that it will explore the possibility that this could happen again as permafrost continues to melt.\nCurrently, NTEC teams are conducting an inspection of emergency diesel fuel storage facilities, with special attention paid to assessing the risks of sinking soil under hazardous objects installed in permafrost.\nIf not controlled soon, the oil spill in Russia will go on contaminating the surrounding water sources and terrain, subsequently harming the ecosystems and its inhabitants. If oil reaches the Arctic Ocean, many marine species will perish, creating a huge imbalance in the ecosystem.\nAlthough it is hoped that it would be soon under control, the damage done is irrevocable.","Prince William Sound is a beautiful complex of deep, glacially-carved fjords on the north Gulf of Alaska. Enclosed by high mountains and glaciers, and protected from the high seas by barrier islands, the area is a boater’s paradise. Fiords, bays, coves, lakes, glaciers, mountains and hundreds of islands provide a rich and unspoiled beauty. The area’s vast size and limited development provides a true wilderness setting.\nPrince William Sound is also widely used for marine commerce. Oil tanker vessels, log transport ships, fuel barges, freighters, oil industry work boats, ferries, and cruise ships make routine stops at ports in Prince William Sound. Commercial fishing boats, sport fishing charter boats, sightseeing tour boats, and privately-owned vessels also use local the harbours and docks of Prince William Sound. Communities on Prince William Sound include Whittier, Valdez, Tatilek, Cordova, Port Chalmers, and Chenega.\nBecause the Sound and Resurrection Bay is surrounded by mountains filled with glaciers adjacent to a relatively warm ocean current, the weather tends to be quite wet and cool. The weather is unpredictable, and although the sound is protected, it can still become a flurry of whitecaps.\nHistorically, Prince William Sound was a meeting place of diverse cultures, including the resident Chugach Eskimos and Eyak Indians, and Asian and European explorers and pioneers.\nThe location of Prince William Sound, where north temperate and sub-arctic conditions overlap, results in a wealth of natural resources\nLocation: Prince William Sound is an inlet of the Gulf of Alaska, located east of the Kenai Peninsula in South Alaska. Valdez and Cordova are the largest towns on Prince William Sound.\nGlaciers: Prince William Sound has the highest concentration of glaciers in Alaska, and the densest concentration of tidewater glaciers in the world, some flowing a dozen miles from ice-capped peaks to terminate in cliffs of ice towering hundreds of feet above the water. Visitors can spend an afternoon cruising through glacier-carved fjords, watching glaciers calve, and experiencing the untamed wilderness and calm waters of Prince William Sound. Visitors will also view the abundant marine wildlife and learn about the natural history of the Prince William Sound. Columbia Glacier, with a three-mile wide face, is the second largest tidewater glacier in North America, extending 40 miles (64 kms) out of the Chugach Mountains. Columbia Glacier is located about 25 mile (40 kms) west of Valdez. Hubbard Glacier in the Gulf of Alaska is the largest tidewater glacier on the North American continent.\nWildlife: Prince William Sound and Resurrection Bay are premier ecosystems with a diversity of wildlife and habitat. Prince William Sound’s waters and its roughly 3,000 miles of shoreline support large numbers of sea birds, marine mammals and, until recently, some of the world’s richest herring and salmon fisheries. Five major salmon hatcheries operate in the Sound. Other important fisheries include dungeness crab, shrimp, halibut and other groundfish. The Sound is home water to one of the world’s densest populations of sea otters and killer whales. Seals, sea lions and other cetaceans are also residents and, when combined with the otters and orcas, may represent the most profuse population worldwide of marine mammals.\nPrince William Sound is the northern boundary of the world’s farthest north temperate rainforest. This coastal forest strip, primarily comprised of slow growth Sitka spruce, mountain hemlock and western hemlock, is critical habitat to Sitka black-tailed deer, mountain goats and is home to a wide variety of other animals. Two hundred and forty species of birds, including about 3,000 bald eagles and a dozen major seabird colonies, are found in the Sound and the adjacent Copper River Delta region.\nThe Copper River Delta is a two-million acre complex of sedge marshes, braided glacial streams, intertidal sloughs and shallow ponds. Approximately 60 miles (96 kms) across, the Delta is ringed by glaciers and snowcapped peaks in the Chugach Mountain Range. The Delta is prime habitat for waterfowl, salmon, bear, wolves, goats, coyotes and a healthy population of transplanted moose. The Delta’s mudflats and sandbars support the largest concentration of shorebirds in the Western Hemisphere when these small birds migrate through the area each spring, some coming from as far away as Chile. The Dusky Canada goose and Trumpeter swan are just two of the bird species which nest in the Delta.\nThe small boat harbour at Whittier is the Gateway to Prince William Sound from Anchorage. Established during the Second World War as a military supply port, Whittier is located on the west side of Prince William Sound. Whittier is a year round ice-free port and is a focal point for marine activity and freight transfer from sea train barges serving South Central Alaska.\nValdez is located on the north shore of Port Valdez, a deepwater fjord in Prince William Sound. The crashing glaciers and towering Chugach mountains rising from the sea make Valdez absolutely picturesque. The growth and settlement of Valdez was attributed to fur trading, salmon canning, and gold and copper mining. During the Klondike Gold Rush of 1897-98, prospectors came to Valdez believing the Copper River and Valdez Glacier to be the entry to the interior gold fields. From 1910 to 1916, copper and gold mining flourished in the area. In the early 1970’s, Valdez became the staging area for work on the lower portion of the Trans Alaska Pipeline. Today, Valdez hosts the Valdez Marine Terminal, which is the southernmost end of the 800-mile pipeline. Valdez is situated 120 air miles (192 kms) east of Anchorage, or 305 miles (488 kms) by the Richardson and Glenn Highways.\nCordova is a community of 3,000 year-round residents on the eastern shore of Orca Inlet, offering good access to both Prince William Sound and the adjacent Copper River Delta, the largest contiguous wetlands in the Western USA. Once a railroad gateway to Alaska’s interior, Cordova today is a hub of commercial fishing activity with a growing tourism industry, a modern airport with daily jet service to Anchorage and Seattle, and regular ferry service to Valdez, Whittier and Seward. Access to Cordova is limited to airplane and ferry – no roads connect the town to the rest of the state.\nState Marine Parks in Prince William Sound\nParks near Whittier: Many of the parks in Prince William Sound can be reached from Whittier. There are seven parks near Port Wells and another further south on Latouche Island. These parks offer the weekend boater plenty of options for protected anchorages in forested or glaciated surroundings. Special attractions include wildlife viewing, good camping areas, historic sites and one of the world’s largest fish hatcheries. Decision Point State Marine Park is located at the eastern end of Passage Canal approximately eight miles from Whittier. Entry Cove State Marine Park is located two miles directly east of Decision Point on the northeast corner where Passage Canal and Port Wells meet. Surprise Cove State Marine Park is located on the western side of the mouth of Cochrane Bay. Ziegler Cove State Marine Park Ziegler Cove is located on of northern side at the mouth of Pigot Bay 18 miles from Whittier. Granite Bay State Marine Park is located on the northwest corner of Esther Island about 25 miles from Whittier. South Esther Island State Marine Park is located on the southern end of Esther Island. Bettles Bay State Marine Park is an excellent anchorage near the mouth of Bettles Bay. Horseshoe Bay on Latouche Island is located three miles northeast of the new town of Chenega Bay.\nParks Near Valdez: Sawmill Bay, Shoup Bay and Jack Bay parks offer a variety of recreational opportunities within an hour’s boat ride from Valdez. While located next to the main shipping channel to the Valdez Oil Terminal, these parks offer excellent opportunities for a wilderness experience. The serene, forest-ringed Sawmill Bay SMP offers protected anchorages and good camping. For island camping and a fair-weather anchorage, visit Jack Bay SMP. Shoup Glacier spills into an iceberg-filled saltwater lagoon accessible on most high tides by small shallow-drafted boats. Visitors will enjoy views of the glacier, interesting geologic features and a large kittiwake rookery. Shoup Bay State Marine Park is located five miles southwest of the Port of Valdez. Sawmill Bay State Marine Park is 15 miles from Valdez and three miles west of Valdez Narrows. Jack Bay State Marine Park is located 15 miles from Valdez, southeast of Valdez Narrows.\nParks near Cordova: Three diverse marine parks are located near Cordova. Canoe Passage SMP is a shallow wetlands passage cutting through Hawkins Island. Boswell Bay SMP, adjacent to the Copper River Delta Critical Habitat Area, contains vast tidelands and high energy beaches. Beachcombers and wildlife watchers will find this area an excellent spot for exploring. Kayak Island SMP is located at the approximate site of the landing of the Bering Expedition in 1741 – the first Europeans to set foot in Alaska. A large dagger of land thrust out into the cold, unpredictable waters of the Gulf of Alaska, it offers a rare adventure to the few who visit it. Canoe Passage State Marine Park is located on Hawkins Island, eight miles west of Cordova. Boswell Bay State Marine Park is located on the eastern tip of Hinchinbrook Island. Kayak Island lies in the Gulf of Alaska some 50 miles southeast of Cordova.\nOn 24 March 1989 the Exxon Valdez ran aground on Bligh Reef in Prince William Sound, spilling approximately 11 million gallons of crude oil into the previously pristine and biologically rich area. More than 700 miles of Alaska shoreline were covered with crude oil, which fouled the waters and shorelines of both National Wildlife Refuges and National Parks. As a result, more than 36,000 migratory birds were killed, including at least 100 bald eagle, along with numerous other varieties of wildlife.\nThe images of heavily oiled shorelines, dead and dying wildlife, and the thousands of workers mobilized to clean beaches, were viewed with horror around the world. Since the spill, teams of biologists, geologists, and chemists have monitored and studied the affected shorelines of Prince William Sound to assess its recovery from the environmentally devastating accident.\nThe plant and animal populations of the sound are still adjusting themselves in response to the spill, but many haven’t yet completely recovered. Impacts from the spill remain in Prince William Sound, in the form of oil still leaching from beaches. Although little of the spilled oil remains, oily vestiges are still evident on many beaches. Beaches that had been denuded of plants and animals by the toxic effects of oil and by the intense cleanup efforts show extensive recolonization and are similar in appearance to areas that were unoiled. In some areas, intertidal animals such as mussels are still contaminated by oil, and rich clam beds that suffered high mortalities from oil and extensive beach cleaning have not repopulated to their previous levels. Some rocky sites that were stripped of heavy plant cover by high-pressure, hot-water cleaning remain mostly bare rock.\nExxon Corporation and Exxon Shipping Company agreed with the US Department of Justice to pay a record one billion dollars in criminal fines and civil damages in connection with the oil spill, by far the largest single environmental criminal recovery ever exacted. Exxon has also spent approximately $2.2 billion to clean up Prince William Sound and the Gulf of Alaska. In February 2007, Exxon reported the largest annual earnings ever of a US company: $39.5 billion. The penalty and cost of the oil spill was relatively minor compared to their earnings.\nA tsunami caused by the Good Friday Earthquake (also called the Great Alaska Earthquake) in 1964 killed a number of Chugach villagers living on the shores of Prince William Sound in Chenega. The tidal wave also destroyed the town of Valdez. The magnitude 9.2 earthquake, which resulted in 131 deaths, was centered in Prince William Sound."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:d6c08928-08a1-46b4-bb8b-bcfe24276d8e>","<urn:uuid:e0a5c705-4f7d-4241-9796-adb5e866966f>"],"error":null}
{"question":"What were the historical impacts of mining in Butte on the Clark Fork River ecosystem, and how are ospreys being used today to monitor its recovery?","answer":"Mining in Butte severely impacted the Clark Fork River ecosystem through tailings discharge and acid mine drainage. Fine-grained mine waste was discharged into Silver Bow Creek, spreading toxic metals across the floodplain. The mining operations produced sulfuric acid and dissolved metals like copper, zinc, lead, cadmium, and arsenic, which were toxic to aquatic life. By mid-20th century, fish were essentially gone from the affected headwaters. Today, ospreys serve as indicator species to monitor the river's recovery. Scientists study heavy metal concentrations in osprey blood since these birds feed exclusively on fish near their nests. While most heavy metals like copper, zinc, and lead appear well-regulated in ospreys, mercury levels are concerning - osprey chicks show levels 100 times higher than what's considered problematic for humans, ranging from 100-800 micrograms per liter.","context":["From the discovery of gold on Silver Bow Creek west of Butte in the 1860s, the Clark Fork River and surrounding ecosystems of western Montana have been heavily impacted by mining and smelting. With the discovery of gold and other valuable minerals in Montana, development quickly followed. By the turn of the 20th century, there were at least a dozen concentrators, smelters and precipitation plants on Silver Bow Creek just below the Butte Hill, where the urban center of the city of Butte was taking shape. Underground mines, marked by the large black headframes still seen today, were scattered throughout the city.\nThe city boomed as demand for copper soared. Thomas Edison made the first public demonstration of his incandescent light bulb on December 31, 1879, in Menlo Park, New Jersey, and the race to electrify America and the world was on. Copper from the Butte mines was in high demand for its role as a conductor. By the 1890s the mining interests of Butte were consolidated under the three Copper Kings: Marcus Daly, William Clark, and Augustus Heinze. Copper King William A. Clark’s Colorado Smelter and Butte Reduction Works were the largest. Fine-grained mine waste, called tailings, was discharged into Silver Bow Creek, and the discolored water in the creek earned it the local nickname of “Copper Creek.”\nThe mining operations of Clark, Daly and others churned furiously 24-hours a day, 365 days a year, taking copper ore out of the ground and putting it to use in the industrial revolution’s most high tech commodity: the transmission of electricity. The mines, mills and smelters of Butte and Anaconda were producing more copper than any one place on the planet, accounting for more than 1/3 of the United States supply when demand was at its peak.\nThe influence of the Copper Kings was felt beyond Butte. Resources in the Milltown and Missoula area, particularly timber, were critical for the successful expansion of Butte operations. In the 1890s, when health concerns and a scarcity of water and timber drove smelting out of Butte, smelters were constructed in Anaconda and a refinery was built in Great Falls.\nEarly Environmental Damages\nThe intensive mining and smelting underway in Butte and Anaconda had serious environmental consequences for the entire Upper Clark Fork Basin, consequences that have persisted for over 100 years and that will continue to require community attention and management for the foreseeable future. The photo at left, taken around 2000, shows one such impact: mine tailings deposits along Silver Bow Creek, a primary headwater of the Clark Fork River. Tailings are particularly harmful to aquatic life. Ongoing remediation and restoration have greatly improved the health of the creek from Butte to Ramsay; work is underway on the remaining contaminated sections.\nThe amount of waste rock and tailings was immense. For every shovel of rock mined, a tiny bit of copper or other valuable ore is produced. Butte earned the nickname “Richest Hill on Earth” because of the richest veins on the hill, some of which were reportedly more than 30% copper. As a point of comparison, the ore mined today at the Continental Pit in Butte is roughly 0.25-0.35% copper. On average, the ore mined from the Butte hill over the last century is approximately 10% profitable metals (copper, silver, molybdenum).\nThe rest becomes waste, either waste piled at the mine site or the waste from processing, also known as tailings. Because of the tailings’ fine-grained nature, they were exported through Silver Bow Creek in the absence of any environmental laws. On the floodplain, when the water spread out, it slowed and the tailings dropped out; water without the tailings was allowed to drain back into the channel. This spread tailings across the Silver Bow Creek floodplain in the Butte valley and downstream to the west and north over a large area. As more tailings were flushed downstream, the floodplain deposits would grow thicker.\nThe coarse, granitic rock of the Butte ore body contains substantial amounts of pyrite. The Butte geology is a sulfide ore-body, meaning all of the metals found in the rock are sulfide minerals that can produce sulfuric acid as one of the products of their oxidation when exposed to air and water. Exposed minerals (copper, zinc, lead, cadmium, arsenic, etc.) are dissolved when pH is low (acidic), as in sulfuric acid. The dissolved minerals can be very toxic to fish, bugs, plants and other aquatic life in streams, and can potentially contaminate water supplies used by humans, such as groundwater. When pyrite mixes with air and water to produce sulfuric acid, iron is left to settle out, giving areas where this happens an orange rust color – a tell-tale sign of acid mine drainage. Although this process is often called acid mine drainage, it does not necessarily have to be caused by mining. Any time pyrite is exposed, this process can occur, which is why we refer to it as acid rock drainage.\nSmelting is the final step in the process where copper concentrate from milling and processing is roasted into pure copper. Airborne byproducts of smelting include arsenic and acidic sulfur fumes. These fumes greatly affected public health; in the worst cases, people experienced bloody noses and vomiting, and sometimes, death. In Butte in the winter months before smelting operations were moved to Anaconda, air inversions would trap smelting pollution in the valley, and Butte death rates were higher than in New York and Chicago.\nThe fallout from smelting in Butte and, later, Anaconda spread as far north and west as Avon, Montana on the Blackfoot River. These impacts were more intense in the Deer Lodge Valley; Anaconda and its immense smelter sit at the south end of the valley. The northern portion near the town of Deer Lodge has traditionally been more agricultural. When Anaconda smelting was at its peak in the early 20th century, livestock in the valley were dying, and farmers and ranchers could not sell their hay because of high, potentially toxic levels of arsenic. The book Smoke Wars by Donald MacMillan is an excellent resource on this topic. The following information is excerpted from Smoke Wars: In 1902, in its first year of operation, the Anaconda Stack pumped 30 tons (60,000 pounds) of arsenic trioxide and 150 tons (300,000 pounds) of sulfur dioxide in the air each day. The stack was built to a height of 585 feet in order to further spread and dilute pollution in prevailing air currents. It did nothing to reduce the total volume of pollution. The acidic fumes and resulting fallout caused areas of dead vegetation around Anaconda and in the Deer Lodge Valley.\nLater Environmental Damages\nIn Butte in 1955 excavation on what would become the Berkeley Pit, named from one of several nearby historic underground mines that the Pit would later swallow, began in a transition from underground to open pit mining. The Pit would, in the next decade, swallow Butte neighborhoods like Meaderville, Dublin Gulch, and McQueen. The transition to open pit mining, a highly mechanized form of mining, also meant fewer jobs for the city’s miners. But mining had always been the lifeblood of Butte, and so the community embraced the new mine, and there was little objection to the sacrifice of some of the city’s neighborhoods. While some structures were moved elsewhere, many, like the Holy Savior church, were simply buried.\nLike the underground mines, the Pit led to unforeseen environmental consequences. Steep, continuous declines in copper prices in the 1970s led to the eventual shut down of Berkeley operations in 1982. Throughout the history of mining in Butte, pumps were used to de-water the underground mines and, later, the Berkeley Pit. On April 23, 1982, ARCO (now BP-ARCO), the owners of the former Anaconda Company holdings, announced that they were suspending their Butte operations. Along with the announcement, the underground pumps in the Kelley Mine were shut down. The result: the underground mines and the Berkeley Pit began to fill with acidic water.\nOver the active lifespan of the Berkeley, approximately 320 million tons of ore and over 700 million tons of waste rock were mined from the Pit. Put another way, “The Richest Hill on Earth” produced enough copper to pave a four-lane highway four inches thick from Butte to Salt Lake City and 30 miles beyond. Today, the Pit is managed as a federal Superfund site. Plans are in place to monitor water levels to ensure that contamination caused by the Pit does not spread. For much more on Berkeley Pit history and ongoing management, visit the www.pitwatch.org website.\nSuperfund & The Beginning of Restoration\nIn 1980, the U.S. Congress passed the Comprehensive Environmental Response, Compensation, and Liability Act, also known as CERCLA, or, more commonly, Superfund. Combined with the Clean Air Act of 1970 and the Clean Water Act of 1972, Superfund provided the impetus for the beginning of cleanup in the Clark Fork and in Butte. But actual on-the-ground remediation and restoration were still a decade or two away.\nStarting in the mid-1980s, sites around the basic were gradually added to the Superfund National Priorities List. The EPA began to negotiate the details of remediation with the State of Montana, local communities, and the Potentially Responsible Parties (PRPs), the formal term for the business entities liable for cleanup costs. In the case of the Clark Fork, BP-ARCO, who purchased the old Anaconda Company in 1977, is the main PRP, although there are others.\nConcurrently, the State of Montana filed a lawsuit against ARCO to cover costs for restoration, going beyond EPA-mandated reclamation. A portion of the lawsuit was settled in 1998, and $85 million was allocated for the restoration of Silver Bow Creek. Settlements soon followed for other sites, such as the Milltown Dam, although settlements for other sites in the basin are still being negotiated.\nFor more information on western Montana Superfund sites and ongoing reclamation and restoration, visit the EPA Montana Superfund page, the Montana Department of Environmental Quality Clark Fork River site, and the Montana Department of Environmental Quality Silver Bow Creek site.","Montana Osprey Project\nHeavy metal contamination is a big challenge facing Clark Fork River ospreys. Establishing baseline data on heavy metal concentrations in osprey blood is one of the hallmarks of this study. By determining contamination levels, researchers can assess the overall health of the river ecosystem and start to examine the long-term effects of these heavy metals.\nIris brings in a fish for the Hellgate chicks. Photo courtesy of William Munoz.\nSo why ospreys?\nOspreys (Pandion haliaetus) are a valuable indicator species because they are at the top of the food chain and have one main food source—fish. Ospreys hunt close to the nest, which means the chicks have only eaten food from a very specific area. And true to the maxim “you are what you eat,” heavy metals from the river, some of which are concentrated in fish, then get into the osprey chicks’ blood. These measurements are a barometer of the whole system’s health.\nEvening feeding at the Hellgate nest July 2012\nPeople care about the health of the Clark Fork River ecosystem because it is a part of a large Superfund complex. Huge copper mining and processing operations around Butte and Anaconda were the source of waste material laden with toxic elements: namely arsenic, cadmium, copper, lead and zinc. The fish were essentially gone from the affected headwaters of the Clark Fork River by the middle of the last century, and the aquatic ecosystem was badly affected. Environmental damage wasn’t limited to the rivers - there are accounts of cattle dying in the Deer Lodge valley due to toxic air fall from the Anaconda Smelter.\nA century later, the upstream source for contaminated sediment has been eliminated by creating the Warm Springs Ponds and by the clean-up of Silver Bow Creek near Butte. Milltown Dam near Missoula, where some of the contaminated material had been trapped, has been removed as a contamination source to the river.\nThis holding pond near Opportunity, Montana, settles out sediment removed from the Milltown Dam site. The photo was taken by William Munoz from a Cessna plane flown by Erick Greene.\nAll along the river, efforts to remove and clean up contaminated sediment are underway. The work started several years ago and is projected to continue for about a decade and half. Check out the Clark Fork Coalition page for more info.\nOver time, fish have returned to the river where they were gone or decimated. And, with the fish, other parts of the aquatic ecosystem have come back to life – among them the ospreys. However, the natural system won’t be back to normal for a long time. Missing or altered vegetation along the upper reaches of the Clark Fork River, unnatural invertebrate (insect) fauna and lower than normal fish density are some of the more obvious problems, as well as severe limits to human use due to high arsenic and mercury levels. Understanding how heavy metals affect ospreys and tracking any changes over the years will help scientists and policymakers assess the effectiveness of the Clark Fork River Superfund clean-up.\nView of the Clark Fork River from the Hellgate nest; shadow of the nest in the left lower corner.\nNow let’s get into the nitty-gritty details.\nThe graph below is from a study done by our researchers. It shows concentrations of arsenic, cadmium, copper, lead and zinc. Contaminant concentrations are shown in the blood of osprey chicks raised along the Clark Fork River (left panel) and tributaries (right panel). Each dot represents the average of one nest over the years, and the vertical bars show the range of measurements among all chicks and years from the same nests.\nThese levels are compared against contamination in river sediments to discern patterns. Generally, concentrations in sediment decrease in the downstream direction away from the areas of higher contamination. See this study here from one of our researchers about heavy metals in the Clark Fork River.\nOther studies reveal how these patterns translate into biological systems: a lot of organisms in the river are heavily affected by these concentrations. For example, the fish communities in contaminated sections are less abundant and/or less desirable than in uncontaminated sections, and there have been a number of infamous fish kills that were linked to the metals, especially copper, even in the recent past.\nResults from the osprey project work show that the contaminant levels in ospreys do not generally follow this trend. Arsenic is the only one of the elements following the same trend as in the river, but concentrations are not considered in the toxic range. The diagram above shows copper, zinc and lead concentrations are very similar throughout the watershed (cadmium levels are not shown because they were very low and are not of concern either.) This indicates that the ospreys are able to regulate these metals very well. So, while the fish supply may be affected by the metals coming from Butte and Anaconda, the ospreys do not seem to have a lot of direct problems with these contaminants.\nBut then scroll up and check out the mercury levels.\nAlthough this metal has not been considered at all in the planning of the Clark Fork River remediation, it is now the main driver of the osprey study.\nMercury levels in osprey chick blood spike after Flint Creek meets the Clark Fork River. The photo was taken by William Munoz from a Cessna plane flown by Erick Greene.\nMercury is a potent neurotoxin, meaning that it especially affects the brain. In the graph above mercury levels in osprey blood vary between 100 and 800 micrograms per liter (µg/L) throughout the study area. To put this in perspective, the accepted threshold for humans is 5.8 µg/L, above which there is a measurable loss in IQ.\nThis means the average osprey chick has about one hundred times more mercury in its blood than what is considered problematic for humans. And consider this: As long as the ospreys grow, a lot of the mercury from their food is deposited in growing tissues, especially the feathers, which keeps the concentrations in the blood low. This usually coincides with chick sampling, so the mercury measurements are low compared to non-growing birds.\nOur researchers draw blood from chicks to determine the heavy metal content. Photo courtesy of William Munoz.\nAs soon as growth stops, shortly before fledging, mercury levels start rising really fast because the birds keep all the mercury from their food within their body. For example, a sample from an injured osprey a month after fledging had mercury levels at nearly 3000 µg/L, up from about 300 when s/he was a chick in the nest.\nSome researchers assume that the toxicological effects of mercury in humans and birds are the same, so there must be some health effect on these ospreys. On the other hand, fish-eating predators such as ospreys must have evolved with higher mercury exposures than others, because rivers and the ocean are naturally higher-mercury environments. There is not enough scientific evidence to prove either suggestion right or wrong. For example, 3000 µg/L is in line with concentrations where researchers have found decreased reproductive success or behavioral abnormalities in birds (NPS; Bio One). Others have not found any drop in the number of chicks fledged from nests with similar or higher blood mercury concentrations (AECT).\nIn summary, we don’t know for sure how much mercury the osprey can handle, because nobody ever truly tracked chicks that grew up in high-mercury environments. In the worst imaginable case, osprey in the Clark Fork River may not live very long and may be replaced periodically by others from less contaminated regions. Our study tries to fill in this gap by banding the osprey chicks and studying their movements as well as monitoring nests along the Clark Fork River.\nThe nests are ideal natural labs that provide a greater understanding how heavy metals affect ospreys. This not only reveals contamination thresholds for the species, but may help researchers and policymakers assess the effectiveness of cleaning up the Clark Fork River."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:119b9261-55c4-435d-a259-1a4e53a86cfb>","<urn:uuid:c1a9aaa7-a052-4fc6-88bf-0610cb903bd2>"],"error":null}
{"question":"¿Cómo desarrollar un Plan de Gestión de Señales de Tráfico efectivo?","answer":"To develop an effective Traffic Signal Management Program Plan (TSMPP), agencies should minimally identify objectives and performance measures that can be clearly communicated to agency professionals, policy makers and the public. The FHWA document 'Improving Traffic Signal Management and Operations: A Basic Service Model' provides guidance and an outline for developing such a plan. Agencies that document their operations philosophy, objectives and performance measures in a TSMPP have consistently shown better capacity to implement systems, sustain resources and maintain a proficient workforce.","context":["Traffic Signal System Management\nTraffic signal management involves the planning, design, integration, maintenance, and proactive operation of a traffic signal system in order to achieve policy based objectives to improve the efficiency, safety and reliability of signalized intersections. Scores on the Management section of the National Traffic Signal Report Card have ranged from F to D from 2005 to 2012.\nAgencies that develop and document their operations philosophy, operations objectives and performance measures in a Traffic Signal Management Program Plan (TSMPP) have consistently demonstrated the capacity to implement and operate systems, sustain resources and a maintain a proficient workforce. The FHWA document titled Improving Traffic Signal Management and Operations: A Basic Service Model describes and provides an outline for a TSMPP. Agency TSMPPs should minimally identify objectives and performance measures linked to those objectives that can be clearly communicated to agency professionals, policy makers and the public.\nThe key publications identified below provide guidance on the development of TSMPPs aimed at delivering high quality basic service. The resources identified on the left including the FHWA Resource Center Operations TST, Planning for Operations and NTOC Traffic Signal Library and User Forum and Training and Workshops offer opportunities to gain additional information and technical assistance on this topic.\n- Traffic Signal LED Module Specification Workshop and Informational Report for Snow Conditions (HTML, PDF 5MB) (FHWA-HOP-13-010) - A stakeholder workshop was held on December 14–15, 2010 that created a set of proceedings documenting the issues, findings, and potential next steps in addressing the snow and ice-covered LED traffic signal lens issue. One of the outcomes of the workshop was the initiation of a small research task to further explore the environmental conditions (i.e., weather) upon which the snow and ice problem occurs. This report summarizes the research effort completed to fulfill that outcome.\n- Improving Traffic Signal Management and Operations: A Basic Service Model (HTML, PDF 284KB) (FHWA-HOP-09-055) - This report provides a guide for developing a Traffic Signal Management Plan based on achieving good basic service in the context of current agency capability and resources.\n- National Traffic Signal Report Card Technical Report & Traffic Signal Operations Self Assessment Survey - This report provides a summary of the state of the practice in traffic signal management and operations. Agencies can gain insight on the fundamental principles and core objectives involved in effectively managing and operating traffic signal systems.\n- Traffic Signal Operations and Maintenance Staffing Guidelines (HTML, PDF 821KB) (Publication Number: FHWA-HOP-09-006) - This report provides a guideline to estimate the staffing and resource needs required to effectively operate and maintain traffic signal systems.\n- The FHWA Model Systems Engineering Documents for Adaptive Signal Control Technology (ASCT) Systems (HTML, PDF 1.2MB) (Publication Number: FHWA-HOP-11-027) - This report facilitates the development of documentation that informs the design, implementation and testing of adaptive control systems that support and work within the context of the agencies objectives and needs.\n- NCHRP 560 Guide to Contracting ITS Projects (PDF 883KB) - Guide to Contracting ITS Projects provides guidance on the procurement of intelligent transportation systems (ITS), including variable message signs, traffic detectors, signal controllers, and a variety of other hardware and software that entails applications of advanced electronics and information management to regulate and facilitate traffic flow. The report highlights best practices and recommends contracting strategies and contract types, terms, and conditions for ITS development, integration, system acceptance, warranty, maintenance, and upgrade.\n- Additional Publications\nThe FHWA Resource Center Operations Team is pleased to offer its services to assist agencies with the development of traffic signal management plans. This is typically accomplished through a review of current practices and collaborative development of strategies to achieve excellence in traffic signal management."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:f281b660-76f5-48df-8300-d830eff0c7e1>"],"error":null}
{"question":null,"answer":null,"context":null,"question_categories":[],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":null,"error":"Conversation generation aborted while generating turn 1: It is impossible to generate a valid next turn even after 3 trials."}