{"question":"日本面条夏天咋吃？need quick recipe","answer":"During summer in Japan, noodles are traditionally served cold for a refreshing meal. Somen noodles are especially popular in summer due to their light and delicate texture. They are typically served cold with a dipping sauce and garnished with green onions, grated ginger, or shredded nori seaweed. For a quick and refreshing preparation, cook the noodles until slightly chewy, then immediately rinse under cold water and serve floating in a bowl of ice water with a chilled dipping sauce. This makes for a perfect solution when you have unexpected guests, as you can prepare a nutritious and satisfying meal in about twenty minutes.","context":["An Exploration of Japan’s Beloved Noodle Varieties\nWhen it comes to Japanese cuisine, noodles play a central role in delivering rich flavors and satisfying meals. Among the wide array of noodle varieties available, ramen, udon, and somen noodles stand out as some of the most popular and beloved options. In this article, we’ll delve into the differences between these three noodle types, exploring their origins, unique characteristics, and how they are enjoyed in Japanese culture.\nUnderstanding Ramen Noodles\nRamen noodles have gained a global reputation for their delightful flavors and diverse preparations. Originating from China, ramen noodles have evolved into a quintessential dish in Japanese cuisine. Made from wheat flour, salt, water, and kansui (alkaline water), ramen noodles offer a distinct chewy texture that sets them apart.\nRegional Styles of Ramen\nOne of the fascinating aspects of ramen is the variety of regional styles found throughout Japan. From the rich and creamy tonkotsu ramen of Fukuoka to the soy-based shoyu ramen of Tokyo, each region boasts its own unique twist on this beloved dish. The choice of broth, toppings, and even noodle thickness can vary significantly, resulting in a diverse culinary landscape.\nExploring Udon Noodles\nUdon noodles, on the other hand, offer a thicker and more substantial experience. Made from wheat flour, salt, and water, udon noodles are known for their soft and chewy texture. These noodles are believed to have originated in Japan during the Nara period and have since become a staple in Japanese cuisine.\nTraditional Preparation and Variations\nUdon noodles are traditionally prepared by kneading the dough and rolling it out before cutting it into thick strands. The resulting noodles are then boiled and served in a variety of dishes, such as hot noodle soups or chilled with a dipping sauce. With its versatility, udon can be enjoyed in a range of flavorsome preparations, including kitsune udon with sweet fried tofu or tempura udon with crispy battered vegetables.\nUnveiling Somen Noodles\nSomen noodles, while less widely known outside Japan, hold a special place in Japanese culture. These thin, delicate noodles are made from wheat flour and are often associated with summertime due to their light and refreshing qualities. Somen noodles have a silky texture that distinguishes them from other varieties.\nTraditional Serving Methods\nSomen noodles are traditionally served cold, accompanied by a dipping sauce and garnished with toppings like green onions, grated ginger, or shredded nori seaweed. During the warmer months, somen is often enjoyed as a chilled dish, offering a delightful way to beat the heat. The delicate nature of somen noodles makes them a perfect canvas for absorbing the flavors of the dipping sauce.\nFAQ: Common Questions about Ramen, Udon, and Somen Noodles\n1. What are the key differences between ramen, udon, and somen noodles?\nRamen noodles are thin and springy, udon noodles are thick and chewy, while somen noodles are thin and delicate.\n2. Are the cooking methods different for each noodle type?\nYes, the cooking methods vary. Ramen noodles are typically boiled, udon noodles are boiled and then rinsed, while somen noodles are boiled and then chilled.\n3. Which noodle type is best for soup-based dishes?\nRamen noodles are commonly used in soup-based dishes, as their texture holds up well in flavorful broths.\n4. Can these noodles be used interchangeably in recipes?\nWhile each noodle type has its own distinct characteristics, they can be substituted in certain recipes depending on personal preference. However, for an authentic taste, it’s best to use the recommended noodle type.\nIn conclusion, ramen, udon, and somen noodles are three distinct and beloved varieties that showcase the diversity of Japanese cuisine. Ramen noodles offer a range of regional styles with unique broths and toppings, while udon noodles provide a thicker and heartier experience. Somen noodles, on the other hand, offer a delicate and refreshing option, particularly suited for warmer seasons. Whether you’re a fan of the rich flavors of ramen, the chewy satisfaction of udon, or the light elegance of somen, these noodles are sure to delight your taste buds and transport you to the vibrant culinary world of Japan.\nSo why not embark on a culinary adventure and try all three noodle varieties? Indulge in the diverse flavors and textures, and discover your personal favorite among these iconic Japanese noodles.\nRemember, the world of noodles is vast and ever-evolving, so keep exploring and savoring the wonders it has to offer!\nArticle word count: XXXX words","Japanese Linguine Style Noodles\nA thick, cream-colored wheat noodle, udon, resembles linguine. Udon noodles are made from 100 percent whole wheat flour or a combination of whole wheat and unbleached white flour. One hundred percent whole wheat udon is a sturdy noodle with a full whole wheat flavor. There are other lighter, smoother udons that readily absorb the flavors of broths, sauces, and seasonings. Brown rice udon (genmai udon), a combination of brown rice flour and wheat flour, is not a traditional Japanese food, but rather was developed by Mitoku especially for the natural foods market.\nSakurai noodles are an extraordinary line of traditionally made, whole grain Japanese pasta that have been made for the last 80 years by the Sakurai family in their small shop located in the foothills of the Japanese alps. The family freshly mills select grains for each batch, and prepares their pastas by the centuries-old-roll-and-cut method. The long strands are slowly air-dried, using no artificial heat, then cut and packaged. Their skill and commitment to quality is evidenced by the fresh, homemade flavor and uncommonly good texture of Sakurai pastas. Exceptional!\nUses: Though usually served in broth or with a strong-flavored dipping sauce, udon is also good when pan-fried. It can be used in noodle salads as well. Easy to prepare, udon is ideal when on the road or camping.\n- Use the Tabs below to Select your Favorite Recipe...Bon appétit!\nCooking with Udon\nDuring the heat of summer, noodles are deliciously refreshing when served floating in a bowl of ice water and accompanied by a chilled dipping sauce. For warmth in the winter, noodles are commonly served in piping hot broth. Whether in soups or salads, sautéed with vegetables, deep-fried, baked, or topped with sauce, noodles are delicious.\nQuick to prepare, they provide the perfect solution when you have unexpected guests. In the time it takes for the water to boil and the noodles to cook, you can prepare a broth or sauce and a vegetable dish and voilà! In twenty minutes you can create a nutritious and satisfying meal.\nSince most Japanese noodles are made with salt, it is not necessary or advisable to add salt to the cooking water. In a large pot, bring the water (about ten cups of water for every eight ounces of noodles) to a full rolling boil. Add the noodles a few at a time so as not to completely stop the boiling. Stir gently until the water is boiling rapidly again to prevent the noodles from sticking to the bottom of the pan. If too many noodles are added at once, the water will not quickly return to a boil and the noodles will be overcooked on the outside and undercooked on the inside. Also, using too little water will result in sticky, unevenly cooked noodles.\nThere are two acceptable methods for cooking noodles. The first method is quite basic. Simply bring the water in the pot back to a rapid boil once all the noodles have been added (as just described), then cook the noodles over medium heat until done. The second method is known as the \"shock method\". Once the noodles have been added to the pot and the water returns to a rolling boil, a cup of cold water is added to \"shock\" the noodles. When the water returns to a boil again, another cup of cold water is added. This is repeated three or four times until the noodles are cooked. No matter which method is used, noodles should be tested often to avoid overcooking. A properly cooked noodle should be slightly chewy. When broken in half, the noodle should be the same color throughout.\nOnce cooked, immediately drain and rinse the noodles in two or three cold-water baths or under cold running water to prevent further cooking and to keep the noodles from sticking together. When they have cooled enough to handle, drain and set aside until ready to assemble your dish. If reheating is necessary, place individual noodles in a strainer or colander and submerge in a pot of boiling water until just heated. Drain well and serve.\nThe noodle cooking water can be reserved, allowed to sour slightly, and then used as a natural leavening agent in breads, muffins, and pancakes.\n|Noodles in Broth|\nThis popular, satisfying dish takes little time to prepare. Simply served with a garnish of scallion, Noodles in Broth makes a filling lunch or a substitute for soup in a heartier meal. You can top the noodles with a colorful assortment of steamed, simmered, or deep-fried vegetables; fish; tofu; mochi; or seitan for a complete dinner. Udon or soba are recommended.\n3 cups Shiitake Dashi (see Shiitake Mushroom recipes)\n1 package Mitoku 100% Udon\n1/8 teaspoon Masu 100% Sea Water Salt\n2 tablespoons Mitoku Yaemon Tamari\n1 1/2 tablespoons Mitoku Mikawa Mirin\n1-2 teaspoons fresh ginger juice\nFinely minced scallion for garnish\nCook noodles according to the directions on the package, then rinse under cold running water, drain, and set aside. Meanwhile, in a medium-sized pot combine the Shiitake Dashi, salt, tamari or shoyu, and mirin. Simmer 1 minute. Remove from heat and add ginger juice.\nTo serve, divide noodles in deep individual serving bowls. Ladle hot broth over noodles to almost cover, and garnish with minced scallion or your choice of topping.\nJapanese-Style Fried Noodles\nIn this tasty Japanese dish, cooked noodles are simply tossed in a pan with sautéed vegetables and a small amount of sweet and savory sauce. Mirin and mellow miso combine for a perfect marriage of flavors. Minced scallion adds color as well as fresh crispness.\n8 ounces uncooked Mitoku Sakurai Udon\n2 1/2 tablespoons Mitoku Sweet White Miso\n2 1/2 tablespoons Mitoku Mikawa Mirin\n1 tablespoon Mitoku Virgin or Toasted Sesame Oil\n2 tablespoons minced shallot or 2 cloves garlic, finely minced\nMinced scallion for garnish\nCook noodles according to the directions on the package, then rinse under cold running water, drain, and set aside.\nCombine miso and mirin in a small bowl. Heat oil in a large skillet, add minced shallot or garlic, and sauté over medium-low heat 1 minute. (Be careful not to brown garlic or it will become bitter.) Add miso-mirin mixture to the skillet, then add noodles and toss to evenly coat. (It may be necessary to add a little water.) Sauté 1 minute more, then remove from heat. Serve immediately with a generous sprinkling of scallion."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:52f9b3a0-8315-4be8-a113-63c19cdd02d4>","<urn:uuid:dbbae74b-5007-4656-97a8-05f056e39546>"],"error":null}
{"question":"From an agricultural development perspective, how do the research focuses of PROCROP and MENTICA differ in their approach to understanding early farming communities - specifically regarding plant domestication in the Middle East versus modern crop improvement through apomixis?","answer":"PROCROP and MENTICA have distinct agricultural research focuses. PROCROP concentrates on modern crop improvement through genetic mechanisms, specifically studying apomixis (clonal seed production) to enable instantaneous fixation of elite crop genomes, with potential economic impacts of €1800-2300 million per annum per crop. In contrast, MENTICA studies the historical origins of agriculture in the Zagros mountains of Iraq and Iran from 17,000-7000 BC, examining how early communities first domesticated plants and animals, including cereals and goats, establishing the foundations for settled farming communities along ancient trade routes.","context":["Explore the words cloud of the PROCROP project. It provides you a very rough idea of what is the project \"PROCROP\" about.\nThe following table provides information about the project.\nUNIVERSITA DEGLI STUDI DI PERUGIA\n|Coordinator Country||Italy [IT]|\n|Total cost||738˙000 €|\n|EC max contribution||738˙000 € (100%)|\n1. H2020-EU.1.3.3. (Stimulating innovation by means of cross-fertilisation of knowledge)\n|Duration (year-month-day)||from 2015-02-01 to 2019-01-31|\nTake a look of project's partnership.\n|1||UNIVERSITA DEGLI STUDI DI PERUGIA||IT (PERUGIA)||coordinator||225˙000.00|\n|2||UNIVERSITA DEGLI STUDI DI MILANO||IT (MILANO)||participant||211˙500.00|\n|3||POLO D'INNOVAZIONE DI GENOMICA, GENETICA E BIOLOGIA SCARL||IT (SIENA)||participant||157˙500.00|\n|4||INSTITUT DE RECHERCHE POUR LE DEVELOPPEMENT||FR (MARSEILLE)||participant||144˙000.00|\n|5||CENTRO DE INVESTIGACION Y DE ESTUDIOS AVANZADOS DEL INSTITUTO POLITECNICO NACIONAL||MX (MEXICO)||partner||0.00|\n|6||CONSEJO NACIONAL DE INVESTIGACIONES CIENTIFICAS Y TECNICAS (CONICET)||AR (BUENOS AIRES)||partner||0.00|\nIncreased crop productivity through genetic improvement of plants has significantly impacted world agriculture and the world’s population. Crop plants have followed the general pattern of introduction, selection, and hybridization. Once introgressed, selection and breeding strategies have led to new cultivars with improved yield and adaptation. Unfortunately, many of these important traits are typically polygenic. The consequence is that only certain unique allele combinations comply to generate elite performing genotypes. The fixation of a given genotype occurs naturally in species that display an asexual type of seed production named apomixis (i.e. clonal seed production). Unfortunately, apomixis does not naturally occur in major crop species with few exceptions (Citrus, mango and mangosteen). In crop species, apomixis would enable the instantaneous fixation of the complete genome of the best plants. When coupled with male-sterility systems, apomictic reproduction (with no need for male contribution) could help in addressing issues related to transgene escape from GM crops to organic or conventional crops, and thereby allow for better coexistence systems. This trait by itself is highly valuable for agriculture, but despite many efforts it has never been possible to introduce it into the domesticated crop species of today. The financial and economic impacts of the development of apomixis technology and its application to major crops are amazing (€1800-2300 million per annum per crop). The overall goal of the proposal is to allow for a synergy of inter-related European and international expertise to better understand the mechanisms of sexual/apomictic plant reproduction and to facilitate the application of this increased knowledge in the development of new approaches for agriculture and food industry to increase productivity.\n|Delivery of candidate genes||Documents, reports||2019-08-06 17:50:54|\n|Genomic sequence annotation||Documents, reports||2019-08-06 17:50:54|\n|Mutant/transformants functional analysis||Other||2019-08-06 17:50:55|\n|Final Workshop||Other||2019-08-06 17:50:54|\n|annotation of genes involved in apomixis||Documents, reports||2019-08-06 17:50:55|\n|Hypotheses for ASR functional role||Other||2019-08-06 17:50:54|\n|A list of candidates showing the same pattern of differential expression in aposporous and diplosporous plants||Other||2019-08-06 17:50:54|\n|list of selected candidates suitable to be tested using reverse genetics approaches||Other||2019-08-06 17:50:54|\n|Organization of Workshop 2||Other||2019-08-06 17:50:54|\n|Website & Depository of sequence||Other||2019-08-06 17:50:54|\n|Identification of gene expression patterns distinguishing reproductive behaviours||Documents, reports||2019-08-06 17:50:54|\n|Organization of Workshop 1||Other||2019-08-06 17:50:54|\n|Delivery of DNA sample from 2x Paspalum to INDEAR||Other||2019-08-06 17:50:54|\nTake a look to the deliverables list in detail: detailed list of PROCROP deliverables.\n|year||authors and title||journal||last update|\nMicaela Mancini, Hugo Permingeat, Carolina Colono, Lorena Siena, Fulvio Pupilli, Celeste Azzaro, Diva Maria de Alencar Dusi, Vera Tavares de Campos Carneiro, Maricel Podio, JosÃ© Guillermo Seijo, Ana MarÃa GonzÃ¡lez, Silvina A. Felitti, Juan Pablo A. Ortiz, Olivier Leblanc, Silvina C. Pessino\nThe MAP3K-Coding QUI-GON JINN (QGJ) Gene Is Essential to the Formation of Unreduced Embryo Sacs in Paspalum\npublished pages: , ISSN: 1664-462X, DOI: 10.3389/fpls.2018.01547\n|Frontiers in Plant Science 9||2019-08-06|\nIngrid Garbus, JosÃ© Rodolfo Romero, Juan Pablo Selva, MarÃa Cielo Pasten, Carolina Chinestra, JosÃ© Carballo, Diego Carlos Zappacosta, Viviana Echenique\nDe novo transcriptome sequencing and assembly from apomictic and sexual Eragrostis curvula genotypes\npublished pages: e0185595, ISSN: 1932-6203, DOI: 10.1371/journal.pone.0185595\n|PLOS ONE 12/11||2019-08-06|\nJuan Pablo A. Ortiz, Santiago Revale, Lorena A. Siena, Maricel Podio, Luciana Delgado, Juliana Stein, Olivier Leblanc, Silvina C. Pessino\nA reference floral transcriptome of sexual and apomictic Paspalum notatum\npublished pages: , ISSN: 1471-2164, DOI: 10.1186/s12864-017-3700-z\n|BMC Genomics 18/1||2019-08-06|\nJuan Manuel Rodrigo, Diego Carlos Zappacosta, Juan Pablo Selva, Ingrid Garbus, Emidio Albertini, Viviana Echenique\nApomixis frequency under stress conditions in weeping lovegrass (Eragrostis curvula)\npublished pages: e0175852, ISSN: 1932-6203, DOI: 10.1371/journal.pone.0175852\n|PLOS ONE 12/4||2019-08-06|\nAre you the coordinator (or a participant) of this project? Plaese send me more information about the \"PROCROP\" project.\nFor instance: the website url (it has not provided by EU-opendata yet), the logo, a more detailed description of the project (in plain text as a rtf file or a word file), some pictures (as picture files, not embedded into any word file), twitter account, linkedin page, etc.\nSend me an email (firstname.lastname@example.org) and I put them in your project's page as son as possible.\nThanks. And then put a link of this page into your project's website.\nThe information about \"PROCROP\" are provided by the European Opendata Portal: CORDIS opendata.","Major archaeological project wins EU funding to investigate the earliest settled societies in Iraq and Iran, 17,000-7000 BC\nRelease Date 08 June 2018\nUniversity of Reading archaeologists have succeeded in winning a highly prestigious European Research Council Advanced Grant of €2.5 million (£2.2 million) for a multi-disciplinary project entitled MENTICA Middle East Neolithic Transition – Integrated Community Approaches.\nThe five-year grant will support six work packages involving archaeological excavations and scientific analysis of early agricultural communities in the Zagros mountains of Iraq and Iran, one of the core regions where human societies first made the transition from mobile hunter-gatherers to settled farmers, founding the basis for the cities and civilisations that followed.\nThe study is one of the largest of its kind ever undertaken to provide the scale needed to investigate the local and regional development of communities along a major ancient routeway (later the Silk Road). It is led by Professor Roger Matthews with Dr Wendy Matthews, both in the Department of Archaeology, University of Reading. All work will be in full collaboration with colleagues in universities and state antiquities departments in Iran and Iraq.\nProfessor Roger Matthews said: “This generous grant from the ERC will allow us to highlight the unique archaeological significance of this region of the Middle East for one of the most significant episodes of change in the history of humanity. Our project is also seeking to establish a sense of community ownership of heritage sites, through engagement with local and national communities, which is vital for their protection in conflict-affected regions.”\nThe team of researchers and excavators will excavate at the important heritage sites Zarzi and Bestansur in Iraq, and Sheikh-e Abad in Iran which span these transformations from 17,000 to 7000 BC. These key sites lie within the under-researched area of eastern Fertile Crescent in the Middle East where communities were first able to settle and farm on fertile land in the biodiverse uplands east of the Tigris and Euphrates rivers.\nThe team will explore how ancient communities in the highlands and lowlands of Iran and Iraq were connected with each other and with other contemporary communities across the Fertile Crescent and beyond, following the routes later known as the Silk Road. Evidence previously uncovered at Sheikh-e Abad and Bestansur suggests people traded in materials across more than 1,500 km and that they were amongst the first communities to domesticate plants and animals, such as cereals and goats.\n\"Prehistoric people in this region faced many challenges to their new community lifestyle which they had not experienced before\" - Dr Wendy Matthews, University of Reading\nPrevious excavations by Roger and Wendy in this region, supported by grants from the Arts and Humanities Research Council, the British Academy, National Geographic Society and other bodies, have uncovered evidence of Neolithic buildings constructed by prehistoric communities to support farming and trading lifestyles. The elaborate buildings and the artefacts discovered within them are being studied using a range of state-of-the art techniques.\nDr Wendy Matthews said: “Human societies have always had to adapt to change. Prehistoric people in this region faced many challenges to their new community lifestyle which they had not experienced before, such as living in larger more densely populated settlements and diseases that affected them, their crops and livestock. We hope to learn lessons from the past about how humans addressed these global challenges and climate change, allowing more complex societies to thrive.”\nRoger and Wendy are the only archaeologists currently directing archaeological research in both eastern Iraq and western Iran in collaboration with Iraqi and Iranian colleagues. Together with these colleagues they are providing unique insights into one of the core regions in the origins of agriculture and settled life, which are the foundation of communities today. As President of RASHID International, Professor Matthews works closely with local communities and authorities to preserve and promote awareness of Iraq’s cultural heritage. The organisation collects information on damage sustained at the sites and lobbies for international laws and agreements to protect them."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"content_constrained"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:8819c13f-1063-46b4-a09a-59e014bee7e2>","<urn:uuid:ac206501-fd05-4f28-a93c-69b62256b806>"],"error":null}
{"question":"How do governance frameworks address service quality control versus project quality control?","answer":"Service quality control and project quality control are addressed differently in governance frameworks. For ITSM governance, service quality control focuses on managing service-impacting incidents, implementing change control processes, and ensuring service delivery meets customer needs through policies, processes, and metrics that control and evaluate service-related issues. In contrast, project quality control under frameworks like PRINCE2 involves specific quality management processes for checking the quality of work done on the project through testing or reviewing work, while also managing risks and controlling changes to prevent the project from going in the wrong direction.","context":["Delivering consistent and quality IT services for customers is not easy – and can be even more challenging – if they are not governed effectively. For example, how can an IT organisation look to improve if it doesn’t measure the amount of service-impacting incidents properly?\nTake the high profile service outages of several major banks in recent years for example. Their customers were unable to make transactions or access services for periods of time. Even in such a highly regulated environment as financial services, where IT is governance is generally tighter, there are no guarantees that the outages could’ve been prevented by governance alone.\nEqually, too much governance could be seen as overly bureaucratic. A complicated – and lengthy – change control process could drive the wrong behaviour from some members of the IT organisation in that they may simply bypass the process.\nIn any case, a business is often dependent on its IT services, and as such, there needs to be controls in place to not only protect – but gain value for – their customers. This of course needs to be appropriate as not all businesses are financial service providers needing tight control.\nWhat is governance and why is it important?\nBefore implementing any type of governance, it is worth understanding what it actually is. According to Wikipedia, “governance refers to all processes of governing undertaken…and relates to the interaction and decision-making among the actors involved in a collective problem”.\nThe Harvard Business School describe IT governance as “specifying the decision rights and the decision-making mechanics to foster the desired behaviour in the use of IT”.\nA key thing to note is that governance is not the same as management. Ultimately, ITSM governance is concerned with control, compliance and performance.\nIt is important that ITSM governance has effective decision-making in place; drives the right behaviours (and, by implication, discourages the wrong behaviour); and has policy and processes are in place so that it is easier to discover issues and remedy them quicker.\nGoing back to our banking example earlier, HSBC had an issue with ATMs and Online Banking in 2011 but were able to pinpoint it and restore service within 2-3 hours. If they didn’t have good governance in place, it feasibly could have taken considerably longer to obtain information and decisions.\nWhat are the different aspects of ITSM governance?\nIn order to understand, design and communicate effective ITSM governance, Harvard Business School suggests “a decision, rights and accountability framework” should be created that covers aspects like:\n- What decisions should be made and what information should be considered\n- Who can make decisions and who is accountable for them\n- How can decisions and governance be measured?\nYou might also want to consider different aspects like those the in the table below:\n|Aspects||Questions or things to consider|\n|1. People||Communicating with guiding principles that inform and involve all relevant staff; leverage their expertise; and ensure strong input from Senior Management|\n|2. Process||Governance should be controlled and executed through policy, process, ownership and performance|\n|3. Technology||What technology and tools are required to support the process?|\n|4. Information||What data such as measurements and metrics are required to inform decision making?|\n|5. Services||What are they; how much do they cost; and how do they add value to the business?|\n|6. Suppliers||What are their processes and metrics and how are they involved in your governance?|\n|7. Customers||Who are your customers and how do they benefit from your governance?\nHow can you evidence your governance improves service costs, their perception and value delivery?\n|8. Corporate Governance||How does your governance align to the corporate governance, strategic objectives and architecture; and are IT involved at the right level within the organisation in this regard?|\nHow is ITSM governance executed?\nAfter considering what aspects to include in ITSM governance, it is equally important to consider how to design and execute it in practice. The following are some suggestions you might want to consider when implementing ITSM governance.\nFirstly, identify the types of frameworks and methods to be used – particularly if you are starting from scratch. Whilst not exhaustive, the following are some common methods and how they can be applied:\n- COBIT is an IT governance framework that focuses on what should be covered in processes and procedures and they can be directed and controlled.\n- ISO/IEC standards like 20000 (Service Management), 27000 (Security) and 38500 (IT Governance) are international standards provide specific advice and controls IT can be audited against to gain industry recognised certification\n- TOGAF is a framework for enterprise architecture that provides an approach for designing, planning, implementing, and governing an enterprise and service orientated architecture\n- Other specific best practices for governance such as PRINCE2 for projects; USMBOK and ITIL for service; MoR for risk management; CMMI for benchmarking and maturity.\nSecondly, ITSM needs to be involved with – or even own – certain internal governing bodies like:\n- IT Pipeline and Portfolio Board to understand the upcoming projects and be ready to design, transition and operate the services being delivered as necessary\n- Architecture Governance Board to influence and ratify all architecture designs and decisions\n- Change Advisory Board to review/approve changes – particularly to the live production environment\n- Other Governance or Steering Groups involving the business to ensure IT is represented appropriately\nThirdly, ITSM Governance needs to ensure key policies, processes and metrics in place. This may vary depending on the needs of the organisation but things like incident, change and release policies should be created to ensure service-related issues or changes are controlled, evaluated, measured and resolved in appropriate way to ensure minimum risk and impact to the business.\nFinally, and arguably, the most important thing is to build an improvement culture that involves the support of the whole IT organisation. By establishing quick wins; involving staff in the policy development; and empowering them to take ownership as appropriate; and using improvement techniques Deming’s Plan Do Check Act cycle; ITSM governance is more likely to be established accepted and acted upon by the IT organisation.\nThe key things to remember when implementing ITSM governance are to:\n- Ensure it is appropriate for your organisation and limit bureaucracy were possible\n- Remember that governance is not management and is primarily about driving effective decision-making and ensuring control and performance of services\n- Make sure it aligns to the strategic and corporate governance and objectives of your organisation\n- Control, improve and mature governance through policy, process, benchmarks and measurements using industry best practice if practicable to do so.\n- Develop and maintain an improvement culture within the IT organisation so that staff understand the value of – and contribute to the success of – ITSM governance\n- UK banks struck by IT outages – Information Age (2011)\n- SOA governance – Wikipedia (2016) –\n- ISO/IEC 38500 – Corporate Governance of IT – Wikipedia (2016)\n- The Role of ITIL in IT Governance – Rick Leopaldi RL Information Consulting LLC (2016) –\n- Why Does IT Governance Matter? – Ricardo Chavira Associate Director, Governance, Service Management & eServices (Yale) (2013)\n- Governance over IT Service Management Processes using COBIT 5.0 – Ben Martin (2013)\n- 6 Success Factors for ITSM Governance Structures – Edward Rivard for ITSMWatch (2011)\n- IT Service Management leads to IT Governance – ITSM Academy (2010)\n- Governing ITIL with COBIT – David Nichols (2008)\nThis article was contributed by Jon Morely – Vice-Chair of the itSMF UK Service Transition Special Interest Group and IT Service Transition Manager at the University of Nottingham.","What is PRINCE2?\nBefore we go into the specifics of PRINCE2, there are some general points about the subject of project management, which should help put everything into context…\nWhenever we decide we want to do something, go somewhere, build something, achieve something, we need to know the answer to some questions. What are we trying to do? When will we start? What do we need? Can we do it alone, or do we need help? How long will it take? How much will it cost? These are typical questions asked at the start of any project and the answers are the building blocks of project management – defining what we want to do and working out the best way we can do it.\nStructured project management means managing the project in a logical, organised way, following defined steps. A structured project management method is the written description of this logical, organised approach. PRINCE2 is a structured project management method.\nWe know from experience that projects which aren’t organised and controlled properly usually go disastrously wrong. Some of the big ones hit the press. London Ambulance and Channel Tunnel, for example, both experienced very public problems of systems not working properly and huge overspends. Structured project management methods have been developed to try to prevent such disasters.\nWe have briefly covered what structured project management, and hence PRINCE2, are all about. Now for some more details about the PRINCE2 method.\nPRINCE2 says that a project should have:\n- An organised and controlled start, ie organise and plan things properly before leaping in;\n- An organised and controlled middle, ie when the project has started, make sure it continues to be organised and controlled;\n- An organised and controlled end, ie when you’ve got what you want and the project has finished, tidy up the loose ends.\nIn order to describe what a project should do when, PRINCE2 has a series of processes which cover all the activities needed on a project from starting up to closing down.\nOrganising and controlling a project means that we need to have someone responsible for doing the organising and controlling – this person is called the Project Manager. The Project Manager will select people to do the work on the project and will be responsible for making sure the work is done properly and on-time. The Project Manager draws up the Project Plans that describe what the project team will actually be doing and when they expect to finish.\nThe person who is paying for the project is called the customer or executive. The person who is going to use the results or outcome of the project, or who will be impacted by the outcome of a project, is called the user. On some projects, the customer and user may be the same person. The person who provides the expertise to do the actual work on the project (ie will be designing and building the outcome) is called the supplier or specialist. All of these people need to be organised and co-ordinated so that the project delivers the required outcome within budget, on time and to the appropriate quality.\nEach PRINCE2 project will have a Project Board made up of the customer (or executive), someone who can represent the user side and someone to represent the supplier or specialist input. In PRINCE2, these people are called Customer, Senior User and Senior Supplier respectively. The Project Manager reports regularly to the Project Board, keeping them informed of progress and highlighting any problems he/she can foresee. The Project Board is responsible for providing the Project Manager with the necessary decisions for the project to proceed and to overcome any problems.\nProviding an independent view of how the project is progressing is the job of Project Assurance. In PRINCE2, there are three views of assurance; business, user and specialist. Each view reflects the interests of the three Project Board members. Assurance is about checking that the project remains viable in terms of costs and benefits (business assurance), checking that the users’ requirements are being met (user assurance), and that the project is delivering a suitable solution (specialist or technical assurance). On some projects, the assurance is done by a separate team of people called the Project Assurance Team, but the assurance job can be done by the individual members of the Project Board themselves.\nOn most projects there is a lot of administrative work needed, keeping everyone informed, arranging meetings, keeping plans up-to-date, chasing things up, keeping files, etc. Project Managers often do all this work themselves, particularly on smaller projects. But if there are a number of projects going on at the same time, a Project Support Office can be setup to help the Project Managers with this work.\nApart from describing the different people involved on a PRINCE2 project, and what they are each responsible for, the method also explains how to manage risk, how to manage quality, and how to control change on the project. Risk Management is about working out what could go wrong and planning what to do if it does. Quality Management is about checking the quality of work done on the project, either by testing it or reviewing the work in some way. There are always lots of changes during the life of a project; people change their minds, other things happen, which affect what the project is doing. PRINCE2 has a technique of controlling the way changes impact the project in order to prevent the project going off in the wrong direction.\nSo, PRINCE2 is a method for managing projects. It helps you work out who should be involved and what they will be responsible for. It gives you a set of processes to work through and explains what information you should be gathering along the way. But PRINCE2 doesn’t do the work for you, it cannot guarantee that your projects will be successful. Good projects, which deliver quality results, on-time and within budget are dependent on the quality of people involved from Project Board down to individual team members."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:a71d3327-558e-4e1e-bb75-d46547ba9dd1>","<urn:uuid:1ade8067-3b42-4297-9243-ba028c51d4dc>"],"error":null}
{"question":"How do vitamin supplementation needs compare between breastfed babies and formula-fed babies in their first year? 💊","answer":"Breastfed babies and those who don't drink enough formula may need vitamin D supplements, while infants who consume more than one liter of iron-fortified formula daily typically don't need vitamin and mineral supplements. Both groups might need fluoride supplements after 6 months if the water supply lacks adequate fluoride. Additionally, for both feeding types, vitamin A supplementation should begin at 6 months of age, with capsules given every six months between 6 and 9 months of age in vitamin A-deficient areas.","context":["Choosing how to feed your baby is an important decision that has life-long effects for your baby and for you. What you have seen and learned about infant feeding from your family, friends, and teachers is likely to influence your attitude and perceptions. Whether you definitely plan to breastfeed or you are still uncertain, consider the fact that your milk is the best milk for your baby, and it is the ideal first food for your baby's first several months.\nNature designed human milk especially for human babies, and it has several advantages over any substitute ever developed. Your milk contains just the right balance of nutrients, and it contains them in a form most easily used by the human baby's immature body systems. Because it was developed for your human baby, your milk is also the most gentle on your baby's systems.\nIf you decide not to breastfeed, or are unable to breastfeed, commercial iron-fortified formulas can provide adequate nutrition for your infant. Infant formulas contain adequate amounts of protein, calories, fat, vitamins, and minerals for growth. However, formula does not contain the immune factors that are in breast milk. The immune factors that are found in breast milk can help prevent infections.\nInfants who take enough iron-fortified infant formula(over one liter a day) usually do not need vitamin and mineral supplements. Fluoride is sometimes needed after 6 months of age, if the water supply does not have enough fluoride. Consult your infant's doctor about the need for fluoride supplements. If your baby is breastfed or still too small to drink enough formula, ask your infant's doctor about vitamin D supplements.\nThe following are some helpful hints for feeding your baby:\nBreast milk is best for your baby and is beneficial even if you only breastfeed for a short amount of time, or part-time.\nOffer cow's milk-based formula with iron as first choice of formula, if you do not breastfeed.\nKeep your baby on breast milk or baby formula until he/she is 1-year-old.\nStart solid foods when your baby can hold up his/her head, sit-up with support, and no longer has tongue thrusting (4 to 6 months).\nWhen starting solids, begin with rice cereal mixed with breast milk or formula on a spoon. Do not give solids in the bottle or with an infant feeder.\nOnce your baby is tolerating cereals for two to three weeks, offer vegetables, then add fruits, and then meats.\nAsk your child's doctor about the best way to add new foods to your baby's diet.\nProgress in texture of foods so that your baby is eating table foods by his/her first birthday.\nDo not give these to your child during his/her first year of life:\nFoods that can be easily choked on (such as hot dogs, peanuts, grapes, raisins, popcorn)\nLimit juice to 4 to 6 ounces a day. Water is not necessary for infants; better choices include formula, breast milk, or juice for added nutrition.\nUnless your child is known to have or has severe allergies (for instance, breaking out in hives, vomiting, or having trouble breathing), recent reports and studies have shown that introducing whole eggs and peanut butter at a young age - even at 4 to 6 months - reduces the chance of your child developing allergies to these foods. Talk to your child's doctor about whether these foods are appropriate for your child.","As with breast-feeding, child deaths can be reduced by 6 per cent by improving complementary feeding. Complementary feeding (CF) is one of the top three preventative interventions impacting child mortality. Appropriate CF refers to the requirements for feeding an infant and includes timely introduction of solid, semisolid or soft foods in the period of 6–8 months of age in addition to, or complementary to, breast milk. It is important to emphasise that CF is taken together with continued breast-feeding. It also includes the Minimum Acceptable Diet which has two separate components:\n- Minimum Dietary Diversity - receiving foods from four or more different food groups in a single day\n- Minimum Meal Frequency - the number of times per day that a young child receives solid, semi-solid or soft food. The appropriate number of times per day differs by age group, as follows:\n- 6-8 months, breast-fed: 2-3 meals/day\n- 9-11 months, breast-fed: 3-4 meals/day\n- 12-24 months, breast-fed: 3-4 meals + 1-2 snacks/day\n- 6-24 months, but not breast-fed: 4 meals + 1-2 snacks/day\nAppropriate CF also includes iron-rich foods. This includes iron-fortified foods that are specially designed for infants and young children, home-based fortified products, such as Sprinkles, and foods naturally rich in iron, such as animal-source foods and dark green leafy vegetables. 7-11 programming raises awareness around all of these appropriate CF components, especially promoting locally available foods. Again, in order to ensure that the child is eating appropriately and is healthy, the caregivers should participate in a growth monitoring and promotion programme. Once the child has completed his/her immunisations, he/she should be taken for weighing every two to three months, which provides another opportunity to reinforce the above messages.\n- Attend growth monitoring\n- Responsive feeding with continued breast-feeding\n- Diet includes minimum acceptable diet\n- 6-24 months: give 4-5 meals + 1-2 snacks per day from 4 food groups\n- Caregiver provides variety of food that includes animal-source foods using responsive feeding techniques\n- 6-8 months: small frequent feeds through the day and no watery foods (2-3 meals/day)\n- 9-11 months: increase frequency and amount of foods (3-4 meals/day)\n- 12-24 months: 3-4 meals + 1-2 snacks/day\n- Caregiver takes child to growth monitoring and promotion every month until immunisations are complete and then every 2-3 months, to ensure appropriate weight gain and identify problems before the child becomes undernourished\nVitamin A Supplementation\nVitamin A is essential to health. When the body does not have enough vitamin A, the immune system function is compromised, making the child more susceptible to disease, and more likely to die from that disease. by providing vitamin A (through supplements, fortification or adequate diet) to children under 5 years in vitamin-A-deficient areas, mortality rates in general could be reduced by up to 23 per cent.\nAs with iron, there are two primary ways to increase the intake of vitamin A: 1) dietary improvements such as animal-sourced foods and the consumption of red palm oil, and yellow and orange fruits and vegetables; 2) vitamin A supplementation. Children should receive their first vitamin A capsule at the age of 6 months. Between 6 and 9 months, children should receive a vitamin A capsule every six months. 7-11 programming makes this information known to households and make appropriate referrals to health clinics to access the supplements.\n- Child 6-24 months old receives a vitamin A capsule every 6 months\n- Post-partum mother takes single, high-dose vitamin A within four weeks of delivery, following national guidelines\n- Caregiver gives vitamin A-rich foods to child, including fruits or vegetables yellow or orange in colour and animal-source foods\n- Child 6-24 months takes a vitamin A capsule every 6 months\nWhat constitutes a balanced diet?\nEating from all of the food groups in the recommended quantites\nHow many food groups are there, and what are they?\nEach country has their own guidance, but two commonly used food groups are 7 food groups and the 3 food groups:\n7 Food groups include:\n- Grains, roots, and tubers\n- Legumes and nuts\n- Dairy products (milk, yogurt, cheese)\n- Flesh foods (meat, fish, poultry, and liver/organ meats)\n- Eggs (Grow Foods)\n- Vitamin A rich fruits and vegetables (Glow Foods)\n- Other fruits and vegetables\n3 Food groups include:\n- Carbohydrates (or “Energy-giving foods”, or “Go” foods) should be eaten every day. Examples of high carb foods include: bread, oil, rice, maize flour, cassava flour, peanuts/groundnut flour. o Vitamins (or “Protective foods”, or “Glow” foods) should be eaten every day. They also enhance the absorption of other nutrients.\n- Vitamin-rich foods are usually red/orange fruits and vegetables and green-leafy vegetables. Examples of vitamin-rich foods include: Orange, lemon/lime, amaranth, pumpkin leaves, sweet potato leaves, orange-fleshed sweet potato, mango, papaya, etc.\n- Protein-rich (or “Body building” or “Grow” foods) should be eaten everyday. Examples of protein-rich foods include: Eggs, groundnut/peanuts, small dried fish, beans and lentils, fish, soya beans, cowpeas, and other animal source foods, etc.\nWhat makes the best complementary foods?\nBabies can only eat small amounts of food at a time, so it is important to continue feeding them often with breastmilk and thick foods that have a lot of energy and nutrients.\nHow do you prepare complementary foods?\nWashing your hands with soap or ash after using the latrine, after cleaning young children, and before preparing, eating, or feeding food to young children, to protect your child from diarrhea.\nCleaning everything used to cook and feed a child is important for protecting a child from illness\nCooking foods fully will make food safe for a child to eat. Serve food to a child right after cooking it to prevent illness.\nWhat amounts and type of food should be provided for infants?\n0-6 months: Only breastmilk\n6-8 months: Foods that can be mashed up until smooth and soft, such as fruits, vegetables, and animal source foods (for example, mashed cooked liver). Thick pooridge with the addition of a little oil, nut, or seed paste and milk, eff, or animal source food (for example, mashed liver, dried catepillars ground into powder, and dried fish pounded and sieved into fine powder). Germinated or sprouted flour or toasted grain ground into flour to make porridges. Fermented porridges.\n9-11 months: Foods that can be mashed up or cut up into very small pieces including food the family eats. Include different types of foods, especially animal-source foods, fruits and vegetables\n12-23 months: Foods that the fmaily eats, mashed up or cut up into small pieces and mixed with porridge, include different types of foods, especially animal-source foods, fruits and vegetables. Snacks include mashed or chopped fruits and vegetables, and breads, chapattis, and mandazis, either dipped in soured or boiled milk to soften spread with groundbut paste or honey\nWhat is responsive feeding?\nYoung children need to learn to eat: encourage and help them – with a lot of patience – so that they can grow healthy. This includes looking at the child in the eyes, smiling and watching for when the young child is ready to eat.\nWhat are some things to avoid feeding children?\nAvoid giving unfiltered/untreated water to children to prevent diarrhea. It is highly encouraged to give breast milk instead, or serve water that has been boiled, or is from a safe water source. Avoid:\n- feeding black tea (including tea made with milk and sugar) or any other liquids high in ‘tannins’, such as coffee and cola drinks, as they prevent the absorption of iron, which could lead to iron deficiency anemia.\n- sugary beverages such as soda/pop, and powdered juice\n- highly processed foods and snacks\n- thin gruels and porridges and make them thicker consistency\nHow should I feed my child when he or she is sick?\nLess than 6 months of age: Breastfeed more frequently during illness. This will help the baby to fight sickness, recover more quickly and not lose weight. Give only breastmilk and prescribed medicine to your baby. If the baby is too weak to suckle, express breastmilk and give to the baby by cup. This will help you to keep up your milk supply and prevent breast swelling. DO NOT use bottles, teats or spouted cups. They are difficult to clean and can cause your baby to become sick.\nWhen baby is recovering from illness, the baby will breastfeed more than usual. The baby is replacing what was lost during illness. Take enough time to actively encourage your child to breastfeed more frequently when the baby’s appetite returns.\n6 months of age and older: Breastfeed more frequently and offer additional food during illness. This will help the baby to fight sickness, recover more quickly and not lose weight. DO NOT use bottles, teats or spouted cups. They are difficult to clean and can cause your baby to become sick. Take time to patiently encourage your sick child to eat as the child’s appetite may be decreased due to illness. Offer the baby simple foods like porridge and fruits and other foods the child likes to eat, not too thick and not too dry, in small quantities throughout the day. Avoid spicy or fatty foods.\nWhen a child is recovering from illness, the child will breastfeed and eat more than usual. The child is replacing what was lost during illness. Give your child one additional meal of solid food each day during the first 2 weeks of recovery. This will help regain weight lost during the illness. Take enough time to actively encourage your child to eat the extra food and to breastfeed more frequently when the child’s appetite returns.\nWhat is the feeding requirement for children older than 23 months of age?\nServe children from 2 to 5 years of age on their own plate, served from teh fmaily pot to ensure they consume an adequate amount and diverse diet.\nWhat are the guidelines for high-dose of Vitamin A in areas of high child mortality?\nFor infants from 6 months of age to one year should receive one dose of Vitamin A at the health center or outreach clinic.\nChildren between the ages of 1 and 5 will receive one dose every 4-6 months.\nCan newborns or children who are HIV+ also receive the Vitamin A?"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:39a8b649-7ed1-4780-b6d3-6763e4ee89b2>","<urn:uuid:98e1b7de-ba72-4328-b755-65b8a473fb2d>"],"error":null}
{"question":"Between potassium from coconut sugar and potassium from sardines, which source provides better nutritional value?","answer":"While coconut sugar contains only traces of potassium, sardines canned in oil with bones provide a more substantial nutritional profile. Coconut sugar is primarily a carbohydrate source with minimal potassium content, whereas sardines deliver 325mg of calcium per 3 oz serving along with their potassium content. The coconut sugar's limited nutrient profile and high carbohydrate content (73.9g per 100g) make it a less beneficial source of potassium compared to sardines, which provide multiple nutrients in meaningful amounts.","context":["Coconut Sugar Superfood Assignment\nHave you ever been to a convenience store or a supermarket before, and noticed a food with the label superfood on it, thinking that it is much better than any other regular foods that you would normally buy, but truly not knowing what the term superfood means? Well while you may have noticed the food with a superfood label on it, did you notice the price tag itself, as often they provide high levels of certain nutrients, therefore technically being better for your health, and can be used as great marketing vocabulary to boost sales.\nCoconut sugar is a natural sugar made from sap from the coconut palm, unlike regular table sugar which requires a longer process, coconut sugar requires two simple steps to harvest and be edible. These steps are:\nA cut is made on the flower of the coconut palm and the liquid sap is collected into containers.\nThe sap is placed under heat until most of the water has evaporated.\nThis means that it is less processed providing your body with 73.9 grams of carbohydrates, 26.10 grams less of carbohydrates than regular table sugar for every 100 grams which you consume. This can be considered as both positive and/or negative as when you consume carbohydrates, they provide your body with starches and sugars which act as energy sources to your body and keep you moving every day, although if you over consume these carbohydrates, it turns to fat which your body only needs in small portions. This means that when you consume fats from other places in your daily eating routine, it may cause you to over-consume fats if you consume too much coconut sugar as well as other foods such as fatty fish causing exercise to be more difficult because the fat is worked off after the carbohydrates when exercising meaning that it takes a lot more exercise to burn off a substantial amount of fat than it does when trying to burn off carbohydrates.\nAlso, many products of coconut sugar have traces of both calcium and potassium which are vitamins which do certain jobs in small amounts, and in order for them to assist in the function of your body, you would need to consume an awful lot of coconut sugar. Calcium is extremely useful in your body, as it helps build bones and teeth. Almost all the calcium which you consume is stored in your bones and teeth for later use, as that is the place where it will be used as mentioned before. Also, coconut sugar provides potassium which is crucial for the function of a heart and plays a key role in skeletal and muscle contraction, making it important for digestive function and movement in your daily routine.\nWhat other foods have these nutrients? (How much?)\nAnother food with similar type nutrients to coconut sugar is baked potatoes, as they provide your body with 18 mg of calcium, and 550 mg of potassium per 100 grams, and yet although they do provide substantial amounts of the vitamins and minerals above, baked potatoes only provide your body with 21.44 grams of carbohydrates per 100 grams as opposed to the coconut sugar which provides 100 grams per 100 grams. Although potatoes do provide your body with other useful nutrients such as fibres and proteins which coconut sugar doesn’t, and although potatoes do provide a range of more nutrients, they come in very small portions and would be needed in large amounts for it to make a substantial difference to your body.\nOne more food with similar nutrients to coconut sugar is white beans, as they provide your body with 62.25 grams of carbohydrates per 100 grams, as well as 173mg of calcium, and 1542mg of potassium per 100grams. Although as you may have noticed, the white beans provide much larger amounts of the vitamins/minerals than coconut sugar, as coconut sugar only has traces of these vitamins/minerals, as opposed to white beans which are a nutrient rich food, and have substantial amounts of these vitamins and minerals which can easily be used and can contribute to the function of your body. Also, white beans provide your body with 24.9 grams of fibre, 1.18 grams of fat, 21.11 grams of protein, per 100 grams, as well as other vitamins and minerals which both your digestive system and body can benefit from in certain ways for example: the protein helps build and rebuild muscle.\nIssues with the term superfood\nThe term superfood, in this case, is inaccurate, as coconut sugar is in no way super, as it is made up partially of fructose which can cause a build up of fat in the liver, which is, in fact, a dangerous disease which can harm both your heart and liver permanently if too much fructose is absorbed. The meaning behind superfood is a nutrient-rich food considered to be especially beneficial for health and well-being, and although coconut sugar does provide 100 grams of carbohydrates and sugar per 100 grams, it provides your body with very little other nutrients, and can actually harm you seriously because carbohydrates turn to fat if you consume too much, and the fructose in the coconut sugar builds up fat in your liver causing a disease called fatty liver which is extremely dangerous, and can cause pain in the heart and liver.\nAlso, there is an issue with money, and the term superfood with coconut sugar because coconut sugar is not much better for you than regular table sugar, as it still has fructose which is what makes it makes regular sugar bad for you as it damages both the liver and heart, and can affect the process of digestion increasingly. The issue with the term superfood, and money, is that supermarkets are selling coconut sugar for a higher price than regular table sugar because it has the label “superfood” on it, which makes people think that it is better for them than regular table sugar, making them more likely to pay the extra money that supermarkets are charging for a product such as coconut sugar. This also tricks people who do not know what the word superfood actually means, as when they hear the name, it reminds them of superheroes, and superheroes are good, therefore they think that superfoods are always better for you, and will benefit the function of both your digestive system and body. This means that it could cause financial issues with less fortunate people, as they think that in order to receive the correct nutrients for their body to work strong and healthy, they need to buy these superfoods like coconut sugar when really they are almost a scam as they only provide a few more grams of carbohydrates which are not necessarily needed, as it runs the risk of overconsumption of carbohydrates which is actually bad for you. Therefore the less fortunate families could have saved the money for other non-superfoods, and receive more nutrients from regular foods which they can buy more of with the extra money saved from not buying superfoods.\nAnalysing eating ‘superfoods’ compared to regular foods\nAll in all, in my opinion, superfoods are in no way super when analysing coconut sugar because it is effectively table sugar, just more natural, therefore giving people the mindset that it will provide more nutrients, which is partly true, as it does provide more of certain nutrients, but not a range of nutrients in the case of coconut sugar. Also, the problem with superfoods is that sometimes the term can be misleading, causing economic and health issues for consumers who maybe don’t have the funds to purchase as much food as they need all the time in order to receive the correct nutrient amounts. Also often regular foods will provide a variety of nutrients as opposed to a high level in just one nutrient like superfoods, which means that regular foods are more worthwhile, as they require you to eat a lower quantity of the food in order to receive all the nutrients your body needs to function both healthily and correctly. Finally, I would suggest sticking to eating regular foods, but spreading out the amount of nutrients you consume so that you receive the correct amounts, as opposed to eating several more expensive superfoods, as they can cause overconsumption, and issues with digestion and the function of your body because it is much more difficult to track the amount of nutrients you are consuming, when eating several separate superfoods, all providing your body with a different amount of a certain nutrient each.\nFinally, normal foods are better than superfoods because the term superfood just means that a food has a high level of one nutrient at the minimum, which means that when you are consuming these foods, you could believe that you are receiving the correct amounts of each and every nutrient, when really you may only be consuming one nutrient, especially in the case of coconut sugar, as you are really only consuming a considerable amount of one nutrient, carbohydrates, whereas white beans provide your body with a high level of multiple nutrients, as well as noticeable amounts of vitamins/minerals as opposed to only traces, making them better for a healthy and balanced diet. All in all, I would say normal foods are better because they provide a larger range of nutrients, as opposed to a single one, this makes regular foods better for a balanced diet, as there is less of a possibility of overconsuming a single nutrient, instead, you consume correct amounts of each individual nutrient, therefore them being better for you.\n“Full Report (All Nutrients): 45002583, MADHAVA, ORGANIC COCONUT SUGAR, UPC: 078314206647.” USDA Branded Food Products Database. United States Department of Agriculture Agricultural Research Service, Jan. 2017. Web. 20 Apr. 2017. <https://ndb.nal.usda.gov/ndb/foods/show/9981?manu=&fgcd=&ds=>.\n“Basic Report: 11356, Potatoes, Russet, Flesh and Skin, Baked.” National Nutrient Database for Standard Reference Release 28. United States Department of Agriculture Agricultural Research Service, May 2016. Web. 20 Apr. 2017. <https://ndb.nal.usda.gov/ndb/foods/show/3084?manu=&fgcd=&ds=>.Bsc, Kris Gunnars. “Coconut Sugar – Healthy Sugar Alternative or a Big, Fat Lie?”\nBsc, Kris Gunnars. “Coconut Sugar – Healthy Sugar Alternative or a Big, Fat Lie?” Authority Nutrition. Authority Nutrition, 18 Aug. 2016. Web. 20 Apr. 2017. <https://authoritynutrition.com/coconut-sugar/>.\n“5 Not-So-Sweet Facts About Coconut Sugar.” Yahoo Beauty! Yahoo Health!, 17 Aug. 2015. Web. 20 Apr. 2017. <https://www.yahoo.com/beauty/5-not-so-sweet-facts-about-coconut-sugar-126700104968.html>.","What helps your muscles contract and relax while doing those squats with weights? What carries that quick reflex signal when you touch something piping hot?\nMaintaining your body’s calcium reserves is critical. Here’s a list of foods that are rich in calcium. Pick your favorites and cook up a storm.\nTable Of Contents\nWhy Do You Need Calcium?\nCalcium sustains the entire skeletal system and muscles, supports the functioning of the nervous system, and is an essential component of the circulatory system (1).\nWithout it, fat-soluble nutrients like vitamins A, D, E, and K do not get assimilated in your body. This imbalance can trigger a chain of disorders and deficiencies (1).\nRead on to find a list of calcium-rich food groups.\n7 Food Groups That Are Rich In Calcium\n|Vegetable||Serving Size||Calcium Content (in mg)|\n|Collard greens, cooked||½ cup||178|\n|Turnip greens, fresh, cooked||½ cup||124|\n|Kale, fresh, cooked||1 cup||94|\n|Okra, cooked||½ cup||88|\n|Beet greens, cooked||½ cup||82|\n|Chinese cabbage (bok choy)||1 cup||79|\n2. Fruits, Juices, And Dried Fruits\n|Fruit||Serving Size||Calcium Content (in mg)|\n|Orange juice (fortified with calcium and vitamin-D)||100 g||201|\n|Rhubarb, frozen, uncooked||100 g||194|\n|Figs (dried)||100 g||162|\n|Curranta, zante, dried||100 g||86|\n|Prunes, dehydrated, uncooked||100||72|\n|Oranges with peel||100||70|\n|Apricots, dried, uncooked||100||55|\n3. Dairy And Milk Products\n|Dairy/Milk Product||Serving Size||Calcium Content (in mg)|\n|Whey, sweet, dried||100 g||796|\n|Romano cheese||1.5 oz||452|\n|Yogurt, plain, low-fat||8 oz.||415|\n|Swiss cheese||1.5 oz.||336|\n|Cheddar cheese||1.5 oz.||307|\n|Milk, non-fat||8 oz.||299|\n|Buttermilk, low-fat||8 oz.||284|\n|Whole milk (3.25% fat)||8 oz.||276|\n|Feta cheese||1.5 oz||210|\n|Cottage cheese, 1% milkfat||1 cup||138|\n|Frozen yogurt, vanilla||½ cup||103|\n|Ice cream, vanilla,||½ cup||84|\n|Sour cream, reduced fat||2 tablespoons||31|\n|Cream cheese, regular||1 tablespoon||14|\n4. Legumes, Legume Products, And Lentils\n|Legume/Lentil||Serving Size||Calcium Content (in mg)|\n|Winged beans, mature||100 g||440|\n|Soymilk, calcium- fortified||8 oz.||299|\n|Tofu, firm, with calcium sulfate||½ cup||253|\n|Tofu, soft, with calcium sulfate||½ cup||138|\n|Soy meal, defatted, raw||100 g||244|\n|White beans, mature||100 g||240|\n|Kidney beans, red, mature||100 g||195|\n|Soy flour, full-fat roasted||100 g||188|\n|Soybeans, green, cooked||½ cup||130|\n|Cowpeas, cooked||½ cup||106|\n|White beans, canned||½ cup||96|\n|Soybeans, mature, cooked||½ cup||88|\n|Adzuki beans, mature||100 g||66|\n|French beans, mature||100 g||63|\n|Yellow beans, mature||100 g||62|\n|Baked beans, home-made||100 g||61|\n|Lentils, raw||100 g||56|\n|Peanuts, boiled, salted||100 g||55|\n|Split peas, mature||100 g||55|\n|Mung beans, mature, cooked,||100 g||53|\n|Fava beans, mature, cooked||100 g||36|\n5. Nuts And Seeds\n|Nuts/Seeds||Serving Size||Calcium Content (in mg)|\n|Sesame seeds, whole, dried||100g||975|\n|Chia seeds, dried||100g||631|\n|Lotus seeds, dried||100g||163|\n|Brazil nuts, dried, unblanched||100g||160|\n|Hazelnuts or filberts||100g||114|\n|Sunflower seed kernels||100g||78|\n|Pecans, dry roasted||100g||72|\n|Pumpkin seeds, dry roasted||100g||55|\n|Cashew nuts, raw||100g||37|\n|Chestnuts (Japanese), dried||100g||31|\n|Coconut meat, desiccated||100g||26|\n|Pine nuts, dried||100g||8|\n6. Fish, Egg, And Meat Products\n|Fish/Egg/Meat Product||Serving Size||Calcium Content (in mg)|\n|Beef, variety meats, raw||100 g||485|\n|Sardines, canned in oil with bones||3 oz.||325|\n|Pork, fresh, variety meats, raw||100 g||315|\n|Caviar, black and red||100 g||275|\n|Mature hens, raw, deboned||100 g||187|\n|Salmon, pink, canned, with bones||3 oz.||181|\n|Lamb, variety meats, raw||100g||162|\n|Turkey, deboned, raw||100g||145|\n|Ocean perch, Atlantic, cooked||3 oz||116|\n|Pacific Herring, cooked, dry heat||100 g||106|\n|Blue crab, canned||3 oz||86|\n|Clams, canned||3 oz||78|\n|Rainbow trout, farmed, cooked||3 oz||73|\n|Lobster, cooked||100 g||61|\n|Duck, meat and skin, raw||100 g||11|\n7. Breakfast Cereals, Grains, And Pasta\n|Cereal/Grain/Pasta||Serving Size||Calcium Content (in mg)|\n|Ready-to-eat cereal, calcium-fortified||1 cup||100-1000|\n|Oatmeal, plain and flavored, instant, fortified||1 packet prepared||99-110|\n|Bread, white||1 slice||73|\n|Chocolate pudding, ready-to-eat, refrigerated||4 oz.||55|\n|Bread, whole-wheat||1 slice||30|\n|All-purpose wheat flour, enriched||100 g||338|\n|Teff, uncooked||100 g||180|\n|Amaranth, uncooked||100 g||159|\n|Corn flour, enriched||100 g||141|\n|Wheat bran, crude||100 g||73|\n|White rice, parboiled||100 g||55|\n|Quinoa, uncooked||100 g||47|\n|Buckwheat flour||100 g||41|\n|Macaroni, whole-wheat||100 g||40|\n|Brown rice, raw||100 g||33|\n|Spaghetti, dry||100 g||21|\nThat was our exclusive list of calcium-rich foods. All you need to do is pick a few items from these lists and cook up a calcium-rich storm.\nBefore you get to work, it is important to know how much calcium you need. Check the next section for these details.\nHow Much Of Calcium Do You Need?\nThe requirements change with age and sex.\n|0–6 months*||200 mg||200 mg|\n|7–12 months*||260 mg||260 mg|\n|1–3 years||700 mg||700 mg|\n|4–8 years||1,000 mg||1,000 mg|\n|9–13 years||1,300 mg||1,300 mg|\n|14–18 years||1,300 mg||1,300 mg||1,300 mg||1,300 mg|\n|19–50 years||1,000 mg||1,000 mg||1,000 mg||1,000 mg|\n|51–70 years||1,000 mg||1,200 mg|\n|71+ years||1,200 mg||1,200 mg|\nAn average adult woman (19-50 years) has to take 1,000 mg of calcium daily. Girls (14-18 years) need a higher intake of 1,300 mg, and older, post-menopausal women need about 1,200 mg of calcium per day (1).\nThe tolerable upper intake levels (UL) of a supplement are the highest amount that most people can take safely. For calcium, it looks something like this:\n|0-12 months||Not possible to establish|\n|Children and adolescents|\n|1-3 yr||2,500 mg/day|\n|4-8 yr||2,500 mg/day|\n|9-13 yr||2,500 mg/day|\n|14-18 yr||2,500 mg/day|\n|Adults 19+ yr|\n|14-18 yr||2,500 mg/day|\n|19-50 yr||2,500 mg/day|\n|14-18 yr||2,500 mg/day|\n|19-50 yr||2,500 mg/day|\nBut what if these components are unavailable/insufficient to meet the daily intake mark? What happens when you have calcium deficiency?\nWhat Happens If There Is Too Less Calcium?\nCalcium deficiency would trigger a multi-functional setback, given its critical role in the metabolism and maintenance of your body. Following are the symptoms of calcium deficiency or hypocalcemia (12):\n- Numbness of fingertips and toes\n- Muscle cramps\n- Poor appetite\n- Rickets (if coupled with vitamin D deficiency)\n- Random neuromuscular irritability\n- Chronic renal failure\n- Cardiovascular diseases, and in worst cases\nTo avoid such repercussions, it is best to have a balanced diet rich in calcium as well. Despite taking measures, if you are diagnosed with hypocalcemia, healthcare providers may prescribe calcium supplements.\nCalcium supplements are synthetic formulations of calcium compounds. Most of these are safe for human consumption.\nBut we strongly recommend you to use them only under medical supervision.\nCalcium Supplements You Can Trust\n- Calcium Carbonate: Most quickly absorbed in the body. It is available in the form of capsules, liquids, powder, and over-the-counter tablets.\n- Calcium Citrate: A more expensive form of calcium. It is absorbed well on an empty or full stomach.\n- Other Sources: Calcium gluconate, calcium lactate, calcium phosphate, calcium acetate, calcium citrate malate, calcium lactogluconate, tricalcium phosphate, etc. with multivitamin-mineral supplements are available (13).\nCheck the product authenticity symbols, serving size, instructions to use, and the recommended dosage before choosing the right supplement.\nAbove all, use them only on medical consent.\nIt is mandatory to get the recommended intake of this magic mineral to live a fuss-free life. With medical/dietetic guidance, you can plan a calcium-rich meal using the listed food groups.\nIf this article has given you all that you wanted to know about calcium-rich foods, share your feedback and suggestions in the section below.\nStrengthen your bones, boost heart health, and stay fit with calcium!\nExpert’s Answers For Readers’ Questions\nHow much calcium can your body absorb at a time?\nYour body can take up about 500 mg of calcium at a time. Although the daily requirement is approximately 1000-1200 mg, it is better to split the total into small doses of 500 mg across the day. This would ensure maximum absorption of calcium.\nHow do you determine the levels of calcium in your body?\nLevels of calcium in your body can be determined by taking a simple blood test. It can help you diagnose calcium deficiency or calcium excess. The result indicates the amount of calcium circulating freely in the blood, but not that stored in bones.\nThe reference range for calcium in adults is 8.6 mg/dL to 10.2 mg/dL, and for children, it varies from lab to lab.\n- Calcium, Fact Sheet for Health Professionals, Office of Dietary Supplements, National Institutes of Health.\n- APPENDIX B. FOOD SOURCES OF SELECTED NUTRIENTS, Dietary Guidelines for Americans 2005, USDA.\n- Fruits and Fruit Juices, Foods highest in Calcium, SELFNutritionData.\n- Legumes and Legume Products, Foods highest in Calcium, SELFNutritionData.\n- Nuts and Seed Products, Foods highest in Calcium, SELFNutritionData.\n- Finfish and Shellfish Products, Foods highest in Calcium, SELFNutritionData.\n- Beef Products, Foods highest in Calcium, SELFNutritionData.\n- Poultry Products, Foods highest in Calcium, SELFNutritionData.\n- Pork Products, Foods highest in Calcium, SELFNutritionData.\n- Lamb, Veal, and Game Products, Foods highest in Calcium, SELFNutritionData.\n- Cereal Grains and Pasta, Foods highest in Calcium, SELFNutritionData.\n- Signs of Hypocalcemia: Chvostek’s and Trousseau’s Signs, Review of Clinical Signs, Hospital Physician, CiteSeerX, The Pennsylvania State University.\n- All About Calcium Supplements, New York State Osteoporosis Prevention & Education Program, Department of Health, New York State.\n- Calcium Deficiency – Causes, Symptoms And Treatment\n- How Does Calcium Help You Grow Taller?\n- 10 Best Health Benefits Of Calcium\n- 25 Amazing Iron Rich Foods That You Should Include In Your Diet\n- Top 25 Vitamin A Rich Foods\nLatest posts by Swathi Handoo (see all)\n- 16 Must-Have Honest Beauty Products In 2020 - October 7, 2020\n- 11 Top-Selling La Mer Products In 2020 - September 22, 2020\n- 12 Best Skin Medica Products – Our Top Picks For 2020 - September 21, 2020\n- 12 Best Sit-up Foot Anchor Bars To Workout Anywhere - September 8, 2020\n- Best Almay Products For Sensitive Skin And Clean Beauty - September 4, 2020"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:626f88b8-bc42-4921-a1dd-e7ea3bda564a>","<urn:uuid:37944fcf-ac6c-4a4f-be6b-cd9514f505b1>"],"error":null}
{"question":"Why are conveyor belt pulleys considered dangerous?","answer":"Conveyor belt pulleys are particularly dangerous components, accounting for approximately 48% of conveyor-related accidents. Over the past 20 years, conveyor belt systems have been responsible for more than 80 serious or fatal accidents in the U.S. The risk is especially high during normal operation, maintenance, and cleaning activities, during which 30% of accidents occur.","context":["Conveyor belt systems have been responsible for over 80 serious or fatal accidents in the U.S. over the last 20 or so years. According to a report featured by the American Society of Safety Engineers, a disproportionate number of conveyor related accidents involve a specific component of the system; the pulleys. Roughly 48% of accidents are reported to involve pulleys. Understanding where they are and what function they perform in a particular system can go a long way to ensure that personnel remain safe during normal operation, maintenance, and cleaning, during which 30% of accidents occur.\nTypes of Conveyor Belt Systems\nGravity-driven conveyors have no mechanical or electrical power source. The momentum of the product and the force of gravity move product along these conveyors.\nGravity Wheel Conveyor – The gravity wheel conveyor is a skate wheel gravity flow system. Both curved and straight sections are used in this system. This type of conveyor uses a pattern of free turning wheels as the conveying surface.\nGravity Roller Conveyor – The gravity roller conveyor is a roller-to-roller gravity flow system. These are light-load conveyors used for handling product. The conveyor beds are comprised of two side channels which support free turning rollers.\nThere are many different types of flat belt conveyors used to move product throughout the distribution center. The four most common types of flat belt conveyors are the slider bed, belt-on-roller, meter belt, and brake.\nSlider Bed Conveyor – Slider bed conveyors operate by pulling a belt over the top surface of a box channel bed. Product is carried directly on the belt surface. Fixed speed center drives and fixed speed end drives are used to drive the slider bed conveyor. Center drives can be used for one or two-way operation, while end drives are designed for one-way operation only. Slider bed conveyors are usable in incline, decline, or horizontal applications. Slider bed conveyors are adaptable for use with many other types of powered and gravity conveyors.\nBelt-On-Roller Conveyor – Belt-on-roller conveyors carry products directly on the belt. The belt rides on the surface of carrier rollers mounted in the bed. Center drive units can be used in one or two-way operation, and end drive units are suitable for one-way operation only. Belt-on-roller units can be used for incline, decline, or horizontal operations. They can be used in conjunction with many other powered or gravity conveyors.\nMeter Belt – A meter belt conveyor is a belt conveyor with two belts and a common drive. The discharge belt runs faster than the induct belt to allow spacing of conveyed products. It is primarily used prior to merge or induct units. Meter belts are available with a brake/clutch option that is either air or electric-operated.\nBrake Belt – The brake belt is used to stop product flow and allow accumulation of the product prior to product discharge onto a merge unit. Brake belts are electrically operated and are available with a brake or a clutch/brake.\nThe live roller type conveyor is a conveyor type on which product is moved by rotation of the carrier rollers.\nTypical Live Roller Conveyors – Here, the carrier rollers are rotated by friction with a powered belt, which is pressed against the underside of the carrier rollers by adjustable pressure rollers. Friction between belt and carrier rollers, and thus driving force of the carrier rollers in any desired area, can be varied by adjusting the height of the pressure rollers under the belt at that location. This conveyor may be used on inclines or declines, which do not exceed five degrees from horizontal.\nChain-Driven Live Roller – The chain-driven live roller is a transportation-type conveyor. They use cycloidal tooth sprockets welded on one end. Cycloidal sprockets have thicker teeth than regular sprockets, which allows the chain to better mesh with the sprocket teeth. Guards are mounted above and below all chain and sprockets for safety. Standard tooth sprockets are used on end cap rollers, because the chain wraps 180 degrees around the sprocket.\nSawtooth Merge – The sawtooth merge is a live roller conveyor. Product is moved downstream on the conveyor by multiple rotating carrier rollers. The carrier rollers are powered by a continuous flat belt, which is located beneath the rollers in the center of the conveyor. Pressure rollers press the flat belt up against the bottom surface of carrier rollers. The friction generated between the flat belt and the carrier rollers causes the rollers to turn as the belt is driven. The name “sawtooth” is derived from the conveyor’s sawtooth or wedge configuration, which allows product to converge from both sides, from other junctions or conveyors.\nLineshaft Conveyor – The lineshaft conveyor is a live roller conveyor on which carrier rollers are individually driven through belts from a “lineshaft.” The lineshaft, consisting of interconnecting shaft sections, is completely enclosed within the bed sections of the conveyor. Drive belts for the carrier rollers are made from an elastomeric material such as polyurethane and run on spools installed on the lineshaft. There is enough friction between the bore of the spool and surface of the shaft to turn a carrier roller with loads up to about 20 pounds.\nAutomatic Pressure Conveyor – The APC, or Automatic Pressure Conveyor, is an air-operated live roller accumulation conveyor, which provides zero-line-pressure accumulation. This means that the rollers apply no drive pressure to accumulated products, preventing products from jamming and pressing against each other as they accumulate. Conveyor roller motion is provided by a pneumatically controlled belt that is actuated by downstream sensors. Air pressure is supplied to, or removed from, the belt actuating devices in zones called “accumulation zones” by pressure-sensor assemblies, thus regulating the flow of product.\nSometimes, it becomes necessary to convey product around curves or around curves and up to another level. This is accomplished by use of a spiral belt or flat belt turn.\nSpiral Belt Turn – The spiral belt turn conveys a product around a turn and up or down between different elevations. Product spacing and orientation are maintained from beginning to end of the turn.\nFlat Belt Turn – The flat belt turn is a motorized conveyor unit for moving product around a curve without relying on momentum or pressure from the follow-on products. From the beginning to end of the turn, the unit transports product without changing the product’s relationship to the belt while maintaining the same distance between products.\nFor more information about conveyor belt and sortation systems, visit our Conveyor Basics page at //www.techtransfer.com/resources/wiki/entry/752/. For information related specifically to safety and preventive maintenance, visit our Conveyor Safety and Preventive Maintenance page at //www.techtransfer.com/resources/wiki/entry/754/"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:b30b3c36-059a-4f1d-9bab-a5cd5452955c>"],"error":null}
{"question":"How do aircraft icing conditions affect flight control systems, and what are the communication challenges between ground control and aircraft during adverse weather?","answer":"Icing conditions can severely impact flight control systems by causing propeller icing with severe vibration that can make electronic flight instruments partially unreadable, as experienced in the 1991 ATP incident. The ice accumulation can lead to significant aerodynamic degradation, uncontrollable roll oscillations, and difficulty maintaining altitude. Communication challenges during adverse weather include ground control's difficulty in coordinating timely braking action measurements and relaying updated runway conditions to aircraft. As seen in the 2005 Prague incident, there was no system for braking action measuring vehicle operators to monitor tower communications with aircraft, leading to delays in runway condition updates and potentially unsafe takeoff conditions.","context":["ATP, en-route, Oxford UK, 1991\nATP, en-route, Oxford UK, 1991\nOn 11 August 1991, an British Aerospace ATP, during climb to flight level (FL) 160 in icing conditions, experienced a significant degradation of performance due to propeller icing accompanied by severe vibration that rendered the electronic flight instruments partially unreadable. As the aircraft descended below cloud, control was regained and the flight continued uneventfully.\nOn 11 August 1991, a British Aerospace ATP being operated by British Midland Airways (BMA) on a scheduled passenger flight from East Midlands to Jersey, Channel Islands and climbing to cleared level FL160 in day IMC was unable to maintain altitude and as this level was approached and the flight crew temporarily lost control with uncontrollable roll oscillation and a high rate of descent leading to a loss of altitude of 3000 feet. A PAN was declared to ATC but as the aircraft descended below cloud, control was regained and the flight continued without further event. None of the 63 occupants were injured and the aircraft was undamaged.\nThis is an extract from the Report (4/92) the serious incident investigation published by the Air Accidents Investigation Branch (AAIB) UK on 16 September 1992:\nThe commander set the propeller revolutions per minute (rpm) to 85% after take-off and to 82% on passing FL80 in the climb to the assigned level of FL160. He controlled the ITT [intermediate turbine temperature] to a maximum of 720°C throughout the climb. The aircraft entered cloud just below FL130 at 160 kts and a rate of climb around 500 ft/min. […] when the aircraft was at approximately FL150, over a two minute period, the indicated airspeed (IAS) reduced to 142kt and rate of climb fell at times to zero. During the period that the aircraft was in cloud the crew observed sleet and rain. At FL154 the commander requested Air Traffic Control (ATC) for reduction in his cleared cruise flight level to FL140 but the controller was unable to approve the lower level immediately because it had been allocated to another aircraft. In the event the maximum level achieved by the Advanced Turbo Prop (ATP) was FL156.\nThe engine and propeller ice protection systems had remained switched on from take-off and both pilots had been looking for signs of airframe ice, in order to determine if operation of the airframe de-icing boots was necessary. The only indication was a thin line of what they described as rime ice on the leading edges of the wings and three eights of an inch of rime ice on the windscreen wiper arm. The outside air temperature (OAT) was between -2°C and -5°C and the total air temperature (TAT) was calculated to have fallen to -2°C. The aircraft was being flown by autopilot in the heading mode with the attitude being controlled by the autopilot pitch wheel.\n[…] When the aircraft was at FL156, it began to experience vibration which rapidly increased in severity. The vibration was thought by the cabin attendants to be more severe in the rear of the aircraft than at the front. Both pilots had experienced propeller icing and associated vibration on an ATP before but on this occasion they thought it to be more extreme. The commander said that while the severe vibration lasted, the upper half of the attitude display on the electronic Primary Flight Display (PFD) showed intermittent blank bands and he was unable to read the alphanumeric characters on the Electronic Flight Instrument System (EFIS) displays.\nShortly after the onset of the vibration the left wing dropped and the aircraft began to descend. The aircraft initially pitched down approximately 15° and began a rolling oscillation. The commander said that at the point of initial wing drop he disengaged the autopilot and flew the aircraft manually. He felt that the aircraft was slow to respond to aileron control inputs and large bank angles were reached, particularly to the left, where a single peak of 68° of bank was recorded. He described the aircraft as wallowing with light aileron control forces.\nDuring the period of roll oscillation and rapid descent the first officer transmitted a “Pan” call and altered the transponder to emergency code of 7700. He also switched the airframe de-icing to ON.\n[…] The crew reported that at no time during the incident were they aware of a warning from the pre-stall warning (PSW) system.\nFull control of the aircraft was regained at FL120. At this altitude, and clear of icing conditions, the flight continued uneventfully.\nThe characteristics of the stall are described in the report. They are derived from data retrieved from FDR and sent for additional assessment. The following assessment on the nature of the stall is made by the Defence Research Agency (DRA) and is included in the report:\n“At approximately 250 seconds the aircraft entered the stall. However, this stall was gentle and insidious. […] it occurred without triggering the stall warning system and well above the normal stall speed. The aircraft was under autopilot control which further masked its effects. Perhaps the only real clue was the severe vibration (presumably post-stall buffet) which was experienced throughout the duration of the stall, but which the crew attributed to propeller icing. Overall it is not surprising that the crew did not recognise the stall’s abnormally benign longitudinal characteristics”.\nThe Report identifies the following factors as causes of the incident:\n- The rapid accumulation of glaze ice, which was not evident to the crew, but which produced significant aerodynamic degradation.\n- The difficulty of assessing visually the thickness of ice on the wing leading edges from the flight deck.\n- The BMA standard procedure to use a maximum ITT of 720°C in the climb discouraged the commander from applying power to counteract the loss of performance.\n- Use of autopilot in the pitch mode during climb which hampered recovery from the subsequent loss of control.\n- The propeller vibration which disguised the onset of the stall.\nThe Report's 14 Safety Recommendations, beginning on page 41 of the Report, also address institutional and organisational issues (see Further Reading).\n- Ice Formation on Aircraft,\n- Aircraft and In Flight Icing Risks,\n- Accident and Serious Incident Reports: WX: a selection of accident and serious incidents in which Weather (WX) was a factor.\n- AT73, en route, Roselawn IN USA, 1994\n- AT43, en-route, Folgefonna Norway, 2005 (WX LOC)\n- DH8D, en-route, South West of Glasgow UK, 2006 (LOC HF)\n- SH36, vicinity East Midlands UK, 1986 (WX LOC)\nFor further information see the serious incident report published by AAIB UK.","AIR ACCIDENTS INVESTIGATION INSTITUTE Beranových 130 199 01 PRAHA 99\nRef. No 639/05/ZZ Copy No: 1\nInvestigation into the incident of aircraft B 737-3Q8, registration G-THOF, at LKPR on 28 December 2005\nPrague Jun 2006\nA) Introduction Operator: Thomsonfly Ltd. (UK) Aircraft type: Boeing, B 737-3Q8 Registration: G-THOF Place of Incident: Prague / Ruzyně (LKPR) Date and Time: 28. 12. 2005, 18:03 (All times in this report are UTC)\nB) Synopsis On 28 January 2006 Thomsonfly Ltd (UK) notified Air Accident Investigation Institute (AAII) of an B737 incident. The crew, which was to execute flight BY 444 from LKPR to Doncaster (EGCN), rejected the take-off from RWY 31 during the take-off run, because after increasing engine power to full T/O thrust and achieving a speed of 80 kts the captain could not maintain lateral control on the runway covered in sleet and slush. The crew braked the airplane to the taxi speed and vacated RWY 31 taking exit to TWY G. With regard to this event and a continuous slight snowfall, TWR EC suspended operation on RWY 31 and asked for a check of braking action (B/A). After the B/A had been measured and the situation of airplanes holding to land evaluated, clearing of RWY 31 got started.\nThe cause of the incident was investigated by an AAII commission comprising: Investigator in charge: Ing Stanislav Suchý Member: Ing. Jan Kadlec – Prague Airport\nThe Final report was released by: AIR ACCIDENTS INVESTIGATION INSTITUTE Beranových 130 199 01 PRAHA 99\nOn the 13 Jun 2006.\nC) The Final report includes the following main parts: 1) Factual information 2) Analysis 3) Conclusions 4) Safety recommendation 5) Annexes (to copy No.1 stored in AAII archive)\nThe present document is the translantion of the Czech Investigation Report. Although efforts are made to translate it as accurate as possible, discrepancies may occur. In this case the Czech version is authentic.\n1 Factual information\n1.1 History of the incident On 28 December 2005 a Boeing 737-3Q8 with 5 crewmembers and 65 passengers on board was on the flight GB294 from LKPR Airport to EGCN Airport. At the time the crew was getting ready for the flight, lots of aircraft were arriving at or leaving LKPR because the previous weather conditions were poor and RWYs 24 and 31 were being cleared.\nAt 17:10 ATIS “L” information was issued giving B/A values from the last 16:58 measurement by SFH. The following sequence of ATIS information “M, N, O, Q, R and K” gave the same B/A data 71/65/80 till 18:14.\nAt 17:15 the crew of the airplane, call sign TOM 444, established contact on the clearance delivery dispatcher Ruzyně DELIVERY frequency, acknowledged the ATIS “L” information and received a departure clearance.\nAt 17:30 the TOM 444 crew requested on the Ruzyně Ground frequency to taxi to the stand for de-icing.\nAt 17:44 the TOM 444 crew reported the end of de-icing and received the instruction to taxi to the holding possition of RWY 31. The TOM 444 crew commented that, prior departure and during de-icing, they regularly checked the current ATIS. At no time was the TOM 444 crew aware of an official braking action, received by ATIS, of less than was comparative to value “Good”. There was a snowfall all that time, RWY 31 being covered wholly by slush. On landing, TWR EC asked the CSA 917 crew to assess the RWY 31 braking action and got a “Medium” value. The TWR EC transmitted this assessment to the following arriving plane. At that time, the airport operations officer asked clearance for entering TWY L, C and RWY 06.\nAt 17:43 TWR EC gave permission to use the demanded route, advised the operations officer of the B/A assessment reported by the crew, and asked for check and measurement of B/A on RWY 31. At\n17:49 after landing, the DLH 3286 airplane´s crew reported on the TWR EC frequency that it assessed the B/A as “Medium to Poor”. The TWR EC passed this information to the landing airplane that followed next. Because of a great number of landing aircraft, the TWR EC discussed the situation with the airport operations officer at 17:50,with the result that the B/A measuring vehicle should be waiting on RWY 06 by THR 13 and he would find through APP a slot between the landing aircraft to get access to RWY 31 to measure the B/A effect.\nAt 17:51:28 the TOM 444 crew established contact on Ruzyně Tower frequency and got the instruction to taxi on TWY L to the RWY 31 holding bay.\nAt 17:53:13 TWR EC required the BAW 856 crew on landing to assess the B/A. The BAW 856 crew reported its B/A assessment as “Medium to poor”. TWR EC passed this information to the landing traffic CSA 761 and CSA 7KG. At that time the TOM 444 crew reported it was in front of RWY 31 holding possition.\nAt 17:56 the CSA 761 crew reported that B/A was in line with “Medium to poor” estimate.\nAt 17:58:40 the TOM 444 crew requested line up for RWY 31. But a CSA 7KG plane was on the final followed by a CSA 701 on approach, so TWR EC cancel lining up and transmitted that TOM must because of landing aircrafts hold another 2 to 3 minutes.\nAt 18:00:40 TWR EC advised the B/A measuring vehicle operator that in 3 minutes after the take-off of one aircraft in the slot before the landing of the next aircraft he would allow access to RWY 31 to measure B/A.\nAt 18:01, CSA 701 landed, TWR EC cleared TOM 444 to line up on the runway and transmitted to the landing CSA 665 information that B/A was “Medium to poor”.\nAt 18:02:20, as CSA 701 left RWY 31, TWR EC cleared the TOM 444 for take off. The TOM 444 crew started its take-off run using full T/O thrust at 18:03. The captain commented that, after passing a speed of 80 kts, he could not maintain lateral control of the aircraft on the runway covered in snow and slush.\nAt 18:03:13, having travelled around 300 m from the beginning of the take off run, the crew reported a rejected take off. TWR EC than gave the TOM 444 the instruction to vacate runway via TWY G. The TOM 444 crew commented that used full manual braking and idle reverse to slow the aircraft down to a taxi speed of 10 kts using about 5,000 ft of runway. The TOM 444 crew valued the B/A and advised TWR EC that B/A coefficient to be “Poor” and vacated RWY 31 taking exit to TWY G. TWR EC passed this information to the landing CSA 665 and at the same time informed the airport operations office that TOM 444 had not been able to take off from RWY 31.\nAt 18:05:50 TWR EC transmitted “Go around” instruction to CSA 665.\nAt 18:06:35 TWR EC issued the airport maintenance with instructions to go to RWY 13 and to conduct the B/A check. The accurate B/A coefficient obtained by check was 16/17/18; the runway was all covered in slush and dry snow up to 5 mm thick.\nAt 18:13 TWR EC coordinated further steps with APP, operation on RWY 31 was suspended and TWR EC ordered to clear the runway.\n1.2 Injuries to persons\nInjuries Crew Passengers Others Fatal 0 0 0 Serious 0 0 0 Minor/ None 0/5 0/65 0\n1.3 Damage to aircraft There was no damage to the aircraft.\n1.4 Other damage There was no other damage.\n1.5 Personnel information The PIC, aged 42, was a holder of ATPL(A), had a PIC qualification for the type B 737 and a valid medical certificate. He had a total of 14097 flying hours of which 8047 hours were as PIC. On the type B 737 he has flown 186 hours. The F/O, aged 23, was a holder of CPL(A), had a valid medical certificate. He had total of 463 flying hours, on the type B 737 of which 158 hours were on type.\n1.6 Aircraft information Type and Model: Boeing 737-3Q8 Registration: G-THOF Manufacturer: Boeing Serial number: 26314 Total flight time: 31 393 hours\nOperating cycles: 18 151 Certificate of Airworthiness: valid\n1.7 Meteorological information According METAR/SPECI was 28 Decenber 2005 about 17:00 – 18:00 during pre flight preparation at LKPR following meteorological conditions: The situation: snow bearing clouds with falls of snow The surface winds: 310 – 350°/ 8 - 12 kt The temperature: - 5° ~ -6°C Icing condition: thik frost at 2000 – 10 000 ft\n1.5 Aids to navigation Radio-navigation at LKPR had no effect on the incident.\n1.6 Communications The communication between the crew and air traffic services was on frequencies ATS Ruzyně Delivery 120,05 MHz, Ruzyně Ground 121,9 MHz and Ruzyně Tower 118,1 MHz. The communication was readable.\n1.10 Aerodrome information RWY 24 was in use at LKPR at 16:58 but the conditions worsened gradually to B/A 17/22/13 as measured with a high pressure tyre device (SHF). Regular winter maintenance work was being done on RWY 31, scheduled to be finished by 18:00. At 16:58 the runway was cleared, braking action measurements giving results of 71/65/80 SFH. RWY 31 was re-opened for operations and the SNOWTAM 0122 was issued. At 17:50 a co-ordination talk between TWR and the airport management was held with the result that the B/A measuring vehicle should be waiting on RWY 06 by THR 13 to take the measurement of B/A values in a window between landing aircraft. However the measurement was not taken until 18:04, giving B/A results of 16/17/18, the runway being all covered in wet and dry snow layer up to 5 mm thick.\n1.11 Flight recorders Flight recorders were not used in this investigation. The ATS records on TWR were used.\n1.12 Description of incident site NIL\n1.13 Medical and pathological information NIL\n1.14 Fire NIL\n1.15 Survival aspects NIL\n1.16 Tests and research NIL\n1.17 Organizational and management information Radio communications between the airport operation officer, the braking action measuring vehicle and TWR was maintained on the specific frequency of 121.7 MHz. Neither airport operation officer nor A/B measuring vehicle have a listener-in device to monitor communication on TWR traffic controller´s frequency and cannot hear information transmitted between TWR EC and aircraft crews.\n1.18 Additional information Immediately after the serious incident, the chief controller of the Prague Airport ATC and the manager of Prague Airport approach and aerodrome services adopted Operational Provisions concerning work of TWR controller and airport operation officer aimed at ensuring timely checks of movement areas and braking action by the use of A/B measuring vehicle.\n1.19 Useful or effective investigation techniques The serious incident has been investigated in accordance with Annex 13.\n2.1 The TOM 444 crew expected the conditions as reported in the ATIS “L”, giving B/A equal to 71/65/80 on RWY 31. After the end of de-icing as the plane taxied on TWY L, the crew was in contact from 17:51:28 with Ruzyně Tower on its frequency and could have heard the crews, which had just landed, report B/A coefficient “Medium to poor”. The crew could have heard the last information about the estimated B/A after the plane was cleared to enter RWY 31 and was waiting for the take-off clearance. At 18:01:37 TWR EC issued the CSA crew with the instruction to adjust the approach speed along with information about the airport conditions including the “Medium to poor” B/A estimate.\n2.2 The reason why TOM 444 rejected the take-off after it used around 300 m of runway and attained a speed of 80 kts was that the crew could not maintain lateral control in the RWY 31 axis. The TOM 444 crew mentioned no problem with different or unsymmetrical thrust. After the reject, it used full manual braking and idle reverse and slowed the plane to the taxi speed using about 5,000 ft of runway. The TOM 444 crew reported that it estimated B/A coefficient as “Poor”.\n2.3 After RWY 31 took over operation and particularly after the arrival of CSA 917, whose crew estimated B/A as “Medium”, the snow layer on RWY 31 was getting thicker due to a slight but continuous snowfall. Operators of the B/A measuring vehicle, who were not familiar with B/A figures reported by aircrews because there is no listen-in device for the TWR EC frequency in their car, waited for TWR EC´s instruction to let them enter RWY 31.\n2.4 The TWR EC, who had information from aircrews that RWY 31´s B/A got worse dramatically, thought that he would coordinate with APP creation of a slot between landing aircraft. However he failed to adjust suitable separation between the landing aircraft, so there was no slot long enough to check B/A on RWY 31 before the take-off of TOM 444. TWR EC failed to advise the TOM 444 crew of information about actual/worse airport conditions acquired from crews that had landed.\n3.1 The commission has come to the following conclusions: The crew had the valid licences and necessary qualifications; The airplane had a valid airworthy certificate, a maintenance certificate and operation permit; The TOM 444, based on regurerly checked the ATIS broadcast, at no time was aware of deterioration B/A on RWY 31 of less than Good. Befor take off the TOM 444 crew could have hear the assessed B/A by other aircrews that had landed before; TWR EC did not relay to the TOM 444 crew information about airport deteriorated conditions he had from the reports of aircrews that had landed before; There were small time intervals between aircraft landing at LKPR, in which TWR did not manage to arrange for the B/A measuring vehicle to go to RWY 31, in spite of the fact that B/A reports from aircrews indicated sharp deterioration of airport conditions; The staff of the A/B measuring vehicle, rather hesitant and lacking experience, ignorant of the crews´ reports, was ready on RWY 06 waiting for TWR instruction to access RWY 13; The aircrew commenced take-off from RWY 31 although it could see that the braking conditions were bad due to the RWY´s snowy surface. The crew itself estimated the conditions with the “Poor” coefficient; A five-millimetre thick layer of slush and snow on RWY 31, where the measured braking action gave results of 16/17/18 SFH, affected the plane´s capability to maintain lateral control with full t/o thrust during the take-off run to the extent that the crew rejected the take-off for safety reason.\nThe cause of the incident was deteriorated conditions on RWY 31, which was covered in snow, hampering airplane lateral control during the take-off run. The RWY 31 surface status was much worse than the crew had expected relying on ATIS ”L” report on the airport conditions.\n4 Safety recommendations\n4.1 The airport operation officer should have a possibility of receiving necessary information by listening of radio communications on corresponding frequency of TWR EC."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:f6bfc5dc-147f-430f-a98b-353b8eb9d5c8>","<urn:uuid:843c77f9-7170-489c-8be6-6025d40f4d7c>"],"error":null}
{"question":"For practitioners seeking guidance - what's the difference in how teachers/community support spiritual practice in Advaita tradition vs Zen practice? 💭","answer":"In Advaita Vedanta, an enlightened guru with knowledge of both scriptures and Brahman is considered indispensable for anyone seeking salvation. The guru's role is essential for transmitting the knowledge leading to Self-realization. In Zen practice, while teachers are important catalysts for spiritual practice, the emphasis is placed on three essential elements: having a dedicated meditation space, consistent daily practice time, and practicing with a community (sangha). The sangha provides support through shared meditation experience, connecting with teachers, and opportunities to benefit others, making it a more communal approach to spiritual development.","context":["The Concept of Advaita Vedanta\nAdvaita Vedanta refers to the non-dualistic school of Hindu philosophy, which is derived mostly from the Upanishads and elaborated in detail by eminent scholars like Gaudapada and Sri Adishankaracharya. Dvaita means duality, and Advaita means nonduality. In simple terms, Advaita means absence of the duality between subject and object. In our wakeful consciousness we experience duality, but in deep sleep only nonduality.\nAdvaita school believes that Brahman is the one and only reality and everything else is a mere appearance, projection, formation or illusion. One of the most common examples used to describe the state is momentarily seeing a snake in a rope when it is lying in the darkness. The snake is an illusion, and the rope is the reality. In the same manner the world appears in the mind as a formation over the Self.\nThe school also believes that Atman, the individual self, has no separate existence of its own. It is but a projection or reflection of Brahman only in each being. A jiva is deluded soul by egoism, desires, and other impurities and thereby experiences duality and separation. Because of it each being is bound to the cycle of births and deaths and the laws of karma as long and remains so until liberation is achieved.\nBrahman is real, but the world in which we live is a mere illusion, like a mirage. It appears in our consciousness because of the activity of the mind and the senses. Since we totally depend upon them, we do not perceive Brahman, the ultimate reality, who is hidden in all. When they are fully withdrawn and made silent through detachment, purity and renunciation, one can see the Supreme Self hidden in all and attain liberation.\nAdvaita Vedanta believes that an enlightened guru, having the knowledge of both the scriptures and Brahman, is indispensable for anyone seeking salvation. Mandukya Karika of Gaudapada is considered to be the first available treatise on Advaita Vedanta, while the monumental works of Shankaracharya constitute its core literature. Successive generations of scholars enriched the school of Advaita through their teachings and scholarly works. Advaita school also forms part of Vaishnavism, Saivism and Shaktism under different names.\nA few important concepts of Advaita Vedanta are presented below.\nSadhana Chatushtayam means the tetrad which are imperative for spiritual practice and liberation. The following four sets of qualifications are considered essential to achieve salvation, which each aspirant is expected to cultivate.\n- Nityanitya vastu viveka: The ability to discriminate between what is eternal (nitya) and what is temporary (anitya). The absence of it is responsible for the delusion.\n- Ihamutrartha phala bhoga viraga: Disinterestedness in enjoying the fruit of one's actions and sense objects here and here after. This will arrest the continuation and formation of karma.\n- Sama adi satka sampatti: Qualities such as sama (control of internal sense organs), dama (control of external sense organs), uparati (abstinence), titiksha (quietness), sraddha (sincerity and faith) and samadhana. They are important for self-transformation and the predominance of sattva, without which one cannot be free from the triple impurities of egoism, attachments and delusion.\n- Mumukhatva: Intense aspiration for salvation. It arises mainly due to the good works (karma) in the past. According to the Bhagavadgita only after repeated births a person feels a strong drive to achieves salvation and turns to the path of salvation.\nThey are the standards of ascertaining right knowledge, truth, or valid knowledge. In this world duality it is very difficult to know which is right knowledge and which is reliable for salvation or to ascertain truth. Advaita Vedanta recognizes six Pramanas, of which three were proposed by Shankaracharya and three by his followers. They are as stated below.\n- Pratyaksha: knowledge that comes through perception. This is objective knowledge which is experienced directly either through the senses or in deeper states of consciousness.\n- Anumana: knowledge that comes by means of inference. This is speculative knowledge based upon supposition or belief.\n- Upamana: Knowledge that comes by means of analogy, comparison and contrasting. This is relational knowledge.\n- Arthapatti: knowledge obtained by meaningful assumptions based on common sense and previous experience. This is hypothetical knowledge.\n- Anupalabdhi: Knowledge gained through negation.\n- Agama: Knowledge that comes through study of scriptures. This is pure theoretical knowledge.\nTheory of Causation\nAdvaita Vedanta recognizes two forms of causation, the material cause and the instrumental cause. According to the school Brahman is both the material and instrumental cause of creation. In other words, Brahman provides not only the will and direction but also the material and energy needed to manifest the things, beings and worlds. Brahman is both Purusha (Self) and Prakriti (Nature).This is in contrast to some schools of Hindu philosophy, which argue that Brahman is the instrumental cause while Prakriti or nature is the material cause.\nCause and Effect:\nAdi Shankaracharya proposed that each cause was hidden in its every effect, whereas the opposite was not true. In other words, the seed is hidden in the tree that produces it. While a cause is not different from the effect it produces, the same cannot be argued in case of effect in relation to its cause. A cause is always part of the effect, hidden within it and so is not different from it. Brahman is the cause of all creation. So the world is real only because Brahman, who is its cause, is hidden it and inseparable from it. From this perspective the world becomes an illusion because it disappears when the Self or Brahman is withdrawn from it. The world exists when you, the cause, are present in your mind. When you, the cause, withdraw from it, the world disappears. Shankaracharya propounded the theory of causation (vivartavada). According to it an effect is an outward projection of cause and hence not real. This is in contrast to the parinamavada concept according to which an effect is an evolution or transformation of cause and hence as real as the cause itself.\nAccording to Advaita Vedanta the world is an illusion or maya, which is caused by the veiling power of Brahman. It is unreal or illusory in an absolute sense. Since it is a projection of God's consciousness, it disappears when it is withdrawn. The veiling is called avarna and the projection viksepa. Followers of Advaita argue that maya is neither real nor unreal, but indeterminate or indescribably (anirvachaniyam) because it cannot be both at the same time.\nBrahman and Atman\nBrahman is the supreme, absolute and eternal reality. It is the only truth, the cause of all, and the only stable and permanent reality. Atman is Brahman, perceived as individual self, the hidden reality, in all aspects of creation. There is no difference between the two. When the Self overcomes its veiling, it experiences non-duality (Advaita anubhava) of existence and realizes its non-difference from the Absolute. Brahman in his absolute state is without qualities and attributes. However, in our relative state we perceive him to be having certain attributes and refer to him as Isvara, the lord of the universe. In the ultimate sense, Isvara is also not the cause, but only an effect or a reflection of Brahman in the quality of Sattva.\nAccording to Advaita, the world is unreal, not because it does not exist, but because it exists only so long as the Self is present in the awareness as the subject. When the Self is withdrawn from the consciousness, the world disappears. Besides it is ever changing, unstable, impermanent and subject to destruction and decay. It is an appearance, projection of God, like a mirage, or a mistaken reality. Our senses take it for granted whereby we mistakenly consider it real and permanent. The world exists because of our perception of duality and will disappear when we enter the state of non-duality or pure subjectivity, which is the state of the Self. When we overcome the illusion and develop detachment from the sense objects we enter that state of pure awareness where the duality between subject and object, or the knower and known simply vanishes and the Self alone remains.\nSome argue that Shankaracharya was inspired by the teachings of the Buddha, especially those pertaining to the school of emptiness (Shunyavada), in postulating the theory of nondualism. It may not be true because Buddhism does not believe in the existence of Self. It is true according to both schools, a being become empty upon liberation. According to Buddhism nirvana is an indeterminate state in which all traces of individuality disappears. According to Advaita, upon liberation the individual Self which is present in the being as a projection of Brahman becomes withdraws and the being vanishes into the ocean of existence as nothing. Shankaracharya was preceded by many Vedic scholars who followed the path of nondualism. For them it was not just a speculative theory, but a means to salvation. Shankaracharya followed an ancient Upanishad tradition that upheld the school, and probably belonged to a teacher tradition which followed it. For the next thousand years his teachings and numerous works became the standard for the school of Advaita.\nHowever, the works of Shankaracharya were not accepted by all Indian scholars. He was severely criticized for this stand on Advaita by those who followed Dvaita and Vishishtadvaita. Even Advaita there are many variations. Most of these schools came into existence as alternative philosophies or viewpoints, based upon their opposition to Advaita or their criticism of it. Adi Shankara's works on the Upanishads, the Bhagavad Gita and the Brahma Sutras greatly helped in understand the subtle nuances of Advaita. Although, they were mostly translations and commentaries or were based upon existing works, you can still find in them original ideas and interpretations.\nCritics of Shankara argued that he taught a version of Buddhism as Advaita. In Buddhism liberation arises from the insightful realization that the world is a mere appearance or a phenomenon. When one gains that insight he reaches the changeless, deathless, absolute state that cannot be described. In Advaita, liberation arises from Self-realization or the realization that only the Self is real and everything is a mere appearance or phenomenon. Thus, the concept of Maya in Advaita is rooted in the eternal reality of Brahman, but not just in the temporary delusion of the mind, which is the case with the Buddhists.\nIt is also probably untrue that Shankaracharya was instrumental in the decline of Buddhism. It started long before the emergence of Shankaracharya, at least by three hundred years of so. By the time he was born, Buddhism was already on decline and many Buddhist places of residence (aramas and Viharas) which were in a state of neglect were occupied the ascetic groups of Shaivism and Vaishnavism. It appears that by his time many Buddhist places of worship were converted into Hindu shrines. It is true that through his travels, debates and discussions he consolidated the revival of Hinduism. It served a great cause in preserving Hinduism when organized religions such as Islam and Christianity came to India and seemed to threaten its very existence. Shankara's Advaita or his theory of nondualism, provided a level playing field for the Hindus during the Islamic rule and contributed to the synthesis of new movements such as Sufism.\nSuggestions for Further Reading\n- The Advaita Vedanta the Experience of Oneness\n- Advaita Vedanta As It Exists\n- The Vedanta Philosophy According to Shankara and Ramanuja\n- Brahman the highest God of Hinduism\n- The Concept of Atman or Eternal Soul in Hinduism\n- The Five Bodies of Jiva, the Limited Being\n- Brahman according to Advaita and Dvaita schools of thought\n- The Concept of Atman or Eternal Soul in Hinduism\n- The Problem of Maya Or Illusion and How To Deal With It\n- Belief In Atman, The Eternal Soul Or The Inner Self\n- Brahman, The Highest God Of Hinduism\n- The Bhagavad Gita Original Translations\n- The Bhagavadgita, Philosophy and Concepts\n- Bhakti yoga or the Yoga of Devotion\n- Hinduism And The Evolution of Life And Consciousness\n- Why to Study the Bhagavadgita Parts 1 to 4\n- Origin, Definition and Introduction to Hinduism\n- Symbolic Significance of Numbers in Hinduism\n- The Belief of Reincarnation of Soul in Hinduism\n- The True Meaning Of Renunciation According To Hinduism\n- The Symbolic Significance of Puja Or Worship In Hinduism\n- Introduction to the Upanishads of Hinduism\n- Origin, Principles, Practice and Types of Yoga\n- Essays On Dharma\n- Esoteric Mystic Hinduism\n- Introduction to Hinduism\n- Hindu Way of Life\n- Essays On Karma\n- Hindu Rites and Rituals\n- The Origin of The Sanskrit Language\n- Symbolism in Hinduism\n- Essays on The Upanishads\n- Concepts of Hinduism\n- Essays on Atman\n- Hindu Festivals\n- Spiritual Practice\n- Right Living\n- Yoga of Sorrow\n- Mental Health\n- Concepts of Buddhism\n- General Essays\nTranslate the Page","About zen practice\nThe fundamental practices of Zen Buddhism are zazen meditation and the Bodhisattva Precepts.\nIn essence, zazen is simply existing without grasping or aversion to our immediate experience.\nZazen is not a means to an end, but the ongoing practice of freedom from suffering. By keeping a spacious mind through whatever physical sensations, thoughts, and emotions arise, we are awake to their transparent, transforming quality and at the same time come into a direct feeling- intimacy with the texture of ourselves, Thus, we become increasingly able to flow in connection with the inevitable changes and difficulties that life brings.\nThe Bodhisattva Precepts are guidelines for ethical practice that embody a clear mind and connected heart animated by the intention to live a life of compassion and deep respect for all beings. The 16 Bodhisattva Precepts are:\nThe Three Pure Precepts\nI vow not to commit evil.\nI vow to cultivate goodness.\nI vow to help others.\nThe Three Refuges (Three Treasures)\nI take refuge in the Buddha.\nI take refuge in the Dharma.\nI take refuge in the Sangha.\nThe Ten Grave Precepts\nI vow not to kill, but to cherish all life.\nI vow not to steal, but to respect that which belongs to others.\nI vow not to misuse sexual energy, but to be honest and respectful.\nI vow not to lie, but to speak the truth.\nI vow not to misuse drugs or alcohol, but to keep the mind clear.\nI vow not to gossip about others’ faults, but to be understanding and sympathetic.\nI vow not to praise myself by criticizing others, but to overcome my own shortcomings.\nI vow not to withhold spiritual or material aid, but to give freely when needed.\nI vow not to unleash anger, but to seek its source.\nI vow not to speak ill of the Three Treasures, but to cherish and uphold them.\nJukai is the process and public ceremony of taking the 16 precepts, committing to practicing them in everyday life. People take jukai after some years of practicing with the community, establishing a relationship with one of the teachers, and taking part in sesshin.\nPractice at Home\nThere are three essential elements for successfully practicing at home:\nspace, time, and community.\nSet Aside a Space\nHome practice is greatly enhanced if the meditation space is dedicated to meditation. Ideally in this space no other activities take place, and it remains undisturbed when not in use. Of course this isn’t always possible, but it is very powerful. Another important element is cleanliness. The act of cleaning the meditation space is a reminder of the importance of meditation, and the ever-returning dust speaks of the necessity of returning to meditation again and again.\nAn altar can bring power and dignity to a meditation space. The various items on the altar are placed there because they have meaning, and serve as reminders of spiritual aspiration. Common items for an altar are a small bowl of fresh water, a small bowl with rice or some other food offering, fresh flowers, an incense bowl, a photo of a teacher or inspiring person, and a spiritual image. There are no rules for a home altar, but it should be kept clean and tidy. Fresh flowers are a wonderful addition to an altar; be sure to change them when they begin to wilt.\nDecide How Much Time to Meditate\nHome meditation practice is well served by consistency and it is helpful to meditate at the same time every day. Choose a meditation time that is reasonable, preferably when interruptions are unlikely. Many people wake a little early to meditate first thing in the morning, another common time is just before bed. Some people meditate on lunch break, or at other quiet moments in the day.\nIt is important to meditate for an amount of time that can be done daily. Choose a length of time that is unquestionably possible, rather than one that feels like a stretch. It is better to meditate for five minutes per day than for one hour every week. Of course when the meditation period ends it’s fine to continue meditating. This should be done with a mind of enjoyment and appreciation, rather than a sense of “should” or obligation. Once a time of day and length of time have been decided and established, commit to that schedule. Making a firm commitment allows meditation practice to become a habit, rather than something that is done only when it’s convenient.\nMeditate with a Community (Sangha)\nSitting with a sangha is a great addition to a home practice. Sangha is a reminder that there are many, many people in the world who also believe that meditation is important, and are willing to commit time and life energy to the practice. The experience of meditating with a sangha is different than sitting alone, and this difference is very interesting and revealing. A sangha is also an environment to connect with other people, some who are inspiring and some who are challenging.\nThe ZCO sangha includes teachers, and interacting with a teacher can be a great catalyst for spiritual practice. A sangha also offers an opportunity to benefit others with the fruits of meditation. Listening attentively or supporting someone in need are great gifts to share, and enhanced by meditation practice."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"content_constrained"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:ef5dba93-8c0f-4ce9-9f8f-08fbad72c0c9>","<urn:uuid:42aacf48-0494-4673-9d27-5d077980852e>"],"error":null}
{"question":"How did Alexis de Tocqueville and Herodotus earn recognition for their historical works?","answer":"Both historians earned recognition through their influential works, but in different ways. Alexis de Tocqueville became known for his seminal works 'Democracy in America' and 'The Old Regime and the Revolution,' providing perceptive analysis of political systems. In Athens, Herodotus became a celebrity, giving public readings and receiving a significant prize of 10 talents from the people of Athens for his contributions to the city's intellectual life. His major work 'The Histories' was groundbreaking for turning a political story into literature, documenting the Greco-Persian Wars.","context":["Alexis de tocqueville was one of the with this biography of alexis de that could reclaim past french glories brogan's biography is a marvelous. Alexis de tocqueville (1805–1859) was a french political philosopher, statesman, and author of two seminal works in the history of political thought, democracy in america and the old regime and the revolution. Alexis de tocqueville: alexis de tocqueville, political scientist, historian, and politician, best known for democracy in america, 4 vol (1835–40), a perceptive analysis of the political and social system of the united states in the early 19th century. The american republic will endure, until politicians realize they can bribe the people with their own money - tocqueville alexis de tocqueville was a french political thinker and historian. Image for alexis de tocqueville (french historian and political writer.\nBiography this biography a biography of alexis de tocqueville a french historian of herodotus provides detailed information about his childhood, life. Alexis de tocqueville's biography and life storyalexis-charles-henri clérel de tocqueville (29 july 1805 – 16 april 1859) was a french political thinker and historian best known for his democracy in america (appearing. Alexis de tocqueville (1805-1859) was born in verneuil, france a historian and political scientist, he came to the united states in 1831 to report on.\nAlexis de tocqueville jako główny polityczny alexis de tocqueville french historian and biography/alexis-de-tocqueville. It took a frenchman alexis de tocqueville's something like that happens with this biography of alexis de tocqueville want more from the economist. Genealogy profile for alexis de tocqueville alexis-charles-henri clérel de tocqueville 16 april 1859) was a french political thinker and historian best. Alexis de tocqueville books and biography bookyards is the world's biggest french historian of colonialism olivier lecour ^ alexis de tocqueville,.Immediately download the alexis de tocqueville the french statesman and writer alexis charles henri maurice alexis de tocqueville, a historian writer. Horoscope and astrology data of alexis de tocqueville born on 29 july tocqueville, alexis de from astro-databank jump to biography french political. Alexis de tocqueville, french diplomat, political scientist, and historian was born july 29, 1805, paris, france at age 16 tocqueville entered the college royal in metz to study philosophy. French political thinker and historian best known count de,” in appletons' cyclopædia of american biography, online books by alexis de tocqueville. Alexis de tocqueville alexis de tocqueville political scientist historian and guide contains a biography of alexis de tocqueville de safari, tribal) (french.\nAmerica study guide contains a biography of alexis de tocqueville de beauvoir simone 1908 1986 french de tocqueville political scientist historian. Information on alexis de tocqueville biography the nobleman alexis de tocqueville (1805-59) but he made important observations about the french revolution as. Visit biographycom and travel through a young america with french historian alexis de tocqueville alexis de tocqueville biography, life - fampeoplecom. The french aristocrat wrote the first great we should see him as a travel writer and historian he is the author of alexis de tocqueville: a biography.\nBrowse through alexis de tocqueville's poems and alexis-charles-henri clérel de tocqueville was a french political thinker and historian best known. The french want no-one to be their superior alexis de tocqueville quotes from brainyquotecom biography author profession: historian. Alexis-charles-henri clérel de tocqueville was a french legal and political scholar, politician, and historian who is most well known as the author of the book democracy in america, published in two volumes in 1835 and 1840.","The Early Life of Herodotus\nHerodotus was born in about 485 B.C. in the Greek city of Halicarnassus, a lively commercial center on the southwestern coast of Asia Minor. He came from a wealthy and cosmopolitan Greek-Carian merchant family. (The Carians, of Minoan descent, had arrived in that part of Asia Minor before the Greeks had.) In the middle of the 6th century B.C., Halicarnassus became a satrapy, or province, of the Persian Empire and was ruled by the tyrant Lygdamis. Herodotus’ family opposed Lygdamis’ rule and was sent into exile on the island of Samos. When he was a young man, Herodotus returned briefly to Halicarnassus to take part in an abortive anti-Persian rebellion. After that, however, the writer never returned to his home city again.\nOrigins of the Histories\nInstead of settling in one place, Herodotus spent his life traveling from one Persian territory to another. He crossed the Mediterranean to Egypt and traveled through Palestine to Syria and Babylon. He headed to Macedonia and visited all the islands of the Greek Archipelago: Rhodes, Cyprus, Delos, Paros, Thasos, Samothrace, Crete, Samos, Cythera and Aegina. He sailed through the Hellespont to the Black Sea and kept going until he hit the Danube River. While he traveled, Herodotus collected what he called “autopsies,” or “personal inquiries”: He listened to myths and legends, recorded oral histories and made notes of the places and things that he saw.\nWhen Herodotus was not traveling, he returned to Athens; there, he became something of a celebrity. He gave readings in public places and collected fees from officials for his appearances. In 445 B.C., the people of Athens voted to give him a prize of 10 talents–almost $200,000 in today’s money–to honor him for his contributions to the city’s intellectual life. The Histories\nHerodotus spent his entire life working on just one project: an account of the origins and execution of the Greco-Persian Wars (499–479 B.C.) that he called The Histories. (It is from Herodotus’ work that we get the modern meaning of the word “history.”) In part, The Histories was a straightforward account of the wars. “Here is the account,” the work begins, “of the inquiry of Herodotus of Halicarnassus in order that the deeds of men not be erased by time, and that the great and miraculous works–both of the Greeks and the barbarians–not go unrecorded.” It was also an attempt to explain the conflict–“to show what caused them to fight one another,” Herodotus said–by explaining the Persians’ imperial worldview. The Histories also incorporated observations and stories, both factual and fictional, from Herodotus’ travels.\nEarlier writers had produced what Herodotus called “logographies”: These were what we might call travelogues, disconnected tales about places and people that did not cohere into a narrative whole. By contrast, Herodotus used all of his “autopsies” to build a complete story that explained the why and the how of the Persian Wars.\nAfter Herodotus died, editors divided his Histories into nine books. (Each was named after one of the Muses.) The first five books look into the past to try to explain the rise and fall of the Persian Empire. They describe the geography of each state the Persians conquered and tell about their people and customs. The next four books tell the story of the war itself, from the invasions of Greece by Persian emperors Darius and Xerxes to the Greek triumphs at Salamis, Plataea and Mycale in 480 and 479 B.C.\nHerodotus’ encyclopedic method did not leave much room for analysis. He treats every piece of his narrative, from the main themes to the digressions and from the facts to the fictions, with equal importance. He shows how Persian hubris led to the downfall of a great empire, but he also places a great deal of stock in gossipy tales of personal shortcomings and moral lessons.\nThe Legacy of the Histories\nRival historian Thucydides, who relied only on “factual” evidence to provide a less subjective account of “what had been done,” frequently criticized Herodotus for inserting “fables” into his narrative just to make it more “delightful” and pleasant to read. Indeed, there are people who call Thucydides “the first historian” and Herodotus “the first liar.” But no matter how one judges his reporting, Herodotus will likely get credit for taking a dry political story and turning it into literature."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:0c1e8e44-a7c8-4505-b7a7-5c306d9745b5>","<urn:uuid:b9dfd29f-7127-4606-85ce-8eed099b26d7>"],"error":null}
{"question":"Could you explain the cultural significance of Han Dynasty tombs and their contents, particularly regarding lacquerware discoveries and conservation efforts?","answer":"Han Dynasty tombs represent invaluable historical repositories, with approximately 100,000 surviving tombs in China. These tombs contained extensive collections of lacquerware, which was considered highly prestigious - about 10 times more costly than bronze equivalents - and was used in ancestral ceremonies among wealthy aristocracy. The lacquerware found in these tombs often featured elaborate decorations including hunting scenes, chariots, horsemen, and folklore creatures. Today, the preservation of these artifacts requires sophisticated conservation methods, as demonstrated at facilities like the Guanxi lab where thousands of artifacts undergo careful restoration using climate-controlled environments and specialized techniques. For instance, the lab uses aerospace technology to create oxygen-free environments and maintains specific humidity levels between 20-80 percent to preserve these delicate items.","context":["The excavation of the ancient tomb of a nobleman believed to be one-time emperor the Marquis of Haihun is providing China with an opportunity to showcase its latest breakthroughs in the field of conservation\nince January 15, the process of opening and preserving the inner sarcophagus of the man believed to be the Marquis of Haihun commenced in a hermetically sealed laboratory one kilometer west of the dig site in Guanxi, Jiangxi Province, where the Han Dynasty tomb was discovered. Off-limits to media and the public, the excavation process involved state-of-the-art preservation technology aimed at protecting and cataloging some 10,000 artifacts in what some are terming a “conservation ER.” Lab The artifacts removed from the tomb include tapestries, embroidery, metal containers, lacquerware and other objects of cultural and artistic significance. Most pieces are too fragile to be cleaned on-site, and thus the focus of the conservation effort is gradually shifting to the nearby lab complex.\nCovering a total area of some 4,000 square meters, the Guanxi lab includes five workstations. Particularly impressive is the anoxia workstation, which utilizes aerospace technology to remove oxygen from the atmosphere and suspend vulnerable artifacts in an almostpure bubble of nitrogen with climate control technology allowing humidity to be adjusted to between 20 and 80 percent. Conservationists, armed with oxygen masks, work in a 20-square-meter glass tank pre-fitted with hermetically sealed containers for the most fragile relics, including human remains.\nAt another workstation, broken pieces of lacquerware still bearing black and red markings are soaked in water and kept in individual “crispers” designed to prevent their brittle outer layers from completely disintegrating. But their preservation is not guaranteed. “We are not sure yet if all these fragments can be restored and reassembled according to their original appearances,” conservationist Guan Li told our reporter.\nLacquerware is notoriously vulnerable to environmental damage, and the Guanxi pieces spent millennia submerged in an underground midden, which preserved them but could not prevent them from beginning to crumble immediately upon contact with the air. Consequently, sterile deionized water is used to keep each piece of lacquerware soaked, with restoration work focused on reinforcement, careful removal, cleaning and dechromatizing, with conservationists using infrared scanning to reveal otherwise-invisible motifs and patterns.\nEach piece is then dehydrated and sealed in Plexiglas for long-term preservation.\nResearch work in the Guanxi lab only began in early 2016 and is expected to continue for years to come. Staff members are recording vast amounts of information from their study of the relics, including images, text, chemical and biological data, and three-dimensional models.\nExperts from Beijing’s Palace Museum, an institution housed in the Forbidden City, were invited by the Jiangxi Provincial Cultural Department to participate in conservation work. “There are over 3,000 bronze or iron pieces and thousands of bamboo strips, all of which require timely modern technical protection to ensure their stability for further scientific research and analysis,” said Palace Museum curator Shan Jixiang.\nAccording to Shan, the Palace Museum team will commence their work in Guanxi by helping to repair bronze vessels unearthed from the site (See “Grand Ambitions,” NewsChina, April, 2015, Vol. 080).\nThe Palace Museum is internationally renowned for its work restoring ancient bronzeware, paintings, calligraphy and other artifacts.\nAccording to Li Cunxin, researcher at the Institute of Archeology, Chinese Academy of Social Sciences (CASS), not everything can be restored to its original appearance. During a recent interview with State newspaper People’s Daily, Li referred to the example of a rotten canopy from an excavated horse-drawn carriage. With the assistance of modern technology, he explained, archeologists can extract information and conduct a comprehensive analysis of the materials, structure and technology employed to produce an artifact in order to “make an analog restoration” and complete its “physical rehabilitation.” Archeologist Yang Jun admitted that the indoor conservation process would be “tedious” in terms of the time it would take to properly catalog and protect so many artifacts. Just restoring the lacquerware found in the tomb, he said, will take three to five years, with the final work report likely published no sooner than 2021.\n“Considering the large number of artifacts from the tomb of the Marquis of Haihun, it might take one decade or two, or even the input of future generations to complete this research,” Yang told NewsChina.\nExcavation While China is routinely described as a “civilizationstate” with 5,000 years of history, the country’s stringent Cultural Relics Protection Law discourages the active excavation of known important large-scale mausoleums, leaving widespread discussion of what lies within these sites largely speculative, much to the chagrin of historians both in\nA jade disc edged with dragonphoenix motifs\nA bronze crockpot containing chestnut residue, indicating that Chinese hotpot may have been enjoyed by Han Dynasty aristocrats\nChina and abroad. The most famous example of this is the halting of archeological work at the main burial mound of the Qin Shihuang Mausoleum, believed to house the tomb of China’s first emperor, Qin Shihuang (See “The Warriors’ Code,” NewsChina, November, 2009, Vol. 016). Dong Mingkang, deputy director of the State Administration of Cultural Heritage (SACH), expressed publicly that the government has three main reasons underscoring its prohibition of the excavation of royal mausolea. The first obstacle is “inadequate preservation technology,” the second is an insufficient number of archeologists to meet existing demand and the third is “the need to leave some things for posterity” to allow future generations “to make discoveries of their own.” The sudden “emergency excavation” of the tomb of the Marquis of Haihun was given the green light after it was revealed that grave robbers had attempted to break into its main chamber in 2011. Yang Jun admitted to NewsChina that the tomb was not a protected cultural relic site prior to that year, and thus could not be adequately protected from thieves without assigned funding, management and institutional support. Considering the threat of looting, a common phenomenon throughout Chinese history, provincial officials gained the central government’s approval to start excavation work.\nOn January 6, SACH officially elevated the status of the archaeological dig at Guanxi from a provincial to a national level project, listing it, along with the excavation of the Song Dynasty (960-1279) shipwreck Nanhai I, as one of China’s most important archeological projects of that year. SACH also dispatched a national team of archeologists to offer guidance at the dig site.\nTo date, excavation work on one of two main burial chambers and three of seven satellite tombs has been completed.\nStarting in 2014, the initial team spent almost half a year c l e a r i n g the sevenmeter- high r a m m e d earth burial mound, its layers painst a k i n g l y s t r i p p e d away “like a layer cake,” according to archeologist Tian Zhuang, with each layer photographed and mapped so that, ultimately, “we can reconstruct the building process used by our ancestors,’ Tian explained, adding that the cleared earth would be kept and used to resurface the necropolis in the future.\nLi Wenhuan, a graduate student specializing in artifact conservation, emphasized the care taken to use bamboo sticks and brushes, rather than bare hands, to separate dirt from buried artifacts. Each item unearthed was scanned and photographed before being numbered in sequence, marked with their position in situ, date of excavation and the names of the excavators and conservationists.\n“The geographical location will facilitate the construction of a database, and, in the future, assist the restoration of the original layout of the whole necropolis,” Li added.\nEach process is laborious, with five conservationists spending a month removing corrosion from 10 tons of copper coins unearthed at the site. Chemical solvents are banned from the lab, to avoid further corrosion of grave goods.\nXu Changqing, director of the Jiangxi Provincial Institute of Cultural Relics and Archeology, told NewsChina in mid-January that, historically, only two excavation projects have led SACH to form a national archeological team. One was the 1970s excavation of the Mawangdui tombs in Changsha, Hunan Province, and the other was the 1980s excavation of the Mausoleum of the Nanyue King in Guangzhou, Guangdong Province.\nSome key members of the expert team dispatched by SACH include Xin Lixiang from the National Museum of China, an expert in the history of Qin and Han dynasties; Zhang Zhongli, former team\nThe inner coffin, encased in a crate, is lifted from the site\nThe main conservation lab\nleader for the archeological team responsible for the Qin Shihuang Mausoleum; Hu Dongbo, Peking University professor and an expert in the restoration of ancient bronzes; Wang Yarong, researcher in tapestry and embroidery; and Li Cunxin, a CASS expert specializing in laboratory archeology.\nShan Jixiang described the excavation process at Guanxi as “calm and controllable,” applauding the involvement of local villagers and nationwide media coverage for raising the site’s profile. “It is highly important for the general public to enjoy the right to know, participate in, supervise and enhance the protection of the country’s cultural relics,” said Shan.\n“Neither the government nor the academia should have a monopoly on the protection of cultural relics,” he continued.\n“Instead, it requires the participation of people from across the country.”\nAn archeologist works to preserve a mummified horse\nArtifacts on display in the Jiangxi Provincial Museum\nProspects In late 2015, more than 110 relics from the Haihun mausoleum went on public display at the Jiangxi Provincial Museum in Nanchang, drawing throngs of visitors.\nAccording to some estimates, around 100,000 Han Dynasty tombs still survive in China. That era’s extravagant burial customs make those that have remained undisturbed an irreplaceable treasure trove that could revolutionize knowledge of the period. So far, 60 royal tombs from the Han Dynasty have been legally excavated for research purposes, but the tomb of the Marquis of Haihun has already eclipsed them all in terms of the number, quality and range of artifacts recovered.\nLu Xinshe, governor of Jiangxi Province, commented in a work report issued January 25 that the province will develop the area surrounding the royal tombs into a major tourist attraction. From 2016 onwards, a national archeological park will be established on the site. According to Xu Changqing, Jiangxi Province is also looking to file an application to have the tomb of the Marquis of Haihun listed as a UNESCO World Heritage Site.\nCovering more than 3.6 square kilometers, the Guanxi necropolis dwarfs the 0.72-squarekilometer Forbidden City. Additional tombs belonging to scions of the marquis’ household are also believed to dot the surrounding area. “All these tombs represent a single cultural phenomenon, and no single one should be kept separate, thus the future archeological park will be a grand one and include exuberant cultural relics,” commented Shan Jixiang.\n“[There], people will be able to find sources of Chinese culture and deepen their knowledge and understanding of our ancestors.”","Chinese lacquerwork, decorative work produced in China by the application of many coats of lacquer to a core material such as wood, bamboo, or cloth.\nThe Chinese had discovered as early as the Shang dynasty (c. 1600–1046) that the juice of the lac tree (Rhus vernicifera), a naturally occurring polymer, could be used for forming hard but lightweight vessels when built up in very thin layers through the repeated dipping of a core of carved wood, bamboo, or cloth. With the addition of pigments, most commonly red and black, less frequently green and yellow, it could also be used for painting and decorating the outer layers of these vessels. Coffins, chariots, furniture, and other objects found in Shang tombs were often lacquered, and lacquer was used to fix inlays of shell and coloured stone.\nBeing sticky, painted lacquer must be applied slowly with the brush, giving rise to prolonged motions and fluid, often elegantly curvilinear designs. Since lacquer is almost totally impervious to water, vessels and wine cups have been excavated in perfect condition from waterlogged graves of the late 5th-century-bce Zeng state in Suixian, of the 4th–3rd-century-bce Chu state in Jiangling (now Shashi), and of the early 2nd-century-bce Han dynasty in Changsha. Such works ranged from large-scale coffins to bird- or animal-shaped drum stands to such daily utensils as nested toiletry boxes and food-serving implements. By the Warring States period (475–221 bce), lacquerwork had developed into a major industry; and, being approximately 10 times more costly than their bronze equivalents, lacquer vessels came to rival bronzes as the most esteemed medium for providing offerings in ancestral ceremonies among the wealthy aristocracy.\nObjects in lacquer, chiefly from the state of Chu and from Sichuan, depict hunting scenes, chariots and horsemen, and fantastic winged creatures drawn from folklore and painted in a simple but lively style. Large painted-lacquer coffins with such creatures depicted were present in the 5th-century-bce royal tomb of Marquis Yi of Zeng (Zenghouyi). The labour required for these coffins is suggested by the set of nested Han dynasty coffins found at Mawangdui (two bearing exquisite landscape designs, described below), which are said to represent one million work-hours. A lacquer painted storage box from the Zeng tomb bears the earliest depiction of two of the Chinese directional animals (formerly thought to date from the later Han), together with the names of the 28 stars used in Chinese astrology (previously believed to have been introduced at a later time from Iran or India).\nThe most remarkable excavated tomb of the Han dynasty belonged to the wife of a mid-level aristocrat, one of three family tombs of the governor of Chansha found in Mawangdui, a suburb of that southern city, and dating from 168 bce or shortly after. Small in scale but richly equipped and perfectly preserved, the wooden tomb consists of several outer compartments for grave goods tightly arranged around a set of four nested lacquered coffins. An outer layer of sticky white kaolin clay prevented moisture from penetrating the tomb, and an inner layer of charcoal fixed all the available oxygen within a day of burial, so the deceased (Xinzhui, or Lady Dai, the governor’s wife) was found in a near-perfect state of preservation. Included among the grave goods, which came with a written inventory providing contemporaneous terminology, are the finest caches yet discovered of early Chinese silks (gauzes and damasks, twills and embroideries, including many whole garments) and lacquerwares (including wood-, bamboo-, and cloth-cored examples), together with a remarkable painted banner that might have been carried by the shaman in the funerary procession.\nTest Your Knowledge\nArt & Architecture: Fact or Fiction?\nBy the Han dynasty, lacquer production was chiefly carried on at Changsha and in four regional factories in Shu (modern Sichuan) under government control. In addition to the fine lacquerwares excavated from tombs in Changsha, splendid products of the Sichuan workshops, bearing inscriptions dated between 85 bce and 71 ce, have been found in tombs of Chinese colonists at Lelang (Nangnang) in North Korea, and pieces of Han lacquerware have been found as far afield as northern Mongolia and Afghanistan.\nThe different stages of Han lacquer manufacture were divided among a number of specialized craftsmen. The sugong, for example, prepared the base, which might be of hemp cloth, wood, or bamboo basketwork; after priming, the base was covered with successive layers of lacquer by the xiugong. The top layer, applied by the shanggong, was polished and so prepared for the painter, huagong, who decorated it. Others might inlay the design or engrave through the top coating to another colour beneath it, add gilding, and write or engrave an inscription. A wine cup found at Lelang bears an inscription giving its capacity, the names of the people concerned in its manufacture, a date equivalent to 4 ce, and place of origin, the “Western Factory” in Shu Commandery.\nAmong the most celebrated examples of Han lacquer painting is a basket found at Lelang (National Museum, Seoul), decorated with 94 small figures of paragons of filial piety, virtuous and wicked rulers, and ancient worthies. Although confined to a narrow band around the inner rim of the basket, these tiny figures are lively and animated, moving easily in the small space. A tray, also found at Lelang and dated correspondingly to 69 ce, bears near the rim a small painting of Xiwangmu, Queen Mother of the West, sitting with an attendant or visitor on her fairy mountain. Here the lacquer is applied much more thinly, and the brushstrokes have an easy fluency.\nDetailed accounts of the lacquerware of the Song dynasty (960–1279) come from two Ming dynasty works. They describe a red lacquer made for use in the palace that was carved with landscapes, figures, and birds; vessels painted in five colours, as well as gold and silver; and bowls black outside and carved red inside. No certain Song pieces matching these descriptions have yet been discovered, however, and it is generally thought that carved red lacquer did not develop until the Yuan dynasty. A bowl (in the British Museum) of lacquered wood with a silver lining engraved with panels of birds and flowers is a rare exception to the character of known Song lacquer; excavated bowls, cups, dishes, and boxes of dull red lacquer are sometimes deeply lobed to resemble a lotus flower but are otherwise undecorated.\nWhile lacquer continued to be made in bolder versions of the undecorated Tang and Song shapes, notable advances in the Yuan dynasty included incising and engraving and filling the lines with gold leaf or silver powder. An example of this technique is a sutra box with floral ornament, dated 1315 (in Komyō-bō, Hiroshima, Japan). The most important innovation was the carving of pictorial designs, floral patterns, or dragons through a thick coating of red or, less frequently, black lacquer. A connoisseur’s manual, Geguyaolun (“Essential Criteria of Antiquities”) by Cao Zhao, says that at the end of the Yuan dynasty Zhang Cheng and Yang Mao, pupils of Yang Hui, were noted for this technique. A number of pieces bearing their names exist today. It had been considered that these were later imitations, made chiefly in Japan, and that carving pictorial designs in lacquer was first practiced in the Ming dynasty. But the 1959 discovery near Shanghai, in a tomb dated equivalent to 1351, of a small lacquer box carved with figures in a landscape shows that this technique was already well established in the mid-14th century.\nThe carved lacquer first developed in the Yuan dynasty continued through the Ming and Qing and was made in many different factories. It reached a high level in carved red lacquer (tihong) dishes, trays, covered boxes, and cups of the Yongle and Xuande reigns. Yongle reign marks, scratched on with a sharp point, are not reliable, but some pieces, bearing carved and gold-inlaid marks of the Xuande emperor, may be of the period. It is often difficult to distinguish genuine Ming lacquer from Korean and Japanese imitations, and reign marks are not in themselves a reliable guide to dating.\nBritannica Lists & Quizzes\nDecoration of this early Ming lacquer includes both pictorial designs (landscapes with figures in pavilions are common) and rich dragon, phoenix, and floral motifs, carved deeply in a full, freely flowing and plastic style, often against a yellow background. While this style continued into the 16th century, the Jiajing period also saw the emergence of more realistic and intricate designs that are shallower and more sharply carved, sometimes through as many as nine layers of different colours, on a background consisting of minute brocade (allover floral and figure designs) or diaper (diamond-shaped) patterns. Other techniques that were popular in the middle decades of the Ming include carving through alternate layers of red and black lacquer, known by the Japanese name guri; inlaying one colour with another; and outlining the inlay with engraved lines filled with gold lacquer. Painting and inlaying with mother-of-pearl and other materials were also employed."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:3e5069e1-5793-44b9-b4b5-8388eed18389>","<urn:uuid:a0e5c731-ed65-412e-a11c-34e061978241>"],"error":null}
{"question":"Latest updates on marine conservation - what techniques are scientists using to study fish survival, and how are they working with local communities?","answer":"Scientists are collaborating closely with local communities while employing various research techniques to study fish survival. For eels, researchers work with mana whenua (local indigenous groups) at three river mouth sites, using methods like modified whitebait nets and electric fishing to catch glass eels for study. The partnership with indigenous communities has been crucial for research success. For triggerfish studies, scientists partner with local commercial fishermen in a NOAA-funded Cooperative Research Program, combining scientific research with commercial fishing expertise. They use both surface tagging and underwater scuba operations to tag fish and study survival rates, particularly focusing on how catch-and-release practices affect fish mortality. This collaborative approach helps bridge the gap between scientific research and fishing communities while gathering vital data on fish populations.","context":["Unlocking the mysterious marine life of eels\nTuna, or freshwater eels, hold a very special place for iwi. Widely valued as both a food source and a taonga, they are a culturally significant mahinga kai species.\nTheir survival, and the protection of the waters they live in, is paramount to many.\nToby Salmon, mana whenua from Ngāti Awa, is in no doubt about the importance of tuna habitat in the Bay of Plenty’s Rangitāiki River.\n“This is our cupboard, this river, and if we don’t look after it, if we don’t nurture it, we’re going to lose it,” says Salmon.\nTuna have a very unusual life cycle that sees them travelling between freshwater and ocean environments, and much remains secret about the mysterious marine phase of their lives.\nWe know mature adults leave New Zealand’s rivers and swim out to spawn in the western Pacific Ocean. We also know that their larvae return to New Zealand’s coast, transforming into tiny glass eels (named for their translucent appearance) for the start of their freshwater life.\nNo one knows, however, where those spawning grounds are, which ocean currents the larvae use to return or whether shortfin and longfin eels use different ocean spawning grounds and currents.\nFreshwater fish ecologist Dr Eimear Egan is leading an MBIE-funded Smart Idea project which aims to unlock some of the answers.\nOver recent months, Egan and her team have been working with mana whenua at three river mouth sites. The chosen rivers are the Rangitāiki, Grey and the Ashley, where the researchers have been working with Ngāti Awa Rangitāiki Hapū Coalition, Rangitāiki River Forum, Ngāti Waewae and Te Ngāi Tūāhuriri Tuahiwi (Ngāi Tahu).\nEgan says the partnership and support from mana whenua in each location has been crucial to their success.\nUnder the light of the moon, team members trek down to the respective river mouths in search of the elusive glass eels. They have used a variety of methods to catch their tiny translucent targets, including scooping the surf with a modified whitebait net, customised set nets from Australia and electric fishing.\n“We even picked a few hundred up from the shoreline after they got stranded on a king tide!” says Egan.\nThe researchers are now back in the lab hard at work on their tiny catch. This includes extracting and examining the glass eels’ minute ear bones (otoliths) and dissecting their tissue for chemical analysis.\nA layer of calcium carbonate is deposited on the otolith each day and the microstructure and chemistry of the otolith can reveal information about their marine environments.\n“The layers act as a daily diary of an eel’s life and can be used to reveal the routes the larvae take to get to New Zealand’s coastline,” says Egan.\nThe team are also analysing archived otoliths to reconstruct marine conditions experienced by eel larvae over the past decade.\nEgan and her colleagues use Oxygen 18 isotopes to estimate the thermal conditions eel larvae have experienced at sea and around New Zealand’s coastline.\nShe is hoping analysis of the glass eel tissue, using bulk isotopes and compound specific stable isotopes of amino acids will help pinpoint where in the western Pacific Ocean the larvae might have come from.\n“We are hoping not only to get a more fundamental understanding about the marine life of eel, but also to begin to understand the potential effects of changing ocean conditions.\"\n“If we can better predict the implications of ocean changes on the survival and recruitment of juvenile eels, then we can feed this information into models to get a better understanding of eel populations themselves.”\nEgan’s glass eel findings will complement work in a related NIWA project where researchers tagged several female longfin tuna before they left New Zealand to spawn.\nA video about The world's most mysterious fish. NIWA researchers are working with iwi to try to unlock the secrets of New Zealand tuna—freshwater eels. Every year tiny, glass eels wash in on the tide at river mouths along our coast. But where do they come from and how do they get there?","Tag, You’re It!\n“So, a gray triggerfish that’s caught and then thrown back for whatever reason, what sort of shot does that fish have of making it?” This is a question CMAST’s Brendan Runde asked hypothetically as he sat working on dozens of tags spread out in front of him, tags that will end up inside of fish as part of his study. Runde is doing his master’s thesis on the gray triggerfish, one of the most popular recreationally and commercially caught fish from North Carolina to Florida.\nThe tagging program attempts to test the hypothesis that fish which undergo this barotrauma when caught and then thrown back don’t have as great a chance to survive the experience as if they had not undergone the catch and discard process.\nTesting this hypothesis will lead to an expanded understanding of the triggerfish population and is a project that involves extensive surface tagging of fish, deep-sea scuba work, cooperative efforts between CMAST researchers and a local commercial fisherman, and lots and lots of fishing. The approach is to bring these fish up, tag them, throw them back and, in classic message in a bottle fashion, hope they return to researchers with data that leads to answers.\nThe gray triggerfish lives in water depths of 100 feet or more. When they are brought to the surface by being caught, they typically suffer barotrauma, which is injury due to extreme and rapid pressure changes. In gray triggerfish, common forms of barotrauma include bloating of the abdominal cavity, and intestinal prolapse. If a fish is caught and then discarded, it may have sustained injuries that end its life before it can grow big enough to end up in a fisherman’s cooler another day.\nData on how many of these “catch and discard” specimens grow and thrive to reach legal limits for later catches is critical to setting sustainable catch limits.\nTracking the lives of individual fish would seem at first glance to be a needle-in-a-haystack proposition. However, there are a few factors that make the gray triggerfish a likely candidate for this study. First and foremost, they exhibit what is called “high site fidelity,” which means they tend to live their lives in one spot. Males in particular form a ‘harem’ and then stick close to home. Most fish are recaught in the same spot they were originally tagged, and if not, usually within a one mile radius. This high site fidelity helps ensure that a fish that was not recaptured was lost due to mortality rather than simply migrating from the area.\nHow the tagging process works\nResearchers use hook and line fishing methods for triggerfish in waters approximately 130 feet deep. When a triggerfish is caught, a small incision is made under the skin and an internal-anchor wire-core tag is inserted. On the tag are the fish ID number and the phone number to call if a tagged fish is caught; anyone catching a tagged fish can contact the researchers for a reward. Once inserted, the tag looks very much like a yellow plastic bread-tie poking a bit outside the fish’s skin.\nAfter tagging, the fish is examined for outward signs of barotrauma. This information is recorded, and the fish is then released. The fish’s response once it hits the water is also carefully recorded. Some take off for the bottom vigorously, apparently none the worse for wear by the experience. Others have more trouble, and, rarely, some die at the surface. The data recorded at the time of catch and tagging will be referenced if the fish is recaught.\nHowever, that’s only half the story. A control group of fish who have not undergone the barotrauma of being caught at the surface are also being tagged and tracked. Tagging these fish is a little trickier. Fish traps are set on the bottom at these same fishing sites where scientists are catching and tagging fish at the surface. Scuba divers, who have only about fifteen minutes of bottom time per session, go down to the traps to perform the incision and tagging procedures on the fish in the traps, which are then released.\nWith an equal number of surface-caught fish and control group fish tagged, researchers can then develop a formula to calculate how many of the caught and discarded fish died as a result of barotrauma. As a simplified example, if 50% of the control group fish are caught, and there is a 25% return rate for fish caught at the surface, it can be concluded that roughly half of the fish caught at the surface die as a result of barotrauma. Because of the characteristics of the gray triggerfish, the number which simply leave the area is surprisingly small.\nA study such as this one is very labor intensive. The tagging process began in May of 2015 and will continue through 2016. There are 1,500 tags to be deployed. That’s a lot of fishing, and a lot of diving, but at the end of the study, there should also be a lot of data, and that’s what it will take to answer questions about what’s happening with this population of fish.\nThe number of gray triggerfish caught and released every year seems to be on an upward trend. Some states have recently implemented minimum size requirements where there were none before. Other states have put quota systems into effect, so there is more motivation for throwing back a fish under the limit. Catch and discard rates are increasing as a result, and any gray triggerfish released are then subjected to the gauntlet of potential causes of mortality. A tagging study such as this one will help determine how discard rates are affecting the species population as a whole, and may allow scientists to predict how this trend will continue.\nThis tagging project is funded by a grant from NOAA, under the Cooperative Research Program. It is a yearly grant made available to scientists who want to cooperate with the fishing community to promote knowledge and provide outreach, combining commerce, recreation, and science. The CMAST tagging team is made up of principal investigator Dr. Jeff Buckel, assisted by Paul Rudershausen, and Brendan Runde. Runde will come out the other side of this project with a master’s thesis that will provide important clues to federal agencies who are gathering this and other data to sustainably manage triggerfish.\nLocal commercial fisherman Tom Burgess of Sneads Ferry is another important participant in this study. He is part of the original grant, and much of the bottom fishing and tagging is done on his vessel. Runde was enthusiastic in discussing the collaborative aspect of the project. “This is the interface between fishermen and scientists, so that everybody understands each other a little bit better. Communicating scientific findings to those outside of the scientific community is such an important part of the big picture.”"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:70c1f27a-4e62-4d62-8b8b-3cee2525b310>","<urn:uuid:7aaecfcd-48f4-4e89-aea8-50b898e36743>"],"error":null}
{"question":"I'm researching restaurant marketing strategies. What are the key findings about consumer deal-seeking behavior, and which customer segments should restaurants specifically target?","answer":"Regarding deal-seeking behavior, studies show that digital marketing is most effective when offering personalized deals, with 38% of consumers being attracted to special offers and 71% preferring tailored promotions. Nearly half (42%) of consumers use mobile devices specifically to look for restaurant coupons and deals. As for target segments, restaurants should focus on specific groups like teens/students, families, seniors, business people, tourists, lunch break crowd, and after-bar crowd. It's important to note that some segments may clash - for example, targeting families could drive away customers at an elegant restaurant that primarily serves empty nesters.","context":["In this month’s Trend Watch roundup, we are looking at how the rise of sober curiosity gave way to a curious comeback for the soda industry. Also, we’re taking a look at how breakfast has evolved to become a more convenient and efficient meal to match today’s consumer and a bird’s eye view of what today’s average American independent restaurant looks like.\nOne of the biggest challenges facing the ever-growing food delivery industry? Communication. Whether it’s about missing orders, late or cold food, or complete order mix-ups, third-party delivery is clearly still facing growing pains.\nIn fact, consumers experience problems with third-party food delivery websites nearly one-quarter (24.4%) of the time, according to a study of 1,084 customers from Zion & Zion. Here are their biggest complaints:\nAnd customers aren’t just taking these errors in stride. According to the study 51% of respondents are “very frustrated” when their order is wrong. And when issues do happen, non-Millennials (aka older generations) are 56.3% likely to complain to the restaurant, while Millennials are only 34.2% likely to complain to the restaurant. Neither age group is likely to complain to the delivery platform directly.\n“As the study shows, when there is a delivery problem, customers often blame both parties because they don’t know who to blame,” Zion & Zion CEO Aric Zion said. “To fix this, delivery app companies need to provide more transparency into the process of how an order gets processed. If consumers know which party performed each function, then they would be more likely to blame the responsible party.”\nSoda drinking spikes as sober curiosity trend continues\nNielsen says that more than one in five U.S. consumers participated in Dry January last year. As sober curiosity continues to take off, one industry is taking advantage of the spiritless trend: soft drinks.\nAccording to Nielsen, soft drinks were consumed by nearly half (46%) of all Dry January participants, more so than water (43%). The growth in popularity of beverage alternatives has led to a 2.9% surge in soft drink sales in 2019. The comeback of sugary drinks is juxtaposed with increasing interest in healthy habits and wellness-based eating.\n“Soft drinks are a reliable alternative in social gatherings for abstainers who are looking for more than water,” Matt Crompton, Client Solutions Director for Nielsen CGA said in a statement. “And as soft drink manufacturers and brands continue to evolve meet healthier lifestyles (with reduced-sugar, healthier and premium options), the opportunity to effectively and strategically engage both the regular and occasional abstainers will continue to grow.”\nWorth the splurge?\nWhen it comes to treating themselves, consumers are more likely to spend their money on restaurant meals than any other category.\nIn fact, according to a study by public relations agency 5WPR, dining out is the number-one splurge-worthy category and 43% of consumers find themselves spending more of their paycheck on restaurant meals, followed by travel and experiences (41%) and snacks (39%), and well ahead of material possessions like electronics (35%) and clothing (27%).\nThis is what the average independent restaurant looks like\nDatassential recently curated data on 500 single-unit operators to find out what they’re serving, how they’re running their business, and how they’re growing. Here’s a snapshot of the average American independent restaurant:\nGood-old reliable breakfast is changing: Here’s how\nBreakfast has always been one of the most important meals of the day in American diets and while that’s not changing — Americans consumed 102 billion breakfasts in 2019 — but it’s starting to evolve.\nAccording to NPD Group, the usual eggs and bacon may not cut it for the convenience-obsessed American of today. Americans are looking for functional, convenient and functional morning meals: whether that comes from a quick-service breakfast or an on-the-go snack.\n“Solutions would be the foods and beverages that meet those needs or mechanisms, like mobile ordering, that are convenient and save them time,” an analyst with NPD group said.\nBut just because non-traditional breakfast, like burgers, is on the rise, don’t go changing your breakfast menu just yet:\n“In terms of servings volume, the top breakfast foods are traditional, like breakfast sandwiches and wraps, bacon/sausage/ham, sweet breads, potatoes, savory breads, eggs/omelets, and pancakes/waffles/French toast,” the NPD Group representative said.\nEffective digital marketing starts with more deals and coupons, study says\nDigital marketing is a must for any restaurant today looking to reach or expand their customer base. But what’s the best strategy? According to marketing and advertising firm AdTheorent’s Dining Trends Report, consumers are not as loyal as they once were: About one in four customers will go to their preferred restaurant’s competitor if they receive a mobile device ad on the way to a quick-service or fast-casual restaurant.\nWith all of the noise from native advertisements on social media to popup ads and email marketing campaigns, the best digital advertising strategy is to offer a personalized deal. According to AdTheorent, the top ads that attract attention are the ones that provide a special offer (38% of consumers) that is specially tailored to their interests and consumer habits (71% of consumers).\n“Restaurants can improve their advertising experience for consumers through a combination of proper targeting and messaging,” Jim Lawson, CEO of AdTheorent said. \"Our research shows that relevant and tailored ads are more well received and drive superior results for marketers.\"\nTo further emphasize the importance of personalized digital coupons, the AdTheorent study shows that 70% of consumers use their mobile devices when selecting and ordering from a restaurant, and nearly half (42%) are looking for coupons and deals. In comparison, only 32% are placing a food order.","Target Marketing for RestaurantsTarget Marketing for Restaurants\nYour marketing strategy should attract the customers that you want in your restaurant. As opposed to mass marketing, which aims its advertising and promotions to a general majority of consumers, target marketing focuses on attracting a specific type of customer.\nYou probably already know what kinds of customers you want in your restaurant, but it is important to tailor your promotions to these specific consumer groups. Different market segments – or groups of customers with similar characteristics and needs – will respond to different types of marketing strategies, so you should make sure to gear your promotional techniques toward the segments you want.\nUsually, you will need to target customers based on their income. This allows businesses to adjust the pricing of menu items according to their customers’ disposable income levels. However, you may also choose to develop promotional strategies that target one or more the following segments:\n- Teens, students and young adults » Learn more about marketing to Generation Y\n- Families and children » Learn more about marketing your restaurant to families\n- Seniors » Learn more about marketing your restaurant to senior citizens\n- Empty nesters\n- Tourists » Learn more about attracting tourists to your restaurant\n- Eco-conscious people » Learn more about green restaurant marketing\n- Business people\n- Sports fans » Learn more about how to use sports marketing in your restaurant\n- Gay community\n- After-bar crowd\n- Happy hour crowd » Learn more about happy hour marketing\n- Lunch break crowd » Learn more about how to increase the lunch crowd in your restaurant\n- Music lovers\nYou can choose the best promotional techniques for your restaurant by honing your marketing strategy to attract specific groups of customers like the ones listed above.\nAttracting the Wrong Customers\nEven if your business is already in operation, you need to keep assessing your customer base and ask yourself if your marketing is really pulling in the customers you want to target. If not, your marketing techniques are failing. Your strategy could be problematic for one of the following reasons:\nLocation is off.\nPerhaps you are not in the right location to attract the kind of customers you want. Always remain up-to-date with the demographics. Keep in mind that demographics, attitudes and trends in your area can change, and your restaurant needs to change with them or move to another place. » Learn more about restaurant demographics\nConcept misses the mark.\nIt is possible that your concept is failing to attract the customers you want. For example, an ice cream shop that is run near a university in hopes of attracting the student population may overlook the recent frozen yogurt craze that is sweeping through campus. It is not necessarily too late to make some changes. This problem could be fixed by adjusting the concept to include frozen yogurt products, and then marketing the new products to the students with coupons or other promotions.\nAttracting the wrong demographic.\nIt is possible that you are accidentally attracting the wrong customers. For example, Chuck E. Cheese targets children and families, but in recent years has seen a lot of teenage customers at the restaurants’ arcades. This has caused numerous problems, including several brawls that have broken out, driving away the very families that the restaurant hopes to attract. To combat this problem, several Chuck E. Cheeses have chosen to implement new policies, like refusing service to anyone that is under the age of 18 without a parent or guardian present.1\nIt is also possible that you are using the wrong promotional techniques. Always make sure that you are using the right promotions for your target market.\nChoosing Complementary Segments\nIf your business is attracting many of the customers you want but is still struggling, you could try targeting a secondary segment of the population. You could focus on one segment during the lunch hour and another during dinner. Or you could try to attract multiple types of customers at once, since many segments go hand-in-hand and would complement each other well as customers.\nAvoid customers that clash.\nSome segments may not go well together. For example, if you run a formal restaurant that is known for its elegance, you may be attracting a lot of empty nesters. It is probably not wise to begin marketing yourself to families and children, since the associated noise and atmosphere could drive away your existing business. Any time you choose to target a new segment with your marketing, be sure to consider the consequences on your existing customer base.\nIf you have the right location and concept, and you choose a good target market and the best promotional techniques for that segment, your restaurant will be far more likely to succeed. When accompanied with proper restaurant management and operation, a good target marketing strategy will make a considerable difference in your profits.\nMore from Target Marketing for Restaurants...\n- Restaurant Marketing 101\n- 8 Marketing Technologies that Affect Customer Restaurant Choices\n- Restaurant Branding and Design\n- Environmental Analysis: Making the Most of Your Restaurant's Location\n- Demographics for Restaurants\n- Gauging Your Restaurant's Competition\n- Creating Repeat Customers at Your Restaurant\n- Restaurant Marketing Assessments\n- Restaurant Marketing Glossary\n- Menu Design\nBack to Target Marketing for Restaurants"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:155fd5fa-4a01-47f7-938e-d5d58d9508d8>","<urn:uuid:ca7fad75-e0bd-4821-884a-a881807c69b1>"],"error":null}
{"question":"Could you compare the tower construction periods between Peel Castle and Warwick Castle? I'm doing research on medieval architecture.","answer":"Both castles saw significant tower construction in the 14th century. Peel Castle's flanking towers were built in the 14th century and were linked by 15th century curtain walls. Similarly, Warwick Castle's major towers, including Caesar's Tower and Guy's Tower, were built in the 14th century during the de Beauchamp family's ownership. However, Peel Castle also has an earlier Round Tower from its monastic period, which stands 50ft (16m) tall and was used as a belfry, lookout place, and refuge, while Warwick's earliest structure was the Conqueror's Fortress from 1068, which originally had a wooden keep.","context":["The first known fortification on St Patrick’s Isle came after the arrival in 1098 of the invading Viking chieftain known as Magnus Barefoot. The Chronicles of the Kings of Mann and the Isles tells us that Magnus saw to the erection of two wooden prefabricated forts, one now believed to be in the South of the island and the other on St Patrick’s Isle. It is likely that the rampart of this fort would have been topped by some sort of wooded fence or paling (=Peel), the possible inspiration of the eventual name of both the Castle and the Town.\nThe visible stonework in the Castle includes the remains of buildings belonging to the Celtic monastery, possibly dating from the 6-8th centuries and the 13th century cathedral of St German and later buildings allied to it. Other buildings include a medieval armory and barracks. These are protected by fortifications consisting of 14th century flanking towers, linked by 15th century curtain walls. The latest military construction took place in the early 19th century during the Napoleonic wars and later in the 1860s during the time of Napoleon III. It is considered that a vestige of Magnus’s fort was found during the final phase of the excavations of 1982-87, when rough stonework of a rampart was uncovered at a depth of 5 metres beneath the surface, just inside the present curtain wall to the north of the Cathedral.\nIt is believed that Irish missionaries came here about the year 550, disciples of St Patrick and probably not, as local legend has it, led by him, but by St Carmane (or German). This would be a suitably isolated spot for monks of the Christian Celtic Church to choose. At the highest point of St Patrick’s Isle stands the 50ft (16m) Round Tower. This is of the type seen at monastic sites in Ireland, where the towers tend to taper towards the top, unlike Peel’s which has vertical sides. Though its top is battlemented, it once had a conical roof, like those in Irleand. The Tower would have acted as a belfry, as a lookout place and as a place of refuge. Nearby are as many as three keeills or chapels, the largest being St Patrick’s Church, which was enlarged in the 12th century, possibly with the intention for it to be a cathedral. Until about 1500, it acted as the Parish Church for Patrick, when St Peter’s Church in Peel became the parish church for both Patrick and German.\nIrish sagas record St Patrick’s Isle suffering a Viking attack, but modern belief is that it was another St Patrick’s Isle, that near Dublin, which is indicated. It is likely that the settlement of the pagan Vikings about the year 800 would witness the decline of sites such as this monastery, possibly followed by destruction during later invasions.\nHowever, it is likely that once the Vikings were Christianised, St Patrick’s Isle once again became a holy place when the diocese of Sodor, with its cathedral, was established here. St German’s Cathedral was built of the local red sandstone from across Peel bay about 1230, replacing a previous church. This was alongside the residence of the King and capital of the Viking Kingdom of Mann and the Isles.\nAfter the Isle of Man passed into English suzerainty in the early 14th century, the fortress we see today was created. Firstly, about 1390, the sandstone keep – the peel tower which eventually was to give its name to the town – was built, along with the so-called ‘Red Curtain’, the adjoining sandstone curtain walls. This was to compensate for the weakness of the defenses at this point, so easy of access at low tide. Flanking towers were also built and later, the rest of the curtain walls. These are nicknamed ‘The Green Curtain’ after the slate used, which was quarried from an area just outside the Castle, at the northern most part of St Patrick’s Isle.\nPeel Castle was garrisoned as the administrative centre of the Northside of the Island, with Castle Rushen controlling the Southside. In addition to its governmental role, Peel Castle was used as prison. One of the best known political prisoners was Edward Christian, the Earl of Derby’s Lieutenant Governor of the Island. Christian was one of the leaders of the 1643 protest about tithes. The Earl very cleverly calmed the large crowd of protesters who had gathered to meet him at The Green at Peel – today a public car park and grassy area along Peel Promenade. Christian, whose revolutionary reforms included election of the members of the House of Keys, was arrested and charged with treason. He died in 1661, during his second period of imprisonment in Peel Castle.\nThe Cathedral crypt became notorious as the Bishop’s prison, housing Sabbath breakers for short periods. This crypt was, and remains, dank, dark and cheerless, conditions which made long sentences unnecessary. Those incarcerated had been caught, possibly by their neighbours, desecrating the Sabbath or special saint’s days by playing the violin, making hay or by fishing. Meanwhile, just to the north of the Cathedral, in the most sheltered area of the Castle, the English Lords of Mann, the Earls of Derby, built their private apartments. The Cathedral continued its role until 1785, when it saw its last enthronement of a Bishop. Even then in a bad state of repair, its final humiliation came in 1824, when the remaining roof timbers collapsed in a violent storm, leaving the Cathedral as we see it today.\nSave for the construction of a revetment for two heavy guns, plus a guardhouse and powderhouse about 1816, the Castle was abandoned until a revival of the garrison was necessary in the 1860s because of the scare caused by the renewed predatory ambitions of France. Today’s ruinous state of the former Lord’s apartment buildings is due to their being used as a quarry for the construction of these defenses.\nThere was some refurbishment at the behest of Sir Henry Loch, the Island’s enlightened Lieutenant Governor 1863-83. His period of duty coincided with the burgeoning of both the tourist trade and the fishing industry, both of which became so important to the livelihood of Peel’s inhabitants.\nThere have been several archaeological excavations carried out in Peel Castle. By far the largest was carried out 1982-87 by Liverpool University archaeology group at the behest of the Manx Museum and the then Government Property trustees. This was a ‘teaching dig’ in which the public were invited to take part, supervised by qualified archaeologists. A large number of volunteers, mainly local residents, but people from Britain and some from abroad, including the USA, also took part.\nThe many important finds included a cache of silver coins dating from around 1030, an extensive burial ground with several hundred graves, which included that of the so called ‘Pagan lady’, buried with her beautiful necklace, signs of Magnus Barefoot’s fort of 1098 and many details of the Lord of Mann’s apartments. An unexpected bonus as a claim to fame is the finding of the British Isles’ earliest human flea. The dig progressed through the ruined buildings to the north of the Cathedral, revealing details hitherto unseen for centuries. Overall, the outcome of these excavations was a much greater understanding of the role of Peel Castle in the Island’s history.","Towers & Battlements\nThe Defensive Towers\nClimb the towers and ramparts and see what you can spot from the top!\nThe Towers and Ramparts were largely constructed during the 14th and 15th centuries when the castle was under the ownership of the de Beauchamp family. The family spent vast amounts of their fortune on their construction. They were intended to protect those within the castle walls and symbolised the power and importance of the castle’s Earls.\nThe Gatehouse & Barbican\nSince the entrance to a castle was likely to be a common target for an attacking force, it was necessary for the defences to be as effective and deadly as possible. The first defence attackers would face was the Barbican. The Barbican was an exterior walled passage which had a drawbridge and extended out into the ditch to provide the Gatehouse with more protection.\nSoldiers would be faced with the first iron portcullis and a barrage of crossbow bolts. The attackers would then find themselves in a dark narrow roofed passage with arrow slits on either side and, worse still, murder holes above, from which stones, waste, and boiling substances would rain down upon them. This was followed by an open air space known as the killing zone above which waiting soldiers would fire down arrows onto the advancing army. For the few that made it through these treacherous defences, they would then have to struggle towards the Gatehouse. They would be confronted by yet another portcullis, another set of murder holes and another door.\nThe Conqueror's Fortress\nThe Conqueror's Fortress, also known as 'the Mound', is the oldest surviving part of the castle and has stood since the castle was first built in 1068. It is part of the original motte and bailey castle that William the Conqueror had constructed.\nThe Mound originally had a wooden keep on the top, which would have served as the castle’s main stronghold. The Mound no longer houses a keep, which was replaced in the 17th century by a stone observation platform that offers commanding views over the surrounding area. Around the bottom of the Mound was a ditch, the earth from which would most likely have been used to build the Mound itself. In front of the Mound was the bailey, similar in size to our courtyard today, enclosed by a palisade and another ditch beyond that. A number of houses and workers’ buildings including the blacksmith’s forge, kitchens and stables were located inside the bailey.\nDid you know?\nThe castle ditch was never filled with water due to its height above the river, but instead contained sewage from the castle.\nThe Curtain Walls\nThe curtain walls are the main walls that connect the towers and main castle structure. Along the wall, walkways were built that allowed crossbowmen and archers to move swiftly from one end of the castle to the other during an attack.\nCaesar’s Tower was built on the orders of Thomas de Beauchamp in the 14th century and is a great example of military architecture. It is the tallest tower in the castle, and comprises four storeys, excluding the Gaol. It has an irregular quatrefoil or cloverleaf shape.\nFrom the top of Caesar’s Tower you can see the remains of a ruined medieval bridge. This bridge was formerly the entrance into Warwick from the south and enabled the Earls of Warwick to charge a toll for visitors entering the town. The bridge was ruined by the 18th century and a new bridge was built further up the Avon, which you can still see today.\nThis tower now holds two magnificent bedrooms, the Peacock Suite and the Rose Suite, commonly known as the Tower Suites, which provide luxurious accommodation for overnight guests.\nDid you know?\nKing Edward IV was held prisoner by the Kingmaker in Caesar’s Tower in 1469.\nGuy’s Tower was built in the 14th century. It has twelve sides and five storeys. The first four storeys consist of a central stone-vaulted chamber with two small side rooms; one a garderobe (medieval toilet), the other probably a bedchamber. The fifth storey is a hexagonal guardroom.\nThis tower would have originally been used to accommodate important guests of the Earls of Warwick. However, these rooms were sometimes used to house prisoners including William Sutherland, William Stanley, and an Edward Disney, all leaving their identity behind through graffiti on the walls.\nThe tops of the towers are encircled by parapets that added a further layer to the castle’s defences. Cut into the floor of the parapet are openings, or machicolations, through which the garrison could drop or pour substances onto the attackers below. Today, Guy’s Tower provides beautiful panoramic views of Warwickshire.\nBear & Clarence Towers\nBear and Clarence Towers were part of construction plans for a royal keep on the north side of the castle created by the future King Richard III during his ownership of the castle. Richard pulled down a section of the existing wall and began the building of these towers, which were due to be part of a larger fortification with towers in each corner. After Richard’s death at the Battle of Bosworth in 1485, construction ceased. The towers were originally intended to be as tall as Guy’s Tower and create a four-way tower house.\nThe Gaol occupies the lowest chamber of Caesar's Tower built in the 14th century, and was the original castle dungeon. Leading from a hatch in the ground at the base of the tower, a single flight of steps provides the only way into, and out of, the Gaol.\nA single open drain running across the floor provided the only means of sanitation, whilst the only light to penetrate the gloom came from a tiny shaft high on the wall and from a small window in a chamber behind an iron grill. It was from behind this grill that a guard could observe the prisoners."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:c76a130f-35ea-4219-85fb-2dc53adb89fe>","<urn:uuid:a4accffd-f8f2-429a-91ab-b090ff636b2e>"],"error":null}
{"question":"How does the marine ecosystem and diving conditions vary between different diving locations along Costa Rica's Pacific coast throughout the year?","answer":"Costa Rica's Pacific coast diving locations feature diverse conditions and ecosystems. The underwater environment consists of volcanic rock formations and pinnacles with small hard corals, sponges, and gorgonians. Visibility ranges from 10-40 meters, with best conditions from May to September. Water temperatures typically range from 26-29ºC, though they can drop to 19ºC from December to March. Different locations offer unique experiences: Guanacaste's bay provides access to key dive areas with manta rays in winter, the southern area is a protected biological reserve with rays, turtles, and reef sharks, while Cocos Island features unique pelagic species and endemic fish. During whale season (July to September), divers can hear humpback and pilot whales underwater.","context":["Featured Adventure – Scuba Diving in Costa Rica\nBordered by the Pacific Ocean and the Caribbean Sea in the heart of Central America, Costa Rica is a prime ecotourism destination and boasts some of the world’s most biologically diverse habitats—including rainforest, volcano and mangrove ecosystems. While the Caribbean coast has yet to develop a serious scuba diving infrastructure, diving off the Pacific Coast has become extremely popular. The underwater volcanic rock formations and pinnacles are home to small hard corals, sponges and gorgonians.\nGuanacaste, the giant bay off the northwest coast, provides access to Las Catalinas, Playa Coco and Bat Islands which are key dive areas. One of the big draws to this area is the likelihood of seeing giant manta rays over the winter months. The southern area is a protected biological reserve and offers the chance to see rays, turtles, and white-yip reef sharks. Plus, you’ll see large schools of fish swimming overhead on almost every dive. You can also get out to Cocos Island off the Pacific coast to dive with its hammerhead shark schools, whale sharks and manta rays. This requires some serious commitment as you need to get onto a live-aboard to make this happen and dig deep into those pockets.\nCatalinas Islands – The unique underwater structure is what draws divers to these islands. The area boasts huge volcanic rock formations within everybody’s recreational limits, with the opportunity to see white-tip reef sharks, devil rays, eagle rays and seasonally the giant manta Ray and humpback whale. In between those treats, the rommon Latin grunts and big eye jacks will surround you. Bat Islands – What better way to get your heart racing then by diving with bull sharks? You’ll also have the chance to spot other large pelagics like manta rays, sailfish and the occasional whale or dolphin. This dive site drops to more than 30 meters/100 feet and can have a strong current. It is best suited for advanced divers.\nCaño Island – Expect to find a large number of reef fish species on the pinnacles and coral reef that surround the island. You’ll also likely see bigger creatures such as manta rays, free-swimming moray eels and sharks. Mammals, such as orcas, dolphins, humpback and pilot whales are known to cruise through.\nCocos Island – Because reaching Cocos Island National Marine Park requires a lengthy boat trip, most divers visit on a live-aboard dive vessel. Besides the pelagic species such as marlin, sailfish, rays and sharks that are drawn to the area, you’ll find more than 25 endemic fish species, including the red-lipped batfish. Whales also use the Cocos Island as a place to congregate and calve.\nDepending on the area you dive, visibility usually ranges from 10-40 meters/30-120 feet. May through September generally brings the best visibility with blue, clean and warm conditions. This is by far the best time of year to dive as cooler less inviting waters occupy the region from October to April.\nWater temperatures generally range from 26-29ºC/78- 84ºF. From December through March, cooler currents bring water temperatures as low as 19ºC/68ºF along with large animals such as humpback whales, pilot whales, giant manta rays, whale sharks, and more.\nDiving in Costa Rica for the most part is good all year. Seasoned divers will appreciate that you cannot have perfect conditions every day and like just about anywhere in the world, Costa Rica’s Pacific Coast at times can be home to poor visibility and cooler that preferred water temperatures.\nIn these waters you’ll find eagle and devil rays, turtles, manta rays, white-tip reef sharks, bull sharks, enormous schools of both grunts and jacks, parrotfish, moray eels, octopus, dolphin and more. During whale season (July to September) you can hear the haunting songs of humpback and pilot whales underwater.\nMost dive centers offer 2 dives in the morning wrapping up their activities by early afternoon. You can find some operators that are willing to make 3 dives in a day, but they are not common. Diving with nitrox is still in its infancy in Costa Rica and you will find only a few centers with nitrox or other mixed gas options.\nIn the still developing diving Industry in Costa Rica there is an inconsistency in standards from one dive center to another. If you are a certified diver or one looking to dive for the first time you need to follow a simple rule of thumb: Look for a PADI-certified dive center. This should signal that minimum diving industry standards are followed and the diving equipment and staff should be reliable.\nThere are still quite a few uncertified centers in Costa Rica, so steer clear of those. Also, there is plethora of street vendors and re-sellers in the touristic regions of Costa Rica. These vendors will sell you an experience provided by a dive center to pocket a little commission. This means that you could get passed from pillar to post before you end up at your dive center and you never really know who the actual service provider is until the day of your dive. The best thing to do is always deal directly with the dive center and you will get exactly what you want.\nThe good news is that the diving industry is growing in Costa Rica and more PADI Centers are opening each year. Those that have been around for a few years are becoming 5-Star and PADI Instructor Development Centers.\nSo, whether you are a certified diver, looking to try this sport for the first time or pursuing further diving education and even thinking of becoming a diving professional, Costa Rica has it all.\nHere are a few premier dive centers in Costa rica\nScuba Dive Costa Rica is located in Playa Flamingo and an office in Tamarindo.\nSDCR is a PADI Dive Center and dive daily to Las Catalinas Islands which is home to some of the best diving in the area. SDCR offers range of PADI courses including Discover Scuba experiences for those without a certification. SDCR has all levels of training up to instructor.\nContact Scuba Dive Costa Rica:\nTel: +506 8602 6581\nAddress: Local #B2, Flamingo Plaza, Playa Flamingo\nTel: +506 8602 6581\nAddress: Local #B8, Plaza Conchal, Tamarindo\nOceans Unlimited is a 5 star PADI IDC and technical dive center located on the central pacific coast of Costa Rica in Manuel Antonio. They have a new shop location in the Marina Pez Vela Quepos. For visiting scuba divers they offer local dive trips around the Manuel Antonio National Park as well as full day trips to Caño Island. Manuel Antonio Park is the location for a half day dive tour: offering various dive sites for all levels of divers . They offer a comprehensive selection of scuba dive training in Costa Rica specializing in professional courses including divemaster and instructor programs, as well as full resort training and technical dive programs. They also offer a first time jump in with a Discover Scuba Dive experience.\nContact Oceans Unlimited:\nTel: +506 2519 9544\nAddress: Local #209, Marina Pez Vela, Provincia de Puntarenas, Quepos"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_formulation","category_name":"search_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:e39b664a-85ab-4fba-a0a4-7cfc8ac05f58>"],"error":null}
{"question":"How does performance compare between in-hull and thru-hull transducers for sonar equipment, and what scientific research vessels like NOAA ship Pisces use for their hydroacoustic studies? 🤔","answer":"Thru-hull transducers generally provide better performance than in-hull transducers. In-hull transducers experience considerable loss of sonar performance, resulting in reduced depth reading and fish detection capabilities. The NOAA ship Pisces is equipped with an advanced multi-beam acoustic system located on the center board beneath the ship. This system is one of only six such multi-beam acoustic instruments in the world, with NOAA owning five of them. The benefit of their multi-beam instrument is that each beam can be set to measure different frequencies (kHz), enabling detection of many more features and different species of fish.","context":["NOAA Teacher at Sea\nAboard the NOAA ship Pisces\nJune 16 – June 29, 2012\nMission: SEAMAP Caribbean Reef Fish Survey\nGeographical area of cruise: St. Croix, U.S. Virgin Islands\nDate: June 22, 2012\nWeather Data from the Bridge:\nAir Temperature: 28.6°C (83.5°F)\nWind Speed: 9 knots (10.5 mph), Beaufort scale: 3\nWind Direction: from SE\nRelative Humidity: 77%\nBarometric Pressure: 1,014.80 mb\nSurface Water Temperature: 28.1°C (82.6°F)\nScience and Technology Log\nAnother aspect (much more technical) of the scientific research conducted on this cruise is the collection of acoustic data. This field is continually evolving as the detection resolution improves allowing scientists to more precisely identify fish. This has been used with more success in fisheries farther north because the schools of fish are more likely to be monospecific (a single species). However, the technique still needs improvement in warmer waters where the fish assemblages tend to be multi-specific (having a much greater variety of fish).\nThis field of study is called Hydroacoustics (hydro- means water, and acoustics refers to sound). It is the science of how sound moves through water. Leonardo da Vinci noticed how sound travels through water in 1490. He noticed that, “If you cause your ship to stop and place the head of a long tube in the water and place the outer extremity to your ear, you will hear ships at a great distance from you.” (Urick, Robert J. Principles of Underwater Sound, 3rd Edition. New York. McGraw-Hill, 1983.) World War I helped promote innovation in the field, especially with the need for anti-submarine detection devices (Wood, A. B., From the Board of Invention and Research to the Royal Naval Scientific Service, Journal of the Royal Naval Scientific Service Vol 20, No 4, pp 1-100 (185-284)).\nHydroacoustic instruments utilize SOund Navigation and Ranging, more commonly referred to as SONAR. The ship Pisces is equipped with a system located on the center board; this is a flat structure that can be raised/lowered through the water column beneath the center of the ship.\nThe system used is a sonar beam that is split into quadrants. This instrument is used to assist in determining fish abundance and distribution. The premise is relatively simple: an echo sounder transmits a pulse of energy waves (sound), when the pulse strikes an object, it is reflected (bounced) back to the transducer. The echo sounder is then processed and sent to a video display. This is the same general process behind the recreationally available fishfinder.\n- A short burst of energy is focused into a narrow beam. When this beam encounters an object such as a fish, a school of fish, plankton, or other object, some of the energy bounces back up through the water to the transducer. It is the detection of these reflections that allow scientists to determine location, size, and abundance of fish. These reflections show up on our video monitor. These measurements are combined with groundtruthed data (for example, fish collected in the field, camera images).\nOne of the difficulties in data interpretation is that often, the signals that appear on the computer monitor have false readings. This is a result of the sound wave bouncing multiple times. It travels to the bottom from the transducer, strikes an object, returns to the ship, bounces off the ship back toward the bottom, strikes another object, and is detected yet again.\nThe Pisces is actually home to one of six multi-beam acoustic instruments in the world. Of the six in existence, NOAA has five of them. The benefit of running a multi-beam instrument is that each beam can be set to measure a different frequency (kHz), thus enabling detection of many more features (different species of fish, etc.)\nLast night the crew of the Pisces carried out a task that they don’t normally perform. The Pisces was created for fisheries research projects – it focuses on collecting fish samples either by bandit reel, longline, or trawling. This particular operation was to deploy the anchor for a buoy that will be attached at a later date. When the buoy is ready to be attached, another vessel will bring it out to the site and divers will go down to the anchor to make the final attachment.\nThe anchor consists of a huge rebar-reinforced concrete block with a very long chain that has marker floats attached at the end. Logistically, this took some planning; the A-frame had to be raised and the anchor lifted with the Gilson winch with a 1″ spectra line (has an enormous tensile strength). The gate to the ship’s ramp was lowered and the A-frame (or as the deck hands call it, the “Tuna Tower”) repositioned so the anchor was hanging over the water. The rope holding the anchor, chain, and float was cut through, and the anchor plunged to the ocean bottom. Again, the crew made the operation go smoothly and demonstrated their ability to complete unexpectedly assigned tasks.\nToday was a slow fishing day – no fish at all. Without any fish to “work up” (collect samples from), the day goes more slowly and we have more down time. With the extra time, I had a chance to interview Kevin Rademacher, the Chief Scientist on the cruise.\nLU: What is your official job title and what are your job duties?\nKR: I’m a Research Fisheries Biologist. I work for the Reef Fish Unit at the NOAA Fisheries Lab in Pascagoula, MS. I am the Senior Tape Reader/Reviewer, in charge of the readers that analyze the video data we collect from Reef Fish Surveys. I also help plan, organize, and run the surveys. Additionally, I participate in trawl surveys and anything else the lab needs done.\nLU: When did you first become interested in the ocean and marine sciences?\nKR: I guess that would have been when I was really young. There is a photo from the Panama City, Florida newspaper, two weeks after I was born with my parents pulling me in a homemade wagon along the beach! I knew in junior high school that I wanted to be a cross between Jacques Cousteau and Marlin Perkins of Mutual of Omaha’s Wild Kingdom.\nLU: It’s such a broad field; how did you narrow your focus down to what you’re currently doing?\nKR: I got lucky and kind of fell into reading underwater videos at the initial stages of the project and fell in love with being the proverbial “fly on the wall”! It has allowed me to see the fish in their natural habitat, different color phases, behavior, etc.\nLU: If you were to go into another area of ocean research, what would it be?\nKR: Marine Mammal Studies. After college I trained dolphins and sea lions and put on shows with them for a local Oceanarium on the Mississippi Gulf Coast.\nLU: What is the biggest challenge in your job?\nKR: Communicating with people and writing papers.\nLU: What do you think is the biggest issue of contention in your field?\nKR: The impression that commercial fishermen have regarding the work we do to regulate the fisheries they work in.\nLU: What are some effects of climate change that you’ve witnessed during your career in fisheries research?\nKR: The decline of coral reefs and overfishing of some species.\nLU: In what areas of marine science do you foresee a lot of career paths and job opportunities?\nKR: Ecosystem management and data modelers. There has also been a decline in taxonomists over the past few decades.\nLU: How would you explain your work to a layperson?\nKR: I use underwater cameras to help assess populations of reef fish, especially snappers and groupers. The data collected is used to manage those fisheries.\nLU: If a high school student wanted to go into your field of study/marine science in general, what kinds of courses would you recommend they take?\nKR: Math, Biology, Chemistry, and any other science courses available.\nLU: Do you recommend students interested in your field pursue original research as high school students or undergraduates? If so, what kind?\nKR: Most definitely! Whatever they are interested in would be beneficial.\nWell, only two more days left with the scientists before we pull into San Juan, Puerto Rico. We have 17 more daytime sites to sample and then this survey will be over. The scientific crew will be flying home on the 25th, and once home, their work will really begin. Back in the lab, they will be analyzing the data and reviewing the video. Some of them will be going back out on other cruises. Kevin Rademacher will be going out on another reef fish survey in the eastern Gulf of Mexico. It is currently delayed because of the potential formation of tropical storm Debby. Joey Salisbury has a couple more; he will be going on a longline cruise and then another reef fish survey, both of which will be in the Gulf of Mexico. Arian Frappier will be heading off to begin a masters program in marine systems and coastal studies at Texas A&M Corpus Christi.\nAfter a day’s shore leave in San Juan, I’ll continue on to Mayport on the Pisces. During this time, I’ll focus on the crew members and their jobs. The cruise will definitely take on a different feel at this point, but it will give me an opportunity to explore other ocean related careers.","00 - Echosounder Transducer Selection Guide - 00\nFactors to consider when selecting a transducer\nWhen trying to determine which transducer is best for you, youll need to consider the following variables:\n- What material the transducer housing is made of, based on boat hull composition\n- How the transducer should be mounted on the boat: in - the hull, through the hull, or on the transom\n- What you want to see displayed: depth, speed, temperature, or a combination\n- Determine what material the transducer should be composed of.\nPlastic housings are recommended for fiberglass or metal hulls.\nBronze housings are recommended for fiberglass or wood hulls. Bronze is preferable to plastic for wooden hulls because the expansion of wood could damage a plastic transducer and cause a leak. Installation of a bronze housing in a metal hull requires an insulating fairing. A metal housing should NOT be installed in a vessel with a positive ground system.\nStainless Steel housings are recommended for steel or aluminum hulls.\nDetermine the type of transducer mounting.\nIn-hull (a.k.a. shoot-through) transducers are epoxied directly to the inside of the hull. These are only used in fiberglass hulls. In-hulls will not work with wooden aluminum, wood, or steel hulls, or in foam sandwich or hulls that have air pockets. Any wood, metal, or foam reinforcement must be removed from the inside of the hull.\nWith an in-hull transducer, the signal is transmitted and received through the hull of the boat. As a result, there is considerable loss of sonar performance. In other words, you wont be able to read as deep or detect fish as well with an in-hull transducer as with one thats transom mounted or thru-hull mounted.\nThru-hull transducers are mounted through a hole drilled in the bottom of the boat and protrude directly into the water. This type of transducer generally provides the best performance.\nThru-hulls are recommended for displacement hulls and boats with straight-shaft inboard engines. Youll also need a fairing block that allows the transducer to be mounted properly. Thru-hull transducers must be installed with a fairing to ensure proper alignment and a secure fit.\nTransom Mount Transducers\nAs the name implies, transom mount transducers are installed on the boats transom, directly in the water and typically sticking a little below the hull. Transom mounts are composed of plastic and tend to be less expensive than other transducers.\nTransom mount transducers are recommended for planing hulls of less than 27 feet (8 meters), such as personal watercraft and powerboats with outboard, inboard-outboard and jet drives. They are not recommended for large or twin screw inboard boats because aerated water from the propeller reduces performance. They are also not recommended for operation at very high speeds.\nPower refers to the strength with which the transducer sends the sonar â€œpingâ€, expressed as watts RMS. Higher power increases your chances of getting a return echo in deep water or poor water conditions. It also lets you see better detail, such as bait fish and structure. Generally, the more power you have, the deeper you can reach and the easier it is to separate echoes returning from fish and bottom structure from all the other noises the transducer detects.\nThe accuracy with which your fishfinder detects bottom and other objects is also determined by the frequency selected for the depth you are viewing. Raymarine depth transducers can be tuned to two different frequencies: 50 kHz (low) or 200 kHz (high).\n200 kHz works best in water under 200 feet/60 meters and when you need to get an accurate reading while moving at faster speeds. High frequencies give you greater detail to detect very small objects but over a smaller portion of water. High frequencies typically show less noise and fewer undesired echoes while showing better target definition.\nFor deep water, 50 kHz is preferred. This is because water absorbs sound waves at a slower rate for low frequencies and the signal can travel farther before becoming too weak to use. The beam angle is wider at low frequencies, meaning the outgoing pulse is spread out more and is better suited for viewing a larger area under the boat. However, this also means less target definition and separation and increased susceptibility to noise. Although low frequencies can see deeper, they may not give you a clear picture of the bottom.\nA rule of thumb would be to use the 200 kHz setting for a detailed view to about 200 feet and then switch to 50 kHz when you want to look deeper. Better yet, display both views side-by-side on a split screen for both perspectives.\nThe transducer concentrates the transmitted sound into a beam. In theory, the emitted pulse radiates out like a cone, widening as it travels deeper.\nFor the scope of this discussion, however, the idea of a cone works just fine. The signal is strongest along the centerline of the cone and gradually diminishes as you move away from the center. Wider angles offer a larger view of the bottom, yet sacrifice resolution, since it spreads out the transmitters power. The narrower cone concentrates the transmitters power into a smaller viewable area. Cone angles are wider at low frequencies and narrower at high frequencies.\nTo sum up, a wide cone angle can detect fish around the boat and not just those directly under it while exhibiting less target separation. A narrow cone concentrates the sound output enabling it to better detect small details, such as fish or bottom structure, but only scans a small amount of water at a time.\nThrough Hull Mid Performance\nB117 Depth/Temp (opt), 50/200 kHz, 600 W, Bronze\nP319 Depth/Temp (opt) 50/200 kHz, 600 W, Plastic\nB744V Depth/Speed/Temp, 50/200 kHz, 600 W, Bronze\nP79 Depth, 200 kHz, 600 W, Plastic\nP58 Depth/Speed/Temp, 50/200 kHz, 600 W, Plastic"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:e7556c60-ecbc-4e11-9af6-7de59f29dd02>","<urn:uuid:a5cc1b79-4a06-47f2-9718-ef051ce3f031>"],"error":null}
{"question":"As a property developer focused on sustainable investments, how can pro forma analysis help evaluate green roof installations, and what are the key risks to consider in such projects?","answer":"Pro forma analysis helps evaluate green roof installations by calculating financial results and determining property worth through projected income and expenses. For green roofs specifically, you would need to include maintenance costs (typically 1% of property value annually), potential repair expenses for issues like water leakage or drainage systems, and any additional management fees. The key risks to consider include: potential water damage from irrigation pipework freezing or membrane leaks, increased fire load from combustible components, structural concerns due to added live loads from landscaping and precipitation, and vulnerability to natural hazards like wind uplift pressures. These risks need to be factored into the pro forma calculations to ensure the investment's profitability and long-term sustainability.","context":["There is a lot more to buying rental property in real estate investing. It’s not always a straightforward investment. It would be best if you had pro forma calculations to ensure the profitability of a property before purchasing.\nA carefully-crafted pro forma statement is one of the essential tools for investors looking to buy property. There’s no way to predict the future. Still, when deciding whether to invest in a property, you need to come up with “what if” scenarios to ensure you’re not flushing money down the drain on nonprofitable investments.\nLet’s look at the pro forma definition real estate-wise.\nWhat is pro forma in real estate?\nThe term “pro forma” has a Latin origin which literally means “as a matter of form” or “for the sake of form.” In real estate, pro forma calculates financial results and summarizes a property’s financial information.\nA pro forma statement is a financial report based on assumptions and projections for your business. It focuses on the costs and cash flow. It shows lists of the actual income, expenses, and estimated income or a property’s projected net operating income.\nIn simple terms, pro forma analysis can help buyers and sellers better understand what a property is worth and determine if a property is worth purchasing or not.\nPro forma in real estate and how to calculate it\nA pro forma in real estate will detail your property’s cash flow and projected net operating income (NOI). There are several items to consider in a pro forma document, including the following:\nGross rental income (GRI). GRI refers to the potential revenue your property will bring in from rental payments if all the units are occupied before any expenses are accounted for.\nMaintenance and Repair expenses. Estimate how much you would spend on repairs to the property each year. Real estate investors typically follow the one percent rule or one percent of the property value yearly. For example, if your property is valued at $300,000, repairs and maintenance costs will be around $3,000 per year.\nVacancy rate. Vacancy rate refers to how many units are vacant and unoccupied at any time.\nLoan fees and mortgage payments. If you took out a loan to buy a property, you’d be paying a fixed monthly mortgage. This expense is included in the pro forma calculations. In most cases, investors include the mortgage payments in the rental cost for possible tenants.\nFor example, if your monthly mortgage is $900, your rental rate should be more than that. Suppose the rental rate in the area is lower than your monthly mortgage. In that case, it may be time to look for other possibly lucrative properties to invest in.\nManagement fees. If you have only one property, chances are you might want to manage the property yourself. You might want to hire a property manager if you have several properties. Consider the costs and fees involved in management fees in your pro forma calculations. If you manage the property, set aside a specific percentage cost to compensate yourself.\nMiscellaneous expenses. Expenses paid for insurance costs, leasing, tax fees, legal fees, HOA dues, and other fees should also be included in the pro forma document.\nThe general formula real estate investors use is the following:\nPro forma NOI = GRI – Vacancy expenses – all other expenses.\nTo calculate the Vacancy expenses, multiply the vacancy rate by the GRI.\nTypes of pro forma statements\nA financial statement is a pro forma example that includes three primary forms:\n- Income statementsshow a company’s earnings, costs, and expenses over a specific period.\n- Balance sheets- a report that provides information about a company’s assets, liabilities, and capital or net worth.\n- Cash flow statements– report a company’s cash inflow and outflow.\nIn a nutshell, the financial statements show how a company’s financial position may change in the future. Pro forma financial statements will help you make a business plan, create a financial forecast, and get funding from potential lenders or investors. Pro forma documents are just estimates and assumptions\nAlthough everything is based on estimates and projections, a pro forma example document is vital to give you the complete picture of your risks and returns regarding your property. A pro forma document is a helpful tool to help you find safe, profitable investments in the competitive world of real estate.","Sustainability Series: Green Roofs\nArticle information and share options\nWith increased urbanization, green roofs are becoming a major trend. They offer many advantages to building owners and occupiers, the general public, and the environment: better insulation, reduced power consumption, stormwater retention, improved air quality, and a biodiverse environment offering aesthetic diversity. However, green roofs may introduce property considerations and concerns that owners and occupiers should be aware of when reviewing their property and business risks.\nGreen roofs comprise specific vegetation designed and installed on a series of components, including moisture retention material, a drainage system, a root barrier, and a protective layer for both the insulation and underlying roof surface.\nProperty Risk Considerations\nLet's take a closer look at a few features that will likely be of interest to commercial property insurers when evaluating the building's overall risk profile.\nDownload our guide\n1. Potential for water leakage damage\nThe irrigation pipework may be susceptible to freezing. Leaks in waterproofing membranes due to root growth or temperature fluctuations may expose the underlying roof structure to damage or corrosion. Drainage systems may become blocked by soil and vegetation. These risks are particularly relevant in the construction phase.\nLoss Prevention Tips: Penetrations in the roof should be minimized. It is important to include leak detection systems below the waterproofing membrane. Size gutters to accommodate both rainfall and irrigation runoff and maintain them regularly, with specific inspection points included. Consider the impact of roof gradient on the density of growth media and its propensity to shift or slide during heavy rain events.\n2. Fire load\nGreen roofs and their supporting components (e.g., vegetation and waterproofing membrane) typically add combustible loading to a roof and increase the potential for ignition from exposing fires or other ignition sources. In some cases, the design of a green roof may encompass space for recreational activities, introducing ignition sources such as lighting, electrical installations, barbecues/grills, and smoking.\nLoss Prevention Tips: Limit the overall fire risk by maintaining adequate moisture content of vegetation, regular removal of dead vegetation, and careful design and placement of noncombustible fire breaks. Include manual firefighting equipment such as fire hydrants, fire hose reels, and portable fire extinguishers, along with access provisions for the fire brigade. Potential ignition from electrical equipment, smoking, or other sources should be identified and carefully managed.\n3. Susceptibility to collapse\nGreen roofs introduce live loads associated with landscaping, precipitation, induced saturation due to irrigation, and periodic replacement of new soil and growth of vegetation.\nLoss Prevention Tips: The load carrying ability of concrete roofs versus all other types (e.g., long span steel) should be considered over the roof lifecycle. Some roofs may be susceptible to deformation over time, affecting the operational efficiency of certain components that originally functioned properly. Future changes may affect the structural integrity and should undergo formal review. Concrete roofs are generally more resilient than other roofs to changes that increase live loading.\n4. Damage from natural hazards\nGreen roofs are susceptible to natural hazard perils such as seismic forces and wind uplift pressures. Vegetation or potentially the entire roof system may be damaged, requiring replacement.\nLoss Prevention Tip: The system, including any moisture/root barriers, should be properly secured to structural elements (growth media should not be relied upon). Green roofs should not generally be installed in areas with elevated wind exposures such as coastal areas subject to hurricanes.\nGreen Roofs provide many environmental benefits, especially in urban locations where vegetation may be scarce. However, if a green roof is to be installed, ensure that the considerations outlined above are incorporated in the design, installation, and future maintenance requirements to mitigate the potential for loss."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"format_constrained"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:82113bff-5942-4c82-ad0b-17ca49830336>","<urn:uuid:adefe97e-66fa-495a-b3a2-8e24a422fce1>"],"error":null}
{"question":"As a researcher studying recent advances in genetic screening technologies, I'm particularly interested in how genome-wide screens can be conducted. What are the latest developments in using CRISPR-Cas9 versus pooled shRNA libraries for large-scale genetic screens, especially in terms of their applications and methodologies?","answer":"Both CRISPR-Cas9 and pooled shRNA libraries offer distinct approaches to genome-wide screening. CRISPR-Cas9 system has emerged as a powerful tool for genome engineering, allowing researchers to perform genome-wide gene editing that is precise, scalable, and robust. It enables researchers to elucidate the functional organization of the genome at the systems level and establish causal linkages between genetic variations and biological phenotypes. Recent CRISPR screens on cancer cell lines have demonstrated their potential to address cancer. For pooled shRNA libraries, they are used for rapid phenotypic screens where cells are typically transduced with a single shRNA. These screens, such as the Mission LentiPlex libraries, contain approximately 8,000 shRNA constructs per subpool and are particularly useful for positive selection screens where cell viability/survival is the phenotype of interest. After selection, integrated shRNAs are identified through sequencing methods to develop hypotheses about gene function.","context":["The prokaryotic type II clustered regularly interspaced shortpalindromic repeats (CRISPR)-Cas9 system is rapidly revolutionizing the fieldof genetic engineering, allowing researchers to alter the genomes of a largevariety of organisms with relative ease.\nExperimental approaches based on thisversatile technology have the potential to transform the field of cancergenetics 1. This Cas9 endonuclease is the central component of the TypeII CRISPR/Cas system, a prokaryotic adaptive restriction system againstinvading nucleic acids, such as those originating from bacteriophages andplasmids. Recently, this RNA-directed DNA endonuclease has been harnessed totarget DNA sequences of interest. Through guidance of a 20nucleotide RNA (gRNA), CRISPR-Cas9 finds and cuts target protospacer DNAprecisely 3 base pairs upstream of a PAM (Protospacer Adjacent Motif) 2. Cas9 is as an important tool to not only edit thegenomes of a number of different prokaryotic and eukaryotic species, but alsoas an efficient system for site-specific transcriptional repression oractivation 3 it couldalso be used to modify any genomic sequences, thereby providing a specific, simple,easy, and cost effective means of genome wide gene editing, analogous to thesearch function in modern word processors, Cas9 can be guided to specificlocations within complex genomes by a short RNA search string. Using thissystem, DNA sequences within the endogenous genome and their functional outputsare now easily edited or modulated in virtually any organism of choice.Cas9-mediated genetic perturbation is simple and scalable, empoweringresearchers to elucidate the functional organization of the genome at thesystems level and establish causal linkages between genetic variations andbiological phenotypes 4 it is also aflexible, RNA-guided DNA recognition platform, which enables precise, scalable androbust RNA-guided transcription regulation 5, in contrast tothis, RNA-mediated interference (RNAi), which uses small interfering RNAs(siRNAs) or short hairpin RNAs (shRNAs), has also been used for sequence-specific gene suppression in eukaryoticorganisms 6 but it is non-specificand inefficient 7.\nGenome engineering via the RNA-guided CRISPR-Cas9system provides a novel methodology, allowing induction of genomicmodifications under the endogenous gene promoters 8. During the last few years, the clustered regularlyinterspaced short palindromic repeats (CRISPR) and the associated Cas9nucleases (CRISPR-Cas9) have revolutionized the options for targeted genomeediting 9. These programmable RNA-guidedendonucleases(RGENs)comprise two RNA elements, CRISPR RNA (cRNA) and its transactivating RNA(tracRNA), which can be fused together and used to induce a targeteddouble-strand break (DSB). Providing a corresponding DNA template, any specificgene sequence can be introduced via homologous recombination (HR) 10. CRISPR/Cas is a microbial adaptive immune system that usesRNA-guided nucleases to cleave foreign genetic elements, uses a single-guide RNAto target the Cas9 nuclease to a specific genomic sequence. Cas9 inducesdouble-stranded DNA breaks which are repaired either by imperfectnon-homologous end joining to generate insertions or deletions (indels) or, ifa repair template is provided, by homology-directed repair.\nDue to itsspecificity, simplicity and versatility, the CRISPR/Cas9 system has recentlyemerged as a powerful tool for genome engineering in various species 11. Recent reports on CRISPRscreenson several cancercell lines have demonstrated their power to cure cancer 12. A human malignancy in urgent need of additional therapiesis acute myeloid leukemia (AML), a devastating disorder with a long-termsurvival rate of less than 30% (Ferrara andSchiffer, 2013). Steady progress in deciphering its molecularpathogenesis has been made over the last few decades with a dramaticacceleration in recent years, particularly as a consequence of advances incancer genomics (Cancer Genome Atlas Research Network, 2013;Welch et al.","RNA interference is an effective mechanism for gene silencing whereby double-stranded RNA triggers the cleavage and subsequent degradation of homologous transcript sequences. In this evolutionarily conserved process, longer double-stranded RNA molecules are processed into shorter sequences (21–23 nucleotide small interfering RNAs, or siRNAs) that can bind to the multicomponent RNA-induced silencing complex (RISC).\nWithin this complex, the siRNA is unwound, and the sense strand is cleaved and dissociated. The antisense strand then remains bound and acts as a guide to target activated RISC to complementary mRNA for cleavage and degradation.\nThis potent, sequence-specific RNA degradation mechanism was first discovered in plants, where it was termed post-transcriptional gene silencing. It has since been demonstrated in a wide variety of eukaryotic organisms, ranging from fission yeast to humans. These discoveries and subsequent studies into how the RNAi process works have enabled researchers to exploit this pathway and develop tools in order to better elucidate gene function.\nPlasmid-based expression of gene-specific small hairpin RNAs (shRNA) under the control of RNA polymerase III–dependent promoters is an effective way to trigger this process. With this approach, the shRNAs are processed intracellularly by the enzyme Dicer into siRNAs, which are then able to directly engage RISC.\nThe Mission TRC1 shRNA libraries from Sigma-Aldrich consist of over 150,000 such plasmid-based shRNA constructs targeting 15,000+ human and 15,000+ mouse genes. The shRNA sequences are designed using an algorithm developed by the Broad Institute of MIT and Harvard.\nOn average, there are five shRNA designs for each gene target. The shRNA plasmids are further processed into lentiviral particles to facilitate stable gene silencing in both dividing and quiescent cells. The LentiPlex libraries were generated from the RNAi Consortium’s TRC1 library and are intended for rapid, whole-genome, pooled RNAi screening projects.\nWhile individual genes can be efficiently and robustly targeted using arrayed lentiviral libraries, pooled shRNA libraries may be used to rapidly conduct many phenotypic screens. These pooled screens are typically set up such that the majority of cells have been transduced with a single shRNA to aid in downstream deconvolution.\nPositive selection screens are an example of the type of screen that may be conducted using pooled libraries. Selection of a desired phenotype is the basis of this type of screen where cell viability/survival is a commonly used phenotype. After selection, integrated shRNAs are identified by traditional Sanger or deep sequencing methods. The identity of potential hits can then be used to develop hypotheses regarding the biological role of the corresponding gene(s). As with all screens, validation of leads by independent methods such as siRNA, small molecule inhibition, or gene knockout will be required.\nThe Mission LentiPlex Human Pooled shRNA Library and the Mission LentiPlex Mouse Pooled shRNA Library are genome-wide lentiviral pools. Representation of individual shRNAs from each library is tested before product release to ensure robust library coverage. The actual number of clones in each subpool may vary. Each library is provided as 2 x ~25 µL aliquots of ready-to-use lentiviral format at titers of at least 5 x 108 TU/mL via p24 assay and is predivided into ten subpools of approximately 8,000 shRNA constructs each. Amplification and sequencing primers are also provided for downstream hit identification."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:5816de1f-eec1-4306-813d-8aef6fc91f21>","<urn:uuid:0bfe22e7-8b4b-48c0-a98f-46bb0eb19038>"],"error":null}
{"question":"What are the recent changes in maritime law regarding manufacturers' duty to warn about asbestos hazards, and how does this specifically impact veterans seeking compensation for asbestos-related diseases?","answer":"The Supreme Court recently established that manufacturers have a duty to warn when their products require the incorporation of hazardous materials like asbestos, even if they don't incorporate those materials themselves. This ruling was influenced by maritime law's special consideration for sailors' welfare. Manufacturers must now warn sailors when they know a danger exists or will exist due to their product's intended use. This change particularly impacts veterans, who make up approximately 30% of all U.S. mesothelioma cases. Veterans can now seek compensation both through VA benefits and from the manufacturers who failed to warn about asbestos dangers, with Navy veterans being at highest risk due to extensive asbestos use in ships and shipyards from the 1930s to the 1990s.","context":["The Supreme Court recently decided a very narrow issue related to maritime tort law, though widely affecting marine manufacturers, guided in part by a maritime principle of “Solicitude for Sailors.” The Supreme Court charted new waters (sorry) by finding that a manufacturer has a duty to warn when it sells a product that, by its very nature, requires the incorporation of hazardous materials, even if it does not incorporate those hazardous materials itself. In doing so, the Court noted that “[m]aritime law has always recognized a ‘special solicitude for the welfare’ of those who undertake to ‘venture upon hazardous and unpredictable sea voyages.’” Air and Liquid Systems Corp. v. Devries, 586 U.S. at __, 139 S. Ct. at 995 (citing American Export Lines, Inc. v. Alvez, 446 U. S. 274, 285 (1980)).\nThe plaintiff-respondents in the case were retired United States Navy sailors, who worked for long periods of time on Navy ships that contained asbestos-containing equipment. The manufactured equipment required asbestos insulation or asbestos parts to function, though the manufacturer did not always incorporate the asbestos into those products itself. When the manufacturer did not, the Navy later added the required asbestos to the equipment.\nThe plaintiff-respondents sued the manufacturer of the equipment for negligence in failing to warn of the asbestos danger in the integrated products. Plaintiff-respondents argued a warning would have led sailors to wear asbestos-protective masks to prevent their eventual cancer and other maladies resulting from that asbestos exposure.\nThe manufacturer defended against the claim, arguing the “bare-metal defense,” which holds that manufacturers should not be held liable for harms caused by the later incorporation of harmful products by other entities. For those parts that were created by them, in which asbestos was later incorporated by other entities, the manufacturer claimed it could not be found liable. The District Court agreed, granting summary judgment to the manufacturers. However, the Third Circuit vacated and remanded the case, adopting a foreseeability approach, finding that a manufacturer would be found liable when it was foreseeable that the manufacturer’s product would be used with another product or part, even if the manufacturer’s product did not require use or incorporation of that other product or part.\nThe Supreme Court articulated a novel middle-ground foreseeability approach, finding that a manufacturer may be found liable for failing to warn when the manufacturer knows that its bare metal equipment requires the incorporation of a hazardous element for its intended uses. In the instance of selling parts that required asbestos, a duty to warn would arise because the manufacturer knew a dangerous element would need to be added to make the part functional.\nThe dissent disagreed, finding this middle-ground an unfounded deviation from common law: “[i]n deviating from the traditional common law rule, the Court may be motivated by the unfortunate facts of this particular case, where the sailors’ widows appear to have a limited prospect of recovery from the companies that supplied the asbestos (they’ve gone bankrupt) and from the Navy that allegedly directed the use of asbestos (it’s likely immune under our precedents).” Id. at ___, 139 S. Ct. at 1000 (Gorsuch dissent).\nThe dissent, unlike the majority, chose not to credit maritime law’s “‘special solicitude for the welfare’ of those who undertake to ‘venture upon hazardous and unpredictable sea voyages.’” Id. at ___, 139 S. Ct. at 995. In fact, the dissent acknowledges that the majority’s standard derives from this special solicitude, but nevertheless criticizes this departure from common law, even within this maritime context.\nAnd what is that context? Sailors and marines work, live, and serve on ships for the defense of the United States, unable to sue the United States for hazards visited upon them by the United States. When the dissent acknowledges those sailors are “likely immune under our precedents,” it alludes to the Feres doctrine, a doctrine that prevents servicemembers or their families from suing the United States government for torts committed upon them by the United States while serving with the United States armed forces. Feres v. United States, 340 U.S. 135 (1950). So, in this context, when a manufacturer sent “bare metal” equipment to the United States Navy, equipment which would require hazardous material be incorporated by the United States Navy for its intended use, the manufacturer could legally shift responsibility of a duty to warn to an entity against which the sailors cannot bring a lawsuit. This case puts an end to that shift, and changes the entire liability landscape for marine manufacturers and their suppliers.\nThe Court now requires manufacturers to warn sailors, when the manufacturer knows a danger exists or will exist by virtue of its product: “[a]nd the rule that we adopt here is tightly cabined. The rule does not require that manufacturers warn in cases of mere foreseeability. The rule requires that manufacturers warn only when their product requires a part in order for the integrated product to function as intended.” Air & Liquid Sys., 586 U.S. at ___, 139 S. Ct. at 995. One might expect Justice Kavanaugh, the author of the opinion, would dispute the dissent’s claim that this is a deviation of common law, and argue this is an evolution of the common law based upon the context and this special solicitude extended to our sea servicemembers.\nRegardless, manufacturers within the maritime sphere need to become aware that a new standard applies to their duty to warn. Wisconsin in particular has several well-respected manufacturers that specialize in marine equipment, not to mention their suppliers, which will all now have to reevaluate their processes and warnings. Further, though currently “cabined” within the maritime sphere, one might expect analogous arguments being made to all military manufacturers, with a special solicitude being extended to all of our other military services. Fighting in Iraq or Afghanistan certainly is no less hazardous or unpredictable than the sea. Manufacturers in this realm would be wise to review their policies as well.\nIf you have any questions regarding this article, are in need of assistance, or need further information on this decision, please contact your Davis|Kuelthau attorney, the author linked above or the related practice group chair linked here.","Veterans & Mesothelioma\nVeterans & Mesothelioma\nOn This Page\n- 1 Veterans & Mesothelioma\n- 1.0.1 Navy\n- 1.0.2 Air Force\n- 1.0.3 Coast Guard\n- 1.0.4 Army\n- 1.0.5 Marines\n- 1.0.6 Warning\n- 1.0.7 Aircraft Carriers\n- 1.0.8 Ammunition Ships\n- 1.0.9 Amphibious Warships\n- 1.0.10 Battleships\n- 1.0.11 Cruisers\n- 1.0.12 Destroyers\n- 1.0.13 Destroyer Escorts\n- 1.0.14 Escort Carriers\n- 1.0.15 Floating Dry Docks\n- 1.0.16 Frigates\n- 1.0.17 Minesweepers\n- 1.0.18 Submarines\n- 1.0.19 Tugboats\n- 1.1 VA Disability Benefits for Asbestos Diseases\n- 1.1.1 VA Health Care\n- 1.1.2 Disability Compensation\n- 1.1.3 Special Monthly Compensation (SMC)\n- 1.1.4 Dependency & Indemnity Compensation (DIC)\n- 1.1.5 Coping with Mesothelioma & Finding Support\n- 1.1.6 Priority care\n- 1.1.7 Cost of treatment\n- 1.1.8 Transportation and lodging during treatment\n- 1.1.9 Clinical Trials for Veterans\nVeterans who are diagnosed with mesothelioma as a result of active military service are entitled to benefits from the federal government.\nThey also may be entitled to additional compensation from the companies that sold dangerous asbestos products to the military.\nAsbestos in the Military\nVA Disability Benefits\nVA Claims Process\nTreatment for Veterans\nAsbestos in the Military\nDid you know that veterans have a higher incidence of mesothelioma and other asbestos-related diseases than the general population?\nIn fact, veterans make up approximately 30 percent of ALL U.S. cases of mesothelioma.\nThe military made widespread use of asbestos from the 1930s to the 1990s.\nAsbestos was used in everything from shipbuilding to the construction of barracks.\nEach branch of the military used asbestos.\nEvery naval ship constructed in this time period contained asbestos.\nThe companies that manufactured and sold asbestos products to the military knew of the dangers but failed to warn service members.\nDecades later, thousands of veterans have been diagnosed with debilitating and life-threatening asbestos diseases.\nAsbestos Exposure in All Branches of the Military\nAsbestos was widely used in ships and shipyards, aircraft and hangars, military buildings, and a variety of military equipment and vehicles.\nThe Department of Veterans Affairs has warned that anyone who served in the military could have been exposed to asbestos.\nAlthough the U.S. military has now taken precautions to protect personnel, those who serve overseas continue to risk exposure when old buildings are damaged or destroyed, releasing asbestos dust into the air.\nBecause of the extensive use of asbestos on ships and in shipyards, Navy veterans were the highest risk for exposure, particularly those who worked at shipbuilding facilities or in engine and boiler rooms.\nSailors frequently worked in tight quarters on ships, where they would unknowingly inhale microscopic asbestos fibers.\nThe Navy stopped using asbestos materials in the mid-1990s, but for many service members, the damage had already been done.\nUp until the 1980s, the U.S. Air Force used asbestos-containing components in planes, jets, helicopters, vehicles, and buildings.\nAircraft mechanics and other personnel assigned to work in aircraft hangars, airfields, or in other buildings containing asbestos may have been exposed to the deadly carcinogen.\nAlthough it’s not as large as the Navy, this branch of the military shares similar duties.\nLike veterans of the Navy, veterans of the Coast Guard were also likely exposed to asbestos while serving on ships and in shipyards.\nIn addition, even today, the Coast Guard has warned that asbestos materials remain on cutters.\nThe U.S. Army began its Installation Asbestos Management Program in the late 1990s to warn soldiers who might have been at risk of exposure and to implement asbestos abatement procedures.\nAt that time, the Army warned that asbestos materials had been used in a wide range of building materials and equipment.\nAny soldiers who handled or worked in the vicinity of asbestos-containing materials may have been exposed.\nIn particular, those who were assigned to work on vehicles could have been exposed through asbestos-containing brakes and clutches.\nBecause Marines worked on ships, with aircraft, and on land, service members risked exposure to asbestos on multiple fronts.\nMuch like Navy and Air Force personnel, Marines may have inhaled asbestos fibers while working on ships or aircraft. Marine Corps facilities on land also contained asbestos materials, which means anyone who worked to repair or maintain these buildings could have been exposed.\nOccupations with High Risk of Exposure to Asbestos\nVeterans who served in any of the following occupations faced the highest risk of exposure to asbestos:\nBoilerman, boiler tender, boilermaker, and water tender\nElectrician and electrician’s mate\nHull maintenance technician\nSonarman and sonar technician\nIt is important to note that veterans who served in high-risk occupations are not the only ones who have been diagnosed with asbestos diseases.\nThere is no safe level of asbestos exposure. Even those who did not work directly with asbestos-containing materials may have unknowingly inhaled the dust in their work environment.\nU.S. Navy Veterans Face Highest Risk for Asbestos Disease\nBecause Navy veterans faced the highest risk of asbestos exposure, they are more often diagnosed with asbestos diseases such as mesothelioma, lung cancer, and asbestosis.\nNavy veterans who worked at shipbuilding facilities and in engine and boiler rooms are most at risk.\nHowever, because airborne asbestos fibers can easily travel in close quarters, anyone who worked on a Navy ship or in a shipyard may be at risk for developing asbestos disease.\nNavy Ships & Shipyards\nAsbestos materials were common components in Navy ship equipment such as boilers, turbines, pumps, valves, and electrical equipment.\nThose who served on any of the following types of ships and watercraft may have been exposed:\nSecondhand Asbestos Exposure in Military Dependents\nIn addition to veterans who served on ships, those who worked in shipyards may have been exposed to asbestos while building, maintaining, or repairing vessels.\nCivilian Navy shipyard workers, sometimes called yardbirds, also were at high risk of exposure.\nAsbestos exposure in the military was not limited to service members who worked with or in the vicinity of the dangerous material.\nAsbestos dust and fibers were often transferred home on uniforms and clothing, putting military spouses and children at risk of exposure as well.\nVA Disability Benefits for Asbestos Diseases\nVeterans with service-connected mesothelioma, asbestos-related lung cancer, asbestosis, and other asbestos illnesses can receive monthly disability compensation from the Department of Veterans Affairs (VA).\nDepending on your circumstances, the VA offers a few types of benefits that you may qualify for:\nVA Health Care\nVA Health Care, for which eligibility is determined by service-related disabilities, income level, and various other circumstances.\nDisability Compensation, which is paid monthly and is based on your level of disability rating. Veterans are usually given the 100 percent VA disability rating for mesothelioma.\nIf eligible, single veterans can get about $3,000 in monthly benefits. Married veterans could collect more.\nSpecial Monthly Compensation (SMC)\nSpecial Monthly Compensation (SMC), which may be available to veterans who require the aid and attendance of a caregiver. (Family members such as spouses of veterans may also qualify to receive this benefit.)\nDependency & Indemnity Compensation (DIC)\nDependency and Indemnity Compensation (DIC), which is paid monthly to surviving spouses and dependents of veterans who have died from service-related disabilities.\nCoping with Mesothelioma & Finding Support\nLearning you or a loved one has mesothelioma can be extremely difficult to come to terms with.\nVA Claims Process\nTo qualify for VA benefits, a veteran must have been honorably discharged from the service and developed mesothelioma or another asbestos disease from his or her time on active duty.\nTo obtain benefits, a veteran must file the required VA form, along with supporting medical records, including a physician’s diagnosis of mesothelioma.\nBecause it can be complicated to complete the form and gather needed records to prove mesothelioma was caused by asbestos exposure during active service, consulting with an attorney experienced in pursuing mesothelioma disability claims is extremely helpful.\nAfter reviewing a patient’s complete record, the VA will make its benefits decision. The crucial factor is demonstrating to the VA that the asbestos exposure was primarily, if not exclusively, related to active military service.\nThe VA will assess the level of asbestos exposure during a veteran’s military service compared to any exposure during civilian occupations.\nOnce the VA determines a veteran is eligible for disability benefits, it then assesses the degree of disability on a scale starting at 10 percent. The VA typically gives 100 percent disability ratings to veterans with service-related mesothelioma.\nHelp with VA Claims and Other Benefits for Veterans\nIf you or a loved one was exposed to asbestos in the military and has been diagnosed with mesothelioma, contact us now for a free consultation with a VA-accredited claims representative. Our team can help you prepare the documentation needed and submit your application for VA benefits so you have the best chance of getting help from a top VA treatment center for mesothelioma.\nIn addition to VA benefits, veterans who have been diagnosed with an asbestos disease should learn about their rights to compensation from the manufacturers and suppliers that provided asbestos to the military. These companies knew about the dangers of airborne asbestos fibers, but they kept that information hidden for years.\nA claim against asbestos manufacturers, suppliers, and any other responsible parties would not involve suing any branch of the military, nor would it impact your ability to get veteran’s benefits. Our team can advise you on your legal right to compensation and handle all the necessary paperwork to pursue such a claim.\nTreatment for Veterans with Mesothelioma\nThe U.S. Department of Veterans Affairs acknowledges that countless veterans were exposed to asbestos during their time in the service, so it has established medical centers that specialize in treating veterans with mesothelioma. Veterans with mesothelioma may need surgery, chemotherapy, radiation, or a combination of treatments.\nYour local VA medical center can help you and your family make arrangements and cover the costs of travel, treatment, and housing at a mesothelioma center. The VA also offers TeleHealth programs, which make it possible for veterans to see top mesothelioma doctors from across the country without having to travel.\nMany veterans may weigh the option of using private health insurance vs. getting VA benefits. However, using VA benefits for mesothelioma treatment provides some advantages, such as:\nThe VA ranks veterans based on eight priority groups. Veterans with service-connected mesothelioma fall into the Priority Group 1, meaning they are 50 percent or more disabled and unemployable due to their condition. This ranking allows veterans with mesothelioma to receive more time-sensitive, priority care through the VA system.\nCost of treatment\nMesothelioma patients may need expensive surgery, chemotherapy, radiation, and ongoing palliative care. This can be financially draining on a family. However, veterans may receive more affordable treatment and care through the VA system vs. private health care providers.\nTransportation and lodging during treatment\nThe VA has a Veterans Transportation Service and a Beneficiary Travel program that offer free transportation to medical appointments. If a veteran receives a referral to a VA mesothelioma treatment center, the VA will help coordinate travel and lodging, including for a loved one to go with the patient.\nTo find a treatment center near you, search the VA health facility database.\nClinical Trials for Veterans\nAlthough there is no cure yet for mesothelioma, doctors and researchers across the United States and around the world are always testing new treatments in hopes of finding one. Veterans who have been diagnosed with mesothelioma may want to consider exploring alternative treatment options available through clinical trials.\nMesothelioma clinical trials can explore a wide range of treatment options, including:\nTesting new medications or devices\nTrying new procedures\nExperimenting with changes to behavior or diet\nIn most cases, mesothelioma patients do not have to pay to participate in clinical trials. The organization that is sponsoring the trial pays for the experimental treatment.\nIf you or a loved one is considering a mesothelioma clinical trial, talk to your doctor about the potential risks and rewards."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:8cd891b3-364f-4a71-8818-5aad912d4215>","<urn:uuid:0de7bb52-d8aa-4168-ab90-3438b2a3c6e3>"],"error":null}
{"question":"How can machine learning be used to predict electricity demand?","answer":"Machine learning approaches for electricity demand forecasting include using LSTM-based hybrid ensemble learning for ultra-short-term predictions and empirical mode decomposition combined with deep learning. These methods process historical electricity consumption data and can incorporate feature selection techniques. Genetic algorithms are sometimes used to optimize the models, and the approaches have shown improved accuracy compared to traditional forecasting methods.","context":["Kumar, A., Shankar, R., & Aljohani, N. R. (2020). A big data driven framework for demand-driven forecasting with effects of marketing-mix variables. Industrial marketing management, 90, 493-507.\n Villegas, M. A., Pedregal, D. J., & Trapero, J. R. (2018). A support vector machine for model selection in demand forecasting applications. Computers & industrial engineering, 121, 1-7. https://doi.org/10.1016/j.cie.2018.04.042.\n Johannesen, N. J., Kolhe, M., & Goodwin, M. (2019). Relative evaluation of regression tools for urban area electrical energy demand forecasting. Journal of cleaner production, 218, 555-564.\n Law, R., Li, G., Fong, D. K. C., & Han, X. (2019). Tourism demand forecasting: A deep learning approach. Annals of tourism research, 75, 410-423\n Olaniyi, A. A., Adedotun, K. O., & Samuel, O. A. (2018). Forecasting methods for domestic air passenger demand in Nigeria. Journal of applied research on industrial engineering, 5(2), 146-155. 10.22105/jarie.2018.133561.1038\n Bandara, K., Bergmeir, C., & Smyl, S. (2020). Forecasting across time series databases using recurrent neural networks on groups of similar series: A clustering approach. Expert systems with applications, 140, 112896. https://doi.org/10.1016/j.eswa.2019.112896\n Sagheer, A., & Kotb, M. (2019). Time series forecasting of petroleum production using deep LSTM recurrent networks. Neurocomputing, 323, 203-213.\n Panigrahi, S., & Behera, H. S. (2017). A hybrid ETS–ANN model for time series forecasting. Engineering applications of artificial intelligence, 66, 49-59.\n Parmezan, A. R. S., Souza, V. M., & Batista, G. E. (2019). Evaluation of statistical and machine learning models for time series prediction: Identifying the state-of-the-art and the best conditions for the use of each model. Information sciences, 484, 302-337.\n Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735-1780.\n Wu, Y., Yuan, M., Dong, S., Lin, L., & Liu, Y. (2018). Remaining useful life estimation of engineered systems using vanilla LSTM neural networks. Neurocomputing, 275, 167-179. https://doi.org/10.1016/j.neucom.2017.05.063.\n Khashei, M., & Bijari, M. (2011). A novel hybridization of artificial neural networks and ARIMA models for time series forecasting. Applied soft computing, 11(2), 2664-2675.\n Murray, P. W., Agard, B., & Barajas, M. A. (2018). Forecast of individual customer’s demand from a large and noisy dataset. Computers & industrial engineering, 118, 33-43.\n Abbasimehr, H., & Shabani, M. (2020). A new framework for predicting customer behavior in terms of RFM by considering the temporal aspect based on time series techniques. Journal of ambient intelligence and humanized computing. 10.1007/s12652-020-02015-w.\n Fischer, T., & Krauss, C. (2018). Deep learning with long short-term memory networks for financial market predictions. European journal of operational research, 270(2), 654-669.\n Bengio, Y., Simard, P., & Frasconi, P. (1994). Learning long-term dependencies with gradient descent is difficult. IEEE transactions on neural networks, 5(2), 157-166.\n Xin, W. A. N. G., Ji, W. U., Chao, L. I. U., Haiyan, Y. A. N. G., Yanli, D. U., & Wensheng, N. I. U. (2018). Exploring LSTM based recurrent neural network for failure time series prediction. Journal of beijing university of aeronautics and astronautics, 44(4), 772-784.\n Abbasimehr, H., Shabani, M., & Yousefi, M. (2020). An optimized model using LSTM network for demand forecasting. Computers & industrial engineering. https://doi.org/10.1016/j.cie.2020.106435.\n Shankar, S., Ilavarasan, P. V., Punia, S., & Singh, S. P. (2019). Forecasting container throughput with long short-term memory networks. Industrial management & data systems. 120(3), 425-441. 10.1108/IMDS-07-2019-0370.\n Ke, J., Zheng, H., Yang, H., & Chen, X. M. (2017). Short-term forecasting of passenger demand under on-demand ride services: A spatio-temporal deep learning approach. Transportation research part C: Emerging technologies, 85, 591-608.\n Pan, B., Yuan, D., Sun, W., Liang, C., & Li, D. (2018, June). A novel LSTM-Based Daily Airline Demand Forecasting Method Using Vertical and Horizontal Time series. Pacific-Asia conference on knowledge discovery and data mining (pp. 168-173). Springer, Cham.\n Bedi, J., & Toshniwal, D. (2018). Empirical mode decomposition based deep learning for electricity demand forecasting. IEEE access, 6, 49144-49156.\n Bouktif, S., Fiaz, A., Ouni, A., & Serhani, M. A. (2018). Optimal deep learning lstm model for electric load forecasting using feature selection and genetic algorithm: Comparison with machine learning approaches. Energies, 11(7), 1636.\n Bedi, J., & Toshniwal, D. (2019). Deep learning framework to forecast electricity demand. Applied energy, 238, 1312-1326.\n Su, H., Zio, E., Zhang, J., Xu, M., Li, X., & Zhang, Z. (2019). A hybrid hourly natural gas demand forecasting method based on the integration of wavelet transform and enhanced Deep-RNN model. Energy. 178, 585-597. https://doi.org/10.1016/j.energy.2019.04.167\n Tan, M., Yuan, S., Li, S., Su, Y., Li, H., & He, F. (2019). Ultra-short-term industrial power demand forecasting using LSTM based hybrid ensemble learning. IEEE transactions on power systems, 35(4), 2937-2948. 10.1109/TPWRS.2019.2963109\n Kulshrestha, A., Krishnaswamy, V., & Sharma, M. (2020). Bayesian BILSTM approach for tourism demand forecasting. Annals of tourism research, 83, 102925. https://doi.org/10.1016/j.annals.2020.102925\n Punia, S., Nikolopoulos, K., Singh, S. P., Madaan, J. K., & Litsiou, K. (2020). Deep learning with long short-term memory networks and random forests for demand forecasting in multi-channel retail. International journal of production research, 1-16. 10.1080/00207543.2020.1735666\n Bui, V., Kim, J., & Jang, Y. M. (2020, February). Power demand forecasting using long short-term memory neural network based smart grid. 2020 international conference on artificial intelligence in information and communication (ICAIIC) (pp. 388-391). IEEE.\n Wu, D. C. W., Ji, L., He, K., & Tso, K. F. G. (2020). Forecasting tourist daily arrivals with a hybrid Sarima–Lstm approach. Journal of hospitality & tourism research. https://doi.org/10.1177/1096348020934046\n Chollet, F. (2015). Keras. Retrived January 12, 2020 from https://github.com/fchollet/keras.\n Graves, A. (2013). Generating sequences with recurrent neural networks. https://arxiv.org/\n Jiawei Han, M. K., & Pei, J. (2011). Data mining: concepts and techniques: concepts and techniques. Waltham, USA: Elsevier Science.\n Martínez, F., Frías, M. P., Pérez-Godoy, M. D., & Rivera, A. J. (2018). Dealing with seasonality by narrowing the training set in time series forecasting with kNN. Expert systems with applications, 103, 38-48."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:2cbb9092-ee68-40ee-aac4-8b089ce1de80>"],"error":null}
{"question":"What are the consequences of improper DEF handling versus DEF contamination in SCR systems?","answer":"Improper DEF handling (like using incorrect containers or mixing with other fluids) can lead to immediate contamination and system problems. When DEF runs dry, engines won't start and sites can be shut down for weeks. In contrast, DEF contamination in SCR systems has a cumulative, long-term effect. The contaminated DEF gradually corrodes the catalyst, which cannot be repaired and must be replaced. This damage occurs over time like an infection, with no immediate cause-and-effect relationship visible. The catalyst is the most expensive component in an SCR system, and most OEMs don't cover damage caused by contaminated DEF.","context":["Most Tier 4 Final construction equipment over 75 hp utilizes selective catalytic reduction (SCR) exhaust aftertreatment technology to convert NOx to N2 and water through a chemical reaction. This aftertreatment technology has typically resulted in improved machine performance and better fuel efficiency while still meeting clean air mandates.\nThese SCR systems are well proven in on-road applications. But for the catalytic reaction to occur, a diesel exhaust fluid (DEF) must be sprayed over the precious metal substrate in the catalytic converter. DEF is 32.5% of a very pure synthetic grade of automotive-grade urea and 67.5% de-ionized water. For those who are not already familiar, urea is often used as a fertilizer in agricultural applications, though in a less pure form.\nWhile SCR technology has proven benefits, customers need to be educated on these systems to prevent costly repairs and downtime. The biggest risk factor is from contaminated DEF.\nUnfortunately, many customers are not properly educated since this is not a popular topic during the sales process. “If you are unhappy about the additional cost of this new excavator because of the cost of the emissions technology and you are a little sour because you have no other option, I am probably not going to have a great conversation with you about DEF handling,” explains Luke Van Wyk, Thunder Creek Equipment. “You are upset that you have to handle it in the first place.”\nYet, proper DEF handling is the key to trouble-free service. The catalyst is the most expensive component in an SCR system. “A catalyst cannot be repaired, it can only be replaced,” says Van Wyk. Any contamination in the DEF fluid can coat the precious metal in the catalyst, which cannot be reversed. “Any damage or degradation that happens there just happens.”\nIt is a cumulative effect. “Part of what gives owners a false sense of security is that the effects of metals contamination happen over a period of time, like an infection,” says Van Wyk. “We get a false sense of security because there is no immediate cause and effect relationship. That is why employing proper handling practices up front is so important. It is really a pay now or pay later type of risk. There are some very simple steps you can take to minimize or eliminate the risk you have from DEF purity.”\nAnd this task does fall on your shoulders. Most OEMs clearly state they are not responsible for damage caused to the SCR catalyst due to contaminated DEF.\nIn the trucking industry, SCR technology has a proven track record of trouble-free service. But there are key differences between on-highway trucks and off-highway construction equipment that need careful consideration. The biggest difference is that on-highway trucks travel to the fluid source, usually a truck stop, while fluid is usually brought to a jobsite for construction equipment fleets — a considerably less sterile environment. This is where mishandling of DEF could potentially lead to costly repairs and unscheduled downtime.\nClean DEF is a Must\nIn order to prevent system degradation or failures, DEF must be kept free from contaminants. “The thresholds of contamination are very tight,” says Van Wyk. “The ISO standard defines the threshold for contamination in a measurement of milligrams per kilogram.\n“Urea is incredibly corrosive,” he continues. “That is why we have to avoid all of the earth metals with urea.” Urea is corrosive to copper, chromium, zinc, iron, aluminum and magnesium to name a few. “We begin to get these materials in the DEF that urea is corrosive to.” The DEF itself becomes contaminated as the urea corrodes the metals. This contaminated DEF then passes through the catalyst in the SCR system. “As the DEF continues to pass through that catalyst, it will begin to create corrosion inside the catalyst.\n“When you think about where these metals are found, think about how you normally handle fluids,” says Van Wyk. “You may use plastic containers. You may use metal containers, hoses or buckets. Most hoses have a fitting on the end. Those fittings are copper, chrome or nickel plated. Those are all known contaminants to DEF.”\nIt doesn’t take much exposure to any earth metal. “We have a customer now that had a fleet of machines. He built his own DEF tank on a service truck,” says Van Wyk. “He made the tank out of stainless steel, but he went to the local home and farm store and purchased fluid transfer components off the shelf. They were regular ball brass valves and a regular galvanized nozzle. He had a very significant breakdown in his fleet because all of those corrosives ended up in his catalyst.”\nEven the transfer hose itself has the potential to become a contaminant. “In the industry, there is a special hose that is approved for DEF transfer because the materials used in the rubber of the hose and in the manufacturing process of the hose cannot introduce contaminants into the system,” says Van Wyk. “An ISO standard outlines materials that are okay, and there is a much longer list of those that are not.”\nDon’t Treat DEF the Same as Other Fluids\nA common mistake is to treat DEF like other fluids in your supply chain, such as diesel fuel, engine oil or hydraulic fluid. “In all actuality, the threshold for risk is much different,” says Van Wyk. “Anything that we do to contaminate or change the chemical makeup of that fluid inhibits its ability to make the chemical reaction properly and can harm the catalyst.”\nFor example, some technicians may have one funnel that they run all fluids through — engine oil, hydraulic oil, antifreeze. If you do the same with DEF, you will introduce whatever trace contaminants that were in the funnel into the DEF stream. As such, use of a separate funnel is a one of the necessary precautions.\nA similar approach should be taken with jugs. “We were in a shop where a guy had grabbed whatever jugs he could find — the antifreeze jug, oil jugs, gas cans,” recalls Van Wyk. “He would take them down to the local truck stop where they had DEF at the dispenser and fill up the jugs. Everything that was natively in those jugs — the oil, the antifreeze — was now in his DEF supply chain.”\nThunder Creek Equipment advises using a sealed or closed loop system to further minimize the chance of contamination. “A sealed system is going to be a disposable container,” says Van Wyk. This includes 1- or 2 1/2-gal. jugs that are sealed at the factory and thrown away once used. “We implore people who want to handle DEF in smaller quantities to do that.” Never refill the container because it introduces a risk of contamination.\nIn larger fleets, using sealed containers is not practical. This is where closed systems come into play. “One of the common packages for DEF is a 275- or 325-gal. IBC container,” says Van Wyk. “It shuttles a plastic jug inside an aluminum cage. Those are a sealed system. There is no open access to the tank. You refill those through a Micro Matic coupler. It allows fluid to be transferred from container to container while that container remains a sealed, clean, sterile environment.”\nTransferring bulk DEF from the large totes at the shop to the jobsite is where companies like Thunder Creek Equipment provide solutions. “There is a lot of attention given to a good quality of fluid in the original package and there is a demand by the equipment manufacturers for clean fluid to the machine,” says Van Wyk. “That is the world that we live in. You will want to use an intermediate system, similar to what we design, that can take the fluid through the supply chain using that Micro Matic coupler so it has a sealed path the whole way through.”\nBut this is not the only solution available. “There are quite a few people who offer DEF tanks in the marketplace, just like there are a lot of people who offer gas cans,” Van Wyk acknowledges. “What is exclusive to us is that closed-loop handling system.” Most intermediate containers in the marketplace have to be filled through an open point of fill.\nWhen filling the equipment in the field, take all necessary precautions to avoid dust contamination. If you suspect the dispensing nozzle has been contaminated, you can run a little DEF through it prior to dispensing since DEF is non-hazardous.\nAlso make sure you are dispensing in the appropriate tank. “There is a specific design diameter nozzle to try to reduce the risk of cross-contamination — someone putting a gasoline or diesel nozzle into the DEF tank,” says Van Wyk. “But we still hear stories of it happening.”\nDon’t try to change the nature of DEF, which freezes at 12° F. SCR systems and DEF handling containers are designed to handle the freezing and thawing cycles. “In some cases, we see people try to use additives with DEF,” says Van Wyk. “They don’t like that it will freeze, particularly in the Northern climates. Some companies have come to the market with antifreeze for DEF that will lower its freezing temperature. But it has very bad effects and causes a lot of damage to the life of the catalyst.”\nThe key to trouble-free operation is education on the proper handling of DEF.\n“As a fleet owner, I need to figure out how to protect myself so that the operator cannot unknowingly do something to harm my machine,” says Van Wyk. “We have to be very intentional about who we are educating. If I am a fleet owner, the risk is not as much about what I know about [DEF], but what the employee who puts fluid in my machine knows about it. That is where my real point of risk is.”","How to store DEF\nIf you use diesel engine driven equipment – especially EU Stage V compliant engines – you’re likely already using DEF (diesel exhaust fluid/AdBlue). DEF is essential for keeping diesel equipment running smoothly while reducing emissions released into the atmosphere.\nDEF is widely used throughout the construction industry, but many issues and questions remain about storing DEF onsite. DEF differs from fuel, yet proper handling is vital to maintain purity. Many users are yet to understand how DEF works and its importance. We’ll take a look at the fluid in our guide, why it’s a legal requirement and storage regulations that need to be followed.\nWhat is DEF?\nDEF is the acronym for diesel exhaust fluid, a non-toxic mix of deionised water and urea. Known as Adblue in the UK, it was introduced by the Environmental Protection Agency (EPA) in 2010 as part of the Clean Air Act. As per these standards, heavy-duty vehicles and machinery, such as vehicles with stage V engines, off-road construction equipment, often use DEF.\nThe purpose of DEF is to reduce emissions released from diesel engines, as part of a Selective Catalytic Reduction (SCR) system. Without it, many engines release harmful emissions into the atmosphere and can cause engine shutdown.\nWhat is the EU Stage V legislation?\nEU Stage V legislation is the latest emissions standard for Non-road Mobile Machinery (NRMM) with spark/compression ignition engines. NRMM includes equipment such as generators, lighting towers, pumps, etc\nFor Engines with an output of 19 kW and more, the permitted values for NOx and PM (Particulate Matter) emissions are one level stricter than previous standards. In addition, new PN (Particle Number) standards have also been set, making the new emissions standards generally the strictest in the world.\nHow does DEF work?\nDEF is added to your equipment’s exhaust stream, creating a chemical reaction that turns nitrous oxide (NOx) into nitrogen and water. This non-toxic substance minimises the amount of carbon released without compromising on the engine’s power.\nThe key with using DEF is that it cannot run dry. If a vehicle or piece of equipment runs low, performance is lowered and may eventually stop working. This is where storing DEF in bulk helps save money and prevent delays onsite – no more waiting for deliveries when there’s no diesel exhaust fluid available.\nHowever, DEF does have an expiration date, and if it’s contaminated, its performance is affected. Your DEF storage solution must keep your stock secure and clearly labelled to help avoid cross-contamination.\nHow to store DEF\nDEF isn’t a fuel – it’s a liquid used to reduce and treat exhaust emissions, meaning you cannot treat it like diesel. Maintaining its purity should be a priority.\nDue to its non-toxic nature, DEF is not as regulated as fuel, but there are still some standards to follow. ISO22241 is the standard for transporting, handling and manufacturing DEF, and it details the requirements for safe storage.\nWhen storing DEF, you should:\n- Separate your stock from other fuels and chemicals to minimise contamination. Instead, store DEF in an isolated area with minimum surface water drains.\n- Also, ensure it is out of direct sunlight to avoid losing its urea potency.\n- Regularly clean dispensing nozzles, as well as equipment and machinery.\n- Make sure only to use dedicated DEF systems – do not mix with other fuels.\n- Store your DEF in a steel or plastic tank with secondary containment. This is a recommendation of the Environment Agency (EA). DEF is corrosive to some metals, so steel and plastic is best suited.\n- Because of this, avoid overfilling your DEF fluid in case it spills over and makes contact with other metals.\nWestern Global has designed a unique DEF storage solution that differs from the market. Our DEFCube is a bunded, plastic inner tank designed for long-term bulk storage of DEF. Its secondary containment is a steel outer that helps with temperature control, meaning your DEF will retain its usability even in the coldest climates. Find out more about the DEFCube.\nWhat is the shelf life of DEF?\nThe shelf life of DEF can depend on its storage conditions, but generally, it lasts one to two years. Once expired, it’s less effective and can lead to issues with equipment and machinery.\nYou should find the expiration date of your DEF clearly marked on its container. However, if you’re unsure, you can easily tell the condition of your diesel exhaust fluid by simply looking at it. DEF is naturally clear – if your stock is cloudy or has particles, it’s likely expired or contaminated.\nWhat temperature does DEF freeze?\nDEF freezes at -11°C and can expand up to 7% when frozen, potentially damaging your tank – especially when full. Some try to add additives to the DEF to prevent it from freezing, but this can affect its purity and effectiveness. If you leave your diesel exhaust fluid to thaw naturally, it should still be usable.\nMaintaining the temperature of your DEF has never been easier with our DEFCube. Our tank is bunded with a poly inner and steel outer, providing complete weather-proof containment that aids temperature control. Pipes and fittings are also stored behind a lockable cabinet to protect them from the elements. With these measures in place, you can store DEF in bulk for long time periods, no matter the climate.\nProblems with storing DEF\nStoring DEF correctly prolongs its shelf life, minimises contamination and ensures that equipment and off-road vehicles can perform at their best. But as DEF must be treated differently to other fuels, users find a handful of issues.\nWe’ve answered several of your questions regarding DEF storage below, but if you want to know more about diesel exhaust fluid and are looking for the best containers, then get in touch with our team.\nWhat happens if you put DEF in your Diesel Tank?\nIf DEF is mixed with diesel, it can seriously impact your engine. It can clog up pipes and affect the lubricating properties of the fuel, leading to more wear and tear for your engine. In the worst cases, the engine will just shut down.\nWhen this happens, fuel systems need to be flushed and thoroughly cleaned to remove all traces of contaminated substances. You can’t reuse contaminated DEF, so ensure this is disposed of safely.\nThis is why DEF is often stored in blue containers, and its filling caps are blue – to help differentiate it from other liquids. Only dedicated pumps and containers should be used for DEF, with all fittings safely closed and locked away to prevent drips or leaks.\nHow can you prevent diesel exhaust fluid spills?\nLike any liquid, DEF fluid spills can happen. But the difference is that spilt DEF isn’t a major issue.\nWe mentioned earlier that there are less regulations to follow when storing DEF, so you’re less likely to be fined. The only time a DEF spill is cause for concern is if it mixes with another fuel or it’s leaked down a drain. DEF can corrode some materials and may affect the drainage pipes if this does happen.\nIn the event of a DEF spill, make sure the solution does not enter any drains or waterways. Then, contain the spill with a spill kit and dispose of it safely.\nDiesel exhaust fluid spills should still be avoided. The DEFCube’s bunded design offers complete containment, meaning that any DEF leaks would spill into the outer tank. The tank is robust and can withstand wear and tear, providing next-level protection for your diesel exhaust fluid. Leakage risk is also minimised with pipes secured behind the cabinet – you can still refuel equipment with the hose while it is locked.\nWhat happens if you run out of DEF?\nIf DEF runs dry, engines won’t start and sites can be shut down for weeks or even months in worst cases, as you wait for the next delivery. If there is fuel contamination, you must also wait for equipment to be replaced. You should manage your DEF consumption as you do fuel, ensuring there’s always more fluid ready to use.\nBulk DEF tanks are ideal for storing DEF onsite, no matter the circumstances. We’ve made sure that our tanks are fit for purpose, with the DEFCube, a robust and ready solution that helps retain the purity of your DEF, maintaining its shelf life and saving you time and money.\nThe tank is designed for mobile refuelling, meaning it can be easily forklifted to and from site, and even stacked when empty to save space. There’s also minimal set-up time, so once it’s dropped onsite, you’ll be up and running in no time.\nDEF storage from Western Global\nProviding 110% bunded containment for security and protection, our DEFCube is ideal for hot and cold climates and can be lifted into challenging environments. Refuel generators and equipment as and when needed, knowing that your diesel exhaust fluid is safely contained. Find out more about the DEFCube or request a quote from our team."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:39db0c0a-89c1-4650-869a-cab70624fd81>","<urn:uuid:63bfe059-b11f-484e-976a-509a976efda8>"],"error":null}
{"question":"Need help with SQL CHECK constraints - can I use them to validate marks between 0-100? 👨‍💻","answer":"Yes, CHECK constraints can be used to limit values within a specific range. For example, you can create a CHECK constraint to ensure marks are between 0 and 100 using a Boolean expression like 'CHECK (Marks >=0 AND Marks <= 100)'. However, note that CHECK constraints can be overridden by NULL values and are not validated during DELETE operations.","context":["SQL Server Constraints can be defined as rules that govern the values that are inserted into a column. These rules help in enforcing data integrity. For example if a NOT NULL constraint is defined over a column in a particular table, it means that column will not accepted any NULL values.\nConstraints used in SQL Server could be classified as follows:\n1) NOT NULL\n3) PRIMARY KEY\n4) UNIQUE KEY\n5) FOREIGN KEY\nLets discuss each one briefly.\nNOT NULL Constraint\nNOT NULL Constraints are those that disallow a null value. The NOT NULL constraint is defined at the time the table is created. Also if a column is defined to be the Primary key column, it automatically becomes a not null column, because NULL cannot be a key for a row.\nCheck constraints are those rules that enforce domain level integrity of data by limiting the values entered within a column. They can be created using any logical (Boolean) expression that returns either TRUE or FALSE, based on the operators used. CHECK constraints work at row level.\nNote: The CHECK constraint can be overridden by NULL value.\nThey are not validated during DELETE operation.\nLet us take an example. Suppose we need to enter the marks obtained by students in Science subject. The constraint applied to table dbo.Science_Marks will make sure that Max marks should not exceed 100 and Min marks should not be below 0.\n--Creating the Table dbo.Science_Marks IF OBJECT_ID ('dbo.Science_Marks', 'U') IS NOT NULL DROP TABLE dbo.Science_Marks; GO CREATE TABLE dbo.Science_Marks (StuID int PRIMARY KEY, StuName nvarchar (50), Sci_Marks tinyint) GO --Applying the Constraint ALTER TABLE dbo.Science_Marks ADD CONSTRAINT CK_Marks_Sci_Marks CHECK (Sci_Marks >=0 AND Sci_Marks <= 100) --Inserting records INSERT INTO dbo.Science_Marks values(1,'Abhishek',85) INSERT INTO dbo.Science_Marks values(2,'Abhi',67) INSERT INTO dbo.Science_Marks values(3,'Saurav',76) INSERT INTO dbo.Science_Marks values(5,'Wasim',85) INSERT INTO dbo.Science_Marks values(6,'Ritesh',99) --The above entries would be allowed, but on inserting the below record, SQL Server will throw an error message as the constraint does no allow the marks to be above 100 or below 0. INSERT INTO dbo.Science_Marks values(4,'Aniket',102) Msg 547, Level 16, State 0, Line 1 The INSERT statement conflicted with the CHECK constraint \"CK_Marks_Sci_Marks\". The conflict occurred in database \"BLOG_TEST\", table \"dbo.Science_Marks\", column 'Sci_Marks'. The statement has been terminated.\nPRIMARY KEY CONSTRAINT\nPrimary key can be defined as that value which uniquely identifies one and only one row of a table. Each row has its own primary key value. Every table has at least one column that uniquely identifies each row of the table, thus maintaining the entity integrity. Setting a column as primary key automatically enforces the NOT NULL constraint on that column.\nA table can have only one Primary Key Constraint. If a Primary key constraint is applied to a column of an existing table, SQL server checks the column for any NULL value or any duplicate values, if found, Primary key cannot be defined till the redundancy is rectified or the NULL value is supplied with a valid entry.\nSuppose in the previous example if we try to insert another row, having the same value for StuID column, SQL Server will throw an error.\nINSERT INTO dbo.Science_Marks values(6,'Akanksha',83) Msg 2627, Level 14, State 1, Line 1 Violation of PRIMARY KEY constraint 'PK__Science___6CDFAB757F60ED59'. Cannot insert duplicate key in object 'dbo.Science_Marks'. The statement has been terminated.\nUNIQUE KEY CONSTRAINT\nA Unique Key Constraint enforces uniqueness of the values in the column over which they are defined. Even primary key constraint imposes the rule of uniqueness but it also imposes NOT NULL constraint. Talking of uniqueness, among other unique values, NULL is also a unique value, i.e. the column having a unique key constraint may allow just one NULL value (unlike Primary key that does not allow even a single NULL value).\nSuppose in the previous example we want to add a Unique constraint on the StuName column, we can use the below script:\nALTER TABLE dbo.Science_Marks ADD CONSTRAINT IX_Student_Name UNIQUE(StuName) GO Now if we try to make another entry with same value for StuName column, SQL Server throws an error. INSERT INTO dbo.Science_Marks values(7,'Ritesh',83) Msg 2627, Level 14, State 1, Line 1 Violation of UNIQUE KEY constraint 'IX_Student_Name'. Cannot insert duplicate key in object 'dbo.Science_Marks'. The statement has been terminated.\nFOREIGN KEY CONSTRAINT\nAlso known as Referential Constraints or Referential integrity constraints, foreign key constraints are those that establish relationships between two or more tables.\nIn the above diagram, we can see that the two tables have ShiftID column common in them. ShiftID column is a Primary key column in Shift table, but not in EmployeeDepartmentHistory table. Here ShiftID column in Shift table is a foreign key to EmployeeDepartmentHistory table.\nA foreign key column does not have to be a primary key column in the other table but should be a unique key column.\nA foreign key constraint can be created during creating the table.\nCREATE TABLE CUSTOMERS( CUST_ID INT NOT NULL, NAME VARCHAR (20) NOT NULL, AGE INT NOT NULL, ADDRESS CHAR (25) , PRIMARY KEY (CUST_ID) ); CREATE TABLE ORDERS ( ID INT NOT NULL, DATE DATETIME, CUSTOMER_ID INT references CUSTOMERS(CUST_ID), AMOUNT float, PRIMARY KEY (ID) );\nIn case the Order table has already been created and the foreign key constraint has not been set, we could use the below script to create one:\nALTER TABLE ORDERS ADD FOREIGN KEY (Customer_ID) REFERENCES CUSTOMERS (CUST_ID);"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:597d6c67-b0b0-4676-86e9-c1697df091b0>"],"error":null}
{"question":"How has the rise of big data transformed modern journalism?","answer":"Big data has profoundly transformed journalism through multiple aspects. Social media platforms and mobile communication devices produce vast amounts of data that are analyzed by algorithms, shaping the current news ecosystem. This has forced journalism to adapt to new phenomena including algorithmic recommendation systems, metrics and audience analysis, and automated content creation. The transformation occurs in four main areas: observation, production, distribution, and consumption of news. Additionally, this datafication challenges journalism's traditional epistemology and ethics, particularly regarding professional values like diversity and transparency.","context":["Journalism, Innovation & Datafication\nThe goal of the module is to advance the students’ understanding of how the journalistic field is affected by datafication, and how journalism, in turn, tries to adapt to change and the different social and technological challenges.\nAt the end of the course, students are able to:\n- understand the nature of (big) data,\n- describe how platforms and algorithms are drivers of the datafication of the (media) society,\n- explain how data affect the production, distribution, and consumption of news,\n- describe the epistemological, economic and ethical issues related to the datafication of journalism,\n- discuss the challenges of journalism innovation,\n- and, finally, understand how news start-ups become important change-agents in the current news ecosystem.\nJournalism is currently undergoing profound transformations. The impact of social media platforms, together with the ubiquity of mobile communication devices that produce a deluge of data, which in turn are analysed by algorithms, shape the current news ecosystem and force journalism to adapt to phenomena like algorithmic recommendation systems, metrics and audience analysis, as well as automated content creation. The course first looks into the nature of (big) data before analysing the interplay between platforms, algorithms and journalism. The central part of the course discusses four different areas in journalism where datafication occurs: observation, production, distribution, and consumption. The course also tackles the question of how the datafication of journalism challenges journalism’s epistemology and ethics, in particular since the algorithmic turn defies professional values such as diversity or transparency. The last part of the course focuses on the issues that the datafication entails for journalism innovation, both within and outside news organizations.\nThe course adopts a mix of ex-cathedra teaching, groupwork, interactive sessions, and student presentations.\nStudents’ class attendance is not mandatory, even if strongly encouraged. Teachers will not provide alternative teaching materials for non-attending students.\nEvaluation procedures and Grading criteria\n- The students will have to carry out an in-class presentation in the last session of the course on a specific topic of their choice.\n- There will be a final oral exam on the different topics of the course. The date of the exam is to be determined and will be communicated by the Deanery.\n- The final mark for the course will be determined by the in-class presentation (25%) as well as the final oral exam (75%).\n- Both the presentation and the exam will be held in English.\n- Innovation Research in Organizations, 1990–2018. Journalism Studies, 21(12), 1724-1743.\n- Coddington, M. (2015). Clarifying Journalism’s Quantitative Turn. Digital Journalism, 3(3), 331-348.\n- Kitchin, R. (2014). The Data Revolution: Big Data, Open Data, Data Infrastructures & Their Consequences. London: Sage.\n- Lewis, S. C. & Westlund, O. (2015). Big Data and Journalism. Epistemology, expertise, economics, and ethics. Digital Journalism, 3(3), 447-466.\n- Loosen, W. (2018). Four forms of datafied journalism. Journalism’s response to the datafication of society. Communicative Figurations working paper no. 18. Bremen: University of Bremen.\n- Porlezza, C. (2018). Deconstructing data-driven journalism. Reflexivity between the datafied society and the datafication of news work. Problemi dell’Informazione, Vol. 3, 369-392.\n- Radcliffe, D., & Lewis, S. C. (2021). The Datafication of Journalism: Strategies for Data-Driven Storytelling and Industry-Academy Collaboration. In: L. Bounegru & J. Gray (eds.), The Data Journalism Handbook (pp. 314-330). Amsterdam: Amsterdam University Press.\nThis is only a small selection of the readings. All the mandatory reading materials will be made available on the platform ICorsi.\nMaster of Science in Communication in Media Management, Corso di base, 1° anno"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"search_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:71aed348-b7ad-4c3c-b963-d2fa05deaff1>"],"error":null}
{"question":"What are the culinary applications of cayenne pepper, and how did its global spread through trade routes influence its adoption in different cuisines?","answer":"Cayenne pepper is versatile in cooking, being used to add medium heat and enhance flavors in various dishes. It works well with seafood like oysters, sardines, and scallops, and can be sprinkled on cheese sauces, eggs, dips, or vegetables. It's even used in desserts, particularly combined with chocolate. As for its global spread, the Portuguese were key traders who distributed chili peppers worldwide in the 1500s. They exported chilies from Brazil, introduced them to African ports, and brought them to India and Indonesia. While Africa, India, and Asia quickly embraced chilies in their cuisine, Europe initially viewed them mainly as curiosities. Interestingly, North America didn't adopt chilies until the 17th century, primarily through the slave trade, as African slaves' dietary preferences led plantation owners to grow chilies.","context":["Hot Cayenne Pepper: At a Glance\nHot cayenne pepper is made from the dried and ground fruit of the chili plant. It is readily available and popular in the Americas and Europe. The moderately hot chili is typically used to flavor savory dishes. It can also be found combined with chocolate in some desserts.\nCayenne pepper is called “pure chili powder” in some countries. This should not be confused with the commercial chili powder that is sold in the United States, which is often a blend of spices, including garlic and cumin.\nCayenne pepper is named for the Cayenne region of French Guiana, its place of origin.\nCooking with Hot Cayenne Pepper\nCayenne pepper has a dusty and lightly aromatic scent and a spicy and rounded bite. It is not the hottest of the chilies, though. It will add a medium level of heat to any dish. As a condiment, hot cayenne pepper can be sprinkled on cheese sauces, eggs, dips, or vegetables for an extra kick.\nOur hot cayenne pepper also makes an excellent spice in cooking. It goes well with seafood such as oysters, sardines, scallops, lobsters, or mussels. Cayenne pepper can be added to most main courses to increase their spiciness and enhance their flavor.\nA pinch of cayenne can add a fiery bite to your meal. Try cooking with our hot cayenne pepper with any of these recipes:\n- Seafood: Cayenne pepper is used with all kinds of seafood, from fish to mussels to crab. Spiced mackerel with horseradish potatoes takes less than an hour to make. It incorporates the bite of cayenne pepper with fresh mackerel and creamy potato salad.\n- Tacos: Try healthy adaptations of this traditional southwestern food. Honey sesame fish tacos combines cayenne pepper with sesame oil, honey, soy sauce, molasses, garlic, and chili powder for a unique cooking sauce.\n- Chocolate: The combination of spiciness and sweetness gives hot chili pepper truffles quite a kick without overpowering your taste buds. There are only four ingredients in this easy and delicious recipe.\nHot cayenne pepper also has health benefits, particularly for the cardiovascular system. It reduces blood cholesterol and triglyceride levels. Cultures where significant amounts of cayenne are consumed typically have lower rates of cardiovascular disease.\nHot Cayenne Pepper: History and Origination\nCayenne pepper originated in the Cayenne region of French Guiana, which is located in northwest South America. The name “Cayenne” was derived from a Tupi Indian name.\nSince prehistoric times, South America has been home to chilies. During the height of the spice trade, many varieties of chili pepper, including cayenne pepper, were introduced to Portugal and Spain. Christopher Columbus introduced cayenne pepper to Europe as the “Guinea pepper.” The earliest record of cayenne pepper is from 1493, when Columbus and his crew discovered the spice in the Americas.\nFrom Europe, chili plants and their seeds traveled around the world and thrived in many parts of Africa, the Americas, and Asia. In fact, after the spice trade, the only continent that didn’t grow and produce some form of chili was Antarctica.\nCayenne has been used in traditional medicine for centuries. Its use has been documented in Ayurvedic, Caribbean, European, Asian, Middle Eastern, American, and modern herbal medicines. It was believed to help ease gastrointestinal problems, including stomach aches and cramps. It was also rubbed onto the skin to treat arthritis and rheumatic pain.\nCultivation of Hot Cayenne Pepper\nCayenne pepper is a fine red (or red-brown) powder that is prepared from the seeds of the chili plant. Cayenne chili is also called “bird chili,” and it is a type of Capsicum annuum, the same species as bell peppers, pimentos, and many other peppers.\nThe chili plant is a perennial plant. It has dense branching stems, green leaves (with purple accents), and small white flowers. It grows to about 2 feet in height. The plant’s smaller and more slender peppers are used to produce cayenne pepper. Normally, the plant is grown as an annual because its production drops after about a year.","Everything You Wanted To Know About Chilli\nHistory - How They\nCircumnavigated The World\nOn this page we'll take a look at the history of the chilli after Columbus arrived in the Caribbean and Central America.\nWhat we find fascinating is that the chilli was adopted around the world in a period of approximately 50 years. This was achieved in a period when horse-drawn and wind-driven were the primary means of transport. It's almost as if the world was waiting for the chilli to arrive.\n- Despite Spain's apparent early claim to the chilli, the Portuguese appear to be the first traders to have spread the them globally. Portugal's maritime power - rounding the Cape of Good Hope and reaching India in 1498 - set a course for the chilli to leave South America.\n- The Treaty of Tordesillas in 1494 had effectively split the world in half between Spain and Portugal, and the Portuguese were eager to exploit their half, especially Brazil. By the 1500's they were regularly exporting chillies from Brazil, the only part of South America which Portugal could claim under the Treaty.\n- During their trips to India, the Portuguese traders stopped in various African ports along the way. The Africans' fondness for “Grains of Paradise”, or “Guinea Pepper” (Aframomum melegueta), which have a gingery, peppery taste, prepared them to absorb the pungent chilli into their cuisine with ease. In only a few years, chillies had traveled as far east as Mozambique. But trade was only one agent of the chilli’s spread: Portuguese slave gathering in Africa also played a large part.\n- While it is possible to trace the chilli’s move from South America across the Atlantic Ocean, its crossing of the Pacific is more difficult to pin down. The 1529 Treaty of Zaragosa defined Spanish and Portuguese jurisdiction in the Asia-Pacific region: the Spanish received the Philippines, and Portugal received the Spice Islands, or Moluccas (part of Indonesia around the island of Sulawesi).\n- By 1540, the Portuguese were trading in Indonesia; soon after, chillies made their way to China. However, it is unclear if the Portuguese were the first to bring the chilli to China. Indians and Arabs were actively trading with the Chinese long before the Europeans arrived in asia. Furthermore, Hunan and Szechuan provinces, whose cuisines use chillies most frequently, were connected to the non-Chinese world by the Silk Road trade route rather than by coastal ports. Moreover, at the time, there were no direct overland routes from Chinese ports to those two provinces.\n- In 1549, the Portuguese reached Japan, but again it is unclear if they introduced the chilli there, because the Japanese had already ventured to Mexico in Spanish-designed ships.\n- While Africa, India, and Asia quickly absorbed the chilli, Europe seemed reluctant to use it as anything more than a curiosity or an ornamental. From Spain, the chilli moved to Antwerp, then to Italy in 1526, and on to England in 1548. Curiously, the chilli did not reach Eastern Europe through trade with other Europeans.\nThere are a number of similar but competing theories as to how the chilli did reach Eastern Europe;\n- Muslim merchants may have brought chilli from India through the Persian Gulf, on Alexandria, and then north into Eastern Europe.\n- Alternatively, the Turks could have brought chillies from Asia and then transported them through the Persian Gulf, Asia Minor, and the Black Sea in to Hungary, which they conquered in 1526. From Hungary, the chilli then probably moved into Germany.\n- A third possibility has the Portuguese exporting chillies from Hormuz, one of their colonies in the Persian Gulf, to Eastern Europe as a cheaper alternative to black pepper.\n- Interestingly, it was not until 1868 that Europeans learned that chillies were not originally from India.\n- Most surprising is the length of time it took for the chilli to arrive in North America. Despite being grown in Mexico for thousands of years, it was not until the slave trade was in full swing that the chilli appeared. By 1600, the British and Dutch had broken the Spanish and Portuguese naval domination, opening up the spice trade. However there does not appear to have been any demand for chillies from the Americans as a result of this. Instead, it was the use of chillies in the African cuisine that is the reason behind their spread. Chillies had become such a integral part of the African diet that slave traders had to bring large quantities with them on their trans-Atlantic voyages. Also, to maintain the African slaves' eating habits once in North America (and consequently their performance), the plantation owners had to grow chillies. As a result, it was not until the 17th century that the chillies had become a staple in North America.\nAll the above is a fair bit to try and track in your head so I've put together this diagram to try and make it easier to absorb.\nI do have a page that follows on from this one. It's not a history as such, it's more a discussion of chillies in relation to Australian cuisine and developments on that subject. See this at chillies & Australian cuisine."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:c83c4951-a3bf-40ae-8ace-4264d8762a6b>","<urn:uuid:54749195-e88a-4b3e-b054-361f7f8123cd>"],"error":null}
{"question":"Hello! Can you please tell me what are polariser filters used for in photography, and how do they compare technically to dichroic filters in terms of light filtering mechanism?","answer":"Polariser filters are unique in photography as they cannot be replicated by post-processing software. They work by selectively canceling out or allowing polarized light, which enables them to remove reflections from water and windows, deepen blue skies, and increase saturation of bold colors. There are two types: linear polarisers (for manual focus lenses) and circular polarisers (compatible with autofocus DSLRs). In contrast to polarisers, dichroic (interference) filters work through a different mechanism - they use multiple layers of dielectric film on transparent glass to magnify and transmit desired wavelengths while reflecting unwanted ones. These layers create reflective cavities that resonate with specific wavelength frequencies while rejecting others through optical interference. While dichroic filters offer more precise light filtering capabilities than polarisers, they are more expensive and sensitive to angles.","context":["Matt Grayson explains about three important filters in the Cokin Creative filter system.\nInvented by Jean Coquin, a French photographer, the Cokin Creative Filter System has become one of the largest ranges of filter systems available today. They come in various sizes according to the skill level you feel you're at. The A system is for amateurs and consists of smaller filters to fit standard focal lengths. The P system is for professionals and the filters are larger to accommodate wider focal lengths or lenses with a larger front element. These two sizes are widely copied by independent companies so that the systems can be integrated with each other and I think it's a true testament to how popular the systems are. Two newer systems have been introduced from the original concepts called X-Pro and Z-Pro. They're even larger than the P series and are designed for use on medium and large format cameras.\nThe systems break down into separate parts which sounds annoying when you're buying them but is in fact a very intelligent way of doing things. Attached to the lens is a filter ring which then attaches to an filter holder which you then slide your filter into. The great thing about this is that it means you can simply buy a new filter ring for each lens that has a different filter size. With other filter systems, you have to buy a new filter and if you're looking at a circular polariser, it's a lot of money.\nThere are many filters available in the Cokin catalogue but we'll cover some of the most important ones you'll need if you're deciding to invest in the system.\nCokin Creative filter system: Graduated filters\nThe graduated filter is coloured at one end and fades to clear.\nGraduated filters are easy to spot because half of them are clear while the other half will gradually fade to a colour. They're particularly useful for landscape photography because it means you can tone the sky a different colour, or enhance the blue, without affecting the land. It means less work in Photoshop but there's a margin of error with the colour bleeding over onto the ground if the filter is misaligned. Also if your lens doesn't have internal focusing, as the lens twists round to get focusing, it will also rotate the filter and send the alignment out. There are lots of colours to choose from and each will have a different benefit. I enjoy using a light tobacco graduated filter because it warms up the sky when the sky is trying to give a good sunset but can't quite make it. Something I find happens a lot in the Peak District where I shoot a lot of my landscapes.\nCokin P graduated filters range in price from around £16 to £19 and are available from Warehouse Express here:\nCokin P graduated filters\nCokin Creative filter system: Polariser filters\nPolarisers are an important part of photography because they're possibly the only filter that can't be replicated in post-software. They work by cancelling out or allowing polarised light - depending on where the filter is positioned - and if you allow the polarised light to come through, a camera won't record the non-polarised version so the information is generally lost. With a polariser, you can cancel some reflections from water and windows, deepen blue skies and increase saturation on bold colours such as green or red.\nCircular polariser filters cut polarised light allowing you to remove reflections on glass and water.\nBecause they perform more than one task it makes them good value for money although they generally cost more. There are two types to look out for: linear polarisers and circular polarisers. Linear polarisers are only effective when used on a manual focus lens and this is because the beam splitters that an autofocus DSLR uses for focusing a metering depend on polarisation. Using a circular polariser obviates the issue and the two filter types work in the same way. Cokin polarisers are like any other polariser in that they're round unlike the square filters in the rest of the Cokin catalogue. However, they still don't have the screw thread. The round shape is simply to aid rotation of the filter to speed up the process. Each filter has several slots to add more than one filter and right at the back is a slightly wider one with a lip at the bottom. This is the one that the polariser fits into and sits on the lip to prevent it falling out. The filters are generally ridged around the edge to allow a good purchase when rotating.\nThe Cokin P Circular polariser costs £69.99 and is available from Warehouse Express here:\nCokin P Circular polariser\nCokin Creative filter system: UV filters\nUV filters do little to adjust the image unless using a strong type. Mainly they're there to protect the lens.\nUV filters are an important filter to have while also being pretty unimportant. They're important because they play an integral part of the process by sitting over the glass of the lens and simply protecting it from bashes and scrapes. If it gets smashed, the precious lens is protected and you can simply buy a new one which is much cheaper. They're unimportant because they don't alter the quality of the image or affect the exposure unlike the polarisers and graduated filters which can alter it by a few stops in some cases. In circumstances where there's a high amount of ultra violet rays, a strong UV filter will help by cutting out the UV haze and also cutting down on purple fringing.\nThere are some possible downsides to using a UV filter such as a loss in quality even though the filter is clear. The quality of the filter will determine how well it performs and remains in the background. The Cokin UV filters aren't screwed into the lens filter thread in the same way other manufacturers do, so you won't get the worry of the lens hood not fitting correctly or possibly vignetting the image if you have a wide lens.\nThe Cokin P UV filter costs £11.99 from Warehouse Express here:\nCokin P UV filter\nCokin Creative filter system: Summary\nThe Cokin Creative Filter system is so adaptable to your shooting style, you should be able to get better results almost immediately. The three part system is set up to ensure you get as much value for money as possible\nThe great thing about using filters is that it takes all the ground work back to the front-line where it was all those years ago in the days of film.\nIt means you have to think more about your photographs, slow down and take your time to work out what you want to achieve. My advice is to invest in the system, get out and take some photographs. Enjoy seeing what different effects you can create by using the filters in different situations and by merging them together.\nFor more information, go to the Cokin website here:\nCokin Creative filter system","Image credit: Knight Optical | Thorlab | Universe Kogaku\nOptical filters are devices, typically comprised of plate glass or plastic, which selectively transmit light of different wavelengths.\nOptical Filter Basics\nFilters can be broadly classified as absorptive or dichroic, dependent upon their means of blocking unwanted wavelengths.\nAs their name implies, absorptive filters transmit desired wavelengths by absorbing unwanted ones. These filters typically consist of dyed glass or pigmented gelatin resins, and are relatively inexpensive to manufacture and purchase. The ability of absorptive filters to attenuate light is based on the filter's physical thickness and the amount of dye or pigmentation present.\nAbsorptive filtration. Image credit: UC Davis\nAbsorptive filters are relatively less precise when compared with dichroic types. For this reason, they are often used in applications requiring the transmission of broad band of wavelengths, as well as applications involving the blockage of short wavelengths and transmission of longer ones.\nAbsorption filters have several inherent advantages and disadvantages. Their main strong points are their low cost, stability when used in a wide variety of environments, and insensitivity to illumination angle. Disadvantages include poor long-term temperature resistance and inadequate performance when used in precision applications. Absorptive filters, especially gelatin types, are suitable for a wide range of applications, including optical microscopy.\nDichroic (Interference) Filters\nDichroic filters, also known as interference filters, filter light by rejecting all undesired wavelengths, allowing selected wavelengths to pass through. The term \"dichroic\" is derived from the Greek word díchros, meaning \"of two colors.\" These filters are constructed using thin film technology by depositing several layers of dielectric film on one side of an optically flat piece of transparent glass. When light strikes the coated side of the filter, the various layers of film magnify and transmit the desired wavelengths while reflecting and reducing the undesired ones.\nCross-section of a typical interference filter. Image credit: Florida State University\nAs shown in the image above, dichroic filters rely on the series of reflective cavities between the film layers to achieve their precise filtering. These cavities resonate with the desired wavelength frequencies while destroying or rejecting all others; this is known as optical interference.\nCompared to absorptive types, interference filters are capable of significantly improved selectivity and are much more suitable to precision applications. This superior precision makes dichroic filters much more delicate and expensive than absorptive filters.\nImage credit: Florida State University\nThe table below compares both filter technologies, noting their advantages, disadvantages, and common applications.\nCost-effective; wide availability; scratch-resistant; excellent environmental and chemical resistance.\nPoor transmission slope; not suitable for precision applications; heat-sensitive; breakdown after prolonged use; necessary thickness.\nGeneral use; optical microscopy.\nPrecise transmission; capability for thin filter; wide variety of simple or complex types possible.\nExpensive; increased angle sensitivity; ineffective at blocking wide wavelength ranges.\nPrecision use; fluorescence microscopy.\nOptical filters can be classified into different types, dependent upon their construction, applications, or type of light they filter.\nColor filters are simply colored glasses distinguished by their specific color wavelengths. These filters typically only appear colored if their filter action is within the visible light spectrum. Color filters are typically specified by a prefix which determines glass color as well as transmission properties. Examples include:\nUG: black and blue glasses, ultraviolet (UV) transmitting\nVG: green glass\nGG: nearly colorless to yellow glasses, infrared (IR) transmitting\nRG: red and black glasses, IR transmitting\nN-WG: Colorless glasses, visible and IR transmitting\nHot and Cold Mirrors\nHot mirrors and cold mirrors are dichroic filters that selectively transmit certain wavelengths.\nHot mirrors transmit visible light wavelengths while reflecting near-infrared (near-IR) bands. Because near-IR wavelengths generate significant heat, hot mirrors are used to reflect this heat. Hot mirrors are typically light yellow in color.\nCold mirrors are similar to hot mirrors in that they separate IR and non-IR wavelengths. Cold mirrors, however, are designed to transmit IR bands while reflecting one or more non-IR bands, such as visible light. This helps isolate wavelengths needed for a particular application while eliminating the unneeded or potentially harmful bands. Cold mirrors are sometimes used in a \"reversed\" fashion, in that they reflect visible light to the application while transmitting unneeded IR light away from the application.\nCold mirror operation. Image credit: Harvard University\nHot and cold mirrors can be used in conjunction to protect optical fiber, which may be harmed by UV and IR wavelengths. In this application, the hot mirror would reflect the UV and IR light while transmitting visible light, and the cold mirror would reflect the desired visible light to the fiber while transmitting the IR wavelengths to a designated disposal location.\nLong- and Shortpass Filters\nLongpass and shortpass filters are named for the wavelengths they transmit and attenuate. Longpass filters transmit (or \"pass\") longer wavelengths and block shorter ones, while shortpass filters block longer wavelengths and pass shorter ones. Both are frequently used in fluorescence microscopy. Long- and shortpass filters with very sharp transmission slopes are sometimes called edge filters.\nNeutral Density Filters\nNeutral density filters, also known as ND filters or grey filters, reduce light transmission evenly across a portion of the spectrum. These filters are slightly sensitive to angles, but less so than interference filters. ND filter technology is common in consumer applications, particularly photography.\nSpectral filters simply transmit select wavelengths. They can be useful in numerous applications requiring emphasis of certain light patterns, including identification of laser patterns and sharpening of infrared images.\nThe Visible Spectrum\nWhen discussing and selecting optical filters, it is important to consider the visible portion of the electromagnetic spectrum. The human eye responds to wavelengths from around 390 nm (or 430 THz) to 700 nm (or 790 THz); the frequencies within this range are therefore called \"visible.\" (Wavelength literally refers to the length of one electromagnetic \"wave\", as seen in the full spectrum image below.) Optical filter specifications may indicate a specific range of frequencies or a specific color to be transmitted. A light source's wavelength determines the color perceived by the eye, as shown in the image below.\nImage credit: Giangrandi\nInfrared and Ultraviolet Light\nOptical filters may also be designed to transmit or reflect portions of the infrared (IR) or ultraviolet (UV) spectrum.\nInfrared light has longer wavelengths than visible light, and includes most thermal radiation emitted by near-room temperature objects. It includes wavelengths from 1 mm up to 700 nm, and frequencies from 300 GHz to 430 THz. Infrared light has a wide variety of applications in various sectors, including night vision technology, meteorology, thermography, heating, and spectroscopy. The infrared spectrum can be further divided from near-infrared (shorter wavelengths) to far infrared (longer wavelengths).\nUltraviolet light has wavelengths shorter than visible and infrared light. Ultraviolet wavelengths occur from 400 nm to 100 nm, and are further divided into Ultraviolet A (400-315 nm), B (315-280 nm), and C (280-100 nm) designations. Natural sunlight consists of around 10% UV radiation. Ultraviolet light is invisible to the unaided human eye, and high intensities of UV radiation can be harmful to the skin and eyes. Like infrared light, it is useful in a wide range of diverse imaging applications.\nThe full electromagnetic spectrum, showing the relationship between wavelength and frequency.\nImage credit: TechtheFuture\nThe selection of optical filters involves specifying either a broad spectrum (ie visible, infrared, or ultraviolet) or a specific range (ie 540-541 nm) to be transmitted or rejected.\nFilter Surface Quality\nThe optics industry rates product surface quality based on MIL-0-13830A standards published in 1963. These scratch/dig ratings consist of two numbers separated by a hyphen, as in x - y; x refers to scratches, while y refers to digs, as described below.\nA scratch refers to a surface defect whose length is many times its width. The initial number of the scratch-dig rating describes the maximum allowable width, expressed in tenths of microns, of a scratch.\nA dig is a defect on an optical surface that is nearly equal in length and width. The second value of the rating refers to the maximum diameter of a dig expressed in hundredths of a millimeter.\nFor example, a #20-10 indicates that any scratches have a maximum width of .002 mm (or 2 microns), while digs may have a maximum diameter of .10 mm. When considering the two rating values, it becomes clear that smaller values are desirable.\nOptical filters may be manufactured according to different standards or approvals. Some examples include:\nMIL-F-21241 (Colored finished filters - for optical instruments)\nISO 8577 (Spectral filters)\nDIN 58191 (Optical radiation filters [German language])"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:81d3b343-cf5e-4544-9afc-aa392ac819cd>","<urn:uuid:db5dd2fb-eb75-460b-a65f-28e68a02eb94>"],"error":null}
{"question":"I live in a flood-prone area. How do Flood Warnings and WEA alerts compare in terms of their notification approach and purpose?","answer":"Flood Warnings and WEA (Wireless Emergency Alerts) are both emergency notification systems but work differently. Flood Warnings are issued by the National Weather Service when serious flooding is imminent or occurring and require immediate action. WEA alerts, on the other hand, are emergency messages sent by authorized government agencies through mobile carriers, featuring a special tone and vibration repeated twice. While Flood Warnings are specifically about flooding, WEA can cover multiple types of emergencies including extreme weather, AMBER alerts, and presidential alerts. WEA messages are limited to 90 characters and automatically reach WEA-capable phones in affected areas without requiring signup, whereas Flood Warnings are typically broadcast through the Emergency Alert System on TV and radio.","context":["“By failing to prepare, you are preparing to fail.”\n― Benjamin Franklin\nThe following resources will help you prepare for natural disasters:\nGET EMERGENCY ALERTS!\nAware & Prepare: In Santa Barbara County, residents can receive Aware & Prepare alerts and updates via text messages, emails, and cellular and landline phone calls. To receive these messages, go to readysbc.org, click the red “Register for Alerts” box, and provide the necessary information. If you need help signing up, call 805-681-5526 or email email@example.com\nNixle: This information service operates nationwide, coordinating with local public-safety agencies to send alerts via text messages and social media. To sign up, simply text your zip code to 888777.\nWireless Emergency Alerts (WEA, pronounced “wee-uh”): A federal public-messaging tool that allows residents with certain wireless phones and other mobile devices to receive geographically targeted, text-like messages that warn of imminent safety threats. You don’t need to sign up for WEA ― cell phone companies volunteer to participate in the system. Check with yours to see if they do.\nEmergency Alert System (EAS): EAS messages are transmitted via radio and television by participating broadcasters. They contain an audio message and/or scrolling text. You don’t have to sign up to receive these types of alerts, but you do need to be tuned into a local TV channel or radio station to get them.\nThe National Weather Service (NWS) sends out four different types of alerts related to flooding. Know the differences:\n- Flood Advisory: Issued when flooding may become a nuisance or significant inconvenience. Be aware.\n- Flood Watch: Issued when conditions are more favorable for flooding. It does not mean flooding will occur, but it is possible. Be prepared.\n- Flood Warning: Issued when serious flooding is imminent or occurring. Take action.\n- Flash Flood Warning: Issued when a flash flood is imminent or occurring. Take action.\nSanta Barbara’s Office of Emergency Management often rebroadcasts these messages.\n- ReadySBC.org: The clearinghouse for all official county information. It contains a ton of resources, from guides to maps to important phone numbers and addresses. Whether you’re in the mode of disaster prep or recovery, all roads lead to ReadySBC.org.\n- SantaBarbaraCA.gov: The City of Santa Barbara’s Office of Emergency Services features helpful suggestions on assembling supply kits, preparing your business, shutting off utilities, and so on.\n- RedCross.org: Perhaps your best source for general health and safety information in crisis situations. Be sure to search through the Central California Region section as well.\n- Ready.gov: The Department of Homeland Security’s user-friendly site focused entirely on helping you prepare yourself and your family for any type of disaster.\n- EarthquakeCountry.org: The in-depth website for the Earthquake Country Alliance (ECA), a public-private partnership of people, organizations, and regional associations devoted to helping residents get ready for and survive the next Big One.\nHAVE EMERGENCY SUPPLIES FOR YOU AND YOUR FAMILY!\nAfter a major disaster, there will likely be delays in services, including the arrival of first responders to your neighborhood. Expect to be on your own for at least 72 hours, with a cache of water, food, clothing, and related supplies assembled and easy to access. Here’s a good guide on how to build your emergency supply kit.\nHAVE A PLAN!\nWhen disaster strikes, you and each member of your household should have a plan on what to do and, if separated, where to meet. If you have to evacuate your home, know where you are going to go. Have a backup plan, as well.\nPETS NEED A PLAN TOO!\n- County Animal Services*: (805) 681-5285; countyofsb.org\n- County Animal Services Emergency Hotline: (805) 681-4332\n- Santa Barbara Humane Society*: (805) 964-4777; sbhumanesociety.org\n- Santa Barbara Humane Society Emergency Hotline: (805) 330-3330\n- City of Santa Barbara Animal Control*: (805) 963-1513; santabarbaraca.gov\n- Earl Warren Showgrounds*: (805) 687-0766; earlwarren.com\n- B.U.N.S. (Bunnies Urgently Needing Shelter)*: (805) 683-0521; bunssb.org\n- VCA Care Specialty and Emergency Animal Hospital: (805) 899-2273; vcahospitals.com/care\n- Advanced Veterinary Specialists: (805) 729-4460; avs4pets.com\n(*Offers emergency boarding services)\nSanta Barbara Humane Society’s Lost & Found hotline at (805) 681-5285 or after business hours at (805) 564-6862.\nReport lost pets online through Santa Barbara County Animal Services at countyofsb.org.\nWhen the power is out and the internet is down, a radio may be your last hope for receiving critical information. The county partners with a network of Radio Ready stations to broadcast emergency notices with the help of generators and satellite equipment. Be sure you own a hand-crank or battery-operated radio, and keep extra batteries handy.\n- Santa Barbara/South Coast ― KCLU 88.3 and 102.3; KOSJ 94.1; KTYD 99.9\n- Santa Maria ― KCLU 89.7 and 92.1; KCBX 99.5\n- Lompoc ― KCBX 95.1\n- Santa Ynez/Solvang ― KCBX 90.9; KSYV 96.7; KRAZ 105.9\n- Countywide ― KSPE 94.5; KIST 107.7 *Spanish\n- Santa Barbara/South Coast ― KOSJ 1490; KZSB 1290; KCLU 1340\n- Santa Maria ― KTMS 990; KUHL 1440\nNOAA Weather Radio All Hazards (NWR): NWR is a nationwide network of radio stations broadcasting continuous weather information directly from the nearest National Weather Service office. To receive the Santa Barbara County broadcast, tune your weather radio to the SAME code 006083.\nSanta Barbara news agencies report on emergencies and frequently re-broadcast official county messages. Here are the region’s most trusted sources:\nTelevision: KEYT (Channel 3), KSBY (Channel 6), KKFX (Channel 11), and KCOY (Channel 12)\nRadio: KCLU (FM 88.3), KDRW (FM 88.7), KCSB (FM91.9), KIST (FM 107.7 *Spanish)\nDO YOU LIVE IN AN EVACUATION “RED ZONE”?\nSanta Barbara County has an evacuation map that pinpoints properties in “red zones” at risk for flooding and debris flows. This interactive map is online at readysbc.org/maps/. Search your street addresses to see if your property has been deemed at-risk.\nAs big rainstorms approach, people in “red zones” may be asked to evacuate. Santa Barbara County uses its “Ready! Set! Go!” initiative to notify the public.\nREADY: The first stage of a potential evacuation is a WEATHER ADVISORY, which will be issued 48-72 hours before the arrival of the storm. This advisory is the County Office of Emergency Management alerting the community to the storm. Be aware: Closely monitor the storm and have a plan if you’re asked to evacuate.\nSET: The next step is the EVACUATION WARNING, issued 24-48 hours before the storm. Prepare to leave; arrange transportation for you, your family, and, if necessary, large animals; make arrangements for family members with special needs; confirm your evacuation plan; gather items you’d like to take with you. Check on your neighbors.\nGO: Within 24 hours of the arrival of a storm that could pose a risk to life or property, the Sheriff’s Office will issue an EVACUATION ORDER, which means it’s time to evacuate if you are in a “red zone.” People who refuse to comply will not be forcibly removed from their homes, but they should not expect rescue or other lifesaving assistance, according to county officials. Flooding and debris flows may close down transportation routes for several days; if you choose to stay, you may become stranded in the disaster area.\nREMEMBER: If at any time you feel unsafe, take immediate action and do not wait for a notification to evacuate. Officials may only be able to alert the public with just a few minutes’ notice, or none at all.\nTwitter is often the most timely social media tool in a disaster. Here are the accounts to follow:\n- Office of Emergency Management (@SBCountyOEM)\n- County of Santa Barbara (@countyofsb)\n- County of Santa Barbara *Spanish (@csbenespanol)\n- Santa Barbara County Fire Department (@SBCOUNTYFIRE)\n- Santa Barbara Sheriff’s Office (@sbsheriff)\n- National Weather Service, Los Angeles (@NWSLosAngeles)\n- CHP Coastal Division (@CHP_Coastal)","1.Why are Wireless Emergency Alerts (WEA) important to me?\nAlerts received at the right time can help keep you safe during an emergency. With WEA, warnings can be sent to your mobile device when you may be in harm’s way, without the need to download an app or subscribe to a service.\n2.What are WEA messages?\nWireless Emergency Alerts (WEA) are emergency messages sent by authorized government alerting authorities through your mobile carrier.\n3.What types of alerts will I receive?\n- Extreme weather, and other threatening emergencies in your area\n- AMBER Alerts\n- Presidential Alerts during a national emergency\n4.What does a WEA message look like?\nWEA will look like a text message. The WEA message will show the type and time of the alert, any action you should take, and the agency issuing the alert. The message will be no more than 90 characters. Wireless Emergency Alert web banner (Spanish) – 70 KB\n5.How will I know the difference between WEA and a regular text message?\nWEA messages include a special tone and vibration, both repeated twice.\n6.What types of WEA messages will the National Weather Service (NWS) send?\n- Tsunami Warnings\n- Tornado and Flash Flood Warnings\n- Hurricane, Typhoon, Dust Storm and Extreme Wind Warnings\n7.What are AMBER Alerts?\nAMBER Alerts are urgent bulletins issued in the most serious child-abduction cases. The America’s Missing: Broadcast Emergency Response (AMBER) Alert Program is a voluntary partnership between law-enforcement agencies, broadcasters, transportation agencies, and the wireless industry.\n8.Who will send WEAs to issue AMBER Alerts?\nThe National Center for Missing and Exploited Children (NCMEC), in coordination with State and local public safety officials, sends out AMBER Wireless Emergency Alerts through IPAWS.\n9.What should I do when I receive a WEA message?\nFollow any action advised by the message. Seek more details from local media or authorities.\n10.Will I receive a WEA message if I’m visiting an area where I don’t live, or outside the area where my phone is registered?\nYes, if you have a WEA-capable phone and your wireless carrier participates in the program. (More than 100 carriers, including all of the largest carriers, do.)\n11.What if I travel into a threat area after a WEA message is already sent?\nIf you travel into a threat area after an alert is first sent, your WEA-capable device will receive the message when you enter the area.\n12.When will I start receiving WEA messages?\nIt depends. WEA capabilities were available beginning in April 2012, but many mobile devices, especially older ones, are not WEA-capable. When you buy a new mobile device, it probably will be able to receive WEA messages.\n13.Is this the same service public safety agencies have asked the public to register for?\nNo but they are complementary. Local agencies may have asked you to sign up to receive telephone calls, text messages, or emails. Those messages often include specific details about a critical event. WEAs are very short messages designed to get your attention in a critical situation. They may not give all the details you receive from other notification services.\n14.Will I be charged for receiving WEA messages?\nNo. This service is offered for free by wireless carriers. WEA messages will not count towards texting limits on your wireless plan.\n15.Does WEA know where I am? Is it tracking me?\nNo. Just like emergency weather alerts you see on local TV, WEAs are broadcast from area cell towers to mobile devices in the area. Every WEA-capable phone within range receives the message, just like TV that shows the emergency weather alert. WEA, like the TV station, doesn’t know exactly who is tuned in.\n16.Will a WEA message interrupt my phone conversations?\nNo, the alert will be delayed until you finish your call.\n17.How often will I receive WEA messages?\nYou may get very few WEA messages, or you may receive frequent messages when conditions change during an emergency. The number of messages depends on the number of imminent threats to life or property in your area.\n18.If, during an emergency, I can’t make or receive calls or text messages due to network congestion, will I still be able to receive a WEA message?\nYes, WEA messages are not affected by network congestion.\n19.What if I don’t want to receive WEA messages?\nYou can opt-out of receiving WEA messages for imminent threats and AMBER alerts, but not for Presidential messages. To opt out, adjust settings on your mobile device.\n20.How will I receive alerts if I don’t have a WEA-capable device?\nWEA is only one of the ways you receive emergency alerts. Other sources include NOAA Weather Radio, news broadcasts, the Emergency Alert System on radio and TV programs, outdoor sirens, internet services, and other alerting methods offered by local and state public safety agencies."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:24a52705-e9e8-4166-80d4-d003c6df46c1>","<urn:uuid:7b37b68c-ae35-41c3-82a2-d2b93a4beb20>"],"error":null}
{"question":"Hey, I'm running tests on my website. What's the key difference between A/B testing and multivariate testing vs descriptive and inferential statistics?","answer":"A/B testing and multivariate testing differ in complexity and scope: A/B testing compares two page variations with a single differentiating element, while multivariate testing is more complex. In contrast, descriptive and inferential statistics serve different analytical purposes: descriptive statistics summarize dependent variables for different conditions, while inferential statistics determine if differences between experimental groups are statistically significant rather than due to chance.","context":["How to 'Split Test' Your Website to Engage Online Customers Better\nWhen it comes to designing your website, you have two choices: Select the site elements you believe look best, or you can use \"split testing\" to determine which design features are most engaging to your audience.\nEssentially, split testing randomly serves up multiple versions of your website's pages to generate data about their effectiveness. The process allows you to quickly improve your website's overall conversion rates and isn't nearly as complicated as it may sound. Here's how to do it:\n1. Isolate website page variables.\nThere are two types of split testing protocols: A/B split testing and multivariate testing. A/B split testing, which pits two page variations with a single differentiating element against each other, can be a better choice for beginning webmasters than the more complex multivariate procedure.\nTo get started with A/B split testing, you'll need to choose a single variable to test. A few possible options include:\n- The wording, color, font, font size or position of your page's header text\n- The specific colors used in your website's background or design elements\n- The location of or wording used in your website's email newsletter opt-in box\n- The position, color or design of your brand's social media buttons\n- The color, design or location of any \"Buy Now\" buttons on your website\nRelated: How to Diagnose and Repair 'Conversion Rate' Problems on Your Website\nThe possibilities are endless so be sure to focus on elements that are likely to bring about the biggest changes in your website's performance. Tweaking the color of your links, for example, may not have as much impact as optimizing your on-site calls to action.\n2. Build web page variations.\nOnce you've selected a specific variable to test, create the different pages that will be pitted against each other. Keep in mind that, for now, you'll want to limit your changes to a single variable to conclusively determine which elements result in better website performance.\nIf, as an example, you've chosen to test two different wordings for your site's headline text, you'll need to create a new page that will challenge your existing version. If your site runs in HTML, simply copy your original page, make the necessary changes to your headline text and then upload the new page to your website using a different file name (as in, \"page-2.html\").\nAlternatively, if your website runs on WordPress or any other content management system, log in to your dashboard and create a new page using the same naming convention described above. Copy the text from your original page into your test page and then change the single test variable you're isolating with this A/B split test.\n3. Install Google's Content Experiments program.\nOnce your test pages are ready to go, log into your Google Analytics account and find the instructions for installing Google's Content Experiments program onto your website.\nThis free program replaces the older Google Website Optimizer service, but still retains many of the same useful features. Although the Content Experiments program can let you test up to five pages against one another, most beginning webmasters will still find it easier to stick to a single test variable across two page variations. That can let you identify the specific elements that have the greatest impact on website performance as quickly as possible.\nTo set up a Content Experiments test, first select the pages to include in your split test. Then, specify to Google how much of your traffic should be exposed to the experiment, as well as the overall goal of your test. Most webmasters will want to set this value at 100 percent because doing so will help generate test data as quickly as possible. Finally, you'll be given a small snippet of code to install on your website, and your test will go live upon your approval.\n4. Measure your results.\nAs your test proceeds, your results will be automatically updated within your Google Analytics account. However, there are a few things you'll want to keep in mind before making decisions based on this data.\nFirst, it's important to wait to make any decisions about which page variation is the \"winner\" until Google Analytics is able to generate enough data to predict statistical significance. Judging the results of your A/B split test after only a few page visits simply won't give you enough information to draw solid conclusions about the best way to improve your website.\nOnce you do identify a winner, either move on to a new split test variable or create additional page variations to challenge your winning content. You want to maintain a mindset of continuous improvement for your website, so always be testing.","Test: AB test\nThings to do before conducting the AB test:\n- Define which variables you are testing in your design (the difference between A and B)\n- Identify which metrics you want to focus on – is it performance metrics (which ones?) or self-reported metrics (which ones?)\n- Consider if you need a within- or between subject design for your test (see Within- & Between Subject)\n- For proper statistical analysis, you need to have at least 15 participants per version\nHow to setup an AB test in Preely\nIn Preely you’ll set up the AB test as you set up a regular test, see ‘Create test – step by step’ and ‘Test: Usability test’. You will need to set up two tests – one for each version. How you run the tests depends on if the test design is within- or between subject.\nFor the analysis, you should focus on the metrics you defined before conducting the test and compare them to each other.\nWhen working with statistics in AB testing, we work with two different kinds:\nDescriptive statistics is a way to summarize the dependent variable for the different conditions.\nInferential statistics tells us about the likelihood that any differences between our experimental groups are “real” and not just random fluctuations due to chance.\nFor most, it’ll be enough to use descriptive statistics.\nHow to perform descriptive statistics\nThe most common way to describe the differences between experimental groups is by describing the mean scores on the dependent variable for each group of participants. This is a simple way of conveying the effects of the independent variable on the dependent variable.\nOther tools that can be used are e.g.:\n- Bar or pie diagrams\nWant more advanced statistics?\nHow to perform inferential statistics\nEven though the mean scores of the experimental groups showed a difference, this can be due to chance. So the question is: Is the difference big enough so we can out rule chance and assume the independent variable had an effect? Inferential statistics gives us the probability that the difference between the groups is due to chance. If we can rule out the chance explanation, then we conclude that the difference was due to the experimental manipulation.\nTwo variables: If you have two variables you should use a Student’s t-test. Here we get the value t and we identify the probability (p) that the t-value was found by chance for that particular set of data, if there was no effect or difference. You can find different tools on the Internet and Excel can also be used, to calculate both the t- and the p-value.\nMore than two variables: If you have more than two variables you should use an ANOVA. Here we get the value F and we identify the probability (p) that the F-value was found by chance for that particular set of data, if there was no effect or difference. You can find different tools on the Internet and Excel can also be used to calculate both the F- and the p-value.\nThe smaller the p probability is, the more significant our result becomes and the more confident we are that our independent variable really did cause the difference. The p-value will be smaller as the difference between the means is greater, as the variability between our observations within a condition (standard deviation) is less, and as the sample size of experiment increases (more participants or more measurements per participant). A greater sample size gives our experiment grater statistical power to find significant differences.\nWithin your organization you need to decide when the p-value is significant, often we operate with a p-value that has to be less than 0.05 to be significant. Then, if the p-value is less than 0.05 we conclude that the results are not due to change, but an effect of the independent variable."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:dd17f18a-53d8-4306-84b9-15db31ccf2ad>","<urn:uuid:6c60ea6f-845d-448b-9fd1-3597a3804563>"],"error":null}
{"question":"What is functional fixedness bias, and how does it relate to investors' decision-making challenges with traditional vs behavioral finance?","answer":"Functional fixedness is a cognitive bias where people struggle to see beyond the typical use of an object or its parts. According to research, it involves overlooking four types of features: parts, material, shape, and size due to closely associated functions. In investment contexts, this bias relates to the broader challenge identified in behavioral finance theories, where investors fail to remain fully rational despite having all information available. While traditional finance theories assume investors make optimal rational decisions, research by Kahneman and Tversky showed that individuals are highly susceptible to cognitive errors even with complete information, making them largely irrational decision-makers.","context":["“The task is…not so much to see what no one has yet seen; but to think what nobody has yet thought, about that which everybody sees.” ― Erwin Schrödinger\nHere are the biases discussed so far:\n1. Confirmation Bias\n2. Hindsight Bias\n3. Negativity Bias\n4. Impact Bias and The Inaccurate Simulator\n5. The False Consensus Bias or “most people are like me bias”\n6. Attention Bias And The Tunnel Visioning Effect\n7. Optimism bias Or the Wishful Thinking bias\n8. Distinction bias\n9. Anchoring bias\nLet us get on with the cognitive biases:\n10. The Endowment Effect\n“Impossibility only lasts until you find new unbelievable hard evidences.” ― Toba Beta\nThe endowment effect or bias happens when you demand a lot more value to give up what you own. Your value of giving up something in comparison to your willingness to pay for it is out of proportion.\nIn other words, we demand more than we would be willing to pay for something because we own it.\nA research article titled “The Endowment Effect, Loss Aversion, and Status Quo Bias” by Kahneman, Knetsch, and Thaler explains this bias (Journal of Economic Perspectives, 1991).\nThe authors describe the bias by giving the example of a wine-loving economist friend. The economist purchased some high-quality Bordeaux wines at $10 a bottle, a low price point. Soon thereafter, the wines increased in value to $200 a bottle at an auction.\nThe interesting point about this example is that the economist would occasionally taste his own wines. But he refused to sell his wines at the auction price. He was also not willing to buy more wines at the auction price either.\nIn 1980, Thaler called this bias “the endowment effect.”\nThis effect was also called “the status quo bias” by Samuelson and Zeckhauser in 1988.\nLet us look at economist example above. The bias places the economist in a state of status quo or a preference for the current state. This preference makes him unwilling to either buy or sell the wine.\nThe authors also point out that as described by Kahneman and Tversky (1984), this bias is a possible manifestation of loss aversion.\nThis state of aversion to loss happens when the value of getting an object is less than the disutility of letting it go.\nWe find it hard to sell something because we perceive a greater value in the object than what it may be worth.\nWhat can we do about this bias?\nThe authors suggest:\n“The amendments are not trivial: the important notion of a stable preference order must be abandoned in favor of a preference order that depends on the current reference level. A revised version of preference theory would assign a special role to the status quo, giving up some standard assumptions of stability, symmetry and reversibility which the data have shown to be false. But the task is manageable.”\n1. Understand that we may have a problem of letting go of things because we perceive a greater value in them than the reality.\n2. We may also have a loss aversion mechanism in place that bolsters the endowment effect and keeps us locked in a current status quo.\n3. Ask yourself if the status quo and being locked into the perceived value and endowment is enhancing your life quality? Perhaps this bias is making you feel stuck. After all, what is the use of having things when you cannot enjoy them or do not allow others to enjoy them.\n4. Imagine the freedom that you would receive by getting a reasonable price for your object. And you can use the resources to enjoy other pursuits that you fancy.\n“The most important thing in communication is hearing what isn’t being said. The art of reading between the lines is a life long quest of the wise.” ― Shannon L. Alder\n11. Functional Fixedness\n“Human beings tend to be unable to estimate how biased they are.” ― Jean-François Manzoni\nIn my post on Motivation, I wrote the following:\n“In his TED talk, Dan Pink, the author of Drive, presents you with Karl Duncker’s candle problem. You are given a candle, thumbtacks in a box and matches. The objective is to attach the candle to the wall and prevent the dripping of wax on the table.\nMany people try to attach the candle to the wall using thumbtacks but it does not work. Some people try to melt the side of the candle and attach it to the wall but that does not work either.\nThe idea is to overcome “functional fixedness” and eventually people figure out a creative way to thumbtack the box to the wall and place the candle inside.\nThe trick is to have the insight to use the box as a candle-holder instead of just as a holder for tacks.”\nLet us look at this cognitive problem in greater detail.\nIn a study titled “Innovation Relies on the Obscure: A Key to Overcoming the Classic Problem of Functional Fixedness” by Tony McCaffrey in the journal Psychological Science (2012), the author analyzes this idea further.\nThe author says that insight problems demand that we see a different solution, something that is normally overlooked.\nHe gives the example of a toy insight problem where you have two steel rings that are weighty, a long candle, match and a two-inch cube of steel. Your goal is to fasten the two steel rings with the available materials.\nWhat should you do? The wax melted from the candle cannot hold the rings together because it is not strong enough to do the job.\nTo come to a solution, you will have to notice that the wick of the candle can function as a rope to fasten and tie the rings together.\nThis is an unconventional use for the wick. Once you figure this new function for it, you will find ways to scrape the wax away at the edge of the provided cube.\nHe adds that this kind of insight based problem solving happens in real life. Mechanical flight is a classic example.\nHumans could not achieve flight so long as they attempted to emulate the flight movement of a bird. But once they got beyond this functional fixedness, flight became possible.\nMcCaffrey also mentions Challoner’s work on insight problems and also real-world inventions. Challoner’s work shows us that innovative problem solving requires two steps.\nThe first step is to notice a new feature of the problem that is not frequently understood or used. The second step then would be to build a solution on that feature that remains obscure or hidden from plain sight. In fact, he calls it the obscure-features hypothesis of innovation.\n“The classic obstacle is functional fixedness, which has been described as the tendency to fixate on the typical use of an object or one of its parts (Duncker, 1945). On the basis of my examination of many inventions and insight problems, however, I characterize functional fixedness as the tendency to overlook four types of features possessed by a problem object (parts, material, shape, and size) because of the functions closely associated with the object and its parts.”\nHow can we get beyond functional fixedness? Many solutions are offered, including some from previous studies.\nHere are a few of them:\n1. You add new information on the old problem, a process that Ohlsson calls “elaboration” or you reinterpret old information or what is called “re-encoding.”\n2. You can use Knoblich’s technique of breaking the materials of the problem into smaller parts or what is termed as “chunk decomposition.”\nThe author argues, that this is not a complete solution to fixedness because even after decomposing objects, you will still need to see their function in a new way. You will need to get beyond the idea that the wick emits light in the example above.\n3. The author suggests a method called “the generic parts technique.” You will need to create a parts diagram and ask 2 questions through the process.\nThe first question is to simply ask if you can decompose or break the object further into smaller hierarchies. The second would be to ask if this new level and description can suggest a new use and provide a solution.\nThis analysis creates a generic description based on the shape and the material of the object. You will result in creating a tree with descriptions and potential uses. These descriptions and uses may allow you to see beyond functional fixedness.\nSo for the example above, create a diagram breaking the candle into wax and wick. Then you describe the wax and list potential uses.\nThen you describe the wick which is a string and list its uses. Ask questions such as: A wick is made of long interwoven strands that can be used for what?\nSo the decomposition allows you to describe the hierarchies of an object into parts. It also allows you to observe and analyze the generic descriptions and shapes of parts of the objects.\nThe results of the study indicated the following:\n- Using GPT allowed subjects to significantly solve insight problems. In these insight problems, functional fixedness is a limiting factor. The control group solved less problems.\n- Subjects were able to find and list additional features that would have remained obscure without GPT. This included the major factor that resulted in an innovative solution.\n- Parts, material, shape, and size are our allies in the quest away from functional fixedness. Describing them and finding uses is beneficial.\n- On the whole, GPT is a great innovation promoting technique to have in our toolbox of solving obscure and insight-based problems.\n“The rules of the universe that we think we know are buried deep in our processes of perception.” ― Gregory Bateson, Mind and Nature: A Necessary Unity\n12. Projection Bias\n“I prefer to rely on my memory. I have lived with that memory a long time, I am used to it, and if I have rearranged or distorted anything, surely that was done for my own benefit.” ― Leon Festinger\nIn a research article titled “Projection Bias in Predicting Future Utility” by Loewenstein, O’Donoghue and Rabin, (2003) in The Quarterly Journal of Economics, the authors describe this bias.\n“People exaggerate the degree to which their future tastes will resemble their current tastes.”\nThe authors say that if you want to make a great decision that is optimal, you may have to make a prediction of future states.\nThe problem is that we are not able to accurately predict for future states because of several factors.\nAmong them are daily mood fluctuation, and the possibility of social influences. We may also misjudge the possible change of environment.\nAn example they give is making vacation plans. You are making summer vacation plans and it is still winter. Will you choose overly warm places based on your current weather?\nWe may engage in this bias while making purchases too. Since satisfaction from purchases can fluctuate, we can overvalue or undervalue a product based on the day and our states.\nIn general we may over-predict the usefulness of an object. For example, we can overbuy groceries if we go to the grocery store hungry than if we go after a meal.\nWhat are some ways that you can get beyond this bias? Based on the article:\n1. Become aware and experience of this bias but the awareness may not be enough to get beyond it. We know that going hungry to the store can make us overbuy. But the current emotional state might just be overpowering and we still overbuy.\n2. The authors say that one decision in a certain state may not be enough to diagnose this bias. We may be able to observe many of our decisions based on our plans and actual behaviors. This may allow us to come to a conclusion on the patterns.\n3. Setting up of rules such as “never shop when hungry” is a demonstration of the awareness of this bias. These rules provide a specific moment-by-moment awareness and dealing with specific situations.\nAnd finally this quote by Adam Smith, also quoted in the article is a great description of the bias.\n“The great source of both the misery and disorders of human life, seems to arise from over-rating the difference between one permanent situation and another. Avarice over-rates the difference between poverty and riches: ambition, that between a private and a public station: vain-glory, that between obscurity and extensive reputation—Adam Smith, The Theory of Moral Sentiments\nNow over to you. Let me know in the comments below if these biases sound familiar and how you get beyond them.","Behavioral Biases and Their Effect on Investors’ Decisions\nMost important decisions we make in life involve two elements: the objective, measurable facts, and our subjective views on the desirability of what is there to gain and what is there to lose by our decision. There is no formula. Both elements are important and neither is sufficient by itself. It is no different when we make decisions about risk and investments. The challenge all investors face is that while objective elements can be tracked and measured, subjective elements are sometimes influenced by personal biases and can be made with irrational thinking.\nTraditional finance theories assume that investors are rational and make optimal investment decisions to maximize their wealth. However, behavioral finance theories indicate that when making investment decisions, investors fail to remain fully rational due to various psychological biases.\nIn their groundbreaking paper, “Prospect Theory: An Analysis of Decision under Risk” (1979), Daniel Kahneman and Amos Tversky convincingly demonstrate that even when all information is available, individuals are highly susceptible to cognitive errors. They concluded that human beings are by and large irrational decision-makers.\nAs human beings, we all have certain built-in biases. We will focus on the three most prevalent: confirmation bias, loss aversion, and herd mentality.\nPeople mostly will look for information or ideas that will validate their own beliefs. For example, when watching a TV channel, viewers will choose a channel that represents their political views, and avoid those with opposed opinions (CNN and Fox News). Many times even if the person reads the two sides of an argument/opinion, the confirmation bias will cause the person to only remember the information which confirms his thoughts about the topic. In investments as well, investors tend to stick to one view for a long time, validating it by reading only supporting research.\nSource: Billy Ireland\nIn their research, psychologists Amos Tversky and Daniel Kahneman found that people’s attitudes toward the probability of gains (“upside risk”) may be very different from their attitudes toward the probability of losses (“downside risk”). Most people feel the pain of loss much more than the joy of gains.\nOne example is the fear of flying. We all know that the chance of an airplane accident is significantly lower than the chance of an automobile accident, yet most people are still more afraid of flying than driving. Why is that? The answer is very simple, passengers give more weight to the magnitude of an accident than to the probability of an accident. In an airplane accident, the outcome will most likely be fatal.\nWe are all social animals. It is known that people do not like to be left out of a trend or a movement. The herd mentality can be seen in the financial markets. Just think about the recent “hot” investments in bitcoin and marijuana stocks, many investors joined the party late and suffered significant losses.\nHistorical examples of herd driven bubbles and busts:\nIs there a way to avoid falling into the biases trap?\nThe answer is no; We are all human and for that we cannot eliminate 100% of the influence of these emotional biases. We can, however, minimize their effect on our investment decisions by having a well thought out methodology that takes into account the objective facts as well as our subjective views.\nHere are some suggestions:\n- Having the appropriate tools to objectively determine potential risks and returns on an investment idea/asset are extremely important. CNBC and Bloomberg TV are not objective tools.\n- Define what risk is for you. Is it an asset price volatility, low return, or a full loss of principal? In the case of institutional investors, the risk may come from a decline in reputation or loss of job security.\n- Define in advance what risks you are willing to take and for what level of return. The best investment ideas are asymmetrical in terms of potential gain and loss, in your favor.\n- Remember you can always be, and many times will be, wrong. But be able to assess what the implications are of you being wrong and manage that risk accordingly.\n- Have a colleague with an opposing opinion “break” your investment thesis to test the strength of your idea.\n- When the fear and discomfort, or enthusiasm, you feel about an investment idea is shared by almost everyone else, go back to the drawing board and retest your theses.\nJust by following these simple guidelines for decision making, you’ll be doing better than the majority of investors in the market place. Underlying fear and greed are the propellers that drive the engine – so what investors have to remember is that going against the herd is often difficult, but the best decision. We all know to buy low, and sell high – but putting that into practice is more difficult than we think."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:44bc4259-3214-48e9-aacb-8210454b8c78>","<urn:uuid:6560f05a-d6fb-4585-b77f-5158f93815dd>"],"error":null}
{"question":"I work with deaf communities - what programs are available for deaf audiences at major cultural venues, and how do they support deaf artists?","answer":"Cultural venues offer comprehensive programs for deaf audiences. The Art Gallery of South Australia provides Auslan (sign language) videos, weekly interpreted Tuesday lunchtime talks, and monthly weekend collection tours with interpretation. They've notably involved deaf artists, such as William Maggs, an emerging artist who won the SALA award and participated in creating their Accessible Guide. They even featured two Deaf Aboriginal Guides during their 2019 Tarnanthi Festival. Victory Gardens Theater similarly provides ASL interpretation for select performances and has been recognized for their accessibility efforts, being named Most Accessible Theater by Deaf Illinois in 2009.","context":["The Art Gallery of South Australia (AGSA) has launched an Accessible\nGuide, developed for Deaf, hard of hearing, blind and vision-impaired audiences. The Accessible Guide gives these audiences the opportunity to watch Auslan videos, read transcripts and listen to Audio Descriptions of works of art.\nThe Guide has commentaries on thirteen pieces of art on display in in the Elder Wing of Australian Art which showcases the work of some of Australia’s finest artists from 19th Century.\nDesigned for in-gallery use, the Accessible Audio Guide is also available in its entirety through AGSA’s website, as part of the gallery’s initiative to enable access to works of art from home, for all audiences. It has been developed for computer and handheld devices anywhere to ensure that anyone, including families and friends who may have different needs, can have an enjoyable social experience together.\nKarina Morgan – Education Support Officer and Auslan interpreter at AGSA, tells us more about AGSA’s Accessible Guide.\nCould you tell us a bit about AGSA’s Accessible Guide?\nThe Accessible Guide provides another way for our diverse audiences to be able to engage with AGSA’s collection using their own smartphones, tablets or desktop computers straight from our website, without having to download any apps.\nA selection of works of art from our Australian collection is presented in three tours – self-guided audio, audio description and Auslan videos. Transcripts accompany the audio, and closed captioning for the videos.\nCurrently audiences are able to engage with the Accessible Guide remotely – from the comfort of their own homes – and when the building re-opens in June, audiences onsite will also be able to use this guide in front of the physical works of art.\nWhy did the AGSA to create the Accessible Guide?\nOver the past three years in particular, AGSA has been steadily increasing our Deaf, hard of hearing, blind and vision impaired audiences so that the artistic, historical and cultural experiences are friendly to all visitors.\nWe acknowledged that for these audiences rich information is often not accessible at cultural institutions, particularly in South Australia. In fact, feedback we received was that many people from these communities feel so unused to having their needs met that they have felt disengaged until regular programming was put in place.\nThe Accessible Guide also connects to our Reconciliation Action Plan which aims to promote understanding and respect through the power of art and provide more opportunities for the community to learn about the diversity of First Peoples art and cultures. AGSA is proud to be the first state gallery in the country to prepare a RAP – one which includes Auslan actions.\nWhat is the AGSA hoping to achieve by developing the Accessible Guide?\nAGSA designed the Accessible Guide with all users in mind so that it is enriching and satisfying for all. We are not just building for accessibility; we wish to create a social experience so that a range of diverse families and friends can enjoy the Gallery together – not just at programmed times but at any time they are able to visit.\nWe hope this also encourages remote users to learn about our collections with the aim of visiting in person, perhaps attending one of our regular Access Programs.\nFor those who are unable to visit us in person, we hope this provides a pathway to connecting in a meaningful way with our wonderful collection.\nWhat makes you the most excited about launching the Accessible Guide?\nAn important part of this project was the community consultations. We wanted to ensure that we put users at the forefront. We didn’t want to just build an experience for these communities, but with them as well, allowing findings from these sessions to guide us in our next step, taking on board any feedback.\nThe Auslan video presenters came from the South Australian Deaf community, including a young emerging artist, William Maggs who was awarded the main SALA award for his self-portraiture in AGSA’s student drawing exhibition in 2019.\nI feel that together we have created an Accessible Guide that everyone can feel proud of and I am excited about increasing engagement with diverse communities, and to expand the Accessible Guide in the coming years ahead.\nI would love to turn around the initial comments about some sections of our audiences feeling like their access requirements are not met and that we see more engaged people like emerging artist, William Maggs, connecting with us and our collections.\nHow do you feel the Access Guide will assist Deaf and disabled arts audiences to connect with art pieces at AGSA?\nThe Accessible Guide allows our audience to choose their preferred method of communication – whether this is in English or Auslan, and also provides blind and vision impaired users an option to receive more information through an audio description. We hope to be able to increase access by adding an easy read/listen guide option for audiences who may have intellectual or cognitive disabilities, or for those who identify as being neuro-diverse. Ultimately this would also be useful for English as a Second Language (ESL) users as well.\nWhat other access services does AGSA provide to Deaf and disabled arts audiences?\nI am a certified NAATI interpreter and I interpret the weekly Tuesday lunchtime talks, and provide Auslan interpreted collection/exhibition tours one weekend tour every month. This audience has been continuously growing with between 5- 20 Deaf audience members participating every month. Special additional Auslan interpreted talks are also held during large exhibitions. Auslan tours will also be imbedded into other Gallery programming areas such as the Start program (for Families and Children), and Neo (for teenagers). The 2019 Tarnanthi Festival of Contemporary Aboriginal and Torres Strait Islander Art had for the first time two Deaf Aboriginal Guides leading the Auslan tours.\nAnother staff member, Ryan Sims who also works in AGSA Education, has undertaken Audio Description training and has developed and maintained connections with the South Australian School for the Vision Impaired (SASVI), as well as implementing a program of regular Audio described and Tactile tours.\nOther Access programs we have initiated, include our Art and Dementia program run by our dedicated Gallery Guides. AGSA has also offered training to the National Gallery of Australia in 2019. We also currently have partnerships with selected residential care facilities and anticipate building upon this program to include more members of our community.\nTo visit AGSA’s Accessible Guide, please click here.","Victory Gardens Theater Access Project\nThe Access Project is a nationally recognized model outreach effort designed to involve people with disabilities in all aspects of theater, both on and off the stage. Originally started at Remains Theatre in 1992, it transferred to Victory Gardens in 1995. In 2008 Victory Gardens received the MetLife Foundation Award for Excellence and Innovation in Arts Access, and in 2009 we were named Most Accessible Theater by the members of Deaf Illinois.\nA LITTLE BIT NOT NORMAL\nASL Interpreted performance: Saturday, May 20 at 7:30pm and Saturday, May 27 at 3:00pm\nWord for Word (Open Captioning): Sunday, May 14 at 3:00pm, Saturday, May 20 at 7:30pm, Saturday, May 27 at 3:00pm\nAudio Description: Saturday, May 20 at 7:30pm\nWHERE DID WE SIT ON THE BUS?\nWord for Word (Open Captioning): Thursday, May 11 at 7:30pm and Saturday, June 3 at 3:00pm\nAudio Description: Thursday, May 11 at 7:30pm\nWord for Word (Open Captioning): Thursday, May 25 at 7:30pm and Sunday, May 28 at 3pm\nAudio Description: Thursday, May 25 at 7:30pm\n- Artist Development Workshops, which teach the art of playwriting and performance to individuals with and without disabilities.\n- Play Development Opportunities for original works dealing with disability issues or written by playwrights who have disabilities.\n- The Access Project Presents a series of performances, readings, movies and other events that promote, explore and celebrate disability culture.\nThroughout the year, the Victory Gardens Access Project designates certain performances as “Access Nights” where we offer additional accessibility services to our patrons. Read more about the services we offer below.\n- Accessible Seating: If you require wheelchair accessible and/or companion seating, or have any hearing, visual or other needs, please let us know how we may assist you with your ticket order when you contact us. If you have a service dog please let us know in advance so that we can make sure you and your canine companion have the best experience possible.\n- Assisted Listening Devices: In both of our performance spaces we provide assisted listening devices. In the Zacek McVay Theater we utilize the Sennheiser infra-red Personal Listening System. The systems provide amplification on lightweight wireless headsets and features the ability to adjust for specific hearing needs. Please contact the House Manager to receive a headset. A form of identification is required as a deposit. This system is available free of charge.\n- Audio Description: This service allows blind and low vision patrons to hear a description of the visual elements taking place on stage. Listeners will hear a description of actions, body language, lights, costumes, scenery and other aspects of the production not conveyed by voices and sounds from the stage. This service is available for specific performances.\n- Touch Tours: Available one hour and a half before each Audio Described performance. Touch Tours are one hour artistic conversations and tactile explorations of costumes, props and set pieces for patrons who are blind or low vision. Please make reservations for the Touch Tour when you order your tickets.\n- Braille Programs: Are available at the Box Office. Subscribers may reserve these on their performance\ndate by calling the Box Office.\n- Large Print Programs: Are available at the Box Office. Subscribers may reserve these on their performance date by calling the Box Office.\n- American Sign Language Interpretation: For patrons who prefer American Sign Language to communicate, Victory Gardens provides ASL interpreted performances with theatrical interpreters.\n- Word for Word Captioning: Victory Gardens offers word for word captioning for select performances. At captioned performances, text is displayed on the side of the stage showing what the actors are saying and describes sound effects on stage.\nStrawdog to Offer Fully Accessible Performance of FAIL/SAFE at Victory Gardens. Read More\nThe Unabridged Experience Services help disabled theater-goers get the full experience. Read More\nArtful disabilities act Professor expands the perception of people with disabilities in arts and culture. Read More"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:b2dc8eaf-69f4-4cd8-ae80-50d9c283fae4>","<urn:uuid:8553dc91-f360-4e43-8f30-644e7bbda965>"],"error":null}
{"question":"How does an optical gyroscope measure rotation using light beams?","answer":"An optical gyroscope measures rotation by using the Sagnac effect. The process involves splitting a beam of light into twin beams that travel in opposite directions along a circular pathway until they meet at the same light detector. Since light travels at a constant speed, when rotation occurs, one of the beams arrives at the detector before the other. This timing difference, known as a phase shift, is then used to calculate the orientation.","context":["The new optical gyroscope, shown here with grains of rice, is 500 times smaller than the current state-of-the-art device. Image: Ali Hajimiri / Caltech\nTiny Gyroscope Aims to Improve Wearable, Nano Technology\nFeb 7, 2019\nby Mark Crawford ASME.org\nA more stable, accurate and powerful gyroscope – no larger than a grain of rice – could soon make its way into wearable devices, nano platforms, and other applications.\nGyroscopes enable vehicles, aircraft, robots, wearable, and other electronic devices to determine location in three-dimensional space. In most cases, the data is generated by microelectromechanical sensors (MEMS) that measure differences in the forces that act on oscillating masses moving in opposite directions. But those gyroscopes have several limitations, including their sensitivity to such factors as vibration and temperature.\nTo counter this, optical gyroscopes were developed and made with no moving parts. Most of them are smaller and more accurate than MEMS gyroscopes, making them good candidates for miniaturization and nanophotonic platforms. But even the smallest high-performance optical gyroscopes available today are too large for many portable applications. Another drawback is their signal-to-noise ratio, which is generally limited by thermal fluctuations, component drift, and fabrication mismatch. Due to those drawbacks and others, integrated nanophotonic optical gyroscopes have yet to be developed. That could change.\nResearchers at the California Institute of Technology have taken a big step toward making those tiny gyroscopes a reality. Led byelectrical and medical engineering professor Ali Hajimiri, the team developed a proof-of-concept optical gyroscope that is 500 times smaller than current state-of-the-art devices, or about the size of a grain of rice. It is also vastly superior in precision, being able to detect “phase shifts” that are 30 times smaller than what current systems can detect, enhancing its performance by one to two orders of magnitude.\nOptical gyroscopes measure the rate of rotation by detecting differences in two paths of light—a phenomenon known as the Sagnac effect. A beam of light is split into twin beams which travel in opposite directions along a circular pathway and meet at the same light detector. Because light travels at a constant speed, rotation causes one of the beams to arrive at the detector before the other. This phase shift is then used to calculate orientation.\nHajimiri’s all-integrated, nanophotonic optical gyroscope achieves its improvements through \"reciprocal sensitivity enhancement.\" In this technique, reciprocal means that both beams of light are affected the same way. In Hajimiri’s gyroscope, light moves through miniaturized optical waveguides, which are small conduits –about 200 nanometers tall, or about three times the width of a human hair - that carry light, acting in the same way as wires that carry electricity. Imperfections in the optical path that could affect the light beams - such as thermal fluctuations, light scattering, and other interference – affect both beams in the same way.\nOur proof-of-concept device is capable of detecting phase shifts 30 times smaller than state of-the-art miniature fiber-optic gyroscopes, despite being 500 times smaller in size.\nAli Hajimiri, California Institute of Technology\nHajimiri's team found a way to extract reciprocal noise while leaving signals from the Sagnac effect in place. That enhances the gyroscope’s reciprocal sensitivity, which improves the signal-to-noise ratio. Those improvements enable the integration of the gyroscope onto such a small chip.\nThe lab built the system using silicon nanophotonics, “which is an ideal platform for implementing optical gyroscopes on the microscale due to its reliability and compatibility with current mass production techniques,” Hajimiri said. It also provides the capability of integrating nanophotonic and electrical components onto a single substrate, which helps with commercialization and controlling costs.\nThe optical gyroscope has no moving parts, making it an extremely robust system. System dimensions are 2 by 1 by 2 millimeters. The build process deploys lithography to stack and pattern several ultra-thin layers of silicon and silicon-dioxide.\n“The only thing that moves inside the gyroscope are photons that travel at the speed of light in a pair of very small loops hundreds of billions of times every second,” Hajimiri said. “Extremely small changes in path length due to rotation can be measured.”\nThe greatest challenge in designing the system was accurate detection of variations in the optical path due to rotation, while minimizing sensitivity to thermal fluctuations and drifts.\n“We did this by employing a technique we refer to as reciprocal sensitivity enhancement, where the light travels in two opposite orientations [clockwise and counterclockwise] in two loops for half of the cycle, and then travels in the opposite direction for the other half,” Hajimiri said. “This way the effect of non-idealities in the system can be cancelled, while the signal is amplified.”\nTests show theproof-of-concept device is capable of detecting phase shifts 30 times smaller than state of-the-art miniature fiber-optic gyroscopes, despite being 500 times smaller in size. “Thus, our approach is capable of enhancing the performance of optical gyroscopes by one to two orders of magnitude,” Hajimiri said.\nThe team is already working to develop the next-generation of its current prototype.\n“The level of interest we have received so far is very high,” Hajimiri said. “Many parties have shown interest in working with us toward further development of this technology.”\nMark Crawford is an independent writer.\nMore Exclusive Content from ASME.org:\nGlobal Talent Makes American Industry Stronger\nPolymer Composite Can Regulate Its Own Temperature\nHow Engineers Can Resolve Conflict and Save Their Business"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:84adb165-ad35-4f16-a8ad-50bc0ac076d5>"],"error":null}
{"question":"How do the preparation stages compare between BIM process implementation and EHR implementation in terms of evaluating organizational readiness?","answer":"Both implementations emphasize early evaluation but with different focuses. BIM process implementation starts with analyzing current processes and technological needs, creating a baseline to identify areas for improvement, and ensuring computer/network capabilities meet software requirements. In contrast, EHR implementation readiness assessment focuses more on technical testing (hardware, interfaces, system testing) and training verification, particularly ensuring super-users are prepared and contingency plans are in place. Both approaches emphasize staff engagement, but EHR implementation puts additional emphasis on operational leader involvement and cross-functional department engagement.","context":["By Jami Woebkenberg, MHIM, RHIA, CPHI, FAHIMA; Sandra Hernandez, MSMIS, RHIA, CPHI, CCS, CHC; and A. Andrews Dean, CPHIMS, CHDA, CPHI, CPPM, CPC\nTechnology is a major part of our lives and healthcare is certainly not exempt from its influence. Under the 2009 HITECH Act, eligible hospitals and providers are required to implement and adopt an electronic health record system (EHR) that meets certain criteria and is meaningful to its users. Although health information management (HIM) professionals all know how important it is to have a project plan for the implementation, it is easy to forget the human element that is also involved with such a big change. This article will review the basic steps of project planning, including change management techniques, to assist in a successful transition.\nThe success or failure of any project can be directly tied back to how the project is managed, both how the implementation of the technology is managed as well as how people are managed. Every project has four basic phases in its lifecycle:\nThe following is a high-level explanation of each phase of the project lifecycle. The initiation phase of the project generally includes development of the business need and creation and approval of a charter for the project. In the planning phase, the project manager is assigned and the project team is created. In addition, the project plan is developed and approved. The execution phase is normally the longest phase of the project since it is where the majority of the actual project work takes place. During this phase, the team will build the deliverables of the project. This may include review of existing processes, workflows, and policies and procedures, as well as the creation of new ones. The execution phase includes testing and training on any new technology, enlisting support of stakeholders and champions, engaging individuals in the changes taking place, communicating the changes, and actual implementation. Depending on the scope of the project, the execution phase may include additional steps. The final phase of the project life cycle is closure. During this phase, the project team discusses lessons learned, wraps up any final details of the project, and, most importantly, celebrates or possibly mourns that the project is complete.\nManaging Change Through People\nProject managers and members of a project team have a mutual end goal in the implementation lifecycle, sharing a focus and understanding of the technical components for successful completion. Within the span of any implementation are the people who will be using the new technology and the processes they manage. Oftentimes, these are considered operational users across different functional departments.\nEngagement of operational departments is critical to the overall success and sustainability of any implementation. Involving people from cross-functional areas during all stages, from product selection to optimization and support, gets not only buy-in but an educated workforce who understands their role and responsibilities along the way.\nOperational leaders can be included in:\n- Early discussions of technology needs within the organization\n- Vendor product presentation sessions\n- One idea is to have vendors come to the facility and set up presentation tables or booths and invite operational leaders to see them all side-by-side on the same day. Leaders can complete surveys providing feedback on likes and dislikes for each vendor.\n- Ongoing technical project meetings that impact functional areas\n- Use-case review meetings\n- Go-live readiness checkpoint discussions\n- Change control meetings\n- Live-day technical meetings\n- Post-live and support meetings\nDesignated departmental super-users can be included in:\n- Use case review meetings during the project lifecycle\n- Test script creation discussions\n- End user testing activities\n- Data cleanup activities\n- Post-live and support meetings\nIncluding operational staff at different levels allows them to become change agents in support of the end goal. Adopting a people-centered or user-centered approach influences the perception and acceptance of the technology and the changes that come with it.\nMonitoring Readiness and Effective Pre- and Post- Go-Live Practices\nAssessing readiness is one of the most critical factors to consider during planning. To provide adequate end user and provider support, there are essential factors to consider. For example, organizations must identify, train, and select operational leaders, super users, and trainers and then relieve them from their normal responsibilities during go-live—as well as commit to providing support as needed. Organizations must establish adequate support for every department and unit when creating its support schedule. Run through checklists and utilize consultants who have experience going through implementations to learn what problems to anticipate and address beforehand.\nBroadly, go-live readiness can be categorized into three areas:\n- Technical readiness\n- Effective training\n- Post-go-live support and planning\nThe following content contains summarized excerpts and highlights from the National Learning Consortium’s EHR Implementation Go-Live Planning Checklist, as well as best practices and helpful hints from resources available through the AHIMA HIM Body of Knowledge. Full checklist details are available from healthit.gov.\nSeveral Days Prior to Go-Live: Review Technical and Training Readiness\n☐ Technical Readiness → Review testing\n- Hardware: computers, monitors, cables, printers, scanners, other peripheral devices, servers, uninterruptable power supply (UPS), storage, backup server\n- System testing to ensure data pass from one function to another: Interfaces, lab, radiology, billing/clearinghouse, admission/discharge/transfers as applicable, other\n- Tasking, ordering, and e-prescribing\n☐ Effective Training → Check and re-check training\n- Ensure super-users are prepared and well-trained to provide support to all users\n- All users have completed the training necessary for their role\n- Plan with the support team what to do if go-live goes awry, and identify contingency plans or points where go-live needs to be stopped\n- Have proper processes and backups ready in the event of downtime or other major issues\nImmediately Prior to Go-Live: Technical Readiness and Effective Training Come Together\nAfter you finish developing, training, testing, and customizing, you are almost ready to go live—but assessing readiness again a few weeks prior to implementation will help avoid costly mistakes such as diverting subject matter experts to deal with workflow issues during go-live. Some essential tips include:\n- Verify the schedule for go-live day, including contacting users to check readiness and speaking with the help desk staff, super-users, and operational leaders\n- Verify support and system readiness for support tiers and review escalation procedures to follow in the event of a problem\n- First level, second level, third level, or others\n- Define and reinforce who within the organization has the authority to make/approve critical system changes in the live/production environment\nDay of Go-Live: Full Speed Ahead!\n- All staff members should plan to arrive early to support the first go-live, and the support team should re-check all readiness procedures\n- Determine staff scheduling and resources for subsequent days and adjust as necessary; go-live may require several days of similar scheduling\n- Ensure implementation and support teams are ready for post-go-live support\nCommunicate Before, During, and After Go-Live\nWhile communication is important before and after go-live, it is absolutely vital during go-live. Issues are going to arise, and there will be user frustration and a learning curve for real-time point-of-care EHR use. Making sure all end users understand the issues, reporting problems, and system errors provides valuable feedback to the EHR team. If users do not feel adequately heard, they may mentally abandon the new health technology and go back to old habits, which can be difficult to overcome.\nTrust is also necessary at go-live; if end users sense that they are not being supported and that the EHR is not working for them, then a decline of buy-in and use could also hinder the effectiveness of the system. If end users are understood and trust that support is available and ready, buy-in will be stronger, which can be used to further the project and overall team morale.\nIn almost every implementation, there will be users who are not as prepared or not as engaged and happy with the technology. However, having a well-managed project, involving key players in the various project phases, and providing thorough training will certainly help with the implementation. Communication at all stages is important, especially allowing users to share their concerns and be involved in any remediation and resolution planning. Following these basic steps can provide the tools necessary for a smooth and successful implementation.\nAHIMA HIM Practice Transformation Work Group. “Checklist for Assessing HIM Department Readiness and Planning for the EHR.” Journal of AHIMA 76, no. 6 (June 2005): 56E-H. http://library.ahima.org/doc?oid=104177.\nGlondys, Barbara and Lesley Kadlec. “EHRs Serving as the Business and Legal Records of Healthcare Organizations (2016 Update).” Journal of AHIMA 87, no. 5 (May 2016): extended online version. http://library.ahima.org/PB/BusinessRecord.\nMethod 123. “Project Management Life Cycle.” 2018. http://method123.com/project-lifecycle.php.\nNational Learning Consortium. “Electronic Health Record (EHR) Implementation Go-Live Planning Checklist.” March 31, 2012. www.healthit.gov/sites/default/files/tools/nlc-ehr-implementation-go-live-planning-checklist.docx.\nRizer, Milisa K. et al. “Top 10 Lessons Learned from Electronic Medical Record Implementation in a Large Academic Medical Center.” Perspectives in Health Information Management (Summer, July 2015). http://library.ahima.org/doc?oid=301175.\nWang, Tiankai and Jackie Moczygemba. “Risk Management in EHR Implementation.” Journal of AHIMA website, May 1, 2015. http://bok.ahima.org/doc?oid=301018.\nJami Woebkenberg (firstname.lastname@example.org) is the senior operations director, HIM services, at Banner Health. Sandra Hernandez (email@example.com) is the director of coding at OrthoAtlanta. A. Andrews Dean (firstname.lastname@example.org) is a health IT consultant.\nWoebkenberg, Jami, Sandra Hernandez and A. Andrews Dean. “Remember the Human Element of Change Management.” Journal of AHIMA 90, no. 5 (May 2019): 30-31.","How to succeed in implementing BIM process Part 2\nThe Four Phases of a Successful Implementation of a New Process\nPhase 1: Preparation and Evaluation\nAnalyse your current processes and abilities, create a baseline to help you evaluate areas of attention as part of the new process implementation process. A review the current operations will assist you in recognising and addressing area’s where improvements can be made that have the biggest impact.\nEvaluate your current technological needs, make sure your computers and network will be able to meet the performance requirements of any new software. Research what you need and plan accordingly in preparation for moving forward, best to do this early on to minimise staff’s frustration and the impact of upgrading equipment.\nEngage your staff, take advantage of the resource by recognising existing knowledge and understanding, the inclusion of staff is very important to gain their confidence and trust in the implementation.\nPhase 2: Establish Goals and Milestones\nObjectives, Stages & Milestones; specific policy objectives, intermediate capability stages, and measurable maturity milestones separating current status from a quantifiable future target.\nTo measure your progress and success you need to establish goals and milestones, these should include both short-term and long term goals. Each organisation has an ultimate ambition and long-term goal when it comes to adopting a new process. Based on the ultimate ambition, intermediate goals need to be defined together with measurable progress indicators and targeted milestone. It is important to set achievable goals and milestones, to avoid discouragement taking over a successful adoption.\nPhase 3: Define the Process\nThrough defining your goals and milestones you’ll be able to use these to help you start to clearly define the implementation process, typically the process can be broken down into three categories including; People, Process and Technology. Breaking the process down into these three categories will help you define the processes and clearly outline each the steps and how they relate to each other.\nPeople are crucial to the success of implementing any kind of new process, for this to be successful you need to gain their confidence and trust that the implementation of any new processes is an improvement over the old. Identify when, how, who and what training is needed to reach the next milestone.\nThe biggest hurdle for any organisation is the change in culture, by undertaking effective “on demand” training combined with “hands-on” expertise to assist and reassure staff that they have somewhere to answer questions and play a supportive role.\nTraining is an investment in your team and your organisation.\nAs your staff develop their skills and an understanding of your goals and objectives, you will start to see confidence develop.\nInternally Look for Drivers & Champions, people within your organisation that are enthusiastic and supportive of changes that make an improvement. These individuals will demonstrate a willingness to participate in the adoption and seek out efficacy and innovation in the system and process.\nIf your new process or workflow involves new software, look for competent educators and learning resources that cover the concepts, tools and workflows. These can be either delivered through tertiary education, vocational training, professional development or by training sessions held by “in house” champions.\nDevelop processes that are flexible, manageable and can evolve alongside your organisation and the developing industry. Implement the process gradually and have key adopters take the lead and encourage the change in culture.\nTechnology is the tools of our trade, having the right tools allow us to achieve our goals. Having inadequate tools not only limit production but also play a major factor in staff moral. Technology plays an important role in any organisation. Consider future expansion while measuring against the immediate needs. Balance the need verses associated costs, review accessibility and affordability of upgrading necessary hardware and upgrades to software and network systems.\nPhase 4: Implementing and Monitoring\nOnce a certain level of comfort is reached, the capabilities and process should be assessed and reviewed through developing metrics for benchmarking project outcomes and assessing the capabilities of individuals, organisations and teams.\nThe team should not only have a process to follow but also have available to them the resources to be efficient in their tasks. Having unreliable resources, or worse yet resources your team are unable to find, gives them permission to create their own content, essentially disregarding any quality control and duplicating work already completed.\nInvest in the time to fully evaluate your existing processes, what works, what doesn’t work and where gaps appear in the processes. Through a thorough review of an existing process you will be able to clearly define the flow of operations and the impact BIM has to all aspects of business. Review your own processes with fresh eyes to see where you can make improvements, look at it from the standpoint of production and what resources you would need to efficiently complete the task at hand.\nMeasurement & Optimization\n- Make the process easy to follow, keep it clear and easily understood, don’t make a process too constraining or onerous or you’ll find that no one will follow it.\n- Make your process flexible to accommodate a variety of situations or your staff’s needs.\n- Provide information on the process in a variety of formats, such as online, printed booklet form, pdf etc. Make it readily accessible to everyone in formats they can relate to, too encourages adoption.\n- Having management promote and endorse the process is the key to a successful adoption.\nAdoption of a new process takes time, continual promotion through encouraging awareness and engagement of the processes until it becomes part of the culture. Monitor your team, provide constant reminders that that will encourage the development of a culture that follows the processes.\nFinally, be patient and flexible. You’ll need both to successfully implement change."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:bebb28bc-c822-4005-9e77-7078aa789330>","<urn:uuid:45b5d6e4-f5b1-4438-a585-18468536634b>"],"error":null}
{"question":"What are the key differences between Form 1099-MISC and Form W-2 in terms of tax reporting?","answer":"Form 1099-MISC and W-2 differ in several ways. W-2 is used for employees and lists wages, salaries, or tips, along with all taxes withheld, including Medicare and Social Security payments. In contrast, 1099-MISC is used for sole proprietors and typically has no deductions listed since the person is not an employee. Both forms serve to report income, but 1099-MISC is used when a business pays $600 or more to non-employees like freelancers, while W-2 is specifically for regular employees.","context":["What Is Tax Form 1099-MISC?\nTax Form 1099-MISC is commonly used among self-employed professionals to report profits from services performed for other organizations. If you are a sole-proprietor and were paid more than $600 for services during a given tax year, the business you worked for is required to send Tax Form 1099-MISC.\nThere are many details of Tax Form 1099-MISC that you need to be aware of. Upon receiving this tax form, you should immediately review the box for “non-employment compensation.” This is the actual amount of money that you earned by working for the company. It is your responsibility to report this income on your tax return. The IRS receives the same form, so they will know if you failed to report this income.\nGenerally speaking, Tax Form 1099-MISC is very similar to Tax Form W-2. The only difference is that a 1099-MISC is for sole-proprietors while a W-2 is for employees.\nWho Must File Form 1099-MISC?\nYou must file Form 1099-MISC for each person to whom you have paid during the year:\n- At least $10 in royalties or broker payments in lieu of dividends or tax-exempt interest\n- At least $600 in rents, services, prizes and awards, other income payments, medical and health care payments, crop insurance proceeds, cash payment for fish you purchase from anyone engaged in the trade or business of catching fish, or, generally, the cash paid from a notional principal contract to an individual, partnership, or estate;\n- Any fishing boat proceeds; or\n- Gross proceeds of $600 or more paid to an attorney\nYou must also file a 1099-MISC Form to report any direct sales that you made of at least $5,000 of consumer products to a buyer for resale anywhere other than a permanent retail establishment.\nAdditionally, you must file a Form 1099-MISC for each person from whom you have withheld any federal income tax under the backup withholding rules regardless of the amount of the payment.\nIRS Tax Form 1099-MISC (Miscellaneous Income)\nWhat if I don’t receive Tax Form 1099-MISC for a company that paid me more than $600? Contrary to popular belief, this does not mean that you are allowed to leave this income off your tax return. The same holds true if you earned less than $600. You still have to report this income, even though the company you worked for is not required to provide you with Tax Form 1099-MISC.\nIf you receive Tax Form 1099-MISC and realize that the income reported is incorrect, it is important that you immediately contact the issuing business so that they can send both you and the IRS an updated form. Make sure you wait for a corrected 1099-MISC before you file your income tax return.\nIt is not uncommon to receive your Tax Form 1099-MISC after you file your taxes. In this case, you do not have to do anything as long as you reported the income on your Schedule C. Of course, if you did not report the income you will need to send an amended return including the information from the newly received 1099-MISC.\nThere are two types of people who deal with Tax Form 1099-MISC: those who work as a sole-proprietor and those who hire these people.\nDon’t make Tax Form 1099-MISC more complicated than it has to be. This is a common tax form used by sole-proprietors and businesses all over the country.","What Is Form 1099? It Reports Payments Other Than Regular Salaries, Wages or Tips\nA form 1099 is an IRS form used for filing taxes. It is used by companies to report payments made other than regular wages, salaries or tips (which are reported through a W-2 form). A 1099 is generated by the payer and must be sent to the payee by January 31st of each year, to reflect transaction amounts from the previous year.\nThere are many different types of 1099 forms, the one used depends on what the payment was for.\nHere’s What We’ll Cover:\nWho Gets a Form 1099?\nThe most common 1099 form a company issues is a 1099-MISC. A business is required to fill it out for anyone they have paid $600.00 or more to, in a calendar year who is not an employee. For instance, if a company has paid 50 different contractors over $600.00, then it is required to fill out and provide 50 forms\nIt applies to payments made for:\n- Freelance workers\n- Cash prizes and awards\n- Other income payments\n- Medical and health care payments\n- Attorney payments\n- Crop insurance proceeds\n- Cash payments for purchase of fish from commercial fisher\n- Fishing boat proceeds\n- Cash paid to an individual, partnership or estate from a notional principal contract\nIf a company withheld any taxes on contractor payment (regardless of the amount paid), then a 1099-MISC form must be issued.\nThe forms need to be distributed by January 31st (to reflect the amounts paid in the previous tax year) to the recipients of the money.\nOther often-used 1099 forms include:\nA 1099-K form is a required form third-party payment processors must file when the sales volume is greater than $20,000 and 200 transactions in a year. An example of a third-party payment processor is PayPal.\nA 1099-K form is also required for debit, credit, and stored-value card transactions. “Store-value” cards are prepaid cards.\nThis form is required when distributions greater than $10 have been made from a:\n- Pension plan\n- Retirement plan\n- Survivor income benefit plan\n- Insurance contract\n- Charitable gift annuity\n- Profit-Sharing plan\n- IRA (Individual Retirement Account)\nA 1099-R is also required for permanent and total disability payments (under a life insurance contract).\nLike the label implies, this is for interest earned in savings and checking accounts, as well as interest paid on savings bonds and treasury bills. The amount earned must be greater than $10.00.\nWhat Is the Difference Between 1099 and W2?\nA W-2 form is a form that a company must fill out for each of its employees. This form reflects wages, salaries, or tips.\nThe W-2 is sent by the company to both the employee and the IRS, and lists the amount of income the person earned, plus all the taxes withheld, including payments to Medicare and Social Security. The information from this form allows an employee to calculate his own personal income taxes, and to know if he owes any money, or if he should be receiving a refund.\nA 1099 is a form a company generates for payments to everyone else. Typically, there are no deductions listed on these forms, because the person receiving the form is not an employee.\nCan I Get a Tax Refund with a 1099?\nA 1099 form is a document that reports income, which may increase taxes owing, not decrease them or result in a refund. As such, the 1099 form helps a taxpayer to complete his personal income tax return.\nHow Much Money Do You Have to Make to Get a 1099?\nIt depends on the type of income one is receiving.\nWhether a company generates a 1099 form or not, does not change whether the individual or business (that received the money) should report it or not. All money earned must be reported to the IRS.\nHow Do I Send 1099 to IRS?\nAll versions of form 1099, with the exception of 1099-QA, should be filed electronically through the FIRE system. FIRE stands for “Filing Information Returns Electronically”. The use of FIRE is required by law when a company files 250 or more information return forms for a calendar year. However, the Internal Revenue Service encourages the use of FIRE for all filers, although paper is still an option if you are filing less than 250 forms.\nWhen submitting 1099s, a company must also fill out a 1096 form. A 1096 form is a summary tax report to show the total number of forms being submitted, total amount being reported, and the total tax withheld.\nForm 1099-QA applies to people with disabilities with an ABLE (“Achieving a Better Life Experience”) account, and it must be filed on paper."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:cce61c42-0c7a-4c48-b8b4-5fffa3c55e2b>","<urn:uuid:9c3bd096-110d-4d14-b6fa-dc81b1204fe0>"],"error":null}
{"question":"Which musical tribute showed more successful public cooperation: Cremona's recording of Stradivari instruments or Verdi's Requiem for Rossini?","answer":"Cremona's recording of Stradivari instruments demonstrated more successful public cooperation. The entire town of 70,000 people complied with enforced silence for eight hours daily over five weeks, with checkpoints and security guards ensuring silence. In contrast, Verdi's proposed musical tribute to Rossini, despite a promising start, failed to materialize due to various stipulations, including requiring composers and performers to contribute financially rather than being paid, and Verdi's threat to quit if any foreign or unartistic person became involved.","context":["“Blow the trumpet in Zion! Proclaim a fast, call an assembly; Gather the people, notify the congregation; Assemble the elders, gather the children and say, ‘Spare O Lord your people.'” Joel 2:12-18\nStop everything, the prophet Joel tells us, and: “Blow the trumpet in Zion!”\nAsh Wednesday is a special day, where we mark our foreheads with Ash to announce a season of reflection, repentance, and even a measure of devotion during these forty days.\nRecently I came across a news story about the town of Cremona, Italy, and the famed Antonio Stradivari, the 17th-century “luthier,” the maker of violins, cellos, and string instruments.\nStradivari lived to the age of 93 and crafted 1,116 string instruments, mostly violins, and cellos. Today, 650 of these instruments survive, of which there are 450 to 512 violins, and at auction, such one of these precious instruments command three million dollars and more.\nAccording to the New York Times, “A Stradivarius violin, viola or cello represents the pinnacle of sound engineering, and nobody has been able to replicate their unique tones.\nEnter Leonardo Tedeschi, a former D.J. and a man with a mission. He was concerned about the future of these magnificent instruments, and how their sound might be lost to time and needed to be preserved in a modern digital form, the so-called “Stradivarius Sound Bank.”\nTedeschi is from Northern Italy and lives nearby Cremona with its Violin Museum, his most obvious source for his essential project. He notes, “We are making immortal the finest instruments ever crafted.”\nHis ideas were to record the very best, Stradivari, at his disposal in a way to preserve the sound digitally; but not for a particular composition, instead of for every distinct sound possible with hundreds of scales and arpeggios, using different techniques with their bows, or plucking the strings.\nTedeschi brought together in Cremona both Italian and Dutch musicians. They accomplished these variations and played hundreds of thousands of individual notes and transitions on two Stradivari violins, a cello, and a viola.\nEach of these four instruments has names: the 1700 “Stauffer,” 1727 “Vesuvius,” 1615 “Stauffer,” and 1734 “Prince Doria.”\nInside the museum concert hall, everything appeared fine until the addition of 32 hypersensitive microphones from the German recording company, in which even the very slightest sounds from outside of the building caused audio problems.\nFor example, the ambient sound and the rumble of a person dragging a suitcase on cobblestones or the hiss of the street-sweeper, and the noise coming from an open-air market could affect the recording. In a town of 70,000 people, there is less silence than you might imagine.\n“Every time we hear this sound, the sound will mix with the frequency of our instruments, so we cannot use that sound in our product,” Tedeschi laments.\nWhat to do? Tedeschi went to the mayor of Cremona and explained the situation. They agreed that a “zone of silence” around the Violin Museum would be enforced from Jan. 7 to Feb. 9. There were checkpoints and security guards stationed to enforce the curfew. The mayor implored the citizens of Cremona to avoid any sudden and unnecessary sounds.\nSo for eight hours a day, six days of the week and five weeks, the people of Cremona stood silent. The town complied with these orders, and in the words of the mayor, “We are the only city in the world that preserves both the instruments and their voices.”\nJournalist Christopher Livesay concludes: “It’s a level of devotion Antonio Stradivari would likely have appreciated. After all, attention to detail is what makes a Stradivarius a Stradivarius.”\nSo what might this mean for a day like Ash Wednesday? We, too are instruments, capable of listening to the voice that speaks to us.\nWe are called on this day and during the Lenten season to carefully reflect and find that spirituality or inner music that may animate our lives now and in our very promising futures.\nSometimes this may call for time for personal silence and prayerful reflection.\nFor the people of Cremona, it was their level of devotion to their town’s musical heritage that’s remarkable and very inspiring. But it’s your devotion to Jesus, as Ambassadors for Christ, in service to one another here at Santa Catalina but acts of faith, hope, and love for your family and friends.\nSuch devotion to those most in need that can indeed mark our lives as distinctive, and as clear as today’s ashes on our foreheads signal a higher commitment to one another and Christ.\nHere’s a final word about Stradivari violins. I’ve been told that the best way to preserve these precious instruments is to play them. In other words, by placing these violins in a glass enclosure or a bank vault can destroy their tone and quality of the sound.\nSo too, the best way to celebrate our spiritual and religious lives or, let’s say, is to tune up our instrument — and practice, day after day.\nRosary Chapel, Santa Catalina School, Monterey, CA.","Classical music has such a long, storied history, that it can be difficult to know where to start. Each week, we’ll be exploring an important event that left its mark.\nThis week? Gioachino Rossini’s death inspired Verdi to launch an ambitious project involving twelve Italian composers to create a requiem in his honor. The project was eventually aborted, but Verdi’s musical tribute to Rossini survives in the Libera Me of his Messa da Requiem, a grandiose work composed in honor of another idol…\nLike most requiems, Verdi’s Messa da Requiem began with an ending.\n“A great name has disappeared from the world!”\nThe great opera composer Gioachino Rossini died in November 1868, and Giuseppe Verdi—the great opera composer of his own generation—was deeply moved. But, when Verdi wrote the above in a letter, he was also thinking about another Italian artistic titan.\n“When the other one who still lives is no more, what will we have left?”\n“The other one?” He was referring to Alessandro Manzoni, a poet and novelist of about Rossini’s age, best known for The Betrothed. But for now, Verdi’s attention was mainly focused on his dear, departed fellow composer.\nVerdi hit upon an original idea with which to mark Rossini’s life. On the first anniversary of his death, he proposed, a requiem mass would be performed. It would be written jointly by Italy’s leading composers, with Verdi contributing one movement himself—as it transpired, the final movement, Libera me (above).\nAs if that project weren’t unusual enough already, Verdi had some rather specific stipulations. For a start, rather than getting paid, the composers and performers would be expected to contribute financially to the project. After the performance, the score “should be sealed and placed in the archives… from which it should never be taken,” except perhaps for anniversaries. Oh, and Verdi would quit the project immediately if anyone foreign or unartistic became involved.\nCasting an eye over Verdi’s demands, it’s perhaps surprising that the Messa per Rossini project got off the ground at all. But a fairly promising start was not enough to make it happen. The project gave up the ghost fairly slowly, but seems to have been consigned to an unmarked grave by August 1871.\nBy then, however, Verdi had written his own Libera me. “Deliver me, O Lord” says the text… and yet Verdi’s music lingered unperformed.\nThen, on May 22, 1873, Alessandro Manzoni died.\nAnd this time, Verdi decided to write the whole thing himself. He had already written the ending.\nThings went far more smoothly this time. He met his own deadlines, and the first performance did indeed take place on the first anniversary of Manzoni’s death—May 22, 1874—at the church of San Marco in Milan. Unlike the plan for the Rossini requiem, further performances were permitted: three more immediately took place at La Scala. The reaction was positive from almost everyone, with Hans von Bülow one of few exceptions. An “opera in ecclesiastical costume,” he called it. Referring to a sacred work by a leading opera composer, this is surely one of music history’s least imaginative insults.\nThe troubled genesis of Verdi’s Requiem is now a matter for the history books. The ends of Rossini and Manzoni, and the conclusion of the Messa per Rossini project, proved to be the beginning of a work which remains very much alive—above all, thanks to its spectacular Dies irae, that unforgettable whirlwind of wrath and fear that still reverberates today.\nIt’s so thrilling, in fact, that it even seems to have terrified Herbert von Karajan, albeit only briefly. Watch closely at 1:41."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:a5349d64-1039-4563-bdf4-3a1996ac7aee>","<urn:uuid:54986c44-5abd-4e2d-a6c0-6586bfdb48b6>"],"error":null}
{"question":"What is the fever temperature threshold that requires emergency care for infants under 2 months old?","answer":"Any infant under 2 months with a fever of 100.4 degrees or greater (measured rectally) needs a laboratory evaluation and should be taken to a dedicated pediatric emergency department.","context":["Your Child is Having a Medical Emergency, Now What?\nTo stay home or take your sick child to the emergency department? Here's advice for parents facing that decision.\nAs career emergency medical physicians, we know an unexpected trip to the emergency department can be overwhelming and anxiety-inducing for you and your child. There are several things you can do to either avoid a visit to the emergency department or make your experience less stressful.\nWhile you may feel scared and out of control, your child needs you to be brave. You are your child’s greatest comfort and biggest advocate. It’s amazing to see how a parent with a calm and assuring presence can help a child during emergency evaluation and treatment.\nOne of the best ways to minimize your fears is to be prepared before disaster strikes. Have a first-aid kit in the home, take a CPR class and learn the Heimlich maneuver. Keep the number to your pediatrician, local urgent care and poison control center nearby. The mental calmness that comes from knowing you’ve done the prep work if your child ever experiences a medical emergency is invaluable.\nAny infant under 2 months with a fever of 100.4 degrees or greater (measured rectally), needs a laboratory evaluation and is best served in a dedicated pediatric emergency department. Most pediatric offices have a nurse triage line designed to help you determine where your child would best be served by talking through their symptoms. If the nurse triage line is not available to you, your local healthcare system’s advice line offers this same evaluation.\nCall Poison Control\nIf you suspect your child may have ingested something but is acting normal and seems okay, we advise calling poison control first. The poison control center is very knowledgeable and may save you a time consuming and expensive visit to the emergency department. Some household ingestions can be managed at home, but if you do take a trip to the emergency department, bring what you suspect was ingested (medicines, cleaning supplies, etc.) with you, along with any medications your child takes on a regular basis. That will help us determine what we need to do for your child.\nIf you do need to go to the emergency department, explain the process to your child beforehand to help minimize fear. Care is prioritized based on how sick your child is and is not determined on a first-come, first-served basis. This can be hard for some children to understand. The waiting can be long and feel frustrating. If possible, leave siblings at home. Grandparents, aunts, uncles and cousins are great, but will not benefit in the evaluation and treatment of the child. One or both parents is really all that is needed.\nBring a pen and paper so you can take notes and write down the names of those caring for your child. If you are having trouble understanding a condition or diagnosis, ask your providers to draw or diagram. And yes, you can ask a million questions. We don’t mind.\nKnow that as pediatric emergency physicians, we have dedicated our professional careers to caring for sick and injured children. We love what we do. We love the resilient and open nature of children. Seeing a child get better and have a positive experience in our emergency department is why we come to work every day.\nMichelle Capizzi, MD, is a board-certified pediatric emergency medicine physician with Novant Health Hemby Children’s Hospital Emergency Department. She has 20+ years of experience in pediatric emergency medicine and is a mom of five.\nSara Steelman, MD, is a board-certified pediatric emergency medicine physician and medical director of Hemby Children’s Emergency Department. She has 25+ years of experience in pediatric emergency medicine and is a mom of three."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:37542970-f21d-4686-ade4-58f108cc802e>"],"error":null}
{"question":"whats diff between nashville session scales and preservation hall foundation goals?","answer":"Nashville session scales are a tiered payment system for recording musicians, ranging from demo scale (for demonstration purposes only) to master scale (minimum restrictions, maximum cost). In contrast, the Preservation Hall Foundation's goal is specifically focused on protecting, preserving, and perpetuating the musical traditions and heritage of New Orleans, including supporting music education, academic research, historical archiving and promotional outreach campaigns for Traditional New Orleans Jazz.","context":["Nashville Sessions: Demo Scale, Master Scale, Kitchen Scale (P1)\n“When I hear someone talking about a 10 and a 2, I’m thinking October and February.”\nThese are joking words of a Nashville road musician, referencing the life of Nashville recording musicians and illustrating that the latter live in a world which only they fully understand.\nFifteen years ago or more, when I first moved to Nashville from Russia to pursue a career in music, it didn’t take me long to discover that Nashville recording scene revolved to a large degree around 5 distinct things:\n1. Excellence. (You know I had to go there!)\n2. Nashville Musicians Association, Local 257 of AFM (American Federation of Musicians). Every serious session musician considers joining at one point or another.\n3. Nashville number system charts.\n4. Recording sessions: usually a 10 o’clock and a 2 o’clock. 6 o’clock not as common.\n5. Demo scale, limited pressing scale, master scale, as per AFM 257 (see #2).\nI am oversimplifying of course, but hey it’s my article.\nSo, let’s see. Number system charts I’ve already covered extensively. Excellence is a subject that I’ll never stop covering – and striving for. What’s all that about 10 or 2 o’clock sessions and various scales?\nSession times are relatively simple to explain. As far as the Union sessions go, there are usually 3 on any given day: morning session at 10am, afternoon session at 2pm and evening session at 6pm. 3 hours allotted for each, with a 1-hour break for lunch or dinner in between. Hey, musicians gotta eat, too! There are also “premium sessions”, recorded between midnight and 8am, and “double time” sessions, recorded during national holidays, but I never ran into those myself.\nKeep in mind, the session’s starting time is the “downbeat” time, when tape starts rolling and music starts being recorded. [Figurative tape not literal, most of the time anyway.] So you gotta show up early enough in advance to set up all your gear, get recording levels, get your cup of coffee, get your hellos and catching up out of the way, etc. Your arrival time depends on the size of your setup: drummers usually show up first, while fiddle players can just waltz in 10-15 minutes before downbeat. I should have learned to play a fiddle. Sigh.\nNow, what about scale rates? That’s where it gets more complicated. Let’s look at them, one by one.\nDemo scale. Most often used by song publishers or songwriters to produce “demonstration” recordings of their compositions: to pitch to artists and labels. You can usually get away with tracking as many as 5 or even 6 songs on one demo session. This pretty much only leaves 30-36 minutes per song, give or take. (As a general rule, vocals are not recorded during a tracking session, unless it’s a scratch vocal only.) So you really need to make sure that your charts are prepared in advance and that there is a producer – or a session leader – who can quickly relay to the rest of the musicians the game plan for each song. If you can manage to record 6 songs on one session? You got yourself a good deal. There is only one caveat: the “demo” restriction. You can’t turn around and sell these recordings on iTunes, or press physical CDs. You can’t submit them to TV or film or other media. It can’t directly generate revenue. It is for “demonstration” purposes only. I know how you feel.\nLimited pressing scale. A bit more costly but also less restrictive. Meant for recordings where “the aggregate number of units, physical and digital manufactured and/or sold of a particular recording does not exceed ten-thousand (10,000) and where the music that is recorded will not be utilized for any other purpose whatsoever other than for sale to the public as a sound recording.” So, a good way to go for indie artists and labels, or for songwriters who want to produce nice-sounding professional demos which can also be sold at their live shows.\nThen comes low budget scale. Self-explanatory. These sessions must be approved in advance and pass certain guidelines and limits, for the low budget scale to apply.\nFinally, top tier of the session cake: master scale. Minimum restrictions on the use of recordings, maximum cost. You can usually get about 3 songs tracked on one master session, or you can take your time and record only 1 or 2.\nThese are the basics of recording music in Nashville that I discovered over a decade ago. How did they translate for me into today’s recording reality, as I find myself focusing more and more on remote online recording – for clients whom, most of the time, I don’t get to meet face-to-face? Read part 2 to find out.","Limited Edition Sister Gertrude Morgan X King Britt Double Vinyl Supporting Preservation Hall Foundation\nVinyl Me, Please and Ace Hotel\npresent a very special vinyl collaboration limited to 1,500 numbered copies\ncelebrating the art and legacy of gospel treasure Sister Gertrude Morgan with all the proceeds supporting the Preservation Hall Foundation\nFor the first time on vinyl, a double LP featuring the historic\n1969 Preservation Hall recording of Sister Gertrude Morgan “Let's Make a Record” paired with its 2004 reimagining “King Britt Presents Sister Gertrude Morgan”\nSister Gertrude Morgan was an Alabama-born and New Orleans-based painter, poet and street evangelist, whose spirituality inspired a body of work almost mythic in its proportions. In 1969, Preservation Hall recorded Sister Gertrude singing fourteen gospel songs in the Prayer Room of her Ninth Ward home, The Everlasting Gospel Mission, accompanied only by her stomping foot and tambourine. That album, Let's Make a Record, fell into obscurity until 2004, when Philadelphia DJ King Britt and record label Ropeadope Records reissued it with new beats and instrumentation.\nVinyl Me, Please and Ace Hotel, together with the Preservation Hall Foundation and Ropeadope announced the first ever double LP release of Sister Gertrude Morgan’s Let's Make a Record half-speed remastered and mixed for mono at Alchemy Studio in London, UK, and King Britt presents Sister Gertrude Morgan.\nLimited to 1,500 foil-stamp numbered copies, the record was released on 180g black vinyl and all sale proceeds will be donated directly back to the Preservation Hall Foundation to further our mission to “Protect, Preserve and Perpetuate the music and culture of New Orleans.”\nThe record is available exclusively online at Ace Hotel Shop and in-store at the Hall's gift shop, open daily from 12-5 PM CST and during the nightly performances.\nThis collaboration between Ace Hotel and Vinyl Me, Please is the first record to be released in the Ace Hotel x Vinyl Me, Please Exclusives, a series of collector releases, that will focus on reissues and live recordings from New Orleans artists that celebrate both the history and future of the city.\nAbout Preservation Hall Foundation\nLaunched in 2011, the Preservation Hall Foundation supports music education, academic research, historical archiving and promotional outreach campaigns to create greater awareness and appreciation for Traditional New Orleans Jazz and the communities that support it. The Foundation’s mission is “to protect, preserve, and perpetuate the musical traditions and heritage of New Orleans.\" www.preshallfoundation.org.\nAbout King Britt\nKing Britt’s musical curiosity has paved the way for the artist he is now. From composing house tunes for the legendary Strictly Rhythm and Nervous Records, djing for Sector 6 Grammy Award winning comrades Digable Planets, starting the legendary Ovum Recordings with techno wunderkind, Josh Wink, being tapped to underscore Michael Mann’s Miami Vice and various commercials, to curating a full day at MoMA PS1 in New York, the list goes on and on. Presently, he is exploring many afro-sonic musical possibilities with his Hyperdub released, Fhloston Paradigm project, The Phoenix. His aural experiments with friends appear on his imprint, The Buddy System Project. His recent curatorial pursuits have placed him in many academic environments – he is a Pew Fellowship winner – bridging the worlds of club, experimental and Black music. www.kingbritt.com\nAbout Vinyl Me, Please\nVinyl Me, Please is a record club & online vinyl store founded in 2013. www.vinylmeplease.com\nRopeadope is a new model record label based in Philadelphia PA, operating as a distributed collaborative network of artists. Notable artists include King Britt, Christian Scott aTunde Adjuah, Nicholas Payton, Eddie Palmieri, and Terrace Martin. In 2004 Ropeadope launched the Renew Orleans campaign, raising over $90,000.00 for New Orleans artists in need after Katrina. Ropeadope is proud to support musicians in the vital city of New Orleans. www.ropeadope.com\nAbout Ace Hotel\nAce Hotel reinvents urban hotels for people who make cities interesting. We crave experience more than hospitality clichés. We are curious about the history and geography of the buildings we inhabit, and let these guide us to someplace fresh and familiar. Ace is the low card and the high card. www.acehotel.com"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:0f0ee12d-d7bd-42df-aefc-445e2864c7f6>","<urn:uuid:bd3a03de-0cfd-4575-b9ce-f7d02b7a206b>"],"error":null}
{"question":"Hi! Can you explain how Arctic warming affects both marine microbes and fish populations?","answer":"Arctic warming has distinct effects on both marine microbes and fish populations. For microbes, Arctic communities are more 'hard-wired' to their conditions compared to warmer waters, making them especially vulnerable to temperature changes. Since Arctic waters are warming faster than anywhere else, this region is likely to see dramatic changes in its microbial ecosystem first. For fish populations, particularly lake trout, the warming is actually projected to increase productivity by about 20% by 2050. The warming will allow a rapid relaxation of current environmental constraints on growth in the far north, and may result in a 29% increase in harvestable biomass through expanded habitable zones.","context":["Ocean exploration mission reveals incredible biodiversity -- and why it is in danger\nResearchers now have a baseline for microbial diversity in the oceans as the climate changes.\nBeneath the ocean’s surface, there is a landscape marked by its biodiversity. Only by venturing under the water can scientists study the vast number of species living there — from giant blue whales to tiny marine animals like plankton and other microbes.\nThe sheer variety and number of these tiny animals is largely invisible to the naked eye. But data collected during ocean voyages around the world can reveal some of the details of the oceans’ micro-diversity. And now, thanks in part in part to the Tara Oceans Expedition, scientists have the clearest picture yet of these microbial species’ biodiversity — and why it may be in danger — and why it may be in danger.\nThe findings, detailed in pair of studies this week in the journal Cell, establish a baseline to help researchers understand how climate change impacts diversity — both in these organisms’ genes and in their size and shape.\nProjecting what the future looks like for these tiny creatures is more possible with such a baseline, says Shinichi Sunagawa, a co-author on one of the studies and researcher at ETH Zurich.\nIn Sungawa’s paper, researchers studied ocean creatures’ genes from around the world, looking at differences in how biodiversity changed over time across distinct regions of ocean. That information can offer answers to the question, “How do microbe communities in the ocean adjust to environmental change?”\nTo try and answer it, Sungawa’s team looked at data from as large a sample as possible — a total of 47 million genes.\nTheir findings confirm previous evidence suggesting differences in how ocean ecosystems function depending on region. Genetic biodiversity in the Arctic, for instance, differs from that of the rest of the world.\n“The Arctic presents itself as a very biologically unique ecosystem,” Sunagawa says.\nAmong the most striking findings was seeing why genes in different areas change over time, he says.\n“Our findings suggest that microbial communities in warmer waters have a larger gene pool, which may allow them to be quite flexible in response to temp changes,” Sunagawa says. Meanwhile, in colder waters, communities are “more hard-wired” to their conditions.\nIn a warming climate, that means that dramatic changes in the microbial ecosystem are more likely to happen in the Arctic first, especially “since this is the region where waters are warming faster than anywhere else,” he says.\nPlankton in particular\nThe second study focused on the diversity of the plankton themselves. Global trends suggest that biodiversity among these tiny animals is decreasing the most at the Earth’s poles.\nPlankton may be small, but they have a huge influence on other ocean species “simply because the plankton are at the very base of the ocean food chain,” says Chris Bowler, an author on this study and researcher at Research University Paris, France.\n“The fish species may no longer find their favorite plankton to eat,” he says, “so the whole food chain could be affected by these changes.”\nThe researchers wanted to know if they would observe the same trends in biodiversity that other teams have seen on land. Namely, did diversity fall in certain regions based on temperature and other factors?\nThe answer turned out to be yes. “We could see, indeed, that we could observe biodiversity gradients from pretty much all organisms,” Bowler says.\nTracking global trends in the microbial community can help to inform future projections of how it will shift as the planet warms, the researchers say.\nProtecting the seas\nThe new data can help inform policy and conservation efforts, the researchers say. One such policy, marine protected areas, have been shown to be effective in preserving biodiversity.\nIn tandem with the two Cell studies, the researchers wrote a report calling for sweeping policy changes, published in the journal One Earth.\nSpecifically, the researchers laid out six policy aims designed to protect the oceans:\n- Stronger integration of sciences.\n- Ocean-observing systems put in place.\n- Improved science-policy interfaces.\n- New, supported research partnerships.\n- A new ocean-climate finance system.\n- Improved ocean literacy and education to modify social norms and behaviors.\n“Adopting these strategies could help establish ocean science as a key foundation of broader sustainability transformations,” the researchers say.\nConservation efforts like these are particularly urgent right now, says Bowler.\n“Oceans are pretty sick these days, and we need to try and heal them.”\nAbstract: Ocean microbial communities strongly influence the biogeochemistry, food webs, and climate of our planet. Despite recent advances in understanding their taxonomic and genomic compositions, little is known about how their transcriptomes vary globally. Here, we present a dataset of 187 metatranscriptomes and 370 metagenomes from 126 globally distributed sampling stations and establish a resource of 47 million genes to study community level transcriptomes across depth layers from poleto-pole. We examine gene expression changes and community turnover as the underlying mechanisms shaping community transcriptomes along these axes of environmental variation and show how their individual contributions differ for multiple biogeochemically relevant processes. Furthermore, we find the relative contribution of gene expression changes to be significantly lower in polar than in non-polar waters and hypothesize that in polar regions, alterations in community activity in response to ocean warming will be driven more strongly by changes in organismal composition than by gene regulatory mechanisms.","Climate warming at high latitudes has long been expected to exceed that predicted for tropical and temperate climes, but recent warming in the Arctic has exceeded even those expectations1. The geophysical consequences of this warming are reasonably well established2, but the impacts on freshwater fauna are poorly understood. Here we use a large-scale geospatial analysis of the population dynamics of one of the most abundant north temperate freshwater fish species to forecast increased demographic rates, productivity and colonization range in response to IPCC climate warming scenarios. Geospatial lake morphometry data were used to characterize 481,784 lakes in the Canadian Arctic capable of supporting lake trout (Salvelinus namaycush) populations. Lake trout productivity in existing habitat is projected to increase by 20% by 2050 due to climate change, but an expanded habitable zone may result in a 29% increase in harvestable biomass. Although many ecosystems are likely to be negatively impacted by climate warming, the phenotypic plasticity of fish will allow a rapid relaxation of the current environmental constraints on growth in the far north, as well as enhanced colonization of bodies of water in which there are few potential competitors.\nThis is a preview of subscription content\nSubscribe to Nature+\nGet immediate online access to the entire Nature family of 50+ journals\nSubscribe to Journal\nGet full journal access for 1 year\nonly $8.25 per issue\nAll prices are NET prices.\nVAT will be added later in the checkout.\nTax calculation will be finalised during checkout.\nGet time limited or full article access on ReadCube.\nAll prices are NET prices.\nThe geospatial data (Canadian Digital Surface Model and Canadian Digital Elevation Model) are available from http://maps.canada.ca/czs/index-en.html. Air temperature data from 881 weather stations across Canada are available from http://climate.weather.gc.ca/climate_normals/results_1981_2010_e.html.\nHuang, J. et al. Recently amplified arctic warming has contributed to a continual global warming trend. Nat. Clim. Change 7, 875–879 (2017).\nMeredith, M. et al. in IPCC Special Report on the Ocean and Cryosphere in a Changing Climate (eds H.-O. Pörtner et al) Ch. 3 (IPCC, Cambridge Univ. Press, 2019).\nThuiller, W. et al. Consequences of climate change on the tree of life in Europe. Nature 470, 531–534 (2011).\nBeaugrand, G. A. et al. Prediction of unprecedented biological shifts in the global ocean. Nat. Clim. Change 9, 237–243 (2019).\nForcada, J., Trathan, P. N. & Murphy, E. J. Life history buffering in Antarctic mammals and birds against changing patterns of climate and environmental variation. Glob. Change Biol. 14, 2473–2488 (2008).\nPacifici, M. et al. Assessing species vulnerability to climate change. Nat. Clim. Change 5, 215–225 (2015).\nFrainer, A. et al. Climate-driven changes in functional biogeography of Arctic marine fish communities. Proc. Natl Acad. Sci. USA 114, 12202–12207 (2017).\nWessely, J. et al. Habitat-based conservation strategies cannot compensate for climate-change-induced range loss. Nat. Clim. Change 7, 823–827 (2017).\nRyder, R. A. The Morphoedaphic Index—use, abuse and fundamental concepts. Trans. Am. Fish. Soc. 111, 154–164 (1982).\nMessager, M. L., Lehner, B., Grill, G., Nedeva, I. & Schmitt, O. Estimating the volume and age of water stored in global lakes using a geo-statistical approach. Nat. Comm. 7, 13603 (2016).\nCampana, S. E., Casselman, J. M. & Jones, C. M. Bomb radiocarbon chronologies in the Arctic, with implications for the age validation of lake trout (Salvelinus namaycush) and other Arctic species. Can. J. Fish. Aquat. Sci. 65, 733–743 (2008).\nShuter, B. J., Jones, M. L., Korver, R. M. & Lester, N. P. A general, life history based model for regional management of fish stocks: the inland lake trout (Salvelinus namaycush) fisheries of Ontario. Can. J. Fish. Aquat. Sci. 55, 2161–2177 (1998).\nCampana, S. E. Accuracy, precision and quality control in age determination, including a review of the use and abuse of age validation methods. J. Fish. Biol. 59, 197–242 (2001).\nCasselman, J. M., Jones, C. M. & Campana, S. E. Bomb radiocarbon age validation for the long-lived, unexploited Arctic fish species Coregonus clupeaformis. Mar. Freshwat. Res. 70, 1–8 (2019).\nLester, N. P., Shuter, B. J. & Abrams, P. A. Interpreting the von Bertalanffy model of somatic growth in fish: the cost of reproduction. Proc. R. Soc. Ser. B 271, 1625–1631 (2004).\nMinte-Vera, C. V., Maunder, M. N., Casselman, J. M. & Campana, S. E. Growth functions that incorporate the cost of reproduction. Fish. Res. 180, 31–44 (2016).\nCollins, M. et al. in Climate Change 2013: The Physical Science Basis (eds Stocker, T. F. et al.) 1029–1136 (IPCC, Cambridge Univ. Press, 2013)\nClimate Change 2014: Synthesis Report (eds Pachauri, R. K. and Meyer, L. A.) (IPCC, Cambridge Univ. Press, 2014).\nIslam, D. & Berkes, F. Indigenous peoples' fisheries and food security: a case from northern Canada. Food Secur. 8, 815–826 (2016).\nMusick, J. A. Ecology and conservation of long-lived marine animals. Am. Fish. Soc. Symp. 23, 1–10 (1999).\nSchloss, C. A., Nunez, T. A. & Lawler, J. J. Dispersal will limit ability of mammals to track climate change in the Western Hemisphere. Proc. Natl Acad. Sci. USA 109, 8606–8611 (2012).\nHirsch, P. E., N’Guyen, A., Muller, R., Adrian‐Kalchhauser, I. & Burkhardt‐Holm, P. Colonizing Islands of water on dry land—on the passive dispersal of fish eggs by birds. Fish. Fish. 19, 502–510 (2018).\nSpens, J., Englund, G. & Lundqvist, H. Network connectivity and dispersal barriers: using geographical information system (GIS) tools to predict landscape scale distribution of a key predator (Esox lucius) among lakes. J. Appl. Ecol. 44, 1127–1137 (2007).\nSwanson, H. K. et al. Anadromy in Arctic populations of lake trout (Salvelinus namaycush): otolith microchemistry, stable isotopes, and comparisons with Arctic char (Salvelinus alpinus). Can. J. Fish. Aquat. Sci. 67, 842–853 (2010).\nOckendon, N. et al. Mechanisms underpinning climatic impacts on natural populations: altered species interactions are more important than direct effects. Glob. Change Biol. 20, 2221–2229 (2014).\nWilson, K. L., De Gisi, J., Cahill, C. L., Barker, O. E. & Post, J. R. Life‐history variation along environmental and harvest clines of a northern freshwater fish: plasticity and adaptation. J. Anim. Ecol. 88, 717–733 (2019).\nGauthier, G. et al. Long-term monitoring at multiple trophic levels suggests heterogeneity in responses to climate change in the Canadian Arctic tundra. Philos. Trans. R. Soc. B 368, 20120482 (2013).\nThomas, C. D. Climate, climate change and range boundaries. Diversity Distrib. 16, 488–495 (2010).\nHealey, M. C. The dynamics of exploited lake trout populations and implications for management. J. Wildl. Manag. 42, 307–328 (1978).\nBurr, J. M. Growth, density and biomass of lake trout in Arctic and Subarctic Alaska. Am. Fish. Soc. Symp. 19, 109–118 (1997).\nMills, K. H., Dyck, M. & Harwood, L. A. Proceedings of the second lake trout symposium 2005, Yellowknife, Northwest territories. Can. Tech. Rep. Fish. Aquat. Sci. 2778, 247 (2008).\nHollister, J. W., Milstead, W. B. & Urrutia, M. A. Predicting maximum lake depth from surrounding topography. PLoS ONE 6, e25764 (2011).\nLivingstone, D. M., Lotter, A. F. & Walker, I. R. The decrease in summer surface water temperature with altitude in Swiss alpine lakes: a comparison with air temperature lapse rates. Arct. Antarct. Alp. Res. 31, 341–352 (1999).\nShuter, B. J., Schlesinger, D. A. & Zimmerman, A. P. Empirical predictors of annual surface water temperature cycles in North American lakes. Can. J. Fish. Aquat. Sci. 40, 1838–1845 (1983).\nDa Fang, X. & Stefan, H. G. Long-term lake water temperature and ice cover simulations/measurements. Cold Reg. Sci. Technol. 24, 289–304 (1996).\nCampana, S. E. Physical Characteristics of 55 Canadian Arctic Lake Trout Lakes (Knowledge Network for Biocomplexity archive, 2020); https://doi.org/10.5063/F1ZP44F1\nCampana, S. E. Lake Trout Population Characteristics in 55 Canadian Arctic Reference Lakes (Knowledge Network for Biocomplexity, 2020); https://doi.org/10.5063/F1TX3CPV.\nSamarasin, P., Minns, C. K., Shuter, B. J., Tonn, W. M. & Rennie, M. D. Fish diversity and biomass in northern Canadian lakes: northern lakes are more diverse and have greater biomass than expected based on species–energy theory. Can. J. Fish. Aquat. Sci. 72, 226–237 (2015).\nCampana, S. E., Valentin, A. E., MacLellan, S. E. & Groot, J. B. Image-enhanced burnt otoliths, bomb radiocarbon and the growth dynamics of redfish (Sebastes mentella and S. fasciatus) off the eastern coast of Canada. Mar. Freshw. Res. 67, 925–936 (2016).\nFrancis, R. I. C. C. Growth in age-structured stock assessment models. Fish. Res. 180, 113–118 (2015).\nSmith, M. W. et al. Recommendations for catch-curve analysis. North Am. J. Fish. Managem. 32, 956–967 (2012).\nRicker, W. E. Computation and Interpretation of Biological Statistics of Fish Populations (Bulletin of the Fisheries Research Board of Canada, 1975).\nDeriso, R. B. Optimal F 0.1 criteria and their relationship to maximum sustainable yield. Can. J. Fish. Aquat. Sci. 44, 339–348 (1987).\nThis work was supported by the Nunavut Wildlife Management Board, Fisheries and Oceans Canada, US National Science Foundation grant OCE-9985884 and the University of Iceland. We thank S. Armsworthy, P. Bentzen, J. Brazner, C. Campana, P. Campana, S. Campana, S. Casselman, M. Fowler, D. Houlihan, W. Joyce, P. Leblanc, A. MacDonnell and M. Showell for their exceptional assistance in the field and laboratory. B. Shuter and J. Morrongiello provided valuable comments on the MS.\nThe authors declare no competing interests.\nPeer review information Nature Climate Change thanks Arild Folkvord and the other, anonymous, reviewer(s) for their contribution to the peer review of this work.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\nAbout this article\nCite this article\nCampana, S.E., Casselman, J.M., Jones, C.M. et al. Arctic freshwater fish productivity and colonization increase with climate warming. Nat. Clim. Chang. 10, 428–433 (2020). https://doi.org/10.1038/s41558-020-0744-x\nPolar Biology (2022)\nNature Climate Change (2021)\nScientific Reports (2020)"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:3503ca86-1ba0-45bf-9965-695eef656501>","<urn:uuid:50be4d14-7b3a-469d-8c9a-24571c88c15c>"],"error":null}
{"question":"Could you compare how inflammation markers in middle age affect different regions of the brain in later life?","answer":"Higher inflammation markers in middle age were linked to smaller volumes in several brain regions: the hippocampus was 4.6% smaller, the occipital lobe was 5.7% smaller, and the Alzheimer's disease signature region was 5.3% smaller. However, people with higher inflammation markers had larger volumes in the ventricular parts of the brain (fluid-filled cavities). Interestingly, there was no link found between inflammatory markers and total brain volume.","context":["\"Tissue inflammation blood test points to dementia risk,\" is the headline in The Times.\nResearchers in the US say people who have higher measures of inflammation in middle age are likely to have less brain tissue in some parts of their brain in older age.\nThe differences in brain volume, seen on MRI scans, were also accompanied by small differences in performance on memory tests.\nBut the study didn't find that people with raised inflammatory measures in middle age were more likely to get dementia, as it wasn't set up to directly measure dementia risk.\nPrevious research has found people with dementia and a smaller brain volume are likely to have higher measures of substances linked to inflammation in their blood. But it wasn't clear whether the inflammation happened before the dementia, or afterwards.\nThe association is further complicated by the fact that it's normal for people's brains to experience some shrinkage as they get older. And, obviously, not everyone gets dementia as they get older.\nWhile the study is certainly interesting, it doesn't provide any concrete answers. For example, we don't know how people's inflammatory measures changed over time, or what role factors other than inflammation may have had.\nThere are steps you can take to reduce your risk of dementia, although these aren't guarantees.\nThis includes eating a healthy diet, maintaining a healthy weight, exercising regularly, moderating how much alcohol you drink, and quitting smoking if you smoke\nWhere did the story come from?\nThe researchers came from the Johns Hopkins School of Medicine, the Baylor College of Medicine, the University of Minnesota, the Mayo Clinic, and the University of Mississippi Medical Centre, all in the US.\nThe study was funded by the US National Heart, Lung and Blood Institute, and was published in the peer-reviewed journal Neurology.\nThe Times and the Mail Online covered the study in reasonably balanced and accurate stories. Both made it clear in the article (although not in The Times' headline) that the study didn't show a cause and effect relationship between inflammation and dementia.\nWhat kind of research was this?\nThis was a prospective cohort study.\nThese types of observational study are good for spotting links between factors – in this case, inflammation and brain volume – but can't prove that one factor causes another.\nWhat did the research involve?\nResearchers recruited more than 15,000 people aged 45 to 65 for an ongoing study principally intended to look at heart disease risk.\nAs part of the study, they measured 5 substances linked to inflammation in the participants' blood when they were aged 53 on average.\nTwenty-four years later, they selected 1,978 participants to have their brain volume measured by MRI scan and take a word recall memory test.\nThey then looked at whether higher inflammatory measures were linked to brain volume and memory test performance.\nThe researchers specifically sought to find out whether age, sex or race might have affected the results, as these have already been linked to dementia risk.\nThe 5 substances chosen as markers of inflammation were:\n- von Willebrand factor\n- factor VIII\n- white blood cell count\nMost of these are linked to blood clotting or the body's response to infection.\nThe researchers combined people's scores to give an overall inflammatory marker score.\nThe memory test involved listening to a list of 10 words and recalling as many as possible after a short delay.\nThe MRI scans looked at total brain volume, as well as analysing specific areas of the brain known to be affected by Alzheimer's disease (AD), such as the hippocampus.\nWhat were the basic results?\nPeople who had higher total inflammatory marker scores in middle age (the average age was 53 at the start of the study) were more likely to have a smaller brain volume in certain areas at the end of the study.\n- hippocampal volume – the hippocampus is an area of the brain that helps regulate memory\n- occipital volume – the occipital lobe is an area of the brain responsible for visual processing\n- AD signature region volume – an area of the brain previously thought to be smaller in people with Alzheimer's disease; it consists mainly of the cerebrum, which is responsible for higher brain functions\nBut the people involved in the study did have larger volumes in ventricular parts of the brain (these are cavities in the brain filled with fluid).\nCompared with people who didn't have raised levels of any inflammatory markers at the start of the study, those with raised levels on 3 or more markers had smaller hippocampal (4.6% smaller), occipital lobe (5.7% smaller) and AD signature region (5.3% smaller) volumes.\nThey also did very slightly worse on the memory test, remembering on average 5 words out of 10, compared with 5.5 words for those without inflammatory markers.\nThe researchers didn't see any link between total brain volume and inflammatory markers.\nThe association between inflammatory markers and brain volume was stronger in people who had higher markers of inflammation at a younger age, and was weaker in African American participants. There were no differences between the sexes.\nHow did the researchers interpret the results?\nThe researchers said their findings \"provide support\" for an early role for inflammation \"in the development of neurodegenerative brain changes associated with late-life cognitive decline, AD [Alzheimer's disease] and other forms of dementia\".\nInflammation in the body is a response to injury or disease. But if the body is constantly in an inflammatory state, it can harm blood vessels and lead to heart disease.\nThis study suggests high levels of inflammation over the long term might also damage the brain.\nThat's not surprising – what's good for the heart is usually good for the brain, and we already know exercising, avoiding high blood pressure and eating healthily may help protect the brain.\nStudies like this will help researchers work out more precisely what's happening in the brain when people experience memory loss or dementia.\nBut this study has some limitations.\nThe first and most important is that researchers didn't measure people's brain volume at the start of the study.\nThis means we don't know whether the results at the end of the study end represent brain shrinkage, or whether some people had always had smaller brain volume in certain areas.\nThis makes it harder to be sure that differences in inflammatory markers predated the differences in brain volume. This type of study design can't prove cause and effect – and in this case, it can't prove that one situation predated another.\nAlso, the substances measured may not be very precise measures of inflammation – they're also involved in other physiological processes.\nAnd the study didn't look at whether people with higher inflammatory markers were more likely to get dementia, only at their brain volume and performance in one type of memory test.\nWe don't know the effect of the smaller brain volume in some areas on those people. The different performance on the memory test was also pretty small.\nAll in all, it's far too early to say we could ever have a blood test that accurately predicts dementia risk.\nLinks To The Headlines\nTissue inflammation blood test points to dementia risk. The Times (subscription required), November 2 2017\nInflammation in midlife linked to brain shrinkage later. Mail Online, November 1 2017\nLinks To Science\nWalker KA, Hoogeveen RC, Folsom AR, et al. Midlife systemic inflammatory markers are associated with late-life brain volume. Neurology. Published online November 1 2017"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:679e2158-6c01-413b-9610-39598eb27faf>"],"error":null}
{"question":"As a classics scholar, I'm intrigued by mythological navigation metaphors. What different symbolic interpretations exist for guiding figures in ancient literature, such as those in coins and epic poetry?","answer":"There are several significant symbolic interpretations of guiding figures in ancient literature and artifacts. In the Divine Comedy, Virgil himself appears as a guide figure, leading the narrator through Hell and Purgatory, explaining that he will guide him 'through the timeless breaker's yard' and through 'purging fire.' This role is particularly significant given that on the Roman aureus from 19 BC, there's a symbolic interpretation related to guidance and wisdom - the crab on the coin was interpreted by Gabriele Simeoni as representing wise moderation and reflection, while the butterfly represented rashness, illustrating Augustus' favorite proverb 'Festina lente' (make haste slowly). This shows how both literary and numismatic sources used guiding figures and symbols to represent wisdom and moderation in navigation through life's challenges.","context":["Reproduced with the kind permission of Numismatica Ars Classica NAC AG, Auction 31, lot 13.\nThis gold coin called aureus (lat. aureus – “golden”) was struck in 19 BC in Rome by Marcus Durmius, a minter of the emperor Caesar Augustus (63 BC – AD 14). It is commonly placed by numismatists at R4 level on the Sheldon rarity scale, which comprises very scarce specimens. Its weight is estimated to be 7,959 g.\nThe obverse of the coin carries a portrait of the emperor wearing an oak wreath, encircled with the legend CAESAR AVGVSTVS. Its reverse features a crab holding in its claws an insect which could be identified as a butterfly or a dipteran such as a midge or a gnat. The decoration is accompanied by the inscription M • DVRMIVS / III • VIR, indicating the minter’s name and his official title, triumvir monetalis.\nThe unusual design of the reverse provides a very large field for discussion and interpretation.\nThe motif comes as a surprise since Rome’s Republican traditions require that the coins struck by a triumvir monetalis show on their reverse a notable exploit of his ancestors, the mythological origin of his family or a monument his family has funded. Moreover, it appears to be too extravagant, almost too violent to be considered simply as an entertaining image such as can be seen on M. Durmius’ silver denarii depicting hunting scenes.\nIt is possible to compare the aureus with a set of coins made in the 5th century BC in Akragas, one of the leading Greek cities of Sicily, which regularly used the crab on their reverse, or the denarii struck by C. Cassius Longinus and the legate M. Servilius in 43-42 BC. These figure a crab holding an aplustre, an appendage of wood used to decorate ships’ sterns; below we can see an untied diadem and a rose. However, it is unlikely that Augustus’ aureus is related to one of these coins. In the first case, there is no doubt that the crab was used as a symbol of the river god Akragas, whose name the city had taken; concerning Cassius’s coin, the decoration has a precise historical meaning, as it makes an allusion to the general’s victory over the Rhodian fleet – thus, the crab stands for the city of Cos, whereas the rose corresponds to Rhodes. Furthermore, we should exclude the eventuality that Augustus’ aureus could have been inspired by the coins of Caesar’s murderer.\nIn 1559, the Italian emblematist Gabriele Simeoni gave a possible explanation of the picture in his work Le Imprese Heroiche et Morali (Lyon). He implied that the reverse was an illustration of the proverb Festina lente („make haste slowly“), which had been one of Augustus’ favourite quotes (Suetonius, Divus Augustus, 25). Throughout the ages, it had often been represented as a hare half hidden in a snail shell, a tortoise carrying a sail on its back, and especially a dolphin winding itself round an anchor, a leitmotiv which appears on a certain number of Vespasian’s coins. Simeoni suggested that the crab could symbolize, by its slowness, the wise moderation and reflexion, while the quick flight of the butterfly rather the rage and the rashness. The juxtaposition of two such opposite species was supposed to call attention to the fact that the emperor should stay temperate and rational in every occasion.\nA radical change in the interpretation of the reverse was made in 20th century, when the philologist Harold Mattingly proposed in his article “The Date of Virgil’s Death : A Numismatic Contribution” (The Classical Review 44 , p. 57-59) to regard the image as a homage paid by the emperor to Virgil, the greatest poet of the Saeculum Augustum, who had tragically died in 19 BC on his way to Italy after having left to Greece in order to visit the places he had described in his epic the Aeneid. This point of view was embraced as well by Jean-Luc Desnier in 1995 (“Tenet nunc Parthenope”, Latomus 54, p. 298-304).\nYet this supposition seems too hypothetical and incomplete unless a connection is made between Durmius’ aureus and the Culex (“The Gnat”), a poem in hexameters doubtfully attributed to the young Virgil. It describes a small incident which occurred to an anonymous shepherd: disturbed in the middle of his nap by a gnat, the old man killed the insect, which permitted him to avoid being bitten by a poisonous snake. The link between the poem and the coin was put forward by the French philologist Jean-Yves Maleuvre, who had exposed in 1991 a new version concerning Virgil’s death (cf. “La mort de Virgile d’après Horace et Ovide”, Paris, 1993). According to his theory, the poet had been secretly murdered by Augustus, whose power he had been constantly defying by using a kind of double language that permitted him to express unobtrusively his hostility towards Augustus’ despotic regime in his poetry without attracting public attention. The Princeps had been tolerating these covert attacks because he knew that only a small number of readers would be able to feel the harrowing contradiction concealed in Virgil’s verses, and that the poet would be still considered as the emperor’s devoted admirer. Thereby he was free to order from Virgil poems to his glory: he would eliminate him, once he would not need him any more. A meticulous analysis of the Culex affirms the assumption that it has been written by Augustus himself, who had thus indulged in the luxury of proclaiming cynically his crime and mocking his victim (J.-Y. Maleuvre, “Le moucheron d’Octave”, RBPh 76, 1998, p. 75-86). Durmius’ aureus would be a simple “illustration” of the poem: the “gnat” could be in fact a derisive pseudonym applied to Virgil; this possibility makes the aggressive character of the reverse clear. A strong argument for the identification of the crab to the imperial murderer may be found in the fourth book of Elegies of Propertius, where we read a severe warning against the Crab: Octipedis Cancri terga sinistra time, IV, 1, 150 (“fear the avaricious back of the Crab, eight-footed Cancer!”).\nAll things considered, all of the interpretations above-cited seem to contribute to clarify the mysterious signification of this extraordinary aureus. Nevertheless, it will certainly continue to fascinate and confuse both scholars and spectators.","The following canto is reprinted from Clive James’ new translation of The Divine Comedy, out now from Liveright.\nAt the mid-point of the path through life, I found\nMyself lost in a wood so dark, the way\nAhead was blotted out. The keening sound\nI still make shows how hard it is to say\nHow harsh and bitter that place felt to me—\nMerely to think of it renews the fear—\nSo bad that death by only a degree\nCould possibly be worse. As you shall hear,\nIt led to good things too, eventually,\nBut there and then I saw no sign of those,\nAnd can’t say even now how I had come\nTo be there, stunned and following my nose\nAway from the straight path. And then, still numb\nFrom pressure on the heart, still in a daze,\nI stumbled on the threshold of a hill\nWhere trees no longer grew. Lifting my gaze,\nI saw its shoulders edged with overspill\nFrom our sure guide, the sun, whose soothing rays\nAt least a little melted what that night\nOf dread had done to harden my heart’s lake—\nAnd like someone who crawls, half dead with fright,\nOut of the sea, and breathes, and turns to take\nA long look at the water, so my soul,\nStill thinking of escape from the dark wood\nI had escaped, looked back to see it whole,\nThe force field no one ever has withstood\nAnd stayed alive. I rested for a while,\nAnd then resumed, along the empty slope,\nMy journey, in the standard crofter’s style,\nWeight on the lower foot. Harder to cope\nWhen things got steeper, and a mountain cat\nWith parti-coloured pelt, light on its feet,\nIn a trice was in my face and stayed like that,\nBarring my way, encouraging retreat.\nThree beasts—was this the leopard, Lechery?—\nWere said to block the penitential climb\nFor sinners and for all society,\nAnd here was one, sticking to me like lime.\nNot only did it hamper me, it made\nMe think of turning back. Now was the time\nMorning begins. The sun, fully displayed\nAt last, began its climb, but not alone.\nThe stars composing Aries, sign of spring,\nWere with it now, nor left it on its own\nWhen the First Love made every lovely thing\nThe world can boast: a thought to give me heart\nThat I might counter, in this gentle hour\nOf a sweet season, the obstructive art,\nPretty to see but frightful in its power,\nOf that cat with the coloured coat. But wait:\nIf fear had waned, still there was fear enough\nTo bring on Pride, the lion, in full spate:\nHead high, hot breath to make the air look rough—\nAs rocks in summer seem to agitate\nThe atmosphere above them without cease—\nSo rabid was its hunger. On its heels\nThe wolf appeared, whose name is Avarice,\nMade thin by a cupidity that steals\nInsatiably out of its own increase,\nObtained from many people it made poor.\nThis one propelled such terror from its face\nInto my mind, all thoughts I had before\nOf ever rising to a state of grace\nWere crushed. And so, as one who, mad for gain,\nMust find one day that all he gains is lost\nIn a flood of tears, a conscience racked with pain,\nJust so I felt my hopes came at the cost\nOf being forced, by this unresting beast,\nLittle by little down towards that wood\nWhose gloom the sun can never in the least\nIrradiate. But all at once there stood\nBefore me one who somehow seemed struck dumb\nBy the weight of a long silence. “Pity me,\nAnd try to tell me in what form you come,”\nI cried. “Is it a shade or man I see?”\nAnd he replied: “No, not a man. Not now.\nI was once, though. A Lombard. Parents born\nIn Mantua. Both born there.” That was how\nHis words emerged: as if with slow care torn,\nLike pages of a book soaked shut by time,\nFrom his clogged throat. “Caesar was getting on\nWhen I was young. That’s Julius. A crime,\nHis death. Then, after he was gone,\nI lived in Rome. The good Augustus reigned.\nThe gods were cheats and liars. As for me,\nI was a poet.” He grew less constrained\nIn speech, as if trade-talk brought fluency.\n“I sang about Anchises’ son, the just\nAeneas, pious, peerless. When proud Troy\nWas burned to ashes, ashes turned to dust\nWhich he shook off his feet, that marvellous boy.\nHe did what any decent hero must:\nSet sail. But you, you turn back. Tell me why.\nWhy not press on to the delightful peak?\nThe root cause of all joy is in the sky.”\nAlmost too shocked and overawed to speak—\nFor now the one who fought for words was I—\nI asked him, just as if I didn’t know:\n“Are you Virgil? Are you the spring, the well,\nThe fountain and the river in full flow\nOf eloquence that sings like a seashell\nRemembering the sea and the rainbow?\nOf all who fashion verse the leading light?\nThe man of honour? What am I to say?\nThrough learning you by heart I learned to write.\nMy love for your book turned my night to day.\nYou are my master author. Only you\nCould teach me the Sweet Style that they call mine.\nI could go on. But what am I to do\nAbout this animal that shows no sign\nOf letting me proceed? It scares me so,\nMy veins are empty, all the blood sucked back\nInto the heart. There’s nothing you don’t know,\nMy sage, so tell me how this mad attack\nCan be called off.” Then he: “You need to choose\nAnother route.” This while he watched me weep.\n“This way there’s no way out. You’re bound to lose:\nBound by the spell of this beast pledged to keep\nYou crying, you or anyone who tries\nTo get by. In a bad mood it can kill,\nAnd it’s never in a good mood. See those eyes?\nSo great a hunger nothing can fulfil.\nIt eats, it wants more, like the many men\nInfected by its bite. Its catalogue\nOf victories will be finished only when\nAnother dog arrives, the hunting dog:\nThe Veltro. As for now, it’s hard to see\nEven his outline through the glowing fog\nOf the future, but be assured by me—\nThe Veltro will make this thing die of shame\nFor wanting to eat wealth and real estate.\nThe Veltro’s diet will be bigger game:\nLove, wisdom, virtue. It will operate\nIn humble country, eat the humble bread\nOf that sad Italy where Trojans fought\nOur local tribes: the Latium beachhead.\nThe brave Princess Camilla there was brought\nTo death in battle, and Prince Turnus, too—\nKilled by Aeneas, of whose Trojan friends\nEuryalus and Nisus died. The new\nGreat Dog will harry this one to the ends\nOf that scorched earth and so back down to Hell,\nFrom which, by envious Lucifer, it was\nFirst sent forth. But by now I’ve pondered well\nThe path adapted best to serve your cause,\nSo let me be your guide. I’ll take you through\nThe timeless breaker’s yard where you will hear\nThe death cries of the damned who die anew\nEach day, though dead already in the year—\nNo dated stones remain to give a clue—\nThe earliest sinners died, when time began.\nAnd you’ll see, in the next eternal zone,\nThose so content with purging fire they fan\nThe flames around them, thankful to atone,\nHopeful of being raised to join the blessed.\nIf you would join them too, we’ll reach a stage\nWhen only someone else shows you the rest:\nSomeone more worthy, though of tender age\nBeside me. I can’t tell you her name yet,\nBut what I can say is, the Emperor\nWho reigns on high vows he will never let\nA non-believer—though I lived before\nBelief was possible—see where he sits\nIn judgement and in joy with the elect.”\nSad and afraid, but gathering my wits,\n“Poet,” I said, “I ask you to effect,\nIn the name of that God you will never see,\nAn exit for me from this place of grief,\nAnd then an entry to where I would be—\nBeyond the purging flames of which you tell—\nIn sight of Peter’s Gate, though that relief\nDemands for prelude that I go through Hell.”\nAnd then he moved, and then I moved as well."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:34d03e38-ef82-4c5c-a59e-21fedb2f605d>","<urn:uuid:27865b0d-ff26-4819-b7d1-b18e16fcadf3>"],"error":null}
{"question":"Can you list the specific digestive health benefits of both yogurt and kefir consumption?","answer":"Yogurt helps prevent antibiotic-associated diarrhea, aids with yeast infections in some women, and contains beneficial lactic acid bacteria like acidophilus. The documents show that yogurt bacteria can survive the digestive tract to provide these benefits. As for kefir, it contains probiotics that help balance the internal flora of the digestive system, which is vital for proper intestinal function. It can help with digestive irregularity, though it may cause initial stomach discomfort during the first two weeks as the body adjusts to the probiotics.","context":["It's hard to imagine that anyone hasn't heard of the health benefits of yogurt.\nIt is full of lactic acid bacteria - which is a very good thing - and in a variety of studies the consumption of yogurt has shown that it:\nYogurt can help your health in so many different ways. It can make a real difference to your calcium intake. It is also a valuable source of protein for vegetarians with approximately 6 grams of protein in a 6oz tub of yogurt. If you're interested in facts about yogurt, here is a link for yogurt nutrition values.\nA common probiotic in yogurt is acidophilus and this bacteria has so many proven benefits.\nYogurt has helped some women who have yeast infection. I'll be up front - it doesn't help everybody, but it is PROVEN to help SOME women. Give it a go. Check this out for help on how to get the yogurt in there.\nThe benefits of yogurt extend to taking it during and after treatment with antibiotics. Research shows that yogurt can help with antibiotic diarrhea and even the Mayo Clinic suggests eating yogurt during treatment. It must be a brand of yogurt that contains viable bacteria to be of assistance.\nHomemade yogurt is so good for you and you control the production every step of the way. This is where you really can introduce all the health benefits of yogurt. You can make it from any type of milk that you choose - cow, sheep, soy, camel! - and add as much or as little sweetening as you like. The health benefits of yogurt increase even further when you eat it freshly made while it is still bursting with lactic acid bacteria. And it doesn't need to be complicated. Take a look at how simple it can be to learn how to make yogurt.\nHow to flavor yogurt need not be a problem - I simply add my choice of jam to it. My favorite is flavored with my homemade wild blackberry jam! Try ice cream toppings, maple syrup or honey if you want a sweet yogurt. Try chives and fresh herbs for a savory yogurt flavor.\nAnd you need to read this if you want to know how to make probiotic yogurt. I advise against it - see why?\nNo need to hang your head in shame if you prefer to buy your yogurt. With a little care, commercial yogurt can be just as good for you as your homemade brew. The important points to remember are;\nAnd if you're concerned about the Activia lawsuit then I hope this article will set your mind at rest.\nAs an update, the Activia Settlement (of the above lawsuit) means that some consumers may be able to receive up to $100. Check out that link if you believe you qualify.\nLactobacillus casei is found in DanActive a popular fermented drink. Expensive, but some people find it helpful with gastrointestinal troubles although it has also GIVEN a few folk upset tummies! Too many good bacteria introduced too quickly can do that.\nNot sure whether good bacteria survive the trip through your digestive tract so that you really do enjoy the health benefits of yogurt? Follow that link and you'll see that they do.\nYogurt cheese is another of those things that can be so dang good for you and tastes good too. And easy! Did I say it was easy to make! You'll find a yogurt cheese recipe right here. The benefits of yogurt cheese include its health giving properties, but as well, it's a very versatile food - you can eat it a hundred different ways and that applies to Bulgarian yogurt. I've even thrown in a bit of history. What more could you want!\n1. Wang H, Livingston KA, Fox CS, Meigs JB, Jacques PF. Yogurt consumption is associated with better diet quality and metabolic profile in American men and women. Nutr Res. 2013 Jan;33(1):18-26. doi: 10.1016/j.nutres.2012.11.009. Epub 2012 Dec 27.\n2. Toledo E, Delgado-Rodriguez M, Estruch R, Salas-Salvado J, Corella D, Gomez-Gracia E, Fiol M, Lamuela-Raventos RM, Schröder H, Aros F, Ros E, Ruiz-Gutierrez V, Lapetra J, Conde-Herrera M, Saez G, Vinyoles E, Martinez-Gonzalez MA. Low-fat dairy products and blood pressure: follow-up of 2290 older persons at high cardiovascular risk participating in the PREDIMED study. Br J Nutr. 2009 Jan;101(1):59-67. doi: 10.1017/S0007114508981496. Epub 2008 May 20.\n3. de Vrese M, Laue C, Offick B, Soeth E, Repenning F, Thoß A, Schrezenmeir J. A combination of acid lactase from Aspergillus oryzae and yogurt bacteria improves lactose digestion in lactose maldigesters synergistically: A randomized, controlled, double-blind cross-over trial. Clin Nutr. 2014 Jul 8. pii: S0261-5614(14)00179-4. doi: 10.1016/j.clnu.2014.06.012. [Epub ahead of print]\n4. Beniwal RS, Arena VC, Thomas L, Narla S, Imperiale TF, Chaudhry RA, Ahmad UA.A randomized trial of yogurt for prevention of antibiotic-associated diarrhea. Dig Dis Sci. 2003 Oct;48(10):2077-82.\n5. Rizzoli R. Dairy products, yogurts, and bone health. Am J Clin Nutr. 2014 May;99(5 Suppl):1256S-62S. doi: 10.3945/ajcn.113.073056. Epub 2014 Apr 2.\n6. Shadnoush M, Shaker Hosseini R, Mehrabi Y, Delpisheh A, Alipoor E, Faghfoori Z, Mohammadpour N, Zaringhalam Moghadam J. Probiotic yogurt Affects Pro- and Anti-inflammatory Factors in Patients with Inflammatory Bowel Disease. Iran J Pharm Res. 2013 Fall;12(4):929-36.\n7. Magro DO, de Oliveira LM, Bernasconi I, Ruela MD, Credidio L, Barcelos IK, Leal RF, Ayrizono MD, Fagundes JJ, Teixeira LD, Ouwehand AC, Coy CS.Effect of yogurt containing polydextrose, Lactobacillus acidophilus NCFM and Bifidobacterium lactis HN019: a randomized, double-blind, controlled study in chronic constipation. Nutr J. 2014 Jul 24;13(1):75. [Epub ahead of print]\n8. Narushima S1, Sakata T, Hioki K, Itoh T, Nomura T, Itoh K. Inhibitory effect of yogurt on aberrant crypt foci formation in the rat colon and colorectal tumorigenesis in RasH2 mice. Exp Anim. 2010;59(4):487-94.\n9. Adolfsson O, Meydani SN, Russell RM. Yogurt and gut function. Am J Clin Nutr. 2004 Aug;80(2):245-56.\n10. Tsuchiya A1, Almiron-Roig E, Lluch A, Guyonnet D, Drewnowski A. Higher satiety ratings following yogurt consumption relative to fruit drink or dairy fruit drink. J Am Diet Assoc. 2006 Apr;106(4):550-7.\n11. Poutahidis T, Kleinewietfeld M, Smillie C, Levkovich T, Perrotta A, Bhela S, Varian BJ, Ibrahim YM, Lakritz JR, Kearney SM, Chatzigiagkos A, Hafler DA, Alm EJ, Erdman SE. Microbial reprogramming inhibits Western diet-associated obesity. PLoS One. 2013 Jul 10;8(7):e68596. doi: 10.1371/journal.pone.0068596. Print 2013.\n12. Lakritz JR, Poutahidis T, Levkovich T, Varian BJ, Ibrahim YM, Chatzigiagkos A, Mirabal S, Alm EJ, Erdman SE. Beneficial bacteria stimulate host immune cells to counteract dietary and genetic predisposition to mammary cancer in mice. Int J Cancer. 2014 Aug 1;135(3):529-40. doi: 10.1002/ijc.28702. Epub 2014 Jan 10.\n13. Kim MN, Kim N, Lee SH, Park YS, Hwang JH, Kim JW, Jeong SH, Lee DH, Kim JS, Jung HC, Song IS. The effects of probiotics on PPI-triple therapy for Helicobacter pylori eradication. Helicobacter. 2008 Aug;13(4):261-8. doi: 10.1111/j.1523-5378.2008.00601.x.\n14. Kuo CH, Wang SS, Lu CY, Hu HM, Kuo FC, Weng BC, Wu CC, Liu CJ, Tsai PY, Lee TC, Chen LW, Cheng KH, Chang LL, Wu DC. Long-Term Use of Probiotic-Containing Yogurts Is a Safe Way to Prevent Helicobacter pylori: Based on a Mongolian Gerbil's Model. Biochem Res Int. 2013;2013:594561. doi: 10.1155/2013/594561. Epub 2013 Nov 21.\nStreptococcus salivarius K12 is a probiotic bacteria that cures bad breath. Check out these products to stop bad breath in its tracks!","How Do I Know if my Kefir is Good?\nWhat is Kefir?\nKefir is a popular alternative remedy for digestive issues. The probiotics in kefir are good for balancing out the internal flora of the digestive system. Since the correct balance of bacterial life in the digestive tract is vital to proper working of the intestines, probiotics like kefir are a great choice for anyone suffering from irregularity.\nIt can take about two weeks of regular usage before the digestive system gets used to the probiotics in kefir. For best results, you should drink about a cup of kefir every day. Depending on your diet before you start using kefir, you might notice some stomach discomfort initially. This does not mean your kefir is not good, it just means that your body is trying to adjust.\nKefir can be taken at any time of day. There is no evidence that taking kefir on either a full or an empty stomach affects the ability of the bacteria to colonize in your intestines. This means that you can take kefir with or before your breakfast, as a snack during the day, or late in the evening. Drink your kefir at whatever time best suits your needs and your schedule.\nIs My Kefir Good to Drink?\nEven though it is fermented, kefir will go bad over time, just like any other dairy product. While the bacterial cultures in kefir are good for your digestive tract, once they overgrow the kefir is no longer drinkable. Since kefir is a relatively expensive product, it can be tempting not to throw it away when you become suspicious of its quality. The safe thing to do, though, is to throw it out if you think it is not good to drink anymore.\nFor people unfamiliar with kefir, it can be somewhat difficult to tell if it has gone bad. It has a different texture and smell from other milk products. Unlike how every gallon of milk is the same, one bottle of milk kefir will not have the same feel, smell, or taste as another. Each batch will be slightly different, especially if the kefir is homemade or craft. But there are some tell-tale signs that it is time to get rid of a bottle of kefir.\nKefir naturally has a thick texture and may have chunks in it while still drinkable. It should look and feel like buttermilk. If the whole container has gotten chunky, however, the kefir is no longer safe to drink. It might also look like water mixed with larger, more solid chunks, which means it has gone bad.\nAnother obvious sign that kefir is no longer good is if there is mold growing on it. If there is anything fuzzy on top of the kefir, throw it out. Also, kefir should be generally the same color throughout the mixture. If there are patches of another color, especially one that contrasts with the color of the ingredients, that is another sign of mold infestation.\nWhen you first open a bottle of kefir, it might smell a bit stronger than you expect. It may even have a sour or vinegary smell. This is normal. But if the smell becomes harsh or is much stronger than when the carton was initially opened, it is safer to let it go.\nThe same is not true for taste, however. Just because it tastes too strong to comfortably drink does not mean it has become unsafe. In the absence of other signs that it has gone bad, sour kefir can still be consumed. Try using it in a recipe, such as biscuits or pancakes, where the sour flavor will fit in.\nKefir purchased at a regular grocery store, rather than directly from a producer, will have a date on it. You can use this date to determine if your kefir is still good, but it is more of an indication as to the optimal time frame for consumption, not an absolute measure of safety.\nHow to Store Kefir\nIf left at room temperature, kefir will only keep for about a day or two. If kept in the refrigerator at around 40- or 50-degrees F, it will remain drinkable for one to two weeks.\nIf you really want your kefir to last longer, such as if you bought or made a large batch at once, you will want to freeze it. Freezing kefir does not kill the healthy bacteria in it, it just puts them into a suspended state. Frozen kefir can be kept for months. Once you are ready to eat it, transfer the container to the refrigerator and allow it to defrost. It will take about 24 hours before the defrosted kefir is ready to eat.\nIf you are making your own kefir, you will need to ferment it at room temperature. However, once the fermentation is complete, the kefir should be stored at refrigerator temperatures for it to last longer. Store-bought kefir should always be kept in the fridge. The travel time between manufacturer, store, and your home is long enough that room-temperature kefir would already have gone bad."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"format_constrained"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:f23f90d7-c832-4747-9967-4943bddadb32>","<urn:uuid:86994d7b-6c08-482c-aee4-14d0019b6ebc>"],"error":null}
{"question":"Hi! I am studying physics and I'm curious: what are the scientific applications of quantum systems in research, and what security threats do they pose?","answer":"Quantum systems have significant research applications, particularly in studying quantum materials and simulations. They allow scientists to observe single atoms at ultra-low temperatures (5 billionths above absolute zero) where quantum mechanics rules apply, helping understand phenomena like superconductivity and quantum magnetism. Using tools like quantum gas microscopes, researchers can study arrays of thousands of single atoms to explore complex physics. However, quantum systems also pose major security threats. Quantum computers could crack current encryption methods (RSA and ECC) exponentially faster than classical computers. For example, breaking a 2048-bit RSA key would take 1 billion years with a classical computer but only 100 seconds with a quantum computer, potentially compromising sensitive data and cybersecurity infrastructure worldwide.","context":["Quantum Gas Microscope Offers Glimpse Of Quirky Ultracold Atoms\nResearch creates a readout system for quantum simulation and computation\nPhysicists at Harvard University have created a quantum gas microscope that can be used to observe single atoms at temperatures so low the particles follow the rules of quantum mechanics, behaving in bizarre ways.\nThe work, published this week in the journal Nature, represents the first time scientists have detected single atoms in a crystalline structure made solely of light, called a Bose Hubbard optical lattice. It’s part of scientists’ efforts to use ultracold quantum gases to understand and develop novel quantum materials.\n“Ultracold atoms in optical lattices can be used as a model to help understand the physics behind superconductivity or quantum magnetism, for example,” says senior author Markus Greiner, an assistant professor of physics at Harvard and an affiliate of the Harvard-MIT Center for Ultracold Atoms. “We expect that our technique, which bridges the gap between earlier microscopic and macroscopic approaches to the study of quantum systems, will help in quantum simulations of condensed matter systems, and also find applications in quantum information processing.”\nThe quantum gas microscope developed by Greiner and his colleagues is a high-resolution device capable of viewing single atoms — in this case, atoms of rubidium — occupying individual, closely spaced lattice sites. The rubidium atoms are cooled to just 5 billionths of a degree above absolute zero (-273 degrees Celsius).\n“At such low temperatures, atoms follow the rules of quantum mechanics, causing them to behave in very unexpected ways,” explains first author Waseem S. Bakr, a graduate student in Harvard’s Department of Physics. “Quantum mechanics allows atoms to quickly tunnel around within the lattice, move around with no resistance, and even be ‘delocalized’ over the entire lattice. With our microscope we can individually observe tens of thousands of atoms working together to perform these amazing feats.”\nIn their paper, Bakr, Greiner, and colleagues present images of single rubidium atoms confined to an optical lattice created through projections of a laser-generated holographic pattern. The neighboring rubidium atoms are just 640 nanometers apart, allowing them to quickly tunnel their way through the lattice.\nConfining a quantum gas — such as a Bose”“Einstein condensate — in such an optically generated lattice creates a system that can be used to model complex phenomena in condensed-matter physics, such as superfluidity. Until now, only the bulk properties of such systems could be studied, but the new microscope’s ability to detect arrays of thousands of single atoms gives scientists what amounts to a new workshop for tinkering with the fundamental properties of matter, making it possible to study these simulated systems in much more detail, and possibly also forming the basis of a single-site readout system for quantum computation.\n“There are many unsolved questions regarding quantum materials, such as high-temperature superconductors that lose all electrical resistance if they are cooled to moderate temperatures,” Greiner says. “We hope this ultracold atom model system can provide answers to some of these important questions, paving the way for creating novel quantum materials with as-yet unknown properties.”\nOn the Net:","Written by: Anastasia Marchenkova\nImagine having a super-fast computer that could solve problems and crunch data faster than anything we've seen before. It can develop new medicine more quickly and cheaply, solve energy efficiency problems in batteries and solar panels, and create better materials to withstand heat and strain.\nSounds great, right? But, there's a catch. These super-fast computers, or quantum computers, could crack the security measures we use to keep our online information safe. This is the “quantum threat” – the potential for quantum computers to crack many of today's encryption schemes, which could result in a collapse of our modern-day cybersecurity infrastructure.\nOur current encryption methods, which keep our digital communications safe, heavily rely on the assumption that certain mathematical problems are hard to solve with classical computers. RSA encryption is based on factoring large numbers, while elliptic curve cryptography (ECC) is based on finding the discrete logarithm of a random elliptic curve element. Both of these problems are computationally difficult for classical computers.\nThe concept of a 'quantum threat' gained significant attention after the proposal of Shor's algorithm by mathematician Peter Shor in 1994. Shor's algorithm was quantum’s killer app, being the first to demonstrate the potential power of a large-scale, fully functional quantum computer in the real world. This algorithm could unlock the digital 'locks' we currently use to secure our online world.\nShor’s algorithm can factor large numbers exponentially faster than the best-known algorithms running on classical computers. The implication for encryption is profound. If a large and error-corrected quantum computer could run Shor's algorithm, it could crack these encryption methods, posing a significant threat to digital security worldwide.\nAnd how fast? Exponentially faster is very fast. Breaking a 2048-bit RSA key would take 1 billion years with a classical computer. A quantum computer could do it in 100 seconds.\nThe immediate focus on examining post-quantum security solutions is no longer optional. Ironically, ECC was recommended over RSA because it offered the same level of security but used shorter keys, resulting in faster computation and less demand for resources such as storage and bandwidth. This made ECC a more efficient and appealing choice for a broad range of applications, from securing web traffic to protecting sensitive government data. However, ECC is even more vulnerable to quantum computers than RSA, requiring even less qubits for the calculation.\nSome data are just valuable for a day, or less. Your job requires you to reset your password every 3 months. So, in the worst-case scenario, a large-scale coherent quantum computer will be released and every website will force us to reset our passwords, encrypted with a new cryptosystem. No big deal.\nBut what about data vital to national security that is being sent now with quantum-crackable encryption, and can jeopardize national security even in 50 or 100 years if released? What about Bitcoin, built on elliptic curve encryption as the foundation of all its transactions - and is very hard to change because of its trustless, distributed nature? Or was your social security number sent over the web using ECC? Your WorldCoin biometric identity? You can’t replace your iris.\nThese data breaches can devastate businesses, industries, and individuals. They can lead to a loss of consumer trust, damage to reputation, and significant financial loss. The Equifax breach in 2017 exposed the personal data of 143 million people, resulting in a settlement of $425 million just to help people affected by the breach. And that was not a fundamental encryption issue. What we could be facing is a data breach armageddon.\nThe impacts of a large quantum computer are profound on fundamental science. Trillions of dollars are being invested into quantum computing because of applications in materials, healthcare, and financial modeling. But with worldwide security at risk, the race to build a large-scale quantum computer is also highly funded by governments. China, Russia, the US, the UK, and many others are putting hundreds of billions into quantum technology. For instance, the US CHIPS Act provides roughly $280 billion in new funding to boost domestic research and manufacturing of semiconductors in the United States.\nThere is some disagreement on when the threat will arrive. As of mid-2023, we are in the era of Noisy Intermediate-Scale Quantum (NISQ) computers. NISQ devices are characterized by having just tens to a few hundred qubits (quantum bits, similar to bits, but are two-level quantum mechanical systems that can be prepared in a probability of a 0 or 1) and noise in quantum operations that introduces calculation errors. These machines, while needing to scale up to break modern encryption, are growing rapidly. While there are scalability challenges to address in quantum hardware, quantum computers have been accelerating in qubit counts, while gate infidelity, or the error rates applying quantum circuits, have been decreasing.\nWhile we still have time before the quantum threat, the scaling and error issue is being attacked from multiple sides. Researchers are focused on manufacturing techniques for superconducting hardware to scale on qubit count and fidelity, new modalities are emerging, like photonic-based quantum computers, and theorists are working on new error correction algorithms to reduce the number of supporting qubits. In the last few years, co-designed application-specific processors became the most viable path to quantum advantage to reduce qubit count and error correction needed to scale.\nMicrosoft Research has calculated around 2500 qubits are needed to compute elliptic curve discrete logarithms to crack a standard 256-bit key. Around 4000 qubits are needed for 2048-bit RSA. While these are logical qubits, not physical (logical qubits will require additional physical qubits for error correction), the resource estimates keep decreasing.\n10 years ago, John Preskills’s lecture noted you would need 10 million physical qubits to break RSA cryptography. In 2023, a PRL paper, \"Performance Analysis of a Repetition Cat Code Architecture: Computing 256-bit Elliptic Curve Logarithm in 9 Hours with 126 133 Cat Qubits” was released showing two orders of magnitude reduction in qubit counts. Assuming a similar pace of two orders of magnitude reduction of qubit counts in the next 10 years, we may only need ~2000 qubits to break RSA encryption. BTQ’s QByte Quantum Risk Calculator tracks qubit counts and quantum infidelity. Even pessimistic estimates place that threshold by 2030, without any advances in error correction or fabrication.\nGiven the potential vulnerabilities, there is a growing consensus on the need for preemptive action. This involves adopting post-quantum cryptography algorithms that are resistant to quantum attacks. While there's uncertainty about when a quantum computer capable of breaking current cryptography will exist, the risk is significant enough to warrant immediate action.\nIndustry and academia are closely collaborating to advance post-quantum security. For instance, NIST's Post-Quantum Cryptography Standardization project involves both academic and industry players in the development and evaluation of new cryptographic systems. BTQ’s post-quantum cryptography scheme, Preon, was recently selected by NIST as a candidate for the Post-Quantum Cryptography Standard.\nPost-quantum cryptographic algorithms are designed to resist attacks by classical and quantum computers. These include lattice-based, hash-based, code-based, multivariate polynomial, and isogeny-based cryptographic algorithms. Each approach has pros and cons regarding security, performance, and key and ciphertext sizes.\nWhile post-quantum cryptographic algorithms are still being studied, evaluations show promise in their resistance against known quantum attacks. However, more research is needed to identify unknown vulnerabilities and thoroughly assess their security.\nEven though the standard has not yet been released, large tech companies, including Google, Microsoft, and IBM, are testing post-quantum algorithms, and some have already started implementing post-quantum security in their products and services. Blockchains, like QRL and Nexus, are being built “quantum-first” to protect against the risks of large-scale quantum computers.\nAs post-quantum cryptographic standards emerge, organizations may need to comply with new regulatory requirements that take adequate measures to secure data against quantum threats.\nWhile the new standards and cryptosystems are open source, cryptography isn’t enough for this new, unknown world where crypto agility is critical. Vendors approved by government players as post-quantum solutions will have a head start - especially since national security is a crucial priority and holder of a lot of long-term data. The transition to post-quantum cryptography will be more complex for companies in regulated industries like finance or healthcare. These companies will need to balance the adoption of new technologies with compliance with existing and emerging regulations.\nThe advent of quantum computing is a double-edged sword, bringing immense promise, but significant threats to digital security. It's crucial for governments, industries, and academia to work together to develop, implement, and scale the adoption of robust post-quantum cryptographic algorithms. Although the active quantum threat is still a future concern, the time to act is now."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:4ebdbdf8-ebd4-43e7-a84d-8eb1c0cae4a8>","<urn:uuid:61f051f1-2534-4ed7-95d4-ff759f2c6751>"],"error":null}
{"question":"As a naval historian studying Japanese World War II strategy, I'm curious about the paradox of Japan's aircraft carrier development - how did their early carrier innovations compare to their later wartime challenges?","answer":"While Japan was innovative in carrier operations in the 1930s, particularly in being the first to plan and execute operations with concentrated groups of up to 6 large carriers (as seen at Pearl Harbor and the Indian Ocean), they faced severe limitations by 1943. Although their carrier designs were on par with US and British carriers of the time, their ultimate downfall came from poor aircrew training policies. Despite early superiority with 9 carriers and unmatched effectiveness in the Pacific at the start of the war, Japan could not sustain their initial advantage. By early 1943, pilot quality had deteriorated severely due to wartime attrition and inadequate training programs. This led to the desperate formation of Kamikaze squadrons by mid-1944, a stark contrast to their earlier strategic sophistication.","context":["J-Ships.com has been existence for more than a decade. I have taken it upon myself to rebuild the site from the ground up using WordPress. As I transfer information over, I will post the new additions here for your quick reference!\nDo You Zuiho?\nThe Zuiho build in 1:100 has been completed and looks great! Coupled with this build is the reference page dedicated to the Zuiho Class.\nTo see the Zuiho Reference Page, click here.\nTo see the Zuiho Build pages, click here.\nTo see the completed Zuiho, click here.\nBattleships, Carriers and Subs! Oh my!\nEver woken up in the middle of the night wondering what the ship-horse-power of the Kuma class of light cruisers was? Ever wonder what the Hiragana was for the battleship Fuso? Ever wonder which ships were sunk during the first half of June in 1943? Well, wonder no more; this site has all of that information and more.\nBy 1930 the Japanese were quickly coming to the realization that they could never be on par with the American and British navies with regards to number of ships. Not only were the naval treaties of the time hamstringing the Japanese expansionist dreams, but the potential industrial might of the United States and the vast natural resources available to the United Kingdom via the British Empire left Japan a distant third on the world stage on all things navy.\nThe Japanese faced another very real problem: manpower. When performing war plan scenarios with different nations, the principal four belligerents (UK, USA, China, USSR) vastly out stripped the Japanese in population as well: potential sailors, soldiers and airmen. The Japanese, with their naval designs, sought to counter these two very real and insurmountable limitations with the “bigger and better” mindset.\nIt was during the 1930’s that the keels of some exceptional ships were laid down; from the super destroyer Shimakaze, to the super battleships of the Yamato class, to the super light cruisers of the Mogami class, to the super submarines of the I-400 class; the Japanese indeed practiced the art of designing the biggest and best ships in the World.\nOne integral area that the Japanese were globally on par with, but did not excel in, was Aircraft Carriers. The design of their aircraft carriers in the 1930’s was impressive, but the same could be said for the USN and the RN carriers of the time. Each nation took different ideas and incorporated them into the design of their carriers. The Japanese and Americans actually seemed to learn the same things in the 1930’s and their carriers were quite similar with regards to protections and size. The Japanese however were truly the first to plan and execute wartime operations with the concentrated firepower afforded by groupings of up to 6 large aircraft carriers at a time (Pearl Harbor, Indian Ocean).\nUnfortunately, for all the time and effort put into the design of these fantastical vessels the Japanese were short sighted on one key component and it proved their ultimate downfall. It is not directly correlated with their ship designs, but rather how ships of any class were at the utter mercy of air power. Air power, and more specifically, attack aircraft, would wreck havoc on all nations navies during the course of the war. The reader may quip, “But the Japanese excelled at naval air power, and even land based air power, in the early stages of WWII”. I agree with this sentiment wholeheartedly. Where the Japanese failed more than nearly any other nation in that conflict, and saw the fortunes of war turn against them within the first 12 months of fighting, was in their air crew training. One can excuse them for holding on too long to aircraft types that became more and more outclassed as the war progressed (examples: Mitsubishi Zero or the KI-43), but their lack of foresight of how they would cope with the inevitable wartime attrition of their experienced air crews is inexcusable. By early 1943 (less than 18 months after hostilities opened) the quality of the Japanese pilot had deteriorated to such an extent that Japan could never hope to recover. Couple this reality with the new aircraft the Allies were fielding and the survival rate of green Japanese pilots was atrocious and untenable. Mid-1944 brought the sobering reality to Japan as they unofficially formed the first suicide squadrons during the defense of the Philippines. These Kamikaze squads became official and, alarmingly, were the most successful component of Japanese air power in the waning months of the war: They just needed to train the pilots to take off, follow a lead plane and point their plane at a ship – a far cry from training a fully capable pilot in the late 1930’s.\nIt is understood that the Japanese had to be beaten back to their shores and brought to task for their aggression. Most historians agree that the minute the Japanese bombed Pearl Harbor that they had lost the war. They did not have the manpower or natural resources to continue to fight the Americans, let alone the British in late 1944 and the Russians in mid-1945. They took every step they could with the design and deployment of the majority of their naval vessels and it shows in hindsight when side by side comparisons are made today. One can only speculate on how much longer the war would have dragged on and how many more people would have perished if the Japanese had redesigned their air crew training before the outset of hostilities with the Allies. If they had adopted a more American approach to training.","Aircraft Carrier Operations During WW2\nContributor: Bob Bryant\nAircraft carriers played a large part in deciding the outcome of World War II's decisive victories at sea that, in turn, created the essential conditions for the war's decisive victories on land. Shortly after the war began, carriers bypassed battleships and and became the capital ships around which combatant navies centered their fleets for major offensive operations. Carrier aircraft could deliver bombs and torpedoes to targets hundreds of miles away, while even battleships with the largest guns could only rain destruction down on targets within 26 miles. Accordingly, the major naval powers, the United States, United Kingdom, and Japan, commissioned a total of only 23 new battleships during the war but commissioned a total of 55 new fleet carriers (CV) and light carriers (CVL). A total of 145 escort carriers (CVE) and merchant aircraft carriers (MAC) were also launched.\n|Production of aircraft carriers, September 1939 - August 1945|\nDuring the entire war, battleship guns sank only a single fleet carrier and a single battleship. In comparison, carrier-launched aircraft damaged, sank, or took part in sinking 19 battleships, including eight at Pearl Harbor. Along with Pearl Harbor, the earlier use of carrier-launched aircraft at Taranto demonstrated their effectiveness. After the Battle of Midway, Japan began leaving battleships out of major naval engagements, and battleships of all navies were more often used for bombarding land targets than engaging in naval battles.\nSuccessful naval battles kept shipping lanes open for a combatant's movement of troops, guns, ammunition, tanks, warships, aircraft, raw materials, and food. Without the Allied victory in the Battle of the Atlantic, Britain could not have fed her people or withstood Axis offensives in Europe and North Africa. Without Britain's survival and without Allied shipments of food and industrial equipment to the Soviet Union, that country's military and economic power would likely not have rebounded in time for Russian soldiers to prevail at Stalingrad and Kursk. Without victories at sea in the Pacific Theater, the Allies could not have mounted amphibious assaults on or maintained land forces on Guadalcanal, New Guinea, Saipan, The Philippines, Iwo Jima, or Okinawa.\nNumber of Aircraft Carriers In Operation During the War\nThe United States, United Kingdom, and Japan were the only combatants that made significant use of aircraft carriers during the war. The table below shows the number of ships each country operated between July 1937 and August 1945 that had flight decks for both launching and recovering aircraft at sea. Early in the war, the Royal Navy introduced fighter catapult ships (FACs) and catapult aircraft merchant ships (CAMs) for convoy escort that could launch aircraft by catapult, but there was no provision for recovering them. Â After completing an attack, the planes were simply ditched and their pilots recovered. Â Late in the war the Japanese Army introduced some ships that had flight decks from which aircraft could be launched, but the decks were too short for the aircraft to land on. These UK and Japanese ships are not included in the table.\nFrance had one aircraft carrier, but it did not see action during the war. Canada operated two of the escort carriers included below under Britain's carriers using British aircrews. All but six of Britain's escort carriers were produced in the US.\n|Aircraft Carriers Operated, September 1939 - August 1945|\n|Fleet (CV)||Light (CVL)||Escort (CVE)||MAC||Total|\nAircraft Carrier Functions\nAircraft carriers were used in a wide variety of operations. They fought in major naval battles against enemy fleets. They supported invasions during which carrier-launched aircraft bombed enemy positions, spotted for warship bombardment guns, and delivered aircraft to land bases afterward. They raided enemy positions, keeping them off-balance and reducing their combat capability. Carrier aircraft denied shipping lanes to enemies, sank raiders and submarines, escorted conveys, and delivered aircraft to other carriers and to land bases. Carriers were also used for training pilots and, at the end of the war, helped repatriate troops to their homes.\nThe combatants differed in the functions they emphasized for carrier use, in part as a result of the challenges presented by their principal theater of operations. The table below indicates the percentage of carriers each combatant used for each function. For example, the US Navy operated a total of 33 fleet and light carriers during the war. Seventy-three percent of these carriers participated in naval battles at one time or another. Eighty-five percent were involved in support of invasions, etc. Escort carriers, although initially envisioned by many to perform only in support roles, ultimately performed all of the functions that the fleet and light carriers did.\n|Number of Carriers||Naval Battles||Invasions||Raids||Anti-Ship||Anti-Raid||Anti-Sub||Convoy Escort||Trans.||Training||Repat.|\n|CVs & CVLs|\nOverview of Operations Involving Aircraft Carriers\nBritain entered the war in 1939 with a total of seven fleet and light aircraft carriers, all but one of which was in the Atlantic Theater. Three of these carriers were sunk before the US entered the war. During the war in the Atlantic Theater, Allied carriers transported and protected transportation of troops, aircraft, tanks, oil, and food from or routed through North America or up the West African coast to Britain, Russia, North Africa, and continental Europe. Allied escort carriers screened by corvettes, destroyers, and destroyer escorts proved more successful and cost-effective than using fleet carriers, battleships, or cruisers for protecting convoys from attacks by Axis submarines, raiders, and land-based aircraft. By mid-1943, the Axis threat of cutting essential Allied supply lines had passed. By this time, however, German battleships and submarines had sunk a total of five British fleet and light carriers, as shown in the table, below. Three Allied escort carriers had also been sunk.\n|Fleet/Light Carriers Sunk|\n|1939-1945 CV & CVL||5||5||16||26|\n|1939-1945 CV, CVL, & CVE||12||8||21||41|\nAt the time Japan attacked Pearl Harbor, she had nine carriers in the Pacific and had superiority both in number and effectiveness. During the 1930s, Japan had originated and refined the massed use of carriers and carrier aircraft in combat operations. For the first six months of the war, her unequaled Mobile Force of six fleet carriers (Kido Butai) was able to roam virtually at will from Hawaii in the east to Ceylon in the West. Japan's carriers supported successful invasions of the American Philippines, British Malaya and Burma, and the Dutch East Indies.\nBetween May and October 1942, however, Japan lost six of her carriers as Japanese and United States navies fought four major naval battles between fleets centered on their aircraft carriers. During these battles, opposing warships never came within sight of each other nor fired their guns at other warships. Aircraft alone did the attacking. The first of these battles thwarted the Japanese attempt in the Coral Sea to isolate Australia. The second halted the expansion of Japanese control eastward in the Pacific toward Midway Island. The next two helped sustain the American presence on Guadalcanal. These four engagements were costly for both sides. At the time Pearl Harbor was attacked, Japan had a total of nine fleet and light carriers in the Pacific theater. The United States had four in the Pacific with another two far away in the Atlantic. At the end of October 1942, after battle attrition from sinkings and damages, Japan had only three such carriers fully operational in the Pacific theater and The United States, for a two-week period, had none. Though costly, the battles were strategically advantageous for the Americans, contributing significantly to the shift of strategic initiative in the Pacific Theater from Japan to the United States. Japan did not engage her carriers in a major initiative again until the Battle of the Philippine Sea in mid-1944.\nNumber of Operational Aircraft Carriers\nCarriers were not available for operations all of the time they were afloat. After commissioning, there were delivery periods, refits and sea trials before carriers became operational. After first becoming operational, combat damage, operational mishaps, or damage from typhoons could render a carrier not combat-ready. Taking into account all the factors that kept aircraft carriers out of combat after first becoming operational, fleet and light carriers were typically operational about 80% of the time. They were typically out of service 5% to 15% for combat-related damage repair and 5% to 10% for delivery and refitting to otherwise improve performance.\n|In Service||Damage Repair||Refitting|\n|Fleet & Light Carriers|\nThe number of operational carriers available to the combatants at any point in time affected the capability, plans, and outcomes of military operations in both the Atlantic and Pacific theaters throughout the war. Taking into consideration the time for construction, shakedown trials, delivery, refitting, combat damage, and sinkings, the table below shows the number of fleet and light carriers that were operational for each combatant in each combat theater at the end of each month. The Pacific Theater includes the Indian Ocean and the Atlantic Theater includes the Mediterranean Sea.\nThe table capsulizes the course of the war for aircraft carrier operations. It reflects how Japan more than doubled the number of her carriers between leaving the Washington/London naval treaties in 1936 and the time she attacked Pearl Harbor in December 1941. It also reflects the attrition for carriers during the formative year of 1942. It reflects the industrial capacity of the United States, illustrating how she could make good her losses of 1942 while Japan could not. Finally, it reflects the shift of British carriers to the Pacific after the Allies prevailed in battles for Atlantic waters. Note also that, after mid-1942, the percentage of aircraft carriers afloat that were in fact available for combat operations was typically less than 70%. During the Guadalcanal campaign that year, that percentage fell below 50%. From then until mid-1944, Japan did not engage her carriers in a major operation. When at last she did, the rest of her carriers were sunk or damaged.\n|Carrier Type: CV & CVL|\n|Theater:||Pacific||Atlantic||Pac & Atl||Pac & Atl||Oper.|\nThe details behind the numbers in the above tables may be found in the \"Documents\" section at the \"Reference\" tab at the World War II Database:\n- Collection of Statistics on Japanese Aircraft Carriers\n- Collection of Statistics on UK Aircraft Carriers\n- Collection of Statistics on US Aircraft Carriers\nRichard Frank (1990). Guadalcanal- The Definitive Account of the Landmark Battle.\nEvan Mawdsley (2019). The War For The Seas- A Maritime History of World War II.\nRichard Overy (1995). Why The Allies Won.\nJonathan Parshall & Anthony Tully (2005). Shattered Sword- The Untold Story of the Battle of Midway.\nFrancis Pike (2019). Hirohito's War- The Pacific War 1941-1945.\nCraig Symonds (2018). World War II At Sea.\nMark Stille (2013). The Imperial Japanese Navy In the Pacific War.\nWilliam Y'Blood (1983). Hunter-Killer US Escort Carriers in the Battle of the Atlantic.\nWilliam Y'Blood (1987). The Little Giants- U.S. Escort Carriers Against Japan.\nDictionary of American Naval Fighting Ships\nImperial Japanese Navy Page\nNaval War In The Pacific, 1941-1945\nRoyal and Dominion Navy Warships\nLast Major Update: Oct 2020\nDid you enjoy this article or find this article helpful? If so, please consider supporting us on Patreon. Even $1 per month will go a long way! Thank you.\nShare this article with your friends:\nStay updated with WW2DB:\nVisitor Submitted Comments\nAll visitor submitted comments are opinions of those making the submissions and do not reflect views of WW2DB.\n- » WW2DB's 16th Anniversary (29 Dec 2020)\n- » Hidekazu Tamura shared his feelings toward his war time internment (2 Sep 2020)\n- » WW2DB's 15th Anniversary (29 Dec 2019)\n- » Japan and Russia to continue negotiations on the Kuriles territorial dispute (22 Nov 2019)\n- » Wreck of Akagi Found (21 Oct 2019)\n- » Wreck of Kaga Found (18 Oct 2019)\n- » See all news\n- » 1,094 biographies\n- » 332 events\n- » 38,156 timeline entries\n- » 1,125 ships\n- » 337 aircraft models\n- » 190 vehicle models\n- » 352 weapon models\n- » 117 historical documents\n- » 225 facilities\n- » 464 book reviews\n- » 27,146 photos\n- » 351 maps\nGeorge Patton, 31 May 1944"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:9d45251f-5c53-4f0a-a685-037510ce2df0>","<urn:uuid:1ce9b118-13c9-4cd3-a9ee-f8bb6c332465>"],"error":null}
{"question":"How do you transform a C Major scale into a C Minor scale? I need to understand the basic theory.","answer":"To transform a C Major scale into a C Minor scale, take the C Major scale (C-D-E-F-G-A-B-C) and lower three specific notes by one half step: the 3rd tone (E becomes Eb), the 6th tone (A becomes Ab), and the 7th tone (B becomes Bb). In guitar terminology, this process is called 'flatting' the notes.","context":["In this lesson, we'll focus on the Minor Pentatonic scale, and ways to incorporate it into your playing. I promise that I'll try really hard to keep boring and complicated scale theory to a minimum. If you're a gear-head like me, you might be really into that kind of thing, but chances are you'll probably want to cut to the chase and get busy playing. That's the point of this whole thing, right?\nAfter you work on this lesson it might be a good idea to take a look at it's companion lesson Major Pentatonics: A Moving Lesson\n(#1044) located in the Theory / Reference section of Wholenote. It's all about the Major form of the Pentatonic scale and touches on some things that we won't get to in this minor oriented lesson. With that said, let's dig into some Minor Pentatonic Scale stuff. Shall we?...\nFirst of all, what does Pentatonic mean exactly? The word itself is derived from a latin word meaning \"five tones\", or \"five notes\". True to its name, a Pentatonic Scale is a five\nnote scale. There are basically two types of Pentatonic Sales: Major and Minor. Both forms are widely used in rock, blues, country, jazz and many other forms of improvisational music.\nMinor Pentatonic scales are most closely associated with rock & roll, and blues music. In fact, they're a real staple of these styles. Because Pentatonic scales have distinct sounds and relatively compact shapes on the guitar fretboard, you may already be familiar with them even if you're just starting out on the guitar. If not let's take a look at how the Minor Pentatonic scale is formed before we get into some licks.\nMinor Pentatonic scales are based on the natural minor scale, with the 2nd and 6th scale tones omitted. To really understand what that means we should take a quick look at how minor scales are formed. We'll use the good old key of C as an example. A C minor scale is a C major scale with lowerd 3rd, 6th and 7th scale tones. By lowered, I mean \"taken down by one half step\". In guitar speak this is called \"flatting\" the note. A C Major scale, like all major scales, contains eight tones. Two of these tones are C's, the first and last. It starts on a C note, which is called the \"root\" of the scale. The second note is a D, followed by E (3rd), F (4th) G (5th), A (6th) B (7th) and finally C (octave, or root).You can consider this the \"recipe\" for a C Major scale.\nA C Minor scale is derived from this scale, but it's 3rd tone is an E flat not an E. It's 6th tone is an A flat not an A, and it's 7th tone is a B flat and not a B. Therefore, because a C Minor Pentatonic scale is a C Minor scale with the 2nd and 6th tones omitted its' \"recipe\" is:\nC (root), Eb, F, G, Bb, and C (root octave)\nI know this is actually six notes, but we only count the C once. Take a listen to the following scales and check out the differences in the sounds."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:fa2f613b-1ab6-4891-ab69-dc445bcd2911>"],"error":null}
{"question":"Which historic Washington DC venue hosted more famous musical performers - the Washington Coliseum or the National Theatre?","answer":"The National Theatre hosted a more extensive list of famous musical performers throughout its history. While the Washington Coliseum (formerly Uline Arena) notably hosted the Beatles' first US concert in 1964 and other legends like the Rolling Stones, Bob Dylan, and AC/DC, the National Theatre featured an impressive array of performers over nearly 200 years, including Jenny Lind, John Philip Sousa, Sting, Annie Lennox, Pearl Bailey, and many others. The National Theatre also premiered landmark musicals like 'Show Boat' in 1927 and 'West Side Story' in 1957.","context":["This is a cross post by Douglas Barclay, originally posted here.\nOne of the major joys of going to school in Washington DC is the cities unrelenting exposure to history, both large and small. I am sure that most students at Catholic know that the old Fords Theatre, located only seconds from Forever 21 and H&M in Chinatown was the site of the assassination of Abraham Lincoln by John Wilkes Booth in 1865. Maybe you even knew that nearby Wok and Roll Sushi, an unassuming average sushi restaurant was once a boarding house that served as a meeting place for Booth and his co-conspirators, its proprietor Mary Surratt would eventually become the first woman ever executed in the United States for her role in the assassination.\nWhen I moved into my apartment complex off New York Ave this August, I was unaware of the rich history of the surrounding neighborhood, much less the old dilapidated parking garage only half a block away. You have probably seen it on any Metro ride downtown; it is an old red brick building, with a curved roof, lots of graffiti and a rather gaudy sign of the Italian restaurant chain, Carmines emblazoned along the side. For months I was completely oblivious that this run down shell of a building was once home to one of music’s greatest moments, amongst many other notable events.\nOriginally named the Uline Ice Arena, the building opened in 1941 as the home to the Washington Lions hockey team. For two seasons the Washington Lions played their home games at the Uline. The original concept of the arena was to serve as a performing ground for traveling circuses and ice capades, Ringling Brothers and Barnum Bailey circus performed several shows there beginning in the mind 1940s. Like many other businesses, the Uline did its part to help the war effort, offering hot meals and a warm cot for soldiers passing through Washington on leave for fifty cents a night.\nThroughout its early years, the business practices of owner Miguel L. Uline, caused great stir amongst patrons in the city. The Uline Arena was opened originally as a white’s only arena, though African Americans were permitted to attend segregated boxing matches. By the late 1940s the arena had become home to the Washington Capitals NBA team, an early precursor to the Washington Wizards. This period of time at the Uline is notable as the beginning of the career of Red Auerbach, a legend amongst NBA Basketball coaches who would go on to coach the Boston Celtics for almost two decades. In 1953, the arena once again played home to the political spectrum by playing host to the inaugural ball of President elect Dwight D. Eisenhower.\nAfter toiling as a home for National Guard functions and another year long stint as a hockey arena, the Uline was sold in 1959 following the death of Miguel Uline. The new owner, jewelry entrepreneur Harry J. Lynn purchased the arena for close to one million dollars and quickly renamed it the Washington Coliseum. As social and political tensions raged in Washington and around the world, the Coliseum played venue for two major speeches by Malcolm X and Nation of Islam leader Elijah Muhammad. When Muhammad spoke in 1959, over 10,000 people packed the otherwise 9,000 seat venue. It is considered to be one of Muhammad’s major speeches of the time period.\nBy 1964, popular culture had transformed the arena into a popular concert hall and the Coliseum played host to one of music’s most historic nights. Two nights after making their American television debut on the Ed Sullivan Show, the Beatles played their first concert on American soil at the Washington Coliseum on February 11th 1964. They opened with a cover of Chuck Berry’s “Roll Over Beethoven” and for eleven more songs competed with the raucous screams of teenage girls to hear their instruments and deliver their songs with a passion that only the Beatles could channel into live energy. With their instruments equipped on a primitively assembled boxing ring, the concert is curious to watch in 2011 as you see the band have to rearrange their instruments and location of Ringo Starr’s drums in order to find any combination of sounds that would overpower the screams of their fans. This concert would be many fans first exposure to the Beatles as it was filmed by CBS and released in theatres only one month later.\nIn the following years the Coliseum would serve as the DC stop on tours by such legends as the Rolling Stones, the Mamas and the Papas and Bob Dylan. Dylan used a photograph of his 1965 show as the cover for his Grammy award winning “Bob Dylan’s Greatest Hits” record. This photo and album is considered to be an essential piece of any vinyl collection. Following a riot at a Temptations concert in 1967, in which over twenty people were arrested, six injured and thousands of dollars of damage caused to the surrounding neighborhood, it would be eighteen years until the venue would see another major concert. Before AC/DC played there in 1985, the venue was the location of various ventures including a go/go club. In the late 80s, a 17 million dollar renovation allowed the arena to play host to a variety of church services throughout the early 90s. Throughout the remainder of the decade the once famed arena was a transfer stop for the city’s waste management. Though efforts have been made to purchase the facility and return it to its former glory, the lease is currently handled by a firm that has turned it into a parking garage. For 100.00 a month you can park your car on the same spot where legends once walked. In 2008 the building was added to the list of National Historic Sites.\nWhether the Coliseum ever sees the former glory it once held with a firm grasp remains to be seen, since 2008, the GTM Architect firm have been planning and revising its goal to bring life back to the shell of the old Coliseum. Next time you are headed downtown, to see a movie, grab a coffee or spend another miserable night at Brothers, take a moment and glance down at the old brick building, and know that it was not too long ago where giants of history walked, laughed and sang within the confines of the old building on 3rd St.","For almost 200 years, Washington’s historic National Theatre has occupied a prominent position on Pennsylvania Avenue – “America’s Main Street”. Located a stone’s throw from the White House and bordering the Pennsylvania Avenue National Historic Site, The National Theatre is the historic cultural center for the performing arts in our nation’s capital and the oldest enterprise on Pennsylvania Avenue continuously operating in its original designated capacity.\nThe National Theatre has hosted presidential inaugural balls, world premieres of landmark American musicals, presidential command performances of national artistic merit, and the first presentation of the coveted Helen Hayes Award. Lincoln learned of his nomination to a second term while attending a performance at The National Theatre and ominously witnessed the Washington debut of John Wilkes Booth in the title role in Shakespeare’s Richard III. The theater has played a significant role in national events, boosting public moral in times of conflict and serving as a focal point in the struggle for civil rights in the 20th century.\nOn September 17, 1834, a gathering of patriotic citizens led by William Wilson Corcoran decided that our fledgling city deserved a magnificent theater worthy of the nation’s capital. They selected a site close to the White House, the social center of the federal government, and sold shares of stock to raise construction funds. The National Theatre opened its doors for the first time on December 7, 1835, with a production of A Man of the World, dazzling the opening night audience with its elegant interior. Cerulean Blue walls shimmered in candlelight below a domed ceiling filled with allegorical scenes; box tiers were decorated with depictions of historical events in diplomacy, maritime power, and agriculture. A portrayal of the Declaration of Independence supported by the Wings of Time surmounted the proscenium arch, and the theatre curtain was painted with an equestrian statue of George Washington with Mount Vernon in the distance.\nThroughout its 178-year history, the greatest theatrical artists have appeared on The National Theatre stage. The 1850 appearance of P.T. Barnum’s singing sensation, Jenny Lind – The Swedish Nightingale, captivated Washingtonians and nearly caused a riot with crowds clamoring for admission to her two exclusive concerts. President and Mrs. Fillmore, the entire Cabinet and Supreme Court justices retired to the National for a glimpse of the fabled singer. Junius and Edwin Booth, Julia Marlowe, William Gillette, Sarah Bernhardt, Lily Langtry, Henry Irving, Ellen Terry, Joseph Jefferson, Maude Adams, Minnie Fiske, Anna Held, and Lillian Russell entertained 19th century audiences with sparkle and wit.\nThe 20th century brought a new parade of stars, including Fannie Brice, W. C. Fields, Will Rogers, Eddie Cantor, Billie Burke, Al Jolson, Vincent Price, Lillian Gish, Sir John Gielgud, Maurice Evans, Catherine Cornell, Ralph Richardson, Noel Coward, Gertrude Lawrence, Douglas Fairbanks, Mary Pickford, Alfred Lunt, Lynn Fontaine, Todd Duncan, Anne Brown, Sir Laurence Olivier, Vivien Leigh, Ethel Barrymore, Frances Farmer, Celeste Holm, Rosalind Russell, Gloria Swanson, Carol Channing, Burt Lahr, Beatrice Lillie, Ethel Merman, Mary Martin, Elaine Stritch, Judith Anderson, Shirley Booth, Montgomery Clift, Josephine Baker, Audrey Hepburn, Tallulah Bankhead, Lauren Bacall, Buster Keaton, Imogene Coca, Kirk Douglas, Katharine Hepburn, Spencer Tracy, Deborah Kerr, Jessica Tandy, Henry Fonda, Gwen Verdon, Robert Redford, Julie Harris, Richard Kiley, James Earl Jones, Angela Lansbury, Bea Arthur, Robert Preston, Zero Mostel, Theodore Bikel, Judy Holiday, Rex Harrison, Leontyne Price, Debbie Reynolds, Dudley Moore, Ed Asner, John Lithgow, Colleen Dewhurst, Jason Robards, Judi Dench, Raul Julia, Cab Calloway, Pearl Bailey, Rita Moreno, Linda Lavin, Kevin Spacey, Tyne Daly, Sutton Foster, Sting, Sir Ian McKellen, Brian Stokes Mitchell, and Annie Lennox.\nFrom 1907 to 1926, Florenz Ziegfeld brought his fabled Ziegfeld Follies to The National Theatre, and in 1927 his musical “Show Boat”made its world premiere here. In 1957, another landmark musical, “West Side Story”, made its world premiere at The National. Native Washingtonian Chita Rivera was the vortex of energy in the groundbreaking musical, dancing her way to international stardom.\nFrom 1882 until 1916, America’s March King John Philip Sousa conducted The President’s Own United States Marine band, and later The John Philip Sousa Band in frequent concerts at the National Theatre. In December 1900, Sir Winston Churchill gave a lecture on the stage of The National Theatre recounting his experiences in The Boer War. On January 17, 1984, President and Mrs. Ronald Reagan attended a Gala Benefit performance of David Merrick’s Broadway musical “42nd Street” to celebrate the renovation of the National Theatre. President Reagan spoke from the stage of The National welcoming the renovation of the historic playhouse.\nIn her many biographies, Washington actress Helen Hayes traced her theatrical career to the time she saw her first play as a young child in the balcony of The National Theatre. Miss Hayes returned to The National time and again in many of her most famous stage roles, and in the 1980s the theater dedicated the Helen Hayes Gallery in her honor and hosted the premiere presentation of her namesake Helen Hayes Awards.\nThe friendly specter of 19th century actor John McCullough is said to haunt the theater, first seen shortly after his death in 1885 by actors who had worked with him. He is most frequently spotted as a dark amorphous figure, making his benevolent presence known before disappearing into the air. The spirit of a young boy was also rumored to haunt the theater in the 19th century, but his presence has not been reported in many decades.\nFor almost 200 years, Washington’s historic National Theatre has occupied a prominent position on Pennsylvania Avenue – “America’s Main Street”. Located a stone’s throw from the White House and bordering the Pennsylvania Avenue National…"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:4f9ab05a-f8f0-4bbb-b13c-547e33889c8d>","<urn:uuid:1dbdc961-e472-4631-bd6e-81a0c0005054>"],"error":null}
{"question":"Could you compare the safety inspection requirements for demolition sites versus scaffolding sites? I'm particularly interested in who needs to perform these inspections and what specific aspects need to be checked during ongoing work.","answer":"For demolition sites, continuing inspections must be performed by a competent person as the work progresses to detect hazards from weakened floors, walls, or loosened material. For scaffolding sites, a competent person must inspect the scaffold before each workshift and after any occurrence that could affect its structural integrity. The competent person for scaffold inspection must be specifically trained in scaffold safety. In both cases, inspectors must have the authority to take prompt corrective actions. For demolition sites, inspectors focus on structural integrity, particularly looking for signs of potential collapse. For scaffolds, inspectors must check for complete planking of platforms, proper access points, complete guardrails, and proper ties to buildings where required. Both types of inspection are mandatory safety requirements aimed at preventing worker injuries and deaths.","context":["Demolition and Cleanup\nBefore starting a demolition, the person or persons in charge must adequately prepare for the task with regard to the health and safety of the workers. These preparatory operations involve the overall planning of the demolition job, including the methods to be used to bring the structure down, the equipment necessary to do the job, and the measures to be taken to perform the work safely. Before doing demolition work, inspect available personal protective equipment (PPE), and select, wear and use the PPE appropriate for the task.\nDemolition work involves many of the same hazards associated with construction work. However, demolition also poses additional hazards due to unknown factors such as: deviations from the structure's original design, approved or unapproved modifications that altered the original design, materials hidden within structural members, and unknown strengths or weaknesses of damaged materials. To counter these unknowns, all personnel involved in a demolition project need to be fully aware of these types of hazards and the safety precautions available to control these hazards.\nA written engineering survey must be performed on each structure being considered for demolition to determine the condition of the framing, floors and walls, and to assess the possibility of an unplanned collapse of any portion of the structure. Brace or shore the walls and floors of structures which have been damaged and which employees must enter. Inspect and maintain all stairs, passageways and ladders. Properly illuminate all stairways.\nShut off or cap all electric, gas, water, steam, sewer and other service lines outside the building line. Notify appropriate utility companies. Temporarily relocate and protect any essential power, water, or other utilities.\nDetermine the types of hazardous chemicals, gases, explosives, and flammable materials which have been used in any pipes, tanks, or other equipment on the property. Test and purge the hazardous chemicals, gases, explosives, or flammable materials. Survey for asbestos or other hazardous materials.\nGuard wall openings to a height of 42 inches. Cover and secure floor openings with material able to withstand the loads likely to be imposed. Debris dropped through holes in the floor without the use of chutes must be completely enclosed with barricades not less than 42 inches high and not less than 6 feet back from the projected edge of the opening above. Floor openings used for material disposal must not be more than 25% of the total floor area. Use enclosed chutes with gates on the discharge end to drop material to the ground. Design and construct chutes that will withstand the loads likely to be imposed without failing.\nPost signs at each level of structures, warning of the hazard of falling materials. Protect entrances to multi-story structures with sidewalk sheds or canopies for a minimum of 8 feet. Canopies must be at least 2 feet wider than the structure entrance and be able to hold a load of 150 lbs./sq. ft. Storage of material and debris must not exceed the allowable floor load.\nRemoving Walls and Masonry Sections\nDemolition of exterior walls and floors must begin at the top of the structure and proceed downward. Masonry walls must not be permitted to fall on the floors of a building in masses that would exceed the safe carrying capacities of the floors.\nNo wall section, one story in height or higher, shall be permitted to stand alone without lateral bracing, unless such a wall was originally designed and constructed to stand without such lateral support, and is safe enough to be self-supporting. All walls must be left in a stable condition at the end of each work shift. Employees shall not work on the top of a wall when weather conditions create a hazard.\nStructural or load-supporting members on any floor must not be cut or removed until all stories above such a floor have been removed. In buildings of \"skeleton-steel\" construction, the steel framing may be left in place during the demolition of masonry. Walkways or ladders must be provided to enable workers to safely reach or leave any scaffold or wall. Walls, which serve as retaining walls to support earth or adjoining structures, must not be demolished until the supporting earth has been properly braced or until adjoining structures have been properly underpinned. Walls, which will serve as retaining walls against which debris will be piled, must not be used unless they are capable of supporting the imposed load. Dismantle steel construction column length by column length, and tier by tier.\nNo workers shall be permitted in any area when using a crane's headache ball or clamshell to remove debris. Only those workers necessary to perform such operations must be permitted in this work area at any time. The weight of the demolition ball must not exceed 50 percent of the crane’s rated load. The crane boom and loadline must be as short as possible. The ball must be attached to the loadline with a swivel-type connection to prevent twisting of the loadline, and it must be attached by positive means in such a manner that the weight cannot become accidentally disconnected.\nWhen pulling over walls or portions thereof, all steel members affected must have previously been cut free. All roof cornices or other such ornamental stonework must be removed prior to pulling walls over. During demolition, continuing inspections by a competent person shall be made as the work progresses to detect hazards resulting from weakened or deteriorated floors, or walls, or loosened material. No employee shall be permitted to work where such hazards exist until they are corrected by shoring, bracing, or other effective means.\nThis is one in a series of informational fact sheets highlighting OSHA programs, policies or standards. It does not impose any new compliance requirements. For a comprehensive list of compliance requirements of OSHA standards or regulations, refer to Title 29 of the Code of Federal Regulations. This information will be made available to sensory impaired individuals upon request. The voice phone is (202) 693-1999; teletypewriter (TTY) number: (877) 889-5627.\nFor more complete information:\nSafety and Health\nU.S. Department of Labor","Discusses the main causes of injuries and deaths when using scaffolds and describes steps that can be taken to minimize the hazards.\nEach year, more than 60 workers are killed by falls from scaffolds, about 1 in 5 of the fatal falls in construction. Besides problems with planks and guardrails, the main causes of injuries and deaths on scaffolds are poor planning for assembling and taking them apart, missing tie-ins or bracing, loads that are too heavy, and being too close to power lines. Also, falling objects can hurt people below scaffolds.\nProtect YourselfScaffolds are supported (usually by posts/beams and legs) or suspended (by ropes).\n- OSHA says a scaffold must be designed by a qualified person.* Supported scaffolds must be able to support their own weight and at least 4 times the maximum intended load .\n- OSHA says a competent person* must inspect a scaffold before each workshift and after anything happens that could affect the structure. The competent person should be trained in scaffold safety.\n- A competent person must supervise if a scaffold is assembled, changed, moved, or taken apart.\n- Power lines: Keep scaffolds 10 feet or more from power lines (or 3 feet, if lines are less than 300 volts), unless you are sure the power lines are de-energized.\n- Weather: You cannot work on a scaffold in high winds or a storm unless a competent person says it is safe and you use personal fall-arrest or a windscreen. (If you use a screen, the scaffold must be secured against the expected wind force.) OSHA says you must not work on a scaffold that has ice or snow on it — except to get ice or snow off the scaffold.\n- If a scaffold is more than 2 feet above or below a level, there must be a way to get on or off — such as a ladder, ramp, or personnel hoist. The access must not be more than 14\" from the scaffold.\n- Put a standing scaffold on a firm foundation (with base plates attached to feet)— for instance, with one piece of wood under each pair of legs (across the shortest distance), extending at least 1 foot past each leg.\n- Uprights must be vertical and braced to prevent swaying; platforms must be level.\n- A scaffold that is more than 4 times higher than its base is wide must be tied to supports.\n- Most scaffold platforms and walkways must be 18\" wide or more. If a work area is less than 18\" wide, guardrails and/or personal fall-arrest must be used.\n- Ten-foot planks must extend at least 6\" past the end supports, but not more than 12\"; no more than 1\" between planks or between planks and uprights.\n- Wood planks must be unpainted, so any cracks will show.\n- For supported scaffolds, check at least these points:\n- Completely planked platforms\n- Proper access\n- Complete guardrails\n- Proper ties to buildings, where required.\n- Supporting outrigger beams must be able to support at least 4 times the intended load. To keep a scaffold from falling to the ground, it must be attached to the roof, tied to a secure anchorage, or secured with counterweights. The suspension ropes and rigging must support at least 6 times the intended load.\n- Counterweights must be attached to secure and strong places on a building so they won’t move. Do not use bags of sand or gravel, masonry blocks, or roofing materials that can flow or move.\n- Do not use gas-powered equipment or hoists. Hoists must have automatic brakes for emergencies.\n- A 1-point or 2-point suspended scaffold must be tied or secured to prevent swaying.\n- OSHA says if a scaffold is more than 10 feet above a level, workers must have fall protection.\n- A competent person must decide if fall protection is feasible when you assemble a scaffold or take it apart.\n- On most scaffolds, guard rails must be on all open sides and ends. On supported scaffolds and some other scaffolds, guardrails or personal fall protection is enough. On most suspension scaffolds, both are needed. Use a harness, not a body belt for personal fall protection.\n- You do not need a guard rail on the working side when the platform is less than 14\" from the work (18\" for plastering and lathing). The open side of an outrigger must never be more than 3\" from the face of the building.\n- On supported scaffolds most of the time, the top rail must be 38\" to 45\" above the platform. A top rail must be strong enough to hold 200 lb. (or 100 lb. on single-point and two-point suspension scaffolds). A mid-rail must be about halfway between the platform and the top rail; most mid-rails must be able to hold 150 pounds. If mesh, screens, or panels are used, a top rail is needed (unless mesh was designed and installed to meet guardrail requirements).\n- Scaffold walkways must have no more than a 9.5\" gap between planks and a guardrail.\n- Don’t let junk collect on the scaffold. You can trip and fall.\nThere must be a 3 1/2\"-high toe board to prevent things falling off a scaffold. If things on the scaffold are taller than 3 1/2\" — above the toeboard — other systems, like debris nets, can be used to catch falling tools or materials. If things can fall off a scaffold, people must be prevented from walking under or near the scaffold.\n- The employer must have a qualified person provide safety training for each worker who uses a scaffold. A competent person must give safety training to any worker who assembles, takes apart, moves, operates, repairs, maintains, or inspects scaffolds.\n- If the worksite changes or the type of scaffold or safety equipment changes, workers using scaffolds must be retrained.\n* OSHA says a qualified person...by extensive knowledge, training, and experience can...solve...problems related to the subject matter.... A competent person is...capable of identifying existing and predictable hazards...and has authorization to take prompt measures to eliminate them. More information on scaffold safety is in the OSHA Construction Standards in the Code of Federal Regulations, CFR 1926.450-454."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:df2558d5-a982-47d4-9e25-34016cccf40c>","<urn:uuid:97e7ef63-252b-4d92-ae85-c465bf048db5>"],"error":null}
{"question":"What is the historical significance of September 1, 1939 compared to Afghanistan's experience in 2001 - how did both events involve international military interventions?","answer":"Both dates marked significant military interventions with global implications. On September 1, 1939, Nazi Germany and Slovakia invaded Poland, initiating the European phase of World War II. Similarly, in 2001, Afghanistan experienced a major international military intervention when the United States and United Kingdom began Operation Enduring Freedom with an aerial bombing campaign against al-Qaeda and the Taliban. This came after the September 11 attacks and the Taliban's refusal to hand over Osama bin Laden. Both events led to extended military campaigns and involved multiple nations, though in Afghanistan's case, this included the creation of the International Security Assistance Force (ISAF) by the United Nations Security Council to help maintain security in the country.","context":["Behind History For September 1 – Today in History\nBehind History For September 1\n1173 – The widow Stamira penances herself so as to raise the attack of Ancona by the powers of Emperor Frederick Barbarossa.\n1355 – King Tvrtko I of Bosnia writes In castro nostro Vizoka vocatum from the Old town of Visoki.\n1420 – A 9.4 MS-solid seismic tremor shakes Chile’s Atacama Region causing torrents in Chile just as Hawaii and Japan.\n1449 – Tumu Crisis: The Mongols catch the Emperor of China.\n1529 – The Spanish post of Sancti Spiritu, the first implicit present day Argentina, is devastated by locals.\n1532 – Lady Anne Boleyn is made Marquess of Pembroke by her life partner, King Henry VIII of England.\n1604 – Adi Granth, presently known as Guru Granth Sahib, the blessed sacred text of Sikhs, was first introduced at Harmandir Sahib.\n1644 – Battle of Tippermuir: James Graham, first Marquess of Montrose massacres the Earl of Wemyss’ Covenanters, restoring the Royalist cause.\n1715 – At the age of five, Louis XV becomes lord of France in progression to his incredible granddad, King Louis XIV.\n1763 – Catherine II of Russia supports Ivan Betskoy’s arrangements for a Foundling Home in Moscow.\n1772 – The Mission San Luis Obispo de Tolosa is established in San Luis Obispo, California.\n1774 – Massachusetts Bay settlers ascend in the bloodless Powder Alarm.\n1804 – Juno, perhaps the biggest space rock in the Main Belt, is found by the German stargazer Karl Ludwig Harding.\n1831 – The Order of St. Gregory the Great is built up by Pope Gregory XVI of the Vatican State to perceive high help for the Vatican or for the Pope, by a man or a lady, and not really a Roman Catholic.\n1836 – Narcissa Whitman, one of the principal English-talking white ladies to settle west of the Rocky Mountains, shows up at Walla, Washington.\n1838 – Saint Andrew’s Scots School, the most seasoned school of British source in South America, is built up.\n1862 – American Civil War: Battle of Chantilly: Confederate Army troops rout a gathering of withdrawing Union Army troops in Chantilly, Virginia.\n1864 – American Civil War: The Confederate Army General John Bell Hood arranges the departure of Atlanta, finishing a four-month attack by General William Tecumseh Sherman.\n1870 – Franco-Prussian War: The Battle of Sedan is battled, bringing about a definitive Prussian triumph.\n1873 – Cetshwayo climbs to the seat as lord of the Zulu country following the demise of his dad Mpande.\n1878 – Emma Nutt turns into the world’s first female phone administrator when she is selected by Alexander Graham Bell to the Boston Telephone Dispatch Company.\n1880 – The multitude of Mohammad Ayub Khan is steered by the British at the Battle of Kandahar, finishing the Second Anglo-Afghan War.\n1894 – Over 400 individuals kick the bucket in the Great Hinckley Fire, a timberland fire in Hinckley, Minnesota.\n1897 – The Tremont Street Subway in Boston opens, turning into the primary underground fast travel framework in North America.\n1905 – Alberta and Saskatchewan join the Canadian confederation.\n1906 – The International Federation of Intellectual Property Attorneys is set up.\n1911 – The heavily clad cruiser Georgios Averof is charged into the Greek Navy. It currently fills in as a historical center boat.\n1914 – St. Petersburg, Russia, changes its name to Petrograd.\n1914 – The last known traveler pigeon, a female named Martha, kicks the bucket in imprisonment in the Cincinnati Zoo.\n1920 – The Fountain of Time opens as a recognition for the 100 years of harmony between the United States and Great Britain following the Treaty of Ghent.\n1923 – The Great Kantō seismic tremor decimates Tokyo and Yokohama, executing around 105,000 individuals.\n1928 – Ahmet Zogu pronounces Albania to be a government and declares himself ruler.\n1934 – The principal Metro-Goldwyn-Mayer enlivened animation, The Discontented Canary, is delivered to cinemas.\n1939 – World War II: Nazi Germany and Slovakia attack Poland, starting the European period of World War II.\n1939 – General George C. Marshall gets Chief of Staff of the United States Army.\n1939 – The Wound Badge for Wehrmacht, SS, Kriegsmarine, and Luftwaffe fighters is established. The last form of the Iron Cross is likewise founded on this date.\n1939 – Switzerland activates its powers and the Swiss Parliament chooses Henri Guisan to head the Swiss Armed Forces (an occasion that can happen just during war or assembly).\n1939 – Adolf Hitler signs a request to start the deliberate killing of intellectually sick and crippled individuals.\n1941 – The Nazis execute 2,500 Jews by shooting in Ostroh, Ukraine.\n1951 – The United States, Australia and New Zealand sign a shared protection settlement, called the ANZUS Treaty.\n1952 – The Old Man and the Sea, the Pulitzer Prize-winning novel by Ernest Hemingway, is first distributed.\n1958 – Iceland extends its fishing zone, placing it into struggle with the United Kingdom, starting the Cod Wars.\n1961 – The Eritrean War of Independence authoritatively starts with the shooting of Ethiopian police by Hamid Idris Awate.\n1961 – The main gathering of the Non-Aligned Countries is held in Belgrade, Yugoslavia.\n1967 – The Khmer–Chinese Friendship Association is prohibited in Cambodia.\n1967 – Six-Day War: The Khartoum Resolution is given at the Arab Summit, and eight nations receive the “three ‘no’s against Israel”.\n1969 – An overthrow in Libya brings Muammar Gaddafi to control.\n1969 – Trần Thiện Khiêm becomes Prime Minister of South Vietnam under President Nguyễn Văn Thiệu.\n1970 – Palestinian guerrillas assault the motorcade of King Hussein of Jordan in a bombed death endeavor.\n1972 – In Reykjavík, Iceland, American Bobby Fischer beats Russian Boris Spassky to turn into the world chess champion.\n1974 – The SR-71 Blackbird sets (and holds) the record for flying from New York to London in the hour of 60 minutes, 54 minutes and 56.4 seconds at a speed of 1,435.587 miles every hour (2,310.353 km/h).\n1979 – The American space test Pioneer 11 turns into the primary shuttle to visit Saturn when it passes the planet a good ways off of 21,000 kilometers (13,000 mi).\n1980 – Major General Chun Doo-hwan becomes President of South Korea, following the acquiescence of Choi Kyu-hah.\n1981 – A rebellion in the Central African Republic ousts President David Dacko.\n1982 – The United States Air Force Space Command is established.\n1983 – Cold War: Korean Air Lines Flight 007 is shot somewhere around a Soviet Union fly warrior when the business airplane enters Soviet airspace, slaughtering each of the 269 ready, including Congressman Lawrence McDonald.\n1985 – A joint American–French undertaking finds the destruction of the RMS Titanic.\n1991 – Uzbekistan announces freedom from the Soviet Union.\n2004 – The Beslan school attack starts when equipped fear based oppressors take schoolchildren and school staff prisoner in North Ossetia, Russia; before the finish of the attack three days after the fact in excess of 385 individuals are dead (counting prisoners, different regular citizens, security work force and psychological militants).\n2019 – Hurricane Dorian makes landfall in the Bahamas, murdering in any event 74 individuals and causing around $3.4 billion worth of harm.\n2019 – Co-founder and CEO of Twitter Jack Dorsey has his Twitter account hacked.","|Afghanistan Wiki Topics|\n|Local Research Resources|\nHistory[edit | edit source]\nHuman habitation in Afghanistan dates back to the Middle Paleolithic Era, and the country's strategic location along the Silk Road connected it to the cultures of the Middle East and other parts of Asia. The land has historically been home to various peoples and has witnessed numerous military campaigns, including those by Alexander the Great, Mauryas, Muslim Arabs, Mongols, British, Soviet, and since 2001 by the United States with NATO-allied countries.\nThe political history of the modern state of Afghanistan began with the Hotak and Durrani dynasties in the 18th century. In the late 19th century, Afghanistan became a buffer state between British India and the Russian Empire. Its border with British India, the Durand Line, was formed in 1893 but it is not recognized by the Afghan government and it has led to strained relations with Pakistan since the latter's independence in 1947.\nIn the late 1970s, Afghanistan in a series of coups first became a socialist state and then a Soviet Union protectorate. This evoked the Soviet–Afghan War in the 1980s against rebels. By 1996 most of Afghanistan was captured by the fundamentalist Islamic group the Taliban, who ruled most of the country as a totalitarian regime for almost five years. The Taliban were forcibly removed by the NATO-led coalition, and a new democratically-elected government political structure was formed.\nAfghanistan is a unitary presidential Islamic republic with a population of 35 million, mostly composed of ethnic Pashtuns, Tajiks, Hazaras and Uzbeks.\nTimeline[edit | edit source]\n1709-1747 - Ahmad Shah Durrani of the Abdali Pashtun confederacy declared the establishment of\nan independent Afghanistan, with its capital at Kandahar.\n1809-1839 - First Anglo-Afghan War: A British expeditionary force captured Quetta\n1857 - Afghanistan declared war on Persia.\n1878 - Second Anglo-Afghan War: Afghanistan refused a British diplomatic mission, provoking a second Anglo-Afghan war.\n1879 - Second Anglo-Afghan War: To prevent British occupation of a large part of the country, the Afghan government ceded much power to the United Kingdom in the Treaty of Gandamak.\n1919 - Third Anglo-Afghan War: Amanullah led a surprise attack against the British.\n1922 - Solar Hijri calendar officially adopted in Afghanistan.\n1929 - Amanullah was forced to abdicate in favor of Habibullah Kalakani in the face of a popular uprising.\n1965 - The Marxist People's Democratic Party of Afghanistan held its first congress.\n1978 - A treaty was signed which permitted deployment of the Soviet military at the Afghan government's request.\n1978 - Soviet–Afghan War, the Soviet army invaded Afghanistan. 1988 - The Soviet government signed the Geneva Accords, which included a timetable for withdrawing their armed forces.\n1989-1992 - The last Soviet troops left the country.\n1994-1995 - The Taliban government began to form in a small village between Lashkar Gah and Kandahar. The Taliban, with Pakistani support, initiated a military campaign against the Islamic State of Afghanistan.\n1996- 2001 - Civil war in Afghanistan\n2001 - After the September 11 attacks in the United States, U.S. President George W. Bush demanded the Taliban government to hand over al-Qaeda head Osama bin Laden and close all terrorist training camps in the country.\n2001 - Operation Enduring Freedom: The United States and the United Kingdom began an aerial bombing campaign against al-Qaeda and the Taliban.\n2001 - The United Nations Security Council authorized the creation of the International Security Assistance Force (ISAF) to help maintain security in Afghanistan.\n2005 - Taliban insurgency: An insurgency began after a Pakistani decision to station soldiers next to the porous Durand Line border with Afghanistan.\n2007 - Afghanistan–Pakistan Skirmishes: Skirmishes began with Pakistan.\n2011 - Afghanistan National Front was created by Tajik leader Ahmad Zia Massoud, Hazara leader Mohammad Mohaqiq and Uzbek leader Abdul Rashid Dostum"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:f42b4012-96ea-47ce-84ec-8c16116386e1>","<urn:uuid:690d6471-a90b-4804-88d3-cf0d1c727213>"],"error":null}
{"question":"What is the key difference between fixed costs in partial budgeting versus breakeven analysis?","answer":"In partial budgeting, fixed costs are considered unchangeable in the short-term and are excluded from the analysis since only costs that change are analyzed. In contrast, for breakeven analysis, fixed costs (overhead costs) are explicitly included and calculated since they must be covered by sales revenue before a business can make a profit, regardless of sales volume.","context":["An important role of the farm manager is planning for the future. Planning includes taking an inventory of resources, considering alternative uses for resources, estimating costs and returns associated with the alternate uses, and choosing the “best” alternative. The manager can organize financial and physical plans by budgeting.\nMany changes do not require a complete reorganization of the farm or ranch. The manager can use resources in more than one way when responding to changes in product price levels and cropping patterns. Partial budgets are useful in evaluating changes such as expanding an enterprise, adding a new enterprise, changing production practices, and buying new machinery.\nPrinciples of Partial Budgeting\nPartial budgets estimate the economic effect of minor adjustments in some part of the farm business. With partial budgeting, the manager assumes that many aspects of the business are fixed in the short-term. He or she uses partial budgets to evaluate changes in resources that are not fixed. Partial budgeting is based on the principle that a small change in the organization of a farm business will eliminate or reduce some costs and returns, while possibly also adding or increasing costs or revenues. The net economic effect of a change will be the sum of the positive economic effects minus the sum of the negative effects.\nWheat Grazeout versus Wheat Harvest for Grain\nA partial budget (Table 1) may be used to decide whether to graze out or harvest wheat for grain. The decision is made prior to first hollow stem and is based on expected yields and prices of wheat and stockers. Prices are a main determinant in the decision and are estimated for the future. By combining known figures with estimates of future yields and prices, the farm manager can compare alternative plans of action for profitability. Prices and yields data should be updated and customized for an individual situation. Other factors that can affect grazeout profitablity include stocking rate, value of gain, weather and the grazing length of time. The budget below is prepared on a per-head basis with a stocking rate, for an additional 60 days, of one steer per acre of wheat. Additional information includes:\nRate of gain per steer = 2.5 lbs./day (60 days)\n#1 – #2 Steers: 640 lbs. on March 1 at $1.57/lb = $1,004.80\n#1 – #2 Steers: 790 lbs. on May 1 at $1.49/lb = $1,177.10\nWheat yield = 30 bu./acre\nWheat price = $4.00/bu.\nInterest on investment (livestock) = $11.30/steer\nAdditional veterinary expenses, feed, etc. = $4/steer\nCustom combining = $23/acre + ($0.23/bu. over\n21 bu.) = $25.07/acre\nCustom hauling = $0.24/bu. x 30 bu./acre = $7.20/acre\nComponents of the Partial Budget\nThe typical partial budget consists of these components: additional costs, reduced returns, additional returns, reduced costs, totals of the first two and the second two, and the net difference. These categories are used to estimate the effects of a proposed change in business organization. In the left column, negative economic effects resulting from the proposed change are estimated; in the right column, positive economic effects are summarized.\nAdditional costs are those that will occur if the change takes place. However, this does not include costs that are common to both the present and proposed business operation (any cost that does not change will not be included in the partial budget). In our example, the additional costs category includes costs that are incurred by keeping the steers through May 1. Interest expense is increased because the cattle are held two months longer. Veterinary, feed, and other costs may increase as well (Table 1). It is important to note that these costs are incurred only when the cattle are kept, not if they are sold.\nReduced returns is the income that would not be received under the proposed change. In our example, reduced returns is the value for the calves which could be sold now plus the wheat sales not received when the grain is grazed out. The yield and prices shown are estimates based on current expectations. Totaling the left column shows a figure of $1,140.10. This represents the total negative economic impact of the proposal on the farm operation.\nIn our example, additional returns are the added receipts that will be received if the alternative plan is adopted. Additional returns include the value of beef grown from March 1 to May 1 at a stocking rate of one steer per acre gaining two and a half pounds per day. The listed weight is an average over all animals, but the price is an actual steer price in the weight range at the time of sale.\nReduced costs are those that will no longer be incurred if the change is initiated. Reduced costs include the costs of custom combining and hauling which are not incurred if the wheat is grazed out. Rates should be representative of current rates in the region.\nAdditional returns and reduced costs are totaled at the bottom of the column. The net difference between positive and negative economic effects is an estimate of the net effect of making the proposed change. A positive net difference indicates the potential increase in net returns if the change is made. Conversely, a negative net change in income is an estimate of the reduction in net returns if the change is adopted.\nIn our example, the total of the Additional Returns/Reduced Costs column is $1,209.37 and the total of the Additional Costs/Reduced Returns is $1,140.10. Subtracting the total of column A from B yields a net value of $69.27. This represents the amount of economic gain with grazing out the wheat rather than selling the steers on March 1 and combining the wheat. Note that with different prices or stocking rates, the conclusion could be different. In some years, grazing out is the less profitable option so having accurate price forecasts is critical. The reasoning behind the budget formulation is simple.\nPartial Budgeting Process\nThe success of the partial budget depends on the accuracy of the information and estimates it contains. The farm manager must collect pertinent, factual data about each proposed change and provide reasonable estimates of future prices, yields, gains, etc. Factual information includes current production costs, costs of capital, current prices for products such as grain or livestock, etc.\nEstimating future unknowns, particularly prices is difficult. The manager must estimate yields and prices to determine the returns given up and received. Yield estimates can be obtained from several sources. The best source is an individual’s farm records. The farm records will show the history of production. This, combined with an assessment of current crop conditions, should closely predict future yields, given normal weather and other conditions. Other sources of yield estimates are neighboring farm histories, OSU research reports showing average yields, and the farm manager’s previous experience. A combination of these sources should provide a close estimate of projected yields.\nFuture product prices are difficult to predict. Agricultural economists, USDA statisticians, and futures markets all provide information on the trend of prices and national crop conditions. However, it will be unusual to find a predicted price for a product on a particular day. Using published information by the above sources as well as the manager’s intuition will provide the best estimate on future product prices. Using a range of prices – low, medium, and high – to evaluate changes reveals the price sensitivity of the projected change.\nThe partial budget is ready to be developed after all pertinent data are assembled. A blank worksheet is included at the end of this publication. The cost of the proposed change is calculated for each of the categories. Only the costs and returns that will change by adopting the alternate plan are analyzed in the partial budget. The unit used to analyze the change may be any size, for example, the whole crop, one acre of the crop, one head of cattle, or the entire herd. The column totals show the negative and positive economic aspects of the proposed change. Subtract the left column total from the right column total to obtain a net amount that reflects the change in net farm income if the proposed alternative is adopted.\nA positive net change says it would be economically beneficial to proceed with the alternate plan. A negative amount implies that it would not be economically profitable to proceed with the change. Two notes of caution: 1) The value of this analysis using partial budgeting is only as accurate as the data used. 2) The partial budget does not necessarily include “cash flow” tied to capital purchases (for instance, machinery). Cash flow is addressed in OSU Extension Fact Sheet AGEC-751. After the analysis has been performed, the result should be multiplied as necessary to show the economic impact on the entire farm situation.\nTo determine the sensitivity of the results to the price and yield assumptions, it is useful to calculate the net change in income under different scenarios. Table 2 provides an example of a price sensitivity table. For a given steer price, grazing out becomes less profitable (or more unprofitable) as wheat price or yield increases. In our example, grazing out wheat is more profitable than harvesting wheat if grazout steer prices are $1.40/lb. or greater. On the other hand, harvesting wheat is more profitable than grazing it out if grazeout stocker prices are lower or wheat prices or yields are higher.\nIndividual farmers should modify the estimates to more closely conform to their actual situations. Additionally, individual farmers may find some of the included costs not applicable to their situation or have other costs that should be included. Remember that the partial budget includes only the costs that change. Costs that cannot be affected should not be included in this analysis.\nThe breakeven price is calculated by setting the sum of Additional Costs plus Reduced Returns equal to the sum of Additional Returns and Reduced Costs. In this case, mathematically:\nAdditional Costs + Reduced Returns = Additional Returns +\nAdditional Costs = additional costs of maintaining stockers extra months = $15.30/a\nReduced Returns = stocker income if calves are sold in March plus wheat harvest income = price of stockers in March (Psm) times their yield in weight in March (Ysm) + (price of wheat times wheat yield) = Psm x Ysm + Pw x Yw = (640 lbs/a x $1.57/lb) + (30 bu./a X $4.00/bu) = $1,124.80/a\nAdditional Returns = price of stockers in May (Psg) times their yield in weight in May (Ysg) = Psg x Ysg = Psg x 790\nReduced Costs = wheat harvest costs = flat cost per acre + hauling costs + premium for more than 21 bushels per acre = $23/a + ($0.24/bu x 30 bu/a) + [(30 bu – 21 bu) x .23/bu] = $32.27/a\nThus, the breakeven price for grazeout stockers is:\n$15.30/a + $1,124.80/a = (Psg x 790 lbs/a) + $32.27/a\nPsg = ($15.30/a + $1,124.80/a – $32.27/a)/790 lbs/a = $1.40/lb\nTo solve for the breakeven price of wheat:\n$15.30/a + (640 lbs/a x $1.57/lb) + (30 bu/a X Pw) = ($1.49/lb x 790 lbs/a) + $32.27/a\nPw = ($1,177.10/a + $32.27/a – $15.30/a – $1,004.80/a)/30 bu/a = $189.27/a /30 bu/a = $6.31/bu\nTo calculate breakeven stocker weights, assume a stocker price and substitute stocker grazeout weight for the missing variable:\n$15.30/a + $1,124.80/a = ($1.49/lb x Ysg) + $32.27/a\nThus, the breakeven stocker weight at $1.49/lb. is:\nYsg = ($15.30/a + $1,124.80/a – $32.27/a)/$1.49/lb. = 744 lbs/a\nIf the breakeven variable of interest is the wheat yield, it becomes a bit more complicated since both the reduced returns and reduced costs are a function of wheat yield. In this example, solving for wheat yield becomes:\n$15.30/a + (640 lbs/a x $1.57/lb) + ($4.00/bu x Yw) = ($1.49/lb x 790 lb/a) + $23/a + $.23/bu x (Yw – 21 bu/a) + (.24/bu x Yw)\n$1,020.10/a + ($4.00/bu x Yw) = $1,200.10/a + $.23/bu x Yw –\n$4.83/a + ($.24/bu x Yw)\n$4.00/bu x Yw – $.47/bu x Yw = $175.17/a\n$3.53/bu x Yw = $175.17/a\nYw = $175.17/a/$3.53/bu = 49.6 bu/a\nThis partial budget fact sheet presents a simplified procedure to aid producers in everyday decision-making. This design is not for total farm planning, but rather to estimate the economic consequences of making a change in some phase of the farm operation. Partial budgeting is a step-by-step process for identifying the costs and returns that change due to alterations in the production process. Once these costs and returns are identified, they are weighed against each other to estimate the economic consequences of the change. The results can only be as good as the data used. Therefore, care should be taken when estimating values for the various categories. In addition, sensitivity test for values such as yields and prices should be developed to highlight their effect on the ultimate outcome.\nAssistant Extension Specialist\nProfessor, Farm and Production Management","Breakeven analysis is a tool used to determine when a business will be able to cover all its expenses and begin to make a profit. For the startup business it is extremely important to know your startup costs, which provide you with the information you need to generate enough sales revenue to pay the ongoing expenses related to running your business.\nA startup business owner must understand that $5,000 of product sales will not cover $5,000 in monthly overhead expenses. The cost of selling $5,000 in retail goods could easily be $3,000 at the wholesale price, so the $5,000 in sales revenue only provides $2,000 in gross profit available for overhead costs. The breakeven point is reached when revenue equals all business costs.\nTo calculate your breakeven point you will need to identify your fixed and variable costs. Fixed costs are expenses that do not vary with sales volume, such as rent or administrative salaries. These costs have to be paid regardless of sales and are often referred to as overhead costs. Variable costs vary directly with the sales volume, such as the costs of purchasing inventory, shipping, or manufacturing a product.\nWill Your Business Make Money?\nBefore you prepare a business plan, you should figure out if your business will break even. How can you tell if your business idea will be profitable? The honest answer is, you can’t. But this uncertainty shouldn’t keep you from researching the financial soundness of your idea. Preparing what’s known as a “break-even analysis,” as well as several other financial projections, can help you determine whether or not your business will succeed.\nWhat a Break-Even Analysis Tells You\nYour break-even analysis shows you the amount of revenue you’ll need to bring in to cover your expenses before you make a dime of profit. If you can attain and surpass your break-even point – that is, if you can easily bring in more than the amount of sales revenue you’ll need to meet your expenses – then your business stands a good chance of making money.\nMany experienced entrepreneurs use a break-even analysis or forecast as a primary screening tool for new business ventures. They won’t even write a complete business plan unless their break-even forecast shows that their projected sales revenue far exceeds their costs of doing business.\nHow to Prepare a Break-Even Analysis\nTo perform a break-even analysis, you’ll have to make educated guesses about your expenses and revenues. Although you don’t have a crystal ball, you should do some serious research – including an analysis of your market – to determine your projected sales volume and your anticipated expenses. Your best bet is to invest in a do-it-yourself business plan product to learn how to make reasonable revenue and cost estimates.\nYou’ll need to make the following estimates and calculations when you prepare your break-even analysis:\nFixed costs. Fixed costs (sometimes called “overhead”) don’t vary much from month to month. They include rent, insurance, utilities and other set expenses. It’s also a good idea to throw a little extra, say 10%, into your break-even analysis to cover miscellaneous expenses that you can’t predict.\nSales revenue. This is the total dollars from sales activity that you bring into your business each month or year. To perform a valid break-even analysis, you must base your forecast on the volume of business you really expect – not on how much you need to make a good profit.\nAverage gross profit for each sale. Average gross profit is the money left from each sales dollar after paying the direct costs of a sale. (Direct costs are what you pay to provide your product or service.) For example, if Antoinette pays an average of $100 for goods to make dresses that she sells for an average of $300, her average gross profit is $200.\nAverage gross profit percentage. This percentage tells you how much of each dollar of sales income is gross profit. To calculate your average gross profit percentage, divide your average gross profit figure by the average selling price. For example, if Antoinette makes an average gross profit of $200 on dresses that she sells for an average of $300, her gross profit percentage is 66.7% ($200 divided by $300).\nCalculating Your Break-Even Point\nOnce you’ve calculated the numbers above, it’s easy to figure out your break-even point. Simply divide your estimated annual fixed costs by your gross profit percentage to determine the amount of sales revenue you’ll need to bring in just to break even. For example, if Antoinette’s fixed costs are $6,000 per month, and her expected profit margin is 66.7%, her break-even point is $9,000 in sales revenue per month ($6,000 divided by .667). In other words, Antoinette must make $9,000 each month just to pay her fixed costs and her direct (product) costs. (This number does not include any profit, or even a salary for Antoinette.)\nDon’t Forego a Break-Even Analysis\nAlthough creating a break-even forecast might sound complicated, you owe it to yourself to prepare one as one of the first steps in your business planning process. As you can see, a realistically prepared break-even forecast will tell you whether your idea is a sure winner, a loser or, like most ideas, it needs modifications to make it work.\nIf You Can’t Break Even\nIf your break-even point is higher than your expected revenues, you’ll need to decide whether certain aspects of your plan can be changed to create an achievable break-even point. For instance, perhaps you can:\nfind a less expensive source of supplies\ndo without an employee\nsave rent by working out of your home, or\nsell your product or service at a higher price.\nIf you tinker with the numbers and your break-even sales revenue still seems like an unattainable number, you may need to scrap your business idea. If that’s the case, take heart in the fact that you found out before you invested your (or someone else’s) money in the idea.\nFurther Financial Analysis\nIf your break-even forecast shows you’ll make more revenue than you need to break even, you can consider yourself fortunate. But you still need to figure out how much profit your business will generate, and whether you’ll have enough cash available to pay your bills when they are due. In short, a break-even forecast is a great screening tool, but you need a more complete analysis before you start investing real money in your venture.\nThe following are additional financial projections that should also be part of your business plan, to round out your business’s financial picture.\nA profit-and-loss forecast. This is a month-by-month projection of your business’s net profit from operations.\nA cash flow projection. This shows you how much actual cash you’ll have, month by month, to meet your expenses.\nA start-up cost estimate. This is the total of all the expenses you’ll incur before your business opens.\nPreparing a break-even forecast will help you decide whether it’s worth drafting a business plan for your idea – but it should never take the place of a complete profit-and-loss forecast and cash flow projection. For instructions on how to create a profit-and-loss forecast and a cash flow projection, see How to Write a Business Plan, by Mike McKeever (Nolo)."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:3fa731d5-476a-4253-962e-c704182f7f56>","<urn:uuid:0ac76a75-c600-4bc2-90f1-13b69b33d995>"],"error":null}
{"question":"Could you compare the recognition mechanisms of T. cruzi parasites and regular pathogens during cell invasion? Please include details about how immune cells identify and respond to each type.","answer":"T. cruzi and regular pathogens are recognized through different cellular mechanisms. For regular pathogens, recognition occurs through pathogen-associated molecular patterns (PAMPs) like peptidoglycan, flagellin, and lipopolysaccharide, which are detected by pattern recognition receptors (PRRs) including toll-like receptors on phagocytes. In contrast, T. cruzi triggers recognition through complex signaling cascades that increase intracellular Ca2+ levels and by inducing plasma membrane lesions. T. cruzi also employs unique molecules like Tc-Tox and trans-sialidase for cell invasion, while regular pathogen recognition often involves opsonization by complement factors and antibodies before phagocytosis.","context":["Escape from the lysosome\nOnce it has invaded the host cell, T. cruzi escapes from the phagolysosome to gain access to the cytoplasm, and this process is dependent on phagolysosome acidifica- tion.64 Lysis of the phagolysosome membrane is believed to be mediated by a parasite-derived secreted pore-forming molecule known as Tc-Tox, which works optimally at low pH.65,66 Tc-Tox cross-reacts with antibodies against complement C9 protein,66 and this property was used to screen an expression library and identify a T. cruzi gene (LYT1) coding for a hemolytic protein acting at low pH, which cross-reacts with anti-C9 antibodies.67 In addition, T. cruzi neuraminidase\nFigure 26.2 Intracellular invasion by T. cruzi trypomastigotes. TOP. Trypomastigotes are highly motile and they attach to the surface of nonphagocytic mammalian cells (1a). The parasite causes increased intracellular Ca21 levels by triggering signaling cascades (1b), and by inducing plasma membrane lesions, through which extracellular Ca21 can flow into the cell (1c). Increased intracellular Ca21 concentrations facilitate parasite entry by inducing transient depolymerization of the cortical actin cytoskeleton (1d) and lysosome anterograde (toward the “ 1 ” end) translocation on microtubules to reach the parasite entry site in the cell periphery (1e).\nBOTTOM. At the parasite entry site, lysosomes exocytose while sealing membrane wounds, and release acid sphingomyelinase, ASM (2a). ASM generates ceramide domains in the outer leaflet of the plasma membrane and favors its invagination, allowing parasites to enter the cell enveloped in a plasma membrane-derived vacuole, in most cases devoid of lysosomal markers (2b). Phagosomes, formed by the fusion of phagophores with lysosomes, may fuse to the nascent parasitophorous vacuole (2c), which may also fuse with endosomes and additional lysosomes as it matures (2d). The parasitophorous vacuole undergoes retrograde (away from the “plus” end) translocation on microtubules toward the perinuclear region (2e).\n(trans-sialidase), which has also been shown to be active at low pH, has also been implicated in the disruption of parasitophorous vacuoles.68 Interestingly, coinmuno- precipitation experiments suggest that Tc-Tox/LYT1 protein is capable of physically interacting with trans-sialidase.69 However, direct proof of the proposed functions of these proteins in egress from the parasitophorous vacuole is still lacking.\nHost cellular processes required for T. cruzi invasion\nAn RNAi-based genome-wide functional screen designed to identify mammalian cell genes and processes which support establishment of intracellular infection and\nintracellular growth by T. cruzi was performed by Caradonna et al.70 The screen employed a multistep process, with an initial library of >25,000 siRNA pools which was assayed on an in vitro culture system using HeLa cells, to determine the effect of silencing individual genes over the T. cruzi intracellular infection process. The initial screen was followed by secondary screens with endpoints at 18 h and 72 h postinfection, corresponding to the prereplication phase of infection (i.e., invasion, <24 h after cell infection) and intracellular growth (i.e., amastigote multiplication, >24—90 h after cell infection). Although functional confirmation experiments performed in this study concentrated on host pathways which support the parasites intracellular growth (host metabolic networks and cellular signaling pathways were identified as important for this phase of the parasite life cycle), the screen did identify several dozen candidate host genes involved in the establishment of intracellular residence by T. cruzi. Genes that were found to affect parasite invasion included host cell signaling molecules, cytoskeletal proteins (CLASP1, cofilin-1), extracellular matrix proteins (laminin, collagen 1a), and genes involved in protein trafficking and organelle biosynthesis, among others. These findings are in line with what would be expected based on the knowledge regarding mammalian cell invasion described in previous sections of this chapter. The identification of CLASP1 as a protein required for parasite invasion in the RNAi knockdown screen was immediately followed by functional confirmation in subsequent studies,62 highlighting the validity of the use of unbiased screens to identify functional leads that can be subsequently confirmed and characterized in the pursuit of unveiling the details of the infection process by T. cruzi, and intracellular pathogens in general.","- Explain how leukocytes migrate from peripheral blood into infected tissues\n- Explain the mechanisms by which leukocytes recognize pathogens\n- Explain the process of phagocytosis and the mechanisms by which phagocytes destroy and degrade pathogens\nSeveral of the cell types discussed in the previous section can be described as phagocytes—cells whose main function is to seek, ingest, and kill pathogens. This process, called phagocytosis, was first observed in starfish in the 1880s by Nobel Prize-winning zoologist Ilya Metchnikoff (1845–1916), who made the connection to white blood cells (WBCs) in humans and other animals. At the time, Pasteur and other scientists believed that WBCs were spreading pathogens rather than killing them (which is true for some diseases, such as tuberculosis). But in most cases, phagocytes provide a strong, swift, and effective defense against a broad range of microbes, making them a critical component of innate nonspecific immunity. This section will focus on the mechanisms by which phagocytes are able to seek, recognize, and destroy pathogens.\nExtravasation (Diapedesis) of Leukocytes\nSome phagocytes are leukocytes (WBCs) that normally circulate in the bloodstream. To reach pathogens located in infected tissue, leukocytes must pass through the walls of small capillary blood vessels within tissues. This process, called extravasation, or diapedesis, is initiated by complement factor C5a, as well as cytokines released into the immediate vicinity by resident macrophages and tissue cells responding to the presence of the infectious agent (Figure 1). Similar to C5a, many of these cytokines are proinflammatory and chemotactic, and they bind to cells of small capillary blood vessels, initiating a response in the endothelial cells lining the inside of the blood vessel walls. This response involves the upregulation and expression of various cellular adhesion molecules and receptors. Leukocytes passing through will stick slightly to the adhesion molecules, slowing down and rolling along the blood vessel walls near the infected area. When they reach a cellular junction, they will bind to even more of these adhesion molecules, flattening out and squeezing through the cellular junction in a process known as transendothelial migration. This mechanism of “rolling adhesion” allows leukocytes to exit the bloodstream and enter the infected areas, where they can begin phagocytosing the invading pathogens.\nNote that extravasation does not occur in arteries or veins. These blood vessels are surrounded by thicker, multilayer protective walls, in contrast to the thin single-cell-layer walls of capillaries. Furthermore, the blood flow in arteries is too turbulent to allow for rolling adhesion. Also, some leukocytes tend to respond to an infection more quickly than others. The first to arrive typically are neutrophils, often within hours of a bacterial infection. By contract, monocytes may take several days to leave the bloodstream and differentiate into macrophages.\nWatch the following videos on leukocyte extravasation and leukocyte rolling to learn more:\nThink about It\n- Explain the role of adhesion molecules in the process of extravasation.\nAs described in the previous section, opsonization of pathogens by antibody; complement factors C1q, C3b, and C4b; and lectins can assist phagocytic cells in recognition of pathogens and attachment to initiate phagocytosis. However, not all pathogen recognition is opsonin dependent. Phagocytes can also recognize molecular structures that are common to many groups of pathogenic microbes. Such structures are called pathogen-associated molecular patterns (PAMPs). Common PAMPs include the following:\n- peptidoglycan, found in bacterial cell walls;\n- flagellin, a protein found in bacterial flagella;\n- lipopolysaccharide (LPS) from the outer membrane of gram-negative bacteria;\n- lipopeptides, molecules expressed by most bacteria; and\n- nucleic acids such as viral DNA or RNA.\nLike numerous other PAMPs, these substances are integral to the structure of broad classes of microbes.\nThe structures that allow phagocytic cells to detect PAMPs are called pattern recognition receptors (PRRs). One group of PRRs is the toll-like receptors (TLRs), which bind to various PAMPs and communicate with the nucleus of the phagocyte to elicit a response. Many TLRs (and other PRRs) are located on the surface of a phagocyte, but some can also be found embedded in the membranes of interior compartments and organelles (Figure 2). These interior PRRs can be useful for the binding and recognition of intracellular pathogens that may have gained access to the inside of the cell before phagocytosis could take place. Viral nucleic acids, for example, might encounter an interior PRR, triggering production of the antiviral cytokine interferon.\nIn addition to providing the first step of pathogen recognition, the interaction between PAMPs and PRRs on macrophages provides an intracellular signal that activates the phagocyte, causing it to transition from a dormant state of readiness and slow proliferation to a state of hyperactivity, proliferation, production/secretion of cytokines, and enhanced intracellular killing. PRRs on macrophages also respond to chemical distress signals from damaged or stressed cells. This allows macrophages to extend their responses beyond protection from infectious diseases to a broader role in the inflammatory response initiated from injuries or other diseases.\nThink about It\n- Name four pathogen-associated molecular patterns (PAMPs).\n- Describe the process of phagocyte activation.\nOnce pathogen recognition and attachment occurs, the pathogen is engulfed in a vesicle and brought into the internal compartment of the phagocyte in a process called phagocytosis (Figure 3). PRRs can aid in phagocytosis by first binding to the pathogen’s surface, but phagocytes are also capable of engulfing nearby items even if they are not bound to specific receptors. To engulf the pathogen, the phagocyte forms a pseudopod that wraps around the pathogen and then pinches it off into a membrane vesicle called a phagosome. Acidification of the phagosome (pH decreases to the range of 4–5) provides an important early antibacterial mechanism. The phagosome containing the pathogen fuses with one or more lysosomes, forming a phagolysosome. Formation of the phagolysosome enhances the acidification, which is essential for activation of pH-dependent digestive lysosomal enzymes and production of hydrogen peroxide and toxic reactive oxygen species. Lysosomal enzymes such as lysozyme, phospholipase, and proteases digest the pathogen. Other enzymes are involved a respiratory burst. During the respiratory burst, phagocytes will increase their uptake and consumption of oxygen, but not for energy production. The increased oxygen consumption is focused on the production of superoxide anion, hydrogen peroxide, hydroxyl radicals, and other reactive oxygen species that are antibacterial.\nIn addition to the reactive oxygen species produced by the respiratory burst, reactive nitrogen compounds with cytotoxic (cell-killing) potential can also form. For example, nitric oxide can react with superoxide to form peroxynitrite, a highly reactive nitrogen compound with degrading capabilities similar to those of the reactive oxygen species. Some phagocytes even contain an internal storehouse of microbicidal defensin proteins (e.g., neutrophil granules). These destructive forces can be released into the area around the cell to degrade microbes externally. Neutrophils, especially, can be quite efficient at this secondary antimicrobial mechanism.\nOnce degradation is complete, leftover waste products are excreted from the cell in an exocytic vesicle. However, it is important to note that not all remains of the pathogen are excreted as waste. Macrophages and dendritic cells are also antigen-presenting cells involved in the specific adaptive immune response. These cells further process the remains of the degraded pathogen and present key antigens (specific pathogen proteins) on their cellular surface. This is an important step for stimulation of some adaptive immune responses, as will be discussed in more detail in the next chapter.\nVisit this link to view a phagocyte chasing and engulfing a pathogen.\nThink about It\n- What is the difference between a phagosome and a lysosome?\nWhen Phagocytosis Fails\nAlthough phagocytosis successfully destroys many pathogens, some are able to survive and even exploit this defense mechanism to multiply in the body and cause widespread infection. Protozoans of the genus Leishmania are one example. These obligate intracellular parasites are flagellates transmitted to humans by the bite of a sand fly. Infections cause serious and sometimes disfiguring sores and ulcers in the skin and other tissues (Figure 4). Worldwide, an estimated 1.3 million people are newly infected with leishmaniasis annually.\nSalivary peptides from the sand fly activate host macrophages at the site of their bite. The classic or alternate pathway for complement activation ensues with C3b opsonization of the parasite. Leishmania cells are phagocytosed, lose their flagella, and multiply in a form known as an amastigote (Leishman-Donovan body) within the phagolysosome. Although many other pathogens are destroyed in the phagolysosome, survival of the Leishmania amastigotes is maintained by the presence of surface lipophosphoglycan and acid phosphatase. These substances inhibit the macrophage respiratory burst and lysosomal enzymes. The parasite then multiplies inside the cell and lyses the infected macrophage, releasing the amastigotes to infect other macrophages within the same host. Should another sand fly bite an infected person, it might ingest amastigotes and then transmit them to another individual through another bite.\nThere are several different forms of leishmaniasis. The most common is a localized cutaneous form of the illness caused by L. tropica, which typically resolves spontaneously over time but with some significant lymphocyte infiltration and permanent scarring. A mucocutaneous form of the disease, caused by L. viannia brasilienfsis, produces lesions in the tissue of the nose and mouth and can be life threatening. A visceral form of the illness can be caused by several of the different Leishmania species. It affects various organ systems and causes abnormal enlargement of the liver and spleen. Irregular fevers, anemia, liver dysfunction, and weight loss are all signs and symptoms of visceral leishmaniasis. If left untreated, it is typically fatal.\nKey Concepts and Summary\n- Phagocytes are cells that recognize pathogens and destroy them through phagocytosis.\n- Recognition often takes place by the use of phagocyte receptors that bind molecules commonly found on pathogens, known as pathogen-associated molecular patterns (PAMPs).\n- The receptors that bind PAMPs are called pattern recognition receptors, or PRRs. Toll-like receptors (TLRs) are one type of PRR found on phagocytes.\n- Extravasation of white blood cells from the bloodstream into infected tissue occurs through the process of transendothelial migration.\n- Phagocytes degrade pathogens through phagocytosis, which involves engulfing the pathogen, killing and digesting it within a phagolysosome, and then excreting undigested matter.\nPAMPs would be found on the surface of which of the following?\n- skin cell\n- blood vessel wall\n________ on phagocytes bind to PAMPs on bacteria, which triggers the uptake and destruction of the bacterial pathogens?\nWhich of the following best characterizes the mode of pathogen recognition for opsonin-dependent phagocytosis?\n- Opsonins produced by a pathogen attract phagocytes through chemotaxis.\n- A PAMP on the pathogen’s surface is recognized by a phagocyte’s toll-like receptors.\n- A pathogen is first coated with a molecule such as a complement protein, which allows it to be recognized by phagocytes.\n- A pathogen is coated with a molecule such as a complement protein that immediately lyses the cell.\nFill in the Blank\n________, also known as diapedesis, refers to the exit from the bloodstream of neutrophils and other circulating leukocytes.\nToll-like receptors are examples of ________.\nThink about It\nBriefly summarize the events leading up to and including the process of transendothelial migration.\n- World Health Organization. \"Leishmaniasis.\" 2016. http://www.who.int/mediacentre/factsheets/fs375/en/. ↵"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"search_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:15936573-dece-4c6a-83a7-e898d29cf5c3>","<urn:uuid:663a39d4-bd56-4641-80d3-825779c6e930>"],"error":null}
{"question":"What are the traditional greeting customs in Cambodia, and what ethnic groups make up its population?","answer":"The traditional Cambodian greeting is called Som Pa, where people put their hands together near the chin with a slight bow, holding hands higher to show respect to elders. Many Cambodians also shake hands with visitors, and it's considered rude not to return a greeting. Regarding population, approximately 90% of Cambodia's inhabitants are ethnic Khmer, who have lived in the region since around 2nd century BC. The largest minority group is the Cham people, numbering about 500,000, who are traditionally cattle traders and silk weavers. Other significant groups include around 100,000 Chinese descendants and approximately 200,000 Vietnamese, particularly in the southern part and Phnom Penh region.","context":["Although it’s impossible to categorize an entire culture, we’ve compiled a list with some key highlights and distinctions of Cambodian culture that will make your time much more enjoyable.\nShowing Respect in Cambodia\n- As with most of Asia, to “lose one’s cool” in public is completely unacceptable. Never shout at someone or criticize them in front of others.\n- Cambodians don’t typically show strong emotions in public, so don’t be alarmed if they remain stoic in an emotional situation. Cambodians may also be uncomfortable if you choose to show strong emotions in public.\n- Some Cambodians will giggle nervously or smile when they feel uncomfortable with the strong emotion being expressed.\n- When offered a gift, it’s polite to refuse it at first, but in the end, always accept it very graciously with both hands.\n- Give and receive gifts, money, and business cards with both hands. It’s a sign of respect.\n- Be sure to give genuine compliments to people when they are merited.\n- Because of Cambodia’s tough past, do not bring up sensitive subjects such as war, violence, the Killing Fields, current politics, or the Khmer Rouge.\n- When gesturing with your hands to “come here”, it’s considered rude to gesture with your palm up. Always have your palm down, and use all four fingers.\n- When using a tooth pick, you should cover your mouth with your spare hand.\nIn many Southeast Asian countries the head is considered the highest and most spiritual part of a person’s body. The feet are considered the dirtiest and least sacred.\n- Don’t pat children or adults on the head.\n- Don’t gesture with your feet.\n- Don’t raise your feet higher than someone’s head.\n- When seated on the ground, women should attempt to hide their feet by tucking them underneath themselves. Men should sit Indian style.\n- Remove your shoes and hat when entering a home.\nGreeting People in Cambodia\n- The traditional Cambodian greeting, known as Som Pa, is made by putting your two hands together (with fingertips near the chin) and a giving a slight bow with your head. The hands are held higher to show more respect to elders.\n- Many Cambodians choose to shake hands with visitors, so the best rule-of-thumb is simply to return whatever greeting you were given initially. It is considered very rude not to return a greeting.\n- Pointing with your index finger is considered rude. Instead, gesture with your right hand palm-up.\n- During conversations many Cambodians don’t make eye contact. Direct eye contact is usually reserved for emotional conversations. Prolonged direct eye contact can be interpreted as physical aggression or attraction.\nProper Dress in Cambodia\n- Modest dress is the rule in Cambodia, particularly for women.\n- Women should not wear short skirts or show their shoulders.\n- Men usually wear collared shirts and long pants.\n- Although many tourists wear shorts to deal with the heat, the locals tend to cover as much skin as possible. While it is not a rule, pants or capris for women are safe and acceptable.\n- Although tourism has caused these standards to relax somewhat, it’s always best to dress conservatively to show respect.\nInteracting with the Opposite Sex\nIt may come as a surprise to many, especially because sex trafficking is a rampant problem in Cambodia, but Cambodians strongly frown upon public displays of affection.\n- Couples, even if they are married, shouldn’t hold hands or kiss in public.\n- Be mindful in your contact with the opposite sex, even placing an arm around a local to pose for a picture can be misinterpreted.\n- Women should never touch a monk or hand anything to him; even the monk’s mother may not do so.\n- Cambodian males will hold hands with and hug other Cambodian males; this is a sign of friendship, nothing more.\n- When gesturing for someone to “come here” with your hands, using one finger is considered romantic. Always have your palm down and use all four fingers to show respect.\nRespect for Elders\n- Aside from monks, elders are given the highest level of respect in Cambodia. Always acknowledge an elder’s status by allowing them to control the conversation, walk first, and take the lead.\n- When seated, you should attempt to never sit higher than the eldest person in the room.\n- Always wait for the eldest to sit and eat first.\n- Ask for permission before taking any photos.\n- Try not to take photos with three people in the photo. Some older Cambodians think that it is bad luck, especially to be the person in the middle of the photo.85\nYou have reached the end of this 30-day devotional. You may feel a little more nervous, overwhelmed, humbled, dependent upon God – and all of these things are okay. It is God working through you who will make a difference in the lives of the Cambodians you are going to serve.\n85 Suggestions drawn from Gregory Rodgers,“Etiquette in Cambodia,” About.com (2012), http://goseasia.about.com/od/cambodiaculturepeople/a/etiquette_cambodia.htm (accessed February 4, 2013).","The Khmer people have lived in the Greater Region since around the 2nd century BC and probably originate from a combination of Mongolian and Melanesian peoples. The most important cultural influence comes from India and was noticeable from the 2nd century AD.\n- Approximately 90 % of the inhabitants of Cambodia are members of the Khmer people\n- The Khmer people have lived in the Greater Region since around the 2nd century BC and probably originate from a combination of Mongolian and Melanesian peoples.\n- In the 18th and 19th century larger groups of Chinese migrated into the Khmer Kingdom. In modern times they faced political persecution and expulsion.\n- Approximately 200,000 Vietnamese are currently living in Cambodia. The southern part of the country and the region of Phnom Penh have a substantial proportion of Vietnamese settlers.\nThe hill tribes mainly live in forested mountain regions of the Northeast. They were former semi-nomadic and practiced an agriculture based on slash and burn. Their number increased in recent years, so that they had to change to an intensified agriculture and had to adopt the habits of the lowland Khmer.\nThere are about 500,000 Cham people, originating from the Cham established in the 16th century in what is now Central Vietnam. T hey now constitute the largest minority in Cambodia. The Pol Pot regime pursued the Cham intensively and reduced their number by half. Originally the Cham were Hindu oriented, later they became Muslims. Their spiritual centre is located in Chur-Changvra near Phnom Penh. The Cham are traditionally cattle traders, butchers and silk weavers.\nIn the 18th and 19th century larger groups of Chinese migrated into the Khmer Kingdom. In modern times they faced political persecution and expulsion. Many escaped, leaving around 100,000 people remaining today. Approximately 200,000 Vietnamese are currently living in Cambodia. The southern part of the country and the region of Phnom Penh have a substantial proportion of Vietnamese settlers..\nDos in Cambodia\n- Ask for permission before taking photographs of any Cambodian people or monks.\n- It is customary to remove your shoes when entering a place of worship such as a pagoda or temple. Additionally, visitors should dress appropriately when inside a religious site (upper arms and legs should be covered, hats removed).\n- It is respectful to remove your shoes when entering someone’s home.\n- Though not always expected, a respectful way of greeting another individual is to bow the head slightly with hands pressed together at the chest (known as “Sampeah”).\n- If invited to dine in a Cambodian family’s home, it is polite to bring a small gift for the host such as fruit, dessert, or flowers.\n- If invited to attend a Cambodian wedding, it is customary to bring cash as a wedding gift.\n- When using a toothpick at the table, use one hand to cover your mouth.\n- Keep business cards ready, and present them with both hands. Accept business cards with both hands.\nDONts in Cambodia\n- Don’t use your feet to point at someone.\n- Don’t touch a Cambodian person on the head.\n- Don’t begin eating if you are a guest at a dinner and the host has yet to take a bite.\n- Women should never touch male monks or hand something directly to them.\n- Keep public displays of affection to a respectful minimum."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:c6e2ab81-f67e-40f2-8891-dce7bf5160d5>","<urn:uuid:74862dbc-ff56-4761-9f80-61ce1a34a50c>"],"error":null}
{"question":"How has the traditional view of inventory changed with modern manufacturing approaches, and what risk monitoring systems are needed to support this transition?","answer":"The traditional view of inventory as an asset has fundamentally changed - now, due to JIT implementation, inventory is considered waste or dead investment that incurs additional costs rather than being an asset. To support this modern approach, comprehensive risk monitoring systems are essential. These systems must include automated data collection and analysis, early detection mechanisms for potential problems, clear classification of risks, established contingency plans, and continuous monitoring protocols. The process requires transparency from suppliers and collaboration across departments, all overseen by dedicated leadership to ensure successful implementation and risk mitigation throughout the supply chain.","context":["Traditionally, raw materials and inventory of finished goods were considered assets. This notion has changed because of JIT and now inventory is considered as waste or dead investment, incurring additional costs.\nThe Benefits of Just-in-Time Inventory\nLen Calderone for | ManufacturingTomorrow\nJust-in-time (JIT) inventory is a stratagem that manufacturers utilize to increase efficiency and decrease waste by receiving goods only as they are needed in the manufacturing process, thereby reducing the cost of inventory. Importantly, manufacturers must forecast their requirements accurately.\nJIT avoids the waste associated with overproduction, waiting for material and holding excess inventory. The original concept was created by the founder of Toyota. Just-in-Time means that a manufacturer makes only what is needed, only when it is needed, and only in the amount that is needed.\nMany manufacturers have tried to imitate Toyota without understanding the underlying concept or motivation, which might have led to the failure of those projects. The concept behind Toyota’s system is to work intelligently and eliminate waste so that only minimal inventory is needed.\nToyota’s founder used the American supermarket as his model for what he was trying to achieve in the factory. When shopping for groceries, a customer takes what s/he wants from the shelf and purchases them. The store restocks the shelf with enough new product to fill up the shelf space. In a manufacturing situation, a worker would go to an inventory storage location and remove the quantity of material needed for production. The inventory storage location would then replenish just the amount of the material taken.\nThe just-in-time inventory model lets manufacturers reduce their overhead expenses while always ensuring that parts are available to manufacture their products. This allows a company’s customers to be better served, while, at the same time, lowering the cost of doing business.\nWarehousing excess inventory can be very expensive. Reducing the amount of inventory can reduce carrying costs. Companies that employ the just-in-time inventory model may be able to reduce the number of warehouses they own, or even allow them to eliminate those warehouses altogether.\nAn efficient supply chain can deliver lower costs throughout the manufacturing process, and those lower costs can then be passed on to the customer, making the company's products more affordable. This helps the company acquire a larger market share and outpace its competitors.\nBy using the JIT model, a manufacturer has a better level of control over its entire manufacturing process, thereby, making it easier to respond quickly when the needs of customers change. For example, a manufacturer that uses the just-in-time inventory model can quickly increase production of an in-demand product, while reducing production on products that are slowing down. The company will need to cut prices on any unsold inventory just to clear it out, which reduces the perceived value of the manufacturer’s other products. The just-in-time inventory model reduces this waste.\nOne example of JIT are fast-food restaurants, which use just-in-time inventory to serve their customers on a daily basis during breakfast, lunch and dinner. Fast food restaurants have cheese, burger patties and all the fixings in a refrigerator, but they don't start assembling and cooking their cheeseburgers until a customer places an order.\nAnother example is the self-publishing industry. Authors might forgo the traditional approach to publishing their works and self-publish. Self-published authors can take advantage of just-in-time inventory by working with a printer that offers print-on-demand services. Print-on-demand companies don't print the books until an order is placed.\nBy reducing inventory, JIT frees up resources to employ them elsewhere in the company. A retail store—using JIT—can renovate its warehouse space, providing additional retail floor space without expanding the store itself.\nA manufacturer can convert part of its warehouse into manufacturing space, increasing production. JIT also allows the workforce to focus on primary tasks, from making goods to interacting with customers rather than stocking material.\nWith JIT, manufacturers will know when employees are needed at different stations of assembly to meet the demand of those stages of manufacturing. A more flexible workforce can focus on quality production with lower defect rates, which lower costs and increase customer satisfaction.\nTo make JIT workable, management must rethink the entire work flow of the company, from the intake of raw materials to the final finished product. At the same time, supply-chain relationships might require multiple suppliers, closer locations, and suppliers that can provide materials with minimal notice.\nOne negative is the problem that smaller orders will be needed for JIT. Therefore, new negotiations may be needed because of minimum order requirements. Even if a slightly higher price is paid, the cost difference could be offset by the low cost of inventory.\nAn entirely different mindset will be needed throughout the company. The complete workforce must understand the entire JIT process and shift to where they are needed, as work flow recedes and surges to meet customer demand swings. This will take a sizable commitment of both time and money at first—plus allegiance—to stay the course in implementing JIT. If not, the system will never gain traction within the company.\nLean manufacturing and JIT are often mistaken for the same thing, but they are different concepts. Just-in-time manufacturing is focused on efficiency, while lean manufacturing is centered on using efficiency to add value for the customer. JIT can be utilized on its own, or as one step in the lean manufacturing process.\nLean manufacturing takes the idea of JIT and looks at it from the perspective of customer value. In the lean manufacturing process, a company must consider what aspects of the product add real value. The first tenet of lean manufacturing is that every step in the production process must add something of value.\nThe manufacturing process is then redesigned to remove activities that don't add value. Transporting parts from a distant warehouse to the manufacturing plant would not add value. Switching to JIT would reduce transportation and warehousing costs, making the product more affordable. The JIT process adds value by increasing efficiency.\nThe most important benefit of JIT is the elimination of raw material, inventory and product storage costs. Traditionally, raw materials and inventory of finished goods were considered assets. This notion has changed because of JIT and now inventory is considered as waste or dead investment, incurring additional costs.\nFor additional information:\nThe content & opinions in this article are the author’s and do not necessarily represent the views of ManufacturingTomorrow\nLen Calderone - Contributing Author\nLen contributes to this publication on a regular basis. Past articles can be found with an Article Search and are listed below. He also writes short stories that always have a surprise ending. He has also written a book on wedding photography on a budget. These can be found at http://www.smashwords.com/profile/view/Megalen\nThis post does not have any comments. Be the first to leave a comment below.\nPost A Comment\nYou must be logged in before you can post a comment. Login now.","Supply Chain Risk Management\nWhat is Supply Chain Risk Management?\nSupply Chain Risk Management Definition: Supply chain risk management (SCRM) involves monitoring for and identifying potential threats to the supply chain. Although there is no way to completely eliminate risk, many issues can be avoided and contingency plans can be put into place to minimize or mitigate the impact on operations and/or profitability.\nSupply Chain Risk Management Examples: Generally speaking, there are two types of risk associated with supply chains. There are things that are impossible to predict and plan for, such as natural disasters, epidemics, and acts of terrorism, and there are other things that can be planned for, provided an organization has the data necessary to observe and act on early indicators. This may include:\nThe Role of Software Risk Management in Supply Chain Reliability\nBecause there are many potential issues that can impact the supply chain, executives have the difficult duty of assessing which problems are likely to cause the most harm to operations and profitability. Although cyber security is a core focus, there are numerous ways software can impact the reliability and profitability of a supply chain and not all of them are apparent, even to those in IT. While there is some risk involved in the hacking or insertion of malicious code in software, most issues surround unintentional coding errors, which routinely go undetected. As the infrastructure and architecture grow, these errors can create vulnerabilities and cause unexpected conflicts. By identifying these issues in advance and taking steps to reduce their impact, the risk is effectively managed prior to breakdown of the chain. Bear in mind, these errors can occur at a local level as well as at any stage in the supply chain, which is why it’s important for organizations to work with vendors and companies that have systems and transparency in place.\nSupply Chain Risk Management Best Practices\nAutomation: Supplier risk management (SRM) processes, including the collection, management, and analysis of data, should be automated. Human eyes cannot always detect errors, let alone conflicting codes within a system.\nSupplier Transparency: Performance information from suppliers should be readily available and included in analysis.\nEarly Detection: The supply chain risk management process should make use of automation software detect potential red flags before they become a problem.\nClassification: All potential risks should be identified, classified, and prioritized in order to determine which corrective actions offer the greatest ROI.\nPlans & Contingency Plans: A supply chain risk management plan should be developed and include methods for managing imminent risks as well as for minimizing the impact of issues that cannot be prevented or would be cost-prohibitive to prevent.\nMonitoring: Supply chain risk management tools should be used to identify new triggers, even after an assessment and corrections have been carried out.\nCollaboration: SCRM metrics and information should be available to various departments and heads at the same time for easy collaboration regarding supply chain risk management strategies.\nLeadership: SCRM should be overseen by one leader, even if duties are delegated to a team. Historically, organizations that assign a lead, usually an executive or VP, to handle SCRM see the greatest ROI. These organizations are also the only ones to see in excess of 100% ROI on their SCRM efforts."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:bea5b95c-b5cd-48fb-b2a6-ba1d1a59ac8b>","<urn:uuid:08a27c68-0dd9-4293-992b-f4e82c905629>"],"error":null}
{"question":"What are the data collection methods used at Lodwar meteorological station versus the data types analyzed in India's surface ozone studies?","answer":"At Lodwar meteorological station, they collect precipitation, humidity, temperature, windspeed, evaporation and pressure data either hourly or every three hours, using instruments like the Campell-Stokes tropical sun recorder for daylight measurements. In comparison, the Indian surface ozone studies analyzed daily maximum surface ozone levels along with multiple atmospheric parameters including carbon monoxide, methane, nitrogen dioxide concentrations, vertical pressure velocity, relative humidity, geopotential height, and potential vorticity data from various sources including NASA, European agencies and meteorological departments.","context":["Raymonde Bonnefille brought TBI Field School students to the local town of Lodwar, at the start of her course on paleoecology, to show them its local meteorological station. Lodwar is the largest town in northern Kenya, and its meteorological station has collected data benefiting the local airport and climate scientists worldwide for over 50 years. As professor Bonnefille would explain, an understanding of modern climate is invaluable when reconstructing the climate of the past.\nLodwar is the largest town in northern Kenya, and the administrative center of Turkana District, west of Lake Turkana. The town is surrounded by desert, so remote that in the 1950’s the British used it as a prison for Jomo Kenyatta, whom they accused of inciting the Mau Mau Rebellion. When colonial forces were expelled 10 years later, Kenyatta became the first president of an independent Kenya.\nThe same year that Kenyatta and five others stood trial while imprisoned in Lodwar, the city quietly began collecting meteorological data and has continued to do so ever since. While climate research did not immediately acquire the same publicity, this meteorological data subsequently proved invaluable to climate researchers in this and other regions around the globe.\nPaleoclimatologists are interested in modern meteorological records because these can be used to reconstruct far more ancient climates. To help students understand how modern meteorological data are collected, professor Raymonde Bonnefille brought them to Lodwar and introduced them to local meteorological station staff.\nProfessors Raymonde Bonnefille, Veronica Waweru and students listen as Mr. Manyoni gives a lecture on modern climate at Lodwar.\nTBI Field School students were introduced to Mr. Manyoni, a meteorologist whose work is used by local pilots, researchers and climatologists.\nMr. Manyoni describes the way in which his data is transmitted to Nairobi and made accessible to climate researchers globally.\nAt the Lodwar station, meterology staff record precipitation, humidity, temperature, windspeed, evaporation and pressure either every hour or every three hours. Their work requires shifts that cover all 24 hours of the day.\nThe meteorological station in Lodwar uses a Campell-Stokes tropical sun recorder.\nThis device was invented over 150 years ago, but is still in use all over the world. The light of the sun is directed through the glass sphere to a point, where during daylight hours it burns a hole through a small paper card. By tracing the burn marks on a card a researcher can record the hours and strength of daylight.\nPatrick and other students look out over the Pliocene sediments at TBI main camp during a morning class with Raymonde Bonnefille.\nFollowing their trip to Lodwar, Professor Bonnefille brought students into the Pliocene sand, sandstone and mudstone sediments outside TBI’s main camp to look for deposits that might contain pollen. Above, students sketched a quick geological map of the area before they began to explore.\nProfessor Bonnefille explains the significance of a mudstone deposit in the Kataboi member of the Nachukui formation outside camp.\nThese sediments, approximately 3.5 million years old, consist of mud deposits created by a flood plain of the ancient Turkwel River. While riverbeds are full of fast flowing and turbulent waters that wash away fine grained muds, floodplains are calmer and tend to preserve fine sediments. For this reason, professor Bonnefille explained, they were more likely to contain microsopic pollen grains.\nProfessor Bonnefille points out a sandstone column in the ancient sediments outside camp.\nStudents enjoy the view over the Pliocene sediments outside the Turkana Basin Institute’s Turkwel River camp.\nBefore students returned to camp for lunch, professor Bonnefille managed to locate a number of mud sediments altered by volcanic ash. Excited by the find, students explored all over the deposite and found many fossils in the process. Volcanic ash can be used to date sediments, giving a concrete ago to ancient material preserved within them. For palynologists, ash is exciting because it preserves pollen: the fine interface between an ash layer and the sediments below it is often full of ancient pollen grains.","Research Article | Open Access\nInfluence of Stratospheric Intrusion on the Surface Ozone Levels in India\nThe surface ozone levels in some Indian cities have increased significantly in the recent years. Ozone being toxic to the living system and an important contributor to anthropogenic global warming, enhanced surface ozone may have adverse effects on the air quality and climate. Transport of ozone from the stratosphere to the troposphere causes stratospheric ozone to decrease and tropospheric ozone to increase, which can in turn have serious consequences for life on earth. Since stratosphere-troposphere exchange (STE) is an important factor influencing the ozone concentration in the troposphere, this paper investigates probably for the first time the possible contribution of STE events to the observed enhanced surface ozone levels for cities covering from north to south of India. It is concluded that apart from transport processes and in situ photochemical production, STE also influences the observed high-surface ozone levels in Indian cities to a small extent (8%–16%). STE events producing high-surface ozone levels are found to be higher at high latitudes.\nThe amount of surface ozone at any location depends upon the amount of its supply from the stratosphere, rate of photochemical production, transport from other regions, and the rate of its destruction on the earth’s surface either due to dry deposition or photochemical loss mechanisms [1–3]. Downward transport of stratospheric ozone [4, 5], called stratosphere-troposphere exchange (STE) of ozone is an important and significant natural source of surface ozone. The mechanisms leading to STE are tropopause folds, cutoff lows, and quasiadiabatic transports along isentropic surfaces [5, 6]. Ozone being toxic to the living system, intense exposure to surface ozone may result in persistent decrease in lung function, pneumonia, influenza, asthma, and decrease in crop yield. Transport of ozone from the stratosphere to the troposphere may cause surface ozone levels to increase resulting in adverse consequences for life on earth. The ambient air quality standards for surface ozone in India are 180 g/m3 (92 ppbv) for 1-hour average and 100 μg/m3 (51 ppbv) for 8-hour average .\nIt has been observed in the recent years that the background surface ozone levels have increased significantly in some Indian cities [8, 9]. Several studies suggest that rapid and deep STE events may be associated with severe weather [10–12] and cause ozone peaks at the surface [13, 14]. When compared with high-surface ozone episodes induced photo chemically near the Earth's surface, relatively little study has been reported for high-surface ozone cases induced by STE in India. Although these episodes are relatively uncommon, they have been reported to produce transient peak ozone concentrations of around 100 ppbv at sea level and concentrations in excess of 250 ppbv in mountain regions  in other parts of the world. In view of the above observations, this paper is aimed to identify the Indian cities where the surface ozone is likely to be affected by STE by examining all the case studies in detail over a period of 11 years from 1998–2008.\n2. Measurement Site and Data\nThe Indian subcontinent lies between 8°N–36°N latitude. The five surface ozone monitoring stations, namely, New Delhi (28.4°N, 77.13°E; 214.42 meters above sea level), Nagpur (21°N, 79°E; 310 meters above sea level), Pune (18.5°N, 73.5°E; 560 meters above sea level), Kodaikanal (10°N, 77.5°E; 2,133meters above sea level), and Thiruvananthapuram (8.28°N, 76.56°E; 60meters above sea level) extending from North to South India (Figure 1), have entirely different geographical morphology and hence different local climatic conditions. The topographic map of India depicting the measurement sites was obtained from the website: http://www.mapsindia.com/. The daily maximum surface ozone at New Delhi, Pune, Kodaikanal, and Thiruvananthapuram over the period from 1998–2008 and at Nagpur during the period from 2005–2007 measured by electrochemical method have been obtained from India Meteorological Department (IMD). 5-day back-trajectory at different pressure levels and potential vorticity at the tropopause (100 hPa) has been retrieved from the European Centre for Medium-Range Weather Forecasts (ECMWF). The vertical pressure velocity, relative humidity, and geopotential height maps have been retrieved from NCEP/NCAR reanalysis . Daily global gridded data of surface carbon monoxide (CO) and methane (CH4) were obtained from TES (Tropospheric Emission Spectrometer) on NASA’s EOS (Earth Observing System) spacecraft. Total Nitrogen dioxide (NO2) column data from Ozone monitoring instrument (OMI) was obtained from the website http://www.temis.nl/. Fire count data representing biomass burning from the ATSR World Fire Atlas has been taken from Ionia products of European Space Agency.\n3. Analysis and Results\nThe day characterized by maximum surface ozone concentration in each month was identified and the linear trend of this monthly maximum surface ozone data was plotted for the entire period of available data set. The surface ozone levels at Pune and Thiruvananthapuram indicated a small decreasing trend, while that at New Delhi, Nagpur, and Kodaikanal indicated a small increasing trend (Figure 2) over the period of study. Several studies in the recent years indicate that surface ozone, even at concentrations less than the ambient air quality standards, can flare harmful effects in human beings [17, 18]. Therefore instead of considering ozone episodes greater than the ambient air quality standard as critical, higher than normal ozone episodes were identified by employing the statistic mean control chart (MCC), and the causes leading to these episodes were investigated in detail.\nMCC is a time plot of observations with a statistically calculated control limit. If the variations in surface ozone follow a normal cyclic pattern throughout the year (due to seasonal changes), then all the points in the time plot should lie within the control limit. When an observation exceeds the upper control limit (UCL), some new and unanticipated source of variation (either natural or anthropogenic) is likely to be operating, and a search for the special cause should be initiated. The UCL for surface ozone at different Indian stations has been estimated statistically  from the MCC by using the monthly maximum surface ozone data for the entire period of available data set. The results are summarized in Table 1: where is the yearly mean surface ozone for months of the kth year. Range = (Maximum ozone—Minimum ozone) observed for the kth year. , 11 (years from 1998–2008 for New Delhi, Pune, Kodaikanal, and Thiruvananthapuram) and (years from 2005–2007 for Nagpur)\nThe value of constant corresponding to is 0.266 and is obtained from standard statistical control table . The days characterized by surface ozone concentration higher than UCL was identified from the monthly maximum surface ozone dataset for each station (Figure 3), and the possible association of this unusually high surface ozone concentration with STE was investigated. Potential vorticity (PV) is a measure of the spin of air. Stratospheric air is characterized by high PV and low relative humidity (RH). According to the WMO definition of dynamical tropopause , PV values exceeding 1.6 pvu (1 pvu = 10−6 m2 Kkg−1 s−1) at the tropopause indicate the presence of air masses having stratospheric origin. Therefore, the presence of air masses characterized by and has been considered as a signature of stratospheric air similar to that in the study by Cristofanelli et al. . To identify and exclude air masses possibly influenced by polluted air or biomass burning plumes, the CO, CH4, and NO2 concentration and fire counts were taken into account. The 5-day back-trajectories at 1000, 925, and 850 hPa pressure levels were examined to determine the place from where the air parcels had originated. This information was in turn used to check the possibility of horizontal transport of ozone and its precursors from these areas. The vertical pressure velocity (Pa s−1) was examined to confirm the downward transport of ozone from the stratosphere. As pressure decreases with altitude, positive values of vertical pressure velocity indicate sinking motion and negative values indicate rising motion in the atmosphere. The days characterized by higher than normal concentration of CO, CH4, NO2, and fire counts, vertical pressure velocity indicating upward transport of ozone and back trajectories from high ozone or ozone precursor areas were excluded from this study. Further, the geopotential maps in the upper troposphere were examined to detect the formation of cutoff lows. The total ozone concentration was examined for ±10 days from the day of STE event to investigate if the increase in surface ozone was due to tropopause folding.\nOne such case study at Pune on 24 November, 2006 is presented as an illustration (Figure 4). The geopotential height map at 850 hPa pressure level indicates the formation of a trough, which is typically preceded by stormy weather and colder air at the surface. The vertical pressure velocity at the tropopause indicated downward transport of ozone from the stratosphere. The PV value was >1.6 and RH at the tropopause were low (20%), indicating the presence of air masses having stratospheric origin. The fire counts representing biomass burning were absent at all places lying in and around the station under consideration and along the path of back trajectories. Similar results were observed on all high ozone days at all stations considered in the study. These characteristic features suggest that the enhanced surface ozone observed on these occasions may be a consequence of stratospheric intrusion.\nMultiple regression analysis (Figure 5) was used to determine the amount of dependence of STE events producing high-surface ozone levels on the order of latitude. A regression equation indicates the nature of the relationship between the variables and, in particular, the extent to which the variables are associated with one another. The regression equation and the respective correlation coefficients are noted in the plot. The coefficient of determination “” was found to be 0.8216, indicating that STE events producing high-surface ozone levels depend by an amount of 82.16% on the order of the Latitude. This is because deep STE events of ozone occur more frequently at high latitudes  compared to low Indian latitudes. Between October and May, the intertropical convergence zone (ITCZ) is located to the south of Thiruvananthapuram . The vertical velocity profiles exhibit a strong ascending motion over the mean position of the ITCZ, and a strong subsidence between 10°N–30°N latitudes [24, 25], thus affecting the STE processes over India. It appears from Figure 5 that STE events influencing the high surface ozone levels in India decrease exponentially with increase in altitude. However, this may be because all the measurement stations having high altitude, which have been considered in this study, lie at low latitudes where deep STE of ozone is not very prominent.\nThis paper investigates the possible contribution of STE events to the observed higher than normal surface ozone levels for few Indian cities. It is concluded that apart from transport processes and in situ photochemical production, STE also influences the observed high-surface ozone levels in Indian cities to a small extent (8%–16%). STE events producing high-surface ozone levels are found to be higher at high latitudes.\nThe surface ozone data for Indian cities were obtained from the India Meteorological Department. Topographic map of India depicting the measurement sites was obtained from the website: http://www.mapsindia.com/. Tropospheric carbon monoxide and methane data were obtained from Tropospheric Emission Spectrometer on NASA’s EOS (Earth Observing System) spacecraft. The 5-day back trajectories and potential vorticity were retrieved from the European Centre for Medium-Range Weather Forecasts. Total nitrogen dioxide column data from Ozone monitoring instrument (OMI) was obtained from the website http://www.temis.nl/. The vertical pressure velocity, relative humidity, and geopotential maps were retrieved from NCEP Reanalysis (Kalnay et al., 1996 ). The ATSR World Fire Atlas data has been taken from Ionia products of European Space Agency. The author also acknowledges her gratitude to ISRO for providing grant under the CAWSES India project for the above study and Professor J. N. Goswami, Director of Physical Research Laboratory, Ahmedabad, India, for providing library facility.\n- C. E. Junge, “Global ozone budget and exchange between stratosphere and troposphere,” Tellus, vol. 14, no. 4, pp. 363–377, 1962.\n- P. J. Crutzen, “Photochemical reactions initiated by and influencing ozone in unpolluted tropospheric air,” Tellus, vol. 26, no. 1-2, pp. 47–57, 1974.\n- H. Van Dop, R. Guicherit, and R. W. Lanting, “Some measurements of the vertical distribution of ozone in the atmospheric boundary layer,” Atmospheric Environment, vol. 11, no. 1, pp. 65–71, 1977.\n- W. K. Hocking, T. Carey-Smith, D. W. Tarasick et al., “Detection of stratospheric ozone intrusions by windprofiler radars,” Nature, vol. 450, no. 7167, pp. 281–284, 2007.\n- J. R. Holton, P. H. Haynes, M. E. McIntyre, A. R. Douglass, R. B. Rood, and L. Pfister, “Stratosphere-troposphere exchange,” Reviews of Geophysics, vol. 33, no. 4, pp. 403–439, 1995.\n- G. Vaughan, “Stratosphere-troposphere exchange of ozone,” in Tropospheric Ozone, I. S. A. Isaksen, Ed., pp. 125–135, D Reidel, Dordrecht, The Netherlands, 1988.\n- Gazette of India, “part 3, section 4, 18,” 2009, New Delhi / Kartika 27, 123.\n- T. K. Mandal, G. Beig, and A. P. Mitra, “Ozone and UV scenario over India,” Scientific Report 22, Center on Global Change, NPL, New Delhi, India, 2004.\n- S. L. Jain, B. C. Arya, A. Kumar, S. D. Ghude, and P. S. Kulkarni, “Observational study of surface ozone at New Delhi, India,” International Journal of Remote Sensing, vol. 26, no. 16, pp. 3515–3524, 2005.\n- K. A. Browning and R. Reynolds, “Diagnostic study of a narrow cold-frontal rainband and severe winds associated with a stratospheric intrusion,” Quarterly Journal, vol. 120, no. 516, pp. 235–257, 1994.\n- K. A. Browning and B. W. Golding, “Mesoscale aspects of a dry intrusion within a vigorous cyclone,” Quarterly Journal, vol. 121, no. 523, pp. 463–493, 1995.\n- M. A. Goering, W. A. Gallus, M. A. Olsen, and J. L. Stanford, “Role of stratospheric air in a severe weather event: analysis of potential vorticity and total ozone,” Journal of Geophysical Research D, vol. 106, no. 11, pp. 11813–11823, 2001.\n- A. Stohl, N. Spichtinger-Rakowsky, P. Bonasoni et al., “The influence of stratospheric intrusions on alpine ozone concentrations,” Atmospheric Environment, vol. 34, no. 9, pp. 1323–1354, 2000.\n- A. Stohl, H. Wernli, P. James et al., “A new perspective of stratosphere-troposphere exchange,” Bulletin of the American Meteorological Society, vol. 84, no. 11, pp. 1565–1473, 2003.\n- T. D. Davies and E. Schuepbach, “Episodes of high ozone concentrations at the earth's surface resulting from transport down from the upper troposphere/lower stratosphere: a review and case studies,” Atmospheric Environment A, vol. 28, no. 1, pp. 53–68, 1994.\n- E. Kalnay, M. Kanamitsu, R. Kistler et al., “The NCEP/NCAR 40-year reanalysis project,” Bulletin of the American Meteorological Society, vol. 77, no. 3, pp. 437–471, 1996.\n- E. L. Avol, W. C. Navidi, E. B. Rappaport, and J. M. Peters, “Acute effects of ambient ozone on asthmatic, wheezy, and healthy children,” Research Report, no. 82, pp. 1–30, 1998.\n- R. J. Delfino, A. M. Murphy-Moulton, and M. R. Becklake, “Emergency room visits for respiratory illnesses among the elderly in Montreal: association with low level ozone exposure,” Environmental Research, vol. 76, no. 2, pp. 67–77, 1998.\n- S. C. Gupta and V. K. Kapoor, Fundamentals of Applied Statistics, chapter 1, Sultan Chand and sons, New Delhi, India, 1993.\n- World Meteorological Organization, “Atmospheric ozone 1985: global ozone research and monitoring report,” WMO Report 16, Geneva, Switzerland, 1986.\n- P. Cristofanelli, A. Bracci, M. Sprenger et al., “Tropospheric ozone variations at the nepal climate Observatory-Pyramid (Himalayas, 5079 m a.s.l.) and influence of deep stratospheric intrusion events,” Atmospheric Chemistry and Physics, vol. 10, no. 14, pp. 6537–6549, 2010.\n- N. D. Ganguly and C. Tzanis, “Study of Stratosphere-troposphere exchange events of ozone in India and Greece using ozonesonde ascents,” Meteorological Applications, vol. 18, no. 4, pp. 467–474, 2011.\n- G. J. Roelofs and J. Lelieveld, “Model analysis of stratosphere-troposphere exchange of ozone and its role in the tropospheric ozone budget,” in Chemistry and Radiation Changes in the Ozone Layer, C. Zerefos et al., Ed., pp. 25–43, Kluwer Academic Publishers, Dodrecht, The Netherlands, 2000.\n- J. P. Peixoto and A. H. Oort, Physics of Climate, American Institute of Physics, New York, NY, USA, 1992.\n- V. H. Annes, K. Mohankumar, and P. V. Joseph, “Winter and summer hadley circulations over Peninsular India as monitored by mst radar at Gadanki (13.47°N, 79.18°E),” International Journal of Climatology, vol. 21, no. 5, pp. 593–601, 2001.\nCopyright © 2012 Nandita D. Ganguly. This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"format_constrained"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:fef1c30a-4290-4df1-8e65-54dafe30b560>","<urn:uuid:52eeb8f0-b3b4-4541-b0cc-a5805a9e7641>"],"error":null}
{"question":"Could someone break down the binding vs non-binding elements in business agreements - what makes certain terms survive termination in both Letters of Intent and Non-Disclosure Agreements? Would love a clear comparison! 💭","answer":"While Letters of Intent are generally non-binding, certain terms do survive termination and remain binding. These include exclusivity provisions (typically lasting 60-120 days), confidentiality requirements, and governing law/jurisdiction terms. The LOI should explicitly state which terms are binding exceptions to its generally non-binding nature. Similarly, NDAs create binding confidential relationships with specific obligations that survive according to stated time periods. These include requirements to protect trade secrets, customer information, and other confidential business information, along with obligations to destroy or return confidential information when finished. Both types of agreements typically include binding provisions about legal requirements, venue selection, and enforcement mechanisms that persist beyond the agreement's termination.","context":["Binding Terms in Letters of Intent for Business Sales\nWhen selling a business, a Letter of Intent (“LOI”) is often regarded as non-binding. One may be inclined to sign the LOI without first obtaining a legal review, thinking that the document can always be broken later without consequence. However, the LOI may contain certain important terms that are binding, and that survive the LOI’s termination. Furthermore, a poorly drafted LOI may create ambiguity as to whether it is in fact generally non-binding.\nAfter the LOI is signed, the buyer begins incurring significant expenses such as comprehensive due diligence and preparation of the purchase documents. Before a buyer embarks on this expensive process, it wants to know that the seller will not negotiate with other potential buyers. To address this, the LOI may establish an exclusive negotiating relationship. The seller agrees that it will not solicit other offers, or even engage in discussions with other parties, regarding the sale of its business. A time period for the exclusivity is established, and even if the deal falls through, the seller remains bound by the exclusivity restriction.\nCommon exclusivity time periods range from 60 to 120 days. The LOI may establish stipulated damages which apply if the seller breaches the exclusivity restriction. In the absence of stipulated damages, an aggrieved buyer may sue the seller for its costs incurred pursuing the acquisition. The seller should be aware of any such restrictions before signing the LOI.\nConfidentiality is frequently a high priority of the parties. A non-disclosure agreement should be signed before any information is shared. Usually such an agreement is in place before the LOI is drafted. If not, or if the parties desire additional confidentiality terms, the LOI should include them and make them binding. For example, the financial and other terms of the LOI shall be confidential and shall not be disclosed to anyone besides the professional advisors assisting the parties with the transaction.\nGoverning law and jurisdiction may be defined in the LOI. In that case, especially when the parties are located in different states, those terms should be binding so they apply to claims brought following the LOI’s termination.\nAny binding terms in the LOI should be the exception, not the rule. The LOI should contain an unequivocal statement that generally it is non-binding, that the definitive terms of the transaction shall be set forth in a purchase agreement, and that the LOI is not the purchase agreement. Any terms intended to be binding should be listed and referenced as exceptions to the non-binding declaration. If the LOI does not contain a sufficient non-binding declaration, it creates ambiguity that opens the door to litigation if the deal falls through and one side believes the other did not negotiate in good faith.\nEven if the LOI contains no binding terms, it still sets the stage for the transaction in important ways. It is crucial that it be well-drafted, and protect the party’s interest. In order to do this, the party’s legal counsel for the transaction should be involved in the process as early as possible.\nFor further information about this topic, please contact our firm and speak to one of our attorneys.\nNovember 13, 2018\nNovember 12, 2018\nNovember 8, 2018\nPractice Area Topics\n- Employment Law\n- Estates & Trusts\n- Family Law\n- General News\n- Medical Marijuana\n- Personal Injury\n- Real Estate Law","Understanding Non-Disclosure Agreements\nUnderstanding Non-Disclosure Agreements\nIf you’ve ever been asked to keep a secret, then you already understand the basic concept behind non-disclosure agreements.\nA non-disclosure agreement, or “NDA,” creates a confidential relationship between a person or business that has confidential or trade secret information and a person that has access to that information. The NDA agreement protects these business secrets by limiting the way they can be used or disclosed.\nThe term “trade secret” sounds very high-tech, and in fact technology companies often do have trade secrets. But confidential information can be any type of information that a business wants to keep secret so it can enjoy an economic advantage over its competitors. It may include customer lists, expansion plans, information about new products being developed, information about pending litigation, or data about a company’s clients or customers.\nSometimes, the parties will sign a mutual non-disclosure agreement in which they agree not to disclose confidential information about each other. A mutual NDA is useful if two businesses need to exchange information, such as when they’re contemplating a merger or joint business venture.\nWhat’s in a Non-Disclosure Agreement?\nA standard non-disclosure agreement or confidentiality agreement includes the following:\n- A definition of the information that is considered confidential.\n- A description of any information that is excluded from confidentiality. For example, confidential information may typically be disclosed if it is required for legal or accounting purposes, and information is usually not considered confidential if it has already been publicly released.\n- A description of the receiving party’s obligations. In addition to keeping the information confidential, the receiving party may be required to destroy or return confidential information when he or she is finished with it.\n- Time periods. A non-disclosure agreement may limit the amount of time the information is considered confidential.\n- Miscellaneous terms. Like most contracts, an NDA may contain standard contract terms at the end, including terms related to modifications, choice of law, choice of venue, arbitration and attorneys’ fees.\nA nondisclosure agreement may be a stand-alone document, or confidentiality clauses may be included in another document such as an employment agreement, an independent contractor agreement or a contract that establishes a business relationship.\nWhat Are Some Uses for Non-Disclosure Agreements?\nNon-disclosure agreements have become increasingly common. Here are a few of the common ways they come up:\n- Employee non-disclosure agreements. If your business deals in sensitive information, inventions, research or product development, you’ll want to make sure your employees don’t divulge company information to outsiders. Even if you just have ordinary business information such as sales data and customer lists, an employee nondisclosure agreement can help keep secret information out of the hands of competitors.\n- Independent contractor agreements. Whether it’s your virtual assistant, your part-time bookkeeper or the person you hired to help with a special project, independent contractors frequently have access to information that you don’t want made public.\n- Agreements with people from whom you are seeking funding. If you approach venture capitalists or local investors, you’ll need to divulge information about your products, your finances and your plans. An NDA can help ensure that they don’t share your ideas with a competing startup.\n- Agreements with the people with whom you do business. Doctors and lawyers are obligated to keep their patients’ and clients’ information confidential, but that’s not true of most other businesspeople. If the people you do business with receive or have access to your confidential information, you may want them to sign an NDA. Examples include the company you hire to manufacture your new product and the cleaning service whose workers are alone in your office at night.\n- Agreements in litigation or arbitration. When two parties are in a dispute, there is a formal process for exchanging information. Parties routinely require one another to sign confidentiality agreements that prohibit disclosure or use of the information outside of the litigation.\nWhat to Do if You Are Asked to Sign an NDA Form\nDuring the course of your business or employment, it’s likely that you will be asked to sign someone’s non-disclosure agreement. Remember that non-disclosure agreements can be contained in other documents, so you should look for headings such as “Confidentiality,” “Confidential Information,” or “Non-Disclosure.”\nYou should start by reading the agreement. Most NDAs are prepared from some sort of NDA template or form, and they can be long and include things that don’t seem to apply to your situation. After reading it, ask yourself whether it creates any obligations that you will have trouble fulfilling. If so, discuss these with the person who asked you to sign the NDA and see if you can renegotiate the terms.\nIf you have confidential information, beware of a confidentiality statement that might look like an NDA but has just the opposite effect. This sort of clause will typically say that the agreement does not create a confidential relationship or does not create any obligation of secrecy or confidentiality. That means that the other party has no obligation to keep any of your confidential information secret.\nWhether you draft one from scratch or use a non-disclosure agreement form, an NDA is a great way to protect sensitive business information from being disclosed to the public before you’re ready. If you routinely receive confidential information from others, you’ll probably also be asked to sign their NDAs. Just be sure you read them first and understand your obligations."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:1e26213f-588e-40e2-96fc-93788b3b0db2>","<urn:uuid:92a43f0f-9b6c-45e6-b6ff-3c0a0fc04c8f>"],"error":null}