{"question":"How do prevention techniques differ for airplane vs boat motion sickness? Need specific tips for both.","answer":"For airplanes, prevention focuses on selecting seats over the wing and keeping your eyes forward. For boats, the key preventive measures involve choosing midship cabins and lower deck locations where motion is minimized. On planes, focusing the eyes out the window at the horizon can help reduce symptoms. For boats, passengers should try to stay on deck in fresh air and keep eyes fixed on a stable point like the horizon. Both situations benefit from avoiding reading or looking at screens since this increases conflict between visual and inner ear signals. Small, frequent meals are recommended for both types of travel, though timing varies - airplane passengers should avoid large meals before flights while boat passengers should maintain regular small meals throughout the journey.","context":["Diagnosis and treatment of motion sickness\nDiagnosis of motion sickness\nMotion sickness is usually medically diagnosed in a clinical setting, the diagnosis will be based on the presence of any typical symptoms such as nausea, feeling unwell and suffering from a headache in response to motion that is externally imposed or perceived.\nMedical conditions such as allergies, and ear infections. for example a middle (otitis media) or inner ear infection (labyrinthitis) can disrupt balance and cause symptoms similar to motion sickness, as can head injuries, as such a doctor will conduct a thorough physical examination. During the exam, he /she will examine the ears, nose and throat, test balance, hearing, vision and nerve function.\nThe doctor will also obtain a full medical history from the patient. Should the patient have a history of the developing motion sickness and present with or report typical symtpoms, then no additional evualation may be required.\nHowever, should the patient not have experienced any prior episodes of motion sickness and suddenly become prone to the syndrome, develop symptoms when not in motion or develop additional symptoms such as hearing loss, blurred vision and difficulty speaking or walking, then additional diagnostic tests (including blood, urine and imaging tests like X-rays, a CT scan or MRI) and a neurological examination may be recommended. An evaluation of any headaches and migraines experienced may be warranted as some medical evidence suggests that the effective control of headaches and migraines may reduce a patient’s susceptibility to motion sickness10.\nMotion sickness may require a different diagnosis when the patient is displaying symptoms of true vertigo, which is a distinguishing characteristic of some other disorders such as vestibulopathy (this occurs when the parts of the ear responsible for balance are damaged, leading to visual and balance disturbances) and vestibular migraine and may be confused with the symptoms of motion sickness. It is important to note that true vertigo is not considered a characteristic of motion sickness.\n**My Med Memo – True vertigo differs from the dizziness experienced in motion sickness in a number of ways. Motion sickness refers to feeling off balance. This symptom is a result of motion stimuli. However, true vertigo includes symptoms of disorientation and the sensation of spinning or motion when neither of these is occuring.\nVestibular migraine, also known as migraine-associated vertigo, relates to a nervous system issue that results in repeated dizziness, also known as vertigo, in those who have a history of suffering from migraines. Unlike more traditional migraines, vestibular migraines do not always cause the patient to have a headache but do cause some debilitating symptoms that affect the person’s vision and inner ears, which leads to balance and hearing issues. Vestibular migraines do share some similarities to the symptoms of motion sickness, however, if a patient suffers from a vestibular migraine, they will typically experience a headache similar to that of a migraine as well as true vertigo, as opposed to experiencing non-vertiginous dizziness.\nVestibulopathy may increase one’s chances of visual-vestibular conflict (i.e. one’s sight and balance may be off). This condition should be taken into consideration in those who have reported experiencing the symptoms of motion sickness but have not been prone to the condition in the past. If vestibulopathy is suspected, a doctor will generally refer the affected person to a neurologist.\nManagement, prevention and treatment of motion sickness\nA general practitioner may be consulted when attempting to prevent or treat motion sickness. The treatment of acute motion sickness is typically ineffective and while medication can help to reduce the effects of it, it cannot stop it. As a result, emphasis is usually placed on preventing it instead.\nPrevention of motion sickness\nPeople tend to vary considerably in terms of their susceptibility to motion sickness and the symptoms experienced.\nSignificant experiences of motion sickness\nSome people may report the symptoms associated with motion sickness that they experience often interfere with their daily activities and ability to function. For example, some suffer from motion sickness on a daily basis during their commute to work via car or bus.\nThis group will typically ask their doctors how to prevent the symptoms from recurring. There are certain interventions that may play a role in the prevention of the condition which are described as follows:\nIt may be helpful to modify one’s external environment, these modifications should be taken into consideration when the symptoms of motion sickness start to emerge. Environmental modifications include:\n- Looking at a distant or stationary object or at the horizon when travelling.\n- Avoiding reading or screens when travelling as this may increase the conflict between vestibular (inner ear) and visual cues sent to the brain which lead to motion sickness.\n- Selecting seats that experience the least amount of motion. For example, when travelling via ship, the midship cabins and lower deck are recommended. When travelling in a car, it is advised that a person sits in the front seat. A seat over the wing is recommended in a plane and the forward-facing seats of trains and buses are advised.\n- Driving a car is often better for preventing the symptoms of car sickness than sitting in the passenger seat. This is due to the visual information being received being more consistent with the detection of vestibular motion (i.e. the sense of balance and co-ordination of motion within the inner ear) being experienced. If one must be a passenger, then sitting in the front seat with one’s eyes on the road is advised.\nAlternative and complementary treatments and modifications to motion sickness\nSome doctors may recommend that their patients eat hard ginger candy as ginger is thought to reduce the symptoms of nausea. These candies should be sucked on when experiencing or anticipating the symptoms associated with motion sickness.\nThe reasons as to why ginger is a beneficial alternative treatment for the symptoms of motion sickness are uncertain, however, some studies11 suggest that the plant has an effect on gastric mobility. Additional information suggests that ginger has a central impact on the serotonin receptor subtypes in the brain, serotonin is known as the happy hormone and the release of it improves mood and overall wellbeing.\nAnother suggestion is the use of acupressure bands which are applied to both wrists as a preventative measure. Acupressure is beneficial in the treatment of motion sickness as it applies pressure at a specific point on the lower side of the wrist either through manual pressure or through the use of wristbands, and helps to curb the symptoms in some, but not all, people.\nMore information on pressure points for motion sickness\nWhat is the pressure point for motion sickness?\nOne of the pressure points used in acupressure for motion sickness is the P6 acupressure point, also known as the Pericardium 6 or the Nei Guan. This pressure point is thought to be helpful in alleviating the symptoms of motion sickness such as nausea and headaches. This point is located on the inner forearm, roughly the breadth of three fingers below the wrist fold, between the two tendons of the forearm.\nHow do I use this pressure point for motion sickness?\nThe first step in using this pressure point to relieve nausea is to position your hand with your palm facing towards you with the fingers pointing upwards.\nFind the pressure point position by placing the first three fingers from the opposite hand below the fold of the wrist. After you have done this, place your thumb just below your index finger on your wrist, you will be able to feel the two tendons described above.\nThen, using your forefinger and thumb, apply pressure to this point and move your fingers in a circular motion for a few minutes. The pressure should be firm enough to offer relief but not cause pain. Repeat this process on your other wrist.\nMedications - Antihistamines or scopolamine\nIn some cases, medication may be helpful in preventing motion sickness, however, these medications often have sedative effects, therefore, a doctor will weigh the likelihood of the patient suffering from motion sickness against the possibility of potential side effects such as drowsiness and impaired concentration which may occur.\nThe most commonly prescribed medications for motion sickness are antihistamines and scopolamine. These medications are thought to decrease the symptoms of nausea and general malaise by dulling the inner ear's ability to sense motion. This is achieved by blocking the messages sent to the areas of the brain that are responsible for triggering nausea and vomiting. These medications typically cause side effects which may include blurred vision, sedation, dry mouth and in some older patients, urinary retention (i.e. an inability to empty the bladder).\nThe medication chosen by the doctor will be dependent on the specific circumstances, for example, the duration of the motion exposure and the acceptability of the side effects such as drowsiness that these medications tend to have. For example, if a patient intends to enjoy a cruise holiday and as such will be exposed to prolonged motion which may cause seasickness, a doctor may prescribe a scopolamine patch as its long acting effects may be more desirable than taking shorter-acting antihistamines which may cause intense drowsiness.\nIf self-medicating, some antihistamines can be bought over the counter, these include meclizine and dimenhydrinate. However, it is important to note that non-sedating antihistamines are not effective in treating motion sickness, and as such, options should be discussed in detail with the attending pharmacist.\nSome people may need to avoid all forms of sedation as they have to perform important tasks such as driving a car, flying an aeroplane or captaining a ship. In such cases, antihistamines will need to be taken along with a stimulant such as ephedrine or pseudoephedrine which will generally require a prescription in most countries.\nNon-significant experiences of motion sickness\nSome people do not have a significant history of motion sickness but may be concerned about developing it when planning to travel on an extended trip or during a specific activity.\nThis may prompt an enquiry as to whether medication is an option as a preventative measure, however most doctors will generally advise against this due to the sedative and anticholinergic effects. It is cases such as these that environmental modifications are recommended.\n**MyMed Memo: Anticholinergic effects occur with the use of some medications. These drugs block the effects of acetylcholine, a neurotransmitter (chemical messenger in the brain) that controls a variety of bodily functions. Anticholinergic effects include reduced voluntary movement, urination, digestive function and mucus secretion.\nPatients who are pregnant\nSome medications that may be prescribed to pregnant women to alleviate the symptoms of motion sickness include antihistamines such as dimenhydrinate and meclizine.\n10. NCBI. 2018. Effect of Prophylactic Medication on Associated Dizziness and Motion Sickness in Migraine. Available: https://www.ncbi.nlm.nih.gov/pubmed?term=29227453 [Accessed 16 May 2018]\n11. NCBI. 1989. The anti-motion sickness mechanism of ginger. A comparative study with placebo and dimenhydrinate. Available: https://www.ncbi.nlm.nih.gov/pubmed?term=2683568 [Accessed 16 May 2018]","BACK TO ZIGMA SERIES\nMotion sickness is a normal response to real, perceived, or anticipated movement. People tend to experience motion sickness on a moving boat, train, airplane, automobile, or amusement park rides. Although this condition is fairly common and only a minor nuisance for the occasional traveler, it may be incapacitating for people with an occupation that requires constant movement, such as a flight attendant, pilot, astronaut, or ship crew member. Symptoms generally consist of dizziness, fatigue, and nausea, which may progress to vomiting. Fortunately, most symptoms disappear once the journey is over.\nSigns and Symptoms\nThe most common signs and symptoms of motion sickness include:\nMotion sickness occurs when the body, the inner ear (a tiny structure involved in hearing and balance), and the eyes send conflicting signals to the brain. This reaction is generally provoked by a moving vehicle such as a car, boat, airplane, or space shuttle, but it may also happen on flight simulators or amusement park rides. From inside a ship's cabin, the inner ear may sense rolling motions that the eyes cannot perceive, and, conversely, the eyes may perceive movement on a \"virtual reality\" simulation ride that the body does not feel. Interestingly, once a person adapts to the movement and the motion stops, the symptoms may recur and cause the person to adjust all over again (although, this reaction is generally brief). In addition, even anticipating movement can cause anxiety and symptoms of motion sickness. For example, a person with a previous experience of motion sickness may become nauseous on an airplane before take-off.\nThe following are the most common risk factors for motion sickness:\nMost people who have experienced motion sickness in the past ask their healthcare provider how to prevent another episode from occurring in the future; rarely will an individual arrive at his or her healthcare provider's office actually experiencing motion sickness. To establish a diagnosis of motion sickness, the provider will inquire about the individual's symptoms as well as the event that typically causes the condition (such as riding in a boat, flying in a plane, or driving in car). Laboratory tests are generally not necessary to establish a diagnosis of motion sickness.\nThe following general measures may be taken to help avoid the discomfort caused by motion sickness:\nIndividuals who commonly experience motion sickness on a plane should take the following preventive measures:\nIndividuals with a tendency toward motion sickness on a boat should take the following preventive measures:\nWhile medications may be an acceptable treatment for travelers who occasionally experience motion sickness, the goal for individuals who experience motion sickness on a regular basis or whose work is affected by their symptoms is to learn to control–and eventually prevent–these symptoms. This may be accomplished with mind/body practices, such as cognitive-behavioral therapy and biofeedback. Other alternatives to medication include homeopathy, acupuncture, ginger (Zingiber officinale),� dietary adjustments, and physical therapy.\nMedications for motion sickness may cause drowsiness and impair judgement and, therefore, should be avoided in pilots, astronauts, ship crew members, and individuals in any other occupation where heavy equipment is operated or where being alert is critical. The following medications are a reasonable option for infrequent travelers and others who experience motion sickness occasionally:\nNutrition and Dietary Supplements\nGenerally, small frequent meals are recommended for individuals prone to motion sickness. Dietary records of a small group of novice pilots also indicate that an increase in airsickness (motion sickness from air travel) may be associated with the following:\nGinger (Zingiber officinale)\nSeveral studies suggest that ginger may be more effective than placebo in reducing symptoms associated with motion sickness. For example, in one clinical trial of 80 novice sailors (prone to motion sickness), those who took ginger (in powder form) experienced a significant reduction in vomiting and cold sweating compared to those who took placebo. Similar results were found in a study with healthy volunteers. While these results are promising, other studies suggest that ginger is not as effective as medications in reducing symptoms associated with motion sickness. In a small study of volunteers who were given ginger (fresh root and powder form), scopolamine, or placebo, those receiving the medication demonstrated a significant reduction in symptoms compared to those who received ginger. More rigorous trials are needed to confirm the effectiveness of ginger for motion sickness.\nAlthough black horehound (Ballota nigra) and peppermint (Mentha piperita) have not been scientifically studied for their use in treating motion sickness, some professional herbalists may recommend these herbs in combination to alleviate nausea associated with the condition.\nSome preliminary studies suggest that people with motion sickness who receive acupuncture report a significant improvement in symptoms compared to those who receive sham acupuncture (needling at points not indicated for nausea, vomiting, or motion sickness) or no acupuncture at all. Although results have been less convincing, studies also suggest that acupressure may help reduce symptoms of motion sickness in the same way as acupuncture. An acupressure practitioner works with the same points used in acupuncture, but stimulates these healing sites with finger pressure, rather than inserting fine needles.\nMassage and Physical Therapy\nOne case study of a woman with motion sickness suggests that balance training and habituation (reducing or modifying one's response to a stimulus that causes motion sickness) may help diminish the symptoms of the condition. The use of habituation for the treatment of motion sickness is based on the theory that when an individual prone to motion sickness is repetitively exposed to the stimulus that causes motion sickness (such as driving in a car or riding on an elevator) in a controlled, supervised fashion, he or she will habituate, or become used to that stimulus. Over time, the stimulus will no longer evoke the motion sickness response and symptoms will diminish.\nThere have been few studies examining the effectiveness of specific homeopathic remedies. A professional homeopath, however, may recommend one or more of the following treatments for motion sickness based on his or her knowledge and clinical experience. Before prescribing a remedy, homeopaths take into account a person's constitutional type. In homeopathic terms, a person's constitution is his or her physical, emotional, and intellectual makeup. An experienced homeopath assesses all of these factors when determining the most appropriate remedy for a particular individual.\nBiofeedback Training and Relaxation\nIn a study of 55 pilots who had to stop flying due to symptoms of motion sickness, 76% of them successfully overcame their motion sickness and were able to return to work after participating in a biofeedback training and relaxation program. Biofeedback instruments recorded skin temperature and changes in muscle tension while the pilots were exposed to a stimulus that caused motion sickness (sitting in a tilting, rotating chair). While in the chair, the pilots performed various relaxation techniques, such as deep muscle relaxation and mental imagery. Over time, the pilots habituated to the rotating chair; they no longer felt sick in the chair because they learned to relax in it.\nThe goal of cognitive-behavioral therapy is to alleviate the anxiety that some people experience simply thinking about movement or motion sickness. In a study of 50 pilots who occasionally experienced motion sickness, 86% of them successfully overcame their symptoms after cognitive-behavioral therapy. During cognitive-behavior therapy, individuals are exposed to a provocative stimulus (such as a tilting, rotating chair) in a slow, and controlled fashion until they experience some symptoms of motion sickness, but not until they are overwhelming. As the individual performs better and better on the rotating chair, his or her confidence builds and anxiety lessens.\nIn a study of 46 people with motion sickness, those who were instructed to take slow, deep breaths had a significant reduction in symptoms of motion sickness compared to those who breathed normally or counted their breaths. Interestingly, involuntary rapid and shallow breathing often exacerbates symptoms of motion sickness. While it makes sense that slow, deliberate breathing would help reduce the anxiety associated with motion sickness, further studies are needed to determine whether breathing techniques effectively diminish the symptoms associated with the condition.\nTraditional Chinese Medicine\nOne small animal study suggests that Pingandan, a Chinese herbal mixture, may significantly reduce signs of motion sickness (including defecation, urination, salivation, panting, and drowsiness). Pingandan primarily consists of:\nWhile individuals who have used the herbal remedy report fewer side effects than those who take scopolamine, more studies are needed to determine whether Pingandan is a safe and effective therapy for motion sickness.\nDue to a lack of clinical evidence, there is some controversy regarding the safety of ginger taken during pregnancy. In one study of pregnant women, more than 70% reported less morning sickness while taking 250 mg of ginger 4 times per day compared to those who received placebo. Therefore, healthcare practitioners recommend limiting intake of ginger to this amount if used during pregnancy.\nPrognosis and Complications\nWhile motion sickness has no long-term complications, the condition may be devastating for those in an occupation that involves constant movement, such as a flight attendant, pilot, astronaut, or ship crew member.\nThe symptoms of motion sickness generally disappear quickly once the journey (such as a moving boat, train, airplane, or automobile) is over. People who travel infrequently may also become accustomed to movement during a trip lasting several days. Even those who travel often may improve from repeated exposures to the same type of experience. However, people who become anxious before a journey often experience worsened symptoms of motion sickness and tend to require more formal interventions, such as biofeedback and relaxation training."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:2c16e401-3fc7-4617-bb22-9e16f81461c0>","<urn:uuid:1390efeb-caf9-457f-b9d5-0efb352bd527>"],"error":null}
{"question":"How does diversity drive innovation in companies, and what specific actions are organizations taking to promote cultural equity in the arts sector? 🤔 I'm curious about the connection between these two domains!","answer":"Diversity drives innovation by providing different perspectives and ideas - 85% of executives agree it's crucial for innovation, with over 75% planning to increase diversity initiatives in the next three years. Companies are implementing recruitment programs (65%) and development programs (53%) to attract diverse talent. In the arts sector, organizations are taking specific actions including pursuing cultural consciousness through transparent policies, dismantling inequities in their systems and programs, expanding diverse leadership within boards and staff, and ensuring fair distribution of resources. They are also working to improve the cultural leadership pipeline and advocating for policies that promote cultural equity in both public and private sectors.","context":["About the Statement on Cultural Equity\nThe pursuit of cultural equity is an active effort to go beyond diversity policies and anti-discrimination laws. Words on paper, laws in books and organizational policies are the first step toward a very simple idea, the arts are for all of us. The struggles of those who have advocated, demonstrated and fought for the freedom for everyone to participate in the arts cannot be underestimated.\nBuilding on those victories, it is our moral obligation to continue to broaden and deepen opportunities for everyone to take part in “the development of arts policy; the support of artists; the nurturing of accessible, thriving venues for expression; and the fair distribution of programmatic, financial, and informational resources.”\nThe Mimbres Region Arts Council, by unanimous vote, adopted the Statement of Cultural Equity, comprehensively developed by Americans for the Arts, a national arts service organization. Their approach to crafting language around the pursuit of cultural equity is a model that is worthy of adopting in our community. We intend to not only build on these dialogues, but to find ways to implement these ideals at a grassroots level here in Southwest New Mexico.\nThere will be many challenges and progress will undoubtedly be frustrating and slow. Regardless, the Mimbres Region Arts Council is committed to actions that “support a full creative life for all, championing policies and practices of cultural equity that empower a just, inclusive, equitable nation”\nSimply put, the Mimbres Region Arts Council wants everyone to have access to all of the crayons in the box, all of the musical notes, all of the steps, and all of the words to create great art in an appreciative and safe environment. Join us in our efforts and remember; Arte Para Todos!\nStatement on Cultural Equity\nTo support a full creative life for all, the Mimbres Region Arts Council commits to championing policies and practices of cultural equity that empower a just, inclusive, equitable nation.\nDEFINITION OF CULTURAL EQUITY\nCultural equity embodies the values, policies, and practices that ensure that all people—including but not limited to those who have been historically underrepresented based on race/ethnicity, age, disability, sexual orientation, gender, gender identity, socioeconomic status, geography, citizenship status, or religion—are represented in the development of arts policy; the support of artists; the nurturing of accessible, thriving venues for expression; and the fair distribution of programmatic, financial, and informational resources.\nACKNOWLEDGEMENTS & AFFIRMATIONS\n- In the United States, there are systems of power that grant privilege and access unequally such that inequity and injustice result, and that must be continuously addressed and changed.\n- Cultural equity is critical to the long-term viability of the arts sector.\n- We must all hold ourselves accountable, because acknowledging and challenging our inequities and working in partnership is how we will make change happen.\n- Everyone deserves equal access to a full, vibrant creative life, which is essential to a healthy and democratic society.\n- The prominent presence of artists challenges inequities and encourages alternatives.\nMODELING THROUGH ACTION\nTo provide informed, authentic leadership for cultural equity, we strive to…\n- Pursue cultural consciousness throughout our organization through substantive learning and formal, transparent policies.\n- Acknowledge and dismantle any inequities within our policies, systems, programs, and services, and report organization progress.\n- Commit time and resources to expand more diverse leadership within our board, staff, and advisory bodies.\nFUELING FIELD PROGRESS\nTo pursue needed systemic change related to equity, we strive to…\n- Encourage substantive learning to build cultural consciousness and to proliferate pro-equity policies and practices by all of our constituencies and audiences.\n- Improve the cultural leadership pipeline by creating and supporting programs and policies that foster leadership that reflects the full breadth of American society.\n- Generate and aggregate quantitative and qualitative research related to equity to make incremental, measurable progress towards cultural equity more visible.\n- Advocate for public and private-sector policy that promotes cultural equity.\nThis statement adopted Mimbres Region Arts Council’s Board of Directors on January 18, 2017 is modeled on the work of the national arts organization; Americans for the Arts, Statement on Cultural Equity representing the culmination of a year of work and consultation with members, advisory council members, stakeholders in the arts field, board, staff, and partners throughout the nonprofit sector. For information can be found at http://www.americansforthearts.org/about-americans-for-the-arts/statement-on-cultural-equity.","NEW YORK--(BUSINESS WIRE)--As innovation becomes more of a key differentiator for the world’s largest companies, organizations increasingly see having a diverse and inclusive workforce as critical to driving the creation and execution of new products, services, and business processes, according to a new study released by Forbes Insights (available at www.forbes.com/forbesinsights).\n“Fostering Innovation Through a Diverse Workforce” is based on an exclusive survey of 321 executives at large global enterprises ($500 million-plus in annual revenues). All respondents had direct responsibility or oversight for their companies’ diversity and inclusion programs. The study was sponsored by AT&T, L’Oréal USA, and Mattel.\nAccording to the survey, a diverse and inclusive workforce is necessary to drive innovation and promote creativity—85% of respondents agreed (48% strongly so) that diversity is crucial to gaining the perspectives and ideas that foster innovation. As importantly, more than three quarters indicated that their companies will put more focus over the next three years to leverage diversity for their business goals, including innovation.\n“Companies have realized that diversity and inclusion are no longer separate from other parts of the business,” said Stuart Feil, editorial director of Forbes Insights. “Organizations in the survey understand that different experiences and different perspectives build the foundation necessary to compete on a global scale.”\nThe Forbes Insights study also looked at how companies in different regions of the world approach diversity and inclusion, what programs fall into these initiatives, and what structures work most successfully. Other key findings include:\n- A diverse and inclusive workforce is crucial for companies that want to attract and retain top talent. Most companies (65%) have programs in place to recruit diverse employees, but fewer follow that up with diversity-focused development programs (53%), and diversity-focused retention programs (44%).\n- Just about every company surveyed had some kind of diversity and inclusion program, and many go beyond gender and race. Gender diversity programs are the most common (81%), followed by programs focused on ethnicity (77%), age (72%), and race (70%). Regionally, Asia-Pacific companies are more likely to have programs that focus on age and nationality, and European companies are more likely to look at disability or sexual orientation.\n- Not all diversity plans within a company are identical. Half said that their organizations have a global plan that allows for different strategies to address regional or cultural differences, while about a third said their strategies allow for minimal regional deviation.\n- Responsibility for the success of a company’s diversity/inclusion efforts lies with senior management. Seven out of ten companies reported that the buck stops at the C-level and their board of directors.\n- There are still some impediments to companies’ diversity efforts. Respondents felt they’ve made progress in gender diversity, but they feel they’ve fallen short in areas such as disability and age.\nAbout Forbes Insights\nForbes Insights (www.forbes.com/forbesinsights) is the strategic research practice of Forbes Media, publisher of Forbes magazine and Forbes.com. Taking advantage of a proprietary database of senior-level executives in the Forbes community, Forbes Insights’ research covers a wide range of vital business issues, including: talent management; marketing; financial benchmarking; risk and regulation; small/midsize business; and more."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:b013990a-0fdb-4594-aa4c-2670761b0f96>","<urn:uuid:e437b6ae-a0c4-4f6b-889d-cd8ea4e935c9>"],"error":null}
{"question":"What challenges exist in real-time computer vision for transportation systems?","answer":"Real-time computer vision in transportation systems faces multiple challenges. For train systems, there are technical challenges like determining the correct signal among multiple signals, requiring additional techniques like edge detection to identify which signal corresponds to the specific track. Data transmission is also an issue - for example, the NS project had to limit recording to 16 hours per day due to SIM card data transfer costs. Similarly, in road transportation, current detection models struggle with diverse environmental conditions and unstructured settings, making it difficult to generalize across different scenarios like weather changes and dynamic surroundings, which is crucial for safety in autonomous navigation.","context":["With the rapid developments in the computer vision field, a lot has changed with regards to the possibilities of this technique. There have been numerous cases of real time applications which have been developed and put into practice. Some of these examples include: self-driving cars, AI-driven crop machines on farms, detecting weeds and removing them and even the cashier-less Amazon-Go stores.\nThe past couple of months, Xomnia data scientist Romano Vacca worked together with an internal team of Nederlandse Spoorwegen (NS) during a Proof of Concept (POC), in which they tried to answer the question:\nCan state of the art computer vision techniques recognise train signals, as well as distinguish the different colours in real time during a train ride, in such a way that this information may assists the cabin driver in its decision making\nFor this project, one specific train on a specific track was chosen. This track was almost in a straight line, with one big curve in the middle. Furthermore, most of the track was a single track, which made the scope a bit easier.\nMoreover, a decision had to be made on a position to place the camera, taking into account certain rules such as it should not be in the eyesight of the cabin driver. The camera was connected to a small computer, together with gps and internet connection. The data was sent in realtime to the cloud using an IoT service. Dedicating time and looking for the best way to set it up was seen as very valuable.\nHaving a camera record all day can lead up to quite a lot of data. There was no wifi connection possible to send the data to the cloud. This meant we had to use a SIM card for data transfer. While using a Sim card can be convenient, it is also an expensive way of transmitting gigabytes of data. To minimise the amount of data, while still having the right quality and quantity, we decided to only record 16 hours a day, from 7 in the morning to 11 at night on weekdays. This way, we would have bright and dark images to have the model learn to detect the signals in all the possible scenarios. Other scenarios that were taken into account were sunny days, rainy days, foggy days and other weather conditions that can occur.\nLooking at the requirements of a model that has to provide realtime information about certain objects, YoloV3 was chosen. This object detection algorithm is known as one of the fastest, while still having a good accuracy. Detecting signals on the track was not that difficult since traffic lights are one of the objects most sophisticated models are pre-trained on. So, having the weights of the model adapt to train signals which are a little bit different than regular traffics, did not cost a lot of time.\nNext to the algorithm for the signals, some business rules also needed to be taken into account. An example: If there are two signals next to each other, which one is relevant for the train? If these two signals have the same status e.g. green, then there is no problem, however if one is green and the other is red, this could have some serious consequences. To tackle this problem, using some basic computer vision technique such as canny edge detection, a method was created that detected the train track the train was on. With this knowledge, the signal closest to the track can be determined to the one that holds the appropriate information.\nThe initial goal was to demo this in real time, having the model provide information to the cabin driver. Unfortunately, due to the COVID-19 circumstances, this couldn’t happen.\nAs for the next steps, the advanced analytics team of the NS will continue working on this project. They focus on improving the accuracy as well as taking more objects into account so even better advice could be given to the cabin driver.","On Generalizing Detection Models for Unconstrained Environments\nObject detection has seen tremendous progress in recent years. However, current algorithms don’t generalize well when tested on diverse data distributions. We address the problem of incremental learning in object detection on the India Driving Dataset (IDD). Our approach involves using multiple domain-specific classifiers and effective transfer learning techniques focussed on avoiding catastrophic forgetting. We evaluate our approach on the IDD and BDD100K dataset. Results show the effectiveness of our domain adaptive approach in the case of domain shifts in environments.\nObject detection has been a widely studied task in computer vision. It is focussed upon classifying objects present in an image and then regressing bounding boxes over the localized proposals. We have seen remarkable results with CNN based models[NIPS2012_4824] on the COCO dataset[DBLP:journals/corr/LinMBHPRDZ14] [DBLP:journals/corr/abs-1805-09300] [DBLP:journals/corr/abs-1904-08189] [DBLP:journals/corr/abs-1901-01892] [DBLP:journals/corr/abs-1904-11492]. Recently, [DBLP:journals/corr/abs-1906-02659] showed that when commonly used detectors are evaluated on nonstandard settings of objects in an environment, they tend to provide unusual predictions. This is also applicable for autonomous navigation systems operating in unstructured environments (e.g drivable areas except roads etc.) as well. Current detection methods don’t generalize well when they encounter diverse environmental conditions.\nWe witness variety of environmental conditions when it comes to driving such as weather changes, dynamic changes in the surrounding environment, etc. Current detectors have been tested on data obtained from structured environments which are often not representative of real-world conditions. As a result of which, the need for data obtained from nonstandard sources is felt the most for data-driven algorithms to improve and test their generalizing capabilities.\nAutonomous navigation algorithms must perform well on multiple domains especially the ones with corner cases for safety purposes. Most importantly, we want to be able to learn from a large standard data distribution to efficiently learn features in an embedding space and learn progressively from domain-specific data without having access to earlier used data.\nIn this paper, we address the problem of incremental learning and domain adaptation to some extent for object detectors to improve generalizing capabilities. Specifically, we tackle the problem of adapting from a standard data distribution to data obtained from the unstructured environment. We also provide baseline results on IDD and BDD100K for object detection task to compare our proposed methods.111Code for this work can be found here\n2 Related work\nObject Detection: Region proposal based methods introduced in [DBLP:journals/corr/GirshickDDM13] have been widely used as object detectors. It made use of selective search to reduce the number of bounding boxes. Spatial Pyramid Pooling Nets [DBLP:journals/corr/HeZR014] could generate a fixed-length representation in a dynamic manner irrespective of image scale. Fast RCNNs [DBLP:journals/corr/Girshick15] made use of regression for bounding box predictions. [NIPS2015_5638] made use of RPNs and introduced anchor boxes to deal with different aspect ratios and scales. SSD [DBLP:journals/corr/LiuAESR15] method runs a CNN on input image only once and calculates a feature map that doesn’t require proposal generation steps. Stereo RCNNs [DBLP:journals/corr/abs-1902-09738] extends the use of Faster RCNN with stereo images for 2D and 3D bounding box predictions. It is a region proposal based network that works without the need for point clouds. Our approach can also be extended for 3D object detection similarly but we still lack the diversified ground truth data (such as 3D bounding box coordinates or Lidar point clouds obtained from unconstrained environments) for 3D detections.\nLearning from multiple distributions: The concept of making generalizable deep learning models has been widely studied. This often involves retaining what the model has learned in the past and performing incremental learning on multiple domains. [inproceedings] used a GAN[NIPS2014_5423] to approximate the feature distribution in the source domain. [DBLP:journals/corr/LiH16e] [DBLP:journals/corr/abs-1808-06281] addressed the task of incremental learning with architectures that inhibit loss of learned knowledge.  made use of a larger network to train a smaller network to generate close predictions. [DBLP:journals/corr/CourtyFTR15] treated the task of domain adaptation as an optimal transport problem.\nFaster RCNN: It takes an RGB image as an input. The model consists of a feature extractor followed by a feature pyramid network (FPN) and region proposal network (RPN) for generating region proposals which are then used to detect objects. RPNs are more efficient than selective search. They perform a ranking of anchor boxes to reduce their number and propose those which most likely contain an object. Image features are generated by a backbone network which is then fed to an RPN along with images and targets for generating proposals. After RPN, we get proposed regions with different sizes. Region of Interest (ROI) classifier predicts the category label obtained by using ROI Pooling. RPN can output differently sized regions. ROI Pooling can simplify the problem by reducing the feature maps into the same size. The loss is the sum of classification and regression loss defined as:\nWe refer readers to [NIPS2015_5638] for further details about model architecture.\n4 Baseline Model\nWe use a region proposal based approach for the baseline model. For feature extraction which is used by RPN, we use ResNet50[DBLP:journals/corr/HeZRS15] followed by FPN pretrained on COCO. Linear layers after ROI Head were adjusted as per the number of classes. Our RPN generates 5 x 3 anchors per spatial location with 5 different sizes and 3 different aspect ratios. We used random horizontal flipping for augmenting the input data. Our baseline model is trained on a non HQ image set from IDD with batch size set to 4 for 5 epochs per camera orientation. It was optimized using SGD [Sutskever:2013:IIM:3042817.3043064] with momentum and weight decay set to 0.9 and 0.00004 respectively. The learning rate was initially set to 0.001 with the Cyclical learning rate scheduler[DBLP:journals/corr/Smith15a]. We use the same process for performing training on BDD100K. Results are shown in Table 1 and Table 2.\n5 Incremental Learning\nThe following section contains a description of training methodology and proposed transfer learning techniques aimed at minimizing catastrophic learning while adapting to target data distribution.\nOur task is to perform incremental learning on multiple diverse data distributions. The network initially learns the weights from a standard data distribution and the proposed techniques help in performing domain adaptation while remaining consistent with the already learned information. Once trained on one distribution, we don’t require already used data.\n5.1 Domain specific heads\nWe make use of two ROI heads which are combined with the common backbone and RPN for generating domain-specific predictions. We can also have more than two ROI heads depending upon the number of target domains we want to adapt to. The weights of RPN and feature extractor are shared across all domain-specific classifiers. Weight sharing allows the network to learn common features with the proposed techniques across all domains without any increment in the number of parameters. Domain-specific heads also help in cases where classes don’t overlap in both distributions, as in this case.\nAfter the addition of ROI Head to the baseline model, we train the head on to learn domain-specific weights. This is followed by progressive training of other components of the network to avoid catastrophic forgetting as proposed in [howard2018universal] to learn domain invariant features.\n5.2 Discriminative finetuning\nWe use different learning rates to train different layers of our network. As shown in [NIPS2014_5347], different layers of the network are responsible for capturing different types of information. Discriminative finetuning allows us to set the rate at which these different components of the network learn. Since the weights of the backbone and RPN are being shared for all tasks, we want to inhibit the loss of learned information. We use a higher learning rate for domain-specific components and a lower learning rate for components whose weights are being shared. Specifically, we require a lower learning rate for the backbone and RPN since feature extraction and generation of region proposals are common tasks across all domains and a higher learning rate for domain-specific ROI Heads. A general SGD update of a model’s parameters at time step looks like:\nwhere denotes learning rate and denotes gradient with respect to model’s objective function. We split model’s parameters into where contains parameters of the model at the -th layer and denotes the total number of layers of our network. The SGD update then becomes:\n5.3 Gradual unfreezing\nTraining the entire model on a different domain at once leads to catastrophic forgetting, which means the model adapts itself to the target domain on which it is being tuned compromising the performance on source domain on which it was trained. We overcome this issue by gradually unfreezing the components of the network with discriminative finetuning. We freeze all the components initially and unfreeze the domain ROI Head which is fine-tuned until convergence followed by progressive unfreezing and finetuning of FPN and RPN.\n5.4 Cyclical Learning Rate\nWe optimize our network using the Cyclical learning rate (CLR) as proposed in [smith2017cyclical]. Instead of having a gradually decreasing learning rate, as the training converges, we use CLR which cycles the learning rate between lower and upper bound. CLR helps in oscillating towards a higher learning rate wherever necessary. It prevents the network from converging at some poor local minima in loss landscape. We make use of triangular variation for our experiments.\nIn this section, we evaluate our proposed approach on two diverse datasets. One dataset denotes structured environments and the other one denotes unstructured and unconstrained environments to which we want to adapt. The later one simulates high traffic density, rural areas with no proper roads, classes usually not seen in other datasets posing a much harder task for current object detection models.\nIDD: We use IDD for target adaptation tasks. It provides data for object detection in two resolutions. The non HQ set consists of 27072 images taken from 5 different orientations of the camera with two resolutions 964x1280 and 1080x1920. The HQ set consists of 14722 images with two resolutions 720x1280 and 1080x1920. There are 15 classes for this task. Note that we only perform training and evaluation on the non HQ set of IDD. Results can be further improved if high res images from the HQ set are used to train the components of the network. The validation set consists of 10,225 high-resolution images.\nBerkeley Deep Drive: We use BDD100K [DBLP:journals/corr/abs-1805-04687] to denote data distribution obtained from structured environments. We only use the images and their respective ground truths for the detection task. There are 69863 images in train and 10000 in the validation set. We trained our proposed model over 12 classes.\n6.2 Training methodology\nThe proposed architecture has been shown in LABEL:fig:fig_model. This architecture is based on Faster RCNN. The backbone is a ResNet50 pretrained on COCO. We use a batch size of 16. We use the same baseline model with an additional ROI Head. We obtain four feature maps from the batch of images obtained by intermediate layers of backbone to perform multi-scale ROI aligning. These feature maps are shared across all components. The obtained feature maps are then fed to an RPN for generating region proposals followed by domain-specific ROI pooling and prediction layers. While training and inference, only the designated ROI Head is used for the respective domain. This model is trained in an end to end strategy and inference can be performed in a regular manner with learned weights.\nIn the following section, we evaluate the effects of each of the mentioned techniques along with the effect of varying learning rates. As per convention, we use BDDIDD to denote BDD100K as and IDD as . Since we have more data collected from structured environments [DBLP:journals/corr/abs-1803-06184] [DBLP:journals/corr/abs-1805-04687] [Geiger2012CVPR] , the results simulate learning from already existing data distributions to adapt to unstructured environment.\nAdding domain specific head Here we use the same baseline model for BDD. We add a domain-specific head as proposed in Figure 3. In this case, we only perform finetuning of this head on . Apart from the domain-specific head, the rest of the components of the network are kept frozen.\nBy introducing the domain-specific head and training it for 5 epochs, we see a considerable performance on without any performance decrement on . BDDIDD indicates that we use the baseline model trained on BDD100K with the specified method. While reporting for IDDBDD, we only change the domain-specific head, the rest of the network stays the same. The same model can achieve an mAP of 24.3% on IDD and 45.7% on BDD. Results are shown in Table 3\nDiscriminative finetuning and Gradual unfreezing We use the same network and weights as in the previous step. Here, we introduce both techniques. In Table 3, active components denote those components of the network whose weights are being updated during the training process. We experiment with different learning rates with progressive addition of active components with different bounds of learning rate during each step.\n|and||Epoch||Active components (with LR)||LR Range||mAP (%) at specified epochs|\n|BDDIDD||5||+ROI Head(1e-3)||1e-3, 6e-3||24.3|\n|BDDIDD||5,9||+RPN (1e-4)||1e-4, 6e-4||24.7, 24.9|\n|IDDBDD||Eval||+ROI head (1e-3)||-||45.3, 45.0|\n|BDDIDD||1,5,6,7||+RPN (1e-4)||1e-4, 6e-3||24.3, 24.9, 24.9, 25.0|\n|IDDBDD||Eval||+ROI head (1e-3)||-||45.7, 44.8, 44.7, 44.7|\n|BDDIDD||1,5,10||+ROI head (1e-3)||1e-4, 6e-3||24.9, 25.4, 25.9|\n|IDDBDD||Eval||+RPN (4e-4) +FPN(2e-4)||-||45.2, 43.9, 43.3|\nLearning rate plays a very crucial role in determining the performance increment on and retention of learned information. Domain-specific components require a higher learning rate as compared to shared components. The learning rate range plays a crucial role since it determines the rate at which weights change in all active components. In our experiments, we found this range 0.0001-0.006 for the learning rate to work well. As experimental results show, with a little decrement in performance on , our model retains near similar performance on after being trained on . In some cases, we saw an increment in performance on while maintaining the same performance on . This shows that these transfer learning techniques complement each other and are effective in inhibiting information loss while adapting to diverse target distributions.\nIn this paper, we use an incremental learning approach and demonstrate the effectiveness of our method on data obtained from unconstrained environments. The main motivation behind this work is to demonstrate the effectiveness of our approach and encourage further research into building detection systems that generalize well on uncommon data distributions which are well representative of diverse real-world conditions. These proposed approaches can also be extended to other computer vision tasks as well.\nThe author would like to thank Intel AI for providing access to AI Devcloud as part of the Student ambassador program."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:03ddba57-4476-400d-b17f-b434067b327e>","<urn:uuid:b3776ee2-96a6-4ab3-984b-7b39fbb3053b>"],"error":null}
{"question":"What are the benefits of GPS fleet tracking for business operations, and how does it help prevent vehicle theft? 🚛","answer":"GPS fleet tracking offers multiple business benefits including automation of compliance processes which saves man-hours and reduces downtime. It eliminates the need for frequent phone calls and paperwork, cutting unnecessary expenses. Insurance providers often offer discounts on premiums for GPS-tracked fleets. For theft prevention, GPS tracking devices with RFID can stop unauthorized vehicle use and can be equipped with two-way SMS communication to activate alarms and disable vehicles if theft is attempted. The system also allows setting geographic boundary alarms and can track stolen vehicles until recovery, with the ability to activate engine electrical shut-off commands to stop the vehicle.","context":["Adding GPS Tracking To Your Fleet? Here’s A Guide To An Easy Setup\nManaging a transport business is no child’s play. One needs to be completely dedicated and keep a minute-by-minute check on the position of the vehicle to ensure comfortable transportation for passengers or an efficient transfer of goods. Now, manually being on constant contact with your drivers is a difficult task.\nThe situation becomes more challenging when it is hundreds of your vehicles plying on different routes. To ensure that you remain in control through such challenging times, opt for the installation of vehicle tracking technological tools on all your vehicles.\nWith the latest technological aids such as that of the dual dash cam, you will be able to supervise your transport fleet in a much more effective manner.\nTackling Real-World Problems With GPS Fleet Tracking System\nGPS comes with driver identification add-ons, speed collector, detour identifier, and other factors that will help you ensure that your vehicle is being used and maintained to your liking.\nWith a GPS tracking system, you can automate the entire compliance process, thereby saving your organization a considerable number of man-hours. This reduces the downtime required to carry out a task and thus improves the performance of your organization.\nIt may not always be feasible to keep a transport vehicle under 24-hour surveillance. With a GPS tracking system, your truck or transport lorry can be protected from vandalism or unauthorized use at the simple click of a button.\nA GPS-based tracking system does away with the need for frequent phone calls and paperwork, thereby saving you from unnecessary expenses. Several insurance service providers offer a discount on the insurance premiums of vehicle fleets that are GPS tracked.\nInstalling A GPS-Based Tracking System In Your Vehicle\nNow that you are aware of the advantages of having GPS tracking to your fleet of vehicles, the next major step is finding out how you can go about with their installation. From a broad perspective, a GPS-based tracking system may be classified as Hardwired, OBD-II, and Portable systems.\nHardwired Tracking System\nThe GPS tracking devices that derives its power from the vehicle itself are the hardwired tracking systems. Ideally, the device should be in a place where it can function adequately yet remain hidden from the prying eyes of miscreants. The GPS tracker should not be covered by a metallic surface, as GPS signals cannot penetrate through them.\nMost business owners prefer placing GPS devices below the dashboard. While this does an excellent job at concealing, it may not be feasible for all types of devices such as video recorders or any form of truck camera.\nOnce you have figured out the placement of the hardwired device, make sure to use a zip-tie to securely tie the transceiver and protect it from falling off during bumpy rides.\nConnect the red wire on the GPS tracking tool to the 12volt DC power source. The white wire needs to be connected to the ignition or accessory wire of your vehicle. The last step of the installation process entails linking the ground wire of your GPS device to the ground wire of the truck.\nA vehicle has multiple points from where you can draw the power for your GPS tracking device. However, try to avoid taking power from your vehicle radio. The ignition column is the cleanest power source as this receives constant ignition.\nAlso, take special care to ensure that during setup, any of the three wires should not get attached to the airbag. This will compromise your overall vehicular safety.\nThe OBD-II port is a diagnostic port that is found in all vehicles manufactured after 1996. Today, several GPS-based tracking devices come in the OBD-II format wherein you just have to locate the port (it is usually on the left near the door on the left of the clutch) and push the tracking device in the port.\nThis is a great source of power for the tracking device, and its placement is efficient in hiding your tracking device from common sight.\nPortable Tracking System\nPortable GPS-based vehicle tracking systems are becoming increasingly popular. These come with a dedicated power source and do not depend on the vehicle for their functioning or maintenance. The process of installation and setup is much simpler here.\nThere are a host of portable GPS-based tracking devices, and most of these come with a dedicated magnet-resistant case that is made of a highly endurable material. Start the setup process by placing the batteries in their dedicated slots. Next place the GPS device in its case and turn that on. As far as possible, keep the device in a non-obvious place in the vehicle.\nTesting The GPS-Based Tracking System\nNow that you have installed the GPS-based tracking system, the next major step is to test the device to see if it is functioning in the manner it was designed to. This is a simple step, and all that you need to do is take your vehicle out for a drive. Make manual notes on your speed, stop timings, route, and other details.\nOnce you complete the trip, compare the data collected by the GPS tracking tool with your manual notes. If you identify any discrepancies, reach out to the tracking device manufacturer for further help. Chances are the data will match, which is a clear indication of the successful setup and good working condition of the device.\nAs a final step, once your drive is completed, check the position of your tracking device. If you find it to have shifted, use tape to secure it firmly.\nAt the end of the day, it is essential to realize that at the heart of any business lies the idea of meeting the needs and expectations of the customer. With GPS tracking systems enabled on their fleet of vehicles, transport businesses can set a higher benchmark in terms of punctual and efficient service. This makes customers come back to them repeatedly and enable their business to grow exponentially.\nIf you want your transport business to tread on such a successful path, then wait no further. Get a GPS tracking system installed in all vehicles in your fleet and sit back and enjoy the benefits of the same.","From large commercial fleets to just a few company vehicles GPS tracking devices can help reduce your insurance costs and reduce your company’s risk every time a fleet vehicle is out on the road.\nProtect your fleet vehicles from bad drivers\nIt’s easy for a driver to say they are responsible and respectful driver but the real proof can be seen when their driving habits are being tracked.\nJack rabbit starts and hard breaking cause damage wear and tear to vehicles and can put drivers at risk of getting into an accident.\nThere are a number of GPS tracking devices like the fleetminder FM NxtG-V or the Platinum NxtG that can be installed into any vehicle along with a G-force/shock sensor that can monitor harsh acceleration and deceleration (hard braking). This data is sent along with GPS location data so you as a fleet operator know how each driver is operating their vehicle.\nSpeeding and over speed data, alerts and reports can be generated too.\nThe speed of the vehicle is also sent with its location and other data. This is typically at 3 minute intervals. To illustrate how this works:\nDriver in van #1: Date, time 11:07, GPS location point on Main Road 1, speed 70 km/h\nDriver in van #1: Date, time 11:10, next GPS location point on Main Road 1, speed 79 km/h\nDriver in van #1: Date, time 11:13, next GPS location point on Main Road 1, speed 81 km/h\nFrom the raw data, if the speed limit on Main Road 1 is 70km/h you could see that they were speeding.\nNow whilst the above raw data is what is sent to the Live Tracking system every 3 minutes, you don’t need to actually view the raw DATA.\nYou simply use the features of the Live Tracking system such as reviewing a past day and time, receiving alerts or generating a report.\nOver speed alerts with GPS tracking device Live Tracking systems\nTypically you would set the over speed to 110 km/h and then you can choose to receive a SMS text or email alert for any vehicle in your fleet going over that speed.\nGenerating over speed reports\nAs the data your fleet vehicles movements are logged in the Live Tracking system, you can simply run the “Over speed” report for any vehicle in your fleet for a period of time (e.g. for the month of July). The over speed report will show all the times the fleet vehicle was travelling over the set speed.\nYou may only do this when a speeding fine is sent to you for one of your vehicles in your fleet. You can “go back in time” to view the speed of the driver using the GPS tracking device data on the Live Tracking system. For example:\nReviewing speeding on a particular date and time using a Live Tracking system\nIdentifying driver behaviour such as rough driving, harsh braking and acceleration and speeding will assist you in managing your drivers and protecting your fleet vehicles from abuse.\nReduce fleet vehicle theft risk\nGPS tracking devices and the communication that they provide can reduce the chance that your fleet vehicles will be stolen using a number of protective technologies.\nTracking devices wired with RFID can stop unauthorised users from operating the vehicle.\nAdditionally, if a theft is attempted the GPS tracking device with two-way SMS communication can activate an alarm and disable the vehicle preventing it from being taken.\nCar, van and heavy vehicle theft is a major problem in Australia, click here to view the latest national vehicle theft statistics.\nFleet vehicle recovery\nGPS tracking devices provide assistance recovering your vehicle in the event of theft.\nThe on-board tracking devices allow you to set an alarm when the vehicle leaves a specific geographic area and to track the progress of the vehicle until it can be recovered.\nAdditionally, tracking device features can activate an engine electrical shut off command and stop the vehicle from traveling any further.\nThe next time you review your fleet insurance coverage with your company’s agent you will have the tools to reduce your costs and save your company money by installing GPS tracking devices in your fleet vehicles."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:d139d0c2-1e23-4877-a28f-e05844c3f9b5>","<urn:uuid:a4b29411-c105-40f6-b4c1-a5a9800f1c7d>"],"error":null}
{"question":"I'm trying to troubleshoot my gas dryer - the machine turns on but stays cold. What's the step-by-step process to check if it's an igniter problem or something else?","answer":"For a gas dryer that's not heating, first check the igniter. If the igniter glows and goes out but doesn't ignite the gas, the gas valve solenoid is likely faulty. Gas dryers have two or more solenoid coils that open the gas valve to allow gas flow into the burner block. If the solenoids fail, the burner may operate intermittently or not at all. You can also check the flame sensor, which detects heat from the fire, but only after verifying the igniter and thermal fuse are working properly. Use a multimeter to test the flame sensor's continuity at room temperature. If any gas valve coils are defective, it's recommended to replace all of them as a set.","context":["The reasons below, which occur when the dryer does not heat up, are listed from most likely to least likely. Check or test each cause, starting with the most probable cause.\nThe thermal fuse is a safety device designed to protect the dryer from overheating. The fuse is located on the blower housing or on the dryer heating source, such as a heating element (heating element) on electric dryers or on a gas burner on gas models. The fuse must have continuity, which means that current can flow through it. If the fuse overheats, it will blow and the continuity will be lost. A multimeter can be used to check its integrity. Note that a blown thermal fuse is an indicator of limited ventilation from the dryer to the outside. Always check ventilation when replacing a blown thermal fuse.\nGas valve solenoid\nGas dryers have two or more solenoid coils. The gas valve solenoids open the gas valve to allow gas to flow into the burner block. If the solenoid fails, the burner may operate intermittently. If the burner continues to shut down, the dryer does not heat up enough and may not stop. To determine if one or more of the gas valve solenoids has broken, check the igniter. If the igniter glows and goes out, but does not ignite the gas, then the gas valve solenoid is faulty. If one or more of the gas valve coils are defective, we recommend replacing all of them in the kit.\nAn electric igniter (igniter) uses heat to ignite the gas in the burner assembly. If the igniter does not work, the gas will not ignite and the dryer will not heat up. To determine if the igniter is burnt out, use a multimeter to check its integrity and if it is broken, replace it.\nHeating element block\nThe heating element block heats the air before it enters the dryer. Over time, the heating element may burn out and the dryer will not heat up. To determine if the heating element block has burned out, use a multimeter to test for continuity and replace if broken.\nOn a gas dryer, a flame (fire) sensor detects heat from a fire. If the flame sensor does not work, the dryer will not heat up. Before checking the flame sensor, first make sure that the igniter and thermal fuse are not causing damage. To determine if the flame sensor is defective, use a multimeter to check its integrity at room temperature. If the flame sensor is not continuous, replace it. Br> Heating element\nThe heating element (heating element) heats the air before it enters the dryer. The heating element can burn out over time. If the heating element does not work, the dryer will not heat up. To determine if the heating element has burned out, use a multimeter to check for continuity and if it is broken, replace it.\nIncoming power problem\nIf your dryer does not heat up, you may have a problem with the power input,. It often happens that only one fuse is turned off, which leads to the fact that the dryer can work, but does not heat up. Check the fuse box or circuit breaker or measure the output voltage with a multimeter.\nHigh limit thermostat\nHigh limit thermostat monitors the dryer temperature and shuts down the burner if the dryer overheats. If the high limit thermostat is faulty, it can shut off the burner even if the dryer does not overheat. However, this is rare. Before replacing the high limit thermostat, check for increasingly frequent breakage parts. If you have determined that all other components are working correctly, then check the thermostat with a multimeter for continuity and if it is broken, replace it.\nThermostat with a cycle\nThe thermostat with a cycle periodically turns the heating on and off to regulate the air temperature,. If the thermostat is defective, the dryer will not heat up. However, this is rare. Before replacing the thermostat, check for increasingly frequent breakage parts. If you determine that all other components are working correctly, then check the thermostat with a multimeter for continuity and if it is broken, replace it.\nMain control panel\nThe main control board (control panel) may be faulty. However, this is rare. Before replacing the main control board, check for more and more breakage parts. If you have determined that all other components are working correctly, then replace the main control board. (The control panel is difficult to test, but you can try to check it for signs of burnout or short circuit.)\nЕсли сушилка не нагревается, то может быть неисправен таймер. Однако это бывает очень редко. Перед заменой таймера проверьте все более чаще ломающиеся детали. Если же Вы решите, что все остальные компоненты работают правильно, то проверьте таймер с помощью мультиметра и ознакомьтесь с электрической схемой. Если таймер неисправен, то замените его. If you need help with high end dryer repair, call us right now!"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:75982d78-b3fa-400e-a9b9-e1fcd981f560>"],"error":null}
{"question":"What are the key steps involved in preparing seed samples for conservation in a plant genebank?","answer":"The key steps in preparing seed samples for conservation include cleaning, drying, moisture content determination, and packing. After preparation, the seeds' quality is tested to ensure they are viable and free of pests and diseases.","context":["Plant Genebanking: Investing Seeds for the Future\nFor many years, the agricultural sector has worked on the continuous development of sustainable practices to provide sufficient food and medicine supply for a growing population. Among the many challenges they aim to resolve are the issues inflicted by plant disease outbreaks and upsurge, pests, and climate change. The conservation and increase of diversity of plant species are recognized feasible solutions to attain sustainability—and this is where genebank plays a part.\nA plant genebank is a type of biorepository that preserves genetic materials from various plants. The banked seedlings, cells, tissues, or other forms that contain genetic information are used by researchers, breeders, and farmers alike for the research and development of crops and medicines. As global climate changes, this approach is vital for plant varieties to withstand unprecedented weather and natural disasters.\nA Glance at Genebanking Process\nAfter the sample acquisition, a unique identification number is assigned to every material. This allows genebanks to properly manage and document their samples the moment they enter the process up to the time they are distributed.\nThe samples are prepared for conservation wherein methods differ per sample type. Seeds undergo cleaning, drying, moisture content determination, and packing. As for plant materials to be used in tissue culture or cryopreservation, extraction and disinfection are done.\nThe seeds’ and plant materials’ quality are tested to ensure that they are viable and free of pests and diseases.\nThere are two types, the in situ and ex situ. In in situ, germplasms are conserved and maintained in their natural habitat. This type is not deemed to be the best option. On the other hand, ex situ conservation offers an efficient and effective solution. Materials are placed under artificial conditions in a controlled environment such as cold storage. Seeds are stored at -18°C to -20°C to maintain viability and specially prepared in vitro culture samples are stored long-term at -196°C, usually in liquid nitrogen.\nThe expression of highly heritable characters ranging from morphological or agronomical features to seed proteins or molecular markers in plant germplasm is determined. This is done by growing a representative number of plants in the field.\nThis is done to increase the number of initial samples and replenish stocks. This process is very tedious as it requires careful adherence to special requirements to prevent loss of genetic integrity.\nThe preserved samples are made available for germplasm users, breeders, researchers, and farmers.\nThe seeds are investments that are soon to bear the fruits of labor. As the global demands for genebanking continue to increase, this calls for the utilization of engineering controls that are up-to-date and efficient. Taking part in guarding diversity, Esco Scientific innovates its products to help combat undernutrition, discover novel drugs and climate-proof crops, and ultimately, supply food to every table.\nA Horizontal Laminar Flow Cabinet is used to provide sample protection in preparing explants and media for tissue culture.Learn More Request for Quote\n Genebank Procedures Overview. (n.d.). https://cropgenebank.sgrp.cgiar.org/\n Guardians of Diversity: The Network of Genebanks Helping to Feed the World. (2019, November 07). CGIAR. https://www.cgiar.org/news-events/news/guardians-of-diversity-the-network-of-genebanks-helping-to-feed-the-world/"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:0647747c-d18e-447c-81b7-a6d9bf39a397>"],"error":null}
{"question":"What are the primary methods of HIV transmission, and what is the effectiveness of PEP treatment for prevention?","answer":"HIV can be transmitted through unprotected sexual intercourse and contaminated hypodermic or tattoo needles. For prevention, PEP (Post-Exposure Prophylaxis) can reduce the rate of infection by 79% in healthcare workers exposed to HIV, though it's not 100% effective. The treatment involves taking 2-3 anti-HIV medications daily for one month, but may have side effects like nausea, headaches, fatigue, vomiting, and diarrhea.","context":["Immunodeficiency problems take place while one or more components of your immune machine are lacking or while the gadget isn’t always running as presupposed to be. Defects in one’s immune system may be congenital or acquired. Either manner, having issues with the immune machine permits harmful microorganisms to effortlessly penetrate the body’s defenses and make anybody suffer diverse diseases.\nCongenital Or Primary Type\nIf you are born with deficiencies on your immune machine, you are said to be affected by number one immunodeficiency. If you’ve got the primary form of this sickness, you could now not be able to enjoy lifestyles like different people do. Your predisposition to agreement all kinds of contagious diseases may begin in your formative years days or it can no longer make its presence known till later in life. The following are two of the maximum common examples of inborn immunodeficiency disorder.\nIgA refers to your Immunoglobulin A which is basically a group of antibodies which might be especially located to your breathing and gastrointestinal tracts. These antibodies are largely within the form of fluids secreted within the regions referred to and are often described as your body’s first line of protection towards harmful invaders. You also can locate these antibodies to your saliva and the tears for your eyes.\nFor people who have IgA deficiency, the body isn’t producing ok amounts of this particular magnificence of antibodies. Meaning, all harmful microorganisms may be allowed to go into your frame with much less or no opposition at all. Having this disease will make you susceptible to maximum hypersensitive reactions, bloodless viruses, and other infections in the breathing tract.\nSevere Combined Immunodeficiency\nAlso known as SCID, this immune device disorder is defined through a intense disorder inside the body’s production of T-cells. Your T-cells, essentially, can be thought of as the Marines of your complete protection machine; T-cells do not simply provide a protective wall but they are the elite fighting units that actually attack unwanted intruders for your frame. Now, you could simply imagine what’s going to happen if your body has only a few of those infantrymen; it will likely be completely not possible in your frame to hold off infections.\nA conventional case of SCID has been skilled by using a younger boy in Texas who attempted to live inner a sterilized plastic bubble to save you microorganisms from getting interior his frame. This boy even became a stuff of medical legend and has been referred to as the Bubble Boy, which afterward paved the manner for this disease to be referred to as the bubble boy disease.\nAcquired Or Secondary Type\nMost are born without any defects of their immune gadget. However, elements like malnutrition, severe infections or nasty side-effects from a few medicine may additionally motive problems in some human beings that can weaken the body’s immune machine. This form of immunodeficiency may also consist of the subsequent.\nEverybody has heard about the Human Immunodeficiency Virus or HIV and the Acquired Immunodeficiency Syndrome or AIDS. Slowly however really, this disease leads to the overall breakdown of the immune device. The problem is caused by a virulent disease which could annihilate the range of T-cells, in particular the helper cells, in the frame. Without the T-helper cells, your body will no longer be capable of shield itself from all styles of contamination and other dangerous organisms. You can contract HIV from careless or unprotected sexual intercourse and from using hypodermic or tattoo needles which can be contaminated with the virus","HIV PEP(POST-EXPOSURE PROPHYLAXIS) Dr. Dino Sgarabotto Malattie Infettive e Tropicali Azienda Ospedaliera di Padova\nWHAT IS HIV- PEP?• HIV is the human immunodeficiency virus. It is the virus that can lead to acquired immune deficiency syndrome, or AIDS.• PEP involves taking anti-HIV drugs as soon as possible after having been exposed• To be effective, PEP must begin within 72 hours after exposure, before the virus has the time rapidly replicate in your body• PEP consists of 2-3 anti-HIV medications taken every day for 1 month\nPEP means Post-Exposure Prophylaxis• Prophylaxis means disease prevention• PEP involves taking anti-HIV drugs as soon as possible after you may have been exposed to HIV to try to reduce the chance of becoming HIV positive; occupational PEP (health-care setting) and non-occupational PEP (condom breakage, sexual assault, etc.)• The medications may have side effects that can make it difficult to finish the program. PEP is not 100% effective; it does not guarantee that someone exposed to HIV will not become infected with HIV.\nWho needs PEP?• PEP is usually used for anyone who may have been exposed to HIV.• Healthcare workers have the greatest risk. They can be exposed to HIV by: – Needle sticks or cuts – Getting blood or other body fluids in their eyes or mouth – Getting blood or other body fluids on their skin when it is scraped, or affected by dermatitis• The risk of HIV transmission in these ways is low—less than 1% for each exposure\nWhy can’t PEP therapy be taken after72 hours from the point of exposure?• HIV grows faster and faster once it enters your body.• If you start taking PEP more than 72 hours after exposure, the meds can’t keep up, and research has shown that PEP has little or no effect in preventing HIV infection after the 72-hour mark.• Better if PEP starts within 6 hours from injury\nDifferent kind of exposure• Exposure to a large amount of blood.• Blood came in contact with cuts or open sores on the skin.• Blood was visible on a needle that stuck someone.• Exposure to blood from someone who has a high viral load (a large amount of virus in the blood).• For serious exposures, it is recommended using a combination of three drugs for four weeks. For less serious exposure, the guidelines recommend treatment with two drugs for four weeks: AZT and 3TC\nTwo drug regimens• AZT and 3TC: Zidovudine 300 mg BID + Lamivudine 150 mg BID or Combivir 1 Tab BID +• FTC and TDF: Emtricitabine 200 mg OD + Tenofovir 300 mg OD or Truvada 1 Tab OD\nThree drug regimens• Combivir BID or Truvada OD• Associated with: – Lopinavir 400 mg and Ritonavir 100 mg BID: Kaletra 2 tabs BID or – Atazanavir 300 mg OD + Ritonavir 100 mg OD: Reyataz 300 mg OD and Norvir 100 mg OD\nCombivir/Kaletra +BD: twice a day regimen; 6 Tabs a day\nTruvada/Reyataz/Norvir OD: once a day regimen; 3 Tabs a day\nWHAT ARE THE SIDE EFFECTS?• The most common side effects from PEP medications are nausea and generally not feeling well.• Other possible side effects include headaches, fatigue, vomiting and diarrhea.\nPrevention strategies• Health care workers should assume that the blood and other body fluids from all patients are potentially infectious. They should therefore follow infection control precautions at all times.• Routinely using barriers (such as gloves and/ or goggles) when anticipating contact with blood or body fluids,• Immediately washing hands and other skin surfaces after contact with blood or body fluids, and• Carefully handling and disposing of sharp instruments during and after use.\nDisposal containers• Safety devices have been developed to help prevent needle-stick injuries. If used properly, these types of devices may reduce the risk of exposure to HIV.• Many percutaneous injuries, such as needle- sticks and cuts, are related to sharps disposal. Strategies for safer disposal, including safer design of disposal containers and placement of containers, are being developed.\nConclusions• Post-exposure prophylaxis (PEP) is the use of anti-HIV drugs as soon as possible after exposure to HIV, to prevent HIV infection. PEP can reduce the rate of infection in health care workers exposed to HIV by 79%.• The benefits of PEP for non-occupational exposure have not been proven. This use of PEP is controversial because some people fear it will encourage unsafe behaviors.\nTHANK YOU!free download from www.slideshare.net"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:c186b432-61d3-41a0-b42d-cbad4d5322a7>","<urn:uuid:6fd285a7-4fd2-4241-8579-b4517a97b0b8>"],"error":null}
{"question":"What are the economic considerations for professional musicians building specialized recording spaces for woodwinds versus guitar recordings, taking into account potential earnings and studio expenses?","answer":"For woodwind recording, a professional can charge $200-300 per day, with monthly equipment costs around $200 when financing $5,000 over 24 months, which can be covered by just one session. The studio has minimal overhead beyond equipment payments and slightly higher electric bills. While specific earning rates for classical guitar recording aren't mentioned, the setup costs are significantly lower, with microphone options ranging from $60 to $1,600 for a pair, making it more accessible for home recording. Both approaches allow musicians to avoid expensive studio rental fees and offer more flexible recording options for clients.","context":["- Band Management\n- Home Recording\n- Live Sound\n- Best Instruments\n- New Music & Video\nI’m a jazz saxophone player that doesn’t really like to play jazz all that much. I like singer-songwriters, soul music, rock bands, weird string bands, folk music, and electro pop. Cannonball Adderley, Jan Garbarek, and Ornette Coleman got me into the saxophone. Singers and songwriters got me into songs and records. But I never wanted to be a Clarence Clemons. I like being a part of a section. Most bands can’t afford to tour with a horn section, so I got into making sections happen in the studio. Enough people in my widening circle of musical colleagues (most recently The Mountain Goats, Hiss Golden Messenger, Bhi Bhiman) wanted me to put together woodwind parts for their records that I thought it would be worth the investment to build a studio where I focused on just that. So that’s what I did.\nMy wife and I bought a house in downtown Raleigh, NC two years ago that had a detached workshop in the backyard, and I renovated the crap out of it. I ran super clean power to it, installed a powerful mini-split heat pump/AC system, insulated it with dense rockwool, replaced the window and door, installed tongue and groove flooring on the vaulted ceiling, created a slatted wall to give the room some acoustical dimension, and finished the existing yellow pine floors. To give it a more cabin-like vibe, I built a small front porch with an overhang. The total project took about six months, though it probably could have gone faster if we hadn’t had a baby in the middle of that, and we didn’t exceed our budget of $10,000 by all that much.\nWhy is this an economically viable route to take, you ask? Normally, if a woodwinds player gets called to do a session in a studio they would get paid a rate either based on the amount of time spent or by the song. If I’m in a professional studio for a full day, I usually get between $200-300. A totally fair rate. The idea behind my studio is that I could charge similar rates for my services as an instrumentalist, but the artist/producer wouldn’t have to be spending their valuable studio time for my portion of the project. I don’t feel like I’m pulling money away from established studios because, in a lot of cases, an artist would either farm it out to someone like me or just not have horns/ woodwinds at all. With my arrangement, people can still have affordable, orchestrated instruments without impacting their budget too much.\nAre you wondering about quality, now? Are you skeptical as to whether or not I can produce studio quality recordings in my shed? Well, I was worried about that too. Obviously, the quality of the recordings matters a lot. I didn’t want people to be able to tell that I wasn’t in a million dollar studio. My approach to achieving this was simple: after playing in some amazing studios I figured out which sound I liked the best, made a note of what the signal chain was (mic, preamp, converter), and then bought that stuff.\nNow, this equipment gets expensive. I figured I’d start out with a few choice things and go from there. I like ribbon mics on my saxes, so I have a decent AEA ribbon mic, paired up with a Great River preamp, and sent into an Apogee interface with decent converters. Those items cost me roughly $3,800. I steer the ship with Pro Tools and a souped-up Mac Mini. And it all works great. I recommend also having a good dynamic mic, like an SM7 or an RE20, and a midrange large-diaphragm condenser mic like a Mojave MA-300. As far as preamps, start a 500 series rack. Great quality stuff that you can add on to later at a reasonable cost. Not everyone is willing to invest in quality home studio gear, so if you do, you might even be able to make some money renting it out.\nI buy pretty much everything on an online store’s credit card, and keep an eye out for zero interest specials. The minimum payments will have the balance paid off in the zero- interest time frame. Financing $5,000 over 24 months is only around 200 bucks a month, which can be covered with just one session. That’s how I deal with equipment expenses.\nThe renovation expenses were a combination of gifted money, savings, and credit cards. All of which are paid off now. Other than my monthly credit card bill for equipment and maybe a slightly inflated electric bill, the studio doesn’t really have any overhead.\nThough there’s an economic reason for having this studio, my priority is having a space where I can be creative on my own terms. This is not a luxury item for a full-time musician. It is a necessity. When I’m not working on a woodwind part for someone, I can write, practice, and experiment. The more I’m able to do those things for myself, the better I’ll be at producing great work for other people. Creativity and consistency are the greatest assets for an instrumentalist and session musician. But if you can also have a kick-ass little studio in your backyard, all the better.\nABOUT THE AUTHOR\nMatt Douglas is a trained jazz saxophone and woodwinds player who floats between the jazz world and the world of singer/songwriters. He continues to live and work in Raleigh, North Carolina as a multi-instrumentalist, teacher, and songwriter. For more info visit www.mattdouglasmusic.com.","Those of you who read the blog frequently may recall the fabulous Rick Alexander – if not, or you’re relatively new around these parts check out these posts:\n* From Nylon to Steel and Back (a fantastic guest post that Rick wrote for the blog early last year)\nWell, Rick has been kind enough to share his knowledge and experience of recording the classical guitar – how to get set up, gear and so on. And so much knowledge does he have that we have a little mini-series for you on the subject, and so for today and the next couple of Monday morning (AEST) posts I hand you over to Rick!!\nHere you go folks. Thanks Rick!\nRecording Classical Guitar: Part 1 – A Recording Setup – Plus Microphones\nIt’s now easier and cheaper than ever to make good quality recordings of your playing. In this series I’ve put together some tips from my own experience of home recording over the past 18 years.\nWhy record yourself?\nIt’s a lot of fun: You get to hear back the results of the practice you’ve put in. Our guitars and our playing vary over time. It’s great fun and very satisfying to capture your guitar sounding its best with your best playing.\nRecording yourself can help you hear more clearly how you’re playing. Sometimes when you’re working hard on a piece you don’t hear it as others do. Listening back to a recording after a gap of a few days can help you hear problems in your playing. And help you hear what’s working well.\nIf you write your own music you can record as a piece of music develops over time.\nIt’s a way to share your playing. This is one of the most satisfying aspects for me.\nA recording setup for classical guitar\nFigure 1 shows a typical setup for recording solo classical guitar. The main components are:\n- Two condenser microphones for stereo recording (plus mic stands).\n- A USB digital audio interface to digitize the analog signal from the microphones.\n- A computer or tablet to record to.\n- Recording software on the computer or tablet.\n- Plus you’ll need powered speakers or headphones to play back on. (Connected either to the computer or to the audio interface.)\nFigure 1 – Microphone Setup\nWhich factors affect the sound quality of your recording the most?\nThere’s a simple answer: the microphones and the microphone positioning have the largest effect. Next would be the room you’re recording in followed by the audio interface.\nUnfortunately it turns out that, like guitars, all microphones are not created equal! Generally, the more you pay the better the sound quality.\nDynamic vs Condenser Microphones\nThere are two main types of microphone: Dynamic and Condenser. While the dynamic type is more rugged the condenser type reproduces high frequencies better. This better frequency response results in a more realistic sounding recording. The small diaphragm condenser type is generally considered the best for instrumental recording. http://homerecording.about.com/od/microphones101/a/mic_types.htm\nA popular dynamic mic is a Shure SM57. US $99. http://www.sweetwater.com/store/detail/SM57\nExamples of small diaphragm condenser mics are:\nBehringer C-2: US$60 for a pair. http://www.sweetwater.com/store/detail/C2m\nRode M5: US$199 for a pair. http://www.sweetwater.com/store/detail/M5MP\nRode NT5: US $429 for a pair. http://www.sweetwater.com/store/detail/NT5\nShure SM81: US$700 for a pair. http://www.sweetwater.com/store/detail/SM81\nand Neumann KM184. US $1600 for a pair. http://www.sweetwater.com/store/detail/KM184Pair\nWhen buying a microphone ideally try to find a store which will let you try their microphones. In fact I’d recommend asking if you can take your guitar along to the store and make some test recordings.\nI think you’ll find that the more expensive mics do generally sound better but very acceptable results can be had from mics in the few hundred dollar range per mic.\nFor my recent CD “Fine Light” I used Shure SM81 mics for ten tracks recorded at a recording studio and Neumann KM 184 mics for six tracks recorded at home. I haven’t tried the Rode or Behringer mics I’ve listed here but you’ll find positive reviews on the web. E.g. http://www.soundonsound.com/sos/feb14/articles/rode-m5.htm\nMicrophone Polar Pattern\nMicrophones vary in their sensitivity versus the angle of the sound source from the direction the microphone is pointing. This is called the microphone’s “polar pattern”. The most common polar pattern for microphones used for instrumental recording is the “cardioid” pattern for which the microphone is most sensitive straight ahead and sensitivity falls off to zero directly behind. The microphones I’ve listed above all have a cardioid polar pattern.\nIt’s an intrinsic feature of microphones with a cardioid polar pattern that their bass response is increased when the microphone is closer to the sound source. This is called the “proximity effect”. I’ve found that I need to position the mics maybe 25 to 35 cm from the guitar to get a natural sounding bass response.\nNow that you have some microphones the question is how to position them in order to get the best sound quality. In the next post I’ll talk about mic positioning."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:dbe93342-eeba-4208-a040-99c40699f8d9>","<urn:uuid:cc14a3b9-4523-4210-8c0f-cd373d6f338a>"],"error":null}
{"question":"What are the creative elements needed for documentary production, and what financial considerations should be kept in mind for distribution? 🎬","answer":"The creative elements include having a clear vision through proper brainstorming and outlining, maximizing equipment potential through resourcefulness and imagination, and blending technical skills with creative storytelling. Filmmakers need to organize shoots according to their sequence guide and manage lighting, sound, and other production needs effectively. Regarding financial considerations for distribution, filmmakers can save money by distributing their film online, which can be done virtually for free with the right approach. They should also be strategic about film festival submissions, as festivals can charge large entry fees whether or not the film is selected, and attendance costs can be significant for out-of-town events.","context":["Shooting documentaries is becoming a more accessible endeavor to both professional and amateur filmmakers.\nThe history of cinema continues to write itself as the art of motion picture develops further. The definition of a digital camer has evolved through the years. From the late 1990’s to the early 2000’s, it used to pertain to Digital8, Hi8, and MiniDV cameras (as compared to the obsolete Video8 cameras before them), which were all tape-based camera equipment producing standard definition (SD) videos. A few years after came the rise of HD cameras providing high definition (HD) footage in widescreen format.\nKnow Your Camera\nYour digital camera is your eye and brain in the filming process. It sees, it captures, it processes. Yet, it is not exactly like your own eyes and brain. It’s still a tool. It produces moving images according to specifications you give. It can provide you with the best clips only if you know how to operate it well. You still decide the kind of shots, the length of the shots, the movement of the shots, among other things, and relay all these according to the parameters of the camera.\nIf you know how your camera works more than just knowing where the buttons are, you can get the right shots in every given situation. Since documentary filmmaking requires shooting a great number of video clips and sudden filming even in the most unexpected moments, it is best to choose a light but dependable digital camera that can be easily moved around. Having extra batteries and storage space, and also handy light source and microphone, are vital every shooting day.\nBrainstorm and Make an Outline or Sequence Guide\nAs a documentary filmmaker, you don’t follow an exact script in the same way as a narrative. Yet, it doesn’t mean you have to go shoot your subject without any plan or concept. A documentary can easily go the wrong route if you don’t have a clear vision and effective planning. The workflow can become too cluttered and it would be very challenging to edit a solid documentary film.\nBefore starting with the project, visualize and brainstorm. Know what your intentions are and how to fulfill them through the documentary film medium. Make an outline (also called a sequence guide in production terms) to help you focus on the theme, goals, motivations, subjects, conflicts, and concerns you have for the film. In so doing, you are able to utilize your available tools, primarily your camera, by knowing what kind of shots you should have, which subjects should be interviewed, and what questions should be addressed during the shoot.\nIt can also provide you with a better vision of your future output, just like how a script and a storyboard can help you understand the direction your documentary leads to. You can readily revise and develop your ideas further so that when you’re at the shooting field, you clearly know what the film needs and you don’t mislead yourself in doing shots that aren’t really valuable to your story.\nLog and Record Important Details\nLog and make important notes during the shoot. Having shotlists and reviewing every batch of footage you get help you mount your documentary more effectively. If your post-production workflow asks for it (so editing can be faster), record the time codes and shot remarks as well. Specific settings, filters, and other accessories or equipment used are ideally recorded as well. Remember, being organized and keeping information handy have great benefits, especially when editing hours of footage.\nGet a Good Producer or Be a Good Producer\nMaking a documentary entails not only great creative and technical skills but also great producing skills. This is especially important to independent filmmakers who also work as the producers of their works. Some filmmakers prefer to get other people whom they know are capable of doing the job, but for anyone working with a very conservative budget, it may be most practical to be your own film’s producer.\nAs a producer, you are responsible for confirming interviews with your subjects, getting prospective contacts, organizing the logistic aspects of production, preparing release forms and contracts, among other things. If everything is well-organized, the chance of success when shooting is much higher as everything planned can be accomplished well. Each shooting day can go smoothly and successfully if things are under control and backup plans are always made available.\nMaximize the Potential of Your Equipment\nApart from the technical skills needed in making a documentary, shooting digitally also requires resourcefulness and imagination. The creative side should blend well with the technical side in order to maximize the potential of your camera in every shoot. Being a storyteller, if you know all of the features of your camera, you can maximize this key filmmaking tool for all the footage you need as you apply your creative and innovative sides.\nAccording to your sequence guide, list down all equipment you need from the camera to the lighting to the grip requirements. It is best to practically organize your shoot so you just need to bring additional equipment and accessories at specific times (like scheduling the shoot for all those with tripods and big lights on specific day/s so you don’t have to bring them when you just shoot handheld shots and you just need natural or available light in your location).\nAside from issues on picking the right format (HD or SD), aspect ratio (regular TV size or widescreen), running time of the documentary and other technical specifications, you should also be aware of how much storage space you need to accommodate all your filming requirements. You should clearly know your lighting, sound, and other production needs. Make a checklist to ensure you have everything prepared ahead.\nUnlike some narrative films that only use sound as guide and the final sound elements are produced through ADR (automatic dialogue replacement), or those that record live sound tracks independently from the camera, in documentary filmmaking, live sound is almost always crucial. Usually, the most practical way to capture the best audio for a documentary is by directly patching an external microphone (like boom, lapel, or shotgun mic) to the camera. It is not ideal to use the internal microphone of a camera since such kind of mic captures sound in all directions, including the noise around. It is also not practical to have a separate sound track from the video footage as this entails the need for a clapperboard in every single shot. With the hours and hours of footage you have, it is a very daunting task to synch the videos with the sound. Generally, the authenticity of quality live sound for documentary works is out of the question. So if you can, it is still best to record your sound elements with your visuals.","Documentary filmmaking can be quite expensive: budgets can reach hundreds of thousands of dollars or more for a feature-length documentary. If you’re interested in making a low budget documentary film, especially if you’ve never made one before, you’ll need to spend wisely and count every penny.\n10 Low Budget Documentary Film Tips\nDon’t start until you’re really and truly ready to make a documentary film\nThis first tip might be a bit of a buzzkill but the truth is, making a documentary film is extremely hard work and it’s important to know what you’re getting into. Many filmmakers could have saved a huge amount of money if they planned a bit more before they even picked up a camera. If this is your first documentary film, we recommend picking up a good book about documentary filmmaking which will help you plan out your journey ahead.\nIn addition, if you’re paying at least some crew or renting gear, you can save money by doubling up multiple things when you schedule shoot days– i.e. on the same day that you interview a film subject, also get some b-roll of them in their everyday life so you don’t have to get it later, if there’s even a chance you might need it.\nShoot your film with a cheap documentary film camera\nOne of the most expensive things for first-time documentary filmmakers to purchase outright is a good video camera for documentaries. Having your own documentary film camera is quite useful because you never know quite when a potential shoot could come up.\nTo help you find a great doc camera on a budget, we’ve put together a list of some of the best cheap documentary film cameras.\nApply for grants & try a Kickstarter campaign\nFamed hockey player Wayne Gretzky once said you miss 100% of the shots you don’t take. In the same way, you miss out on 100% of the documentary film grants that you don’t apply for. Many of these are extremely competitive, but if you focus on smaller grants, non-documentary specific grants aligned particularly well with the topic of your documentary, or general arts grants for artists in your city or state, you may have a better shot. And of course, you can always try a Kickstarter campaign to raise additional funds for your documentary.\nIt may feel uncomfortable asking for money but take solace in the fact that for crowdfunding campaigns you can let your footage speak for you to a certain extent. If you craft a dynamite fundraising trailer with all your best footage and a compelling or unique premise, you have decent odds of success. Just make sure you lay the groundwork by prepping your potential donors months in advance and reading up on the best way to conduct a crowd-funding campaign.\nEnlist the help of your friends and/or do skill trades for crew\nPaying a documentary film crew can get expensive but if you’re able to interest your friends/classmates/colleagues in your project you can definitely save some money. Just keep in mind, you often get what you pay for. So if you have a critical shoot or event you need covered properly or a very important interview to shoot, you may want to hire a pro to help shoot it or capture audio.\nYou can also do skill trading with others– working on their project in exchange for them working on yours.\nEdit your film yourself– or at least get started on it\nProfessional documentary film editors with many credits under their belt can charge $5,000+ a week and it can take many weeks to edit a film, meaning it’s easy to rack up a huge bill. Often documentaries can get stalled in the post-production phase when they run out of money to pay their editor.\nYou can save huge amounts of money if you’re hiring an outside editor by organizing your footage well and prepping it for editing. We’re talking about putting each shoot in its own folder and making sure your footage is converted if it needs to be (all in the same edit-friendly format) and ready to be edited before you hand it off to your editor.\nYou can, of course, save even more money by editing your documentary yourself. NLEs (non linear editing programs– like Final Cut Pro and Adobe Premiere Pro) are cheaper and easier to use than ever before. And you can find online training tools as well to help you start.\nUse stock music instead of a composer\nComposers for a documentary film can charge $10,000 and up to score your doc. For a low budget documentary that’s often out of the question. But that doesn’t mean you can’t have great music– using stock music from a website like Audioblocks, which charges an annual subscription fee for $99/year allows you to download as many royalty free music tracks as you want to use in your documentary film. Pond5 offers an even larger selection of music and sound effects (though it’s pay per individual track used).\nSave money by using public domain archival footage & photos\nOld archival film footage is another one of those costs that can cost thousands of dollars for even a few seconds. You can save money by using public domain films from places like Archive.org, or CriticalPast.com, which charges a fee but provides HD footage of old public domain films on a variety of topics whose copyright has expired.\nOld photos whose copyrights have expired are also fair game for documentary filmmakers. And so are photographs and videos/film footage shot by US federal government employees as part of their job (which aren’t eligible for copyright in the first place). Pond5 has a great public domain photograph collection which it offers for free.\nLeverage Fair Use to your advantage\nThe Fair Use clause in US copyright law means you can use even copyright material without paying for them– so long as you’re using them to critique them, or in a few other limited circumstances. For instance, if you intend to critique Hollywood’s portrayal of Hispanics, you can excerpt short bits of movies that demonstrate your point, even without paying a dollar in license fees. Check out the Fair Use for Documentary Filmmakers guide for more info on how this works.\nBe strategic and selective about film festivals\nYou might think that once your documentary film is finished, you can finally relax your budget. But the truth is, in the 21st-century film festivals can be a bit of a racket. In order to submit your film, they charge (sometimes large) entry fees whether or not you’re selected, and they can result in very little from your perspective– not to mention cost a lot to attend if they’re out of town. So if you want to send your documentary to film festivals, be strategic about your festival strategy. Check out our article specifically about film festival strategy.\nDistribute your film online for cheap\nDistributing your film online can be done virtually for free– if you do it right. Read our article about online film distribution to learn more about getting your film out there, and getting people to watch it."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:ef80dcc8-0fec-4e51-8ada-13fa56a8ffcc>","<urn:uuid:646fd444-a513-46c6-8df3-30aa396405fa>"],"error":null}
{"question":"Bonjour! Can you help me compare the manufacturing process of woodcarving and pressure gauge? I need examples of how both are assembled step by step s'il vous plaît.","answer":"Both manufacturing processes involve careful assembly steps but differ significantly. In woodcarving, the process starts with a squared block of wood, followed by drawing centerlines and patterns, drilling eye holes at 90 degrees, cutting out top and side views with a bandsaw, and then detailed carving of features like bills and neck rounding. Final steps include sanding and setting the eyes using Plasticwood or Apoxie Sculpt. For pressure gauges, manufacturing begins with forming the Bourdon tube through precision rolling and heat treatment, then creating the socket through machining. The process continues with assembling the movement mechanism (geared components), attaching the Bourdon tube to the socket through soldering/brazing/welding, adding the pointer and dial, and finally calibrating the gauge against a master gauge. The calibration must meet specific accuracy requirements ranging from 2% to 0.25% difference.","context":["Starting with a square block of head stock\ndraw in a center line and locate the top view of the pattern on the\ncenterline and placing the end of the of the bill at the very end of the\nblock. Make sure that the block is slightly wider than the widest part\nof the top view pattern.\nNext locate the eye line\non the top view and using a Square, extend the eye line\ndown the side of the block. Once this is located place\nthe side view of the pattern on the side of the block by\naligning the eye on the eye line (push pin) and rotate\nthe pattern until the end of the bill touches the\noutside edge of the block.\nThis cutout method will only be successful if you are using an accurate\npattern that features matching side and top views. You can usually check\na pattern with a T Square to see if the views match.\nDraw around both the top and side views\nof the pattern making sure to locate the eye on the side view.\nNext using a drill press I locate and\ndrill the eye hole through the entire block making sure that my drill\npress is at a right angle or 90 degrees.\nAt this point I should reiterate that\nit is crucial to make sure you start with a squared block of wood and\nyour bandsaw and drill press must be set up to cut accurately at a right\nWith everything squared up we're ready\nfor the first cut. With a band saw, cut out the top view first by making\na continuous cut from front to back as seen in the illustration below.\nNow its time to cut out the side view\nby putting the three pieces back together forming a square block again.\nI like to use a touch of hot glue to hold the block together keeping in\nmind that a very small drop of glue will do the job. Next cut out the\nhead side view with the band saw.\nSide view cutout\nDiscard the scrap and you have a head\ncutout with two views of the pattern locked in and you are ready to\nproceed to locating the carving guidelines.\nStart the guidelines by drawing a\ncenter line on the top and bottom of the cutout.\nAlso draw in the end of the bill shape\non the bottom of the bill. I like to use Vernier Calipers for\ntransferring measurements from pattern to head. The two points will give\nyou the same measurement as the distance between the large blades.\nNext, referring to your pattern, locate\nand draw in the eye channel and cheek lines on the side view of the\ncutout. The circle with an X represents the high spot of the cheek or\nthe widest part of the head which is cheek to cheek.\nNext, using the top view of the\npattern, locate and transfer the width of the crown.\nNext using the side view of the head\npattern locate and transfer the bill guidelines.\nMake special note of the “U” shape at\nthe lower corner of the bill. It is very easy to locate and transfer\nthis measurement by putting the points of the Vernier calipers on the\nend of the bill and corner of the bill on the pattern and transferring\nthis measurement to the same area on your head cutout. Repeat for the\ntop of the bill to locate the horns.\nLastly I establish the round neck\nguidelines by drawing a circle and X on the bottom of the neck. The X\nhelps to get equal parts.\nI start the carving process by locking\nin the bill. The first cut is at the corner of the bill and I use a half\nround gouge to make this cut. You could use a small cylinder cutter if\nyou are using a power carver to make this cut. The corner of the bill is\nnow the same width as the rest of the bill and the corner is locked in\nNext I lock in the horns of the bill\nwith a right angle /90 degree cut. A knife or cylinder cutter will work\nperfect in this area. Make sure that you do not cut below the horn\nguidelines on the side view of the bill otherwise you have cut away the\ntop of the bill.\nNext I continue the right angle cut\nusing the Eye Channel guideline on the side view and the Crown guideline\non the top view. This cut will lock in the width of the crown. Again it\nis very important to make sue that this is a 90 degree cut. See\nNext step is to start the neck rounding\nprocess by making a groove cut following the illustration. This can be\ndone with a round rasp, round rotary rasp, or round gouge.\nWhen making this cut remember the final\nshape of the neck is round so avoid a flat cut. Guidelines on the neck\nbottom will help visualize the proper rounding.\nNow it's time to round the neck.\nRounding the crown is the next step.\nRound from the centerline on the top of the head down to the eye\nNow you can round from the eye channel\ndown to the cheek line.\nNow you can continue the rounding\nprocess by rounding from the cheek line into the neck area. Remember the\nCircle X is the high spot of the cheek or the widest part.\nNext is rounding and shaping the bill.\nFirst draw in the shape of the bill on the bottom. The red area will be\nremoved. Make sure that you are looking at the bottom of the bill when\nmaking the cut and cut with the grain to avoid splitting the bill end.\nIt's important to note that the widest\npart of the Ducks bill is at the bottom and a warning line is located to\npreserve the bills shape.\nNow we can focus on rounding and final\nshape of the top of the bill.\nTime to sand the entire head.....It is\nnow safe to remove your guidelines being careful to just smooth the\nsurface and remove fuzz and high spots.\nOnce you are finished sanding you can\nre-draw in the bill guidelines and you are ready to set the eyes.\nUsing the pilot hole as a guide open\nthe eye hole to the proper size either using a half round gouge or an\neye hole drill. Mallard eye size is 10mm.\nMake sure you make the hole slightly\nlarger than glass eye and deep enough to allow for setting the eye to\nthe proper depth. Fill the eye socket with either Plasticwood or Apoxie\nSculpt and press in the glass eye. Eyes should be set deep enough so the\nthey are not “bug-eyed” or sticking on the outside of the head. Some of\nthe Plasticwood or Apoxie will ooze around the eye and can be smoothed\nout with a small flat brush. If you are using plastic wood use acetone\nto smooth out, and water if you are using the Apoxie.\nAt this point you can\nsand around the eye being very careful not to hit the\nglass eye as it will scratch.\nNext, it's time to delineate the bill\nguidelines. To do this you can use a variety of tools as demonstrated in\nthe pictures below...\nMake a \"V\" cut using a hook blade and\nthen sand back into the bill leaving defining the separation of bill and\nhead feathers. This will leave the feather area higher than the bill.\nThat edge can also be rounded over to the bill if you prefer a tighter\nThe same procedure can be done with\npower tools also. I prefer using a small cylinder shape diamond cutter\nfor this task. As stated above the object is to delineate the separation\nand then sand smooth back into the bill.\nSome carvers like to use a woodburner\nfor this task. Simply trace the line with the burning pen which defines\nthe two areas. This is a simple way to go and makes painting easier.\nEven simpler is to just paint in the bill without any delineation.\nRemember this is a head for a hunting decoy and the incoming waterfowl\nwill not know which method you used but they will be impressed with your\nI suggest carving several heads in\norder to solidify the carving process and learn the various steps. Once\nyou feel you have mastered the steps then you can apply the steps to\nother species. Questions regarding this demo or various tools used can\nbe answered by contacting The Duck Blind at 1-800-852-7352 or email\nClick below to continue\nMallard Body Carving\nMallard Painting Tutorial","Many of the processes in the modern world involve the measurement and control of pressurized liquid and gas systems. This monitoring reflects certain performance criteria that must be controlled to produce the desirable results of the process and insure its safe operation. Boilers, refineries, water systems, and compressed gas systems are but a few of the many applications for pressure gauges.\nThe mechanical pressure indicating instrument, or gauge, consists of an elastic pressure element; a threaded connection means called the \"socket\"; a sector and pinion gear mechanism called the \"movement\"; and the protective case, dial, and viewing lens assembly. The elastic pressure element is the member that actually displaces or moves due to the influence of pressure. When properly designed, this pressure element is both highly accurate and repeatable. The pressure element is connected to the geared \"movement\" mechanism, which in turn rotates a pointer throughout a graduated dial. It is the pointer's position relative to the graduations that the viewer uses to determine the pressure indication.\nThe most common pressure gauge design was invented by French industrialist Eugene Bourdon in 1849. It utilizes a curved tube design as the pressure sensing element. A less common pressure element design is the diaphragm or disk type, which is especially sensitive at lower pressures. This article will focus on the Bourdon tube pressure gauge.\nIn a Bourdon tube gauge, a \"C\" shaped, hollow spring tube is closed and sealed at one end. The opposite end is securely sealed and bonded to the socket, the threaded connection means. When the pressure medium (such as air, oil, or water) enters the tube through the socket, the pressure differential from the inside to the outside causes the tube to move. One can relate this movement to the uncoiling of a hose when pressurized with water, or the party whistle that uncoils when air is blown into it. The direction of this movement is determined by the curvature of the tubing, with the inside radius being slightly shorter than the outside radius. A specific amount of pressure causes the \"C\" shape to open up, or stretch, a specific distance. When the pressure is removed, the spring nature of the tube material returns the tube to its original shape and the tip to its original position relative to the socket.\nPressure gauge tubes are made of many materials, but the common design factor for these materials is the suitability for spring tempering. This tempering is a form of heat treating. It causes the metal to closely retain its original shape while allowing flexing or \"elasticity\" under load. Nearly all metals have some degree of elasticity, but spring tempering reinforces those desirable characteristics. Beryllium copper, phosphor bronze, and various alloys of steel and stainless steel all make excellent Bourdon tubes. The type of material chosen depends upon its corrosion properties with regards to the process media (water, air, oil, etc). Steel has a limited service life due to corrosion but is adequate for oil; stainless steel alloys add cost if specific corrosion resistance is not required; and beryllium copper is usually reserved for high pressure applications. Most gauges intended for general use of air, light oil, or water utilize phosphor bronze. The pressure range of the tubes is determined by the tubing wall thickness and the radius of the curvature. Instrument designers must use precise design and material selection, because exceeding the elastic limit will destroy the tube and accuracy will be lost.\nThe socket is usually made of brass, steel, or stainless steel. Lightweight gauges sometimes use aluminum, but this material has limited pressure service and is difficult to join to the Bourdon tube by soldering or brazing. Extrusions and rolled bar stock shapes are most commonly used.\nThe movement mechanism is made of glass filled polycarbonate, brass, nickel silver, or stainless steel. Whichever material is used, it must be stable and allow for a friction-free assembly. Brass and combinations of brass and polycarbonate are most popular.\nTo protect the Bourdon tube and movement, the assembly is enclosed within a case and viewing lens. A dial and pointer, which are used to provide the viewer with the pressure indication, are made from nearly all basic metals, glass, and plastics. Aluminum, brass, and steel as well as polycarbonate and polypropylene make excellent gauge cases and dials. Most lenses are made of polycarbonate or acrylic, which are in favor over glass for obvious safety reasons. For severe service applications, the case is sealed and filled with glycerine or silicone fluid. This fluid cushions the tube and movement against damage from impact and vibration.\nMaking the Bourdon tube\n- 1 The Bourdon tube is the most important part of the instrument. The tube may be made from solid bar stock by drilling the length to the desired inside diameter and turning the outside diameter on a lathe to achieve the appropriate wall thickness. However, most general purpose gauges utilize preformed tubing purchased from a metals supplier. The gauge builder specifies the desired wall thickness, material, configuration, and diameter. The supplier provides the material in 10- to 12-foot (3- to 3.65-meter) lengths, ready for production.\n- 2 Most manufacturers have closely guarded proprietary rolling methods for rolling the tubing into the \"C\" shape. The \"C\" shape of the tube is generally formed in an automatic rolling machine. This machine contains two precision, powered rollers, through which the tubing passes. One roller grasps the tubing end and forms the inside radius, while the other provides outside pressure to maintain uniform contact with the tubing. Each roller contains a groove that fits around the outside of the tubing; these grooves allow the tubing to maintain its circular shape rather than being flattened. In the rolling process, a steel mandrel—a bar that guides the tubing into the rollers and helps it keep its shape—is first inserted though the free end of the tubing and positioned just before the rollers. This lubricated mandrel is of the desired interior shape of the oval. The tubing then passes over the mandrel and between the rollers. One roller contains a clip that grabs the tubing; as the roller turns, it pulls the tubing and bends it into the \"C\" shape.\n- 3 The same roller that grabs and bends the tubing also contains a saw blade. As the roller continues turning after creating the bend, the saw blade on it cuts the tubing to the proper length. The tubing is then heat treated in ovens.\n- 4 The socket is basically a block of metal that serves as a connector to the source of the pressure medium; a mount for the case, dial, and movement; and as an attachment slot for the Bourdon tube. One end of the socket is threaded, which allows it to be screwed into the pressure-providing apparatus. The socket may be cast, forged, extruded, or machined from bar stock. Most sockets are made on automated machining centers that turn, drill, mill, and thread all in one cycle. General machining practices apply to most socket manufacture.\n- 5 Movements are geared mechanisms that contain a pinion (a rotating shaft), sector, support plates, hairspring, and spacer columns. The mechanism converts the somewhat linear displacement of the Bourdon tip into rotary movement, as well as providing a means for calibration adjustment. The pointer is fastened to the rotating shaft, or pinion, and sweeps across the graduated dial indicating the pressure amount. Most movements are supplied to the gauge builder ready to use. Many types of manufacturing processes are used to produce the movement components, and the workmanship of the mechanism closely resembles a clockwork when completed.\n- 6 The case, dial, and pointer may be sheet metal stampings, plastic moldings, or castings. Stampings and moldings require little further processing, but castings will require some machining—trimming off excess material, for instance—to meet the final requirements. These components are painted as required, and the dials are printed with the appropriate artwork. Common printing practice, utilizing both offset and direct methods, is used. The lens most commonly is a plastic part made by injection molding, whereby the plastic is heated into a molten state and then poured into a mold of the desired shape. The attachment feature that secures and seals the lens to the case is designed into the mold. Glass lenses are still used, but must be retained by a ring of some type. Glass has fallen out of favor because of the safety problems of breakage.\n- 7 After the Bourdon tube is made, its closed end is attached to the socket by soldering, brazing, or welding. The free end of the Bourdon tube is precisely located during this assembly operation, and then sealed, usually by the same means used to join the tube to the socket. Once the Bourdon tube and socket assembly is secure, the tip of the unsupported end of the \"C\" is attached to an endpiece. This endpiece contains a small hole that connects the tip to the geared movement mechanism. The Bourdon tip doesn't move a great distance within its pressure range, typically .125 to .25 inch (.31 to .63 centimeter). Understandably, the greater the pressure, the farther the tip moves. The other components—the movement, pointer, and dial—are then assembled onto the socket as a group.\nCalibration occurs just before the final assembly of the gauge to the protective case and lens. The assembly consisting of the socket, tube, and movement is connected to a pressure source with a known \"master\" gauge. A \"master\" gauge is simply a high accuracy gauge of known calibration. Adjustments are made in the assembly until the new gauge reflects the same pressure readings as the master. Accuracy requirements of 2 percent difference are common, but some may be 1 percent, .5 percent, or even .25 percent. Selection of the accuracy range is solely dependant upon how important the information desired is in relationship to the control and safety of the process. Most manufacturers use a graduated dial featuring a 270 degree sweep from zero to full range. These dials can be from less than I inch (2.5 centimeters) to 3 feet (.9 meter) in diameter, with the largest typically used for extreme accuracy. By increasing the dial diameter, the circumference around the graduation line is made longer, allowing for many finely divided markings. These large gauges are usually very fragile and used for master purposes only. Masters themselves are inspected for accuracy periodically using dead weight testers, a very accurate hydraulic apparatus that is traceable to the National Bureau of Standards in the United States.\nIt is interesting to note that when the gauge manufacturing business was in its infancy, the theoretical design of the pressure element was still developing. The Bourdon tube was made with very general design parameters, because each tube was pressure tested to determine what range of service it was suitable for. One did not know exactly what pressure range was going to result from the rolling and heat treating process, so these instruments were sorted at calibration for specific application. Today, with the development of computer modeling and many decades of experience, modern Bourdon tubes are precisely rolled to specific dimensions that require little, if any, calibration. Modern calibration can be performed by computers using electronically controlled mechanical adjusters to adjust the components. This unfortunately eliminates the image of the master craftsman sitting at the calibration bench, finely tuning a delicate, watch-like movement to extreme precision. Some instrument repair shops still perform this unique work, and these beautiful pressure gauges stand as equals to the clocks and timepieces created by master craftsmen years ago.\nApplications and Future\nOnce the calibrated gauge is assembled and packaged, it is distributed to equipment manufacturers, service companies, and testing laboratories for use in many different applications. These varied applications account for the wide range in design of the case and lens enclosure. The socket may enter the case from the back, top, bottom or side. Some dials are illuminated by the luminescent inks used to print the graduations or by tiny lamps connected to an outside electrical source. Gauges intended for high pressure service usually are of \"dead front\" safety design, a case design feature that places a substantial thickness of case material between the Bourdon tube and the dial. This barrier protects the instrument viewer from gauge fragments should the Bourdon tube rupture due to excess pressure. The internal case design directs these high velocity pieces out the back of the gauge, away from the viewer. Many applications involve mounting the gauge directly to the running machinery, resulting in the need for liquid filling. Unfilled gauges quickly succumb to the destructive effects of vibration. Special mounting flanges are secured to the cases to allow for panel and surface mounting independent of the pressure plumbing. Case and lens materials are chosen to cope with a variety of abusive or contaminated environments, and are sealed by various means to keep moisture and contaminants out of the movement mechanism.\nThe use of pressure gauges in the future appears to be dependant on the quickly growing electronic sensor industry. These sensors are electronic components that provide an electrical signal and have essentially no moving parts. Many gauges today already have these sensors mounted within the case to send information to process control computers and controllers. These sensors are intrinsically safe, allowing their use in flammable or explosive environments. The whole process control issue has grown in recent years as a result of the need to prevent accidental releases of the process media, many of which are harmful to the environment. As environmental concerns grow, this interface will be in demand and the mechanical gauge may fall out of favor. However, the mechanical gauge does not require the electrical power source or the computer equipment needed by the electronic sensor. That makes the gauge cost effective for most general uses, and it is in this area that industry expects to continue to thrive.\nWhere To Learn More\nKardos, Geza, ed. Bourdon Tubes and Bourdon Tube Gauges: An Annotated Bibliography. Books on Demand, 1989.\nPressure Gauge Handbook. M. Dekker, 1985.\nArslanian, Russ. \"How to Select a Pressure Calibration Device.\" InTech. June, 1989, pp. 84-85.\nGarrett, D. Dewayne and M. C. Banta. \"A Suggested Improvement for the Fabrication of Low-Cost Manometers.\" Journal of Chemical Education. June, 1990, p. 523.\nJimenez-Dominguez, H., F. Figueroa-Lara, and S. Galindo. \"Bourdon Gauge Absolute Manometer.\" Review of Scientific Instruments. March, 1986, p. 499.\n—Douglas E. Betts"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:120050f2-c34b-452e-a2ef-23fd17a58064>","<urn:uuid:7c7759f1-1b3a-4938-9b14-91ed973214b8>"],"error":null}
{"question":"Compare the medication interaction warnings for Black Cohosh vs St John's wort - which interactions should users watch out for?","answer":"Black Cohosh should not be combined with birth control pills, hormone therapy, or tamoxifen. St John's wort may reduce the effectiveness of several medicines, including heart disease medications and hormonal contraceptives (including the Pill and contraceptive implants). In fact, there were 15 reported cases of unplanned pregnancies suspected to be linked to St John's wort interactions with hormonal contraceptives between 2000-2013.","context":["Black Cohosh, also known as Black Snakeroot or Bugbane, is a medicinal root. It is used to treat women’s hormone-related symptoms, including premenstrual syndrome (PMS), menstrual cramps, and menopausal symptoms. Black Cohosh contains potent phytochemicals that have an effect on the endocrine system. How it works is not yet clear.\nBlack Cohosh is widely used in the United States, Australia, and Germany. The German government has approved it as a prescription alternative to hormone therapy. In Canada, Black Cohosh is available without a prescription. Be sure to talk to your doctor before you take it, especially if you have liver problems or if you develop symptoms of liver problems after using Black Cohosh. Symptoms of liver damage can include being more tired than usual, feeling weak, loss of appetite, and yellowing of the skin.\nYou can buy black cohosh as a standardized extract in 20 mg pill form, which is taken twice a day. Root, extract, and tincture forms are also available in health food stores.\nWhen black cohosh is used at regular doses, its only known side effect is occasional stomach discomfort. But black cohosh may have risks that are not yet known, including possible effects on liver function. More research needs to be done before experts can recommend it for long-term use.\nIs it effective?\nStudies on black cohosh have had mixed results. Some studies have shown that black cohosh can relieve menopause symptoms such as hot flashes. But other studies have shown that black cohosh does not relieve symptoms.\nThese mixed results may mean that black cohosh can relieve symptoms in some women, but does not relieve symptoms in others. Or the different results may be because different preparations were used in the studies.\nIn the studies where black cohosh relieved symptoms, it reduced hot flashes, night sweats, and sleep problems.\nIs it safe?\nLarge, long-term studies have not yet been done to confirm whether long-term use of black cohosh is safe. Because black cohosh has benefits somewhat like estrogen therapy, it may also have some risks like those of estrogen.\nExperts do not know for sure if black cohosh causes liver problems. But they have determined that black cohosh products should be labelled with a statement of caution. Stop using black cohosh if you notice that you are weak or more tired than usual, you lose your appetite, or your skin or the whites of your eyes are yellowing. Call your doctor because these symptoms may mean you have liver damage.\nIf you plan to take black cohosh, talk to your doctor about how to take it safely. You may be able to take it short-term (no more than 6 months), or possibly longer but with regular checkups to look for estrogen-related changes in the uterus and breasts.\nEstrogen may increase the risk of cancer in women who have a history of uterine cancer or breast cancer or who are at high risk for breast cancer. Since black cohosh may work in ways similar to estrogen, these high-risk women should avoid using black cohosh until more is known about the long-term risks.\nAs with any medicine, be careful to avoid overdosing with black cohosh. Symptoms of overdose include vertigo, headache, nausea, vomiting, impaired vision, and impaired circulation.\nWhat to avoid\nBlack cohosh should not be used during pregnancy or while you are breast-feeding. Do not take black cohosh if there is any chance that you might be pregnant.\nBlack cohosh should not be combined with birth control pills, hormone therapy, or tamoxifen. It should not be used by women who are allergic to ASA.","St John's wort\nSt John's wort is a traditional herbal remedy used to relieve the symptoms of slightly low mood and mild anxiety. It is also used to relieve the symptoms of menopause, including hot flushes, night sweats, slightly low mood and anxiety.\nSt John's wort is registered by the medicines regulatory body, the MHRA.\nThe registration is based purely on traditional usage and manufacturing standards rather than clinical evidence that any herbal therapy works.\nSt John's wort may reduce the effectiveness of several medicines, including some heart disease medications and hormonal contraceptives, including the Pill and contraceptive implants.\nThe MHRA received 15 reports of unplanned pregnancies that are suspected to be linked to St John's wort and hormonal contraceptives between 2000 and 2013.\nAlways talk to your doctor about all the medications you are taking, including herbal remedies.\nWhat is St John's wort?\nSt John's wort is a wild yellow flower. It has been used for medical purposes in parts of the world for thousands of years. Today, St John's wort is continually being studied to try to validate its possible mood-improving benefits.\nMany clinical studies have been conducted over the years to evaluate the effectiveness of St John's wort. While the true benefits of St John's wort are still being explored, if you do choose to use it, be sure to learn all you can and check with your doctor before taking it.\nIs there scientific evidence that supports the use of St John's wort for depression?\nThere is some scientific evidence that St John's wort is helpful in treating mild to moderate depression. Research into whether it can help with severe depression has produced mixed results and more research is needed before firm conclusions can be made.\nHow do I take St John's wort?\nFollow the directions on the packet making sure you read the information leaflet first.\nWhat should I watch for if I take St John's wort?\nYou should be alert for any of the following effects if you are taking St John's wort:\nWhat precautions should I take with St John's wort?\nHerbal therapies are not recommended for pregnant or breastfeeding women, children, the elderly, or those with compromised immune systems. In addition, some herbs have sedative or blood-thinning qualities. Consequently, they may interact dangerously with NSAIDs or other pain medications. Others may cause gastrointestinal upset if taken in large doses.\nResearch has shown that St John's wort may reduce the effectiveness of several medicines, including birth control pills and some heart disease medications. Talk to your doctor about all the medications you are taking.\nAlways tell your doctor if you are taking St John's wort or any other herbal product.\nWhat other precautions should I take with herbal remedies?\nHere are additional precautions you need to take to increase the safety of using herbal remedies:\n- Discuss any medicines you use, including herbal products, with your doctor.\n- If you experience side effects such as nausea, vomiting, rapid heartbeat, anxiety, insomnia, diarrhoea, or skin rashes, stop taking the herbal product and seek medical advice.\n- Avoid preparations made with more than one herb.\n- Beware of commercial claims of what herbal products can do. Look for the MHRA Traditional Herbal Remedy logo on packets to ensure high quality standards."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:b21815d2-7344-43db-ba09-2fc819736837>","<urn:uuid:5cd4f6e1-b3fb-4a97-9762-b09da5a4ef0f>"],"error":null}
{"question":"Why is carbon capture so important, and how could hemp cultivation contribute to this environmental goal in the UK?","answer":"Carbon capture is crucial as trees help reduce climate change by storing CO2. Broadleaf trees are particularly effective, with UK woodland capturing 20 million tonnes of CO2 annually. In woodland areas, soils store 72% of total carbon, while tree trunks/limbs/leaves lock up 17%, roots 6%, and deadwood 5%. Hemp could be another valuable tool for carbon sequestration in the UK. The plant can store carbon in soil through its strong taproot system, which improves soil structure when it breaks down. Hemp can also sequester carbon through its fibers when used in biocomposite materials for construction or automotive industries. Additionally, hemp requires no agrochemicals and can help increase biodiversity while improving soil health.","context":["Here are Jan Flamank's Nature Notes for October 2020. Click on each photo to see a larger photo in a new page. For downloadable pdf versions of each topic see the links below.\nThe Goldcrest is our smallest native bird, found mainly in coniferous woodland and often heard more than seen, as it is only up to 3.5 inches long and weighs less than a quarter of an ounce, or 5-7 g. It is always on the move in the dense foliage, well camouflaged as it flickers round the branches, hovering and picking insects and spiders from the tree. They can also hang upside down when searching for food beneath leaves, and have very sharp, thin beaks that act like tweezers. Usually well hidden in conifers, they sometimes visit the ground to peck seeds and insects from snow covered surfaces.\nWell named, it has a bright yellow or orange crown on the top of its head, bordered in black, and this bright crown is raised into a small crest when alarmed. They have a high pitched song and alarm call.\nWhat amazes me for such a tiny bird, is that in our autumn, our resident population can increase fivefold by large numbers of Goldcrests migrating across the North Sea to spend winter here. They fly here from the Baltic, Finland and even Russia, covering over 600 miles in one week.\nThey are effective breeding birds, with 2 broods a year of up to 20 nestlings in one season. They have also benefitted from widespread conifer plantations across the UK, with the fast growing trees used in lumber and paper production. Both parents make the small spherical nest, with 3 layers for insulation, made with cobwebs, lichen, moss and lined with feathers and hair. It is so small and well concealed in the dense foliage, it is rarely predated.\nThey feed all day in autumn, building up energy for the cooler nights, huddling together in thick cover to keep warm. I would love to see that! I have seen Goldcrests in conifer woodland, but it was mostly an impression of fluttering movements as they restlessly foraged for insects.\nThe Firecrest is another tiny bird, closely related to the Goldcrest, similarly secretive, but they do sometimes interbreed. Firecrests are less numerous here, but are found mainly in the South East. They migrate from central Europe, but prefer more open mixed woodland, and may also inhabit gardens with exotic conifers. When they feed together with Goldcrests they tend to feed lower down in the trees than the Goldcrest, so leaving space for each other to hopefully find enough food. Sensible and kind, this is a lovely lesson for us all from these very small birds.\nWe tend to think of hail as winter phenomenon, as it is ice, but in fact we always have hailstorms in the warmer summer months, so why is this?\nIt is all because the big convective clouds reach their highest elevation in summer, when the surface is strongly heated by the sun, and also when they have the most moisture in them, so evaporation is at a high rate too. Both of these occur in the summer months. The higher the clouds go, away from the warm surface of the earth, the colder they become.\nCumulonimbus clouds, which produce hail, are convective clouds, formed by warmer, summer air pulling away from the surface of the earth, contrasting with the relatively much cooler air above the cloud. The clouds form and transport heat up into the atmosphere by the process of convection, and the strong updraughts of ascending air, and downdraughts, enable hail to form inside the cloud.\nThese clouds contain large water droplets and hail forms within the cloud from tiny ice crystals called graupel. The hailstones become bigger inside the cloud, due to the accumulation of supercooled water droplets as they are borne upwards on rapidly rising air. The hailstones have to build up sufficient layers of ice to be heavy enough to fall out of the cloud onto earth. They do this by moving up and down in the cloud on the water- rich updraught, adding icy layers to themselves. It is possible to count the ice layers in a large hailstone and have an idea of how many times it moved up and down in the cloud.\nSo, the next time we have a summer hailstorm, think of the tiny ice crystals becoming larger as they move up and down in the cloud, encountering very cold water droplets and making extra layers of ice, prior to making headlines in the local news. It is a summer weather event to marvel at - buy try to avoid being out in it, as they can hurt if they land on your head!\nThis is our most tiny shrew, widespread throughout the UK and surprisingly resilient given its minute size. It is only 2.5 inches long, but its long hairy tail of 1.5 inches adds to the overall length.\nAs such a small warm bloodied mammal, it has a proportionately larger surface area than bigger mammals and so loses body heat quickly. Many mammals grow longer fur to combat the cold, but this tactic would hinder the movement of such a small creature, so after the moult in autumn, its fur stays at about 3mm long all over. We don’t know why it happens or how it is controlled, but the autumn moult starts on the rump, working up towards the long nose, but the spring moult does the reverse, starting at the nose then working towards the rump. Wild indeed.\nBeing so small has some advantages, and the pygmy shrew can hide effectively in a wide range of habitats, including fissures in rocky outcrops, but prefers long grass and shrubby vegetation in woodland where they can also easily feast on their prey. They mainly eat spiders, beetles, woodlice and snails and feed almost constantly, as they have to consume 25% more than their own weight every day, just to keep alive. That is a huge amount of tucker!\nLuckily, it has a very efficient digestive system, and excellent hearing that picks up the minute sounds of moving invertebrates. Their wonderfully long nose is covered with hairs that detect movements of prey in the undergrowth. Once they pounce on a spider or woodlice they bite it with teeth that are red at the tip, due to iron oxide in their enamel. Juicy snails also provide water for them. Pygmy shrews have a very high metabolic rate, with a heart rate of 250 beats per minute, and a breathing rate of 200 breaths a minute and live a short, fast life of only up to a year in the wild, with many dying within 4 months of birth.\nSolitary creatures apart from mating, they usually have 2 litters of 4 to 7 young, born mainly April to September. They grow rapidly on the rich milk of their mother, increasing their size 10 fold in just 2 weeks. They are independent at about 3 weeks old, and then leave the nest to establish their own territory. Wet and cold weather is the biggest cause of mortality, as they impede the shrew’s ability to keep warm and find enough food.\nWe are unlikely to see live pygmy shrews, who are themselves a tasty snack for owls, but the next time you walk near woodland or long grass, imagine them rushing about, foraging and feasting on small invertebrates, resting only in very short naps of a few minutes in their hidden undergrowth homes. And the proud possessors of red tipped teeth.....\nMost of us are much more aware now of the unavoidable fact that humans are by far the most destructive species on this planet. Thankfully, we are also more aware of what we can do to change our behaviours, rebalance our relationship with the natural world and repair some of the damage we have done.\nAn important concern with regard to increasing global warming is the vital issue of carbon capture and the role of trees. As with all these complex environmental challenges, our responses need to be equally multifaceted and thought through to ensure long term benefits. We can do this.\nHigh quality research is ongoing in the UK and beyond, and the Woodland Trust and Wildlife Trusts are doing fantastic work on our responses, along with many environmentalists, Defra and other organisations. This all helps, a lot.\nWe know that trees are excellent at capturing and storing carbon, but which are the best and how can we intelligently plan ahead to reduce climate change and restore some of the vast losses of natural habitats for wildlife? It is not enough to just pledge to plant vast numbers of trees in the UK. We need to know what to plant, where to plant and to avoid the mistakes we made with huge conifer plantations. We must also stop stripping the soil of all its nutrients with chemicals and overgrazing. As I have said before, many times, if we have poor soil, we cannot have healthy plants and without plants, we will ultimately, have nothing....\nSo, a huge challenge faces us, but we have reflection, intelligence (with notable well known exceptions!) and environmental sciences to help us plan well and make a real, positive difference. Phew.\nHere are some useful facts about carbon capture from the Woodland Trust:\nBroadleaf trees are better at storing carbon than conifers, with beech trees in the top 5 for locking up destructive CO 2. This is due to their high timber density. Fast growing conifers are good for easy financial gain, but not for long term sustainability, eco -diversity and reducing climate change. Soils store a huge 72% of the total carbon capture for a wood, with tree trunks, limbs and leaves locking up 17%, tree roots 6% and deadwood 5%.UK woodland captures 20million tonnes of CO 2 annually, and we can improve on this by planting trees most effectively across our small island.\nWe may not all have the resources or space to contribute to mass tree planting, but we are all aware of the small, incrementally useful things we can do to help now:\nIf you have a garden, or community space, plant a new tree if you can. Autumn is the best time to do this, when the earth is still warm and the trees have time to establish before new growth in the springtime. Think small and native, and make a square planting hole as the roots establish more firmly than in a round one. Ensure it is well watered in the first few years of growth.\nAll the usual daily eco-helpers of reduce, reuse, recycle to avoid waste.\nDon’t fly! Leave that to our perfectly adapted feathered friends, but do put out some sustaining seeds and water for them throughout the cooler months.\nEnjoy the glorious colours of autumn as our broadleaf trees lose their leaves and prepare for a period of dormancy over winter.\nIf you get the chance, read The Hidden Life of Trees by Peter Wohlleben, an inspiring book for the longer evenings.\n© Jan Flamank October 1st 2020. All rights reserved. Images used in the documents have been sourced free for use in this social, educational, non-commercial setting.","Hemp, which is also known as cannabis, is a plant that can grow in the UK without causing damage to soil and water. It’s a cash crop, providing a source of income for the British farming industry but also a way to help the environment.\nHemp has been cultivated for thousands of years for its fibrous stalk, used in many applications including clothing and paper. It’s also a highly nutritious food, containing protein, fibre, vitamins and minerals. But, due to its appearance, it has never been grown commercially. Hemp is considered to be a low crop risk due to its short lifespan and high organic yield. Its seed is also considered to be an excellent source of protein, and is now being investigated by the pharmaceutical industry as a means of rapidly producing proteins for human consumption. This article will discuss the need for hemp to be grown in the UK, and the potential benefits of doing so.\nIndustrial hemp could be a valuable tool for UK agriculture, helping with the transition to more sustainable practices and providing a new crop, according to a farmers’ group.\nTo further explore the potential of hemp, five farmers are working with researchers from Cranfield University and the British Hemp Alliance as part of the Association of Soil Scientists’ Innovative Farmers programme.\nThey plan to conduct on-farm field trials to gather science-based data on the ecological benefits of this crop.\nHemp has long been stigmatized as belonging to the cannabis family (Cannabis Stiva L strains have low THC content and are not used for drugs), hemp requires no agrochemicals.\nResearchers say there is growing evidence that it can help increase biodiversity, fight pests, improve soil and sequester carbon.\nIt could also be a useful alternative to rapeseed, which is becoming increasingly difficult to grow in the UK.\nHowever, unlike many other countries, industrial hemp in the UK is still classified as a controlled drug and farmers wishing to grow the plant must apply for a licence from the Home Office.\nAs a result, there are only about 20 licensed producers in the UK with a total area of 2 000 hectares.\nWith post-Brexit agricultural policy focused on delivering public goods such as biodiversity and carbon sequestration, we need to rethink the approach to cannabis cultivation in the UK, says Nathaniel Loxley, project coordinator at Innovative Farmers and co-founder of the British Hemp Alliance.\nHemp could be a very valuable tool in the arsenal of UK farmers, but the UK is currently lagging behind internationally and there is a distinct lack of data in the UK context, said Mr Loxley.\nWe hope that by better understanding the benefits of cannabis and gathering evidence of it, we can pave the way for more producers in the UK.\nIndustrial hemp is a versatile plant used for building materials, clothing textiles, animal bedding and as a plastic substitute.\nManufacturers in the construction, fashion and packaging industries are increasingly looking to the factory for environmental solutions.\nBritain has a long history of hemp cultivation, and during the reign of Henry VIII hemp was so important to the British Navy that landowners were not allowed to grow it.\nHenry Ford even built a car from and with hemp, and the plant was used to remove pollutants after the Chernobyl nuclear disaster.\nMore and more benefits are being discovered from this incredibly versatile plant, says Helen Oldis, Innovative Farmers program manager.\nThere is real potential for farmers in the UK to use cannabis to restore their land, deliver public goods and grow a new crop, so it’s great that Innovative Farmers can get involved.\nIt is vital that farmers and researchers work together on these on-farm trials so that we can determine the true environmental benefits of cannabis and share this knowledge with more farmers.\nCurrently, most of the evidence for the environmental benefits of cannabis is anecdotal or from a limited number of studies in the U.S. and Europe, said Dr. Linda Dix, a researcher at Cranfield University who has conducted research on the plant.\nIt’s been shown to be good for the environment, both on the ground and above ground, but it could potentially provide other environmental benefits, and that’s something we need to understand, says Dr. Dix.\nOne of the areas to be studied is the carbon sequestration potential of cannabis. According to Dr Dix, the plant has the potential to store carbon in the soil and, through the use of its fibres in biocomposite materials, for example for construction or the automotive industry.\nData from other studies show different rates of carbon sequestration, likely related to soil types. So we need to know what that would look like under British conditions, she says.\nIt also has a strong taproot that can contribute to soil health by reducing compaction, improving drainage and aerating the soil.\nWhen the roots eventually break down, the carbon should provide a more stable soil structure.\nStudy participant Nick Voise and his company East Yorkshire Hemp are at the forefront of cannabis cultivation and product innovation in the UK.\nFifteen years ago, he converted most of his farmland to cannabis and now has 450 acres planted with cannabis.\nMr Voaz processes the harvest into fibre, animal bedding, HempLogz (firewood briquettes) and hemp concrete (for construction).\nAlthough we’re not an organic farm, we grow cannabis without pesticides, seed treatments, whatever – it works without them, says Voaz.\nCurrently, there are no fungal diseases or insect pests causing significant damage.\nThat makes it very useful for insects, and at this time of year cattails often congregate on the hemp, he said.\nI’m also interested in the carbon sequestration potential – if we turn hemp into hemp concrete, we may be sequestering carbon as long as the building stands.\nCamilla Heiselden-Ashby from Kent is growing cannabis for the first time on her family’s 320-acre mixed farm. She plans to add hemp to the farm’s crop rotation.\nWhen hemp takes root, it gets a very dense canopy, which shades the weeds – we like that, because we have a problem with quackgrass, says Haiselden-Eshby.\nThe roots of the plant go deep, so they take up nutrients from depth and make them available for future crops.\nI hope that by collecting data on the environmental benefits of cannabis, we can facilitate the licensing of cannabis.\nIt is not a dangerous drug, but a crop with great potential, and farmers should be encouraged to grow it, just as they encourage the cultivation of legumes to improve the soil.\nAll the farms participating in the Innovative Farmers survey are licensed cannabis growers and were selected to represent a wide range of different climatic, topographical and soil conditions in England.\nThey will grow commercial hemp at two sites, including a control plot, and analyze the soil for organic carbon content, soil structure, soil biology and biomass nutrients. They will also conduct biodiversity surveys to observe butterflies, insects and birds.\nPre-seeding for baseline data is currently underway.\nhemp treeshemp global warminghemp climate changehemp and soil carbonembodied carbon hempcretehow fast does hemp grow compared to trees,People also search for,Privacy settings,How Search works,hemp trees,how fast does hemp grow compared to trees,hemp global warming,hemp climate change,hemp and soil carbon,how much co2 does an acre of grass absorb,do different trees have different carbon sequestration abilities,embodied carbon hempcrete"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:11f2c846-62d3-4079-8349-fdbe4fb0a86c>","<urn:uuid:eac3e8af-fbcb-4937-970e-7200e5ca1c11>"],"error":null}
{"question":"What ages do McLean OCD Institute and CASTLE serve for intensive treatment programs?","answer":"The McLean OCD Institute's Adolescent Intensive Outpatient Program serves ages 12-18, while CASTLE's acute partial hospitalization program serves children between the ages of 5 and 15.","context":["Adolescent Intensive Outpatient Program\nWe are proud to introduce a new addition to our programming… Coming April 2021!\nTHIS SERVICE IS OFFERED IN A VIRTUAL FORMAT FOR INDIVIDUALS LOCATED IN TEXAS.\nAdolescent Intensive Outpatient Program (AIOP) – Launching in April of 2021\nThe McLean OCD Institute at Houston offers a highly specialized Adolescent Intensive Outpatient Program for adolescents (ages 12-18) with moderate to severe OCD and other anxiety-related disorders. This program meets virtually every Monday, Wednesday, & Thursday from 4:00 PM to 6:30 PM.\nThis unique program allows adolescents to receive specialized Cognitive-Behavioral Therapy (CBT) in a compassionate and individualized treatment environment. Adolescents in this program will receive three individualized therapy sessions with a licensed Behavioral Therapist and one family therapy session per week in addition to group therapy (CBT 101, Emotion Regulation and Family Dynamics), and Exposure with Response Prevention (ERP) with staff support. Family members are encouraged to join the 2nd and 4th Family Dynamics group every month.\nHow do I know that this program is right for my adolescent?\n- Your child/adolescent is suffering from severe OCD and/or anxiety and outpatient treatment has not been successful\n- You are interested in having your child/adolescent complete a step-down program after residential or inpatient treatment\n- You recognize your child needs intensive treatment but would like to minimize the amount of school your child/adolescent has to miss in order to receive this treatment\nWhat is treatment like at the Adolescent Program?\nYou can begin the admissions process by contacting our program by either completing the below form or calling us directly at 713- 526- 5055. Our Admissions Coordinator will send you the intake paperwork through our secure online patient portal. Please complete this paperwork with your child so as to ensure the most detailed description of their symptoms and treatment goals. Once we receive this paperwork, our Admissions Coordinator will contact you to schedule a brief phone assessment. Next, our Admissions Coordinator will schedule a diagnostic evaluation with one of our Behavioral Therapists. Please plan for both you and your child attend this evaluation. The Behavioral Therapist will assess your child’s symptoms and determine if your child could benefit from our AIOP. If it is determined that your child may not benefit from our AIOP, you will receive recommendations that may be more aligned with your child’s needs and/or treatment goals.\nFirst Week of AIOP:\nYour child will meet with their Behavioral Therapist for three outpatient sessions. These sessions will focus on providing you and your child with psycho-education about your child’s diagnosis and its treatment as well as developing an individualized treatment plan that will guide their work throughout the AIOP. You will also be scheduled for future sessions including Family Therapy as well as for a brief AIOP orientation meeting with one of our Residential Counselors.\nJoining the AIOP Groups:\nYour child will begin to participate in the AIOP group schedule during their second week in the program.\nOur Treatment Approach:\nOur team utilizes evidence-based treatments to care for your child. Specifically, we primarily use Cognitive Behavioral Therapy (CBT), with an emphasis in Exposure and Response Prevention (ERP), which is considered the gold standard treatment for youth suffering from anxiety disorders and/or OCD. CBT examines the inter-connections of our thoughts, feelings, and behaviors and teaches us that challenging our irrational beliefs can greatly impact our feelings and behaviors. ERP, a specific form of CBT, focuses on helping adolescents learn that if they face their anxiety-provoking thoughts, feelings, and situations in a gradual and systematic way, they will feel less anxious over time. Through ERP, adolescents learn that the do not have to engage in their compulsions, which helps decrease the frequency and intensity of their intrusive thoughts.\nOur goal is to not only alleviate your adolescent’s symptoms but to also teach them management strategies should symptoms return. To achieve this goal in the context of our short-term intensive program, we actively address relapse prevention early on in treatment and help the adolescent feel prepared for tackling future challenges after their treatment stay. Finally, as the adolescent is often living with their family, we incorporate family therapy in the AIOP program with our program’s family therapist.\n*AIOP is offered virtually to residents in the state of Texas.\n* Psychiatric services are available upon request at an additional cost","Child/Adolescent Day Hospital programs combine a structured treatment setting, at the core of which is intensive individual, group and family therapies, augmented by recreational and activity therapies.\nIncreased personal responsibility and healthy adoption within the family, work, school and community are the ultimate goals for each patient. Child/Adolescent Day Hospital programs operate Monday through Friday from 8:30 a.m. to 3:30 p.m.\nAdvantages of Partial Day Hospital: Patient continues to live at home, but comes to hospital for care\n- Weekly psychiatric oversight and supervision\n- Skilled nursing interventions\n- Multi-disciplinary treatment team\n- Stabilization of condition that would otherwise result in hospitalization\n- Specialty programs available\nChildren Are Really Extra Special\nThe C.A.R.E.S. Program at St. Francis Medical Center is an acute partial hospitalization program for children ages 3-14 years of age, outpatient program for children ages 3-21 years of age, CARES also has a Pediatric Nursery Partial Day Program for children ages 3 & 4. Our mission is to provide a high quality mental health diagnosis and intensive, compassionate treatment to children that are having difficulty succeeding, despite familial and community supports as their emotional and or behavioral difficulties impede their ability to function well in a social or home environment.\nChildren may be referred to the program by their school, outpatient agencies, inpatient children’s unit, DYFS, and the families themselves. All referrals are made to the Program Director.\nThe staff at the CARES Program are a multidisciplinary team consisting of master level clinicians, bachelor level counselors, a registered nurse, and a child and adolescent psychiatrist. All children receive daily academic instruction, daily group therapy, weekly individual therapy, and weekly family therapy. Upon admission all children receive a psychiatric evaluation.\nOur team members are committed to providing the children and their families with the tools they need to cope and to manage the challenges they are facing in their lives in a respectful, supportive, and nurturing environment. Door- to door transportation and meals are provided for the children.\n601 Hamilton Avenue\nTrenton, NJ 08629\nFor a immediate assistance please call 609-599-6430\nA Step Ahead\nOur holistic approach to child and adolescent mental health helps 3 to 18 year-olds find better pathways to healthy development.\nThis Child and Adolescent Partial Hospital program is available at Inspira Health Center Bridgeton. 2 levels of care are offered: full day program from 9am to 3:30pm and an after school program from 3:30pm to 7pm. Transportation is available for Cumberland and Salem counties but services do not require living in the county. We have served Gloucester and Atlantic county residents as well.\n333 Irving Avenue\nBridgeton, NJ 08302\nA Step Ahead\n525 State Street\nElmer, NJ 08318\nFor an immediate assessment please call 856-575-4196\nChildren Achieving Success through Therapeutic Life Experiences\nCASTLE is an acute partial hospitalization program geared to help children who have severe emotional, behavioral or psychiatric disorders. CASTLE serves children between the ages of 5 and 15 who do not require 24-hour inpatient care, but need more than outpatient care alone. We are open year-round, Monday through Friday. In most cases, door-to-door transportation is provided as are nutritional needs. We offer two levels of care:\nFull Day, Partial Hospital:\nChildren attend the program during typical school hours and receive a complement of intensive therapeutic and academic services.\nHalf Day, After School Partial:\nChildren attend the program after school and receive comprehensive mental health services. These services are also available in our out-patient program\nVirtua CASTLE Camden\n1000 Atlantic Avenue\nCamden, NJ 08104\nVirtua CASTLE Berlin\n100 Townsend Avenue\nBerlin, NJ 08009"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:5a339cb5-7335-436a-a358-1b0980f4a971>","<urn:uuid:4c8cedc7-d9e0-45f2-aa3b-8bf0c3547238>"],"error":null}
{"question":"How does Git handle documentation in monorepos, and what's the workflow for contributing to such projects?","answer":"Git handles documentation in monorepos through a combination of approaches. For large codebases, the conventional method uses README.md files across the codebase or a single root docs/ folder. For more complex cases, tools like Mkdocs with its monorepo plugin can be used, which allows including subfolder mkdocs.yml configurations and merging navigation trees into a single documentation site. As for contributing to such projects, the workflow typically involves forking the repository to your account, cloning it locally, creating a feature branch, making changes, pushing those changes to your fork, and then submitting a Pull Request (PR) to the original repository. Before submitting the PR, it's important to incorporate any upstream changes by pulling from the original repository and resolving any conflicts that may arise.","context":["Solving documentation for monoliths and monorepos\nAt Spotify, we’re always trying to push the boundaries of the way the world experiences audio. In order to achieve this, our engineers need to learn and move quickly. It is the best way we believe we can achieve the Spotify company mission of unlocking the potential of human creativity.\nOne of the ways we can speed up is through documentation. It is how our engineers navigate effectively through Spotify’s vast and complex array of software every single day. We have seen that overlooking and underserving this area has an increasingly negative effect on our developer’s productivity and will continue to grow if it isn’t addressed.\nWe set out to create a clearer and more accountable process for writing documentation for code maintainers along with a consumption method that is more unified and digestible for Spotify engineers. The way we believe we can achieve this is by adopting the Docs Like Code approach where we optimize for bringing docs as close to the associated code as possible.\nThis two-sided problem was something others agreed existed. Every year, Spotify hosts a hack week where Spotifiers can tackle issues they are passionate about, and don’t normally get the chance to work on in their day-to-day work. In 2018’s Hack Week, as there were no clear solutions that addressed the issues we had at our scale, several engineers and tech writers gathered to prototype a “hack” to address this issue. After it was known that discovering technical information was the 3rd biggest blocker to Spotify’s engineering productivity, a cross-functional team (consisting of tech writers and engineers) was formed with two goals: to bring Spotify’s documentation culture on par with the industry and make it best-in-class.\nMonoliths and monorepos\nAlthough Spotify is heavily built on the micro-service architecture model, we continue to operate numerous large scale codebases. These often contain some of Spotify’s most important engineering properties, such as our Desktop app, Web Player, iOS, and Android apps. In the web community, there is also a growing use of monorepos for better dependency management with tools such as Yarn Workspaces.\nThe current documentation convention for codebases is to use\nREADME.md across the codebase, or have a single root\ndocs/ folder. This works well for both open source as well as smaller internal codebases because these tend to have shared collective ownership. Larger internal codebases often have complex and distributed ownership, often with very different objectives and the current convention doesn’t serve them well. So we need to evolve the current convention in order to serve documentation needs for some of Spotify’s larger and more business-critical codebases.\nSolving it with our customers\nAs mentioned earlier, only a few squads operate large codebases at Spotify and it was important we addressed their problems. Fortunately working on the Infrastructure team allows us to have easy access to our customers, as they are also our colleagues. One of our core beliefs in Infrastructure is to build infrastructure together. We built, iterated, validated, and shipped our solution in one week, focusing specifically on our users’ pain points, which are at the heart of the process.\nWe use Mkdocs to power all of our internal technical documentation, and given how much of a need it presented internally we were curious to see if other companies had already made documentation easier in large codebases. We were surprised to learn that this wasn’t the case. Despite this, we saw a consistently strong need to address this problem, and after going back and forth on how to tackle this problem we ended up introducing a new syntax for our engineering squads.\nBuilding monorepo support for Mkdocs\nOne week later, we finalized a Mkdocs plugin which adds monorepo support. This introduces a new\n!include syntax for including subfolder\nmkdocs.yml configuration inside\nsite_name: \"Web Player Documentation\" nav: - Home: index.md - Features: - Home Screen: \"!include src/features/homeScreen/mkdocs.yml\" - Search: \"!include src/features/search/mkdocs.yml\" - Now Playing Screen: \"!include src/features/nowPlayingView/mkdocs.yml\" plugins: - monorepo\nThis new syntax lets you include additional\nmkdocs.yml files. The plugin will handle merging each of their navigation trees into a single “master” navigation tree. You can then generate a single documentation site as you normally would (an example is above). We decided on this approach as it gives us a few clear advantages:\n- Declarative approach to navigation. Each squad will have their own\nmkdocs.yml. This allows them to structure their documentation in a monorepo however they see fit and it will be centralized into a single documentation site.\n- Leverages GitHub Codeowners for configuring multiple owners. Numerous squads at Spotify use GitHub Codeowners to assign “ownership” to different squads on a per-directory or per-file basis. We wanted to tap into this existing GitHub feature.\n- Closer to the code. This is based on our belief that the respective documentation should be as close to the code as possible, but this now introduces support on a folder-level rather than previously on a repository-level.\n- Running Mkdocs inside a folder. We wanted to allow squads to be able to invoke Mkdocs on their own part of the documentation (on a folder level), rather than the entire documentation (on a repository level) to ensure a better developer experience.\nIt was important that we built this infrastructure together with the people who will be using it in their day-to-day workflow, as it allows us to provide the highest level of productivity. It also helps create passionate advocates for the change or improvements we’re working towards.\nHello, open source!\nWe have been trialling our Mkdocs Monorepo plugin for several weeks with internal squads and we’re happy to share this in beta with the greater open source community. You can check out the repository on GitHub here, or install the plugin to get started:\n$ pip install mkdocs-monorepo-plugin\nAnd then simply add the monorepo plugin to your\nplugins: - monorepo nav: - v1 API: \"!include src/versions/v1/mkdocs.yml\" - v2 API: \"!include src/versions/v2/mkdocs.yml\"\nTags: documentation, infrastructure, monoliths, monorepos, techdocs","A Primer On Contributing To Projects With Git\nThere isn’t any shortage of tutorials on this subject, but I haven’t seen any that attempt to guide a person that has zero experience with any of it. So, the goal of this article is to give a fresh “newbie” all of the information they need to collaborate on projects that use the Git SCM. This will not be a complete, or even a full introductory, guide to git; you should read other tutorials, and the reference docs for that.\nWe will focus on using the command line interface. Thus, to start, we need to setup the environment.\nI am not a Windows person. My day job is a Linux administrator and my full-time desktop environment is Appple macOS. Given that, the easiest environment I have found for Windows is to:\n- Install msysgit\n- Install ConEmu\n- Configure the default “task” for ConEmu to the one that uses the Bash shell provided by msysgit\nThis will vary based on the distribution you use. The short of it is that you need to install their package that provides git; typically, the name of the package is simply “git”.\nIf you are using Void Linux (my preference), then I recommend installing both the “git” and “git-perl” packages.\nI prefer using the “git” package from MacPorts.\nBut the easiest way to get started is to simply open a Terminal.app session\n$ git $ # you will be prompted to install the necessary components\nSpeaking of Terminal.app, I recommend switching to iTerm2. It’s just better.\nWith git installed, you should now configure it to know some details about you. These details will be used to identify you on the changes that you make within a project:\n$ git config --global user.name 'FirstName Surname' $ git config --global user.email 'firstname.lastname@example.org'\nWe will discuss central repositories shortly, but regardless of how the project you wish to contribute to chooses to centralize, you will need to authenticate yourself when synchronizing your contributions. You have two options:\n- Communicate with the central repositories over HTTPS\n- Communicate with the central repositories over SSH\nIn case #1, you will be prompted for your user credentials each time you work with the remote system. You can minimize this by using a “credential helper.” A nice overview of getting such a helper setup is available at http://stackoverflow.com/a/5343146/7979.\nAs for case #2, you will need to create an SSH key pair for yourself and configure the remote system to recognize it. Given the complexity of this method, we will assume the first method is being used. If you want to learn about the SSH method, then I am sure whichever central system your project uses will have instructions that will help you out.\nNow that we have a working git environment, we can learn about how to actually use it to collaborate on a project.\nWhether you are working solely within an institution or you are participating in an open source project, you will be working with a central repository. There are many ways a repository can be hosted centrally, but the most common, and the ones we will assume in this article, are provided by the following services:\nAll three offer some form of on-premises solution, public hosted repositories, and private hosted repositories. Regardless of the service provider and its location, you will need an account with the service. Thus, the easiest step is the first – create an account.\nFor the rest of this article we will assume you created a GitHub account at github.com.\nGit And GitHub\ngit is a distributed source code management tool. This means that git is intended to be used locally, without a central server. But all of the sites outlined in account setup are centralized server. So we need to think of sites like GitHub as a system that many people have “local” accounts on where they store their git repositories. This allows the users to make their repositories available to other users of the GitHub system, such that those users can create their own copy of other users’s repositories. In turn, this allows GitHub to provide features on top of the standard git features.\nAs a brief overview of the remainder of this article, the workflow created by this setup is as follows:\n- Bob creates a git repository on his local machine.\n- Bob copies it to his GitHub account, thus making the real respository the one hosted on the GitHub system.\n- Alice decides she likes Bob’s project and wants to help him with it, so she “forks” (copies) the repository to her own GitHub account.\n- Alice “clones” her copy of the repository on the GitHub system to her local computer.\n- Alice tells her local repository about Bob’s “upstream” (original) repository so that she can stay up-to-date with Bob’s changes.\n- Alice creates a “branch” on her local repository, makes changes, and “pushes” those changes to her fork on the GitHub system.\n- Alice uses the GitHub interface to tell Bob about her changes, asking if he’d like to incorprate them into his original repository.\n- Bob decides he likes the changes, accepts them, and his repository on the the GitHub system is updated.\nFork The Project\nFor remainder of this article we will assume that you want to collaborate on the awesome Pino project. Our first step is to create a copy of the project in our account on GitHub. So we navigate to [https://github.com/pinojs/pino] in a web browser and click “Fork” button (currently in the upper right corner of the page). GitHub will then show a screen that the process is happening, and then load the Pino repository in your account.\nNow that Pino has been forked to your account, we will “clone” it to your local machine. Cloning, in this context, is merely copying the repository from your GitHub account to your personal computer. To accomplish this task, we use our terminal and enter:\n$ cd ~/Projects # or any place you want to keep a collection of projects $ git clone https://github.com/your-username/pino.git\nAt this point you will have a directory:\n~/Projects/pino. This directory\nis your local copy of the repository. This local copy is automatically tied to\nyour copy of the repository on the GitHub system. This link between your two\ncopies is known locally by the name “origin”. Within git this is known as\na “remote”. To see the remotes associated with your local repository, which,\nat this point, is only “origin”, issue the commands:\n$ cd ~/Projects/pino $ git remote\nFor more information on remotes, read https://git-scm.com/docs/git-remote.\nLink To Upstream\nBefore you begin working on your changes it is a good idea to connect your local repository to the original repository. Colloquially, this is known as the “upstream” remote. From within your local repository, issue the following command:\n$ git remote add upstream https://github.com/pinojs/pino.git\nCreate A Feature Branch\nWe are now ready to begin working on some changes to the project. The key to successful collaboration is to request the minimal amount of changes as is necessary to implement your idea (or fix). You should do this on a new branch within your repository. A branch is merely a snapshot of the repository at a specific point in time. By working on a branch you lock the project to the state at which you decided you want to add changes, and it makes it easier for the upstream project owners to review your changes when you submit them.\nTypically, you will want to name your branch in such a way that it indicates why the branch was created. So, let’s assume we want to make some documentation corrections. Enter the following command from within your local copy of the repository:\n$ git checkout -b doc-corrections\nThe above command is a shortcut for the following two commands:\n$ git branch doc-corrections $ git checkout doc-corrections\nEvery repository has what is known as a\nmaster branch. At this point, we\nhave started a new branch,\ndoc-corrections from the current state of the\nmaster branch. To see the branches available:\n$ git branch\nBefore moving on, let’s also create the\ndoc-corrections branch within your\ncopy of the repository on GitHub. To do this, we will “push” our branch:\n$ git push -u origin doc-corrections\nThis has done two things:\n- It has created the\ndoc-correctionsbranch in your repository on GitHub.\n- It has configured your local copy of the repository to know that the local\ndoc-correctionscorresponds to the\ndoc-correctionsin your GitHub copy of the repository. This allows for some shortcuts when issuing certain git commands.\nTo learn more about branching and pushing, see:\nNow that we are on our own branch we can make our changes. For now, let’s\npretend you have made some typo corrections to the\nREADME.md file. Which is\nto say, you have opened\nREADME.md in your text editor, adjusted the text\nwithin and saved the document. If you issue the following command:\n$ git status\nYou will see that git has recognized that you made changes to the file. At\nthis point git isn’t going to do anything with those changes. Files that\nare tracked by a git repository have two stages: modified and scheduled\nto be committed. In the “modified” state git merely recognizes that a file\nhas been changed from its initial state. In the “scheduled to be committed”\nstate git will write the changes made to the file into its internal tracking\ngit commit command is run. So, let’s move our changes from the\nmodified state to the schedule state:\n$ git add README.md\nWith the changes scheduled to be committed, let’s actually perform the commit:\n$ git commit -m 'A short summary of the changes'\nThe above is the equivalent of sending an email with nothing more than the\nsubject line filled in. To write a full commit message, simply issue\ngit commit. This will open the default commit editor, probably a variant\nvi. A full commit message should have the following format:\nA short summary of the changes A body describing in further detail your changes. The summary line should not exceed 40 to 50 characters, and the body lines should not exceed 80 characters. Thes are not hard and fast rules, per se, but they widely followed guidelines. Some projects have other requirements for commit messages, and may refuse changes if they are not followed.\nWith your changes committed to the local repository, it’s time to send them to your copy on GitHub:\n$ git push\nWe are able to use this short push command since we linked your local\ndoc-corrections branch to your remote\ndoc-corrections branch. If we hadn’t,\nyou’d have to issue:\n$ git push origin doc-corrections\nYou can learn more about committing at https://git-scm.com/docs/git-commit.\nIncorporating Upstream Changes\nPrior to sending our corrections to the upstream project, it’s a good idea\nto make sure you have any changes that have occurred upstream into your\nrepository. To do that, we need to switch to the\nmaster branch, pull in\nchanges from upstream, and then merge them into your\n$ git checkout master $ git pull upstream master $ git checkout doc-corrections $ git merge master\nAt this point git may tell you that there are conflicts. A conflict arises\nwhen the same file has been edited in both branches of the merge,\ndoc-corrections in this case, such that git can’t decide\nwhich change should “win.” If this happens you will need to fix the conflicts\nand then issue a\nWith all of the changes incorporated, it’s time to push them to your copy\nYou can learn more about resolving conflicts at https://githowto.com/resolving_conflicts.\nSending Your Changes To Upstream\nNow that your changes are pushed to your copy of the repository on GitHub, it’s\ntime to send them to the upstream repository owner(s) for review and possible\ninclusion. To do this, open your copy of the repository on GitHub. It’ll be at\na URL like\nWith your repository open on GitHub in your browser, you should see a message\nsuggesting that you send a “Pull Request” (PR) from your\nbranch to the upstream\nmaster branch. Simply click the button in that message\nand you’ll be taken to a form where you will can describe your PR. It will\ndefault to the last commit message in your branch, but you can change it.\nWhen you are happy with the PR message, submit the PR and wait.\nGitHub is going to email the authors of the upstream project to let them know about your PR. They will review it and probably start a discussion with you, or just accept it if a discussion isn’t necessary. Either way, you will receive emails keeping you informed of the process.\nOnce the PR has been resolved, you can remove your feature branch:\n$ git checkout master $ git pull upstream master $ git branch -D doc-corrections\nWhile it may seem like a complicated process, and in some ways it is, you should now be able to collaborate on a project that uses the Git SCM. In general, this is how most open source projects work. And once you have gone from fork to PR, the process shortens to simply staying synchronized, creating branches, and submitting PRs.\nDon’t be afraid to get involved. If the upstream people ask for changes, in most cases they are not insinuating anything about you personally. The simply want your changes to conform to the nature of their project so that your changes can be included. Or they have suggestions for improvement, so that your changes can be included.\nA great place to get started with almost any project is with the documentation, just as we did in this article. If there’s one thing every project wants it’s someone willing to write documentation. As you get more comfortable, you will certainly start branching out from there.\nBy the way, I’m a maintainer on the Pino project. I look forward to seeing your PRs :)"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:b7ffcfff-125f-4d2e-8958-de917808f04a>","<urn:uuid:755dc951-a3f0-42a8-b4ab-8dc4d65b39c7>"],"error":null}
{"question":"How strong is toughened glass compared to regular glass, and what safety features it provides?","answer":"Toughened glass is four to five times stronger than conventional annealed glass. When broken, it shatters into thousands of tiny pebbles instead of dangerous shards, which practically eliminates the danger of human injury. It must be used in specific critical locations according to building regulations, including all glazing from floor level up to 1500mm height, all doors whether partly or fully glazed, and windows within 800mm of the finished floor level internally.","context":["The glass plays the biggest part in keeping your new windows and doors energy efficient. It forms the largest part of most windows and doors. Glass is also required to provide very important safety features for all doors and particular locations of windows. The provision of safety glass ensures safety comes first.\nIt is a legal requirement that toughened safety glass must be fitted to all replacement doors. This legal requirement for toughened safety glass also extends to windows near floor level and other “critical locations”.\nThe Building Regulations for England & Wales have approved this rule and give clear information where toughened safety glass is required. The same rules apply when existing windows are being replaced or for new windows fitted into new buildings.\nWhat are “critical locations” requiring safety glass?\nThe critical area in a window or a door is the glazing that you are likely to come into contact with as you move around a house or commercial building day to day. This possible contact with glass means that glass must be shielded or protected from impact if glass is to break it must break in such a way that is not going to cause injury, glass must resist impact without breaking. This where you often see a pane of glass ‘shattered’ upon impact but not broken.\nFor any single, double or triple glazed replacement windows or doors, the following types as well as locations will need safety glass.\nAll glazing from the internal finished floor up to a height of 1500mm must be safety glass.\nAll doors whether partly glazed or fully glazed must have toughened safety glass\nWindows must be fitted with toughened safety glass where the window is within 800mm of the finished floor level internally.\nTop-lights above doors are not required legally to have toughened safety glass. However even with clear glass, the toughening process produces a different tint to the glass so we would always advise toughened glass in top-light areas as well to match the rest of the window glazing.\nThere are other areas where you may wish to consider the use of safety glass. If you are fitting a window near your bath or shower the finished floor level will actually be higher if taken from the shower tray or the bath itself.\nWith laminated glass it is effectively two pieces of glass with a film in between. In the event of a glass breakage, whilst the glass will break it will hold together. Toughened glass uses a toughening process different from laminated glass. Whilst it is hard to break toughened glass, should it break it will shatter into thousands of small harmless pieces.\nThe choice of toughened or laminated glass is of-course up to you. Laminated glass tends to be more expensive and heavier than toughened glass but if does often offer a reduction in sound pollution especially if you go for an acoustic laminate. If you want maximum security laminated glass is the best option, it will hold together when broken unlike toughened. Both types are suitable and acceptable under legal requirements and building regulations.","How is Toughened Glass Made Valiant Glass\nIn addition to making tempered glass four to five times stronger than conventional annealed glass, re-heating and rapid quenching dramatically changes the break characteristics of the glass. Consequently, when tempered glass is broken, it shatters into thousands of tiny pebbles—this practically eliminates the danger of human injury caused by sharp edges and flying shards.... One of the questions commonly asked about toughened glass (or tempered glass) cooker hobs is whether the glass will last. In January, Electrolux recalled several glass hob models, while just over the weekend in Pasir Ris, a housewife’s glass stove shattered while she was cooking.\nToughened Glass Alltrade Glass\nToughened glass is treated using a thermal tempering process. This makes it more resistant to breakage than simple annealed glass. Also,if it does break, it doesn’t shatter, but instead, breaks into typically square pieces as opposed to more dangerous shards.... How is glass made? Read about toughened glass manufacture. Have you ever wondered how is glass made? Toughened glass is made from normal, float glass.\nGlass and Thermal Stress Pilkington - First in Glass\nToughened glass is stronger than standard glass and does not shatter into large shards when broken. This is important, because it can greatly minimise potential danger in the case of a break. Manufactured through a process of extreme heating and rapid cooling, Toughened glass is much harder than normal glass. how to clean shower glass with wd40 Tempered Glass Breakage 1. There is frequently a misconception that tempered glass is \"unbreakable\" or \"nearly unbreakable\". This is NOT true. Tempered glass is definitely breakable and many of the things that can break annealed glass can also break tempered glass. 2. Fully tempered glass as supplied for shower door, patio doors, etc., is four to five times as strong as annealed glass of the\nLaminated Glass vs. Toughened Glass and the Benefits of\nSafety Glass is a generic term for glass which is made to be safer than normal, untreated glass. The most common types are toughened glass and laminated glass. how to break up with a guy Spontaneous glass breakage is a situation where a tempered glass breaks without a provocation. Learning tempered glass break pattern to understand the cause of these breakages.\nHow long can it take?\nLaminated Glass Toughened Glass Tufwell Glass\n- Where are Toughened Laminated Glass Balustrades Used\n- Avoid risk of injury caused by broken glass with toughened\n- Toughened Glass Alltrade Glass\n- Glass Types Jason Windows\nHow To Break Toughened Glass\n24/03/2012 · A few examples that come to mind: toughened glass dinner plates are known to shatter for no apparent reason, so are the glass windows in oven doors, and glass coffee table tops. Vehicle windscreens do shatter seemingly for no reason, but generally this can be traced back to a …\n- 8/11/2010 · This video is unavailable. Watch Queue Queue. Watch Queue Queue\n- Toughened glass is often referred to as safety glass or tempered glass. Toughened glass is annealed glass, heated and then rapidly cooled and is also classified as Grade A Safety glass. Toughened glass is five times stronger than standard annealed glass of the same thickness and the toughening process reduces the risk of cracking.\n- Glass can be a dangerous material . When standard glass breaks, it can form dangerous shards and splinters. To avoid risk of injury caused by broken glass, high tech safety glasses are available in toughened and laminated forms – most of which can be used in double glazing window.\n- Okay laminated glass is not tempered glass. Laminated glass is two layers usually with plastic between them. When I worked as a mechanic we could push a lot of the windows out on the V.W.s using a bit of silicon spray around the rubber seal helped..."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:18f305c1-ef09-4b91-a752-254e83d007bc>","<urn:uuid:43e25dea-8de7-44fb-ac51-df6d43654a86>"],"error":null}
{"question":"What similarities exist between Roman Saturnalia feast traditions and modern food preservation practices during holiday celebrations?","answer":"While the Romans surprisingly did not overindulge during Saturnalia itself, their general feast traditions and preservation methods have influenced modern holiday celebrations. Romans preserved various foods including meats, vegetables, and fruits, using methods that are still relevant today. They pickled vegetables, preserved olives, and used salt-curing for meats. Modern sustainable preservation methods continue these traditions, using techniques like drying, smoking, salt-curing, and pickling to prepare foods for holiday celebrations. The Romans' practice of preparing preserved foods for feasts has evolved into our current holiday traditions, where we still use many of the same preservation techniques for special occasions, though today we have additional options like refrigeration and freezing that weren't available to the Romans.","context":["Saturnalia, in Roman times, was the annual festival on 17th December celebrating the end of winter, which quickly expanded into a week of Misrule, feasting, partying, gambling, drinking, playing card games, and general cavorting around the town ...\n|Ruins of the Temple of Saturn|\n(eight columns to the far right) in 2010\nThey even dressed down – togas came off and bright tunics appeared with silly hats. Misrule was a very debauched affair ... masters serving servants ... lawlessness prevailed briefly ...\n... Roman courts were closed, and Roman law dictated that no-one could be punished for damaging property or injuring people during the weeklong celebration.\nThe festivities began when the Roman authorities chose “an enemy of the Roman people” to represent the “Lord of Misrule” – this poor victim was subjected to some rather unpleasant ‘pranks’ ...\n|Saturnalia by Ernesto Biondi (1909) in the|\nBuenos Aires Botanical Gardens\n... at the festival’s conclusion the Roman authorities believed they were destroying the forces of darkness by brutally murdering this innocent – represented by gingerbread men biscuits today.\nSurprisingly during Saturnalia the Romans did not over indulge – our main tradition the lavish and luxurious eating of a fine feast developed over the centuries as we relinquished Saturnalia, paganism and embraced the Christian faith.\nHowever quite a few of the Roman traditions make their appearance in our Christmas of today ... gift giving (sometimes with little verse cards attached), carol singing, gingerbread men biscuits –as we know those had devilish connotations.\n|Dice players in a wall painting|\nChristians adopted the Pagan tradition of tree worshiping, decorating their homes with greenery – ivy and mistletoe ... our Christmas tree today.\nThe forum was the town’s main meeting place where people would mingle, the theatre would show a variety of theatrical entertainments ... with street performers adding to the mix.\nAlthough feasting did not occur – the wealthier Romans ate well ... banquets would consist of a variety of courses – but normal life was as we know it today ... revolving around the rhythms of manual labour (in the fields).\nBreakfast was bread and fruit, a light lunch in the middle of the day consisting of thick porridge, bread, cheese, cold fish, meat and with humble vegetables ...\n... while the main meal of the day was a three course dinner served in the late afternoon in the Triclinium, where diners reclined on three couches, arranged around a low dining table, and served from the kitchen within the villa.\n|A Roman Dinner|\nThe wealthy, patriarchs and plebeians, would have kitchens, but most people, the freedmen (general plebeians), lived in apartments and they had to eat cold meals or buy hot food from the many take-away food shops.\nThe slaves would get handouts of grain which they made into a porridge type gruel, while the freedmen ate this staple porridge supplemented by oil, the simpler vegetables and salt fish.\n|Stuffed Roasted Boar|\nCooks in the homes of the wealthy patricians were valued household slaves – as it was exceedingly hard work - the evening meal often taking all day to prepare ... food being cooked in a brick oven, and was either boiled in a pot or roasted on a griddle over the flames.\nFood was quite varied but depended on the seasons and availability – roasted meats: pigeons cooked inside chickens, wild boar or pigs stuffed with sausages made from extra meats and innards – waste not want not ...\nRicher classes had starters of eggs, seafood or snails ... supplemented with cheeses, olives, lentils, sea urchins, molluscs, shrimp, salted anchovies ... with extra vegetables: kale, chard, nettles and sorrel.\n|Foods from plant sources|\nPickled vegetables were readily available ... olives, chicory, cardoons, broccoli, asparagus, artichokes, leeks, carrots, turnips, parsnips, beets, peas, green beans, radishes, cauliflower, cabbages, lettuces and field greens, onions, cucumbers fennel, capers – early varieties of the types we eat today ...\nThe main dish usually consisted of meat – pork was the most popular – all parts being eaten; beef wasn’t very popular – cattle were working animals; geese, duck, chickens, peacocks and swans all featured ... sausages of various sorts were made. Hares and rabbits were bred and eaten – hares less successfully.\n|Chickens hanging in a shop in Mexico|\nFresh fish was usually only found at lunch times – they were difficult to fatten up ... though freshwater and saltwater ponds existed. Fish sauce was the universal sauce added to everything ... when it was being made the production of garum was banned in the towns ... as the stench was unbelievable!\nThe small sealed amphorae were distributed throughout the Empire and totally replaced salt as a condiment. The remaining solids were sold as a kind of savoury spread ...\n|\"The Mullus\" harvesting pepper taken from a|\nFrench edition of The Travels of Marco Polo\nSpices were imported on a large scale and used copiously ... pepper, saffron, cinnamon, herbs, cloves, nutmeg, ginger ...\nDesserts would include plenty of fruits – fresh or dried – grapes, figs, dates, pomegranates, quinces, apples, apricots et al ... and the Romans loved walnuts, hazelnuts, almonds, chestnuts and pine nuts ...\n|Roman Honey Cakes|\nRoman bakers were famous for the many varieties of breads, rolls, fruit tarts, sweet buns and cakes ... cakes made of wheat and usually soaked in honey were often served.\nDrinks were early types of wine, mixed with herbs and honey, early mead, strong raisin wine, matured spiced wine; beer was known but considered vulgar. Sour wine mixed with water and herbs was a popular drink for the lower classes.\nIn some ways the Romans had the same choices we have today for our Christmas feast ... and our foods have evolved from those early beginnings.\nTheir entertainment was similar to ours today – even in this technological age – musicians, acrobats, poets or dancers .... dances were not usual, as it was considered improper and would not mix well with table manners ... although during the comissatio (a round of drinks) this habit was often disregarded.\nTimes have not really changed that much in two millennia ... different countries have evolved their own cultures out of those early beginnings ...\n|Our Christmas Pudding complete with holly|\nsprig - if the Romans had invented matches\n- then they too would have flamed it\nThankfully we don’t have to choose a Lord of Misrule – except as a revival of those early traditions enacted to remember our roots ... and I love that we don’t forget our history ... it’s made who we are ...\nHappy Christmas preparations ... though I’ll still be around!\nPositive Letters Inspirational Stories","Organic Food Preservation Definition, Importance, & Methods\nFood preservation has long been a necessary pursuit of humans through the ages. While short term food preservation methods are largely dominated by today’s refrigerators, and long term preservation is dominated by canning or freezing, our ancient cultures thrived without such technology by collecting, drying and storing grain in large ceramic pots. Hunter-gatherers preserved meat and fish by air drying or smoking. Salt and sugar, when readily available, were also used as a preservative.\nPreserving food without canning or freezing or refrigerators?\nCountless methods and technologies of food preservation have been attempted through the millennia and a few tried and true methods that are natural and sustainable have been passed through generations. These techniques are part of food preservation history are of course, organic, and still applicable and useful in modern sustainable living.\nPreserving meat through drying.\nThis most ancient form of food preservation is simply the removal of water from any given food by air drying, sun drying, or smoking. Commercially available food dehydrators or a home oven can also be used to preserve food. Nearly all foods can be preserved through drying including meat (jerky), grains, fruits, vegetables, fungi, and even milk. Smoking, is part of the food drying method.\nCuring is a method of preserving food, most often meat and fish, by the addition of salt, nitrates, or sugar. Salt-cured meat works by inhibiting the growth of microorganisms by drawing water out of cells through osmosis. Concentrations of salt up to 20% are required to kill most species of bacteria and smoking the meat adds additional chemicals that reduce the amount of salt required. Common examples of salt-cured meat include bacon, kippered herring, corned beef, and pastrami. Olives, pictured above, are easily preserved through the salt curing method as well.\nSugaring is a method that places dehydrated food into pure sugar. The purpose of sugaring is to create an environment hostile to microorganisms. Sugaring is commonly used to preserve fruits and their peels as well as spices such as ginger root.\nAlso known as brining, pickling is the process of preserving food by anaerobic fermentation in brine (a solution of salt in water) to produce lactic acid, or marinating and storing it in an acid solution such as vinegar. The resulting food is called a pickle and can be any variety of food item including eggs, peppers, cucumbers, or even citrus fruits.\nThe use of nontoxic and beneficial microorganisms can also preserve food. An ancient form of bio-preservation is fermentation which was, and still is, used to produce beer, wine, vinegar, bread, yogurt, cheese, and butter. All of these items can still be produced in the home kitchen using biopreservation techniques.\nFor short term food preservation methods, simple technologies such as the zeer pot, submersion in water (such as a lake or river) and the use of cool caves or underground pits extend the expiration of many perishable foods. These techniques of course can be employed today as well, some more fitting in certain climates than others. -KATHY FAIRCHILD"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:61088313-5a6b-46d9-a14e-eb69faaf036e>","<urn:uuid:263d2c1b-5823-42bd-b845-c8cbcfca9ace>"],"error":null}
{"question":"How does the SP-55 FI TS generator application contribute to hybrid drone solutions, and what role does solar power play in advancing long-duration drone flights?","answer":"The SP-55 FI TS generator application is the first Sky Power International two-stroke engine developed specifically for generating electrical energy. It features an injection system, brushless DC motor for power generation, and an integrated cooling system for both engine and generator. This compact solution helps extend UAV range and flight time while enabling heavy load transport. As for solar power's role, it has proven to be a more efficient energy source for long-duration drone flights. Solar cells provide a constant energy stream while reducing overall weight. However, solar-powered drones face specific challenges - only 5 out of 20+ solar technologies can withstand the high pressure, extreme temperatures, and fluctuating weather conditions experienced during flight. The technology requires specific standards for measurements focused on power-to-mass and power-to-area ratios rather than just energy yield.","context":["Sky Power International unveils new engine innovations at Xponential 2022\nGerman UAS engine manufacturer Sky Power International will be presenting two new Wankel engines, among others, to the trade public for the first time at the leading AUVSI Xponential 2022 trade show, which will take place from April 25-28 in Orlando, Florida. Furthermore, Sky Power International will show the entire portfolio, from small 1- and 2-cylinder engines with carburetor or fuel injection to hybrid engine concepts.\n“As we do every year, we will again be able to present a whole range of product innovations at the Xponential,” explains Karsten Schudt, Managing Director of Sky Power International. The focus will be on the new SP-360 DRE and SP-540 TRE Wankel engines. The SP-360 DRE is a double-blade Wankel engine with up to 51 hp (38 KW) at 6000 RPM. The SP-540 TRE, on the other hand, is a tripple-blade Wankel engine. This one produces 74 hp (55 KW) at 6000 RPM. As a propulsion system, Wankel engines are exceptionally lightweight and require a smaller installation space than comparable two-stroke engines. Minimal radial or torsional vibration makes this motor particularly interesting when high performance optics or sensors are used in UAVs. The ability to mount these motors vertically at 90 degrees increases flexibility and is suitable for helicopter applications. “We are pleased that we can now meet high performance requirements with the extended Wankel motor family,” Schudt continues.\nIn addition to the Wankel engines, however, Sky Power International will also be showcasing its extensive two-stroke engine portfolio. Here, the engines are offered in different configurations within a wide ccm range. This philosophy always includes carburetted and fuel-injected versions in an engine family. HF engines are also available. This means that customers can choose between three different configurations based on one engine core system. Furthermore, all engines can be equipped with a starter / starter generator. This allows the engine with the most important subsystems to be integrated into an aircraft in a space-saving manner.\nThe entire engine portfolio can thus be used for hybrid applications. The SP-55 FI TS generator application in particular has been developed for this application. “This is the first two-stroke engine from our company that has been developed purely for the generation of electrical energy,” says Schudt. With this engine, Sky Power International aims to appeal to users of purely electric propulsion technologies who are looking for an efficient generator solution not only for unmanned aerial systems, but also for other applications. “For UAVs, the benefit of such a generator solution is obvious. The combustion engine can be used to extend the range or flight time as well as to ensure a transport of heavy loads,” Schudt continues.\nThe SP-55 FI TS Generator application is equipped with an injection system and a brushless DC motor for power generation. To cool the entire engine unit, an integrated cooling system was developed for both the engine and the generator. The injection, the control and temperature control as well as the air supply are located on the side of the engine. The generator is mounted on the rear output shaft. The rear output shaft and the position of the fuel injection on the side make this engine one of the most compact 1-cylinder engines on the market.\nSky Power International will be exhibiting at Xponential 2022 at booth 1723.","The key distinction between drones and other aircraft is their ability to fly and operate autonomously, hence the name unmanned aerial vehicles (UAV). With neither the risks nor limitations of having a human onboard, drones can reach higher altitudes and operate for extended periods of time, while controls are “manned”, or monitored, by remotes and computer systems on the ground. The combination of autonomy and advanced technology has allowed drones to take on a variety of functions, such as carrying and delivering goods, surveilling areas of land, collecting data and capturing images for both military and civilian missions (1).\nEngineering a power source that can last for days/weeks and not burden the device with excessive weight is a constant challenge in aeronautics. Drones for civilian use are usually powered by removable battery packs that last for less than an hour before returning to the ground for replacement (2). Those used for extended surveillance and reconnaissance require much larger power sources to carry the weight of multi-rotors and actuators (i.e. cameras, weapons, radars, sensors), which in total can range from below 2kg and over 600kg (1). For high endurance missions lasting days or weeks, 80-90% of the gross weight is in fuel capacity and 10-20% of the weight in the actuators (1).\nSolar-powered drones have proven to be a much more efficient source of energy for drones traveling long distances and over extended periods of time. The use of solar cells not only offers a constant stream of energy to operate the vehicle, but also greatly reduces the overall weight. AeroVironment, a leading energy and aeronautics company in California, designed some of the most groundbreaking solar-powered aircrafts—Solar Challenger, Pathfinder, and Pathfinder-Plus (3). Although these were manned aircrafts, AeroVironment also has a division for unmanned aircraft systems, which may foreseeably be integrated with their solar-energy initiatives.\nSolar-powered drones still require research and experimentation before they can be fully implemented. However, the opportunities and benefits are clear and the industry has become increasingly competitive, with energy, aeronautic innovation, and even digital media companies involved. Solar Impulse is one company working towards clean energy aircrafts. Most recently, the company’s Solar Impulse 2 completed a 43,000 km flight on no fuel and solar energy alone. Bertrand Piccard, the 58-year-old seasoned pilot, told reporters, “We have shown that the plane could fly forever. The limit is the pilot.” (4) Facebook and Alphabet, Google’s parent company, are developing technologies in their projects known as Aquilas and Google Titan to deliver internet connection to any part of the world that still lacks access (5). These missions require devices to sustain itself at high altitudes, 18,000 to 27,000 meters above the ground, and adapt to unexpected weather conditions for a continuous 3 months; current tests remain far from the goal, as Aquila has flown up to 655 meters above ground and for a 96-minute period (6).\nThe vision for solar-powered drone technology has yet to be perfected and authorized under government regulation. As mentioned earlier, photovoltaic technology has proven to be sufficient for fueling aircrafts without interfering with the vehicles’ aerodynamics. However, the conditions for which drones are expected to experience include high pressure, extreme temperatures, and fluctuating weather conditions. With this in mind, researchers have identified that only 5 out of 20+ technologies (crystalline silicon (c-Si), gallium-arsenide (GaAs), amorphous silicon, copper-indium-gallium-selenide and thin gallium-arsenide based photovoltaics) can withstand the conditions described above (7). Furthermore, research around photovoltaic technology specifically for drones requires a different standard for measurements and data collection because the optimal solar panel for powering a drone is not necessarily the technology with the highest energy yield, but the highest power-to-mass and power-to-area ratios (PUAV) (7). Currently, solar-powered drone technology is somewhat dependent upon existing photovoltaic research, until more data can be collected and solar energy systems can be understood in collaboration with aerial vehicles.\nFinding a balance between mass and power sources in drone technology is not the only challenge that innovators are faced with. Researchers and engineers are also racing against the clock as more competition enters the industry and increasing government regulation is put in place. The U.S. Federal Aviation Administration’s (FAA) UAS Rule (Part 107), due to take effect on August 29th, 2016, limits vehicles’ weight at 25kg and altitude at 121.92 meters off the ground unless approved under a certificate of waiver (8). Increasing government regulations may hinder the progress in solar-powered drone research, especially for purposes of testing high endurance and high altitude vehicles. On the other hand, as more companies and organizations occupy the aerial landscape, partnerships across sectors will rise. Facebook and Alphabet are just two of many companies that have found opportunities through such partnerships.\nIncorporating solar energy with drone technology has shown benefits beyond providing clean energy. Solar-powered systems will allow vehicles to spend longer durations in the air and maintain constant surveillance, leading to greater data and accuracy of information collected. These aspects are appealing to industries that may not have had a part in drone technology before, like those in media or telecommunications. Although this new area of research may lead to some interferences from competitors and regulators, other businesses and governments can also offer great networks and opportunities for solar-powered drone technology.\n(1) Gupta, S., Ghonge, M., Jawandhiya, P. M. “Review of Unmanned Aircraft System (UAS)” International Journal of Advanced Research in Computer Engineering & Technology (IJARCET), vol. 2, no. 4, 2014. Accessed on 5 Aug 2016.\n(2) Pullen, John Patrick. “This Is How Drones Work.” TIME. TIME Inc., 3 Apr. 2015. Web. 1 Aug. 2016.\n(3) AeroVironment. AeroVironment, Inc., 2016, www.avinc.com. Accessed 1 Aug. 2016.\n(4) Burgess, Matt. “What’s next for Solar Impulse? Pilots reveal where their iconic plane is going to take them now.” WIRED. Condé Nast Publications, 27 July 2016. Web. 3 Aug. 2016.\n(5) Cuthbertson, Anthony. “Google Tests Solar-Powered ‘5G’ Internet Drones” Newsweek. Newsweek LLC. 1 Feb. 2016. Web. 1 Aug. 2016.\n(6) Vanian, Jonathan. “Facebook’s Solar-Powered Drone Just Hit a Big Milestone” Fortune. Time Inc. 21 July. 2016. Web. 1 Aug. 2016.\n(7) Alta Devices. “White Paper: Selecting Solar Technology for Fixed Wing UAVs” 2015. pdf. 3 Aug. 2016.\n(8) Small UAS Rule, Federal Aviation Administration § 107 (2016). Print.\nImage: © Ivan Cholakov | Dreamstime.com - <a href=\"https://www.dreamstime.com/stock-photo-drone-over-us-city-surveillance-flying-image57023398#res14972580\">Drone over US city</a>\nThe idea of using sunlight and radiant heat to source electrical and heating/cooling systems is no new phenomenon. Solar powered technology has been developing for decades, since the introduction of solar photovoltaics (PV), a medium of conductive materials (i.e. silicon, cadmium, gallium) designed to convert absorbed sunlight directly into electricity (1). Solar engineers later found that the alternate form of solar energy, radiant heat, could generate solar thermal electricity (STE) through concentrated solar power (CSP) technology. CSP structures are strategically placed to reflect and focus sunlight upon a “heat transfer fluid” (i.e. molten salt, synthetic oil), which then transfers the energy to an engine that produces electricity (1). Although solar technology has existed for several decades, solar engineers continue to make material, installation and distribution improvements on existing PV and CSP structures to stimulate and sustain growing demands for renewable solar technology.\nBetween solar PV and CSP technology, households and businesses prefer PV energy conversion systems because they are easier to install, show rapid returns on investment, and are compatible with existing policies/markets (1). CSP technology remains as a major source of renewable energy, but requires much more area and maintenance, making it most effective in arid climates.\nAt 2015 year-end, the worldwide capacity for solar PV was 227 gigawatts, approximately 185 million solar panels, while that of STE was 4.8 gigawatts (3). The U.S. alone increased capacity by more than 10 gigawatts in 2015 and totaled over 800,000 distributed PV systems installed (4). These trends were attained through technological advancement, as well as increasing dialogue around the subject of renewable energy in politics, businesses, and civil engineering. The past decade has shown rising numbers of operating solar energy systems in the U.S. and worldwide due to both scientific and systemic progress.\nMaterial and Manufacturing Improvements\nSolar CSP and PV systems are both constructed with large amounts of metal, where CSP structures have also shown a “high metal depletion burden”, “greater than for other power generators” (1). PV systems generate the most waste during the manufacturing process of using crystalline silicon to build PV modules (1). Developments in solar technology materials and manufacturing processes are foreseeable, as it is in solar technology companies’ best interest to find alternative materials and designs that will increase efficiency and durability.\nSolar Energy Systems for the Long Haul\nSolar power systems’ variability to sun exposure, based on geography, climate, and date, is a continuous challenge for engineers. To account for periods and locations that receive minimal to zero sunlight, solar power engineers have experimented with different materials and chemicals’ power load capacities, battery and storage technology, and transmission systems (1). A broader approach is a systemic change that places solar and wind energy on the power grid, alongside existing electricity grids that are primarily powered by coal and fossil fuels, also known as system value (SV) (5).\nProgress in solar power technology has been driven by increasing awareness of climate change, initiatives to counter act global warming, and economic incentives through lower utility costs and public policy standards. Solar engineers are expected to meet the rising demands of government officials, consumers, and businesses by mid-century (5). Engineers and manufactures will be held responsible for increasing production in the coming years, while continuing to develop solar technology that will maximize energy efficiency, sustainability, and distribution.\n(1) Hertwich, E.G., Alosisi de Larderel, J., Arvesen, A., Bayer, P., Bergesen, J., Bouman, E., Gilbon, T., Heath, G., Pena, C., Purhit, P., Ramirez, A., Suh, S. Green Energy Choices: The benefits, risks and trade-offs of low-carbon technologies for electricity production.\n(2) “Renewables: About solar photovoltaics.” International Energy Agency. n.d. 14 Jun. 2016.\nRenewable Energy Policy Network for the 21st Century. Renewables 2016: Global Status Report. REN21, 2016. 13 Jun 2016.\n(3) Cantwell, Maria, Sen. Hearing on Near-Term Outlooks for Energy and Commodity Markets, U.S. Senate Committee on Energy & Natural Resources. 366 Dirksen Senate Office Building, Washington D.C. 19 Jan. 2016. Opening Statement. 13 Jun 2016.\n(4) Mueller, Simon. Next Generation Wind and Solar Power, From cost to value. International Energy Agency and Clean Energy Ministerial, 2016. 14 Jun. 2016\nImage: © Tangencial | Dreamstime.com - <a href=\"http://www.dreamstime.com/stock-photos-solar-thermal-power-plant-image23345363#res14972580\">Solar thermal power plant</a>"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:da46003b-e105-45fa-b16b-666a736a8efd>","<urn:uuid:0913cb95-c253-4c6c-8417-adb35ebbe7de>"],"error":null}
{"question":"How was the Playfair cipher originally created and what interesting incident led to its public demonstration?","answer":"The Playfair cipher was actually created by Sir Charles Wheatstone, not Playfair. Wheatstone and Baron Playfair of St. Andrew's shared cryptography as a hobby and would break coded messages in the London Times. In one notable incident, they broke a code used between an Oxford student and a married London lady who were planning to elope. Wheatstone warned the lady using their own cipher, resulting in a response saying 'Charles, don't write anymore; our cipher has been broken!' Although Wheatstone invented this superior cipher system, when Baron Playfair published it, he gave proper credit to Wheatstone. Despite this, it became known as the Playfair cipher.","context":["The playfair cipher was created by Sir Charles Wheatstone\n(known for the Wheatstone bridge\n). Wheatstone and Baron\nPlayfair of St. Andrew's both had cryptography as a serious hobby.\nThe London Times frequently carried private advertisements done in code and the two men amused themselves by breaking the code and following the correspondence. One particular correspondence was between a student at Oxford and a married lady in London. At one point, the young man suggested that they should elope. Wheatstone ran a coded message of his own in the cipher used by the couple in which he admonished the lady. One message followed using that cipher - \"Charles, don't write anymore; our cipher has been broken!\"\nWheatstone at that time had a superior cipher system which he had invented. His friend Baron Playfair published it giving proper credit to Wheatstone for its invention. Nevertheless, it is known as the Playfair cipher.\nFrequently a mixed alphabet is used with a mnemonic - taking a shared word and then removing all the letters that occur twice. For the following example the word 'ceaser' (yes, I know its misspelled - I didn't catch the spelling error until after I did the encryption. It is still a 'good' key) will be used. After that word has been written down, the remaining letters of the alphabet follow. This is all written down in a 5x5 box. Changes are necessary to make an alphabet fit this grid. The most common changes are (only one change is necessary):\n- 'i' -> 'j'\n- 'j' -> 'ii'\n- 'W' -> 'VV'\n- 'U' -> 'V'\n- no 'X'\nC E A S R\nB D F G H\nI K L M N\nO P Q T U\nV W X Y Z\nNext, the plaintext is written out with no punctuation in a straight stream. The letters are then divided into pairs. A solitary letter has an arbitrary letter appended to it. Doubled letters have a obvious character (called a 'null') placed between them.\noriginal: hello there bob\npaired stream: he ll ot he re bo b\nmodified stream: he lx lo th er eb ob\nciphered stream: rd qa qi gu ac dc vi\nThere are three possibilities:\n- Both letters are in the same row\nThe characters 'ER' appear in the same row in the above block, and thus they are shifted one set to the right, wrapping around as necessary. 'ER' becomes 'AC'.\n- Both letters are in the same column\nThe characters 'OB' appear in same column, thus to encipher these, the characters one position below are used. 'OB' becomes 'VI'.\n- The letters are neither in the same row or column\nWhen the letters are not in the same row or column, a rectangle is formed with them, and the opposite corners are chosen, with the swap happening between letters in the same column. 'HE' become 'RD' and 'EB' becomes 'DC'.\nIt is easy to remember this system, and thus quite useful on the go. Furthermore, it is easy to change the key and thus the table. It can be cracked with enough texts. Instead of individual letter frequency, it now requires the analysis of letter pair frequency - much more difficult. A further enhancement upon this is to use a serration so that the plaintext of 'hello there bob' becomes:\nplain text: helloth\nHere, the vertical pairs are used rather than horizontal making common pairings such as 'TH' more difficult to find.\nIn World War II, the Playfair cypher was modified to be applied again making the unauthorized decoder's life a bit more difficult. This was called 'Doppelkasten' (double box) by the Germans.\nTwo different boxes are used with different texts as keys. For the left box in this example, the key used will be the 'ceaser' key as above. For the right box, the key 'the quick brown fox jumped over the lazy dogs' (this happens to have each letter at least once).\nC E A S R | T H E Q U\nB D F G H | I C K B R\nI K L M N | O W N F X\nO P Q T U | M P D V L\nV W X Y Z | A Z Y G S\nFor encoding, the plain text is arranged in an even number of equal\nlines. Extra characters (nulls) are added as necessary to the end.\nThe message \"Everything is great, we love it all\" would become:\nThe pair of letters 'EN' is then encoded. Draw a line between 'E' on the left hand box to the 'N' on the right hand box. This is then mirrored to become 'K' from the left hand side and 'E' from the right hand side. This pair is then encoded a second time: 'KE' becomes 'NE'. This flipping is an artifact of the fact that 'E' and 'K' are in the same column in both tables.\nWhat happens if both are on the same row as 'VG' are? This invokes an exception. This often happens if both of the keys are of the same length and the letter pair is in the last row (frequently the same or similar) The pair is mirrored and then displaced by one letter to the left. If this causes it to wrap, it stays in the same box. 'VG' then becomes 'ZY'. The repetition of this shifts it again to 'YA'\n'EI' becomes 'TD' which then becomes 'QP' (invoking the above rule).\n'RS' becomes 'UZ' which in turns becomes 'PU'.\nAnd thus, our cypher text begins:\nDuring World War II, the boxes where changed every 3 hours, however there was enough traffic to provide cryptanalysts enough text to decode these. At the end of the war, the double encryption was relaxed to single encryption.\nThe box of the playfair cypher can be extended to a 3x9 (A-Z, &) or a 6x6 (A-Z and 0-9)"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:7902125f-420a-4345-a334-dd72399a5712>"],"error":null}
{"question":"How do authentication methods for digital repositories compare with email security measures in terms of protecting user access and content privacy?","answer":"Digital repositories and email systems use different but complementary security approaches. Digital repositories employ multiple authentication methods such as login ID and password, IP filtering, web cookies, challenge-response authentication, and referring URLs to validate user identity and control access to resources. They also use digital watermarking and digital signatures to protect and track digital materials. In comparison, email security focuses primarily on encryption through public-key systems to ensure content privacy. While repository authentication aims to control who can access specific resources and verify content integrity through multiple layers of security, email encryption specifically focuses on keeping message contents private by using recipient public keys for encryption and private keys for decryption. Both systems ultimately aim to protect information, but repositories emphasize access control and content authenticity while email security prioritizes message confidentiality.","context":["Authentication and Authorization: Security Issues for Institutional Digital Repositories\nIn this digital age and the development of Information and Communication Technology (ICT) many organizations have realized the benefits of sharing information within the organization as well within the community and globally. These organizations may be corporate company, research organization or academic institutions. In the academic institutions with the higher education, information capturing, dissemination and sharing is practiced most. In spite of Open Source Drive , in the highly competitive environment, many university or colleges raise a paradox between allowing information and knowledge to flow freely, and the need to keep certain information very secure. In restricted or closed-information environment secured information channel, authorization and authentication of both users and digital contents are a burning issue today. Digital contents are managed and stored in repository to share. Repository of an institution can support research, learning, and administrative processes as well as purposes. Standards are followed for the repositories which ensure that the contents contain is accessible in that and it can be searched and retrieved for later use. A wide variety of contents may be included in the digital repositories for the multiplicity of purposes and users. It is the technical ability and administrative policy decision that what kind of materials goes into a repository (Jones, et al 2006). A proper digital repository not only requires an organized collection of digitized content, it also requires that the content be accessed and distributed as widely as possible to legitimate users around the globe. Access management and control is one of the major concerns for content-providers on the Internet. Without a proper access management mechanism confidentiality and integrity of information cannot be guaranteed. Different conventional methods are practiced by the content-providers but not a single method is sufficient for access management (Ray and Chakraborty, 2006). However, the administrators of the digital content-providers mostly expect their preferences for the technology or the procedure to be available which may be best practiced globally.\nInitially, substantial amount of literatures have been reviewed to come up with an idea for formulating this paper which is a review by type. The researches, practices, progresses, development and successes for the access management specially authentication and authorization are reviewed to see the global practices by the repository administrators or managers. Even in Bangladesh there are very few repositories, and the repository managers are interviewed by the author though all are examined also to see the status. Though there are very common types of process or methods observed where traditional or built in securities of repository software or operating systems are adopted most. Based on review and local managers interview this paper gives idea about the current practices about authentication and authorization.\nAccess management typically is a combination of users' authentication and authorization, access permission operations, policies for license agreement and digital materials authentications or digital rights management. Authentication is the process of determining the validity of a user who claims to be, and authorization is the process of determining what resources a user is permitted to access. Digital Rights Management (DRM) is a system of solutions created or designed as a means to prevent unauthorized access, duplication and illegal distribution of copyrighted digital media. In online environment, the scope of DRM can be leveraged to control access to and usage of digital objects and to impose restrictions on their misuse (Functional Groups, 2009). Access Management ensures security of resources on servers but also during communication to ensure authenticity and integrity of data. It is possible for an unauthorized user to snoop on communication between a user's browser and a Web server and hack sensitive information. Occurrences of unauthorized user getting access to important Web sites and defacing them are not uncommon. Electronic content can be copied very easily, it is essential to impose measures to control misuse of digital content. IP authentication and password-based access, two most commonly used authentication methods, are not able to protect the content from being duplicated or shared. Access Management is necessary most for commercial digital contents because their access is restricted to its subscribers or licensed users. Even when access to digital collections is provided openly, access control is required for assigning responsibilities for operations such as, additions, updating, editing and deleting or with-drawing content, and other tasks related to digital collections. Other reasons to control access to materials in a repository may include confidentiality of resources. Tracking of all changes made so that the collections can be restored if any system error occurred. In access management as other matters are related to policy or administrative decisions the user authentication, authorization and digital material authentication are most necessary issues.\nUser Authentication: User Validity\nA user authenticates with his or her organizational or personal identification. The identity provider passes the minimal identity information necessary to the service manager for authentication to enable an authentication decision (ACM, 2009). Digital identities are increasingly being used to facilitate the execution of transactions in various domains. When developing and analyzing digital identity technologies, it is important to consider the type and objective of repository, type of digital content, security of the system, security of communication channel, diversity of users' platform, number of users, even the perceptions and responses of end users also. Different authentication processes are as follows:\nThe most common and familiar authentication process is Log-in ID and Password-based Access (Antón, 2007). Log-in is also called log on, sign in, or sign on which identifies oneself to the system in order to obtain access. The primary use of a computer login procedure is to authenticate the identity of any computer user or computer software attempting to access the computer's services (Logging, 2002). Another popular authentication process is IP Filtering or IP authentication. This process is a packet filter that analyzes TCP/IP packets. That is software routine that analyzes incoming data packets and forwards them or discards them based on one or more criteria such as address, range of addresses and types (IP filter, 2009). Institutions or organizations are encouraged to register for accessing digital contents using IP addresses (ranges) if they are static. This allows for: seamless access (no logon screen), usage statistics for the institution, greater security as no misuse of usernames and passwords, can allow for access for all computers on campus to resources and much more (INASP, 2009). Web Cookie is another process of user authentication, which can be used by a server to recognize previously-authenticated users and to personalize the web pages of a site depending on the preferences of a user (Cookies, 2009). A cookie is a token that the web browser stores on disk in the form of a small text file. Cookies provide a way to track individual users' usage of website. Web Proxy is another way to authenticate. Most proxy programs like Squid, NetCache provide a mechanism to deny access to certain URLs in a blacklist, thus providing content filtering. A content filtering proxy will often support user authentication, to control web access. EZproxy (2008) is also a web proxy server program that provides users with remote access to Web-based licensed content offered by libraries. It is middleware that authenticates library users against local authentication systems and provides remote access to licensed content based on the user's authorization (Proxy server, 2008). Another method Challenge-Response Authentication is used to prove the identity of a user logging into the network. When a user logs on, the network access server, wireless access point or authentication server creates a “challenge,” which is typically a random number sent to the client machine. The client software uses its password or a secret key to encrypt the challenge via an encryption algorithm or a one-way hash function and sends the result back to the network (the “response”). The authentication system also performs the same cryptographic process on the challenge and compares its result to the response from the client. If they match, the authentication system has verified that the user has the correct password (Challenge-response authentication, 2008). Referring URL is a way to authenticate users. From the point of view of a web page or resource, the referrer, or HTTP referrer, identifies the address of the webpage or URL, the more generic URI of the resource which links to it. By checking the referrer, the new page can see where the request came from. referrer logging is used to allow websites and web servers to identify where people are visiting them from, for promotional or security purposes. referrer is a popular tool to combat Cross-site request forgery, but such security mechanisms do not work when the referrer is disabled (HTTP referrer, 2008). referrer is widely used for statistical purposes.\nUser Authorization: Resource Access Permission\nAuthorization defines users' permissions in terms of access to digital resources and extent of its usage. Authorization is granted to the successfully authenticate users according to his/her rights information available in the Access Management System (AMS) (Lynch, 2009). Authorization also addresses the issue of responsibilities assigned to different personnel involved in development of a digital repository/library and their respective authorities in terms of addition, deletion, editing and uploading of records into a digital collection. Authorization is more challenging than authentication, especially for widely distributed digital content providers.\nConventional access control architecture denotes an access control policy as a subject (user) is authorized to exercise some permission on an object. This usual model implicitly assumes that the user population is known more or less. But in a digital content environment the user population is vast, dynamic and impossible to predict all the users. Thus conventional authorization or access control mechanisms that rely on knowing the user and associating permissions with them fail significantly in digital repositories. So, this digital environment demands some further challenge for access control (Bertino, 2002). The access control policies are often based on user qualifications and characteristics. In one of the early works on access control in digital repositories or libraries, Gladney (1997) proposes a scheme called DACM (Document Access Control Methods), where the basic idea is geared toward flexible access control with some extensions to handle mandatory access control. Blaze has proposed credential-based access control (Blaze, 1996), to address the problem of unknown users. In these models a user has to produce one or more testimonials that have been certified by one or more third parties. The credential provides information about the rights, qualifications, responsibilities and other characteristics attributable to its bearer by the third parties. These third parties need to be trusted by the service provider. Winslett (1997) developed a credential based security and privacy related system for enforcing access control in digital contents of repository or system. Access to systems containing protected information resources must be managed based on one or multiple selections of the alternative access control methods. However, different methods are based on, Users identity, Role, Policy, Content Dependency, Context, View, Time, Physical Location, Network Node, Mandatory, and Discretionary. In-addition, a risk assessment is needed to conduct to identify the data or resource risk and severity prior to establishing the level and selection of access controls or authorization to digital contents (Access control, 2009).\nDigital Materials Authentication\nDigital Watermarking and Digital Signature are very common in use to provide a range of solutions for identifying, securing, managing and tracking digital materials (Stallings, 2003). In different types of objects like audio, video, still images and printed documents Digital Watermarking technologies allow users to embed. This technology permits digital code that is imperceptible during normal use but readable by computers and software. The major purpose of digital watermarks is to provide protection for intellectual property that is in digital format. This system does not prevent copying, but ensures that any copies made of the media will be traceable to a particular copy and perhaps to a particular user. In this process, also referred to as data embedding, information hiding, or simply watermarking, a pattern of bits is inserted into a digital image, audio or video file that identifies the file's ownership and can convey additional information like copyright. Unlike printed watermarks, which are intended to be somewhat visible, digital watermarks are designed to be completely invisible, or in the case of audio clips, inaudible. Moreover, the actual bits representing the watermark must be scattered throughout the file in such a way that they cannot be identified and manipulated. And finally, the digital watermark must be robust enough so that it can withstand normal changes to the file, such as rotation, filtering or the application of compression algorithms such as JPEG that discard some of the original data (Watermarking, 2009). On the other hand Digital Signature is an electronic signature which authenticates to identify the sender of the message where original document has been sent remain unchanged. It is easily transportable, cannot be reproduce by someone else, and can be automatically time-stamped. In addition the message is sent, the sender cannot easily reject it later. A digital signature can be used with any kind of message, whether it is encrypted or not, simply so that the receiver can be sure of the sender's identity and that the message arrived intact. A digital certificate contains the digital signature of the certificate-issuing authority so that anyone can verify that the certificate is real (Digital Signature, 2009).\nA digital signature scheme typically consists of three algorithms:\nb. A signing algorithm which, given a message and a private key, produces a signature.\nc. A signature verifying algorithm which given a message, public key and a signature, either accepts or rejects.\nTwo main properties are required. First, a signature generated from a fixed message and fixed private key should verify on that message and the corresponding public key. Secondly, it should be computationally infeasible to generate a valid signature for a party who does not possess the private key (Brands, 2000) .\nAs institutions, information strategies which call for sharing and licensing access to information resources in the networked environment, authentication and access management have emerged as major issues which threaten to hinder progress. While considerable work has been done over the last two decades on authentication within institutions and, more recently, in support of digital repository, a series of new technical and policy issues emerge in the cross-organizational authentication and access management context. In this paper it has been illustrated the secured process digital repository which ultimate objective is information sharing and dissemination. Though it has not shown any model or architecture regarding any types of technical aspects whether it has been tried to introduce the terms and methods or even the factors for access management for digital content provider. A lot of work, however still remains to be done. In future the authors will work on few open source technology for access management of digital library by which the digital content provider can find a solution to gather the working process of those for implementation.\nAbout Digital Watermarking. Available: http://www.willamette.edu/wits/idc/mmcamp/watermarking.htm\nAccess control criteria for right to use automated information resources. Available: http://michigan.gov/documents/Policy_1350_157471_7.40_Access_Control_Final_PDF.pdf\nAntón, L., Jones, A., Earp, J.B. (2007). Towards understanding user perceptions of authenticationtechnologies. Proceedings of the 2007 ACM Workshop on Privacy in Electronic Society. Virginia : ACM.\nBertino, E., Ferrari, E., & Perego, E. (2002). Max: An access control system for digital libraries and the web. Oxford, UK: Proceedings of the 26th IEEE International Computer Software and Applications Conference.\nBlaze, M., Feigenbaum, J., & Lacy, J. (1996). Decentralized trust management. Oakland, CA: Proceedings of the 1996 IEEE Symposium on Security and Privacy.\nBrands, S.A. (2000). Rethinking public key infrastructures and digital certificates: Building in privacy. London: MIT Press.\nChallenge-response authentication definition. Available: http://encyclopedia2.thefreedictionary.com/Challenge-response+authentication\nEZproxy: OCLC – Web and Data Services. Available: http://www.oclc.org/us/en/ezproxy/default.htm\nFAQs on Information resources. Available: http://www.inasp.info/file/188/faqs-on-information-resources.html\nFunctional groups: Access management R & D. Available: http://www.inflibnet.ac.in/functionalgroup/openaccess.html\nGladney, H.M. (1997). Access control for large collections. ACM Transactions on Information Systems 15: 154–194.\nHTTP referrer. http://en.wikipedia.org/wiki/Referrer\nIP filter definition of IP filter in the free online encyclopedia. Available: ttp://encyclopedia2.thefreedictionary.com/IP+filter\nJones, R., et al. (2006). The Institutional Repository. Oxford: Chandos.\nLogging (computer security). Available: http://en.wikipedia.org/wiki/Logging_%28computer_security%29\nLynch, C. (n.d.). A white paper on authentication and access management issues in cross-organizational use of networked information resources. http://www.cni.org/projects/authentication/CNI_authentication.doc\nProxy server. http://en.wikipedia.org/wiki/Web_proxy\nRay, I., & Chakraborty. S. (2006). A framework for flexible access control in digital library systems. Data and Applications Security : 252–266.\nStallings, W. (2003). Cryptography and network security: Principles and practice . New Delhi: Pearson Education.\nTowards understanding user perceptions of authentication technologies. http://portal.acm.org/citation.cfm?doid=1314333.1314352\nWeb handbook – Cookies. Available: http://archive.cabinetoffice.gov.uk/e-government/resources/handbook/html/4-7.asp\nWinslett, M., et al. (1997). Assuring security and privacy for digital library transactions on the Web: Client and server security policies. Proceedings of the IEEE international forum on Research and Technology Advances in Digital Libraries, Washington , DC , USA , pp. 140-151.\nWhat is digital signature? – a definition from whatis.com. Available: http://searchsecurity.techtarget.com/sDefinition/0,,sid14_gci211953,00.html","What are Digital Signatures?\nAs an inevitable consequence of the Internet, electronic communication has become acceptable in many contexts, altogether replacing traditional written communication. However, traditional documents were usually validated with the presence of a signature, which is more difficult to attest on an electronic document. A digital signature conceptually mimics a person’s unique signature, in that it validates an electronic document.\nDigital signatures are composed of two elements – a message hash and a private key. A message hash is a uniquely generated sequence of numbers, which cannot be reverse engineered to obtain the original message. The hash is then encrypted using the sender’s private key. The recipient decrypts the hash using the sender’s public key. The electronic document is also run through the hashing algorithm to check whether both the hashes are the same- thereby confirming the sender did indeed author the document in its current form, and it was not altered in any way before reaching its intended recipient.\nUnderstanding the Email Encryption Process\nThere are a number of methods by which emails can be encrypted for secure transfer, and public-key encryption is one of them. The premise is similar to that used when digitally signing electronic documents, in that there is a pair of keys generated- one private and the other public.\nIn the case of email encryption, the idea is to keep the contents secure from all unauthorized viewing. Therefore a sender identifies a recipient’s public key, and encrypts the email with that key. The private key is retained with the recipient, and is used to decrypt all communication encrypted using the corresponding public key.\nUsing this method, any communication encrypted using the recipient’s public key can only be read by those people with access to the correct private key. Therefore all senders can be assured of complete privacy of their emails, given that the private key is secure.\nComparison of Digital Signatures and Email Encryption\nAlthough both methods of encryption use asymmetrical keys, the goal of a digital signature and that of email encryption are entirely different. A digital signature is used to verify that a particular electronic document was created by a particular individual and has not been altered in the transmission process. The process is used to authenticate the author and the contents of the document beyond a shadow of doubt. Email encryption, on the other hand, is used to maintain the privacy of the contents of an email. Generally, information that should not be privy to everyone is subject to email encryption.\nWhen implementing digital signatures, the public key is used for decryption while its corresponding private key is used for encryption. The process for email encryption is exactly the opposite.\nEmail encryption and digital signatures are certainly not mutually exclusive, even though they have differences. There are occasions where establishing the identity of an email’s author is equally as important as maintaining the security of its contents. This is a scenario where both technologies would be used in conjunction with each other."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:78455cbc-3b75-43c3-8b02-5b9a48f269be>","<urn:uuid:ac6818a5-ca22-4c61-9ce6-514c8477ff4f>"],"error":null}
{"question":"How did the Cuban revolutionaries in 1895 and early Mexican independence movements compare in their attitudes toward Spanish colonial power and U.S. involvement?","answer":"The Cuban revolutionaries of 1895 actively sought U.S. support in their fight against Spain, even drawing parallels between their struggle and the American Revolutionary War, comparing their battles to those at Saratoga and Yorktown. In contrast, Mexico's early independence period showed a more complex and adversarial relationship with both powers - they supported Cuban separatist movements in the 1820s primarily to prevent Spain from using Cuba as a base to reconquer Mexico, while simultaneously viewing U.S. expansionism as a threat to their territorial integrity. Mexico's goal was to break away from Spanish control while also maintaining independence from growing U.S. influence, whereas Cuban revolutionaries initially welcomed U.S. involvement, though this ultimately led to U.S. control through the Platt Amendment.","context":["U.S. dominance of North American seems certain and inevitable. After all, as a regional power, the U.S. has been unchallenged for more than a century, and as the sole global power for more than 25 years.\nIt can be easy to forget that Mexico briefly served as a foil to U.S. regional power. Looking at this facet of the U.S.-Mexico relationship requires examining how the countries compete and interact in the region.\nBeyond the obvious shared border, their rivalry is expressed most notably in the Caribbean, particularly in Cuba, where Washington and Mexico City have competing strategic interests. We are currently in a relative lull regarding this bilateral competition in the Caribbean, but in the decades ahead there is potential for it to resume.\nIn North America’s immediate post-colonial period, Mexico and the United States emerged as the leading candidates for regional powers. The two were rather evenly matched. It was not clear who would emerge as a regional power – if anything, by the early 1800s Mexico appeared to be the more likely candidate of the two.\nAfter its independence from Spain, Mexico held territory stretching north nearly to Canada and west to present-day California. U.S. territory after its independence was initially relegated to the strip of land between the Appalachian Mountains and the Atlantic Ocean.\nTo survive and thrive, the U.S. would need to expand. The two countries thus had fundamentally different mentalities. The U.S. required ambitious expansion while Mexico needed to maintain its territorial status quo and fend off any European attempts at reconquest.\nU.S. expansionism threatened not only Mexico’s territorial integrity but also Mexico’s strategic interests in the Caribbean. The U.S. and Mexico had competing interests in the Caribbean, particularly over Cuba.\nSituated as it is between Florida and the Yucatan Peninsula, Cuba is strategically valuable as a way to block maritime traffic from the Gulf of Mexico to the Atlantic Ocean. These waters are the only way out of the Gulf of Mexico; if closed, they impede the transit of goods between vital ports (New Orleans in the U.S. and Veracruz in Mexico) and shipping routes.\nFor the United States, Cuba represented an enormous economic vulnerability. The Mississippi River runs through the agricultural heartland of the United States and empties into the Gulf of Mexico at New Orleans. Other major rivers in the country’s main agricultural zones feed into the Mississippi as well, and in doing so enable massive amounts of agricultural exports.\nAny major disruption in the maritime access from the Gulf of Mexico to the Atlantic Ocean would cripple U.S. economic activity. It was for this reason that, in 1823, U.S. Secretary of State John Quincy Adams informed U.S. diplomats of the U.S. desire to annex all of Cuba within half a century.\nIn the early days of independence, Cuba was an economic vulnerability and security threat to Mexico. When Mexico’s break with Spain was finally recognized in 1821, Cuba was still under Spanish control. Prior to its own independence, Mexico had strong trade ties with Cuba, which was both a market and transit hub for European markets.\nAfter independence, trade subsided – Spain was not all that interested in helping an economy that was hostile to its interests. From a security standpoint, Cuba was a favorable location from which to stage an attack on Mexico, particularly by Spain to regain territory or by other European power seeking to scoop up the former colony.\nTo mitigate these threats, the Mexican government began to support separatist ideologies and movements in Cuba in the 1820s. The goal was to use separatists to prevent Spain from being able to reconquer Mexico. For Mexico, this would ideally entail Cuba’s breaking away from Spain. This would enhance Mexico’s position in the Caribbean Basin and raise the possibility of actually acquiring Cuba itself.\nIt’s little wonder that the U.S. and Mexico’s first major dispute in the Caribbean would be over Cuba, after the Mexican-American War. Washington’s victory gave it what is now the west and southwest portions of the United States, fulfilling its imperative of westward expansion.\nHaving secured the Mississippi River Basin, the lands west of it, and territory toward the border with Canada, the United States could begin to advance its interests outside the mainland to its periphery, starting with the Caribbean. Washington meant to secure its waterways from the Gulf to the Atlantic. It supported a pro-Spanish Cuba and at one point even attempted to buy the island from the Spaniards. Thus began the U.S.-Mexico competition for influence and security interests in the Caribbean.\nDomestic issues in both countries put a momentary hold on this competition. The U.S. descended in to a civil war in the 1860s. During the same decade, Mexico went through its own version of a civil war over the reinstallation of a European monarchy in the Mexican government.\nBy 1868, hostilities had ceased and each country entered periods of reconstruction. Cuba, on the other hand, was descending into what would be a decade of war after failing to achieve full independence from Spain. By the time Cuba made its second attempt at independence (1895-1898), the U.S. and Mexico had stabilized and largely recovered from their domestic wars.\nOnce again, however, their interests pitted them against each other, this time during Cuba’s push for independence. For the U.S., the Cuban war for independence was an opportunity to expand into the Caribbean.\nFor Mexico, the war was more complicated. On the one hand, an independent Cuba helped ensure Spain’s departure from the region. On the other, Mexico and Spain had built strong trade ties. President Porfirio Diaz found himself with two strong opposing forces over the question of Cuban independence. Rather than risk alienating one of these domestic groups or Cuba or Spain entirely, Mexico pursued more diplomatic, indirect support for Cuban independence.\nLess encumbered, the U.S. formally intervened in Cuba (and Puerto Rico). The result was Cuban independence from Spain but very much under the protection and influence of the United States. This influence was formalized in Cuba’s 1901 Constitution through the Platt amendment, which opened up the island to military installations and U.S. military activity in the event of another civil war or domestic problem.\nDecisive U.S. control over Cuba ushered in an era in which the U.S. presented itself as the protector of the Caribbean while Mexico, for its part, presented itself as an alternative to and rejecter of U.S. intervention in the Caribbean. Washington followed up its actions in Cuba with subsequent government and military interventions in places like the Dominican Republic and Haiti.\nMexico responded with attempts to counterbalance and, again, reject U.S. power in the region. Its primary tools in this regard included soft power (common language, historic and cultural ties, trade, etc.) and alliances and cooperation with Europe. The U.S. used hard, military power, which was superior to Mexico’s.\nMeanwhile, Europe was already dealing with the geopolitical tensions that would ultimately lead to World War I, relegating support to Mexico as a secondary interest. In short, Mexico had only limited options in curbing U.S. power in the Caribbean.\nAfter its military incursions into Mexico in 1914 and 1917, the U.S. emerged as the victor in the bilateral competition for regional power. In 1910, Diaz announced plans to step down from office after serving as the country’s head of state for more than 30 years.\nThis milestone political transition led to a decade of revolutions and counter-revolutions in Mexico. The fighting and general chaos were detrimental to U.S. business interests (to say nothing of the damage they inflicted on Mexico). Pressure grew on Washington to protect its interests and eventually resulted in the occupation of the port of Veracruz by the U.S. Navy for seven months in 1914.\nSubsequently, fighting in northern Mexico spilled over in the United States, which had its army and cavalry ready to meet and pursue Mexican forces to ensure there was no additional spillover. Mexico spent the next two decades trying to consolidate political power and reconstruct its government. So focused was Mexico on its domestic issues – and the associated unrest they caused – that it could not project power abroad.\nSince the beginning of the 20th century, the U.S. has solidified and maintained its status as the uncontested regional leader in North America. Mexico’s domestic problems hamstrung its ability to enhance its power, while the U.S. continued to prosper.\nIn the 1950s and 1960s, Cuba once again stood at the forefront of competing U.S.-Mexican interests in the Caribbean. But by this time, the U.S. was significantly more powerful than Mexico and the latter could do little to challenge or compete with the former.\nMexico opted for a cautious approach toward Cuba, whereby it recognized and accepted the Castro regime on the grounds of self-determination and nonintervention. It thus supported a regime counter to U.S. interests but did not adopt any ideology or policy that would merit U.S. retribution or action against Mexico for showing such support.\nAnalyzing the competition between the U.S. and Mexico in the Caribbean in the 20th century is an exercise in brevity. Simply put, Mexico could not compete.\nWhile present-day Mexico is not in a position to challenge the U.S. in the Caribbean, it would be remiss to assume this will always be the case. Power dynamics are never static. Mexico had the geopolitical advantage over the U.S., and then lost it, in just the first 100 years of its independence. It’s not unreasonable to think Mexico could overtake the U.S once again in the next 100 years.\nUnderstanding why this is so requires an understanding of how geopolitical power shifts over time. Between the two world wars and the fall of the Soviet Union in the 20th century, the seat of global power moved from Europe to North America.\nUnlike much of the rest of the world, North America was spared the ravages of World War I and World War II, at least on its own territory. Its development and prosperity were therefore relatively uninterrupted. North America is, moreover, uniquely placed between the Atlantic and Pacific oceans for trade, which still takes place to an overwhelming degree in the Northern rather than Southern Hemisphere.\nThe most important source of geopolitical power in North America is access to these oceans. Other countries such as Canada or Mexico could assume the role of dominant power in the future. Such was the case during the European era. Spain, Portugal, France and the United Kingdom led the continent at different points in time.\nMexico isn’t yet as powerful as the United States, of course, but it is gaining ground. After spending much of the 20th century dealing with domestic political and economic turmoil, it has entered the 21st century on better terms and now boasts the worlds 15th largest economy. Mexico is not only awash in natural resources but is no longer dependent on commodities for economic growth.\nOver the past few decades, the country has developed high-value manufacturing centers in the north and cheap, basic manufacturing in the south. While most of Mexico’s current trade revolves around the U.S., the country is now exploring other export markets.\nFurther contributing to its economic dynamism is the presence of technology-based industry and services as well as significant capital inflow through remittances and foreign direct investment. And, unlike in the United States, Mexico’s demographic trends will ensure that the country’s population will not shrink and it will have the workforce needed to sustain economic growth in the decades ahead.\nMexico possesses many of the same geographic advantages that form the bedrock of U.S. power. Its demographics and economic diversity bode well for continued economic growth. Mexican immigration into the U.S. will also benefit Mexico in the long term. The power gap between the two countries is wide, but there are indications that it may narrow over time.","Beginning in 1895, Cuban revolutionaries began a war against Spain, their colonial power, in an attempt to become an independent nation. After having organized a new government, written a new constitution, and brought Spain to the brink of defeat, Cuba welcomed the United States’ entrance into the war on the revolutionary side in 1898. Together, Cuba and the US defeated Spain. To this day, in the United States this war is most commonly known as the Spanish-American War. Wait a second… What happened to Cuba?\nNoting its erasure from the historical record, in 1945 the Cuban national congress voted to refer to the war as the “Spanish-Cuban-American War” in all official records. After all, each of the three groups had participated in the war, which had been fought on Cuban ground and for the freedom of the Cuban people. During the war, eastern Cuba, where Guantanamo Bay is located, was the seat of revolutionary Cuba. Many of these revolutionaries drew on rhetoric from the American Revolutionary War in order to draw parallels between their fight and the American colonists’ fight in 1776. Their army was small and somewhat ragged, as had been George Washington’s army, but they fought bravely “‘like the heroes of Saratoga and Yorktown.’” By identifying their cause with that of the American colonists, and their battlefields with the battlefields of New York, the Cuban revolutionaries adopted these metaphors in order to label their war as a fight for freedom and independence.\nAlthough popular sentiment in the US supported the Cuban cause, the American government ultimately had different aims than Cuba. Obtaining a naval base to use as a coaling station at Guantanamo Bay made economic and geographic sense for the US to become a major presence in Latin America. Thus, against the wishes of the Cubans, the US refused to pull the military out of Cuba until it had been granted the base, and the right to intervene militarily in Cuba in order to help Cuba preserve its independence. According to Amy Kaplan, professor of English at the University of Pennsylvania, with “this logic of equating intervention with protection, Cuba’s independence becomes dependent on the U.S. right to violate its autonomy.”\nThese provisions, contained within the Platt Amendment of 1901, diminished the idea of “Cuba Libre,” and have had lasting consequences for GTMO. The Cubans’ revolutionary rhetoric has not had the same lasting impact on the physical landscape, and yet I believe it is an important piece of the war because it reminds us – Cubans, Americans, and the international community at large – that the United States has served both as an inspiration in wars of freedom and independence, and as a partial foil to the revolutionaries’ purpose. In order to understand fully the causes and consequences of any conflict – past or present – it is necessary to label correctly the conflict itself and the aims of the participating parties. If, in conversations of GTMO’s various incarnations, we identify all of the participants involved, and the motives driving all sides, we will better understand the possible decisions to be made in each story.\nPosted by Heather Wilson – New York University\n Perez, Louis A. The War of 1898: The United States & Cuba in History & Historiography. The University of North Carolina Press: 1998. 125-126.\n Hansen, Jonathan M. Guantanamo: An American History. Hill and Wang: 2011. 113.\n Kaplan, Amy. “Where is Guantanamo?” American Quarterly, Volume 57, Number 3, September 2005, 835."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:4e4f77c6-5377-4ed6-aaef-e9e524e64daa>","<urn:uuid:f5923eaa-943b-4890-a510-17106cc9af4d>"],"error":null}
{"question":"How do trip balances and IP67-rated washdown scales compare in terms of their weighing mechanisms and environmental protection?","answer":"Trip balances and IP67-rated washdown scales employ fundamentally different weighing mechanisms and environmental protection approaches. Trip balances use a traditional two-pan system on opposite sides of a lever, where the object is weighed against standard weights for measurement, similar to Egyptian scales. In contrast, IP67-rated washdown scales use modern electronic systems with advanced protection against water jets and waves, offering protection against temporary immersion. The trip balance has no specific environmental protection features mentioned, while IP67-rated washdown scales are specifically designed with sealed stainless steel construction to withstand thorough cleaning and wet environments.","context":["If you are a student of science, and to be more precise of chemistry, you will be familiar with a variety of scales and balances and similar equipment that are needed for accurately weighing ingredients for any chemical experiment. Available from various top brands in this industry, and analytical weighing balance should be very accurately designed so that the user gets the ultimate accurate results every time. These are highly sensitive equipment and can react to the minimum weight. That is the primary reason why it is being used for measuring things as light as dust. In this post, we will share everything that you should be known about analytical weight balances so that you can make the most of these machines. They are not for domestic and commercial use and mostly seen in laboratories.\nDifferent types of weighing balances\nThere are several types of weighing scales and balances which are used in research work and in the laboratories. Analytical weighing balance is perhaps the most complex of all of them. It is designed to measure the weight of the lightest elements precisely. Apart from this equipment, there are several other types of weighing balance that you will find in the lab:\nAnalytical weighing balance- These balances are used for measuring the mass of elements. They are very useful for chemical analysis works. There is a pan in the balance area and other parts include leveling pan, plump bob, etc. All these are enclosed in a glass container which makes sure that the delicate internal mechanisms are not damaged or affected in any way.\nTrip balance- these scales include two pans on the opposite sides of the lever and are the modern version of the Egyptian scales. The object is placed on one side of the scale and equivalent weights are placed on the other side. The sum of the standard weights will be equal to the mass of the object that is being measured.\nPlatform scales- this type of weighing machine works based on multiplying levers on a load soaring platform. The weight is transferred to a beam that can balance by moving the counter pieces and counterbalances the weight on the platform. These machines are useful for measuring the weights of drums and of animals in the veterinary clinic.\nWhy analytical balances are considered different from others?\nUnlike other weight machines, the analytical weight balance is more delicate and have very unique features that make it much different from the others. They can deliver very precise results as low as 0.01g to 1g. They are highly sensitive to any kind of changes and hence they are very accurate and are sealed inside a glass cabinet so that the weighing procedure is not affected by any external elements. However, they do not provide readability more than three decimal places. They are best suited for labs where such a small amount of weights can make a big difference. These machines are not meant for high weight values.\nFeatures of the analytical weighing balance\nThere are several features and functions of the analytical weighing balance. They come with counting and check counting application features. They are very useful for counting pills, small animals, ingredients, and unstable elements like anything liquid. Some of them also have percentage weighing which allows more efficient and accurate formulation and useful is chemistry and pharmaceutical application. You can also accumulate all the elements and get their combined weight which is very useful for finding out doses.\nGLP/ISO reports are essential in various laboratory work and these balances can easily help you find all these values. This means the researchers can get more time for their actual experiment and do not have to calculate and formulate the results which are already being done through these machines. However, these balances take a longer time to stabilize compared to precision balances.\nThese weigh are also seen in school labs for demonstration and application for science students. Compared to other types of weights these are more expensive and you will not find any daily application for these machines. Nonetheless, they are very useful and elemental when seen from the perspective of experimentations and science projects.\nIf you are buying any of these, make sure the balance is stable and accurately designed. You should go to a reputable manufacturer and not comprise the cost if you are looking for high quality. The analytical weighing scales are among the most indispensable laboratory equipment which you cannot do without especially if you are a student or practitioner of chemistry or pharmaceutical science.\nWhen you know about the various nuances associated with these weighing scales, you will be able to choose the best one which is suitable for the laboratory set-up. Moreover, you will also be able to understand its application better and put it to good use.","How to Use Washdown Scales\nWhen you’re talking about scales used for messy applications or in dirty, damp locations, the ability to thoroughly wash a scale is imperative. That's why scale manufacturers provide washdown scales.\nWhat is a washdown scale?\nA washdown scale is outfitted with IP-rated stainless steel construction and thorough sealing, enabling it to withstand thorough, frequent cleaning with a high-pressure washer or a hose. Washdown scales rated IP68 or above can be considered fully waterproof, while scales IP67-rated and lower offer advanced protection against water when compared with non-IP rated scales.\nWashdown scales are used because they allow for the easy removal of any residual particles after weighing, which minimises the possibility of cross-contamination, removes dirt build-up and helps eliminate the growth of bacteria, mould or other toxins.\nUsing a washdown scale\nA washdown scale is used within a variety of industries from food and catering to industrial and pharmaceutical. They are purpose made for wet environments and come with robust stainless steel pans that allow them to be washed without the risk of damaging the internal components of the scale. The fishing industry in particular use washdown waterproof scales for handling and weighing hauls of fish, large stainless steel platforms are often used to weigh fish in bulk, whilst fishmongers use trade approved washdown scales with price calculation features in order to buy and sell fish products by weight.\nWhy you should use a washdown scale for weighing food\nWashdown scales can be used by markets for food weighing applications including the weighing of fruit, vegetables and produce. For example, sacks of potatoes are both heavy and likely to accrue dirt on a weighing scale over time. Therefore, it is essential that you use weighing equipment that is robust, convenient and easy to clean.\nLet's take the potatoes example above to show how a washdown scale is ideal for food weighing:\n- Firstly, a bag of potatoes is likely to weigh enough to make lifting a challenge, the washdown scale features a low weighing pan that is convenient for placing heavy and bulky loads without the need for extended lifting.\n- Secondly, the scale can withstand heavy-duty weighing up to a 32kg capacity due to its stainless steel construction.\n- Lastly, weighing vegetables like potatoes or carrots is guaranteed to produce dirt and bacteria, the stainless steel pan is combined with a protected loadcell chamber which allows you to extensively clean the scale without the risk of internal damage and ensuring food hygiene standards.\nUsing washdown scales for building work\nDue to its versatile design, the washdown scale is ideal for building and industrial jobs from general construction work to mining. The scale’s stainless steel pan is useful for weighing building materials such as slate, limestone or brick for houses due to its robust material composition and easy-to-wash properties. For example, the scale can use checkweighing features for calculating the weight of tile shipments precisely, ensuring you have an ample amount of materials on-site for work to begin. The same checkweighing features can be used for weighing coal for local distribution and selling when using a trade approved washdown scale.\nIP ratings for weighing scales\nManufacturers produce scales with a variety of ingress protection ratings, or IP ratings. These ratings are given to devices based on their level of protection against intrusion of dust and water. By industry definition, washdown scales must have at least an IP65 rating.\n|1||Protected against solid objects greater than 50mm|\n|2||Protected against solid objects greater than 12mm|\n|3||Protected against solid objects greater than 2.5mm|\n|4||Protected against solid objects greater than 1.0mm|\n|5||Dust protected (small quantities OK)|\n|0||No special protection|\n|1||Water dripping/falling vertically|\n|2||Water sprayed at an angle (up to 15º degrees from the vertical)|\n|3||Spray water (any direction up to 60º degrees from the vertical)|\n|4||Spray water from all directions (limited ingress OK)|\n|5||Low-pressure water jets from all directions (limited ingress OK)|\n|6||High-pressure jets from all directions (limited ingress OK)|\n|7||Protection against strong water jets and waves.|\n|8||Protected against temporary immersion.|\n|9||Protected against prolonged effects of high-pressure immersion|\nWhat does an IP rating mean?\nThe International Electrotechnical Commission sets and regulates the standards for defining the type of protection. The IEC is the world’s leading organization that prepares and publishes international standards for all electrical, electronic and related technologies.\nWhat does this mean for someone who’s shopping for a washdown scale? When the internal mechanisms of these devices offer protection against dust and water, they are assigned an Ingress Protection (IP) rating. The IP rating is a number that corresponds with the level of protection. The degrees of ingress protection are expressed as “IP” followed by a two-digit number, such as IP65, IP66 and IP67. The numbers vary depending on the amount of protection provided.\nThe first digit in the IP rating designates how well the product is protected against particulate matter, such as dust. As the number increases, so does the amount of protection.\nThe second digit in the IP rating indicates how well the equipment is protected against water. Just as with the dust figure, a higher number means greater protection.\nInscale offers a selection of washdown scales that feature IP ratings of IP65, IP66, IP68 and IP69.\nIWS Waterproof Scale - Inscale\nOur own IWS waterproof scales features, durable stainless steel housing that has been completely sealed for ultimate water and dust protection. The IWS has built-in checkweighing applications with adjustable high and low limits as well as a vibrant LED display. This scale can easily fit on a worktop or workbench surface and is perfect for food production and general warehouse use.\nWith an IP65 rating, the kern SFB stainless steel platform scale is a good option for damp environments, such as those found in the food industry. The scale is made entirely from stainless steel and has a stainless steel load cell.\nAdam Equipment Warrior Washdown Scale - Adam Equipment\nThe Adam Equipment Warrior washdown scale offers an IP66 rating and is well-suited for harsh settings. Featuring a fully welded grade 304 stainless steel base, top pan, pillar and indicator, the Warrior can be washed thoroughly with water during or after a task without the risk of damage. A sealed load cell and load cell protection chamber complete the protection of the product.\nAn IP68 rating means the A&D SE high-performance washdown scale is dustproof, waterproof, and even submersible in water. Affordably priced, the SE features a n epoxy-treated, corrosion-resistant load cell that is waterproof. Painted mild steel basework and a stainless steel platform mean the SE can withstand demanding industrial settings where frequent cleaning is needed.\nThe popular SW high-pressure washdown scale series comprises of a basework, indicator and complete scale range containing an IP69 protection level. Ideally suited for food-related weighing tasks, the SW can withstand regular cleaning with high-pressure hosing and hot water. A hermetically sealed stainless steel load cell offers premium protection against water intrusion. The polycarbonate/ABS head casing solves condensation issues that occur when stainless steel scales are used in humid locations or areas with a wide temperature range."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:054bd3fe-f0c5-4970-a08d-ab270ae6587b>","<urn:uuid:0e9d57b0-4689-40b7-b2cf-322d4ceba80f>"],"error":null}
{"question":"What are the key differences between how color is approached in creating a brand's visual identity versus designing its logo specifically?","answer":"The approach to color differs significantly between brand visual identity and logo design. In brand visual identity, colors are addressed comprehensively as part of the overall brand aesthetic, including primary colors (used most often to convey atmosphere/mood) and secondary colors (used occasionally to break monotony), with all colors specified in multiple formats (Pantone, CMYK, RGB, and HEXA decimal) for consistent implementation across different media. For logo design, the color approach is more constrained and must follow specific rules: colors should be chosen from adjacent positions on the color wheel, avoid excessive brightness, and most importantly, the logo must remain effective in black and white, grayscale, and two-color versions. Additionally, colors in logo design should evoke specific feelings and moods - for example, red suggesting aggression, love, passion, and strength - and potentially create a distinctive brand association, like 'John Deere green'.","context":["When a brand is in the development phase, many will imbue a lot of their own personalities into the brands (i.e likes, dislikes, aesthetic appeal and so forth) but we have to ultimately remember that this is not personal, it’s business. Now there will be those that argue ‘It’s my business therefore it is personal’, and honestly, I understand. For example, I am currently working for a brand that’s truly personal to the owner (ethically sourced, fair trade and ecologically conscious), but when you’re too close to it, you may lose sight of the larger picture: this is a business. A brand guide helps bring back focus to the purpose of the brand.\nEveryone is going to have very different parts to a brand guide. When I design a brand guide, these are 3 sections:\n1 / The Brand\nThis section usually opens with a mission statement: a high level goal beyond making money. (After this point the order kind of gets wonky). In the brand section, there is always a part about who you are as a brand (not what you sell or produce). This is essentially what makes you different from all the other competitors out there. Next: the values, or pillars (as I like to call them). For example, Starbucks brand values are based around community involvement, ethical sourcing, environmental initiatives and diversity. Now we can’t forget about the history. This isn’t a 5-page long essay; it is literally a paragraph to three max! And of course, the alter-ego.\nWe are Sasha Fiercing this. Beyoncé has mentioned that she has had stage fright and to combat this, created an alter-ego to help her. When she’s on stage she’s no longer simply Beyoncé (when is Beyoncé ever just simple?), but her alter-ego Sasha Fierce – also the name of her third studio album – takes over. Now she no longer needs Sasha because she’s fierce enough.\nMy approach to the alter-ego section is to give your brand a human persona. For example, Bathorium named her Olivia and this section is written in the perspective of Olivia: how her life is going, what she likes, dislikes, what she’s attracted to. This is pretty much her life story. For Mint, we called her Pepper(mint). Same idea as mentioned before. I do this because when you understand who the brand is as a person, you start to design differently. You design with Olivia or Pepper in mind, you make decisions knowing if Olivia or Pepper would like it or not. Essentially, they are my target market when I design.\nYou can expand or reduce the Brand section and not necessarily follow this order. However, when you have these four parts you provide a solid foundation for your brand.\n2/ Visual Identity\nThere are no tricks here. It’s literally as it is said. What does your brand look like? This will take up a substantial page count on your guide.\nThe Visual Identity section often beings with the logo, but I do things a little differently. I begin with a mood board. A mood board is a quick glimpse of the brand’s tone and aesthetic. When looking at the mood board, you should be able to tell if the brand is clean and simple, rustic and holistic, or warm and cozy. By understanding the emotion the brand conveys, you design more effectively.\nNext is followed by the logo design in its truest form. The truest form means the correct colours, right kerning, and the emblem, if there is one. For example, I just finished designing a logo for Stray & Wander. The logo is simply “Stray & Wander” in true black. (A post about this logo will show up the following week. The post will include collateral pieces). It’s the logo that you use most often. Sometimes logos have emblems and sometimes they don’t.\nFollowing your logo is the colour portion. One brand may use black while another may use ash grey as their darkest colour. There are primary colours (colours used most often to convey atmosphere or mood) and then secondary colours (used once in a while to break up the monotony). These colours are listed in Pantone colours: CMYK, RGB and HEXA decimal. This is truly important because all designers will need these codes at some point during the design process.\nNext is your font. Literally all the typefaces your designers can use, from major heading to body text to fine print; considerations for a font-family for print and web; the language such as tone of speech, grammar, and specific word choices – pretty much all the grammar aesthetic component to your brand. You can literally put whatever you like in there as long as it assists with maintaining consistency and it’s on brand.\nThe final part is often not included but I think it’s really important, and that’s the photography aesthetic. Just like designers photographers have their own aesthetic. It’s important when they first join the brand that they should understand the photographic aesthetic, otherwise, you can tell when a brand switches photographers and most of the time, it’s not a smooth transition.\nJust like The Brand section, you can mix and match the order that best fits your needs, but this is usually how I approach it.\n3 / Do’s and Don’ts and Implementation\nThis final section is like do’s and don’t’s and how to use the brand guide. Pretty much the rules. For example: logos can’t be stretched, can’t be of a certain colour, and can’t be mixed with a certain font. These rules should be illustrated by mock ups or examples. For me, I usually show a social media post or a blog post that includes all the rules. How should the photography look? What’s the content like? Is it the correct tone?\nBrand guides are tricky and can be very confusing, but designing a guide helps to ground a lot of ideas and can provide clarity when you (in the thick of it) are lost. As your brand grows and evolves, so should your guide. This is a guideline after all, not law. It flexes to the changes in your brands development and should reflect the future of the brand.","The logo is the face of any brand — the very first impression — so its design is extremely important.\nWhen executed correctly, a logo is a powerful asset to your client’s brand.\nHowever, creating an effective visual representation of a brand requires much more than just graphic design.\nLike any line of work that involves a set of specific skills, logo design requires plenty of practice and experience for it to be successful; knowledge is definitely power for any graphic designer.\nFor this reason, we have outlined 12 essential rules to follow in order to design an effective logo.\n1. Preliminary Work Is a Must\nPreliminary sketches are an important first step in designing an effective logo.\nThese can be as simple as paper and pen drawings or drafts made using a vector program, such as Illustrator.\nThe bottom line is that you compromise the final result if you rush, or skip, this step.\nStart with 20 to 30 sketches or ideas and then branch out to create variations of the original ideas.\nIf nothing seems to work, start over and begin sketching new ideas.\nAn effective graphic designer will spend more time on this preliminary work than any other step in the design process.\n2. Create Balance\nBalance is important in logo design because our minds naturally perceive a balanced design as being pleasing and appealing.\nKeep your logo balanced by keeping the “weight” of the graphics, colors, and size equal on each side.\nThough the rule of balance can occasionally be broken, remember that your logo will be viewed by the masses, not just those with an eye for great art, so a balanced design is the safest approach.\n3. Size Matters\nWhen it comes to logo design, size does matter. A logo has to look good and be legible at all sizes.\nA logo is not effective if it loses too much definition when scaled down for letterheads, envelopes, and small promotional items. The logo also has to look good when used for larger formats, such as posters, billboards, and electronic formats such as TV and the Web.\nThe most reliable way to determine if a logo works at all sizes is to actually test it yourself.\nNote that the smallest scale is usually the hardest to get right, so start by printing the logo on a letterhead or envelope and see if it is still legible.\nYou can also test for large-scale rendering by printing a poster-sized version at a print shop.\n4. Clever Use of Color\nColor theory is complex, but designers who understand the basics are able to use color to their advantage.\nThe basic rules to keep in mind are:\n- Use colors near to each other on the color wheel (e.g. for a “warm” palette, use red, orange, and yellow hues).\n- Don’t use colors that are so bright that they are hard on the eyes.\n- The logo must also look good in black and white, grayscale, and two colors.\n- Breaking the rules sometimes is okay; just make sure you have a good reason to!\nKnowing how colors evoke feelings and moods is also important. For example, red can evoke feelings of aggression, love, passion, and strength.\nKeep this in mind as you try out different color combinations, and try to match the color to the overall tone and feel of the brand.\nPlaying around with individual colors on their own is another good idea. Some brands are recognizable solely by their distinct color.\nFor example, when you think of John Deere, you think of the “John Deere green” color, and this sets this brand apart from its competitors and, more importantly, makes the brand all the more recognizable.\n5. Design Style Should Suit the Company\nYou can use various design styles when creating a logo, and to pick the right one, you should have some background information about the client and the brand.\nA recent trend in logo design is the Web 2.0 style of 3D-looking logos, with “bubbly” graphics, gradients, and drop shadows.\nThis style may work well for a Web 2.0 website or tech company, but may not be effective for other kinds of brands.\nResearch your client and its audience before you begin your preliminary work.\nThis will help you determine the best design style from the start and save you from having to return repeatedly to the drawing board.\n6. Typography Matters… a Lot!\nChoosing the right font type and size is much more difficult than many beginner designers realize.\nIf your logo design includes text, either as part of the logo or in the tagline, you will need to spend time sorting through various font types — often, dozens of them — and testing them in your design before making a final decision.\nTry both serif fonts and sans-serif fonts as well as script, italics, bold, and custom fonts.\nConsider three main points when choosing a font to accompany your logo design:\n- Avoid the most commonly used fonts, such as Comic Sans, or else your design may come off as amateurish.\n- Make sure the font is legible when scaled down, especially with script fonts.\n- One font is ideal, and avoid more than two.\nStrongly consider a custom font for your design. The more original the font, the more it will distinguish the brand. Examples of successful logos that have a custom font are Yahoo!, Twitter, and Coca Cola.\n7. The Goal IS Recognition\nThe whole point of creating a logo is to build brand recognition. So, how do you go about doing this?\nWell, it varies from case to case, but the goal with the logo is for the average person to instantly call the brand to mind.\nA few examples of this are the logos for Coca-Cola, Pepsi, McDonald’s, and Nike.\nJust a glimpse of any of these logos is all you need to recognize the brands.\nThe key to making a popular and recognizable logo is to combine all of the elements discussed in this article: size, style, color, typography, and originality.\nOverlooking any of these during the design process will impair the quality of your final design. Examine your own logo design and see whether it meets all of these criteria.\nA quick test to determine if your logo is recognizable enough is to invert it using any graphic design software and see if you can still recognize the brand. Additionally, you should mirror the logo and see if it’s easily recognizable in this state.\nKeep in mind that logos aren’t always seen head-on in real world situations, for example, on the side of a bus or a billboard that you drive by.\nTherefore, you should make sure to view your logo design from all angles and ensure that it’s recognizable from any direction before submitting it to your client.\n8. Dare to be Different\nTo stand out from the competition, you must distinguish yourself as a designer with a distinct style. Rather than copy another design or style, be innovative and stand out from the crowd.\nSo, how can you be different? Try breaking the rules of design and taking risks.\nTry a variety of styles to find the one that works best for your client. Try different color combinations until you find one that makes your design truly original.\nHave fun with the design program you use, and keep tweaking the design until you feel you’ve got it right.\n9. K.I.S.S. (Keep it Simple, Stupid)\nThe simpler the logo, the more recognizable it will be.\nFor example, the Nike swoosh is an extremely simple logo and is also one of the most recognizable in the world.\nFollow the K.I.S.S. rule right from the start of the design process, when you are brainstorming ideas and doodling sketches.\nOften, you’ll find that you start with a relatively complicated design and end up with a simpler version of it in the end.\nWork the design down to its essentials and leave out all unnecessary elements.\n10. Go Easy on Effects\nAdobe Illustrator, Freehand, Photoshop, and other graphic design programs are extremely powerful tools and have many filters and effects that you can apply to your logo, but don’t get carried away!\nThere’s a time and place for these powerful tools, but it is not necessarily to design a logo.\nOf course, playing around and seeing whether they enhance a logo is fine, but just remember that simplicity is key.\n11. Develop a Design “Assembly Line”\nTo produce consistently high-quality logos, you need to develop your own design process, or “assembly line.” This should include the following steps:\n- Brainstorm and generate ideas\n- Preliminary sketches\n- Develop vector designs\n- Send to client\n- Add or remove anything the client wants\n- Finalize the design and resubmit to client\nAlthough you may want to tweak the order slightly, you should follow these basic steps with each logo design.\nThis will help you streamline your work, stay organized, maintain focus, and deliver better quality and more consistent results with each job.\n12. Use Other Designs for Inspiration Only!\nThe last rule for designing an effective logo is quite simple: don’t copy other designers’ work! While there’s nothing wrong with being inspired by other designers, copying another person’s ideas or work is morally and legally wrong.\nGallery websites exist that let you use vector art images free of charge, with proper attribution under the Creative Commons License, but I strongly recommend not going this route.\nThese websites can be helpful for getting ideas during the brainstorming stage, but you’re better off starting your design from scratch and making it 100% original.\nWritten exclusively for WDd by Jarkko Laine.\nDo you follow these rules when designing your logos? Why or why now? Please share your comments with us…"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:c53b42bb-84dc-47c7-96f4-6cab2b019110>","<urn:uuid:a2e2c2f3-b957-4e3d-b06d-87da7044a407>"],"error":null}
{"question":"How are commercial networks adapting to quantum cryptography, and what challenges do organizations face in this transition?","answer":"Commercial networks are beginning to adapt through trials like Telefónica and Huawei's successful implementation of QKD on SDN-managed commercial optical networks, which demonstrated that quantum and classical networks could converge on existing fiber infrastructure without requiring dedicated quantum networks. However, organizations face significant challenges in transitioning to quantum-safe systems, including re-engineering software and hardware, training staff, ensuring backward compatibility, implementing crypto-agility, and dealing with logistical issues like updating remote devices. The transition requires substantial investment and involves creating dedicated teams to inventory quantum-vulnerable systems, engage with vendors, and develop quantum-readiness roadmaps.","context":["Telefónica and Huawei announced they had conducted a successful field trial of quantum cryptography on commercial optical networks by using SDN.\nThe significance of the trial is that quantum cryptography could provide a solution for the vulnerability of current cryptographic key implementations. Today, cryptographic techniques encrypt data using a secure key, which is only known to the parties using that key for decrypting the messages between them.\nThose cryptographic techniques for key generation are based on highly complex mathematical problems that require long calculations to be resolved. With the growth of computational capacity, the time required to solve these problems becomes shorter, which reduces the security of the keys.\nWith the advent of quantum computers, the principles of quantum mechanics could be applied to break the keys used in today's security implementations\nBy contrast, quantum key distribution (QKD), can be applied to exchange a key between the two ends of a communication. QDK provides protection against the threat posed by quantum computing to current cryptographic algorithms and provides a high level of security for the exchange of data, according to Telefónica.\n\"We can make a random bit sequence to appear at one place and simultaneously at another one, without making it pass in between,\" said Juan-Ignacio Cirac, distinguished physicist specialized in quantum computing and member of the Telefónica board in a prepared statement. \"It is sort of magic, but something that quantum physics predicts. It is a way of exchanging secure keys that we have to make the most of, since it cannot be hacked”.\nThe Telefónica and Huawei trial, which also included (Universidad Politécnica de Madrid), was also notable because it used the service provider's software-defined networking (SDN)-managed commercial optical networks. Previously, QKD feasibility had been demonstrated in labs and field trials, including one conducted by Telefónica and UPM in 2009, but not on commercial infrastructures.\nThe field trial used the optical infrastructure provided by Telefónica Spain, connecting three different sites within the Madrid metropolitan area. Software-controlled continuous variables-QKD devices, developed by the Huawei Research Lab in Munich in collaboration with UPM, were installed along with SDN-based management modules developed by Telefónica’s CTIO Network Innovation team and the components required to integrate QKD with NFV and SDN technologies developed by UPM.\nTelefónica said the combination made it possible to show the use of QKD techniques in a real production environment, combining classical data and quantum key transmission on the same fiber, and demonstrated how the services could be managed and used by different applications.\n“The ability to use new network technologies like SDN, designed to increase the flexibility of the network, together with new QKD technology is what allows us to really converge quantum and classical networks on the existing optical fiber infrastructure,\" said Vicente Martin, Head of the Center for Computational Simulation who leads the team at UPM, in the press release. \" Now we have, for the first time, the capability to deploy quantum communications in an incremental way, avoiding large upfront costs, and using the same infrastructure.\"\nWhile the trial showed the promise of using QDK on commercial networks, which would make it less costly to deploy than on dedicated fiber networks for QDK, Telefónica didn't provide a timeline as to when it would be available.","Cryptography is the cornerstone of digital security, enabling everything from secure communications to online banking. In the other resources in this series, we have covered the rise of quantum computing and how it threatens current cryptographic systems.\nQuantum computing is not theoretical. Real strides are being made towards building scalable quantum computers. However, there's still debate about when quantum machines will become powerful enough to break contemporary cryptographic algorithms. Estimates range from a few years to a few decades, but the general consensus is that it's a matter of \"when,\" not \"if.\" BTQ’s QByte Quantum Risk Calculator tracks two important milestones for quantum advantage: qubit counts and quantum infidelity. Even the most pessimistic estimates on the BTQ Risk Calculator place that threshold in the 2030s, without any breakthroughs or improvements in error correction. Given the foundational role of cryptography in modern digital life, preparing for this inevitability is critical.\nThrough the NIST Standardization Process for Post-Quantum Cryptography (PQC), new cryptographic standards are being released in 2024. But this is not just an academic exercise, it's an urgent goal for organizations to upgrade. If you wait, you’ll be left behind!\nThe National Institute of Standards and Technology (NIST) in the United States has been a key resource in preparing for the post-quantum age. NIST has organized the PQC Standardization Process to identify robust post-quantum cryptographic algorithms for digital signatures and general encryption. Though this process is ongoing, the urgency to identify candidates is increasing as the timeline for scalable quantum computing becomes clearer.\nPost-quantum cryptography refers to cryptographic algorithms that are secure against the capabilities of quantum computers. The objective is to provide the same level of security and functionality as existing cryptographic algorithms but stand against attacks from scalable quantum computers. And, not only do we need to protect against quantum computers, but we also don’t want the new cryptographic standards to be cracked by a Xeon processor.\nTo guard against future quantum threats, the Cybersecurity and Infrastructure Security Agency (CISA), the National Security Agency (NSA), and the National Institute of Standards and Technology (NIST) have jointly created a factsheet to help organizations create quantum-readiness roadmaps before the post-quantum cryptographic standards release in 2024.\nTechnology vendors are urged to start planning and evaluations for integrating post-quantum cryptographic algorithms with these action steps:\nFirst, organizations must create a team to create a quantum-readiness roadmap. To begin this journey, organizations should establish a dedicated project management team. This team will explore and identify the usage of quantum-vulnerable cryptography and inventory quantum-vulnerable systems and assets. This inventory is a roadmap for organizations to start quantum risk assessment processes and identify the more critical systems for migration to PQC.\nA well-structured cryptographic inventory is key in aiding organizations to become quantum-ready. Organizations should regularly audit their systems to identify hard-coded cryptographic algorithms that can become potential points of failure and release new internal processes to make sure any new software packages, technology vendors, and systems have a plan and timeline to move to PQC. This will help identify vulnerabilities, particularly in sensitive datasets, network protocols, end-user systems, and identity services.\nCompanies are recommended to start engaging with technology vendors regarding their quantum-readiness roadmaps. Conversations should focus on vendors’ plans, timelines, and commitment to migrating to PQC. This involves commercial-off-the-shelf and cloud-based products, as well as creating backup plans and vendors. Organizations must prioritize high-impact systems and those with long-term confidentiality needs. Conversations with vendors should include discussions about updates, upgrades, expected costs associated with migration to PQC, and ensuring that future products are delivered with built-in hybrid and PQC options.\nTransitioning to post-quantum cryptography is a huge task. Beyond just selecting an algorithm, challenges include re-engineering software and hardware, training staff, and ensuring backward compatibility, crypto-agility, and fallback plans. There are logistical issues, like updating devices remotely or across regions that may be part of critical infrastructure or embedded systems. Financially, the transition will require significant investment, but the cost of inaction—compromised global digital security—is higher. A business that is not ready will already experience financial losses from government clients today.\nCrypto-agility is the ability of a system to effortlessly switch out cryptographic algorithms and methods without requiring huge re-engineering efforts of the underlying infrastructure. In the past, it took 10 years to upgrade from RSA to ECC cryptography. The idea is gaining traction in today's rapidly evolving digital landscape where algorithms can become obsolete overnight due to new vulnerabilities or technological advancements, like quantum computing.\nCorporations should implement crypto-agility as a foundational principle in their security architecture. The first step is to centralize the management of cryptographic libraries and keys, ensuring they can be updated or replaced without requiring a complete system overhaul. Components that use cryptography—whether hardware or software—should be modular to allow for straightforward replacements or upgrades. APIs should be designed to be algorithm-agnostic, permitting an easier switch between algorithms.\nEssentially, crypto-agility is a form of future-proofing. It allows organizations to swiftly adapt to new cryptographic standards, thereby maintaining the integrity and security of their data and systems.\nThe next step in the transition to post-quantum cryptography is using hybrid systems. These are systems that deploy both classical and post-quantum cryptographic methods in parallel. This way organizations can maintain compatibility with existing systems while integrating stronger, quantum-resistant algorithms. For example, TLS 1.2 and 1.3, a security protocol for information exchange between web clients and servers, has been experimenting with a hybrid mode that uses both ECC and PQC. These hybrid systems act as a bridge, helping organizations become crypto-agile and transition smoothly, without sacrificing current operational capability.\nWhile the transition to post-quantum cryptography is still early, some tech corporations are modeling crypto-agility. For example, Google initiated PQC experiments in 2016 and as of August 2023, added support in Chrome 116 for establishing symmetric secrets in TLS for X25519Kyber768, a NIST PQC finalist.\nThe transition to post-quantum cryptography is not an option, it’s necessary. Whether one believes the quantum threat is around the corner or is still a decade away, the regulations require plans to support upgrades to align with PQC standards now. This is a big opportunity for players in the cybersecurity space where quantum knowledge is critical.\nOrganizations must act swiftly, developing quantum-readiness roadmaps and engaging proactively with technology vendors to ensure a transition to a new era of quantum-resistant cryptographic standards and practices. The guidance provided by CISA, NSA, and NIST helps organizations navigate the quantum future securely and efficiently.\nWhile there are many interconnected systems – databases, servers, web traffic, blockchain, and more – to think about, the challenges of a transition are not insurmountable. Through collaborations from governmental bodies like NIST, industry players creating new products, and academic researchers and organizations finding new algorithms, we see a roadmap to a secure digital future that withstands quantum computing’s threat to encryption."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:b8496e1e-647e-49e2-bf48-ba696e7a9654>","<urn:uuid:2ee8525f-090c-44c3-a8f9-2d750d40e970>"],"error":null}
{"question":"How does the basic hearing evaluation process work for adults? Need step-by-step breakdown 听力测试流程是怎样的？","answer":"The adult hearing evaluation is a 90-minute assessment that includes several steps: 1) Complete case history collection 2) Visual inspection of ear canal and eardrum 3) Pure Tone Audiometry to measure hearing at different pitches and volumes using earphones 4) Word Recognition Testing to assess speech understanding 5) Basic Immittance Test Battery to evaluate eardrum response to sound and middle ear status.","context":["Hearing Services for Children and Adults\nAt Morton Plant Mease Hearing Health Center, we offer comprehensive services to assess and diagnose adult and pediatric hearing loss.\nHearing Evaluations for Adults\nYour first visit to a Morton Plant Mease Hearing Health Center will include a complete, non-invasive evaluation of your hearing using state-of-the-art equipment. It’s a lot more than “just a hearing test.” This is a thorough, 90-minute assessment to determine the exact type and degree of hearing loss, and how well or poorly you understand speech. It includes a complete case history, as well as visual inspection of the ear canal and eardrum. The audiologist may use the following tests:\n- Pure Tone Audiometry: This test measures your hearing ability at different pitches and at different volumes. A machine produces tones that you listen to via earphones and the results are recorded on a graph called an audiogram.\n- Word Recognition Testing: For this testing, an audiologist will test your ability to hear, understand and respond to speech by your repetition of simple words.\n- Basic Immittance Test Battery: These tests measure how the eardrum responds to sound. They provide information about the status of the middle ear, such as the detection of fluid in the middle ear or any perforation of the eardrum.\nAdditional Testing Services for Adults\nThere are a variety of other procedures that assess the auditory system and determine hearing loss. They are sometimes used independently and sometimes to complement the standard series of hearing tests.\n- Auditory Brainstem Response Testing (ABR): This is used to evaluate how well sounds travel along the hearing nerve pathways to the brainstem. It can be used to estimate the hearing sensitivity of an adult who is unable to accurately complete a standard hearing test.\n- Otoacoustic Emission Testing (OAE): OAEs are inaudible sounds produced by outer hair cells in the inner ear when they vibrate in response to an audible sound. The inaudible sound echoes back into the middle and outer ear and can be measured with a small, sensitive microphone placed in the ear canal. The presence or absence of OAEs is important in helping to identify hearing loss.\n- Videonystagmography (VNG) and Rotary Chair: Our inner ear plays a significant role in helping us maintain our balance. Any disturbance in the inner ear may cause a feeling of dizziness. This test is used for patients with balance disorders to help determine the degree and location of a balance dysfunction. For more information, please visit the Morton Plant Neurosciences Balance Clinic.\nHearing health assessment and diagnosis is not a one-size-fits-all process. It is vastly different in children and adults. That's why at the Morton Plant Mease Hearing Health Center at the Ptak Orthopaedic & Neuroscience Pavilion, we separate our children and adult services and use individualized, age-appropriate methods. This includes pediatric-specific booths for evaluating even the youngest patients.\nHearing Evaluations for Children\nAs of July 1, 2000, the State of Florida requires hearing screenings for all newborns prior to discharge from hospitals and birthing centers. Even if a difficulty is determined at this early age, we can provide the necessary follow-up testing. We are equipped and trained to test children of all ages.\n- Pure Tone Audiometry: This test measures your child’s ability to hear different sound pitches and volumes.\n- Speech Understanding Testing: An audiologist will test your child’s ability to hear, understand and respond to speech by having him or her repeat simple words or point to pictures.\n- Basic Immitance Test Battery: These tests provide information about the status of the outer and middle ear such as the detection of fluid in the middle ear, a perforation of the eardrum, or wax blocking the ear canal.\n- Select Picture Audiometry: Your child will use pictures on cards to indicate how he or she hears sounds.\n- Conditioning Play Audiometry: Children ages two to five are conditioned to respond to sound through play, such as stacking blocks or filling a peg board.\n- Visual Reinforcement Audiometry: Used for children age 7 months to 5 years, this test uses lighted animated toys to train children to look toward sound in the testing booth.\nAdditional Pediatric Testing Services\nThere are a variety of other procedures that assess the auditory system and determine the presence of hearing loss. They are sometimes used independently and sometimes to complement the standard series of audiologic tests.\n- Otoacoustic Emissions (OAEs): OAEs are inaudible sounds produced by outer hair cells in the inner ear when they vibrate in response to an audible sound. The inaudible sound echoes back into the middle and outer ear and can be measured with a small, sensitive microphone placed in the ear canal. The presence or absence of OAEs is important in helping to identify hearing loss.\n- Auditory Brainstem Response Testing for Newborns (ABR): This test estimates the level of hearing acuity by looking at brainstem wave responses to a series of clicks presented to each ear. It can be used as a screening procedure for newborns that are at risk for hearing loss, as well as a diagnostic tool to identify infants and small children with a hearing loss.\nFor more information or to schedule an appointment, please call (727) 461-8807."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:f966d642-058d-4b91-a2b2-91f435977de4>"],"error":null}
{"question":"What is the maximum width of a standard travel trailer?","answer":"The travel trailer is usually 8.6 feet wide.","context":["Transporting a travel trailer overseas could be an easy task if you know the technical details related to it. A trailer has a bumper pull-hitch which ranges in size. The travel trailer is usually 8.6 feet wide and up to 45 feet long. If a trailer is over 28 feet long, it must be towed with a ¾ ton pick up truck. Let us see how we can transport a travel trailer overseas.\nTravel trailers in US are classified as RV (Recreational Vehicles) along with pop-up trailers, fifth wheel trailers, truck campers and motor homes. Pop-ups are usually less than 18 feet long and have simple features and can be towed by small cars. Example is the Play Pac which can be hauled by a small car. Mid range trailers are 18 to 25 feet long weighing about 5000 pounds or more and can be towed by pick up trucks. Larger travel trailers range from 25 to 40 feet, come loaded with amenities and are towed with highway tractor or large trucks or SUV’s.\nTransport Travel Trailer: To transport travel trailer overseas, one may use the services of a shipping company / agency. The agency will pick up the trailer from private home, dealership, online auction, Ebay seller, manufacturer or storage facility and deliver the trailer at any overseas port. They take care of all export regulations and paperwork in USA, arrange for all shipping documents and even provide marine insurance for overseas shipping.\nKey Elements: There are two key elements during transport travel trailer overseas. First, you are moving a travel trailer which has requirements different from a truck or a car. Secondly you are shipping it overseas to foreign countries which may have requirements different from the U.S.\nBut there is no need to worry as transport travel trailer overseas has been successfully done by shipping companies with experience and enterprising individuals. The key thing to do is to provide a generous lead time for making plans and to get all documentation in order. Companies experienced in shipping are aware that the process requires time and patience.\nFraudulent offers: One must be careful not to deal with shipping companies that have offers that are over the top. You must realize that no company can deliver your travel trailer in 1 to 2 weeks or deliver it in sound condition after a few days. The truth is that it usually takes several weeks of planning to take care of dock fees, insurance, registration, import costs and customs schedules.\nCareful Planning: To transport travel trailer overseas, careful planning must be completed. You must contact the vehicle service agency abroad to make sure how the travel trailer can be transported over land from the port. This is to make sure that your trailer has all features to make it legal to function in the foreign country. Licensing, lighting, breaks and hitch arrangements will vary in foreign countries. Make sure that your tax and registration documents are in order. Also it would be good to go through excellent websites on transport travel trailer overseas.\nSo go ahead and get the best quotes on transport travel trailer overseas and relax while your travel trailer is delivered securely overseas through one of the best shipping company."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:ed09a420-418f-4972-8293-ea8358d045ed>"],"error":null}
{"question":"How do Julian of Norwich's and Saint Peter of Verona's approaches to dealing with heretics differ in their historical contexts?","answer":"Julian of Norwich and Saint Peter of Verona had markedly different approaches to dealing with heretics. Julian, living in 14th century England, took a more compassionate and inclusive approach, emphasizing all-pervasive love as the response to fear and hatred. She did not denounce people of different faiths, unlike other contemporary mystics who preached against Muslims or Jews. In contrast, Saint Peter of Verona, living in 13th century Italy, took a more direct approach against heresy as an Inquisitor in Lombardy. While he showed some clemency by declaring clemency for those confessing heresy, he actively denounced heretics in his sermons and worked to defend orthodox Catholic faith, which ultimately led to his martyrdom at the hands of Cathar assassins.","context":["“When he broke the fourth seal, I heard the voice of the fourth animal shout, ‘Come’. Immediately another horse appeared, deathly pale, and its rider was called Plague, and Hades followed at his heals.” (The New Testament, John, Rev. C. 6, v. 8) But Julian of Norwich saw that all was created with love and said that all, in all manner is well and will be well. (Julian of Norwich, The Revelations of Divine Love. Showing 13, C. 32)\nJoanne & I traveled to England leaving Christmas Day 2012 to celebrate the baptism of our new grandson Jacob. The day of the baptism was cloudy and cold. Television commentators in the U.K. that morning commented on U.S. issues such as gun control and the immanence of going over the financial fiscal cliff.\nThe ceremony was brief but meaningful. Father McCarthy, the lead celebrant, welcomed the radiant and beautiful Jacob Alan Lange into the community and reminded us of our responsibilities in love to Jacob. Father McCarthy prayed for the support of the ever present “Communion of Saints” including Saints Jacob, Joel, and David. In my own prayer I added Julian of Norwich.\nWe arrived in London a week before the ceremony and had time to visit Norwich a city to the east and north of London and close to the North Sea. The purpose was to discover more about Julian of Norwich, a 14th century anchoress. An anchoress is a female hermit who withdraws from secular life to a sealed room connected to a church for religious reasons.\nNorwich was second to London in population and commercial importance in 14th century England. It was a century of dramatic change similar to our own times. Increased finished cloth production brought in skilled workers to Norwich from Flanders. Many foreign workers in Norwich were murdered during the Peasants Revolt in 1381. (Whittock, Martyn, Life in the Middle Ages, London, 2009, p. 68) Climate change, constant war with France, the Black Death, a corrupt Catholic Church, which had moved its headquarters from Rome to Avignon, and strong theological dissent, responded to by the inquisition, set the background for the writings of mystic Julian of Norwich. (see - Tuchman, Barbara W. A Distant Mirror, Alfred A. Knopf, New York, 1978.)\nA visit to Norwich can bring history alive. Several stops during rainy cold days gave us a greater awareness of anchoress Julian. Let us consider three: the Cathedral, St. Andrew and Black Friar’s Halls and St. Julian’s Church.\nTHE NORMAN CATHEDRAL AND ABBY\nThe Norwich Cathedral, Church of the Holy Trinity, which, since Henry VIII, (1491-1541) has been under the control of the English government and the Anglican Church. The Cathedral was established by Norman Bishop Herbert Losinga and the Benedictines in 1096, and construction was completed in 1499. The Cathedral has differing architecture since its construction continued during centuries of time. For example, the lower part is Romanesque and the upper vault is Gothic.\nAt least three references to Julian can be found in the Cathedral. Various chapels line the inside of the Cathedral. Priests were required to say Mass every day and the BenedictIne Community had many priests and needed many chapels to accommodate. It was believed the Masses provided the ground of existence for mankind. One of the chapels, dedicated to the 9th Army Regiment of Norfolk, has a painted window of “St. Juliana.” It is not known what her real name was, but she is called Julian because she was an anchoress at St. Julian’s Church. She appears in Benedictine habit with a cat at her feet. There are doubts as to whether she was really a Benedictine nun, and those who think she was a nun call her – Dame Julian. She was never officially declared a saint, but her writings cut out the legs supporting Christian theology so official canonization to sainthood would very surprising. The cat, the only animal allowed to be with an anchoress, was useful in catching mice and rats that might enter the cell. Also the founder of Christianity in the region of East Anglia was St. Felix, a name often associated with cats.\nAnother chapel has a painted glass window depicting notable Benedictine monks. Julian is at the base of the window wearing a Benedictine habit. An inscription in Latin, “Ut in omnibus glorificatur Deus” (hence in all God is glorified. - a Benedictine saying, but arguably a summation of Julian’s theology,) The tour guide called her Mother Julian.\nAn entrance to the Cathedral is flanked by contemporary statues of St. Benedict and Julian. We had the good fortune to meet the sculptor, David Holgate, at a tea shop, and he told us he did many months of research for the statues. Julian is carved wearing the typical dress of a 14th century towns-woman. She is appropriately holding a book since she was the first woman to write a book in English, Revelations in Divine Love, and to this day she is a teacher. Julian wrote in a form of English similar to that of Chaucer – a contemporary. St. Benedict, the founder of monasticism, is shown in his habit with his index finger over his lips to show the importance of silence. (“Be still and confess that I am God! I am exalted among the nations – exalted on the earth. The Lord of Hosts is with us, our stronghold is the God of Jacob.” Psalm 46, vs. 11 -12. “Nothing in all creation is so like God as stillness.” Meister Eckhart, Matthew Fox, Original Blessing, Bear & Company, Santa Fe, New Mexico, 1983, p. 133.)\nBLACKFRIARS & ST. ANDREW’S HALLS\nWe visited the St. Andrew’s and Black Friars Halls to get further insights into Julian’s 14th century. The Dominicans, Black Friars, arrived in Norwich in 1226. They took over the priory of St. Andrew’s in 1307, and began constructing the Black Friars Hall in 1345. The Friars (White Friars – Carmelites, Grey Friars – Franciscans, Black Friars – Dominicans) of Norwich differed from the Benedictine Monks in that the Benedictines would establish an Abby that attracted people to form a productive community with its center at the Abby. The Benedictine motto is: laborare est orare (to work is to pray.) The Abbot or Abbess would be the ultimate community authority. The friars differed in that they would go out to the people preaching in parishes, establishing community centers.\nThe family of Thomas Erpingham, hero of Agincourt, 1415 - a major victory for the English in the 100 years war with France, donated money for the construction of the Black Friars Hall. Erpingham’s son was a Dominican; the Erpingham coat of arms is evident in the hall. Some believe Julian was of the wealthy aristocratic Erpingham family, hence the use of the title – Lady Julian. The reasoning is that Julian was an educated woman, even though she claimed not to be, and someone had to support her as an anchoress; it could have been the bishop, but this is unlikely. The Bishop, Henry Despenser, and Thomas Erpingham were enemies.\nA plaque at the Black Friars Hall commemorates two anchoresses that were attached to the hall – Katherine Foster and Katherine Mann. Did the Dominican charism, “contemplata aliis trader” (giving the fruits of contemplation to others) influence Julian? The first Dominicans were cloistered nuns.\nSt. JULIAN CHURCH\nOf course the most important place we went to on our pilgrimage was St. Julian’s Church. Even in the cold and rain we recognized it as Holy Ground. This is where Julian lived as an anchoress for probably over 40 years in a room attached to the church. The room was sealed but she had a window looking into the church so that she might participate in the liturgy and a window to the street so she could be available to console, council and encourage others.\nIt is a small stone church still in use for prayer. It is estimated that there has been a church on this site since 950. The name is probably from Bishop Julian of Le Mans (4th century). In 1135 King Stephen gave the Church to the care of the nearby Benedictine Nuns of Carrow. The Church is close to the river Wensum which connects to the North Sea and facilitated Norwich to be a port of entry for the Normans and people from the low countries. Julian the anchoress gets her name from the Church. A replica of the Church was re-constructed in 1953 because of bombing damage suffered during W.W. II.\nAmong the interesting features of the Church is a medieval baptismal front which dates from about 1420. Several well known saints are carved on the font, but at the base are saints that a Church brochure notes with a question mark. One is William of Norwich, a boy whose murder was blamed on the Jews.\n“Unwanted children were often sent into the forest to die, as the\nstory of Hansel and Gretel recalls; parents could easily explain their\ndisappearance by blaming it on the Jews. In Norwich Cathedral in\nEngland, one can see a very apologetic plaque commemorating the\nboy William of Norwich who in the twelfth century was said to have\nbeen stolen by the Jews and crucified.” (We did not see the plaque.)\n“In compensation he was made a saint.” (Middle Ages, Editors of\n“Horizon Magazine,” American Heritage Publishing Co. Inc. New York,\n1968. p. 131.)\nThe Jews were expelled from England in 1290 by Edward I.\nNot far from the Church was “Lollards Pit” where followers of dissident Oxford Priest and scholar John Wyclif were dumped after execution. Julian’s showings supported Wyclif; which raises the question – who was Julian’s protector?\nTHE MARRIAGE OF THE MYSTICAL AND THE ACTIVE\nThe profundity of Julian’s book goes much deeper than we are capable considering, but look at the following. Julian wrote at a time of rampant fear and hatred, but she pointed to all pervasive love as the response. She saw the “passion” not just as her suffering or Jesus’ suffering but as the suffering of humanity which elicits love – compassion – and justice. Julian uses the term “humanity” in the “realistic” sense of Benedictine Anselm Archbishop of Canterbury (1033-1109). Ironically, I believe that 14th century (c1290 -1349) polar opposite Franciscan “nominalist” William of Ockham would have had no objections to Julian’s work.\nFor Julian, God was the mother of all. She did not preach a crusade against the Muslims as did 14 century mystic Catherine of Sienna, nor did she march with the flagellants preaching apocalyptic fear and denouncing Jews, as did the defender of Avignon, St. Vincent Ferrer.\nJulian agreed with Wyclif as to the equality of the people of God. Her term was “even Christians.” Her God was a God of motherly love, not one of wrath that required mediators.\nThe connection – the unity of heaven and earth was clear to her. Humanity was in partnership with the Creator to restore all. Jacob’s ladder is an appropriate symbol. Fourteenth century German Dominican theologian Meister Eckhart has an explanation: “down is up and up is down.” (Fox, Matthew, A Spirituality Named Compassion, Harper, San Francisco, 1979, p. 40.)","Saint of the Day – 29 April – St Peter of Verona OP (1205–1252) also known as St Peter Martyr – Dominican Priest and Friar, a celebrated Preacher, miracle-worker, Marian devotee. He served as Inquisitor in Lombardy, was killed by an assassin and was Canonised 11 months after his death, making his the fastest Canonisation in history. Patronages – inquisitors, midwives, Castelleone di Suasa, Italy, Verona, Italy, diocese of, Guaynabo, Puerto Rico. St Peter is the the first Canonised martyr of the Dominican Order.\nIn the English-speaking part of the world especially, all too little is known about this illustrious Friar Preacher. Possibly this is in part due to the well-known bias in England against the old-time inquisition, which spread thence into the colonies founded by that country, for Saint Peter was closely connected with that institution. Indeed, by not a few he is considered as a man without a heart. Yet he was most compassionate. His character was rounded out by an admirable strength of will and a mind so judiciously balanced that he neither shrank from duty, whatever the sacrifice, or even danger, it involved, nor allowed his heart to control his judgement.\nFather Thomas Agni of Leontino, another noted Dominican, archbishop of Cosenza and later patriarch of Jerusalem, was the first to write a life of the blessed martyr. His testimony should he all the more reliable because he lived for many years with Saint Peter of Verona, had been his superior and was an eye-witness of the principal events in his life. The work shows no signs of undue predilection. Agni’s original manuscript was for long years at Saint Mark’s Convent, Florence. Another, with some additions by Father Ambrose Taegio, was preserved in the Convent of Nostra Donna delle Grazie, Milan.\nPeter was born in Verona, Italy in 1205, of parents who had embraced the heresy of Cartharism but he did attend a Catholic school. He was educated at the University of Bologna and was accepted into the Dominican Order by Dominic himself.\nBecause the Dominicans were theologically trained preachers, the popes entrusted the Inquisition to them. In 1234, Pope Innocent IV recognised Peter’s virtues (severity of life and doctrine, talent for preaching, and zeal for the orthodox Catholic faith) and appointed him Inquisitor in Lombardy. He spent about six months in that office and it is unclear whether he was ever involved in any trials. His one recorded act was a declaration of clemency for those confessing heresy or sympathy to heresy. In 1251 his jurisdiction was extended to most of northern Italy. Although he attracted huge crowds with his preaching, as an inquisitor he also made enemies.\nMarvellously filled with the gifts of the Holy Spirit, he laboured continually for the propagation and defence of the true faith, being zealous for its promotion among the people. To this end he established the Association of the Faith and the Confraternity of the Blessed Virgin Mary. He was a fervent of promoter of community and fraternal life and served the brethren wisely as a prior. He was also greatly solicitous for the spiritual good of the sisters, lovingly assisting them with his advice and exhortations to their spiritual benefit.\nIn his sermons he denounced heresy and also those Catholics who professed the Faith by words but acted contrary to it in deeds. Crowds came to meet him and followed him, conversions were numerous, including many Cathars who returned to orthodoxy.\nBecause of this, a group of Milanese Cathars conspired to kill him. They hired an assassin, one Carino of Balsamo. Carino’s accomplice was Manfredo Clitoro of Giussano. On 6 April 1252, when Peter was returning from Como to Milan, the two assassins followed Peter to a lonely spot near Barlassina and there killed him and mortally wounded his companion, a fellow friar named Domenico.\nCarino struck Peter’s head with an axe and then attacked Domenico. Peter rose to his knees and recited the first article of the Symbol of the Apostles (the Apostle’s Creed). Offering his blood as a sacrifice to God, according to legend, he dipped his fingers in it and wrote on the ground: “Credo in Deum” “I believe in God”, the first words of the Apostles’ Creed. The blow that killed him cut off the top of his head but the testimony given at the inquest into his death confirms that he began reciting the Creed when he was attacked. Domenico was carried to Meda, where he died five days afterwards.\nThe murderer Carino, renounced heresy, became a Dominican co-operator brother and died with a reputation for sanctity. He is the subject of a local cult as Blessed Carino of Balsamo.\nWherever he went, the deaf, the dumb, the blind, the lame and people sick with every kind of ailment were brought to him. Ordinarily all were benefited by his prayers. They praised God for the power of healing which He had given His servant.\nPeter of Verona and with reason, was considered a learned doctor. Yet he ever continued to store his mind with new knowledge, whether through prayer, meditation, or reading the Sacred Writings. The example which he set his religious brethren showed them by what means they could perfect themselves in their state of life and make themselves useful to the Church. Never did his degree of Master in Sacred Theology cause him to neglect study. Study never prevented him from being the first at all the regular exercises. Well did he know how to combine the practices of the cloister with the labours of the apostolic life.\nIn private conversation, just as in his sermons, he stimulated the faithful with his personal sentiments of love for the Blessed Virgin. Because of his influence in their favour the Servites (they were investigated to ensure their orthodoxy) have ever regarded Peter of Verona in the light of a second founder of their order. After his Canonisation, they placed him on the list of their holy patrons and protectors.\nThe Bull of Canonisation was sent at once to all bishops and ecclesiastical superiors, with an order that the feast of Peter of Verona should he celebrated every year on 29 April. This day was chosen for the celebration because that of his martyrdom, 6 April often falls in Holy Week, or within the octave of Easter. Alexander IV and several of his successors, prescribed that the feast should he of the same obligation as that of Saint Dominic. Finally, Clement X, by a papal decree, ordered that the feast of Saint Peter Martyr should have the rank of a duplex for the whole Church. This was in 1670 and the practice is in use today, wherever the Roman breviary is recited.\nHowever, veneration of Peter of Verona is especially noteworthy in the Order of Friars Preacher and in that of the Servites. It is particularly the case in Italy, the land of his birth, the field of his labours and the place of his holy death. There many are the churches, chapels and confraternities erected in his honour.\nThe body of the martyr is still preserved and venerated in a magnificent chapel of Saint Eustorgio, Milan. Princes and noblemen of France, Germany, England and Italy (particularly the archbishops of Milan) imitated the king and queen of Cyprus with their rich gifts for the enshrinement of the saint’s relics. At each time of their various translations (1253, 1340, 1651 and 1736) many miracles were wrought. Suffice it to say that the Acta Sanctorum, in the third volume for April, where they treat of our martyr, give a long list of attested wonders worked by him.\nSaint Thomas of Aquinas, the Angelic Doctor, was an ardent admirer of Peter of Verona. In 1263 he visited the martyr’s sepulchre. While at Saint Eustorgio’s Convent, the great theologian and poet wrote the following verses in eulogy of the valiant athlete of the faith, which were afterwards engraved on a marble slab and placed near his tomb, where they may still he read:\nHere silent is Christ’s Herald.\nHere quenched, the People’s Light.\nHere lies the martyred Champion\nWho fought Faith’s holy fight.\nThe Voice the sheep heard gladly,\nThe light they loved to see\nHe fell beneath the weapons\nOf graceless Cathari.\nThe Saviour crowns His Soldier.\nHis praise the people psalm.\nThe Faith he kept adorns him\nWith martyr’s fadeless palm.\nHis praise new marvels utter,\nNew light he spreads abroad\nAnd now the whole wide city\nKnows well the path to God."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:6552549c-50f4-4261-83ea-8347e695fde2>","<urn:uuid:14e5eec5-29bf-412d-8d71-122d56852b2b>"],"error":null}
{"question":"What are the key differences between stone construction in Jericho's Stonematters project and classical architectural stone elements?","answer":"The Stonematters project in Jericho uses an innovative construction principle with 300 mutually supported unique stone pieces following a minimal surface geometry with geodesic lines, spanning 7 meters with a 12cm depth. In contrast, classical architectural stone elements follow strict traditional orders (Corinthian, Doric, Ionic, Tuscan, and Composite) with specific mathematical formulas and proportions, typically used in features like columns, pilasters, and entablatures positioned above supporting columns and below the roof.","context":["- City : Jericho\n- Country : Palestine\nText description provided by the architects. Palestine suffers of a misuse of stone as a structural material: while it was an abundant material used for structural purposes in the past, it is now used as a cladding material only and the know-how of stone building is disappearing.\nThe research aims at including stone stereotomy – the processes of cutting stones – construction processes in contemporary architecture. It relies on novel computational simulation and fabrication techniques in order to present a modern stone construction technique as part of a local and global architectural language.\nOur research department – SCALES – and GSA (Geometrie Structure Architecture) lab are leading this research on stone construction techniques. The results of the research will be used to build the el-Atlal artists and writers residency in Jericho. As such, Stonematters is the first module of the residency and the first built vault of our research.\nStone matters is built on an innovative construction principle allowing for unprecedented forms for such structures. The architectural innovation is born from structural morphology and stereotomy. The vault covers a surface of 60 m2 and spans 7 meters with a constant depth of 12 cm. The geometry follows the shape of a minimal surface on which geodesic lines are drawn and set the pattern of the interlocking stones. The whole structure is made of 300 mutually supported unique stone pieces.\nBeyond the scientific and technical issues that make Stonematters a unique object, the project represents as well a cultural challenge: it has been entirely built with available know-hows in a peripheral zone of the culturally marginal city of Jericho. Processes of several factories have been combined in order to use existing known techniques for new uses. The polystyrene blocks, for example, have been roughly cut in a factory and transported to another for a robotic carving process. Right after its completion Stonematters attracted curious inhabitants of Jericho and elsewhere in Palestine, putting together the seeds of the future el-atlal artists’ and writers’ residency.\nThrough the understanding of our historical cities the research tries to link techniques of constructions to urban morphologies. It puts a non-hierarchical hypothetical link between the scale of stereotomy and the scale of urban fabric. In that context, the idea is to suggest new urban morphologies linked to the scientific use of a largely available material in Palestine.\nStone in Palestine\nIn Palestine, the most common construction material is stone. Stone is abundant, widely available and – foremost – an urban law imposed by the Ottomans requires stone construction in order to unify the built landscapes. This law underlines the shift from a self- managed urbanism to an authority urbanism.\nNot only is the stone a marker of the transition in the urban and social structures, but it shows the evolution of the Palestinian city’s morphology. The construction techniques’ evolution, from fabrication to implementation, has an effect on the entirety of the Palestinian city.\nThe church of Sainte Anne is one of Jerusalem old city’s most valuable witness of crusaders’ architecture. The church has been built in the 12th century by the Crusaders. The church of Saint Anne offers a complete example of what was the architecture of the Crusaders in Palestine; a combination of different architectural elements that they brought from abroad and indigenous elements that they found in situ.\nThe vault covers a surface of 60 m2 and spans 7 meters with a constant depth of 12 cm. The geometry follows the shape of a minimal surface on which geodesic lines are drawn and set the pattern of the interlocking stones. The whole structure is made of 300 mutually supported unique stone pieces.\nThe geometry of the vault follows the shape of a minimal surface on which geodesic lines are drawn and set the pattern of the interlocking stones. The whole structure is made of 300 mutually supported unique stone pieces. Each stone has 4 incliced interfaces, that allow the assembly of the different stone voussoirs.\nBased on geometrical parameters as the overall shape, the density of the paving, the inclination of contact surfaces, the size of the voussoirs, and number of voussoirs types, a specific structural criteria can be improved.\nThe formwork of the vault is made out of blocks of polystyrene of variable heights, carved with the shape of each stone. When arranged together they form a continuous counter-form of the entire structure. On each block, stones are referenced and placed at their exact position.\nWhile the polystyrene blocks were digitally cut using robots, the main formwork was created by the local artisans using usual wooden formworks. Different altimetries were defined, generating a stepped formwork that receives the carved polystyrene blocks.\nStone voussoirs are assembled on the mounted polystyrene blocks. Each stone’s location is defined on the formwork. The mounting started from the upper center of the vault progressively advanced towards the edges in a concentric process. The inclined interfaces between the stone voussoirs generate the interlocking system of the structure.\nSpanning 7m, the vertical displacement of the vault has been delicately controlled. The initial shape has been adjusted several times, adding a counter jib in the center of the gyroid form, increasing the curvature thus reducing the displacement.\nDismantling of the Scaffolding\nThe dismantling of the scaffolding was sequenced by different steps. Laboratory glass plates were first installed at few locations of the structure, allowing the measuring of the movements of the structure during the un-mounting of scaffolding. The formwork jacks were then taken down of a few centimeters, letting the whole structure hold by itself. After a few hours, the glass plates were inspected, and no brakes were visible. The scaffolding was gradually taken off.\nThe el-Atlal project is meant to be a model of construction techniques. It allows to envision new possible cities’ morphologies, new construction techniques and a sophisticated use of stone. The project has the ambition of creating a mode of urbanism, and as the harat succeeded in building a city that fits their needs through stone construction techniques, el-Atlal expects to be a breeding ground of inclusive approaches to Palestinian urbanism. An urbanism whose scales are profoundly associated. A technical and durable urbanism leaving a trace on the city’s evolution and on the Palestinian landscape.\nUntil the beginning of the previous century stereotomy, or the art of cutting and assembling stones, played an important role in the esthetics as well as in the structural construction principles. Today’s stone factories only produce standardized blocks of few cm, used as cladding of a reinforced concrete structure. The techniques used for building with stone have an effect on the speed of construction, the urban spread of territorial boundaries, and the morphology of all buildings. In other words, stereotomy and the construction methods leave a trace on the palestinian landscape.\nReinvigorating stereotomy as a way of optimizing structural performance using advanced design simulations is the founding principle of stone matters. Based on geometrical parameters as the overall shape, the density of the paving, the inclination of contact surfaces, the size of the voussoirs, and number of voussoirs types, a specific structural criteria can be improved. The whole process is an in-progress workflow which will aim at using computational design and advanced fabrication techniques in order to present a modern stone construction technique as part of a local and global architectural language.","A traditional balustrade is a timeless architectural compilation which adds sophistication to any exterior or interior setting. Balustrades can be used to create a dramatic spiral stairway, an elegant balcony, or to delineate a courtyard or garden terrace from a lawn.\nA stone balustrade is comprised of a top rail, balusters, and bottom rail; which is received by cast stone piers or newel posts. There are numerous design options available to enrich your balustrade’s flair. For example, balusters can be custom made to emphasize or contrast other features in a home or building; piers can be accented with decorative inlay panels and adorned with ball finials or urns.\nA less formal and more decorative alternate to balustrading, parapet screening provides an exceptional solution. Cast stone parapet screening typically uses a repeating quatrefoil pattern, which can be embellished by our sculptors with a high relief.\nInitially designed for structural use, columns and pilasters are now commonly integrated into projects as design elements. CGA Stoneworks can provide your project with columns and pilasters in all five Architectural Orders: Corinthian, Doric, Ionic, Tuscan, and Composite. In addition to the standard orders, CGA specializes in creating one of a kind custom columns and pilasters. From ornate Greek Dolphin Capitals to Sailfish embossed column shafts, our sculpting studio can incorporate the theme of your project or its surroundings into these iconic architectural elements.\nPreserving the ancient styles of classical architecture along with retaining the proportions of each Architectural Order is of the utmost importance. The ancient orders of architecture provide designers with a set of guides and mathematical formulas which result in an aesthetically pleasing design. Unfortunately, many Cast Stone companies ignore these rules and substitute their “stock molds” to keep their costs down. This results in a column or pilaster that is out of proportion, appearing too skinny or too fat, when viewed as a complete architectural element. At CGA Stoneworks, all of our columns and pilasters are designed and manufactured to uphold the proportions and characteristic profiles and details of the Architectural Orders. If we don’t have the correct size mold on hand, we will fabricate one in our mold and sculpting department. We never sabotage the intended design to reduce our expenses. This is just another way that we separate ourselves from the rest of the industry.\nAll CGA Stoneworks columns and pilasters can be fabricated in Architectural Cast Stone, Architectural Precast, GFRC, or hand-carved Natural Stone, to meet the design objectives of your project.\nIn classical architecture the entablature is the upper portion of a building, above the supporting columns and below the roof. There are three components of an entablature: the architrave (lower banding), frieze (middle banding), and the cornice (upper banding). In modern architecture it is common for projects to be designed with all three elements of the entablature, as well as just the cornice.\nThe cornice and entablature is the exterior equivalent of crown molding and offers the same benefits; a finished elegant design that accentuates the buildings architecture. From elaborate repeating patterns, to an integrated gutter system, the design options are truly endless.\nCast Stone arches enrich the appearance of any well designed residence or commercial project by accentuating its entries and hallways.\nCast Stone surrounds, sills, and keystones can be added to doorways and windows creating a look of elegance. We can produce any design, whether it is traditionally simple or intricately ornate.\nOf course, we’re not limited to just Cast Stone. At CGA Stoneworks we can fabricate some, or all of these architectural moldings in multiple materials. With GFRC (glass fiber reinforced concrete) the options are endless, due to the material’s unique ability to be manufactured in very large pieces at a fraction of the weight while integrating seamlessly with the cast stone in color and finish. Prefer the inherent beauty of natural limestone? Not a problem, we will carve the moldings to match your specified profiles and patterns from hand selected blocks of stone.\nDecorative cast stone moldings and trim can be used to create elegant styling or intricate embellishments around your building’s exterior and interior. Various moldings such as a base molding, watertable base, and string course can define the levels of a building or simply break up large facade. A compilation of bandings can be used to generate a focal point like a classic portico, loggia, or dress the edge of a balcony or terrace. Moldings around windows and doors, typically referred to as surrounds, are an excellent way to add elegance and provide a finished look. Moldings are typically used in conjunction with other architectural dressings such as cornices, quoins, plinths, and veneer cladding.\nWhere a design or structure calls for very large and long moldings, glass fiber reinforced concrete (GFRC) is the ideal material to be used. Our GFRC has a high strength to weight ratio and high flexural strength which enable large moldings to be fabricated in the same color range and finish as our cast stone products.\nCGA Stoneworks has an extensive library of ornate sculpted items including: medallions, rosettes, cartouches, corbels, brackets, columns, balusters, moldings, planters, urns, and finials. In fact, we have hand-carved custom reliefs and engravings on virtually every architectural element imaginable; even pavers with engraved sea shells and seahorses.\nEven though we have a vast collection of sculpted architectural elements, many of our Clients have a unique vision that they would like to see brought to life. Whether it’s a few elaborate medallions or intricately ornate moldings encasing every window and door, our skilled artists can create any design imaginable with uncompromising attention to detail.\nOur sculpting and mold-making department is the back-bone of our manufacturing facility, and second to none. We fuse modern technology with old-world tradition, enabling us to efficiently and accurately work on large scale and complex projects. Our laser scanners, 3D software, and CNC router allow us to replicate architectural artifacts, and carve any shape or pattern you can dream of. While modern technology is critical, our most important asset is our team of talented artists and hand-carvers who have honed their techniques for decades. Our machines and 3D software augment the sculptors’ skilled hands and eyes.\nCGA Stoneworks offers an extensive collection of custom designed and exquisitely crafted fireplace mantels. We recognize that the fireplace mantel is often the signature statement of a home, reflecting its quality as well as the superior craftsmanship of its builder. At CGA you always receive a custom designed mantel that boasts uncompromising attention to detail and impeccable craftsmanship.\nEvery fireplace mantel we create is hand-made to order from the finest materials: natural limestone, marble, coral stone, cast stone, and GFRC. Our ability to work with so many different materials ensures your mantel will be crafted from the most suitable product to meet your design criteria while adding beauty and value to your investment.\nThe entrance to a home or building should be a focal point; after all first impressions are everything. A dramatic grand entryway will dramatically enhance a projects curb appeal and command attention. Whether your project incorporates a courtyard to the main entrance of a home, or an entryway with gate piers and a perimeter wall, adding cast stone elements which unite the building’s architecture will complete your project’s design.\nLandscapes designed with natural and cast stone are admired and recognized for their inherent beauty. Whether your style is classical, traditional, or contemporary, there are numerous cast stone elements such as; planters, fire pit bowls, urns, pergola columns, gazebos, balustrades and fountains that can be incorporated to distinguish your project and have an immediate and lasting impression.\nFountains are a popular amenity for gardens and landscapes both large and small. The added element of flowing water infuses tranquility and is essential to completing any outdoor living space.\nCast stone planter designs enhance gardens and terraces adding style to your container planters; including bowls, boxes, baskets, vases, and urns are ideal for your patio or terrace, garden or landscape. A traditional or contemporary planter is not just a plant pot or box. It is a work of art, exuding quality and adding elegance to your grounds. Cast stone planters can be placed on a deck, or positioned atop a pedestal or newel post of your balustrade. Incorporating cast stone planters is an easy way to add grandeur to one’s estate.\nCGA’s standard planters are available in a wide range of sizes and styles both classical and contemporary. In addition to our stock models, we specialize in creating custom cast stone planters and urns in a limitless range of styles and designs.\nCast Stone wall and pier caps provide the finishing touch to an entry gate pier, newel post, and property walls. At CGA, all of our pier and wall caps are made to your specified profile and dimensions. If a cap design has not been provided, our talented detailers will submit several options for your selection which complement the buildings architecture.\nPiers and walls can be further accented with base moldings, wall bandings, veneer cladding, and a wide range of finials.\nCast stone paving and flooring is ideal for patios, terraces, decks, walkways, driveways, and stepping stones due to its inherent strength and durability. At CGA Stoneworks we are not limited to ‘available sizes’ like most paving manufacturers. All of our pavers are made to order, to virtually any size or shape, so landscape architects and designers have the freedom to create their ideal paving pattern without having to compromise their design.\nOur pavers are available in a wide range of standard or custom finishes as well as standard or custom colors. Our pavers have been fabricated to simulate a wide range of natural cut stones including: Florida Coral stone, Dominican Coral stone, Limestone, Keystone, Shell-stone, Granite, Sandstone, and Bluestone. In addition to simulating natural stone, we have generated numerous unique textures and finishes to achieve an exclusive look for our Clients.\nCast Stone steps can be fabricated for straight, radius, or spiral stairs both exterior and interior. As with all of our products, standard and custom colors, finishes, profiles, and sizes are available. Of course, no staircase is complete without complementing balustrades designed to tie a building’s design and landscape together."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:a35e3d5a-f0a7-4ab1-9dda-cbb63f7baba6>","<urn:uuid:ca0b7f69-bec4-4752-b784-7aaf975d65ce>"],"error":null}
{"question":"What are the characteristics of mountain basin formation, and how do debris slides affect these geological structures?","answer":"Mountain basins form through integrated geodynamic processes involving tectonic environment, geologic history, rock weathering, erosion, and sediment transport. These formations are influenced by mechanisms for basin formation and subsidence, sedimentary record preservation, and sedimentary geometry. In areas like the Southern Appalachians, these basins are significantly impacted by debris slides and flows, which typically occur in pre-existing depressions and expose bedrock through erosional scars. Over time, these exposed areas fill with loose sediment that creeps and washes into channelways, while debris fans become incised and revegetated. The presence of old, deeply weathered sediment fans in these regions raises questions about their formation processes under different climatic conditions.","context":["Pittsburgh, PA 15260\nDr. McQuarrie received her Ph.D. at the University of Arizona in 2001 with an emphasis in Structure and Tectonics. She was a Post-Doctoral Scholar at California Institute of Technology from 2001-2004 and an assistant professor at Princeton University before joining the Department in 2011. Dr. McQuarrie spent a year at Tuebingen University in Germany as an Alexander von Humboldt Research Fellow (2011-2012).\n- Laboratory Members\nOur group studies the geometric, kinematic and erosional evolution of mountain belts, particularly in the Himalaya, Andes and Appalachians. Current research activities focus on linking the geometry and kinematics of mapped structures to thermochronometer cooling ages to look at the interplay between tectonics and erosion on exhumation patterns and morphology of mountain ranges. Research projects start with structurally based field studies, typically through the creation of new geologic maps at previously unpublished scales or resolutions. Using this high resolution geological mapping as a foundation, the projects expand to include the creation and sequential restoration of geologic cross sections (providing kinematics) as well as new mineral cooling ages to determine the distribution, magnitude and rate of deformation. Details of current projects available here.\n*Buford Parks, V.M., McQuarrie, N., (accepted) Kinematic, flexural, and thermal modelling in the Central Andes: Unravelling age and signal of deformation, exhumation, and uplift. Tectonophysics.\n*Olsen, J.E.S., McQuarrie, N., Robinson, D., (2019) Determining kinematic order and relative age of faulting via flexural-kinematic restoration: a case study in far-western Nepal. Basin Research, doi: 10.1111/bre.12362\n*Gilmore, M.E., McQuarrie, N., Eizenhöfer, P and Ehlers T.A., 2018, Testing the effects of topography, geometry and kinematics on modeled thermochronometer cooling ages in the eastern Bhutan Himalaya: Solid Earth, v. 9, p. 1-29, https://doi.org/10.5194/se-9-1-2018.\n*Rak, A., McQuarrie, N. and Ehlers T.A., 2017, Kinematics, exhumation, and sedimentation of the north-central Andes (Bolivia): An integrated thermochronometer and thermokinematic modeling approach: Tectonics, v. 36, doi: 10.1002/2016TC004440.\n*Tate, G.W., McQuarrie, N., Tiranda, H., van Hinsbergen, D.J.J., Harris, R., Willett, S.D., Reiners, P.W., Fellin, M.G., Zachariasse, W.J., 2017, Reconciling regional continuity with local variability in structure, uplift and exhumation of the Timor orogen: Gondwana Research. Gondwana Research, v. 49, p. 364–386, http://dx.doi.org/10.1016/j.gr.2017.06.008\nSchepers, G., van Hinsbergen, D.J.J Spakman, W. Kosters, M., Boschman, L., and McQuarrie, N., 2017, South-American plate advance and forced Andean trench retreat as drivers for transient flat subduction episodes: Nature Communications, v. 8, 15249, 10.1038/ncomms15249\nGarzione, C.N., McQuarrie, N., Perez, N.D., Ehlers, T.A., Beck, S., Kar, N., Eichelberger, N., Chapman, A.D., Ward., K.M., Ducea, M., Lease, R.O., Poulsen, C.J., Wagner, L.S., Horton, B.K., Saylor, J.E., Zandt, G., 2017, The tectonic evolution of the Central Andean Plateau and geodynamic implications for the growth of plateaus, Annual Reviews of Earth and Planetary Sciences, v. 45, p. 529-559. https://doi.org/10.1146/annurev-earth-063016-020612.\nMcQuarrie, N. and Ehlers T.A., 2017, Techniques for understanding fold-thrust belt kinematics and thermal evolution: in Law, R.D., Thigpen, J.R., Merschat, A.J., Stowell, H.H., (eds) Linkages and Feedbacks in Orogenic Systems, Geological Society of America Memoir, 213, doi:10.1130/2017.1213(02).\n*Tate, G.W., McQuarrie, N., van Hinsbergen, D.J.J., Bakker, R., Harris, R.A. and H. Jiang, 2015, Australia going down under: Quantifying continental subduction during arc-continent accretion in Timor-Leste: Geosphere, v. 11, doi:10.1130/GES01144.1.\nMcQuarrie, N. and Ehlers T.A., 2015, Influence of thrust belt geometry and shortening rate on thermochronometer cooling ages: Insights from the Bhutan Himalaya: Tectonics, 34, doi:10.1002/ 2014TC003783.\nChapman, A.D., Ducea, M.N., McQuarrie, N., Coble, M., Petrescu, L., Hoffman, D., 2015, Constraints on plateau architecture and assembly from deep crustal xenoliths, northern Altiplano (SE Peru): Geological Society of America Bulletin doi:10.1130/B31206.1\nEichelberger, N., and McQuarrie, N., 2015, Kinematic Reconstruction of the Bolivian Orocline: Geosphere v. 11, p. 445-462, doi:10.1130/GES01064.1\nEichelberger, N., and McQuarrie, N., 2015, 3-D Finite Strain at the Andean Orocline and Implications for grain-scale shortening in orogens: Geological Society of America Bulletin. v. 127, no. 1-2, p. 87-112 doi:10.1130/B30968.1\nTate, G.W., McQuarrie, N., van Hinsbergen, D.J.J., Bakker, R.R., Harris, R., Willett, S., Reiners, P., Fellin, M.G., Ganerød, M., Zachariasse, J-W., 2014, Resolving spatial heterogeneities in exhumation and surface uplift in East Timor: constraints of deformation processes in young orogens: Tectonics v, 15, p. 1089–1112. doi:10.1002/2013TC003436\nMcQuarrie, N., Tobgay, T., Long, S.P., Reiners, P.W., Cosca, M.A., 2014, Variable exhumation rates and variable displacement rates: documenting a recent slowing of Himalayan shortening in western Bhutan: Earth and Planetary Science Letters, v. 386, p. 161-174, doi.org/10.1016/j.epsl.2013.10.045\nEichelberger, N., McQuarrie, N., Ehlers, T.A., Enkelmann, E., Barnes, J.B., Lease, R.O., 2013, New constraints on the chronology, magnitude, and distribution of deformation within the central Andean orocline: Tectonics, v. 32, p. 1-22, doi:10.1002/tect.20073.\nLeier, A., McQuarrie, N., Garzione, C., Eiler, J., 2013, Oxygen isotope evidence for multiple pulses of surface uplift in the Central Andes, Bolivia: Earth and Planetary Science Letters, v. 371-372, p. 49-58, doi: 10.1016/j.epsl.2013.04.025\nMcQuarrie, N., Long, S.P., Tobgay, T., Nesbit, J.N., Gehrels, G., Ducea, M., 2013, Documenting basin scale, geometry and provenance through detrital geochemical data: lessons from Neoproterozoic to Ordovician strata of Bhutan: Gondwana Research, v. 23, p. 1491-1510, doi 10.1016/j.gr.2012.09.002.\nMcQuarrie, N., and van Hinsbergen, D.J.J., 2013, Retrodeforming the Arabia-Eurasia collision zone: Age of collision versus magnitude of continental subduction: Geology, v. 41, p. 315-318, doi:10.1130/G33591.1.\nFull list of Publications available here.\nGEOL 1100 Structural Geology\nAn introduction to basic geologic structures, including the development of folds, faults, joints, and foliation. The use of these structures in geologic mapping, and their interpretation in terms of structural geometry will be covered. The concepts of stress and strain will also be introduced. These basic concepts will be integrated into a study of the evolution of mountain belts.\nOffered every spring term. Lecture 3 hours; laboratory 2 hours.\nGEOL 2110 Plate Tectonics\nHistorical background of the concept of plate tectonics. Geophysical evidence for reconstructing the motions of continental and oceanic areas. Plate tectonic processes and characteristics of plate boundaries. Dynamics -- the nature of the driving forces. Geosynclines, orogenic belts and crustal evolution will be examined with regard to plate tectonic theory. Types of plate boundaries (divergent and convergent zones, transform faults) will be studied and compared to existing geotectonic features.\nOffered every other year. Lecture, 3 hours.\nGEOL 2120 Basin Analyses\nThe integrated study of sedimentary basins as geodynamic entities, including tectonic environment, geologic history and associated strength of the lithosphere, rock weathering and erosion, and sediment transport. The class will give students a background in driving mechanisms for basin formation and subsidence, sedimentary record preservation and alteration, sedimentary geometry, facies and petrology and provide a basic understanding of the continuum mechanics equations that approximate basin formation.\nOffered every other year. Lecture, 3 hours.\nGEOL 3908 Topics in Geology\nThis course allows the flexibility of exploring, in depth, a new topic in geology each time it is taught. Past topics have included:\nAppalachian Geology -- This seminar course is designed to cover all things Appalachian; tectonic setting, paleoclimate record, thermal history, sedimentology, stratigraphy, depositional environments, structural geology (regional, outcrop, to micro scale), geomorphology (regional and local scales), geochemical signatures, etc.,\nFold-thrust belts and sedimentary basins -- Paired fold-thrust belts/foreland basin systems produce the largest mountains in the world as well as host a large percentage of the world’s oil and gas reserves. Understanding the geometry and kinematics of this paired system is a requirement for understanding how these mountain systems and the reservoirs of natural resources form. Reading assignments and lectures will cover the origins of fold-thrust belts, thrust belt geometry, mechanics and kinematics as well as learning the technique of balancing cross sections through a fold-thrust belt.","Debris slides and flows in the Southern Appalachians - Historical, Process, and Environmental GeomorphologyG. Michael Clark\nThe University of Tennessee at Knoxville\nContinent: North America\nState/Province: Tennessee/North Carolina\nUTM coordinates and datum: Regional area is centered about 35° 30' N; 83° 30' W\nClimate Setting: Humid\nTectonic setting: Passive Margin\nClick the images for a full-sized view.\nThe part of the southern section of the Blue Ridge province lying mainly within southeastern Tennessee and western North Carolina is a region with the highest U.S. elevations (up to 2000 m) and relief between ridge crests and valley floors east of the Rocky Mountains. Mean annual precipitation values are high, up to 2500 mm on high mountain summits, as are precipitation-frequency values associated with thunderstorms, occluded low-pressure fronts, and hurricane remnants. Thus, topography and climate, plus human land use and other factors, combine to foster rapid mass wasting processes in this region. Written accounts since about 1800 AD allow us to compile a record of major occurrences of events of multiple debris slide and debris flow mass movements produced by intense rainfall events and such areas can be investigated in the field.\nWhat are some process-response linkage problems associated with present-day mass movement occurrences? The heads of almost all slide scars occur some distance below the ridge crests, occur in slight pre-existing depressions, and expose bedrock, suggesting that surface shear stress of overland flow and/or rapid pore-water pressure buildup at the base of the soil cover might be triggering factors. How could we test these postulates? Another set of questions relates to why certain hillslope hollows fail in a storm, and others nearby on the same mountainside do not. Are there local factors, such as concentrations of bedrock fracture zones or minor variations in soil texture or hillslope angle?\nOver succeeding years, the exposed bedrock in older debris slide scars and flow tracks tend to fill in with loose sediment that creeps and washes into the channelways, the debris fans become incised, and revegetation occurs. In addition to debris fans that can be attributed to mass wasting processes since European settlement times, many hundreds of old, very-deeply weathered sediment fans occur in the region, and pose interesting geomorphic questions. Were these pre-settlement fans also produced by cataclysmic rainfall events? Or, might deposits have been deposited by different processes that operated under other climatic conditions? For example, although there is no evidence of glaciation during the Cenozoic Era in the southern Blue Ridge, it is likely that ice age alpine cold-climatic environments occurred at higher elevations, conditions that would have fostered snow avalanches and alpine debris flows events.\nDebris slide and flow events displace great volumes of regolith downslope and the resultant erosional scars bare bedrock and regolith surfaces to surface weathering processes. Some geomorphologists believe that these processes are important agents in the formation of hillslope drainangeways in climatic environments ranging from the subarctic to the humid tropics. And, there are practical reasons for learning more about these mass-wasting processes. Debris flows are high volume rapid mass movements that constitute a major geomorphic hazard. The southern Blue Ridge is conveniently located within driving distance of a large percentage of the US population, has many tourist attractions, and has experienced a rapid increase in visitation especially during summer months when high rainfall events are most common. In addition, many scenic mountain areas are being developed for homesites, second homes, and retirement communities. Many developments are sited in hillslope, footslope, and toeslope locations we know are sites where rapid debris flows occur. And, the effects of many construction projects produce landscape disturbance and subsurface effects that foster increased mass movement activity. Finally, what can be predicted from models of global change? Will mountain hillslopes become more or less stable with changing patterns of precipitation, temperature, and their effects on vegetation and weathering processes?\n- Clark, G.M., 1987, Debris slide and debris flow historical events in the Appalachians south of the glacial border: Geological Society of America, Reviews in Engineering Geology, Volume VII, p. 125-137.\n- Guzzetti, F., Peruccacci, S., Rossi, M., and Stark, C.P., 2008, The rainfall intensity—duration control of shallow landslides and debris flows: an update: Landslides, v. 5, no. 1, p. 3-17.\n- Hack, J.T., and Goodlett, J.C., 1960, Geomorphology and forest ecology of a mountain region in the Central Appalachians: U.S. Geological Survey Professional Paper 347, 66 p.\n- Mills, H.H., 1982, Long-term episodic deposition on mountain foot slopes in the Blue Ridge province of North Carolina; Evidence from relative-age dating: Southeastern Geology, v. 23, p. 123-128.\n- Thornbury, W.D., 1965, Regional geomorphology of the United States: John Wiley and Sons, New York, 609 p.\n- Wooten , R. M., Gillon, K. A., Witt, A. C., Latham, R. S., Douglas, T. J., Bauer, J. B., Fuemmeler, S. J. and Lee, L. G., 2008, Geologic, geomorphic, and meteorological aspects of debris flows triggered by Hurricanes Frances and Ivan during September 2004 in the Southern Appalachian Mountains of Macon County, North Carolina (southeastern USA): Landslides, v. 5, no. 1, p. 31-44."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:e88e21cd-d3ea-40d7-9d30-195d9d069f99>","<urn:uuid:03d23078-1ab5-4a5f-a39e-0fe8d349f8a9>"],"error":null}
{"question":"How does international treaty ratification differ from the process of passing constitutional reforms in the UK - which one requires more steps?","answer":"International treaty ratification involves a multi-step process: text adoption, opening for signature, state signatures indicating intention, and formal ratification/accession procedures involving deposit with the UN Secretary-General. In contrast, UK constitutional reforms can be enacted through a simpler process requiring just an Act of Parliament, without mandatory referendums (unlike many EU states). For example, the 1996 handgun ban required only an Act of Parliament. The UK's uncodified constitution allows for easier adaptation than formal treaty processes.","context":["International law governs relations between states, in matters such as the drawing of boundaries between states, the laws of war, laws governing international trade, and laws regulating the global environment. As well, international law governs relations between states and individuals. It does this by holding states accountable to the international community for the extent to which they recognise and protect human rights within their borders.\nMuch international law is created in the various institutions of the UN, which currently has 193 member states. Australia became a member of the UN when it was founded in 1945 and the Australian statesman, Dr H E Evatt, played a significant role in its establishment. Regional organisations such as the European Community and the Association of South East Asian Nations (ASEAN) also contribute to making international law.\nToday, most international law takes the form of treaties (also known as covenants, conventions, agreements, pacts and protocols), which are binding agreements between national governments. Statements and resolutions made by international organisations like the United Nations, and customary modes of behaviour by states, also contribute to the formation of international law.\nA treaty is defined as:\n‘An international agreement concluded between states in written form and governed by international law, whether embodied in a single instrument or in two or more related instruments and whatever its particular designation.’\nArticle 2, Vienna Convention on the Law of Treaties 1969\nHuman rights in international law\nIn international law, human rights are recognised in three principal ways:\n- international treaties, covenants and conventions (also known as ‘treaty law’);\n- customary international law; and\n- resolutions of the United Nations General Assembly.\nInternational treaties , covenants and conventions\nIn the area of human rights, ‘express agreements’, which include treaties, conventions, covenants, instruments, pacts and protocols, are the most significant source of international law.\nThe term ‘convention’ is frequently employed for agreements to which a large number of countries are parties. ‘Protocol’ usually refers to an agreement that amends or supplements an existing convention or agreement.\nThe law of treaties concerns obligations that result from express agreements. The basic principle of treaty law is that agreements are binding upon the parties to them and must be performed by them in good faith. Similar to a contract, an international treaty imposes binding obligations on states that are parties to it. The parties accept responsibilities towards each other through mutual obligations and as with a contract, one treaty party can call other parties to account for their actions. Treaties can be bilateral (between two countries) or multilateral (between more than two countries).\nBecoming a party to a treaty is a legal process that involves a series of steps. A state usually signs an international treaty and later ratifies it. A state will accede to a treaty it did not sign.\nIn Australia, treaties can only be entered into with the approval of the Federal Executive Council. In theory, at least, there is no need for parliamentary approval before Australia becomes bound by an international treaty: see The treaty-making process in Australia.\nThe process of making a treaty\nIn concluding a multilateral treaty, states generally follow these procedures:\nThe outcome of negotiations is generally the adoption of the text of the treaty in an international forum. Once adopted, the treaty becomes ‘open for signature’.\nBy signing a treaty, a state indicates its intention to become a ‘party’ to the treaty. Whilst signature often constitutes the first step in becoming a party, it does not mean that the state is bound by the terms of the treaty.\nRatification and accession\nRatification and accession are formal procedures by which a state indicates that it intends to be bound by a treaty. Once adopted, the treaty remains open for signature for a specified period of time. This time generally allows for ratification by the number of states that are necessary for the treaty to ‘enter into force’. Ratification is completed by a formal exchange or deposit of the treaty with the Secretary-General of the United Nations in New York. Accession is the process by which a state becomes party to a treaty it did not sign, and is only used in multilateral agreements. Accession may occur before or after a treaty has entered into force, but is usually used when the agreement has been previously signed by other states. These procedures generally occur when necessary domestic legislation or executive action is complete.","Main sources of the Constitution\nStatute law- Acts of Parliament that play a key role in defining the relationship between the gov’t and the people. – The Human Rights Act 1998 or Parliament Acts (1911 and 1949)\nCommon law- Referred to often as case law or ‘judge-made law’. Established customs and legal precedent. Most traditional civil liberties – e.g. freedom of speech (originally established in common law). The royal prerogative is also rooted in common law *inc. power to declare war/agree treaties.\nConventions- Traditions/customs evolved over time – no real legal standing. Can easily be overturned by a passing of a parliamentary statute. Doctrine of cabinet collective responsibility is rooted in convention\nEU laws and treaties -European Communities Act 1972 – UK incorporated Treaty of Rome 1957 into law. Takes precedence over our law, although Parliament reserves right to repeal the 1972 Act – thereby withdrawing from EU\nWorks of authority -Scholarly texts serve to codify practices not outlined on paper elsewhere. E.g. – Bagehot’s The English Constitution 1867, May’s Parliamentary Practice 1844, A.V. Dicey’s An Introduction to the Study of the Law of the Constitution (1885)\nPrinciples and Adaptations\n· Main principles upon which the UK Constitution is based:\nParliamentary sovereignty and rule of law\n· Reasons why the British constitution can easily be adapted to changing circumstances\n1. Uncodified constitution – no entrenched rights enumerated in a constitution like in the USA\n2. Only act of Parl. Needed, so allows events like 1996 – Parl. Outright ban on handguns after Act of Parliament passed in the wake of the murder of 16 school children in Dunblane\n3. Not a legal requirement to have a referendum like with Scottish Parliament in 1998, e.g. H of L’s reform was not – (unlike many EU states)\n4. Also due to significance of conventions – conventions shift, develop and evolve over time, allowing “evolving conventions”, said codified constitutions shape political practice – it is the opposite in the UK\nContrasts between the US and UK\n1. Revolutionary in nature ................................................. 1. Evolutionary in nature\n2. Single, authoritative document ....................................... 2. Draws from many sources\n3. Quite rigid ....................................................................3. Quite flexible\n- In theory it remains a unitary state - yet considerable power has been granted to devolved bodies (e.g. Scottish Parl. 1998, Wales Act 2006). These devolved powers would be difficult to withdraw at this stage now, without a referendum - which would not occur and the results would be negative.\n- The increased use of referendums since 1997, rise of executive dominance, and increasing media criticism of the royal family - signalled a change in the tradition of parl. gov't under a constitutional monarch.\nSeparation of Powers v. Fusion of Powers\nSeparation – Executive, Legislature and judicial should be separated through the construction of independent branches of the governing system – associated with French Enlightenment philosopher Montesquieu – commonly found in presidential democracies\nFusion of powers – Situation when these branches of government overlap – feature of parliamentary democracies\nSeparation of Powers (Against)\n· Three examples which illustrate that the UK does not have a clear separation of powers\n1. Until 2005 (CRA), Lord Chancellor was a full fusion of all branches, being speaker in H of Ls, a gov’t minister heading the Lord Chancellor’s Depart. And also the head of the judiciary.\n2. The executive is drawn from members of the legislature\n3. Members of the judiciary’s increasing politicisation, e.g. Former Lord Chief Justice Lord Woolfe\nGovernment Too Much Power?\n· Main arguments for idea that the absence of a separation of powers in the UK gives the government too much power:\n1. Prime Minister is ‘primus inter pares’ (first among equals) in principle, but in reality he/she can take a stronger role, e.g. Blair + New Labour\n2. If party wins with large majority can pass a lot of Acts in Parliament easily – Blair (1997 -179 majority).\n1. PM cannot do much without the support of Parliament – if the party in power won with a small majority or is in a coalition Cameron now – Hung Parliament\nChanges to the Constitution since 1997\n1. Constitution Reform Act 2005 – UK Supreme Court introduced in 2009, removed judicial functions of H of Ls and transferred them to the Supreme Court\n2. Judicial Appointments Commission established (JAC) – commission created by the CRA. Assumed responsibility as an independent body for selection of judges from Lord Chancellor\n3. Human Rights Act (1997) – Incoroporated European Convention on Human Rights (ECHR) (1953) into Brit Law\n4. Greater London Authority Act 1999 (London given elected mayor + an elected Assembly)\n5. Belfast Agreement (Good Friday Agreement) 1998 – Assembly to Northern Ireland nad power-sharing executive\n6. Scottish Parliament and Welsh Assembly 1998\n7. House of Lords Act 1999 – 92 hereditary peers lose right to sit and vote in the chamber\nConstitutional Reform Act 2005 - Impact\n· Examples of how the Constitutional Reform Act (2005) – has helped to shift the UK towards a clearer separation of powers\n1. Weakened Lord Chancellor’s position – previously was a representation of the fusion between all three branches of gov’t\n2. Established an independent body JAC – less politicised decisions made in the judiciary\nSupreme Court (General)\nOct 2009 - 11/12 Law Lords from the Appellate Committee moved to the Supreme Court\nThe 12 Law Lord Neuberger accepted the position of Master of the Rolls instead\n12th place filled by Sir John Dyson - first member to be appointed by the appointments process\n1. Act as the final court of appeal in England (E), Wales (W) and Northern Ireland (NI)\n2. Hear appeals on issues of public importance\n3. Hear appeals from civil cases in E, W, NI and Scotland\n4. Hear appeals from criminal cases in E, W & NI\nImpact of the New Supreme Court\nJan 2010 - Treasury v. Mohammed Jabar & others. SC ruled the UK treasure had acted ultra vires (above the authority granted)\n- The Supreme Court was doing little more than what the Law Lords previously did - so it can be questionable the need for the Supreme Court and how much/how little of an impact it has had on the legislative process.\n- Appellate Committee once acted as the highest court in the land - until 2005\nTangibles/Intangibles (New Supreme Court)\n- Appointments and composition - Significantly more independent and less opaque than previous system\n- Power - No new powers granted from the powers of the previous Appellate Committee\n- Judicial independence - been enhanced in theory - although can be said functionally the previous committee was independent too\n- Physical separation - New Court granted its own building - likely to raise profile - distinctive character and identity also provided\n- Lifting restirctions on television cameras - Emergence of new relationship between media and snr judges?\n- Changes in delivery of rulings - Website with downloadable rulings - greater public scrutiny of the workings of the Court\nFor – 1. Security of tenure – hard to be removed – impeachment (vote in both houses of Parl.) Two junior judges were removed in 2005 for misconduct, but senior judges are more difficult to remove\n2. Guaranteed salaries – free from political manipulation – drawn from the Consolidated Fund\n3. Contempt of court – under sub judice rules it is an offence for ministers and others to speak out publicly during course of legal proceeding\n4. Independent appointments system under the JAC (new in 2005)\n5. Training and experience of snr. Judges. Pride they take in role and personal legal reputation\nAgainst- 1. Pressure from the Legislature or Executive, or the media\n2. Self interest\n1. Everyone is equal under the law\n2. No one is above the law\n3. Everyone is entitled to a free and fair trial [can be questionable - as those with money can afford better lawyers, and therefore will be more likely to win (so not very fair)].\n1. Attorney General and Solicitor General (two v. important judicial figures) sit in on cabinet meetings - can they really be neutral?\n2. Many judges have a specific background - (white, male, over 40, attended Oxbridge), can they really be free from bias?\n- UK Courts cannot declare statutes unconstitutional - as statute law is the supreme source of constitutional law in the UK\n- However, can determine if gov't officials have acted ultra vires\n- Comparing it to the US - the US courts can be seen as having greater power than the UK Supreme Court - as the US one can declare acts unconstitutional\nOrganisation of the UK Judiciary\nUK SUPREME COURT\n(role previously held by the House of Lords)\nCOURT OF APPEAL\n(Criminal and Civil Divisions)\nCROWN COURT AND COUNTY COURTS\n(Magistrates' courts and tribunals)\n- Judges - JUSTICE IS DONE (main priority)\n- Supreme Court responsibilities include CLARIFYING MEANING of law, not just APPLYING IT.\n- Cases heard in the Court of Appeal and in Supreme Court usually result from confusion in lower courts regarding the meaning of a law. - Courts also deal with major cases arising from teh HRA 1998, or under EU law\n- Higher tiers have the power to set legal precendent - establishing common law\nCivil Law / Criminal Law\n- Individual vs Group\n- E.g. Wills or contracts\n- Most result in compensation rewards\n- Individual vs State\n- E.g. Violent behaviour, fraud, or burglary\n- Most result in fines or imprisonment\nJudicial Appointments Commission\n- Leading to a greater separation of powers - more independence\n- Can result in a senior judiciary that was more representative of the broader population\n- HOWEVER - in 2008 the Guardian reported that the new JAC had approved 21 individuals, 10 of thses had already been given posts:\n- All were white, male, former barristers\n- 2008 appointments, only 8% were from black or Asian backgrounds (14% in 2005-06), only 34% were women (41% in 2005-06)\nAppointment to the Supreme Court\n- QUALIFICATIONS - helf a high judicial office for at least two years OR been a qualifying practitioner for a period of 15 years\n- APPOINTMENTS PROCESS:\n- 1. Vacancy\n- 2. 5-member ad-hoc Selection Commission convened to consider possible nominees\n- 3. Commission submits a report to the Lord Chancellor - naming a nominee\n- 4. Lord Chancellor has three options - a. accept selection by notifying PM, b. Reject selection, c. REquire the commission to reconsider\n- 5. Once notified the PM must recommend the approved candidate to the Queen\n- 6. Individual appointed when Her Majesty issues letters patent\nFactortame Case 1990\n- Before 1990 The European Court of Justice could challenge UK statutes. However, following Factortame UK Courts have also been able to suspend UK statutes that appear to be in violation of EU law.\n- Factortame - Spanish fishermen complained that the Merchant Shipping Act 1988 violated the Single European Act 1986.\n- Highest Court referred the case to the European Court of Justice (ECJ)\n- An instruction was given not to apply key sections of the Act until a final ruling\n- The House of Lords overturned this instruction - arguing that no UK court had the power to suspend a statute - until the ECJ could make a final determination\n- In 1990 - ECJ ruled that the UK courts do have the power to suspend Acts of Parl. that appear to break EU law.\nHuman Rights Act 1998\n- Came into force in Oct 2000 - incorporated most articles in the ECHR into UK law - thereby allowing citizens to pursue cases unter the ECHR rights in UK courts\n- It is not superior to parl. statute - as it is based on ECHR not EU law\n- However, has enhanced individual rights\n- Does not have the same legal status as EU law or the US Bill of Rights\n- HRA does not give the courts the necessary power to stop or overturn gov't action - e.g. 2001 Crime and Security Act - could authorise the indefinite detention without trial of foreign nationals whom the home secretary judged were involved in terrorism\n- HRA been used in a number of non-terrorist-related cases e.g. Jamie Bulger killers granted lifelong anonymity\n- Questionable articles: Article 2: right to life, Article 3: Prohibition of torture, Article 8: respect for private and family life, Article 10: freedom of expression\n- European Convention on Human Rights (ECHR) -\n- Established in 1950.\n- By the Council of Europe - intergovernmental body - totally separate from the European Union"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:8472b217-e27b-488c-91b4-19faa8fabe95>","<urn:uuid:52f67851-11b7-4a27-b325-bb69d301650f>"],"error":null}
{"question":"How do artificial intelligence applications compare between modern warehouses and restaurants? Could you explain the key differences in how AI is being utilized across these two sectors?","answer":"In warehouses, AI is primarily used for asset tracking, managing the flow of goods, and preventing errors in shipping. For example, AI systems help track delays, bottlenecks, and potential errors in real-time through sensor data. In restaurants, AI serves different purposes - it integrates with digital menus, POS systems, and inventory control to automatically adjust menu prominence based on ingredient availability, provide data-based ordering recommendations, and analyze ingredient ROI. Companies like McDonald's are implementing AI systems that not only manage their menu and inventory but even monitor their garbage. Both sectors use AI to improve efficiency, but warehouses focus more on logistics and movement tracking, while restaurants emphasize menu optimization and ingredient management.","context":["Traditionally, warehouses have been sprawling, manually operated facilities with a handful of forklifts, a waiting room for truck drivers and stacks of paperwork to document transactions. However, shifts in retail distribution have boosted the distribution center and warehouse industry into a high-tech, fast-paced world. Today, managers process unprecedented volumes of goods, while keeping workers safe and moving products to and from trucks within hours—sometimes while packing and repacking those goods—and do it with as few errors as possible. Enabling this transition is technology such as robotics, drones, mass conveyor systems and the internet of things (IoT) technology to track what is taking place in real time.\nIntegrators are taking on a variety of installations to help the logistics industry meet the demands of an impatient world.\nThe evolution from standard warehouse to intelligent distribution center was already underway when COVID-19 upended the economy and the supply chain. In fact, digitalization and intelligence in retail distribution vaulted forward in 2020 at a rate that had been on a 10-year trajectory before the pandemic. In part, this was fueled by those ordering goods that they expect to receive today, not in a few days or next week.\n“It starts with the consumer,” said Jim Dempsey, director of U.S. business development and partnerships at Panasonic System Solutions Co. of North America, Newark, N.J. While initially the expectation was for orders to be fulfilled, shipped and received within a week or two, buyers now order food, household goods, office supplies or manufacturing tools and supplies with the expectation of receiving the product in just a few hours.\nThat has required many changes in warehouses. New facilities are opening closer to customers, receiving, storing and shipping a wider variety of products. Because goods move so quickly, more pickers are at work in the warehouses, too.\nAdditionally, Dempsey said returns are skyrocketing. As much as 30% of goods ordered online are being returned today. That means warehouses are receiving a high volume of product from both ends of the supply chain.\nWith the pandemic still untamed, employers need to ensure workers don’t come into close proximity with each other and potentially cause a spreader event in the warehouse. Tech tools can solve most of these problems and are widely employed.\nPanasonic sells products equipped with technology such as Bluetooth, Wi-Fi and RFID for asset tracking, and it also has an IoT Solutions division focused on technology to help numerous industries, including warehouses, track people and equipment. With location-based data, warehouses can leverage real-time and historical information based on sensor data about the flow of goods in the facility, delays, bottlenecks and potential for error.\nHealthcare is another market with critical supply-chain requirements. Pharmaceutical and medical supply company McKesson Corp., Irving, Texas, uses intelligent warehouses to move its healthcare products and medications.\nAt a typical McKesson warehouse, thousands of workers pick and pack goods together with automated systems; speed and accuracy are critical. McKesson uses integrated technology to reduce workers’ steps with A-Frame—a self-contained, automated piece-picking machine—for the fastest moving products. This configurable technology is used for picking and packing small goods such as pharmaceuticals, cosmetics, tobacco, office items and contact lenses. An order storage and retrieval system also brings products to associates as they are packed for orders, and the system automatically detects the goods and their expiration dates before shipping to the customer.\nSystem software, in many cases, collects data about what is taking place in real time so management can oversee operations without being directly on the busy floor, thereby increasing the ability to work remotely and reduce the number of people in the warehouse.\nMobile devices, fixed readers and scanners identify what is being picked or packed, as well as when goods come into and leave the warehouse, reducing the need to stop and scan bar codes and preventing errors before they can happen, such as a carton being loaded on the wrong van.\nBlockchain data captures the details related to what takes place on-site and stores that information in immutable ledgers that can be viewed by warehouse managers, shippers and customers to understand what is occurring during the loading of goods.\nIn addition to conveyor belts, automatic vehicles such as driverless forklifts and drones help keep track of what is on-site. Some of this warehouse automation includes conveyors and robotics—an automated system to receive, store and retrieve goods as needed. Robotic capabilities are continuing to improve. In 2019, Amazon, had 200,000 robots already in use, working alongside hundreds of thousands of human workers.\nOver time, this smart warehouse technology could reduce the carbon footprint of some of the largest logistics centers, since the added efficiency and visibility results in less movement of goods on-site and fewer workers who require lighting and HVAC systems.\nStill, warehousing technology remains woefully behind that of today’s manufacturing facilities. Logistics Bureau, a global specialty management consulting company headquartered in Sydney, has also indicated a fast-paced change in warehouse management even before COVID-19 put new pressure on the supply chain. The long-term goal is to reduce the number of workers on-site by using technology to make warehouse activities more efficient. Faster picking is accomplished through software on forklifts that points workers to the location of goods, using technology such as real-time location systems.\nPaperwork is eliminated as each transaction is automatically stored based on the location of the forklift, the warehouse operator and the delivery vehicle parked at the dock doors. For instance, TagMaster North America, Tacoma, Wash., has a solution in which battery-assisted passive RFID tags can be applied to forklifts to identify when they are approaching an automated door and confirm whether the door will open. The same technology can be used to alert drivers if they come within range of each other, which helps prevent collisions.\nIn the long term, the focus will be on reducing the need for personnel in more automated warehouses, which could reduce expenses on several levels, including the cost of energy used on-site as well as payroll. When it comes to lighting, LED industrial fixtures are replacing the conventional lamps, which use a fraction of the energy required traditionally, while the level of lighting required can be lower, which saves carbon footprint and cost.","Innovation has always been a pillar of the restaurant industry. In fact, the original concept of a “restaurant” itself was arguably one of the most innovative developments in the history of commerce.\nAnd while innovation doesn’t always involve technology, the two have become increasingly intertwined in the digital age. Even in good times, our industry operates on relatively thin margins. This is a “take dollars to make pennies” business.\nWith each passing year, exponential technologies like artificial intelligence and robotics become more efficient, user-friendly, and affordable, creating an inescapable wave of disruption across the global economy.\nThis necessitates a higher level of tech adoption, a faster rate of adoption, and an overall different mindset for running a restaurant than in previous decades.\nThe best chefs and marketers have always embraced these things. But because the pace of change is quickening, restaurant owners and managers need to adopt the innovative, new-is-better mindset as well.\nLuckily, the possibilities that change is affording are not only endless but exciting. The tech innovations we’ll be discussing don’t just offer a competitive advantage; they represent a holistic approach to better business.\nGame-changing tech: Back-of-house\nArtificial Intelligence (AI)\nFor some, artificial intelligence and eating out may seem like totally separate worlds. But we’re not talking about robots running restaurants here (that’s a different discussion), we’re talking about the power of predictive, self-governing software capable of “learning” as it goes.\nPicture this: an AI-run program seamlessly integrated with a restaurant’s digital menu, point-of-sale (POS) system, and its inventory control system. When items are ordered from the menu, inventory data is automatically adjusted. If stock of a certain ingredient is low, the AI can reduce the menu prominence of items containing that ingredient. If another ingredient is overstocked, the program can adjust the menu accordingly. In both cases, it stores information about what gets ordered when and offers data-based recommendations about future ordering. What if that same system can provide data that helps the restaurant manager understand the ROI of each ingredient on the menu?\nFar-fetched though it may seem, some restaurants, including McDonald’s, are already rolling out integrated systems very much like this. Stranger-seeming still, McDonald’s is even using an AI-based system (courtesy of Compology) that monitors their garbage.\nThese are still just the tip of the iceberg when it comes to AI, with many other applications already being used. Educating quick-service restaurant operators on those opportunities is one of the goals of ConverseNow, which raised $3.25 million in funding earlier this year. Their platform is customizable, self-described as “plug and play,” and counts labor savings, higher average tickets, and overall revenue increase among its top benefits.\nConvenient and secure, cloud computing is the foundation of most modern applications of digital technology. For the uninitiated, storing something “in the cloud” just means making it accessible via remote servers. Restaurants have largely embraced the cloud when it comes to POS but less so when it comes to back-of-house business like accounting and enterprise resource planning (ERP). The game-changing potential of the cloud lies with its ability to make information available in real-time from almost any location. Without the cloud, getting the info you need about a particular location often requires you to be in that location or otherwise wait until it can be sent to you.\nWhat does that mean?\nIt means that adding new locations, users, or features to your data systems can be done with a few clicks, instead of requiring substantial investments in space, servers, or other hardware. It means that sharing or aggregating data across locations in order to gain insight into operations can be done almost instantaneously. It means that securing, integrating, and troubleshooting your data systems doesn’t require an IT team to visit your store. Overall, it means easier, more efficient business processes.\nBut what’s described above is, frankly, a bit boring when it comes to how the cloud is actually empowering restaurants, a reality that’s really come home to roost in 2021. Even before the pandemic—but especially because of it—we’re witnessing the rise of entire cloud kitchens, a brand new type of restaurant that totally eschews the need for a front-of-house in the first place (maybe the name Rebel Foods rings a bell). This is unfolding all over the globe in a number of different formats, and the growth of the model is happening at a faster pace than experts predicted.\nInternet of things (IoT)\nEvery kitchen has had its walk-in freezer accidentally left open overnight at least once, just like every chef has forgotten to check the oil level in a cooking vessel during a busy shift. An expectation of these kinds of mishaps is built into the mindset of restaurant managers, but the internet of things is working on changing that.\nIoT refers to the vast network of devices in the world, from sensors to satellites, that are capable of connecting with one another. In a kitchen, it might include everything from deep fryers and food processors to fridge doors and dishwashers. The outcome of such high connectivity is the ability to monitor kitchen ecosystems as a whole. This means not only having the data to support decision-making in real-time scenarios but also the data to better understand operations from a bird’s-eye view, including things like peak energy and equipment usage.\nRestaurant Technologies uses data about cooking oil usage to drive dynamic distribution, making it possible to only deliver oil at the times when customers need it. That same data allows for real-time usage analysis, which means a restaurant manager can be informed immediately via text if too much oil is being used. Operating a fryer might seem simple from an outside perspective, of course, but in reality, it has a huge impact on efficiency and food quality. Even with the relatively small amount of data we collect, it’s possible for us to help customers manage those factors by doing things like monitoring fryer filtration and providing alerts about operating procedure deviations—all made possible via the IoT.\nIn a nutshell, then, the IoT is what makes AI and cloud software such potent and useful technologies for restaurant operators. Without it, implementing AI in most back-of-house processes would be virtually impossible, and cloud kitchens would still be a pipe dream. As it stands, IoT technology has assisted in some major successes for quick-service restaurants and promises to play an even bigger part in the future.\nWant fries with that combo?\nBy now it’s probably obvious that these technologies are closely interlinked and that none of them without the others would have the same impact. It’s this synergy that makes them so powerful and so necessary for restaurants to adopt the innovative mindset that they are a product of. Whether they take the form of point-of-sale software, kitchen display screens, or an ERP suite, the sum of their influence is much greater than its parts.\nIt’s not just about making things easier on chefs or BOH managers. It’s about trimming margins by increasing safety, eliminating data entry, and raising overall efficiency. It’s about making restaurants more sustainable by reducing food waste, improving supply chain and inventory control, and data-based energy management. In a nutshell, it’s about better business in an age where it’s not only possible but necessary. Put simply, tomorrow’s back-of-house looks a lot different than yesterday’s—and you can have it today, if you want.\nP.S. We didn’t even discuss one of the coolest things on the restaurant horizon—kitchen robotics! But don’t worry, we’ll come back to it another day. This is the first in a series of articles exploring game-changing restaurant tech."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:3108f115-691e-4087-9237-359637ce701c>","<urn:uuid:45f9160e-42fb-4d99-9c28-a537eaf92d61>"],"error":null}
{"question":"What are the similarities between modernizing older manufacturing equipment and implementing cloud-based CRM solutions in terms of deployment flexibility?","answer":"Both manufacturing equipment modernization and cloud-based CRM solutions offer flexible deployment options. In manufacturing, equipment is categorized into network-enabled, network-ready, and no network capabilities, with different approaches for each category - from straightforward connections to requiring retrofits and sensors. Similarly, CRM systems like Salesforce offer various deployment options including cloud-based, on-premises, and hybrid environments. Both systems can be implemented gradually - manufacturing plants can start small and add incremental capabilities, while CRM solutions can be scaled and customized based on business needs with options for both cloud and on-premise implementations.","context":["Many manufacturing industries are trying to figure out how to employ brilliant factory technologies to their existing operations in the hopes of maximizing their production output and streamlining costs.\nWhile there is no set way to accomplish this, Katie Moore, industry marketing manager at GE Digital, says starting small is a good approach to get the process rolling. By modernizing plant operations in a food production facility, operators can identify potential issues impacting production, streamline costs, extend the usefulness of existing equipment and empowering them to make better business decisions.\nFood Manufacturing recently corresponded with Katie to discuss these topics and more.\nQ: What are the key benefits to employing brilliant factory technologies to existing operations in the food manufacturing industry?\nA: The bottom line is: it’s about employing a digital thread – seamlessly tying the flow of information from design through manufacturing to end consumers, including the full life cycle of the product. It’s about answering the question: how do we look at manufacturing and introduce data and analytics like never before? Manufacturing digitization is not a new concept, but the data today is stuck in silos. Brilliant manufacturing technologies enable food manufacturers to make better decisions using the aggregated information and data from existing equipment and systems. Ideally, the future is moving from reactive, gut-based decisions to predictive, data-driven decisions.\nQ: If food manufacturers do not want to implement all new technology and change current processes (or are deterred by the costs), what is the best approach?\nA: Start with a data collection strategy. Link it to the primary key outcomes or benefits that you are trying to drive within your food and beverage manufacturing – like improving production efficiency, ensuring product quality, or driving out utility costs. Ensure the key stakeholders are involved including operations, IT, quality and maintenance functions. Digitization is a critical start, but it still doesn’t optimize – so the future must be kept in mind. Many companies ask us why manufacturers are slow to start. Many try to embark upon big bang projects that don’t flex with business changes and/or they invest in local initiatives with little ability to scale. Manufacturers should look at more off-the-shelf solutions; that way, they can begin to eat the elephant one bite at a time. Quickly get insight about basic plant information, start to see value, then get more information. Start simple, and add incremental capabilities to the platform as needed. Ultimately, manufacturers will need to partner with technology firms that have open, scalable platforms to be able to build and grow with them.\nQ: How does consumer demand (like the growing interest in sustainability and all natural ingredients) effect changes in the food manufacturing plants? How can manufacturers determine how to best accomplish what changes need to occur?\nA: Having a data storage and aggregation strategy is of primary importance. Again, it’s about extracting the data, aggregating it and then gaining additional insight, in context, to make better decision. Manufacturers may begin to collect data today that at first, may offer them little perceived value. But what’s interesting is that as consumers needs and demands change, manufacturers can begin to model the data that’s already been collected to understand how these changes may affect operations and existing processes. You don’t know what you don’t know. By starting to collect data, manufacturers can begin to draw inferences and correlations that they never knew were possible before.\nQ: How is Data-as-a-Service a key component to brilliant manufacturing in the food industry? What are the benefits?\nA: Data-as-a-service is beneficial for manufacturers who don’t want or have the capability to manage the software and infrastructure. One benefit of data-as-a-service is that the software and systems are monitored and upgraded on their behalf, for example upgrade security settings as needed. In addition, another benefit is that it helps with speed of deployment and scalability. If manufacturers aren’t ready for cloud, there are on premise offerings available.\nQ: How can food manufacturers modernize older machines to produce valuable data?\nA: Ultimately, determining what critical data must be pulled from machines is a first step and delineating the must-have data from the nice-to-have data would be next. Typically, equipment is broken up into three categories: network enabled, network ready, and no network capabilities. With the former two, connection to those pieces of equipment is fairly straightforward. With the equipment with no network capabilities – this is where the determination of what data is initially critical determines how to go forward. Retrofits, modifications and/or sensors may be necessary in order to get at the critical equipment.\nQ: What are the benefits of moving from a reactive to a predictive field service management model?\nA: As mentioned, moving to data-driven decisions that are predictive versus reactive, improves product quality, reduces production inefficiencies and potentially can lead to substantial savings to the bottom line. It’s really about “preventing” versus “treating” – sort of like healthcare – wouldn’t it be more cost effective and more ideal to prevent a disease versus treat one?\nQ: What processes are best suited for brilliant factory updating and are there other processes that are not good candidates or otherwise shouldn’t be updated?\nA: Brilliant manufacturing technologies can be hosted onsite or in the internal or external cloud, depending on the manufacturer’s preference and existing infrastructure. That being said, if cloud is the choice, the manufacturer should ask their operations and engineering teams if the application requires feedback to the PLC, is hosting in the Cloud really the right strategy? There will always be some need for on premise applications — think real-time transactions versus near-real time.","CRM is a Customer Relationship Management software that enables nurturing relationships with customers and prospects to drive sales and profitability.\nCRM consists of a broad set of applications designed to help businesses manage processes to access business information, customer data, customer interaction, automate sales, track leads, customer support, clients and contacts, support vendor, partner relationships, handle knowledge and training, assets or resources.\nToday’s CRM is highly scalable and customizable, allowing businesses to view business opportunities with predictive analytics. It will hence enable companies to gain actionable insights, streamline operations and personalize customer service based on customer’s history and prior interactions with your business.\nSalesforce CRM is known to be the world’s #1 Customer Relationship Management platform that allows users to track all customer details in one place and helps nurture more leads. It is a cloud-based software which offers a variety of services like Software as a Service (SAAS) and Product as a Service (PAAS) and runs entirely on a cloud, so there is no set up costs, no maintenance, and can work without an internet connection over a smartphone, laptop, tablet and can run on any devices\nKey Feature: Salesforce provides the fastest path from Idea to App and is unique for major reasons that it is easy, quick and effective.\nIt is a free CRM app that comes with robust and effective features. It is a smart and easy option that carries all the necessary basics that can neatly organize every single detail from the customer’s communication. It offers a unique dashboard where all detailed panel can keep up the company’s activities. It is one of the best-integrated CRM platforms that integrate with various tools like Microsoft Dynamics, Salesforce, Zapier, Shopify and many others. Hubspot CRM is a flexible and robust solution that can be linked to all places and locations.\nKey Feature: The CRM that tracks and manages interactions between a company, prospects and its customers enables users to forecast revenue, measure sales team productivity and report revenue scores.\nZoho CRM is an On-demand Customer Relationship Management software for efficiently managing customer relations. This software helps streamline extensive organizational sales, customer support, marketing and inventory management functions in a single system. It is a web-based CRM that is designed to attract, satisfy, retain and grow your business. The core of Zoho CRM is lead & contact management, sales pipeline management, and purchase control. Regardless of small, medium or large companies, Zoho CRM can automate daily business activities by tracking sales and engaging customers on different platforms. It offers real-time service for prospects, customers across all channels from a phone, live email chat, and social media.\nKey Feature: Zoho CRM is available in multiple languages like English, Chinese, French, Russian, German, Spanish, Italian, Japanese, Polish, Dutch, Turkish, Portuguese, and Swedish.\nThe Microsoft Dynamics CRM software includes various modules such as sales, marketing, and customer care. This software helps reduce time to market, improve brand consistency and deepen customer insights that help plan effectively and execute flawlessly. It is better known to engage your customers and helps bring marketing vision to life by allowing users to accelerate marketing around one idea that’s agile. It is easy to use and adapt CRM software for an all in one business solution that connects your finances, service, sales, and operations to streamline business process, enable growth and improve customer interactions.\nKey feature: This software uniquely delivers a modern approach to business applications helping unify data, relationships, build intelligence into decision making and achieve better results.\nSugarCRM is a commercially licensed open source software that provides users with greater control, lower lifetime and ultimate flexibility. It offers a variety of editions, add-on tools to best fit in your CRM vision with your budget and IT resources. Some of the notable solutions of SugarCRM include supporting teams based on user groups per department or job function. The reporting tool enables you to run different types of reports and charts that can be added on the home screen. The web-based versions specific for iPhone and Android smartphones offer plug-ins for Microsoft office, i.e., Outlook, Excel, and Word.\nKey feature: SugarCRM is available in three editions like Sugar Professional, Sugar Enterprise or Sugar Ultimate, which users can choose based on their business needs.\nSAP CRM is an integrated customer relationship management (CRM) software and one of the best tool provided by SAP. It supports an end to end customer-related processes and deals with the acquisition of customers to its final acquisition. The SAP CRM is used to have a good relationship with a customer and attract new customers. It performs CRM activities of an organization related to marketing, sales, service delivery and other client-facing front-end activities.\nKey feature: SAP CRM is one of the import tool in SAP ERP functionality that enables firms to know their customers, maximize profit and expand the revenue.\nInfor CRM is used to accelerate customer engagement, drive profitability and enrich the customer experience. It offers comprehensive capabilities for managing sales, marketing, and customer service activities. This software is best suited for small to medium-sized businesses, organizations and departments with less than 1000 users. The system offers mobile CRM capabilities allowing users to manage accounts, contacts, and opportunities from their mobile devices. It offers multiple deployment options including cloud, on-premises and hybrid environments with named, concurrent, flex and subscription licenses available.\nKey feature: The best part of Infor CRM is it is built for business, deep integration, industry-specific capabilities and unparalleled flexibility.\nOracle CRM offers broadest and most profound capabilities to help organizations drive sales, loyalty, marketing, and service effectiveness. It is built on the open, standards-based architecture that streamlines business processes, improves quality and allows vital divisions to draw from the same source of data. It is a hosted on-demand SaaS application which provides strong analytics with customer service, marketing, and contact management functionalities.\nKey feature: Oracle CRM enables organizations of all types and sizes to get smarter, more productive and get the best value period.\nSage CRM is a perfect tool for small and medium-sized businesses. It is easy to use and quick to deploy in cloud or on-premise and delivers a rapid return on investment. The software is highly scalable and customizable that grows alongside the business. Sage CRM offers rapid ROI and low-cost ownership and optimizes your business, improves customer service, reduces spending, empowers staff, helps identify problems and provides leverage.\nKey feature: Sage CRM works with Sage’s business management solutions to achieve single customer-centric view across the organizations.\nSiebel CRM is the world’s most complete CRM solution by Oracle that helps organizations achieve top-line growth and deliver excellent customer experiences across all the channels. It plays an essential role in providing Oracle Customer Experience across mobile, in-store and leveraging a wide range of Oracle foundation tools. Siebel CRM delivers both on-premise and on-demand CRM solutions tailored as per industry needs covering role-based customer intelligence and pre-built integrations.\nKey feature: Siebel CRM offers Server framework to support Seibel applications while delivering solutions for Deployment, Development, Diagnostic, Productivity, Integration and Mobile services."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:77203da2-2330-4e6a-9505-5a684920c67e>","<urn:uuid:49019803-663a-4bff-bf7b-df2c2863684f>"],"error":null}
{"question":"How do lifetime value (LV) and customer acquisition cost (CAC) metrics compare in measuring customer profitability?","answer":"Lifetime value (LV) and CAC measure different aspects of customer profitability. LV calculates potential revenue by multiplying average sale value, percentage gross profit, purchase frequency, and time period of customer retention. Meanwhile, CAC measures cost efficiency by dividing total sales and marketing costs by number of new customers acquired. Both metrics are essential for business planning - LV helps project future revenue from customers, while CAC helps determine if customer acquisition costs are sustainable compared to potential revenue. Companies often compare CAC to customer lifetime value to determine their return on investment ratio.","context":["- R is change in revenue\n- C is change in the cost of doing business\n- P is change in productivity\n- I is change in intangibles\n- CRM costs are the total costs accrued over the life of the project.\nBeing able to measure increased revenues or profit is simple because your accounting system, or financial statements, should provide an established base line against which to measure improvement. But how do you measure the ROI of greater customer loyalty, or increased customer satisfaction without something to compare them against? This can be tricky, because you have to first establish the current levels of customer loyalty and customer satisfaction. And unless you have existing data that you can use, this part of the exercise is based on thumb sucks and guesswork. Lifetime Value Another method of calculating ROI is based on the lifetime value (LV) for each expected new customer and each existing customer whose business you will retain. ROI is the sum of lifetime values for all the customers minus cost of the CRM system. Of course, each customer has unique purchasing patterns and behavior. The business cycle varies for different situations, meaning that there is no single formula for these calculations. The calculation shown here is a generic formula that can be used as the basis of a calculation of the lifetime value of a customer. LV= average sale value x %GP x PF x time period where:\n- LV is the lifetime value for the customer\n- Average sale value is the average value of sales per customer\n- %GP is the percentage gross profit per average sale value for the customer\n- PF is the purchase frequency for the customer\n- Time period is the length of time they will remain a customer.\nThese calculations are tricky because the values for each customer will differ, to say nothing of the difficulty you will face if any of this information is not available from your present systems. If you only have a few customers or a few whose purchase history you track, the calculations are not too tiresome. But if you have hundreds, it represents a substantial amount of work. CRM System Costs When you calculate the cost of the CRM system, be sure to include all costs. For example:\n- Hardware purchases and/or upgrades\n- Software purchases\n- Training costs for your staff, including travel and accommodation\n- Implementation and customization fees, including consulting fees\n- Cost of software upgrades and/or annual maintenance.\nThus, ROI = Total LV for the life of the system – CRM system costs. As soon as you stop to think about these calculations you can begin poking holes in them, finding fault with aspects that are not accurate or not included. For example, we would expect to be taken to task for not using a net present value calculation to accrue CRM costs, excluding annual fees, over the life of the system. Another aspect to consider is that these calculations are all based on assumptions, and one of the assumptions is that no additional resources will be required to service the ongoing relationship. So, unless you are an accountant, or have money to spare to pay someone to do all the necessary calculations, forget ROI as a justification. Before we leave the subject of justifying your CRM, consider these questions posed by Geoff Ables, in a 2004 paper. Quick, how many customer contacts do you have every month through each channel (direct mail, phone order, on-site, online, etc.)? How much does it cost to supply an order through each channel? Which channel is most profitable? How much unused volume do you have in each channel? How much revenue can you drive through each channel? Can you integrate your channels so that you are touching your customer through all the methods you have at your disposal and creating a 360-degree experience? (Geoff Ables, 2004.) If you cannot answer any of these questions, don’t worry; you are in the same position as most companies without any sort of CRM system. The importance of knowing the answers to at least some of these questions is another justification for your CRM system. As with any project, you need someone senior in the organization to champion your cause. This person has to be someone with executive authority within the business as they, effectively, become the CRM project sponsor. This is generally a separate role from that of project manager although, in a small company and for a small project, the same person could take on both roles. It may be you who fills that role, but you need to carefully consider what is involved before you take on any specific role. If you do not want to be involved with the nitty-gritty of the installation as the project manager, and you have the necessary authority to promote the project internally and externally, set yourself up as the project sponsor. The roles of project sponsor and project manager are discussed more fully under key appointments in The Essential Guide to Articulating CRM Requirements.","Learn all about Customer Acquisition Cost (CAC) in our comprehensive Go-to-Market Dictionary.\nAs a business owner or marketer, you're always looking for ways to grow your customer base. One key metric that can help you measure success in this area is Customer Acquisition Cost (CAC). In this article, we'll take a comprehensive look at CAC from understanding what it is, to calculating it, and how to reduce it through strategic business planning.\nWhen you're trying to increase your customer base, it's essential to know how much it costs you to acquire each new customer or client. This is where the concept of Customer Acquisition Cost (CAC) comes in. CAC is the cost of acquiring a new customer, and it's determined by dividing the total cost of your sales and marketing efforts by the total number of new customers you gain during that time frame.\nEssentially, CAC is a measure of how much it costs to turn a prospect into a paying customer. This metric is essential because it helps businesses gauge the return on investment (ROI) of their marketing and sales efforts. In other words, CAC helps businesses determine if their cost of acquiring customers is higher than the revenue they'll generate from those customers. By keeping CAC low, businesses can maximize their profits and scale their operations effectively.\nOne way to keep CAC low is by focusing on building a strong brand reputation. When customers have a positive perception of your brand, they're more likely to become paying customers. This can be achieved through various marketing efforts, such as social media campaigns, influencer marketing, and content marketing.\nAnother way to keep CAC low is by optimizing your website for conversions. By creating a user-friendly website that's easy to navigate and has clear calls-to-action, you can increase the likelihood of visitors becoming paying customers. This can be achieved through website design and development, as well as through the use of analytics tools to track user behavior and make data-driven decisions.\nThe two primary components of CAC are the cost of your marketing and sales efforts and the number of new customers acquired through those efforts. These costs can include advertising and promotional expenses, salaries and commissions, and any other expenses related to acquiring new customers. By analyzing these costs, businesses can determine which marketing channels are most effective at driving sales and adjust their strategies accordingly.\nIt's important to note that CAC can vary depending on the industry and the specific marketing tactics used. For example, businesses in highly competitive industries may have higher CACs due to the cost of standing out in a crowded market.\nUnderstanding and monitoring CAC is critical for businesses to develop an effective business strategy. By analyzing this metric, businesses can identify areas where they can cut costs and focus on high-performing marketing tactics. Additionally, by comparing CAC to customer lifetime value (CLV), businesses can determine if their investments in acquiring new customers are worth the potential revenue those customers will generate over their lifetime.\nBusinesses can also use CAC to inform pricing strategies. By understanding the cost of acquiring a customer, businesses can adjust their pricing to ensure they're generating enough revenue to cover their acquisition costs and make a profit.\nIn conclusion, understanding and monitoring CAC is essential for businesses looking to grow their customer base and maximize their profits. By analyzing the cost of their marketing and sales efforts and the number of new customers acquired, businesses can make data-driven decisions and adjust their strategies to achieve their goals.\nCustomer Acquisition Cost (CAC) is an essential metric for any business looking to grow and succeed. It measures the cost of acquiring a new customer and is a critical factor in determining the profitability of a business.\nThe formula for calculating CAC is relatively straightforward. Simply divide the total cost of sales and marketing efforts by the number of new customers acquired during that time frame. Here's a more detailed look at how you can calculate CAC for your business:\nIt's important to note that CAC can vary depending on the industry, target market, and marketing channels used. For example, a business targeting high-net-worth individuals may have a higher CAC than a business targeting a broader demographic.\nThere are several common mistakes businesses make when it comes to calculating CAC. Here are a few examples:\nTo avoid these mistakes, it's crucial to keep careful records and track CAC over an extended period. Additionally, businesses should continually assess and optimize their marketing and sales efforts based on their CAC data.\nFor example, if a business finds that their CAC is higher than expected, they may want to explore different marketing channels or adjust their targeting to attract customers with a higher lifetime value.\nHere are a few examples of how to calculate CAC:\nAs you can see, CAC can vary widely depending on the business and its marketing efforts. By tracking and optimizing CAC, businesses can ensure they are acquiring customers at a sustainable cost and maximize their profitability.\nThe cost of customer acquisition (CAC) is an important metric for businesses to track, as it directly impacts their profitability. The ideal CAC varies widely depending on the industry and business model. However, some general benchmarks can help businesses gauge their performance in this area. Here are a few examples:\nOne way to determine your ideal CAC is to compare it to your customer lifetime value (CLV). CLV is the total revenue a customer is expected to generate over their lifetime with your business. By comparing CAC to CLV, businesses can determine the return on investment (ROI) of their customer acquisition efforts.\nFor example, let's say that your business has a CLV of $500. If your CAC is $50, then your ROI is 10:1 ($500/$50). This means that for every $1 you spend on customer acquisition, you can expect to earn $10 in revenue over the customer's lifetime. On the other hand, if your CAC is $200, then your ROI is only 2.5:1 ($500/$200), which means that you will need to acquire more customers to make up for the higher cost of acquisition.\nComparing your CAC to your industry average is another way to gauge your performance in this area. For example, according to a recent industry report, the average CAC for Software-as-a-Service (SaaS) businesses is currently $136. However, it's important to note that this is just an average, and businesses should aim to optimize their CAC based on their specific business model and target market.\nVarious factors can influence CAC across different industries. For example, businesses targeting a smaller market may have a higher CAC due to the limited pool of potential customers. On the other hand, businesses operating in a highly competitive market may need to invest more in customer acquisition to stand out from the competition.\nProduct complexity is another factor that can influence CAC. If your product is complex and requires a lot of education and training for customers to use, then your CAC may be higher as a result. However, if your product is simple and easy to understand, then your CAC may be lower.\nBy understanding these factors, businesses can make strategic decisions to optimize their customer acquisition efforts. For example, if your CAC is higher than you would like, you may need to focus on improving your marketing messaging to better target your ideal customer and increase conversion rates. Alternatively, you may need to invest in new marketing channels to reach a wider audience.\nReducing Customer Acquisition Cost (CAC) is a crucial aspect of any business, as it can help maximize profits and scale operations effectively. CAC is the total cost incurred by a business to acquire one new customer. Here are a few strategies for reducing CAC:\nOne of the best ways to reduce CAC is to focus on optimizing your marketing channels. By analyzing your marketing data, you can determine which channels are generating the most leads and customers. From there, you can allocate more resources to those channels and cut back on less effective channels. For instance, if your Facebook ads are generating more leads than your Google AdWords campaigns, you can allocate more of your marketing budget towards Facebook ads and reduce your Google AdWords spending.\nMoreover, optimizing your marketing channels also means testing and experimenting with new channels. You can try out new social media platforms, email marketing campaigns, influencer marketing, or other channels that might be relevant to your business. By diversifying your marketing channels, you can reduce your dependency on a single channel and improve your chances of reaching your target audience.\nImproving conversion rates can also help reduce CAC. By optimizing your website and landing pages, you can increase the likelihood of converting leads into paying customers. Simple changes like reducing the number of form fields, adding social proof, or improving your website's loading speed can make a huge difference in your conversion rates.\nAdditionally, ensuring your sales team is fully equipped to nurture leads and close deals can also improve conversion rates. Your sales team should have a deep understanding of your product or service, be able to handle objections effectively, and provide excellent customer service. By improving your conversion rates, you can reduce your CAC and increase your revenue.\nFinally, leveraging customer retention and referrals can help reduce CAC. By keeping existing customers engaged and satisfied, you can increase the likelihood of repeat business and referrals. Satisfied customers are more likely to recommend your business to their friends and family, which can help you acquire new customers more cost-effectively.\nAdditionally, incentivizing referrals can help encourage your current customers to bring in new leads and customers more cost-effectively. You can offer discounts, free trials, or other rewards to your customers for referring their friends and family to your business. This can not only help reduce your CAC but also improve your customer retention rates.\nIn conclusion, reducing CAC is crucial for any business that wants to maximize profits and scale operations effectively. By optimizing your marketing channels, improving your conversion rates, and leveraging customer retention and referrals, you can reduce your CAC and increase your revenue.\nCustomer Acquisition Cost (CAC) is a critical metric for businesses to monitor and optimize. By understanding what CAC is, calculating it correctly, and analyzing industry benchmarks, businesses can develop an effective strategy for reducing CAC and maximizing profits. With careful planning, businesses can optimize their marketing and sales efforts, reduce costs, and scale operations more efficiently while driving revenue growth."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:a142c8f2-5d9f-47c2-ac8c-2040c452c99e>","<urn:uuid:4bc69ad6-9cd5-4cbc-a49c-5e616ad2f5ab>"],"error":null}
{"question":"Could you compare the concepts of hedonistic and eudemonic happiness, and explain how they relate to modern perspectives on mental wellbeing? For instance, what evidence suggests that materialistic pursuits (like money or possessions) align with either type of happiness?","answer":"Hedonistic happiness is based on pleasure and was dismissed by Aristotle as a life 'suitable to beasts.' In contrast, eudemonic happiness focuses on meaning, self-realization, and developing one's potential. This distinction aligns with modern evidence about wellbeing - despite Great Britain's population becoming richer over the last 50 years, mental wellbeing has not improved. Similarly, possessions and money for luxury holidays do not lead to lasting improvements in how people feel about themselves and their lives. Instead, modern wellbeing encompasses good mental health, security, friendship, contentment, and a balance of mind, body, and spirit.","context":["By Ashley Jouhar on July 03, 2019\nThe definition of wellbeing is given as ‘The state of being comfortable, healthy, or happy’. The synonyms associated with it are ‘Welfare, Health, Good Health, Happiness, Comfort, Security, Safety, Protection, Prosperity, Profit, Success, Good Fortune and Advantage.\nFrom this we can see that wellbeing encompasses more than simply exercising and eating a balanced diet. Wellbeing also includes good mental health, a sense of security, friendship, some financial security and contentment with your lot in life.\nSo wellbeing is no longer limited to a healthy body; today’s vision is more holistic.\nIndeed, emotional health is now on par with physical health as more and more people find life satisfaction and a greater sense of purpose in the balance of mind, body and spirit.\nNic Marks of the New Economics Foundation says, “Well Being is not only about the individual but also about values grounded in a broader, shared understanding of how the world is and should be.”\nOver the last 50 years, the population of Great Britain has become richer - but despite this, evidence shows that mental wellbeing has not improved.\nMany of the things people often aspire to and believe will improve their mental wellbeing - such as possessions or more money for luxury holidays - on their own, do not lead to a lasting improvement in the way they feel about themselves and their lives.\nStills and video that convey not only physical fitness but also communicate a healthy mind, a positive approach to life, mindfulness, tranquility and a sense of being at one with your surroundings are popular. These images have many uses in marketing too, communicating key concepts around Health, Wellness, Escapism, Freedom, Getting Away from it all, Tranquility, Care, Fitness and Purity.\nBeing aware ofthe present – to your own thoughts and feelings right now, and to the world around you – can improve your mental wellbeing. This is particularly difficult to achieve, however in our tech-driven world, where a moment doesn’t pass, it seems, without us reaching to check our smartphones; our attention directed to a screen, rather than the world around us.\nMindfulness is recommended by the National Institute for Health and Care Excellence (NICE) as a way to prevent depression.\nRichard Davidson, one of the early pioneers, scholars and teachers of mindfulness in the US believes that working with kids and teaching them about mindfulness early on as part of their education is going to be really important. He explains, “Teaching kids these kinds of skills early in life can have multiplicative effects as the kids develop. Being able to practice these skills at a very early age can set a child up for a much more positive developmental trajectory.”\nThis mental wellbeing, self-esteem and self-confidence are very important to young people, along with good relationships, and a positive engagement with the world. As Sarah Stewart-Brown, professor of public health at the University of Warwick says, ”It's useful to start with the idea that overall wellbeing involves both the mind and the body. And we know that physical and mental wellbeing are closely related.\"\nNutrition is another very topical piece in the jigsaw of our overall wellbeing. This includes not only our health but the health of the planet too. The provenance of our food is very important nowadays. How the food was produced, where it’s from and whether it is sustainable are questions that are not going away any time soon. Take for instance beef and its impact on our health as well as the impact of its production on the environment. The demand for beef worldwide is resulting in the destruction of a delicate ecological balance on earth – and this is now a big problem that needs reversing.\nThe knock on effect is that more and more people are choosing plant-based diets and among young people in particular, there is an increase in veganism.\nThe benefits of a plant-based diet are many and varied as such foods are rich in fiber, vitamins, and minerals. This can result in a decrease in both blood pressure and in low-density lipoproteins (bad cholesterol) that in turn reduce the risk of diabetes and help maintain a healthy weight. All of these can result in lessening the chances of heart disease.\nHowever, obesity is a huge global problem that is steadily increasing. In the USA, for example, obesity affects 78% of Hispanics, 76% of Blacks and around 66% of Whites over the age of 20. For the under 20’s the figure is 31%.\nThis is down to an imbalanced diet coupled with inactivity and a sedentary lifestyle where work (and leisure) involves sitting for hours on end looking at a screen. 60% of Americans don’t get the recommended amount of physical activity and 25% of those adults aren’t active at all.\nIn light of the rise in obesity amongst youngsters, a wellness movement called Children’s Healthcare of Atlanta launched a campaign they called ‘Strong4Life’, which makes improving family nutrition and physical activity habits fun and engaging. It also provides parents and caregivers the support they need to accomplish their goals.\nGretchen Reynolds of The New York Times says of physical activity, “You get prolonged life, reduced disease risk — all of those things come in in the first 20 minutes of being active. This is all that’s needed to reach the level where happiness and productivity in every day life peaks.”\nTop concepts associated with stills and motion showing a physically active lifestyle are Competition, Speed, Fun, Vitality, Adventure, Strength, Success, Skill, Determination and Effort.\nPeople are equating happiness with good health more than ever, as good physical and emotional health allows us to do the things in life that we’d like to do. This is the motivation to keep minds and bodies healthy\nIn a study of more than 10,000 participants from 48 countries, psychologists, Ed Diener of the University of Illinois and Shigehiro Oishi of the University of Virginia found that worldwide, people rate happiness as being more important than any other life outcome - including living a life with meaning, become rich, and getting into heaven!","Looking to the Past for the Future of Psychiatry\nWhat it means to be a psychiatrist invites many questions. Can psychiatrists go beyond controlling symptoms and managing maladaptive behavior, helping patients become happier rather than just less depressed? Today, can we do better than Freud who promised patients “much will be gained if we succeed in transforming your hysterical misery into everyday unhappiness”? The answer is more nuanced than expected, but exploring the teachings of ancient philosophers can help us understand its ancient roots.\nPositive Psychology is proactive in that it helps people to have happy and meaningful lives as opposed to treating just dysfunction or mental illness.\nAs a medical specialty, psychiatry has excelled in disease identification (diagnosis); treatment and management of symptoms (therapeutics); and definition of long-term outcomes (prognosis). At the same time, we have not extensively discussed the flip side of mental disorders, i.e., happiness and mental health wellness. A quick search through the American Psychiatric Association main textbook finds the word “happiness” cited a meager six times in more than 1,500 pages.\nPsychology, on the other hand, has undergone somewhat of a renaissance over the last 20 years by building on “Positive Psychology,” under the guidance of leaders Martin Seligman and Mihaly Csikszentmihalyi. Positive Psychology is proactive in that it helps people to have happy and meaningful lives as opposed to treating just dysfunction or mental illness. There are, however, forgotten roots to many ideas currently proposed by Positive Psychology, and they lay in ancient Greece, particularly in the work of Aristotle (384-322 BC).\nThe son of a physician from Stagira, Greece, Aristotle was mentored by Plato, who in turn, had himself been mentored by Socrates. This trio of thinkers revolutionized philosophy; Socrates by questioning how we acquire knowledge – using Socratic questioning still utilized today in cognitive behavioral therapy; Plato by postulating his platonic ideas and ideals; while Aristotle wrote founding books on biology, psychology, logic, physics, political science, meteorology, astronomy and metaphysics. He also wrote a very influential essay on happiness around 350 BC, influencing the likes of St. Augustine and Thomas Jefferson: the Nicomachean Ethics.\nIn his Ethics, Aristotle tackles the question of happiness head-on and reaches conclusions similar to those proposed today by “Positive Psychology” and Positive Psychiatry.\n…eudemonic happiness focuses on meaning and self-realization and defines wellbeing in terms of the degree to which a person is fully functioning and developing her, or his, own potential.\nFirst, he differentiates between two types of happiness: hedonistic and eudemonic. The hedonist’s definition of happiness is based on pleasure and dismissed by Aristotle as being “evidently quite slavish in their tastes, preferring a life suitable to beasts” – Book 1, Part 5. A similar distinction has been articulated by Martin Seligman in his book Authentic Happiness. In contrast, eudemonic happiness focuses on meaning and self-realization and defines wellbeing in terms of the degree to which a person is fully functioning and developing her, or his, own potential.\nSecond, Aristotle correlates happiness with the practice of virtue – a point echoed today in a recent comprehensive review on the relationship between happiness and virtue written by Christopher Peterson and Martin Seligman in their book Character Strengths and Virtues. In that book, Aristotle’s list of virtues is a prominent source for the Athenian virtues of courage, justice, temperance and wisdom.\nIt may be presumptuous to conclude that any field, psychiatry or psychology, can teach individual happiness. However, are science and philosophy necessarily mutually exclusive? The ancient philosophers would have argued no. Indeed, they would have asserted that it is incumbent upon both industries to try, just as the ancient thinkers made it their life calling to imbue human existence with meaning and contribute to the pursuit of happiness.\nRecommended reading and resources about Positive Psychology\nMartin Seligman: Authentic Happiness\nMartin Seligman and Christopher Peterson: Character Strengths and Virtues\nCharles Duhigg: The Power of Habit: Why We Do What We Do in Life and Business\nDilip Jeste: Positive Psychiatry: A Clinical Handbook"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:c7f1aefd-dcda-41ca-bfde-189ef04d50cb>","<urn:uuid:6871db44-ad06-446d-8665-473fc1ebaaa7>"],"error":null}
{"question":"How has membrane roofing evolved from traditional asphalt systems, and what key advantages does it offer for flat roofs? 🤔","answer":"Membrane roofing has evolved as a replacement for older asphalt roof systems, offering several key advantages. Unlike asphalt roofs which often had sealing difficulties at seams and connection points, membrane roofs are either totally seamless or have seams as strong as the roof body. Membrane roofs are more resistant to UV damage and expansion/contraction, whereas asphalt required a gravel layer for UV protection. Additionally, membrane roofs attach directly to buildings, eliminating the need for weight-adding gravel that asphalt roofs required. They're also easier to repair, as leaks are more readily located and patched compared to asphalt and gravel roofs.","context":["Membrane Roofing Systems\nMembrane roofing is a flat type of roofing system for buildings that can be found all over the Fox Valley area. It is used on flat or nearly flat roofs to prevent leaks and move water off the roof. Membrane roofs are most commonly made from synthetic rubber, thermoplastic (PVC or similar material), or modified bitumen (“torch down”). These types of materials have become the replacement for asphalt roof systems, which is an older, less effective type of flat roofing system. Membrane roofs are most commonly used in commercial application, though they are becoming increasingly more common in residential application.\nContractors and engineers will choose a type of membrane roofing based on the requirements of the job and the attributes of the material. Costs for membrane roofing can range from as little as $125 per square (100 square foot area), to $350 per square or more. Membrane roofing systems consist of not only the membrane itself, but also any insulation, flashing, roof accessories and sealants.\nBuilt-up Roofing (BUR)\nBuilt-up roofing, or BUR for short, consists of layers of organic/inorganic felts saturated with liquid asphalt or coal tar. BUR has been providing reliable protection against the elements for many years.\nThe layers are installed in an overlapping fashion; each layer is referred to as a ply. Applications can consist of 2 plies, or as many as 5 plies or more. Commonly referred to as a Tar & Gravel roof, the uppermost surface is often topped with small stones to provide additional protection from damage and UV degradation.\nModified Bitumen, Mopped & Torch Downs\nModified bitumen membrane roofing is essentially a pre-manufactured, built-up roofing system. Thick layers of reinforcing felts and liquid bitumen are bound together in the manufacturing facility and shipped to the job site in roll form. Chemical modifiers enhance the attributes of the liquid bitumen to make it suitable for job conditions.\nModified bitumen roofing can be applied in a method similar to built-up roofing, by using hot liquid bitumen that is mopped in place, or it can be installed by torching it down. As the roof membrane is being unrolled, a worker uses a torch to melt the leading edge of the roll into a liquid that acts as a binder to hold the material down.\nThermoplastic Olefin (TPO)\nThermoplastic Olefin, commonly called TPO, are plastic sheets welded together with hot air creating one continuous membrane. These components can be re-welded, with the exception of CSPE, making repairs easy to administer. These traits lend themselves well to both large and small roof applications because of the hot air versus torch down welding.\nTPO is a single-ply roofing system consisting of a Thermoplastic Polyolefin membrane. The TPO membrane is typically comprised of three layers: a polymer base, a strong, polyester-reinforced fabric center (scrim) and a tough thermoplastic polyolefin compounded top ply.\nOther Types of Membranes\nOther types of membrane roofing such as chlorosulfonated polyethylene (CSPE), ethylene-propylene-diene-monomer (EPDM), polyisobutylene (PIB), and polyvinyl-chloride (PVC) consist of large sheets of material that are joined together using various chemical or thermal methods.\nPrimers and adhesives can be used along overlapping edges to form an impervious joint between sheets, or heat welding can also be utilized to turn all the pieces into one large roof covering.\nThe large sheets are then applied to the roof surface by using one of several methods. A ballasted roof uses the weight of stones or blocks of concrete, similar to pavers, to hold the roofing in place. Strips of metal fastened to the structure at all edges and seams can also be used. Another popular method is to use an adhesive that is similar to contact cement to hold the sheets down.\nAdvantages Over Asphalt Flat Roofing Systems\nThe application types of membrane roofing listed above have distinct advantages over the previous flat roofing method of asphalt and gravel. In asphalt and gravel applications, it can be very difficult to create a proper seal at all seams and connection points. This can cause many roofs to leak early in a lifespan , and require more intensive maintenance.\nWhen installed correctly, newer materials are either totally seamless, or have seams as strong as the roof body. The primary concern people have with flat roofing is leakage, and the newer systems should eliminate many of those issues\nRepairs for asphalt and gravel roofs can be difficult, mainly because it can be hard to locate the exact point of a leak. Newer systems can be patched relatively easily, and cracks and leaks are easier to locate.\nOriginally asphalt roofing required a layer of gravel as the top layer for two reasons. First, asphalt with direct exposure to sunlight degrades faster, due to the expansion and contraction and the damage created by UV rays. Second, asphalt needs the gravel weight on it to hold down the roof because it “sits” on the top of a building, instead of being attached to it.\nEach of the three newer types of membrane roofing systems contain materials that resist expansion and contraction, and have advantages against UV rays. Also, because these membranes have strong seams, when normal expansion and contraction does occur, its damage to these seams is exponentially reduced. Best of all these newer roofing systems are also attached directly to the top of a building, which eliminates the need for excess weight, and greatly reduces stress on your structure."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:a6670357-907d-4bc8-817c-b78e0438bcc2>"],"error":null}
{"question":"What's the fascinating story behind Mexico City's Aztec food revival, and what are the key nutritional benefits of eating insects like the ones served in these restaurants? 🦗","answer":"Mexico City is experiencing a revival of 500-year-old Aztec cuisine, with upscale establishments like the Four Seasons hotel and Fonda Don Chon restaurant serving traditional insect-based dishes. Popular items include chapulines (grasshoppers) fried with coriander, escamoles (giant black ant eggs), and magüey worms. Regarding nutrition, insects are excellent protein sources, providing 13-28g of protein per 100g, comparable to beef, chicken, and fish (15-26g). They're also rich in essential fatty acids - termites contain oleic acid (found in olive oil), while grasshoppers and weevils provide beneficial omega-7 palmitoleic acid, which has anti-inflammatory properties and improves insulin sensitivity. Additionally, insects produce about half as much CO2 as cattle per kilogram of mass gained, making them an environmentally sustainable protein source.","context":["Eating like Aztecs\nTravel journalist Dea Birkett tests her taste buds with a foray into Mexico City's latest food craze - the sometimes grisly cuisine of the ancient Aztecs.\nWhat was on Montezuma’s menu? That’s the latest concern of Mexico City’s chefs, as the craze in the capital is for food to reflect what the Aztecs ate over 500 years ago.\nChicken burritos are taboo at top tables. (The Aztecs didn’t eat chicken, preferring dog.) In Aztec times, you had to eat whatever you could scavenge – from the ground or on plants. And the most nutritious food the hunters found was bugs and grubs. But where once pre-Columbian cuisine was devoured out of necessity, now it’s uber-cool to declare a penchant for magüey (similar to cactus) worm and have a prediliction for snacking on ants’ eggs.\nIn the courtyard of the smart El Bar at the Four Seasons colonial-style hacienda hotel in the Paseo de la Reforma on the edge of Mexico City’s University area and Chapultepec Park, the local elite crunch onchapulines (grasshoppers) lightly fried with coriander and escamoles (the eggs of a giant black ant). An outdoor brazier keeps them warm in the chilly evenings.\nI scoop up my crispy dish of chapulines. It’s not so much the flavour that’s challenging; it’s that I can’t forget the salty barbequed matchstick I’m crunching is, in fact, an insect’s upper thighbone. Splinters get stuck between the teeth. Was this what they meant by Montezuma’s revenge? The escamoles taste rather like soggy popcorn. I smother them in salsa and wrap them up in a tortilla. My magüey worms have a strong resemblance to maggots. I feel as if I’m in Roald Dahl’s The Twits, tricked into eating a plate of Wormy Spaghetti. It’s a meal that needs to be washed down. I go for a bottle of Pacifico beer. More authentically, I should have joined the cashmere-clad couples and diluted my arthropods with a toloache drink, made from the roots of a giant flowering plant, which is said to make your companion fall in love with you.\nThe Four Seasons is dedicated to this 500-centuries-old cuisine’s revival. It even boasts a pre-Hispanic breakfast. The speciality omelette is made with zucchini flowers and huitlacoche - ‘aged corn’. ‘Aged’ is a polite euphemism to put on the a la carte. The corn is so old it’s gone rotten and black with mould. I found the texture a bit mushy, but when I closed my eyes it could be quite tasty, like marmite on soggy cotton wool. Connoisseurs of the New Aztec cuisine fondly call huitlacoche Mexican truffles. (In similar hope, they dubescamoles Mexican caviar.)\nDon’t imagine turning the clock back to pre-Columbian times means the prices will deflate accordingly. Eating like an Aztec can be expensive. But there are a few places where Aztec food is served up without an achingly cool coating. At the café-style Fonda Don Chon restaurant, a short walk from the historic centre, my fellow diners were root-conscious Mexico City bohemians and local families, rather than business people. A stuffed armadillo stands in the doorway, tatty from being stroked. (Armadillo in mango sauce – an Aztec feast – is one of the restaurant’s seasonal dishes.) There’s a curling poster of the emperor on the wall as if he were the local football hero.\nIf the clientele is less chic, the cuisine is more challenging. Best to leave the Spanish dictionary back in the hotel room and delete your iPhone’s translation app, as you don’t always want to know what you’re eating. I had the ant eggs sauted in butter as a starter. I discovered smothering them in raw onion and chilli sauce gave them a crunch and bite they otherwise lacked - unless they’d hatched.\nI settled back and watched the families on the other tables, with piles of grasshoppers, worms and grubs in shared bowls in front of them. I ordered wild boar for my main course, an Aztec emperor’s favourite. I realized that I was beginning to enjoy all these unfamiliar gustatory textures, as if my tongue had learnt an entirely new culinary language. There isn’t much difference, after all, between grasshoppers’ thighs and frogs’ legs. I toast the Aztec emperor’s exquisite palate with another Pacifico and call over the waiter. ‘Más escamoles, por favor.’","NUTRITION FACTS FOR SOME COMMON INSECTS\n89-160 calories (depending on country of origin)\nYellow mealworms (100g):\nhigh source of palmitoleic acid (omega-7 fatty acids)\nAnd now for something completely different. What if, instead of relying so much on traditional sources of animal protein, with their substantial impact on the environment, we switched our focus entirely and agreed to eat more bugs?\nYes, it’s true: There is a small, but growing movement to encourage Westerners to drop their fear of creepy-crawlies and embrace entomophagy — or eating of insects. And while cricket flour or locust meat aren’t necessarily high demand items (yet), that doesn’t mean we shouldn’t at least start to explore our relationship with bugs, which are commonly consumed as sources of energy and protein in many other parts of the world.\nEating cockroaches might not be your cup of tea, so to speak, but there is a good case to be made for them: Insects are natural sources of protein, and in particular contain a full complement of essential amino acids, the building blocks of protein, in many of the same ways that animal products do. At the same time, their environmental impact is far less substantial.\nTermites are particularly rich in oleic acid, the same type of fat found in olive oil\nAccording to a study, published in 2010 in the journal PLOS One, for every kilogram of mass (basically, weight) gained, insects typically produced about half as much carbon dioxide as cattle (an average of 2,835 g of CO2 produced per kilo of mass gained for beef, vs. a range of 337-1539 g of CO2 produced per kilo of mass gained by insects), as well as significantly smaller amounts of harmful ammonia and methane gas. At the same time, insects gained weight at a faster rate than the larger animals, which means that they could be brought to market faster, and yet with less damage to the environment.\nWhile their nutritional value can vary widely from species to species, and across their lifespans, in general, insects offer protein and fat, as well as energy (calories), and some key micronutrients, such as iron, magnesium and zinc. So how many calories does the average grasshopper contain? Estimates range from 89 calories per 100 grams of raw grasshoppers native to Thailand, to 160 calories per 100 grams of home-grown Canadian red-legged grasshopper — values that are comparable to many cuts of chicken, fish, or beef. If mealworms are more your style, however, you’ll want to account for the extra calories: 100 g of mature mealworms offer up 138 calories, but their larvae are relatively rich at 206 calories per 100 g.\nBeyond calories, the protein content of insect is also on-par with meat: Yellow mealworms, for example, provide between 14 g and 25 g protein per 100 g fresh weight, while termites, locusts, and grasshoppers provide 13 to 28 g. By comparison, beef, chicken, and fish typically provide 15-26 g protein per 100 g raw weight. Insects are also typically a source of fats, and often provide essential linoleic (omega-6) and alpha-linolenic (omega-3) fatty acids. Termites are particularly rich in oleic acid, the same type of fat found in olive oil, and numerous insects, including weevils and grasshoppers, boast up to one-third of their fat content as palmitoleic acid, a type of fat known as an omega-7 that has recently been reported to improve insulin sensitivity and have anti-inflammatory effects.\nBUGS FOR ALL?\nWhile eating bugs could provide environmental benefits for the Western world, it could also be of vital importance to developing nations, where malnutrition and deficiencies in protein, iron, and zinc can be major concerns. In some cases, the essential amino acids (the building blocks of protein) in insects are used to complement other foods, such as grains, which lack some of the eight essential amino acids. In Papua New Guinea, for example, palm weevil larvae provide lysine and leucine to those who practice entomopagy — two essential amino acids that are absent in tuberous (starchy) vegetables. Termites have also been suggested as a means of obtaining key amino acids in parts of Africa where maize, which is naturally low in tryptophan and lysine, is a staple food. The movement to promote the use of insects in our diets is significant enough that in 2011, the United Nations Food and Agriculture Organization (UNFAO) published a report, “Edible insects: Future prospects for food and feed security.”\nBefore we get too excited about having crickets for dinner, however, experts caution that we must be careful to develop sustainable cultivation and harvesting methods: there are examples of human overconsumption that has led to the collapse of some insect species. With careful cultivation and further research, however, there is good reason to believe that six-legged critters and their friends could have a valued place on our dinner plates at some point in the not-so-distant future — provided we can first overcome our fear of munching on food that creeps, crawls and flies.\n— Jennifer Sygo, MSc., RD, is a registered dietitian and sports nutritionist at Cleveland Clinic Canada. Visit her on the Web at jennifersygo.com and send your comments and nutrition-related questions to her at firstname.lastname@example.org."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:039c1f2e-ab82-49f4-b7ca-43a1c0e35138>","<urn:uuid:85edd96a-8b5a-46c3-97d6-eabdfa203c47>"],"error":null}
{"question":"How do the diffusing glass one-time pad and quantum random number generator differ in their approach to generating randomness?","answer":"The diffusing glass one-time pad generates randomness by passing light through a slab of diffusing glass that scatters it randomly, relying on the physical complexity of the glass structure. In contrast, the quantum random number generator creates randomness by sending an untrusted light source into a beam splitter while the other input is vacuum, then measuring the results with two optical detectors. The quantum approach relies on the unpredictable behavior of individual photons having a fifty-fifty chance of being transmitted or reflected.","context":["One-time pads are the holy grail of cryptography—they are impossible to crack, even in principle.\nThey work by adding a set of random digits to a message thereby creating a ciphertext that looks random to any eavesdropper. The receiver decodes the message by taking away the same set of random digits to reveal the original message.\nThe security of this process depends on two factors. The first is the randomness of the digits that make up the one-time pad. If this key is truly random, it offers nothing the eavesdropper can use to break the code. Although there are some potential pitfalls, random digits are reasonably straightforward to generate these days.\nThe second factor is the ability to keep this key secret so that only the transmitter and receiver have access to it. That’s much more difficult to ensure.\nDigital communication in the form of 0s and 1s makes copying trivial. Whenever a set of random digits is stored in an electronic memory, there is always a small but finite chance that it can be quickly copied and stolen.\nToday, Roarke Horstmeyer at the California Institute of Technology in Pasadena and a few buddies say they’ve solved this problem. Their solution is based on a special kind of one-time pad that generates a random key through the complexity of its physical structure.\nInstead of creating and storing the one-time pad as a random sequence of 0s and 1s, Horstmeyer and co generate a random signal by passing light through a slab of diffusing glass that scatters it randomly.\nThe security of the system depends on the physical complexity of the glass. Horstmeyer and co say that that this complexity means there is no way for an eavesdropper, “Eve,” to copy the glass without anyone noticing.\nThat cuts out the need to store the key electronically and entirely removes this vulnerability to copying. “We describe an encrypted communication principle that can form a perfectly secure link between two parties without electronically saving either of their keys,” they say\nAnd even if Eve steals the glass, they estimate that it would take her at least 24 hours to extract any relevant information about its structure.\nThis extraction can only be done by passing light through the glass at a rate that is limited by the amount of heat this creates (since any heating changes the microstructure of the material). And the time this takes should give the owners enough time to realise what has happened and take the necessary mitigating actions.\nThe protocol for sending secret messages between “Alice” and “Bob,” say, is straightforward. To start off, both Alice and Bob must have their own slabs of diffusing glass and must physically meet to create a key for encoding a message later.\nThey create this by sending the same random pattern of light through their diffusing slabs and then adding the results to create a combined key.\nThey then publish this combined key and the pattern used to create it.\nTo send a message, Alice sends the pattern through her slab to generate her half of the key and then adds it to her message. She can now send this without fear that Eve can decode it.\nIt’s important to remember that Alice’s random key is a component of the publicly available one. But Eve cannot use the publicly available key to work out what Alice’s key is.\nBob has to go through a slightly different set of steps to decode this cyphertext. First, having received the cyphertext, he adds it to the publicly available combined key.\nNext, he re-creates his own component of the publicly available key by sending the publicly available pattern through his slab. He then adds this to the result of the previous step to reveal the message.\nAs long as both diffusing slabs are physically held by Alice and Bob, the cyphertext cannot be decoded by Eve.\nOf course, this process can be used only once. But Alice and Bob can generate a huge volume of combined keys by passing different random patterns through their slabs when they meet.\nHorstmeyer and co have tested their idea using a spatial light modulator to create random patterns that they then pass through opal diffusing glass to generate about 10 gigabits of randomness. They then used this for sending perfectly secure messages, thereby demonstrating the utility of the technique.\nNevertheless, improvements should be possible, they say. For example, the team says that the system generates a small amount of noise caused by the natural drift of scatterers in the glass over time. But that’s something that should be possible to fix with error-correcting codes.\nAnd it ought to be possible to generate a terabit of randomness from a single cubic millimetre of diffusing glass with higher-resolution equipment.\nAnd even thought this can only be used once, the slabs can be easily reset by heating the glass to change its microstructure at which point Alice and Bob must meet again to create a new set of combined keys.\nThat looks to be a significant improvement over any kind of cryptography that stores keys electronically and is therefore vulnerable to an electronic attack that can copy digital information perfectly.\n“Compared with a large, electronically saved one-time pad, [the new system’s] key is extremely challenging to copy or model and can easily scale to provide terabits of repeatable randomness within a small volume,” say Horstmeyer and co.\nThey have high hopes for this approach: “We hope the convenient properties of optical scattering can solve enough of the one-time pad’s practical shortcomings to rejuvenate interest in its unbreakable security, even in the presence of inﬁnite computing resources.”\nWhat they mean is that this system should be secure even to attack with future quantum computers.\nThat’s not something that can be said about the codes commonly used to protect messages today. With quantum computers now beginning to perform some serious calculations, anybody still using these codes must be losing a significant amount of sleep.\nRef: arxiv.org/abs/1305.3886: Physical Key-Protected One-Time Pad","A team of scientists from NUST MISIS, the Russin Quantum Center, the Clarendon Laboratory, Department of Physics and the Department of Computer Science, (both University of Oxford), the Dahlem Center for Complex Quantum Systems, Freie Universität Berlin, and the Department of Computing, Goldsmiths, University of London have completed a study in optics and quantum information that details their work in developing a fast and affordable quantum random number generator. The paper, Certified Quantum Random Numbers from Untrusted Light, was published in Physical Review X and sets out the team’s work of moving forward with a commercial random number generator (RNG) for cryptography, as well as complex systems modelling.\nThe unique attribute to the team’s random number generator is it produces randomness at a rate of 8.05 GB per second, an impressive rate by anyone’s standards.\n“It is surprisingly hard to generate true random numbers at speeds fast enough for commercial use, which is why most applications have relied on deterministic — or pseudo-random numbers.”\n— Vikram Sharma, QuintessenceLabs Founder and CEO\n“Quantum events allow the generation of numbers whose randomness is asserted based upon the underlying physical processes. Quantum-based random number generators can have very broad applications,” said Alex Fedorov, head of the Russian Quantum Center research group and the Quantum Communications Theory Lab Head at NUST MISIS, on the group’s findings.\nThe impressive achievement was made possible because the team dispatched an “untrusted” light source into one input of a beam splitter while the other input was the vacuum before being measured utilizing two different optical detectors, the result of which produces a truly amazing random number since every individual photon that collides with the beam splitter creates a scenario where the photons in question have a fifty-fifty chance of being transmitted or reflected — detecting the number of photons recorded by either detector is nigh on impossible to fathom or predict, a legacy of Thomas Young’s seminal experiment.\nIn a word, diffraction on acid.\nThe paper begins with the introduction:\n“a remarkable aspect of quantum theory is that certain measurement outcomes are entirely unpredictable to all possible observers. Such quantum events can be harnessed to generate numbers whose randomness is asserted based upon the underlying physical processes. We formally introduce, design, and experimentally demonstrate an ultrafast optical quantum random number generator that uses a totally untrusted photonic source. While considering completely general quantum attacks, we certify and generate in real-time random numbers at a rate of 8.05 Gb=s with a composable security parameter of 10−10. Composable security is the most stringent and useful security paradigm because any given protocol remains secure even if arbitrarily combined with other instances of the same, or other, protocols, thereby allowing the generated randomness to be utilized for arbitrary applications in cryptography and beyond. This work achieves the fastest generation of composably secure quantum random numbers ever reported.”\nAs already mentioned: the 8.05 GB a second of real-time random numbers is great, but what’s freaky is the composable security parameter of 10−10. Whether the team — or individuals from it — pursue this any further to keep us safe, shielded and guarded against the hooded claw of internet infractions is another matter altogether.\nWhatever the outcome, Fedorov and his international team of scientists are sure to make a mark in the cryptography market with their research in QRNG."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:c99b3338-d3e8-4cb3-89b7-5921fe37fcae>","<urn:uuid:d04b38a2-3e55-4fe2-81b0-10bd3ee0f4df>"],"error":null}
{"question":"How can we prepare ourselves for a happy and healthy life in old age? I'm really curious about practical steps! 🤔","answer":"There are several key steps to prepare for a healthy old age: 1) Envision and imagine what it would be like to live a healthy, happy 100 years. 2) Design your social and physical environments, including your home, spending habits, and eating habits, so that your daily routine reinforces your goals. 3) Diversify your expertise and activities, and avoid putting all your social investments into only your spouse, children, or job. Additionally, pursuing education is important - research suggests that even one additional year of education likely increases life expectancy by more than a year.","context":["TORONTO - As more people live well into their 80s and 90s, it's reassuring to know that most people get happier as they age and exert more emotional control than younger adults, according to researchers who spoke at the 117th Annual Convention of the American Psychological Association.\n\"Life expectancy changed because people changed the way they lived,\" said Lauren Carstensen, PhD. \"Now that we're here, we have to keep adapting. We are in the middle of a second revolution and it's up to us to make adulthood itself longer and healthier.\"\nCarstensen, a psychology professor at Stanford University and founding director of the Stanford Center on Longevity, said the percentage of people on the planet who are over 65 is expected to more than double by the year 2050, and the fastest-growing segment of the population is people over age 85.\nSusan Turk Charles, PhD, of the University of California, Irvine, presented a review of several psychological studies on aging and mental health. She found that except for people with dementia-related diseases, mental health generally improves with age. One study she cited - a 23-year longitudinal study looking at three groups of people, each at different stages in their lives - found that emotional happiness improved with age.\nResearch has also shown that older adults exert greater emotional control than younger adults, meaning older adults are more likely to actively avoid or limit negative, stressful situations than do younger adults, Charles said. She presented results from one study in which younger and older adults reported their thoughts and emotions after hearing personal criticism by two other people. Younger adults focused more on the negative comments and demanded more information about the origin of the criticism. Older adults were less likely to dwell on the negative comments and their responses were less negative overall compared to those of the younger adults.\n\"Based on work by Carstensen and her colleagues, we know that older people are increasingly aware that the time they have left in life is growing shorter,\" said Charles. \"They want to make the best of it so they avoid engaging in situations that will make them unhappy. They have also had more time to learn and understand the intentions of others which help them to avoid these stressful situations.\"\nHowever, Charles also said that these age-related benefits for older adults may not appear when older adults are faced with prolonged, distressful situations with no way to escape. \"Older adults may have more difficulty with these situations because distressing events require both psychological and physical resources,\" she said. \"We know that older adults who are dealing with chronic stressors, such as caregiving, report high rates of physical symptoms and emotional distress.\"\nIn separate addresses, Carstensen and Charles both acknowledged the importance of social relationships on longevity. Scientists have been uncovering evidence that the quality of people's relationships can influence the way their brains process information and how they respond physiologically to stress.\n\"These changes have a profound impact on health outcomes,\" Carstensen said. She cited a recent study of more than 1,000 Swedes in which those who had a strong social network were 60 percent less likely to have symptoms of cognitive impairment than those who did not. None of the participants showed signs of dementia before the study. The researchers assessed participants' social situations, including whether they were married or single, lived alone, and enjoyed their social circle.\nAnd while older people's minds may appear to be slowing down to those around them, Meredyth Daneman, PhD, of the University of Toronto at Mississauga, said that may not always be the case. In a series of studies comparing young adults to older adults during various cognitive and hearing tests, she found age-related declines in the ability to understand spoken language are often the result of a decline in hearing, rather than a decline in brain function.\nHealthy aging, though, isn't just about looking at the very old. It's also about looking at the very young, Carstensen said. She pointed to a growing and compelling body of research that suggests even relatively small increases in education pay off in the quality and length of life. \"Independent studies agree that even one additional year of education very likely increases life expectancy by more than a year,\" she said.\nCarstensen had several suggestions for people who want to prepare for old age now:\n- Envision ways to thoroughly enjoy the years that lie ahead and imagine what it would be like to live a healthy, happy 100 years.\n- Design your social and physical environments - home, spending habits, eating habits - so that your daily routine reinforces your goals.\n- Diversify your expertise and activities and avoid putting your social investments into only your spouse, children or job.\nSymposium: Advances in Experimental Research on Aging: \"Emotional Experience Across the Adult Life Span: A Story of Strengths and Vulnerabilities,\" Susan Turk Charles, PhD, University of California, Irvine, \"Age-Related Changes in Spoken Language Comprehension,\" Meredyth Daneman, PhD, University of Toronto at Mississauga, Session: 2266, 2:00 - 3:50 PM, Friday, Aug. 7, Metro Toronto Convention Centre, North Building - Level 200, Meeting Room 206F\nFor more information or an interview, contact Susan Turk Charles at 949-824-1450 or by e-mail at firstname.lastname@example.org\nInvited Address: \"A Long Bright Future: Aging in the 21st Century,\" Laura L. Carstensen, PhD, Stanford University, Session: 3371, 4:00 - 4:50 PM, Saturday, Aug. 8, Metro Toronto Convention Centre, South Building - Level 700, Meeting Room 717A\nFor more information or for an interview, contact Laura Carstensen at 650-725-0347 or by e-mail at email@example.com\nThe American Psychological Association, in Washington, D.C., is the largest scientific and professional organization representing psychology in the United States and is the world's largest association of psychologists. APA's membership includes more than 150,000 researchers, educators, clinicians, consultants and students. Through its divisions in 54 subfields of psychology and affiliations with 60 state, territorial and Canadian provincial associations, APA works to advance psychology as a science, as a profession and as a means of promoting health, education and human welfare."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:04ca1ead-e7a4-4963-aa71-2499df33d9ea>"],"error":null}
{"question":"How do walnuts and pistachios differ in their recommended storage methods and shelf life when refrigerated?","answer":"When refrigerated, walnuts and pistachios have the same shelf life of up to one year. However, both nuts can be stored longer in the freezer - walnuts for two or more years, and pistachios for two years. For pistachios, if they lose their crispness, they can be restored by toasting the nutmeats at 200°F for 10 to 15 minutes.","context":["Walnuts are my hands down favorite nuts with the added benefit of not only being high in protein but omega-3 fatty acids as well. Omega-3 fatty acids are touted as extremely beneficial for the heart but beyond that, they are delicious! What better reason to grow your own? The question is, when are walnuts ready to pick and what is the best way to pick walnuts?\nWhen are Walnuts Ready to Pick?\nWalnuts may be either English or the black walnut varieties, with the latter having a thicker shell and more intense flavor. Both types are fruiting, deciduous trees that are fairly easy to grow and lacking in few serious issues especially once mature.\nThey can grow to 100 feet (30 m.) tall and 50 feet (15 m.) across, which makes the tree a bit unmanageable for some landscapes. Luckily, young trees can be trained via pruning. Walnut trees can be grown with a central leader or remove the leader which will encourage side shoot growth and restrict the tree’s size.\nA pitted shell encases a fibrous, leather sheath that splits as the nuts begin to ripen in the fall and indicates that walnut tree harvesting is nigh. Once you are done harvesting the walnuts, you can eat them right away, but keep in mind they won’t be quite like those purchased ones at the grocers.\nThe nuts will be rubbery in texture and are, thus, usually dried which also extends their shelf life. Think your nuts are ready for harvesting but don’t know the best way to pick walnuts? Keep reading to find out how to harvest walnuts.\nHow to Harvest Walnuts\nDepending upon the variety and region they are grown in, walnut tree harvesting starts from early September to early November. At this point, the kernels are light in color and the membrane between the halves has turned brown.\nTo determine if your nuts are ready for harvest, crack a few open. The nuts should show browning of the membrane and loosening of the hull. Take your nut samples from as high up in the tree as possible since those that are at this height ripen latest. Also, if your tree is water stressed, harvesting walnuts will be delayed. To speed things up, be sure to keep the tree well watered through harvest.\nBegin harvesting when you estimate that at least 85% of the nuts can be easily removed from the tree. Delay too long and insects and birds may get to the nuts before you do. Additionally, if you delay too long, the outer husks become soft and black and the resulting nut has a bitter, rancid flavor.\nTo begin harvesting walnuts, you will need a pole or a pole combined with a hook for larger trees. Shake the nuts loose using the pole. Immediately pick the walnuts up from the ground. If they lie there too long, they will either begin to mold or become over run with ants, or both. The hulls of walnuts contain phenols, chemical compounds that cannot only stain hands but for some people cause skin irritation, so when handling walnuts, wear rubber gloves.\nOnce you have harvested the walnuts, hull the nuts using a pocket knife. Wash the hulled nuts and then dry them in a single layer on a smooth, flat, shaded area. Stir the nuts around on a daily basis to promote drying. If drying outdoors, cover the nuts with plastic netting to deter birds. The length of time until complete drying depends on temperature but, generally, will be dry in three to four days. At this point, the kernels should be brittle as well as the membrane separating the two halves.\nStore the cured walnuts in a cool, dry area or to extend their shelf life, in the refrigerator or freezer. They can be stored for up to a year in the fridge and for two or more years in the freezer; that is, of course, if you can stay out of them that long.","What's green and beige, slightly salty and half-opened ready for eating? The answer can only be… pistachios! And what an interesting and unusual nut the pistachio is. That inviting slightly-split shell and the sound the empty shells make when you shake a handful of them together make the pistachio a fun nut to eat. Like many people, my first exposure to pistachios was with the traditional red variety commonly thought of as natural (wow, these nuts grow bright red!). Now it is generally known that red dye was used to disguise imperfections and also to attract attention to this formerly little-known tree fruit.\nUntil the mid-1970s, the majority of pistachios were imported from the Middle East and were long regarded as an integral part of Eastern and Mediterranean cuisine. While it's taken North Americans a bit longer to latch on to this tasty nut, pistachios are now one of California's major crops and have gained enormous popularity as a tasty, addictive snack. A healthy addiction, as it turns out, despite the consistent myth surrounding nuts and fat. Like all nuts, pistachios are composed primarily of monounsaturated fat (the good kind) and have absolutely no cholesterol (see below: Nutrition and Weight Loss).\nPistachios trees take from 7 to 10 years to start producing nuts. The trees which comprise the majority of California's crop are, at 20 years, considered relatively young since many pistachio trees in the Middle East have reached the ripe old age of 200 years! A relative of both the cashew and the mango, the pistachio tree is both deciduous (dropping its leaves each year), and comes in both sexes. The trees are wind-pollinated, with the male trees bearing pollen and the female trees bearing the nuts. The resulting tree fruit is comprised of kernel, shell and hull. When the kernel ripens to maturity during mid-summer, the kernel expands causing a natural splitting of the shell. The shell is protected by the hull but at maturity the hull starts to break away from the shell and is easily removed at the processing plant and very promptly as they cause staining if left longer than 24 hours after harvesting. Some growers we have visited in California store huge piles of the removed hulls, allowing them to decompose and then selling them as \"green manure\" to other growers. A whiff of the odour produced by the decomposing hulls makes the term manure very appropriate indeed!\nWhy are some pistachios red? The first pistachios available to consumers were imported from the Middle East. American importers dyed the shells red, both to disguise staining from antiquated harvesting methods and to make pistachios stand out among other nuts in vending machines.\nUntil the 1970s, there was no domestic pistachio industry in the United States. California harvested its first commercial crop in 1976. The entry of California pistachios into the marketplace made available nuts with clean, naturally tan shells. California's Kerman variety is also larger in size with a more vibrant green nut colour. A small percentage of California's crop is dyed red, not by necessity, but to meet the needs of those consumers who prefer the colourful shell.\nHow can I open slightly split or nonsplit pistachios? As the pistachio kernel grows, it naturally expands within the shell until it splits open. Nonsplit shells usually contain immature kernels and should be discarded. Tip: slightly-split shells can be opened using one half of the shell from an already-opened pistachio. Wedge the tip of a half shell into the split and turn it until you can retrieve the kernel.\nHow does the pistachio kernel get its green colour? Plants make a variety of pigments which contribute colour to plant parts such as the flowers, leaves and fruit. The green in the pistachio nut is the result of chlorophyll, the same pigment that makes the leaves green.\nHow long do pistachio trees live? How long do they produce? The pistachio trees planted in California are still fairly young, but there are pistachio trees in the Middle East that are over 200 years old and are still producing!\nWhere in California are pistachios grown? Approximately 98% of the pistachios produced in the U.S. are from California. Pistachios are grown as far north as Shasta County and as far south as Riverside County. Because of their desert-like climate and soil, the San Joaquin Valley counties of Kern, Madera, Kings, Fresno and Tulare are the major producers.\nBest Storage Method for Pistachios?\nPistachios stored in an air-tight container in the refrigerator have a shelf life of one year; in the freezer, they have a shelf life of two years.\nTo restore pistachios that have lost their crispness toast the nutmeats at 200°F for 10 to 15 minutes.\nPistachios, Nutrition and Weight Loss\nIn addition to their great taste, a one-ounce serving of pistachios (47 kernels according to the USDA) is crammed with nutrients, containing more than 10 percent of the Daily Value for key nutrients like dietary fibre, vitamin B-6, thiamin, magnesium, phosphorus and copper. A serving of pistachios contains 170 calories and is low in saturated fat, containing primarily monounsaturated fat, and is cholesterol-free.\nThe October 2001 issue of the U.S. International Journal of Obesity\nfound that people who followed a Mediterranean-style diet that incorporated moderate amounts of monounsaturated fat from sources such as pistachios and other nuts not only lost weight but were able to maintain their weight loss much longer than people who followed a traditional low-fat diet.\nMany nutritionists consider a one-ounce serving of pistachios and other nuts to be a wise snacking choice since they are literally \"nutrition in a nutshell.\" In any comparison of nuts to other snacking foods such as chips and ice cream which are literally loaded with saturated fats not to mention many undesirable chemical additives, pistachios and their siblings come out on top nutritionally.\nIn further support of pistachios as a healthy snack food choice, the USDA Nutrient Database for Standard Reference\nrecently updated (November 1999) the comprehensive list of nutrients found in pistachios. While a one-ounce serving (47 kernels) of pistachios contains 160 calories and is low in saturated fat and cholesterol-free, they are also an excellent source of critical nutrients like vitamin B6 (25% daily value) and copper (20% daily value). Vitamin B6 helps produce other body chemicals, including insulin, hemoglobin and antibodies that fight infection; copper also helps the body make hemoglobin and serves as part of many enzymes that produce energy for cells in the body.\nPistachios in the Kitchen\nYes, you do have to shell them first, but do it ahead of time and get some help. Just make sure you account for the ones that don't make it to the mixing bowl! Two cups of in-shell pistachios will yield about a cup of kernels and take about 15 minutes to shell.\nFrittata with California Pistachios\n2 tablespoons olive or vegetable oil\nOne-half red bell pepper, seeded and sliced\n1/2-lb. zucchini, sliced\n1 tomato, sliced\n1/4-teaspoon black pepper\nFresh herbs (such as thyme, basil, sage), chopped\n1/2-cup natural California pistachios, coarsely chopped\nPreheat oven to 350 degrees F. Beat eggs and set aside. Oil a 9 inch quiche pan or round shallow baking dish. Arrange bell pepper, zucchini and tomato in dish. Bake uncovered in 350 degrees F. oven for 9 minutes. Sprinkle with black pepper and a handful of herbs. Pour eggs over and sprinkle with pistachios. Reduce oven to 300 degrees F. Return pie to oven and bake for 25 to 30 minutes longer or until centre is set and sides are puffy and golden. Cover loosely with foil if it starts to get too dark before it is done. Cool slightly, then cut into wedges. Serve with fresh fruit for brunch or for light supper accompanied by a green salad.\nMakes 6 servings.\nBrie and Pistachios\nThis one is so simple you'll wonder why you never thought of it yourself!\n8 ounces softened Brie (Camembert cheese can also be used)\n1/2-cup coarsely chopped, roasted/salted California pistachios\n1 loaf French bread baguette, sliced\nGenerously spread softened Brie cheese on wedge-shaped slices of French bread baguette. Top with chopped California pistachios for an elegant, yet simple, appetizer.\nGarden Vegetable Pistachio Potato Salad\n2 lbs. new red potatoes\n1 cup petite frozen peas, defrosted (see note)\n1 large carrot, pared, sliced (1 cup)\n1 cup fresh corn kernels (2 ears)\n1 cup broccoli flowerettes, cut into small pieces\n1/4-cup sliced green onion\n1/2-cup natural California pistachios\n3/4-cup plain nonfat yogurt\n3/4-cup fat-free mayonnaise\n1 teaspoon dill weed\n1/2-teaspoon black pepper\nCook whole potatoes in boiling water about 15 to 20 minutes, or until tender; drain. Cool, then slice potatoes 1/4-inch thick. Combine potatoes with peas, carrot, corn, broccoli, green onion and pistachios in large bowl. Stir yogurt with mayonnaise, dill and pepper; combine with vegetables and toss gently.\nNote: To thaw peas, pour hot water from cooking potatoes over peas in sieve.\nMakes 12 servings.\nDate Nut Bread with Pistachios\n1 cup dates, chopped\n1 cup boiling water\n1-1/4 cups flour\n1/2-cup natural California pistachios, chopped\n1 teaspoon each baking powder and baking soda\n2 eggs, beaten\n2 tablespoons butter or margarine, melted\n1 teaspoon each vanilla and grated orange peel\nSoak dates in water; cool (do not drain). Combine flour, pistachios, sugar, baking powder, baking soda and salt. Combine dates with water, eggs, butter, vanilla and orange peel; add to flour mixture. Mix only until moistened. Spoon into 8-1/2 x 4-1/2 x 2-1/2-inch greased loaf pan. Bake at 350 degrees F. 45 to 50 minutes or until wooden pick inserted near centre comes out clean.\nMakes 1 loaf.\n1/2-cup soft butter or margarine\n1 teaspoon vanilla extract\n2 large eggs\n2 tablespoons rum\n1-1/4 cups flour\n1/2-cup ground California pistachios\n2 teaspoons baking powder\n1/2-teaspoon ground cinnamon\n1/2-teaspoon ground cardamom\n1/2-teaspoon grated lemon peel\n1/2-cup chopped California pistachios\nPreheat oven to 350 degrees F. Beat butter with sugar and vanilla until creamy and light-coloured. Beat in eggs, l at a time. Beat in milk and rum. In a separate bowl, mix flour, ground pistachios, baking powder, salt, spices and lemon peel. Add this mixture to the butter-sugar mixture just until blended (don't overmix). Lightly fold in 1/4-cup chopped pistachios. Grease 12 muffin cups or line cups with muffin papers. Evenly portion batter into cups. Sprinkle with remaining 1/4-cup chopped pistachios. Bake for 30 minutes or until done.\nMakes 12 muffins.\nTip: Pulse pistachios in a food processor until coarsely ground."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:4af4de5f-a843-406d-82d0-6486ce3bc644>","<urn:uuid:773bfd4d-6022-49ef-8503-92dd240bcf3d>"],"error":null}
{"question":"What happened to John Billington, one of the early Plymouth settlers, and when did his widow remarry?","answer":"John Billington was hanged for murder in September 1630. His widow, Elinor/Ellen, later remarried in September 1638 to Gregory Armstrong.","context":["As of June 25, 2022:\n- Recruiting Billington Y-DNA descendants for NGS/WGS testing. Interested parties can contact MayflowerDNA1620@gmail.com\n- Two Big Y-700 test results confirm the Y-DNA profile for the family.\n- Billington surname DNA Project created.\nFor more information see: Hodge, Harriet Woodbury (rev. 2001 by Robert S. Wakefield) (rev. 2014 by John Bradley Arthaud), Mayflower Families through Five Generations. Vol. 21 Billington. Names in bold text have had a Y-DNA descendant Y-DNA tested.\nJohn BillingtonPilgrim Hall Museum John Billington Biography was born circa 1580 in England. His life in England is basically unknown. He had a wife Elinor/Ellen and two sons. The Billingtons were not part of the Separatist Congregation in Leiden, but were apparently recruited in England. The family was often in trouble with the authorities in Plymouth and the senior John Billington was hanged for murder in September 1630 and the widow married in Sep 1638 to Gregory Armstrong. John and Elinor had two sons. The older son John had died before his father (apparently never married) so all of the descendants are from the younger son, Francis.\nFrancis BillingtonPilgrim Hall Musuem Francis Billington biography deposed on 10 Jul 1674 that he was \"68 years of age\" so he was born about 1606, possibly in Lincolnshire, England. He married in Plymouth in Jul 1634 Christian PENN Eaton, the widow of Mayflower passenger Francis Eaton. They resided in Plymouth for most of their married lives, but late in life they moved to Middleboro, where they both died.\nJoseph Billington was born in Plymouth on 2 Feb 1636. He married a wife named Grace at New Shorham, RI on 26 Sep 1672. Joseph died at Block Island, RI on 7 Jan 1684. Joseph and Grace had three children (New Shoreham [Block Island], RI): Mary (1674), Francis and Elisha (1676).\nIsaac Billington was born in Plymouth in 1644. He married Hannah Glass about 1674. They had six children: Desire (c1675), Lydia (1677), Eleanor (c1683), Mary (1685), Seth (c1687 likely never marr.) and Isaac (c1692).\nFrancis Billington born Block Island, RI abt 1676 and died bef. 29 Jul 1724. He married Plymouth 17 May 1702 Abigail Churchill) and they had seven children: Sarah (1702), Marcy (1704), Francis (1708 likely never marr.), Jemima (1710), Content (1712) Abigail (1716) and Joseph (1718/9 likely never married).\nElisha Billington was born Block Island abt 1660 and died bef. 13 Apr 1741. He married Jane ___ and they had five children: Daniel (1710 m Mary Austen), Joseph (c1712 m Abigail Braman), Jemima (1715), Sarah (1718) and Mary (1720).\nIsaac Billington (II) was born abt. 1692 and died Middleborough 19 Jun 1779. He married there 5 Mar 1729/30 Mary Donham and they had five children: Isaac (1730/1 poss. never marr.), Nathaniel (1732/3 m. Mary Donham), Seth (1735 m Deborah Smith), Ichabod (1737 m Betty Pack) and Mary.\n- There are some indications where he may have been from Spalding, Lincolnshire, especially after his marriage.\n- A Francis Billington, son of John (John was named heir to a lease of crown land in Lincolnshire in 1613). In 1650 Francis was stated to be living in New England and aged 40 years or thereabouts. There are some issues with assigning This Francis Billington of Lincolnshire as identical to the Mayflower passenger.\n- MD 2:46, citing PCR 1:81\n- After the death of Francis Billington, Abigail married Nathaniel Howland\nDNA Results R1b-FTA18300\nThe Mayflower DNA Project lists two patrilineal (all male line) descendants of John Billington. Neither of these individuals currently have any Y-SNP results. One individual has tested 111 Y-STRs. FTDNA has only predicted where the family falls under the R-M269 Haplogroup. Plugging in the 111 Y-STRs in the Nevgen Y-DNA Haplogroup Predictor gives a prediction falling somewhere below R1b-U106>>Z381>>L48>>Z8>>Z12>Z8175>CTS10742.\nNext Generation Sequence/Whole Genome Sequence testing\nAs of June 25, 2022 two Big Y-700 test results have been posted to the Mayflower DNA web site. This result came back with a subclade of CTS10742, BY14508>BY145737>FTA18300. Further testing of Billington patrilineal descendants would possibly define subclade branches of the Billington family.\nThe following families married daughters of John Billington.\nEdward May married Dorcas Billington\nJohn Martin married Mercy Billington\n- Elizabeth had a third husband, Thomas Patey, but no children by this last marriage.\nReferences and External Resources\n- Hodge, Harriet Woodbury (rev. 2001 by Robert S. Wakefield) (rev. 2014 by John Bradley Arthaud), Mayflower Families through Five Generations. Vol. 21, Billington General Society of Mayflower Descendants, Plymouth, MA 2nd Ed. 2014\n- Arthaud, John Bradley, Mayflower Families through Five Generations vol. 21 [Billington], part 3. General Society of Mayflower Descendants, Plymouth, MA, 2018\n- Roser, Susan E., Mayflower Increasings From the Files of George Ernest Bowman at the Massachusetts Society of Mayflower Descendants, Genealogical Publishing Co., Baltimore, MD, Second edition 1995, 1996. pp. 16-18.\n- Roser, Susan E., Mayflower Passenger References, (from contemporary records & scholarly journals) [www.stewartbooks.com Stewart Publishing & Printing], Canada. Second edition 2015\n- wikipedia article on John Billington\n- wikitree profile for John Billington\n- Caleb Johnson's MayflowerHistory.com article on John Billington\n- FTDNA GSMD Mayflower DNA Project\n- YSEQ.net Mayflower Group\n- History of Massachusetts biography of John Billington with great quotes from contemporary and scholarly sources.\n- Billington Family Society and Facebook Group\n© 2015-2023 mayflowerdna.org All Rights Reserved"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:2e4428a8-1a83-4799-bf07-2da4eac6aef9>"],"error":null}
{"question":"Did the United Methodist belief in the priesthood of believers originate from Luther's teachings?","answer":"The concepts have some overlap but different emphases. Luther taught that every baptized believer is part of the priesthood of all believers, with multiple holy vocations including pastoral office, family roles, and civil positions. United Methodists, while also believing in universal incorporation into the Christian community through Baptism, focus more on being part of Christ's universal church through Baptism and Holy Communion, emphasizing transformation by the Holy Spirit and faithful discipleship rather than specifically discussing the priesthood of all believers.","context":["Our Common Heritage as Christians\nAs United Methodists, we share a common heritage of faith with Christians of every age and nation. This heritage is grounded in the Biblical witness to Jesus Christ as Savior and Lord, which is the source and measure of all valid Christian teaching.\nWith Christians of other denominations, we confess belief in the triune God-Father, Son, and Holy Spirit. At the heart of the gospel of salvation is God’s incarnation in Jesus of Nazareth. Scripture witnesses to the redeeming love of God in Jesus’ life and teachings, his atoning death, his resurrection, his sovereign presence in history, his triumph over the powers of evil and death, and his promised return. Because God truly loves us in spite of our willful sin, God judges us, summons us to repentance, pardons us, receives us by that grace given to us in Jesus Christ, and gives us hope of life eternal. Through faith in Jesus Christ, we are forgiven, reconciled to God, and transformed as people of the new covenant.\nWe United Methodists understand ourselves to be part of Christ’s universal church. We are incorporated into this world-wide community of faith by Baptism, receiving the promise of the Holy Spirit that re-creates and transforms us. Through the regular celebration of Holy Communion, we participate in the risen presence of Jesus Christ and are thereby nourished for faithful discipleship.\nOur Distinctive Heritage as United Methodists\nJohn Wesley (1703-1791), founder of the Methodist movement, combined belief in grace, justification, assurance, and sanctification in a powerful manner to create distinctive emphases for living the full Christian life. The underlying energy of our Wesleyan theological heritage stems from an emphasis upon “practical divinity,” the implementation of genuine Christianity in the lives of believers.\nGrace pervades our understanding of Christian faith and life. By grace we mean the undeserved, unmerited, and loving action of God in human existence through the ever-present Holy Spirit.\nPrevenient Grace – We acknowledge God’s Prevenient grace, the divine love that surrounds all humanity and precedes any and all of our conscious impulses. This grace prompts our first wish to please God, our first glimmer of understanding concerning God’s will, and our first hint of conviction of having sinned against God.\nGod’s grace also awakens in us an earnest longing for deliverance from sin and death, and moves us toward repentance and faith.\nJustifying Grace – We believe God reaches out to the repentant believer with accepting and pardoning love.\nIn justification we are, through faith, forgiven our sin and restored to God’s favor. This righting of relationships by God through Christ calls forth our faith and trust as we experience regeneration, by which we are made new creatures in Christ. Our Wesleyan theology also embraces the scriptural promise that we can expect to receive assurance of our salvation as the Spirit “bears witness with our spirit that we are children of God.”\nSanctifying Grace – We hold that the wonder of God’s acceptance and pardon does not end God’s saving work, which continues to nurture our growth in grace. Through the power of the Holy Spirit, we are enabled to increase in the knowledge and love of God and in love for our neighbor.\nNew birth is the first step in this process of sanctification. Sanctifying grace draws us toward the gift of Christian perfection, which Wesley described as a heart “habitually filled with the love of God and neighbor.”\nFaith and Good Works – We see God’s grace and human activity working together in the relationship of faith and good works. God’s grace calls forth human response and discipline. While faith is the only response essential for salvation, Wesley believed that salvation evidences itself in works of piety and mercy.\nMission and Service – We insist that personal salvation always involves Christian mission and service to the world. By joining heart and hand, we assert that personal religion, evangelical witness, and Christian social action are reciprocal and mutually reinforcing.","Reformation Celebration | faith\nA Clear Voice\nFor almost 200 years, voices in the church had been clamoring for religious reform. John Wycliffe (1320 c.-1384), seminary professor and Bible translator, opposed the opulent wealth of the clergy and called for them to give up their property. His conflict with the church continued even after his death. Wycliffe was posthumously declared a heretic; his remains were exhumed from sacred ground and burned. John Huss (1373-1415), university professor and priest, cried out against the church’s sale of indulgences. He was burned at the stake. Girolamo Savonarola (1452-1498), Dominican friar and preacher, also denounced the corruption of the church. When he was summoned to Rome by the Pope, Savonarola refused. After excommunication, he was hanged and burned. These were the predecessor voices to the Reformation. They denounced the church’s practices and criticized the church’s leaders. They condemned ecclesiastical exorbitance and the sale of indulgences. They supported Scriptural authority and Biblical translation into the vernacular. But, these faithful voices were muted and muzzled by ecclesiastical hierarchs, regional nobility, and the momentum of cultural tradition.\nOn October 31st, 1517, a new voice rang out. It was clear and strong, scriptural and specific. It articulated a list of grievances with the church, 95 Theses for debate. Throughout the country and across the continent, Dr. Martin Luther’s Theses denounced the impious practices of a sinful church. Luther abhorred the indulgence industry that terrorized God’s children with the fires of purgatory until they paid the church for the release of their souls. He railed against the false theology that claimed that the sacrifice of Christ and the benefits of the Cross could be purchased with money. He vilified those who used their churchly positions to defraud the poor and deceive the biblically uneducated. With the posting of his 95 Theses on the Wittenberg Church door, Luther began a career that would last almost 40 years, as the great reformer of the Christian church.\nThere are many things for which Luther is famous and many positive results of the Reformation. For the sake of brevity, let us focus on three. Luther taught that:\nWe are justified by grace through faith in Jesus Christ, apart from any works of our own. All of the work that is necessary for salvation has been accomplished by Jesus Christ on the Cross and there is nothing that should be or can be added to it. There is nothing lacking in His all-sufficient work of atonement. By faith in Christ, God conducts “the great exchange.” God imputes to Christ all of our sin; God imputes to us the complete righteousness of Christ.\nEvery baptized believer is a member of the priesthood of all believers. As such, we each have a holy vocation or calling. The first vocation of every believer is faith. This is our first and greatest calling. But, God has not only called us to Himself, He has placed us in the world. There, we have multiple vocations. Luther spoke of three institutions: 1) the pastoral office or holy orders, 2) the household or family, and 3) society or civil government. A pastor is given a vocation from God, but so are a father, mother, sister, brother, husband and wife. These are holy vocations too. They are established by God and lived out through God. A third holy vocation involves servants and maids, builders and workers, judges and mayors. God has established and blesses these vocations as well. Instead of our faith removing us from the world, Luther proclaimed that our faith places us in the world, as the hands and heart of Christ.\nThe Holy Bible is God’s true and living Word, a personal Word to each one of us. It tells the story of God’s love and the truth of our disobedience. Most especially, it declares the forgiveness of sins and the promise of eternal life to all believers through the life, death, and resurrection of Jesus Christ. God desires that none should perish, but that all should come to the saving knowledge of Jesus Christ. For this reason, the Bible should be available to all people in a language that they can understand. God’s Word is inspired, infallible, and inerrant, and the highest authority in all matters of faith and life.\n500th Anniversary of the Reformation\nOctober 31, 2017 will mark the 500th Anniversary of Luther’s posting of the 95 Theses. We are grateful to God and to His faithful servant, Martin Luther, for reforming the church according to the teachings of the Holy Scriptures. Surely, every AALC pastor and congregation will be mindful of the significance of this celebration throughout the year. However, October 31st, Reformation Day and November 1st, All Saints’ Day, will be days of extraordinary thanksgiving and celebration!\nThe AALC has planned special festivities for the 500th Anniversary of the Reformation. Every pastor and every congregational member of our church body is invited! We will hold a two-day Festival Celebration: Tuesday, October 31st and Wednesday, November 1st, 2017. The event will take place at Grace Lutheran Church in Deephaven, Minnesota. Pr. Dan Sollie and the people of Grace Lutheran Church will host the event!\nThough our plans are not fully formulated, this is the beginning of what we have in mind:\n- Each day will begin with Coffee and Fellowship from 9:00-9:30 AM.\n- Beginning at 9:30 AM, we will have two presentations.\n- A luncheon will be provided.\n- In the early afternoon, a third presentation will be given.\n- (The first day, the presentations will focus on justification, the ministry, and the church. The second day, the presentations will focus on the beginning of the Christian life, living the Christian life, and the completion of the Christian life.)\n- During the middle and late afternoon, folks will have free time to socialize or visit area locations.\n- A choir practice will be held at 2:00 PM to prepare for the evening worship.\n- (Pr. Eric Ishimaru will be our Choir Master. Special music is being selected and will be sent to every AALC congregation and choir. Any and all choristers are welcome to attend, rehearse, and sing with the Association Choir!)\n- A Special Entertainment Surprise is being planned! Also, a Commemorative Gift will be given to every participant!\n- Each evening will conclude with a Service of Worship at 7:00 PM, featuring a Brass Ensemble, the Association Choir, and your favorite Lutheran hymns.\nOur hope is that this will be one of the largest gatherings of The AALC in all of its history. Information about registration, overnight accommodations, and cost soon will be forthcoming!\nMake plans now to attend! Registration will be limited to the first 330 people!"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:35dca6e5-40e2-49c7-a978-085c4f4b1dd0>","<urn:uuid:06aa5349-875f-43d3-b512-ab769416ed67>"],"error":null}
{"question":"Can someone explain what DWR is and how to reapply it when it wears off? I'm so confused! 😅","answer":"DWR (durable water repellent) is a thin layer applied to technical garments that repels oil, grease, dirt, and water, preventing the fabric from getting saturated. When it wears off, you can reapply it using either spray-on or wash-in products. Spray-on treatments work best for insulated waterproof jackets as they don't coat the jacket liner, while wash-in waterproofing provides better coverage for hard shell jackets both inside and out. Before applying new DWR, the jacket must be cleaned to remove dirt and oil, allowing the new coating to bond properly to the fibers.","context":["HOW TO REPROOF A WATERPROOF JACKET\nThere’s nothing quite like a modern waterproof jacket. Today’s shells and insulated waterproof jackets are built to withstand heavy downpours, while allowing sweat and moisture from your skin to evaporate, keeping you dry in the wettest of weather – unlike the old rubber rain slickers, which created an internal sauna. Also unlike rubber, modern waterproof jackets can lose their waterproof properties over time (as well as their wicking properties), so it’s important to properly maintain a waterproof jacket to get the maximum benefit and life out of it. Here’s how.\nHOW TO WASH A WATERPROOF JACKET\nWashing your waterproof jacket and maintaining its repellent are key to its performance. Most modern waterproof jackets are treated with a Durable Water Repellent (DWR) coating, which covers the threads of the shell with a membrane that repels water droplets, while leaving holes small enough that only evaporated sweat can escape. Problems with this membrane (and the jacket’s overall water repellency and wicking ability) start when a jacket becomes dirty. Dirt and oil work against the DWR finish and allow water droplets to soak into the outer shell, while blocking the tiny holes that allow evaporated sweat out, causing the garment to lose both its wicking and water repellent properties.\nSo how do you prevent your waterproof jacket from losing its water repellency? The answer is simple, but may seem counterintuitive for a garment treated with a DWR coating – wash it. Washing DWR treated jackets removes the oil and dirt that can cause your jacket to lose its functional properties. Simply follow the directions on your waterproof jacket’s label and allow the garment to dry properly. Most DWR garments suggest washing in cold water with a gentle detergent and then drying on medium heat, but this can vary so make sure to check the directions.\nOnce your waterproof jacket has been washed and dried, you should see a renewed water repellency close, if not equal to brand new. Maintaining a waterproof jacket will help give it a long life, but eventually the DWR finish will wear away and you’ll need to reapply a similar finish.\nREPROOFING A WATERPROOF JACKET\nThe first step in reproofing a waterproof jacket is washing it as described above. This will remove any dirt and oil, allowing for the new DWR finish to bond directly to the jacket fibers.\nNext, choose the proofing agent that is best for your garment. Some are built for hard shells, others for soft shells, and still others are specifically for insulated garments; for each of those you may have the choice between a spray-on or wash-on agent. A spray-on treatment tends to work best for insulated waterproof jackets, as it does not coat the jacket liner with waterproofing. For hard shell jackets, wash-on waterproofing will generally provide the best application of waterproofing to the fabric inside and out.\nOnce you’ve decided on a waterproofing agent, simply follow the directions and allow your jacket to dry; you’ll find its water repellency to be like new again and you can continue to maintain it through multiple wash cycles as described above in “How to Wash a Waterproof Jacket.”\nA waterproof jacket is a necessary tool in everyone’s closet, whether you’re fishing in the wet weather of coastal Washington, or hailing a taxi in New York City. Properly maintaining a waterproof jacket and reapplying waterproofing when necessary can maximize the life of your garment with very little effort. Follow these simple steps and your favorite waterproof jacket will be with you for the long haul.","What is DWR?\nDWR stands for “durable water repellent,” a thin layer of liquid applied to the outside of technical garments to repel oil, grease, dirt, and water. Garments treated with a DWR treatment will not get saturated with water as easily as garments without, and will be less prone to “wet out,” (when saturated outer fabrics get heavy and cling to your skin making you feel clammy and damp). Naturally, rainwear with a DWR treatment will keep you comfortable and dry more effectively than rainwear without.\nRainwear with highly effective DWR is highly desirable in the outdoor world. Instead of soaking to the core, hikers, backpackers, and campers who wear rainwear with DWR can roll or shake off water and remain dry inside and out. Unfortunately, a DWR treatment is not a permanent property of most outerwear and periodic care is necessary to maintain peak performance.\nHow does DWR work?\nDWR is science at work.\nHave you ever seen how many drops of water you can put on a penny in a science class? (Try it if you haven’t, it’s really surprising.) The water stacks up and forms a tall bubble. The tall water droplet has a high contact angle with the penny. When the droplet finally bursts and the water spreads out, the contact angle is very low.\nSimilarly, an effective DWR treatment keeps water droplets at a high contact angle with the fabric of the rainwear. This minimizes the surface area of the wetted fabric and better allows water droplets to roll off. Less water on the outer fabrics allows the technical waterproof fabric to breathe its best which helps keep you more comfortable. More importantly, keeping dry is a vital component to staying safe on any adventure.\nHow Long Does DWR Last and How Can I Test My Jacket?\nHow long a DWR treatment lasts depends mainly on garment use. If you use your garment several times each week for intense periods of activity, the DWR will not last as long as a jacket that is used, say, only a few times per month. And DWR applied at the factory to a new garment typically lasts longer than reapplications.\nTo test your jacket’s DWR, sprinkle on some water. If the water beads up, your DWR is in good shape. If the water soaks in and obvious darker areas appear, it is time to restore your DWR. Be sure to test your jacket all over, as soaking can occur from any breach.\nHow Do I Restore Existing DWR?\nRestoring your factory applied DWR is easy, simply wash and apply heat. To restore the water repellency of GORE-TEX outerwear, GORE-TEX brand recommends the following steps:\n- Machine wash your garment as described in the wash instructions. Line dry your garment or tumble dry it on a warm, gentle cycle.\n- Once it is dry, tumble dry your garment for 20 minutes to reactivate the durable water-repellent (DWR) treatment on the outer fabric.\n- If unable to tumble dry, iron the dry garment on the gentle setting (warm, no steam) by placing a towel or cloth between the garment and the iron. This will help reactivate the DWR treatment on your garment’s outer fabric.\nThese steps will restore the factory durable water repellent treatment and water may bead up on your outerwear. If it does not, it’s probably time to apply new DWR to your jacket.\nHow do I apply a new DWR treatment?\nSometimes, the factory applied treatment can no longer be reactivated. In that case, apply a new water-repellent treatment available as a pump-spray or wash-in product to the garment's outer fabric.\nCheck the label on your jacket to verify the appropriate option. Spray-on products are easy to apply using the built-in pump.\nShould I Use a Spray-On or a Wash-In Product to Restore My DWR?\nBoth products will work to restore the DWR to the exterior of the garment. The main concern with wash-in products is that the DWR is also applied to the inside of the clothing. This may compromise the ability of the interior fabric to wick perspiration away from your skin, and may adversely impact garment breathability.\nA wide variety of DWR products are available and, as of 2016, new DWR formulas are entering the market. The three main groups are fluorocarbon-based, silicone-based, and hydrocarbon-based polymers. It’s worth noting that each type has its strengths and weaknesses, such as repelling oils and dirt very well, but water not as well or vice versa. Something else to keep in mind is that the product you used previously may have had a change in the formula as a result of improved performance or environmental impact.\nWill Restoring the DWR Make My Jacket More Waterproof?\nThe simple answer is no. DWR is a surface treatment and does not impact the performance of the waterproof membrane.\nBoth properties are separate, meaning when the DWR wears off, a waterproof jacket is still waterproof. Waterproof ratings are determined without considering DWR. A well-maintained DWR treatment will, however, help your jacket breathe very efficiently by minimizing saturated outer fabric and maximize your comfort.\nNo matter your product, the story is relatively the same. It’s important to monitor and maintain your jacket for optimal performance."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:47281a3e-da32-4127-a497-445d7be1afa2>","<urn:uuid:ecbca3e7-0d14-488d-85c4-62f31c7da337>"],"error":null}
{"question":"What are the physical characteristics of Saturn's moon Mimas, and how does its composition compare to Saturn's ring system?","answer":"Mimas is a small moon, measuring only 243 miles (392 km) in diameter, and is most notable for a massive crater that makes it appear like a strange, bulging eyeball. This crater, called Herschel Crater, has towering cliffs higher than Mount Everest, reaching 13,123 feet (4,000 m), with a central peak of 32,800 feet (10,000 m). The impact that created this crater was so significant that it nearly destroyed the moon. In contrast, Saturn's rings are composed primarily of water ice mixed with smaller amounts of dust and rocky matter, forming a complex system of thousands of individual ringlets rather than solid structures.","context":["A Look at Saturn by Ray Spangenburg; Kit Moser\nBy Ray Spangenburg; Kit Moser\nRead or Download A Look at Saturn PDF\nBest supernatural books\nIt’s homemade season and the DIY initiatives are in complete swing with enthusiastic owners tearing down partitions, sanding previous flooring and waking up ghosts. “It will be referred to as DYI for do-yourself-in,” Mia commented because the demands PEEPs flood the switchboards and crash their desktops.\nS. T. Joshi is likely one of the top professionals on bizarre fiction, and during this international fable Award-winning examine he presents a entire historical past and research of the whole variety of strange fiction from antiquity to the current day. For the 1st time, the entire contents of either print volumes can be found jointly in one digital booklet dossier.\nDuring this vigorous and considerate booklet, Keith Tester explores the placement of people within the modern international, their methods of experiencing this international and their methods of performing on it.\nAn important variety of americans spend their weekends at flying saucers conventions listening to whispers of presidency cover-ups, at New Age gatherings studying the keys to enlightenment, or ambling round ancient downtowns studying approximately resident ghosts in tourist-targeted “ghost walks”. they've been fed a gentle vitamin of fictional exhibits with paranormal issues corresponding to The X-Files, Supernatural, and Medium, exhibits which may search to easily entertain, but additionally serve to disseminate paranormal ideals.\nExtra info for A Look at Saturn\nOnly 243 miles (392 km) in diameter. Thanks to a huge crater that extends across Mimas's face, the looks like a strange, bulging eyeball. This which dominates the moon's cavity, Crater. has towering It Mount higher than The crater (4,000 m) 32,800 cliffs Everest — surface, the tallest is —and moon that was blown the peak formed at its by — one-third of center stands 13,123 must have been caused this small Mimas may once have been to pieces Herschel mountain on Earth. high. Scientists think the crater experts think as (10,000 m) high.\nCassini will continue to orbit Saturn for at least 4 years and carry out close-up studies of as Meanwhile, atmosphere. protect its the many of Saturn's moons Huygens The probe spacecraft's probe will plunge as possible. into Titan's has three parachutes and a heat shield to aluminum body as it falls toward the moon's surface and then gently touches down. If everything goes according to plan, the probe will take transmit them to scientists more than 1,000 photographs of Titan's to the Cassini orbiter, which will surface and then send the images on Earth.\nOf forces at work inside These multicolored and outside the Saturn rotates so rapidly that equator, giving the planet a queen of the planets the solar system. within its is The just its planet. atmosphere bulges out pronounced oval shape. 54 hours long LOOK AT S1TUR at the day on the the second-shortest in planet's rapid rotation causes the substances Some bands parallel contain very dense materials, while others contain materials with lower densities. « A atmosphere to separate into colorful bands that run to the planet's equator.","Saturn’s most distinctive feature is the thousands of rings that orbit the planet. Despite the fact that the rings look like continuous hoops of matter encircling the giant planet, each ring is actually made of tiny individual particles. Saturn’s rings consist largely of water ice mixed with smaller amounts of dust and rocky matter. Data from the Cassini spacecraft indicate that the environment around the rings is like an atmosphere, composed principally of molecular oxygen.\nThe ring system is divided into 5 major components: the G, F, A, B, and C rings, listed from outside to inside (but in reality, these major divisions are subdivided into thousands of individual ringlets). The F and G rings are thin and difficult to see, while the A, B, and C rings are broad and easily visible. The large gap between the A ring and and the B ring is called the Cassini division. One of Saturn’s moons, namely; Enceladus is the source of Saturn’s E-ring. The moon’s geyser-like jets create a gigantic halo of ice, dust, and gas that helps feed Saturn’s E ring.\nEnceladus has a profound effect on Saturn and its environment. It’s the only moon in our solar system known to substantially influence the chemical composition of its parent planet. The whole magnetic environment of Saturn is weighed down by the material spewing from Enceladus, which becomes plasma — a gas of electrically charged particles. This plasma, which creates a donut-shaped cloud around Saturn, is then snatched by Saturn’s A-ring, which acts like a giant sponge where the plasma is absorbed.\nCredit: Bjorn Jonsson, NASA/JPL/SSI\nSeriously thinking of making some new characters based of these African Gods\nsomebody should make a sick movie with these guy\nI love this so deep and sexy at the same dam time\nLet’s face it - English is a crazy language. There is no egg in eggplant nor ham in hamburger; neither apple nor pine in pineapple. English muffins weren’t invented in England or French fries in France. Sweetmeats are candies while sweetbreads, which aren’t sweet, are meat. We take English for granted. But if we explore its paradoxes, we find that quicksand can work slowly, boxing rings are square and a guinea pig is neither from Guinea nor is it a pig.\nAnd why is it that writers write but fingers don’t fing, grocers don’t groce and hammers don’t ham? If the plural of tooth is teeth, why isn’t the plural of booth beeth? One goose, 2 geese. So one moose, 2 meese? One index, 2 indices? Doesn’t it seem crazy that you can make amends but not one amend? If you have a bunch of odds and ends and get rid of all but one of them, what do you call it?\nIf teachers taught, why didn’t preachers praught? If a vegetarian eats vegetables, what does a humanitarian eat? In what language do people recite at a play and play at a recital? Ship by truck and send cargo by ship? Have noses that run and feet that smell? How can a slim chance and a fat chance be the same, while a wise man and a wise guy are opposites?\nYou have to marvel at the unique lunacy of a language in which your house can burn up as it burns down, in which you fill in a form by filling it out and in which an alarm goes off by going on. English was invented by people, not computers, and it reflects the creativity of the human race (which, of course, isn’t a race at all). That is why, when the stars are out, they are visible, but when the lights are out, they are invisible."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:48ff793d-6da6-4aad-b461-bd9ad3b352d1>","<urn:uuid:2574aaf7-0f90-4cf2-9356-965de3f72ef6>"],"error":null}
{"question":"What are the similarities between operationalizing abstract concepts in political science research and measuring a child's reading comprehension?","answer":"Both processes require converting complex concepts into quantifiable measurements. In political science, abstract concepts like political efficacy must be broken down into measurable components through specific survey questions. Similarly, reading comprehension is operationalized through structured assessments like the DRA or Lexile Framework, which convert the abstract concept of reading ability into concrete numerical scores. Both fields emphasize the importance of reliable and valid measurement tools to capture the full complexity of what they're measuring.","context":["Levels of measurement are important because they serve as a way to think about both the amount of information available in a measure and the mathematical properties of the measure. In this exercise you are going to consider the amount of information available in variables that measure the same concept with different levels of measurement.\nFor each of the variables below, identify the level of measurement. Second, explain why one variable provides more information than the other. Finally, why might you prefer to use one measure over the other? Why is capturing more information important?\nVariable #1: What is your highest completed level of education?\nNo formal education\nHigh school College\nVariable #2: How many years of formal education have you completed?\nHerrmann, Tetlock, and Visser1 define the disposition of military assertiveness as “the inclination toward different methods of defending American interests abroad, in particular, whether a person prefers more militant and assertive strategies or more accommodative and cooperative approaches.” To measure military assertiveness, they used ten items. For the first eight items, they asked respondents to indicate whether they strongly agreed, agreed, neither agreed nor disagreed, disagreed, or strongly disagreed with the statement.\nWhich of the following items do you think are the most valid measures of the concept of military assertiveness and why? Which ones do you have trouble relating to the concept and why? What kind of validity (face or construct) do you think the items exhibit?\n1. The best way to ensure world peace is through American military strength.\n2. The use of military force only makes problems worse.\n3. Rather than simply reacting to our enemies, it’s better for us to strike first.\n4. Generally, the more influence America has with other nations, the better off they are.\n5. People can be divided into two distinct classes: the weak and the strong.\n6. The facts on crime, sexual immorality, and the recent public disorders all show that we have to crack down harder on troublemakers if we are going to save our moral standards and preserve law and order.\n7. Obedience and respect for authority are the most important virtues children should learn.\n8. Although at times I may not agree with the government, my commitment to the United States always remains strong.\n9. When you see the American flag flying, does it make you feel extremely good, somewhat good, or not very good?\n10. How important is military defense spending to you personally: very important, important, or not at all important?\nMost valid measures of the concept of military assertiveness:\nWorst “fit” for concept:\nKind of validity:\n1Richard K. Herrmann, Philip E. Tetlock, and Penny S. Visser, “Mass Public Decisions to Go to War: A Cognitive-\nInteractionist Framework,” American Political Science Review 93 (September 1999): 554.\nOperationalization is deciding how to record empirical observations of the occurrence of an attribute or a behavior using numerals or scores. In other words, it is deciding how to move from defined concept to quantifiable variable. In this exercise you are going to consider the challenges involved in quantifying both concrete and abstract concepts that are commonly used in political science research. You will find below a series of conceptualized terms.\nYour job is to explain how you would operationalize each term for use in a survey research project by creating the questions that would yield the appropriate variable for each concept. (Hint: Concrete terms are much easier to work with than abstract terms. Pay close attention to the abstract terms, such as ideology and efficacy.) Example: Voter registration: Whether someone is currently registered to vote. Answer: Ask each respondent to indicate whether he or she is currently registered to vote by asking, “Are you currently registered to vote in your state?” (1) Yes, I am registered to vote; (0) No, I am not registered to vote.\n1. Gender: Male and female\n2. Household income: The amount of money earned by all members of a household in a year\n3. Race: The race each respondent most closely identifies with\n4. Ideology: A set of beliefs and ideas, including one’s moral code and worldview. The most important issues and ideas involve how the government should address those unable to provide food, health care, and housing for themselves and their children. The extent to which the government should extend services to support those in need in these areas makes up the worldview.\n5. Political efficacy: The belief that one’s political action will have a meaningful effect. In particular\nI define political action as interpersonal communication with elected officials.\nBelow you will find a series of hypotheses. For each hypothesis, 1) identify the independent and dependent variables, and 2) explain how you could measure each variable. When explaining your measurement strategy, be careful to consider validity and reliability.\n1. Small business owners are more likely to support tax cuts than other voters.\n2. The availability of government-subsidized childcare causes household income to rise.\n3. An increase in the number of nongovernmental organizations operating in an authoritarian state increases the rate at which the state democratizes.\n4. Access to clean drinking water causes life expectancy to increase.","Your Child’s Reading Level Explained\nAshley Day-Bohn, M.Ed.\nHow is reading level measured?\nThere are several common assessments used to measure a child’s reading level, such as the Developmental Reading Assessment (DRA), which is typically administered one-on-one with a teacher. Another popular leveling system is the The Lexile Reading Framework, which is usually assessed on the computer. Both types of tests will give you a numerical score which identifies your child’s reading fluency (speed and accuracy of decoding the text) and comprehension compared to grade level expectations.\nIf your child attends school and you are not sure of his or her reading level, contact the classroom teacher, who will likely have this information on file. If your child is home-schooled, you can contact a certified teacher/evaluator through Avenue Education to obtain a complete diagnostic reading assessment.\nWhat does reading level mean?\nThe instructional reading level is the highest level of text your child can read without reaching the point of frustration. It is the optimal level for teaching reading concepts, since the text provides just enough challenge to introduce new concepts without overwhelming the student. Accordingly, children may need help with a few words here and there when reading text at the instructional level. This is different from the independent reading level, which involves easier text that can be read without help. The ideas of instructional and independent levels are derived from a theory of learning called the “Zone of Proximal Development” (Vygotsky, 1978). For example, a Lexile score of 500-550 represents a ZPD ranging from independent (500) to instructional (550).\nHow do I use reading level when buying books?\nBooks for your child’s home reading library should fall somewhere in that ZPD range, depending on how you plan to use them. Books that are slightly above your child’s reading level are great for reading aloud, such as for a bedtime story. Shared reading builds vocabulary and comprehension skills. Independent reading should be fun and stress-free. Scholastic.com/teachers has a great tool called the Book Wizard, which can help you find materials within their inventory according to reading level. Another excellent resource is ARBookFinder, which allows you to search the Accellerated Reader catalog to identify the approximate level of a given book you may already have in mind.\nWhat if I don’t know the level of a book?\nIf you are in the library or bookstore with your child, you can use a simple trick teachers like to call, “The Five Finger Rule.” Your child reads one page from the book aloud. If you count more than five errors on one page, the book is too difficult for them to read alone. If your child makes fewer than five errors, this book is probably a good fit for independent reading. If they make no errors, the book may be too easy. This method is so simple, children can start using it to make good selections for themselves. It can teach them responsibility and self-awareness, plus save you time and frustration!\nMcLeod, S. A. (2012). Zone of Proximal Development. Retrieved from www.simplypsychology.org/Zone-of-Proximal-Development.html\nFor more tips on choosing books for your child:\n\"3 Simple Tips for Choosing Age-Appropriate Children’s Books\"\nLeveled Reading Sets\nNonfiction books support new Common Core Standards. The books below have articles written on four different reading levels! They include comprehension questions for each article."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:a871c94e-d3c1-4396-86d6-ceaec174e078>","<urn:uuid:f8b1047c-ea96-4f77-be63-7d8e6d13d175>"],"error":null}
{"question":"How does chervil preservation compare to managing fungal diseases in gardens?","answer":"Chervil can be preserved through multiple methods: drying in a sunny location or oven, freezing either plain or in ice cubes, mixing with butter, or preserving in white wine vinegar. For fungal disease management, copper fungicide offers an organic solution but requires careful application. The key is prevention - proper garden management includes spacing plants for air flow, using drip irrigation instead of overhead sprinklers, pruning infected leaves promptly, and practicing crop rotation. When fungal diseases do occur, copper treatments must be applied according to exact manufacturer specifications to avoid plant damage or soil contamination.","context":["Chervil: Indoor or Direct Sow Herb\nHow to Sow and Plant\nSowing Seed Indoors:\n- Sow indoors 4-8 weeks before the last frost in spring using a seed starting kit.\n- Sow seeds 1/8 inches deep in seed-starting formula.\n- Keep the soil moist at 70 degrees F\n- Seedlings emerge in 14-28 days\n- As soon as seedlings emerge, provide plenty of light on a sunny windowsill or grow seedlings 3-4 inches beneath fluorescent plant lights turned on 16 hours per day, off for 8 hours at night. Raise the lights as the plants grow taller. Incandescent bulbs will not work for this process because they will get too hot. Most plants require a dark period to grow, do not leave lights on for 24 hours.\n- Seedlings do not need much fertilizer, feed when they are 3-4 weeks old using a starter solution (half strength of a complete indoor houseplant food) according to manufacturer’s directions.\n- If you are growing in small cells, you may need to transplant the seedlings to 3 or 4 inch pots when seedlings have at least 3 pairs of leaves before transplanting to the garden so they have enough room to develop strong roots.\n- Before planting in the garden, seedling plants need to be “hardened off”. Accustom young plants to outdoor conditions by moving them to a sheltered place outside for a week. Be sure to protect them from wind and hot sun at first. If frost threatens at night, cover or bring containers indoors, then take them out again in the morning. This hardening off process toughens the plant’s cell structure and reduces transplant shock and scalding.\n- Transplant carefully as plants develop tap roots.\nSowing Directly in the Garden:\n- Direct sow in average soil in full sun to light shade 2-3 weeks before the last frost date, after danger of heavy frost. In frost-free areas, sow from fall to early spring. In very warm areas plants may benefit from afternoon shade.\n- Remove weeds and work organic matter into the top 6-8 inches of soil; then level and smooth.\n- Sow seeds evenly and cover with 1/8 inches of fine soil.\n- Firm the soil lightly and keep evenly moist.\n- Seedlings will emerge in 14-28 days.\n- Thin to 10 inches apart when seedlings are 1-2 inches tall.\nHow to Grow\n- Mulches also help retain soil moisture and maintain even soil temperatures. For herbs, an organic mulch of aged bark or shredded leaves lends a natural look to the bed and will improve the soil as it breaks down in time. Always keep mulches off a plant’s stems to prevent possible rot.\n- Keep plants well-watered during the growing season, especially during dry spells. Plants need about 1 inch of rain per week during the growing season. Use a rain gauge to check to see if you need to add water. It’s best to water with a drip or trickle system that delivers water at low pressure at the soil level. If you water with overhead sprinklers, water early in the day so the foliage has time to dry off before evening, to minimize disease problems. Keep the soil moist but not saturated.\n- Pinch plants and deadhead to encourage better growth.\n- Chervil will produce more foliage in cooler weather.\n- Plants develop a tap root and do not like to be disturbed.\n- Monitor for pests and diseases. Check with your local Cooperative Extension Service for pest controls recommended for your area.\nHarvest and Preserving Tips\n- Fresh chervil leaves are prized for their anise flavor and are a common ingredient in many fine herb mixtures. Add young leaves to fish, poultry, salads, omelets and sauces.\n- Harvest as needed throughout the season.\n- To dry, cut a bunch of stems on a sunny morning, tie them loosely and hang them in a dry, airy location out of the sun. Or, dry herbs in the oven for 2-3 hours on a cookie sheet at the lowest heat, leaving the over door open. Or, use a dehydrator following the manufacturer’s instructions. When thoroughly dry, store herbs in a tightly sealed glass jar in a dry, dark location, such as a cupboard.\n- Chervil may also be drained, dried and chopped and frozen, or frozen in water in ice cubes.\n- You can also mix chervil with butter, then refrigerate or freeze.\n- Chervil may also be preserved in a white wine vinegar.\nCommon Disease Problems\nDamping Off: This is one of the most common problems when starting plants from seed. The seedling emerges and appears healthy; then it suddenly wilts and dies for no obvious reason. Damping off is caused by a fungus that is active when there is abundant moisture and soils and air temperatures are above 68 degrees F. Typically, this indicates that the soil is too wet or contains high amounts of nitrogen fertilizer. Burpee Recommends: Keep seedlings moist but do not overwater; avoid over-fertilizing your seedlings; thin out seedlings to avoid overcrowding; make sure the plants are getting good air circulation; if you plant in containers, thoroughly wash them in soapy water and rinse in a ten per cent bleach solution after use.\nDowny Mildew: This fungus causes whitish grey patches on the undersides and eventually both sides of the leaves. Burpee Recommends: Rotate crops with plants in a different family. Avoid overhead watering. Provide adequate air circulation, do not overcrowd plants. Do not work around plants when they are wet.\nPowdery Mildew: This fungus disease occurs on the top of the leaves in humid weather conditions. The leaves appear to have a whitish or greyish surface and may curl. Burpee Recommends: Avoid powdery mildew by providing good air circulation for the plants by good spacing and pruning. Contact your Cooperative Extension Service for fungicide recommendations.\nRust: A number of fungus diseases that cause rust colored spots on foliage and stems. Burpee Recommends: Practice crop rotation. Remove infected plants. Contact your Cooperative Extension Service for recommendations.\nSclerotinia: Also called white mold, this fungus looks like a spiderweb crawling on the surface of the growing medium. It can climb onto plants and kill them in time. Burpee Recommends: Decrease humidity and increase air circulation. Avoid overcrowding seedlings. Clean seed starting supplies thoroughly before reuse.\nCommon Pest and Cultural Problems\nAphids: Greenish, red, black or peach colored sucking insects that can spread disease as they feed on the undersides of leaves. They leave a sticky residue on foliage that attracts ants. Burpee Recommends: Introduce or attract natural predators into your garden such as lady beetles and wasps who feed on aphids. You can also wash them off with a strong spray, or use an insecticidal soap.\nDeer: Plants may be eaten to the ground. Burpee Recommends: Try a deer repellent or physical barrier for young plants.\nGroundhogs: Groundhogs can eat chervil to the ground. Burpee Recommends: Physical barriers work best. Contact your Cooperative Extension Service for recommendations.\nRabbits: Chew on plant leaves. Damage is similar to deer damage but not usually as extensive. Burpee Recommends: Use a hot pepper wax spray or rabbit repellent.\nSlugs: These pests leave large holes in the foliage or eat leaves entirely. They leave a slime trail, feed at night and are mostly a problem in damp weather. Burpee Recommends: Hand pick, at night if possible. You can try attracting the slugs to traps either using cornmeal or beer. For a beer trap, dig a hole in the ground and place a large cup or bowl into the hole; use something that has steep sides so that the slugs can’t crawl back out when they’re finished. Fill the bowl about ¾ of the way full with beer, and let it sit overnight. In the morning, the bowl should be full of drowned slugs that can be dumped out for the birds to eat. For a cornmeal trap, put a tablespoon or two of cornmeal in a jar and put it on its side near the plants. Slugs are attracted to the scent but they cannot digest it and it will kill them. You can also try placing a barrier around your plants of diatomaceous earth or even coffee grounds. They cannot crawl over these.\nCan I grow chervil as a houseplant? Yes, chervil works well indoors.\nMy chervil is blooming, is it still good to use? Cut flower stalks as they appear to continue your chervil harvest as the quality of the foliage will decline when the plants go to seed. You can allow the flowers to seed themselves at the end of the season, or cut them for dried bouquets.\nWhy should I grow chervil? Fresh chervil is difficult to find in stores and loses its flavor quickly after harvest. It’s also easy to grow on your own, and attractive in the garden.\nCan I eat chervil flowers? Yes, they have a delicate anise flavor.\nHow can chervil be used as a companion plant? Chervil makes radishes hotter and crisper. It benefits lettuce and broccoli.","Copper is an organic fungicide that can treat or prevent fungal disease on your plants.\nFungal diseases can be a real problem in some areas of the country, especially where it’s cold and wet. They can kill your plants and some of them are highly contagious.\nThat’s why any tool that can help in the fungus battle is welcome, in my book. Copper, the humble metal they used to make pennies out of, is extremely effective.\nWhat Are Fungal Diseases?\nFungal diseases in plants can be among the worst problems for the home gardener and farmer. They’re caused by various fungi that travel in the air or live in the soil. Most of them are contagious.\nThese fungi enter your plants through the stomata (natural pores in the plant) or through a pruning cut or wound. They get into the plant’s cells and start to destroy the plants from the inside out.\nOne challenging thing about fungal diseases is that you can’t pick them off like you can a pest. There are no eggs or larvae to signal a problem is looming. Sometimes they can consume your plant quickly.\nThat’s why it’s so important to be able to identify different fungal diseases and have a solution ready to go.\nWe’ve provided some prevention tips below to help you avoid the diseases in the first place, but if that fails, we’ll teach you how and when to use a copper fungicide.\nMost Common Fungal Diseases\nBefore you can start attacking your disease problem, you need to know what you’re dealing with. Here are the most common fungus diseases you’ll encounter in the garden.\nThere are several types of blight that are common on tomatoes and peppers. Early blight usually hits in spring and thrives in wet weather. It starts off as brown spots with a distinctive “bull’s eye” pattern. Leaves then turn yellow and die.\nLate blight is the most severe and can decimate a crop. It comes from the organism Phytophthora, which in Latin roughly translates to mean “plant destroyer.” Yikes!\nLate blight starts as a bluish-grey blotch which then darkens, destroying leaves and fruits. The spores are carried by the wind and the disease spreads quickly.\nMany fungus diseases occur in late summer. However, downy mildew is a spring problem and can stunt the growth of new plants. Grey or white “fuzz” will occur on the undersides of leaves.\nDowny mildew is caused by the parasite organisms Peronospora or Plasmopara. It’s actually not a true fungal disease, although it’s often lumped in that category. Luckily, copper works on it as well.\nFireblight is another disease actually caused by a bacteria, in this case Erwinia amylovora, but it’s often put in the fungus category because it has similar symptoms and treatments. It can affect all plants, but it’s particularly devastating in orchards.\nThe best prevention is to buy resistant varieties. For example, I love honey crisp apples, but in my Kentucky orchard, I am prone to fireblight and have not been able to raise the susceptible variety. Instead, I have been successful with varieties such as Liberty and Winesap.\nDowny mildew and powdery mildew have similar names but are very different problems.\nA white grey appears on the plant leaves with this common fungus. This occurs when there’s high humidity. It can block the plant from photosynthesizing and causes leaves to yellow and die.\nLight cases of powdery mildew may be lessoned with an application of hydrogen peroxide. Use nine parts water to one part of hydrogen peroxide. Spray this solution on the plants one time per week.\nIf that fails, break out the copper.\nThere are several kinds of rust that often affect corn and beans. They’re spread by wind and favor high humidity but cool, mild temperatures.\nSmall, reddish-brown eruptions form on the underside of the leaves and the fruit. It looks like a bad case of freckles.\nSeptoria Leaf Spot\nThis fungus shows up as brown spots and primarily affects tomatoes. It can occur at any stage in the plant’s life.\nHow Does Copper Work On Fungus\nCopper’s a metal and, when put in a liquid state, can be sprayed or wiped on plants. The copper penetrates the leaves of the plant and can kill harmful organisms such as funguses.\nThere are many types of copper products that you can choose from.\nCopper sulfate, which is sometimes referred to as bluestone, was one of the first types of copper used as a fungicide.\nBordeaux is a combination of copper sulfate with calcium hydroxide or lime. The lime works to balance the acid in the copper sulfate. This makes it safer and helps to reduce plant damage.\nBordeaux mix has been around for a long time and works on both fungal and bacterial issues. The lime works to adhere the mixture to the plant so it lasts longer.\nHow To Use Copper Products\nCopper can be used both as a treatment for when you already have a disease and as a preventative if you’ve struggled with a disease in the past. You can also spray copper on plants that are near diseased plants as a preventative.\nMost of the time, copper fungicide is applied as a spray, but it can also be applied as a paste. This is particularly common in orchards.\nCopper is powerful, so it needs to be used with caution.\n- Always, always, always, follow the manufacturer’s recommendations. In this case, more is not better.\n- Follow the recommended temperature and weather limits during the application.\n- Mix copper exactly using the stated ratio.\n- Make sure that the product you have is right for the type of plant you want to treat.\n- Follow frequency requirements and reapply according to the direction.\nAlways buy copper in its original container and not loose at a feed store.\nI can’t tell you how important this is. Several years ago one of my farmer friends learned this lesson the hard way. After recognizing blight on her tomatoes she went to the feed store to buy some copper to combat the disease.\nThe feed store measured it out and wrote instructions for her on a paper bag. She went home and mixed the product and sprayed her half-acre patch of tomatoes.\nEvery single plant died!! She had an extension agent come and it was determined that the mixture they gave her was four times too strong.\nNot only did the plants die, but she also couldn’t use that area of the garden for some time due to copper in the soil.\nWe have talked about taking precautions to protect your plants from too much copper, but we also need to talk about protecting yourself.\nThe package will give you advice on covering your face and body to protect your self from drift while spraying or dusting your plants. Take these guidelines seriously.\nYou should wear a mask to protect your mouth and nose. Wear goggles to protect your eyes and wear a long-sleeved shirt and pants.\nWhen you are done treating your plants remove your clothing, place in the laundry and take a shower.\nTips to Prevent Fungus\nCopper’s an OMRI-listed organic fungicide. That doesn’t mean its safe for all situations. Copper’s a strong fungicide. Using it appropriately is important.\nWhen you can, you should always try to avoid getting fungus in the first place so you don’t have to turn to the big guns. Here are some tips:\nGarden management is your first go-to.\n- Identify what plants grow well in your climate and region. Those plants will be well adapted and healthier.\n- Give your plants the right amount of space so that air can flow in between them.\n- Don’t use overhead sprinklers, which can help transmit fungal disease. Use drip irrigation, or water at ground level.\n- Prune out infected leaves at the first sign of an issue.\n- Plant varieties resistant to fungus diseases common in your area. Many seed catalogs list the disease resistance of vegetables especially susceptible plants like tomatoes and peppers.\n- Fall clean up is important. Organisms such as downy mildew can overwinter in decayed plant material\n- Rotate your crops. Never place plants in the same family in the same spot for three years.\n- When you have a disease or pest, it’s vital to identify it properly. Your local extension office can help with this. You don’t want to treat for rust when you actually have aphids.\nCopper and Soil and Plant Health\nAnother reason to use copper carefully is so that the excess won’t break down in your garden soil. Copper accumulates in the soil and can reach harmful levels in one season if you use it a lot.\nYoung plants and leaves can be sensitive to copper and caution should be used when spraying them. With young plants, you can dilute the mixture to half strength.\nYoung leaves and flowers on fruit trees can be damaged from copper sprays. Purdue University recommends using a copper-based solution on trees at bud break to curtail fireblight.\nOne of the formulas that Purdue University Extension recommends is the 4-4-50 formulation. The formula (like an NPK reading) tells you what’s in the mixture.\nThe first number – in this case, 4 – is the number of pounds of copper sulfate. The second number is for pounds of lime. The number 50 tells you the gallons of water. This is to help keep the mixture at a safe balance for you and your plants.\nFind the Right Balance\nIt’s always a balancing act when trying to decide whether to use a strong fungicide such as copper. Practice good management and prevention to reduce your likelihood of having fungal diseases hit your garden.\nWhen copper is needed to control a problem, make sure to follow the manufacturer’s directions. This will make your application safe and successful.\nHave you ever had to use copper to tackle a fungus issue? How did it work? Let us know in the comments."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:cfb24e1c-4081-4f7a-a7d8-5393db98bbb4>","<urn:uuid:5bf37594-729a-47e2-b95e-b90a25c9665e>"],"error":null}
{"question":"How do WordPress and Semantic MediaWiki differ in their approach to content organization?","answer":"WordPress and Semantic MediaWiki have different approaches to content organization. WordPress is primarily a content management system that evolved from a blogging platform, using plugins, widgets, and themes to manage content, with PHP and MySQL as its technical foundation. On the other hand, Semantic MediaWiki extends traditional wiki categorization by adding semantic annotations, allowing users to explicitly define the meaning of links and text through properties (like 'capital of' or 'population'), making information more structured and machine-readable. While both systems help organize content, SMW focuses more on creating meaningful relationships between data through semantic annotations, whereas WordPress relies on a more traditional CMS structure.","context":["- Introduction to Semantic MediaWiki\n- Browsing interfaces\n- Semantic search\n- Using the API modules\n- Semantic Web\nThis section explains how to edit pages in Semantic MediaWiki. As explained in the introduction, SMW introduces special markup elements which allow editors to provide «hints» to computer programs on how to interpret some piece of information given in the wiki. Such hints are called semantic annotations and they are created with a special markup of SMW. Besides this, editing in SMW is just the same as in MediaWiki. Users who are not familiar with basic editing yet, should first read about how to edit pages in MediaWiki. Editors may or may not provide annotations on wiki pages as they like – it is an added feature that is completely voluntary.\nOverview of SMW editing features\nAnnotations in Semantic MediaWiki can be viewed as an extension of the existing system of categories in MediaWiki. Categories are a means to classify articles according to certain criteria. For example, by adding [[Category:Cities]] to an article, the page is tagged as describing a city. MediaWiki can use this information to generate a list of all cities in a wiki, and thus help users to browse the information.\nSemantic MediaWiki provides a further means of structuring the wiki. Wiki pages have links and text values in them, but only a human reader knows what the link or text represents. For example, «is the capital of Germany with a population of 3,396,990» means something very different from «plays football for Germany and earns 3,396,990 dollars a year». SMW allows you to annotate any link or text on the page to describe the meaning of the hyperlink or text. This turns links and text into explicit properties of an article. The property capital of is different from on national football team of, just as the property population is different from annual income.\nThis addition enables users to go beyond mere categorisation of articles. Usage and possible problems with using these features are similar to the existing category system. Since categories and properties merely emphasize a particular part of an article's content, they are often called (semantic) annotations. Information that was provided in an article anyway, e.g. that Berlin is the capital of Germany, is now provided in a formal way accessible to software tools.\nBesides annotations, SMW also allows editors to embed semantic queries into articles. Thereby, readers of the wiki can view ready-made query results without having to learn the SMW query language. This feature is explained in the section on inline queries.\nCategories are an editing feature of MediaWiki, and the main reference for their use is the MediaWiki documentation on categories. Categories are used as universal \"tags\" for articles, describing that the article belongs to a certain group of articles. To add an article to a category Example category, just write\nanywhere in the article. The name of the category (here, \"Example category\") is arbitrary but, of course, you should try to use categories that already exist instead of creating new ones. Every page can be assigned to a category by writing [[Category:Example category]] anywhere in the source text of the page. The category's article can be empty, but it is strongly recommended to add a description that explains which articles should go into the category.\nOn wikis like Wikipedia, categories are used for many different purposes. For example, the Cities category contains both individual cities, related subcategories like \"City nicknames\" and abstract concepts like \"Digital city\".\nIn Semantic MediaWiki-using sites, categories tend to be used much more sparingly, since inline queries make many categories superfluous. For example, a subcategory like Large cities could be replaced by a query for articles with Category:Cities with an area larger than 10 km², or a population larger than 1,000,000. In addition, categories tend to be used more exactly: a page like \"Digital city\" might end up in a category like \"City-related terms\" instead of \"Cities\", so that it wouldn't show up in a query on the \"Cities\" category.\nHelp:Editing en 1.0","Every one that wishes to start a website or a blog needs a way to manage content. Content Management System (CMS) is found and created to fulfill this task for you. Content Management Systems is designed to simplify the publication of web content to websites and blogs, allowing content creators to submit content without requiring technical knowledge of HTML or the uploading of files. Several content management systems exist both in the Open Source and commercial domains. In this article I collected 17 free to use content management systems that will make your content creation process an easier task.\nWordPress started as just a blogging system, but has evolved to be used as full content management system and so much more through the thousands of plugins, widgets, and themes available to suit your needs. WordPress is also free and open source CMS and it is considered one of the most used content management systems out there and I might dare to say it is ruling out there. It uses PHP as a server side language and MySQL as a database.\nmojoPortal is a free and open source content management system built using the Microsoft ASP.NET and supports various databases like MySQL, MS SQL and PostgreSQL. It comes with a lot of built in features such as: Image Gallery, Event Calendar, Polls, Blogs, Forums and many more.\nDrupal is a free and open source content management system that allows an individual, a community of users, or an enterprise to easily publish, manage and organize a wide variety of content on a website.\nJoomla is a free content management system, which enables you to build web sites and online applications. Many aspects, including its ease-of-use and extensibility, have made Joomla one of the most popular content management systems available. Best of all, Joomla is an open source solution that is freely available to everyone. It has many built in features but if that’s not enough you can take a look at the 4000+ extensions from the community.\nPligg is a free and open source content management system. Pligg CMS provides social networking software that encourages visitors to register on your website so that they can submit content and connect with other users. You can create websites where stories are created and voted on by members, not website editors. Use Pligg content management system to start your own social networking community in minutes.\nSilverStripe is an open source and free content management system. Besides their feature rich CMS they developed Sapphire which is an object-oriented PHP5 web framework designed to let you either build standalone applications or extend your SilverStripe CMS-powered site.\nPlone is a powerful, flexible Content Management solution that is easy to install, use and extend. Plone is created for non-technical users to create and maintain information using only a web browser. Perfect for web sites or intranets, Plone offers superior security without sacrificing extensibility or ease of use for non-technical users.\nBlogEngine.NET is an open source and free .NET blogging engine that was created to offer a better blog platform. A blog platform with less complexity, easy customization, and one that takes advantage of the latest .NET features. BlogEngine.NET was designed using the current .NET framework and focused on simplicity, ease of extendability, and innovative features. despite it packs a lot of built in features along with extensions available by the community it still lacks attention for the little details that is found in other content management system.\nSymphony is XSLT-powered open source content management system. Symphony leverages open standards like XML and XSLT, and good old XHTML and CSS. Even the admin interface employs the widely-used jQuery library, so extension developers don’t have to learn a whole new framework when extending the back end. Symphony is comprised of discrete, fully configurable components. Its data, logic, and templating layers are all independent, meaning that whatever you implement can be modified, added, or removed with minimum effort.\nsNews is a completely free, standards compliant, PHP and MySQL driven Content Management System. sNews is extremely lightweight, simple and customizable. It’s easy to install, and use via a simple web interface. sNews consists of only one core engine file, one independent template file and its accompanying CSS stylesheet file, plus an .htaccess file that makes all URLs search engine friendly.\nCushyCMS is a Content Management Systems that is truly simple. It’s free for unlimited users, unlimited changes, unlimited pages and unlimited sites. It’s built from the ground up with ease of use in mind – for both content editors and designers. It’s such a simple CMS that it takes less than 3 minutes for a web designer to implement. No PHP or ASP required for this CMS. If you can add CSS classes to HTML tags then you can implement CushyCMS. It’s also a hosted CMS, so no installation or maintenance is needed either.\n12. Frog CMS\nFrog CMS simplifies content management by offering an elegant user interface, flexible templating per page, simple user management and permissions, as well as the tools necessary for file management. Born as phpRadiant in January 2007, Frog CMS is a PHP version of Radiant CMS. Frog CMS requires PHP, a MySQL database or SQLite.\nRadiant is an open source content management system designed for small teams. It is built using Ruby on Rails and using the SQLite as a database.\n15. CMS from Scratch\nCMS from Scratch is a quick, easy, open source and FREE solution that lets web designers give their customers a web site they can edit themselves. It is now also an open source project, so you can use the source PHP scripts for FREE.\nMODx helps even regular individuals manage content on their websites simply, quickly and intuitively. For the geek-elite, MODx is an Open Source PHP web application framework with a capable built-in Content Management System.\nTYPO3 is a free Open Source content management system for enterprise purposes on the web and in intranets. It offers full flexibility and extensibility while featuring an accomplished set of ready-made interfaces, functions and modules.\nAhmad Hania is a professional designer and developer with a B.Sc. in Computer Engineering. He is interested in freelancing and blogging. He is a cofounder of Wateen Technologies an application development and hosting company. He loves video games and sports. Read more articles by Ahmad Hania at his blog Follow Ahmad Hania at Twitter and Facebook."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:4d78718f-687b-4990-ac21-d5f0b1e0e9bf>","<urn:uuid:75e09e9f-df24-4953-a19f-709df66e9b10>"],"error":null}
{"question":"Which method more effective for agriculture - lagoon water irrigation or biochar soil amendment?","answer":"Both methods address different agricultural challenges. Lagoon water irrigation provides a system for managing and applying nutrients (N and P) from animal waste, with improvements in estimation methods allowing for more precise seasonal nutrient tracking. Biochar soil amendments, particularly those made from pine chips and pine chip-poultry litter blends, effectively improve water movement through compacted soil layers, though pure poultry litter biochar was less effective. The choice depends on the specific agricultural need - nutrient management versus improving soil water infiltration.","context":["Submitted to: Irrigation Science\nPublication Type: Peer Reviewed Journal\nPublication Acceptance Date: 2/17/2016\nPublication Date: 3/21/2016\nPublication URL: http://handle.nal.usda.gov/10113/62581\nCitation: Mclaughlin, M.R., Brooks, J.P., Adeli, A., Jenkins, J.N. 2016. Improving estimates of N and P loads in irrigation water from swine manure lagoons. Irrigation Science. 34(3):245-260.\nInterpretive Summary: Nutrient management plans for confined animal feeding operations must track nitrogen (N) and phosphorus (P) loads from land-applied manure, including irrigation water from lagoons. By state regulation in Mississippi, nutrient records from manure lagoon irrigation water must be based on at least one analysis of the lagoon water annually; however, the present study showed that nutrient estimates based on a single annual analysis may over- or under-estimate N and P. The research reported here also confirmed that N and P levels in lagoon water, and the N:P ratio, varied significantly, but predictably, during the year, with seasonal changes in lagoon water temperature thought to be the driving force. The report showed that the predictable variation could be described by a simple mathematical model. The model was then applied to develop easy-to-use, calendar-based, lagoon-specific tables of N and P levels for lagoon water for any given day of the irrigation season. The tables enabled the land nutrient manager to more accurately track land-applied N and P loads from irrigation events throughout the irrigation season. This report describes the development and use of these improved methods for more precisely estimating N and P loads in irrigation water from a swine manure lagoon in Mississippi and also uses published data from earlier studies to examine the feasibility of applying these methods to swine manure lagoons in North Carolina.\nTechnical Abstract: The implementation of nutrient management plans (NMPs) for confined animal feeding operations (CAFOs) requires recording N and P loads from land-applied manure, including nutrients applied in irrigation water from manure treatment lagoons. By regulation, lagoon irrigation water nutrient records in Mississippi must be based on at least one lagoon water nutrient analysis annually. Research in Mississippi has shown that N and P levels in lagoon water, and the N:P ratio, vary significantly through the year. Nutrient estimates based on one annual analysis do not account for this variability and may overestimate or underestimate N and P loads. The present study reports an improved method to more precisely estimate N and P loads in irrigation water from swine manure lagoons. The method is based on predicable annual cycles of N and P levels in lagoon water and employs simple curve-fitting of lagoon-specific formulas derived by analyses of historical data. Similarity of curves from analyses of Mississippi lagoons and other lagoon studies suggests that the method can be applied using the often limited nutrient data for a lagoon to more precisely estimate seasonal shifts of N and P and to improve the precision of estimates for N and P in irrigation water. Although the present study focused on swine manure lagoons in the southern US, recognition that the annual N cycle in lagoon water is temperature driven, suggests that additional research incorporating temperature into future models could extend these models to other types of waste treatment lagoons and climates.","|Watts, Donald - Don|\n|CANTRELL, KERI - Former ARS Employee|\n|JOHNSON, MARK - Environmental Protection Agency (EPA)|\nSubmitted to: Chemosphere\nPublication Type: Peer Reviewed Journal\nPublication Acceptance Date: 6/20/2015\nPublication Date: 1/15/2016\nCitation: Novak, J.M., Sigua, G.C., Watts, D.W., Cantrell, K., Shumaker, P.D., Szogi, A.A., Johnson, M., Spokas, K.A. 2016. Biochars impact on water infiltration and water quality through a compacted subsoil layer. Chemosphere. 142:160-167.\nInterpretive Summary: Many soils in the Southeastern United States Coastal Plain region have compacted subsoil layers which limits root penetration and water movement. During drought periods, shallow roots cannot find sufficient water for transpiration which impacts crop productivity. Deep tillage practices are often used to fracture the compact subsoil layer, however, this requires large farm machinery which also consumes copious amounts of fuel. Studies have investigated improving water movement characteristics of compacted horizons with the additions of composts, peat/manures, and crop residues. Unfortunately, these amendments do not last in the hot and humid soils of the Southeastern United States. We investigated if biochar produced from pine chips, poultry litter, and blends added to columns containing a compact sandy soil layer could improve physical properties and allow faster water movement rates. This was determined by applying water to the top of the soils and measuring the amount of water moved as a function of time. Except for the biochar produced from poultry litter, all other applied biochars resulted in faster rates of water movement compared to the control. We concluded that biochars produced from pine chips and blends of pine chip and poultry litter can be a suitable amendments to improve water movement through sandy compacted soil layers.\nTechnical Abstract: Soils in the Southeastern United States Coastal Plain region frequently have a compacted subsoil layer, which is a barrier for water movement. Four different biochars were evaluated to increase water movement through a compacted horizon from a Norfolk soil (fine-loamy, kaolinitic, thermic, Typic Kandiudult). In addition, we also evaluated biochar effects on water quality. Biochars were produced by pyrolysis at 500° Celcius from pine chips, poultry litter feedstocks, and as blends (50:50 and 80:20) of pine chip: poultry litter. Prior to pyrolysis, the feedstocks were pelletized and sieved to > 2-millimeter pellets. Each biochar was mixed with the subsoil at 20 grams per kilogram of soil and the mixture was placed in columns. The columns were leached four times with water over 128 days of incubation. Except for the biochar produced from poultry litter, all other applied biochars resulted in significant water movement increases compared to the control. However, water movement rates in each treatment were influenced by additional water leaching. Leachates were enriched in anions and cations after addition of poultry litter biochar, however, their concentrations declined in pine chip blended biochar treatments and after multiple leaching. Adding biochars (except 100% poultry litter biochar) to a compacted subsoil layer can initially improve water movement, but, additional leaching revealed that the effect remained only for the 50:50 pine chip: poultry litter blended biochar while it declined in other biochar treatments."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:4b4cc50e-5df6-4edf-88bc-2ab786d4eb2f>","<urn:uuid:ee281197-fac7-402a-bf2d-4318bd150562>"],"error":null}
{"question":"How do fractals manifest in natural phenomena, and what mathematical properties make them unique in theoretical physics?","answer":"Fractals appear extensively in nature, showing up in phenomena like snowflakes, river networks, flowers, trees, lightning strikes, and blood vessels. These natural fractals exhibit self-referential patterns that repeat at smaller scales, though they can only replicate through several layers. In theoretical physics, fractals have remarkable properties - they are infinitely deep yet infinitely ghostly, meaning you can keep focusing on them endlessly, but their total drawn area remains zero as they're just infinite lines. In physics, fractal structures are particularly significant in quark-gluon plasma (QGP), where they explain particle behavior during high-energy collisions. When QGP disintegrates into particle jets, it shows a self-similar cascade pattern typical of fractals. This fractal structure is responsible for particles following Tsallis statistics rather than traditional Boltzmann statistics in high-energy collisions.","context":["Quark-gluon plasma (QGP) is a state of matter existing at extreme temperatures and densities, such as those that occur in collisions of hadrons (protons, neutrons, and mesons). Under so-called “normal” conditions, quarks and gluons are always confined in the structures that constitute hadrons, but when hadrons are accelerated to relativistic velocities and made to collide with each other, as they are in the experiments performed at the Large Hadron Collider (LHC) operated by the European Organization for Nuclear Research (CERN), the confinement is interrupted and the quarks and gluons scatter, forming a plasma. The phenomenon lasts only a tiny fraction of a second, but observation of it has produced important discoveries about the nature of material reality.\nOne of the discoveries, evidence for which is steadily accumulating, is that quark-gluon plasma has a fractal structure. When it disintegrates into a stream of particles propagating in various directions, the behavior of the particles in the jets is similar to that of the quarks and gluons in the plasma. Moreover, it decays in a cascade of reactions with a pattern of self-similarity over many scales that is typical of fractals.\nA new study, published in The European Physical Journal Plus, describes a mathematical tool with which to understand more about the phenomenon. The authors focus on a technical aspect of the solution to the Klein-Gordon equation for the dynamics of bosons, relativistic particles with zero spin that share the same quantum states and are therefore indistinguishable. In a Bose-Einstein condensate (BEC), moreover, particles behave collectively as if they were a single particle. BEC research has yielded new atomic and optical physics. Potential applications include more accurate atomic clocks and enhanced techniques to make integrated circuits.\n“Fractal theory explains BEC formation,” said Airton Deppman, a professor at the University of São Paulo’s Institute of Physics (IF-USP) in Brazil, and principal investigator for the study, which was supported by FAPESP.\n“The study was part of a broader research program that had already resulted in 2020 in the article ‘Fractals, nonextensive statistics, and QCD’ published in Physical Review D, demonstrating that Yang-Mills fields have fractal structures and explaining some phenomena seen in high-energy collisions where quark-gluon plasma is formed,” Deppman added.\nFormulated in the 1950s by Chinese physicist Chen-Ning Yang (joint winner of the 1957 Nobel Prize in Physics) and United States physicist Robert Mills, Yang-Mills theory is highly important to the standard model of particle physics because it describes three of the four fundamental forces in the Universe: the electromagnetic, weak, and strong forces (the fourth is gravitational interaction).\n“In high-energy collisions, the main outcome is particle momentum distributions, which follow Tsallis statistics instead of traditional Boltzmann statistics. We show that the fractal structure is responsible for this. It leads to Tsallis rather than Boltzmann statistics,” Deppman continued. Constantino Tsallis was born in Greece in 1943 and became a naturalized Brazilian in 1984. He is a theoretical physicist primarily interested in statistical mechanics. Ludwig Boltzmann (1844–1906) was an Austrian physicist and mathematician who made important advances in statistical mechanics, electromagnetism, and thermodynamics.\n“With this fractal approach, we were able to determine the Tsallis entropy index q, which is calculated using a simple formula relating it to the key parameters of Yang-Mills,” Deppman said. “In the case of quantum chromodynamics [QCD, the theory of the strong interaction between quarks mediated by gluons], these parameters are the number of particle colors and flavors. With these parameters, we found q = 8/7, compatible with experimental results where q = 1.14,” he said.\nColors in QCD refer not to the usual concept but to color charges, relating to strong interactions between quarks. There are three possibilities, symbolized by red, green, and blue. Quarks also have electric charges, which relate to electromagnetic interactions, but color charges are a different phenomenon. Flavors describe the six types of quark: up, down, charm, strange, top, and bottom. This picturesque nomenclature reflects the sense of humor of Murray Gell-Mann (1929–2019), an American physicist who won the 1969 Nobel Prize in Physics for his work on the theory of elementary particles, and later scientists who also contributed to QCD.\n“An interesting aspect of the evolution of our knowledge is that before high-energy collisions were experimentally performed in large particle colliders, and even before the existence of quarks was proposed, Rolf Hagedorn, a German physicist who worked at CERN, set out to predict the production of particles in these collisions,” Deppman said. “Solely on the basis of research into cosmic rays, he formulated the concept of fireballs to explain the cascade of particles created in high-energy collisions. With this hypothesis, he predicted the threshold temperature corresponding to the phase transition between confined and deconfined regimes. The key element in his theory is the self-similarity of fireballs. Hagedorn didn’t use the term ‘fractal’ because the concept didn’t exist yet, but after the term was coined by Mandelbrot, we saw that fireballs were fractals.” Benoît Mandelbrot (1924–2010) was a Polish-born French-American mathematician.\nAccording to Deppman, Hagedorn’s theory can be generalized by including Tsallis statistics. Indeed, Deppman did so in an article published in Physica A in 2012.\n“With this generalization, we obtain a self-consistent thermodynamic theory that predicts the critical temperature for the transition to quark-gluon plasma, and also supplies a formula for the hadron mass spectrum, from lightest to heaviest,” he said. “Strong evidence exists for a conceptual continuity in the description of hadronic systems from quark-gluon plasma to hadrons, and for the validity of the fractal structure of QCD in both regimes.”\nDeppman questions if fractal structures could also be present in electromagnetism. This would explain why so many natural phenomena, from lightning to snowflakes, have fractal structures, as they are all governed by electromagnetic forces. It might also explain why Tsallis statistics are present in so many phenomena. “Tsallis statistics have been used to describe scale transformation invariance, a key ingredient of fractals,” he said.\nMight fractal theory be extended to gravitational phenomena? “Gravitation lies outside the scope of our approach, since it doesn’t come into Yang-Mills theory, but there’s nothing to stop us speculating whether fractals express an underlying pattern in all material reality,” he said.\n- This press release was originally published on the Fundação de Amparo à Pesquisa do Estado de São Paulo website","Mathematics is visible everywhere in nature, even where we are not expecting it. It can help explain the way galaxies spiral, a seashell curves, patterns replicate, and rivers bend.\nEven subjective emotions, like what we find beautiful, can have mathematic explanations.\n“Maths is not only seen as beautiful – beauty is also mathematical,” says Dr Thomas Britz, a lecturer in UNSW Science’s School of Mathematics & Statistics. “The two are intertwined.”\nDr Britz works in combinatorics, a field focused on complex counting and puzzle solving. While combinatorics sits within pure mathematics, Dr Britz has always been drawn to the philosophical questions about mathematics.\nHe also finds beauty in the mathematical process.\n“From a personal point of view, maths is just really fun to do. I’ve loved it ever since I was a little kid.\n“Sometimes, the beauty and enjoyment of maths is in the concepts, or in the results, or in the explanations. Other times, it’s the thought processes that make your mind turn in nice ways, the emotions that you get, or just working in the flow – like getting lost in a good book.”\nHere, Dr Britz shares some of his favourite connections between maths and beauty.\n1. Symmetry – but with a touch of surprise\nIn 2018, Dr Britz gave a TEDx talk on the Mathematics of Emotion, where he used recent studies on maths and emotions to touch on how maths might help explain emotions, like beauty.\n“Our brains reward us when we recognise patterns, whether this is seeing symmetry, organising parts of a whole, or puzzle-solving,” he says.\n“When we spot something deviating from a pattern – when there’s a touch of the unexpected – our brains reward us once again. We feel delight and excitement.”\nFor example, humans perceive symmetrical faces as beautiful. However, a feature that breaks up the symmetry in a small, interesting or surprising way – such as a beauty spot – adds to the beauty.\n“This same idea can be seen in music,” says Dr Britz. “Patterned and ordered sounds with a touch of the unexpected can have added personality, charm and depth.”\nMany mathematical concepts exhibit a similar harmony between pattern and surprise, elegance and chaos, truth and mystery.\n“The interwovenness of maths and beauty is itself beautiful to me,” says Dr Britz.\n2. Fractals: infinite and ghostly\nFractals are self-referential patterns that repeat themselves, to some degree, on smaller scales. The closer you look, the more repetitions you will see – like the fronds and leaves of a fern.\n“These repeating patterns are everywhere in nature,” says Dr Britz. “In snowflakes, river networks, flowers, trees, lightning strikes – even in our blood vessels.”\nFractals in nature can often only replicate by several layers, but theoretic fractals can be infinite. Many computer-generated simulations have been created as models of infinite fractals.\n“You can keep focusing on a fractal, but you’ll never get to the end of it,” says Dr Britz.\n“Fractals are infinitely deep. They are also infinitely ghostly.\n“You might have a whole page full of fractals, but the total area that you’ve drawn is still zero, because it’s just a bunch of infinite lines.”\n3. Pi: an unknowable truth\nPi (or ‘π’) is a number often first learnt in high school geometry. In simplest terms, it is a number slightly more than 3.\nBut Pi is a lot more than this.\n“When you look into other aspects of nature, you will suddenly find Pi everywhere,” says Dr Britz. “Not only is it linked to every circle, but Pi sometimes pops up in formulas that have nothing to do with circles, like in probability and calculus.”\nDespite being the most famous number (International Pi Day is held annually on 14 March, 3.14 in American dating), there is a lot of mystery around it.\n“We know a lot about Pi, but we really don’t know anything about Pi,” says Dr Britz.\n“There’s a beauty about it – a beautiful dichotomy or tension.”\nPi is infinite and, by definition, unknowable. No pattern has yet been identified in its decimal points. It’s understood that any combination of numbers, like your phone number or birthday, will appear in Pi somewhere (you can search this via an online lookup tool of the first 200 million digits).\nWe currently know 50 trillion digits of Pi, a record broken earlier this year. But, as we cannot calculate the exact value of Pi, we can never completely calculate the circumference or area of a circle – although we can get close.\n“What’s going on here?” says Dr Britz. “What is it about this strange number that somehow ties all the circles of the world together?\n“There’s some underlying truth to Pi, but we don’t understand it. This mystique makes it all the more beautiful.”\n4. A golden and ancient ratio\nThe Golden Ratio (or ‘ϕ’) is perhaps the most popular mathematical theorem for beauty. It’s considered the most aesthetically pleasing way to proportion an object.\nThe ratio can be shortened, roughly, to 1.618. When presented geometrically, the ratio creates the Golden Rectangle or the Golden Spiral.\n“Throughout history, the ratio was treated as a benchmark for the ideal form, whether in architecture, artwork, or the human body,” says Dr Britz. “It was called the ‘Divine Proportion’.\n5. A paradox closer to magic\nA famous geometrical theorem called the Banach-Tarski paradox says that if you have a ball in 3D space and split it into a few specific pieces, there is a way to reassemble the parts so that you create two balls.\n“This is already interesting, but it gets even weirder,” says Dr Britz.\n“When the two new balls are created, they will both be the same size as the first ball.”\nMathematically speaking, this theorem works – it is possible to reassemble the pieces in a way that doubles the balls.\n“You can’t do this in real life,” says Dr Britz. “But you can do it mathematically.\nThat’s sort of magic. That is magic.\nFractals, the Banach-Tarski paradox and Pi are just the surface of the mathematical concepts he finds beauty in.\n“To experience many beautiful parts of maths, you need a lot of background knowledge,” says Dr Britz. “You need a lot of basic – and often very boring – training. It’s a bit like doing a million push ups before playing a sport.\n“But it is worth it. I hope that more people get to the fun bit of maths. There is so much more beauty to uncover.”"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:aa4377c5-1817-487f-835b-55be0b734c74>","<urn:uuid:108e2e0b-4188-46aa-8039-eb8c6d94f7e8>"],"error":null}
{"question":"Compare the historical transitions in names between ISG Tecumseh Redevelopment Inc. facility and Edwards Air Force Base - how did their names evolve over time?","answer":"The Bethlehem Steel facility underwent ownership and name changes from Bethlehem Steel Corporation to ISG Tecumseh Redevelopment Inc. following bankruptcy in 2003. Meanwhile, Edwards Air Force Base went through several name changes: it began as Muroc Bomb and Gunnery Range in 1933, became Muroc Army Air Field in July 1942, then Muroc Air Force Base in September 1947, and finally was renamed Edwards Air Force Base in December 1949 in honor of Glen Edwards, who died in a crash there in 1948.","context":["ISG Tecumseh Redevelopment Inc.\n(Formerly: Bethlehem Steel Corp.)\nOn This Page\nEPA Project Manager\nMs. Linda Matyskiela\nU.S. Environmental Protection Agency - Region III\n1650 Arch Street\nMail code: 3LC30\nPhiladelphia, PA 19103-2029\nPhone: (215) 814-3420\nRCRA Corrective Action activities at the Bethlehem Steel Works facility are being conducted as a joint lead by EPA and PADEP’s Land Recycling Program (Act 2).Environmental Investigation and clean-up are proceeding in concert with the redevelopment and sales of individual parcels of the property. Investigation and remediation, approved by both EPA and PADEP occurs before re-sale. As each of the parcels is sold, an environmental covenant is signed by the new and former owners, detailing the type of land-use and groundwater use controls appropriate for the parcel. This covenant runs with the land and binds each new owner to its restrictions.\nLocated on the south side of Bethlehem, PA, Bethlehem Steel’s Bethlehem Plant has been in operation since the 1880s. BSC closed its last operations in March, 1998. The site, part of which borders then Lehigh River, consists of approximately 1650 acres, on which are a coke production plant, steel iron making operations, finishing and forging operations, and a chemical plant. In May 2003, Bethlehem Steel Corporation went into bankruptcy and this Bethlehem Steel site was sold to an International Steel Group subsidiary, Tecumseh Redevelopment Corp. In May 2004, the Coke Works, Saucon, East Lehigh, and Greenway parcels were sold to Lehigh Valley Industrial Park, Inc, a local redevelopment firm and owners of several industrial parks. In 2004, Tecumseh Redevelopment sold Bethlehem Works to BethWorks Now. Tecumseh continued to own the 450 -Acre area. In 2004, a Dutch steelmaker, Mittal Steel, purchased ISG, including Tecumseh and the 450-Acre area. In December 2007, Majestic Realty Co. and Bethlehem Commerce Center Corp. purchased the 450-Acre area for warehousing.\nEPA, PADEP and Bethlehem Steel Corporation-Bethlehem Plant (BSC) formed a team in 1998 to proceed with the clean-up of the facility. The facility is subject to RCRA corrective action, however, BSC’s redevelopment plans hinged on obtaining a release from liability which is available for remediation performed under the Pennsylvania Act 2 Land Recycling Program. The redevelopment plan is designed to revitalize South Bethlehem, which has been in an economic slump since BSC started downsizing in the 1970's.\n- Some of the site’s key documents of interest are accessible below:\n- Environmental Indicator Determination - Human Exposures [PDF, 6 pages, 25 KB, About PDF]\n- Corrective Action Statement of Basis[PDF, 29 pages, 17.25 MB, About PDF]\n- PA DEP Environmental Covenant - Deed Restrictions Artsquest Parcel [PDF, 20 pages, 866 KB, About PDF]\n- PA DEP Environmental Covenant - Deed Restrictions Majestic Parcel [PDF, 13 pages, 922 KB, About PDF]\n- PA DEP Environmental Covenant - Deed Restrictions Lots 6 & 7 [PDF, 10 pages, 803 KB, About PDF]\n- PA DEP Environmental Covenant - Deed Restrictions Lot 29 [PDF, 9 pages, 638 KB, About PDF]\n- PA DEP Environmental Covenant - Deed Restrictions -LVIP -1235 Easton Rd[PDF, 20 pages, 880 KB, About PDF]\n- PA DEP Environmental Covenant - Deed Restrictions -LVIP -1245 Easton Rd[PDF, 11 pages, 626 KB, About PDF]\n- Documents and reports regarding this facility also can be reviewed in person at these locations:\nU.S. EPA Region III\nLand & Chemicals Division\n1650 Arch Street-11th Floor\nPhiladelphia, PA 19103\nCall for an appointment.\n- Submit a FOIA Request\nGet instructions on how to submit a FOIA request. Additional fee for requests over 100 pages.\n|ISG Tecumseh Redevelopment Inc. -Former Bethlehem Steel||ISG Tecumseh -Former Bethlehem Steel Geospatial PDF Map [PDF, 1211 KB, 1 page, About PDF]||ISG Tecumseh -Former Bethlehem Steel Geospatial Bethworks PDF Map [PDF, 1302 KB, 1 page, About PDF]|\nClick on a thumbnail to enlarge the photo or GeoSpatial PDF Map )\n- The EPA is dedicated to providing you with timely and accurate information about our work at this site. If you have any questions or concerns, please contact EPA Project Manager: Ms. Linda Matyskiela (215) 814-3420.","In 1948, an experimental bomber crashed in the Mojave Desert, killing all five crew members. One was 30-year-old Glen Edwards.\nThus ended a brief but significant Air Force career, and Muroc Air Force Base, Calif., was rechristened Edwards Air Force Base in his honor.\nThat honoree, Glen Walter Edwards, was born in Medicine Hat, Canada, in 1918, to American parents. At 13, he and the family left for California. Bright and highly motivated, Glen took a degree in chemical engineering from University of California, Berkeley in June 1941.\nHis was not to be the life of a civilian engineer, however.\nPearl Harbor was still six months in the future, but young Edwards quickly joined the Army Air Forces and took flight training at Luke Field, Ariz., where he was commissioned in early 1942.\nEdwards won early fame as an A-20 light bomber commander in North Africa, where he led his squadron on risky, low-level attacks on tanks, convoys, and troops of Rommel’s Afrika Korps.\nFifty combat missions later, he returned to the States in 1943 bearing four Distinguished Flying Crosses and six Air Medals.\nUnbeknownst to Edwards, though, his combat career was over. An equally important one—military test pilot—was just beginning.\nEdwards was sent to Flight Test Division at Wright Field, Ohio, and was among the first to graduate from Flight Performance School (now Air Force Test Pilot School). He spent much of his time at Muroc Army Air Field, Calif., on the Mojave, flying highly experimental aircraft such as the XB-42 and XB-46 prototypes.\nIn late 1945, he flew the XB-42 cross-country at an average speed of 433.6 mph, setting a new transcontinental record.\nThe highly regarded Edwards nearly became project pilot for the Bell X-1 bid to exceed the sound barrier. When that job went to Capt. Chuck Yeager, Edwards instead was sent to Princeton, where he took cutting-edge courses in aerodynamics and control.\nArmed with that knowledge, he joined a vanguard of select USAF pilot-engineers blazing the way to advanced air and space flight.\nEdwards soon began testing experimental “flying wings,” aircraft, devoid of conventional fuselage and tail. These included Northrop’s piston-engine-driven XB-35 and YB-35.\nIn May 1948, Edwards joined a Muroc team evaluating the Northrop YB-49, a jet-propelled flying wing. He was not impressed, finding it “quite uncontrollable at times.”\nOn June 5, 1948, Edwards and four others took off in the YB-49. Maj. Daniel Forbes was pilot, Edwards was co-pilot. For reasons still unknown, the futuristic jet broke apart in the sky and crashed to Earth not far from Muroc. There were no survivors.\nThe California base has been the scene of many advances in flight. On Oct. 14, 1947, Yeager broke the sound barrier in the X-1 aircraft. The landing of the first Space Shuttle took place there in 1981. Virtually all of America’s early jets—both Air Force and Navy—were tested there, as have all advanced aircraft in years since. It is home of Hq. Air Force Test Center, U.S. Air Force Test Pilot School, and 412th Test Wing.\nGlen Walter Edwards\n- Born: March 5, 1918, Medicine Hat, Alberta, Canada\n- Died: June 5, 1948, near Mojave, Calif.\n- Citizenship: U.S. and Canada (dual)\n- Colleges: UC Berkeley, Princeton University\n- Occupation: Engineer, U.S. military officer\n- Services: U.S. Army Air Forces, U.S. Air Force\n- Main Eras: World War II, Postwar\n- Years Active: 1941-48\n- Combat: North Africa Theater 1942-43\n- Final Grade: Captain\n- Honors: Distinguished Flying Cross (4); Air Medal (6); Aerospace Walk of Honor (posthumously inducted)\n- Resting Place: Lincoln Cemetery, Calif.\nEdwards Air Force Base\n- State: California\n- Nearest City: Rosamond\n- Area: 407.3 sq mi / 301,000 acres\n- Status: Open, operational\n- Opened as Muroc Bomb and Gunnery Range: September 1933\n- Renamed Muroc Army Air Field: July 1942\n- Renamed Muroc Air Force Base: September 1947\n- Renamed Edwards Air Force Base: December 1949\n- Current owner: Air Force Materiel Command\n- Former owners: Army IX Corps Area, GHQAF, AAC Southwest Air District, AAC Fourth Air Force, AAF Materiel and Services Command, AAF Technical Service Command, AAF Continental Air Forces, AAF Air Technical Service Command, USAF Air Materiel Command, USAF Air Research and Development Command, USAF Air Force Systems Command"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:e6223633-dbe7-4333-b601-15cbec5409c7>","<urn:uuid:2ae8066a-fddd-4fca-9b9e-c21a17eb2bb0>"],"error":null}
{"question":"How are traditional ethnographic methods transforming modern research practices, and what effect does this have on environmental conservation efforts?","answer":"Traditional ethnographic methods like fieldwork, participant observation, and dialogue are being transformed and applied to new contexts beyond anthropology. In environmental conservation specifically, these methods are being used to integrate Indigenous knowledge with scientific approaches. This integration is proving valuable, as evidenced by research showing that Indigenous natural resources management programs, which often incorporate traditional ethnographic approaches, are successfully combining conservation goals with improvements in Indigenous peoples' quality of life and wellbeing.","context":["The University of Wisconsin Press\nAnthropology / Social Sciences\nTransforming Ethnographic Knowledge\nEdited by Rebecca Hardin and Kamari Maxine Clarke\nAn engaging, reflective, and deeply personal book that prompts a rethinking of both the limits and possibilities of ethnography\nThe ethnographic methods that anthropologists first developed to study other cultures—fieldwork, participant observation, dialogue—are now being adapted for a broad array of applications, such as business, conflict resolution and demobilization, wildlife conservation, education, and biomedicine. In Transforming Ethnographic Knowledge, anthropologists trace the changes they have seen in ethnography as a method and as an intellectual approach, and they offer examples of ethnography’s role in social change and its capacity to transform its practitioners.\nSenior scholars Mary Catherine Bateson, Sidney Mintz, and J. Lorand Matory look back at how thinking ethnographically shaped both their work and their lives, and George Marcus suggests that the methods for teaching and training anthropologists need rethinking and updating. The second part of the volume features anthropologists working in sectors where ethnography is finding or claiming new relevance: Kamari Maxine Clarke looks at ethnographers’ involvement (or non-involvement) in military conflict, Csilla Kalocsai employs ethnographic tools to understand the dynamics of corporate management, Rebecca Hardin and Melissa Remis take their own anthropological training into rainforests where wildlife conservation and research meet changing subsistence practices and gendered politics of social difference, and Marcia Inhorn shows how the interests in mobility and diasporic connection that characterize a new generation of ethnographic work also apply to medical technologies, as those mediate fertility and relate to social status in the Middle East.\nRebecca Hardin is associate professor in the School of Natural Resources and Environment and in the Department of Anthropology at the University of Michigan, Ann Arbor. She is coeditor of Corporate Lives: New Perspectives on the Social Life of the Corporate Form, a special edition of the journal Current Anthropology.Of Related Interest:\nKamari Maxine Clarke is professor of anthropology at Yale University and author of Fictions of Justice: The International Criminal Court and the Challenge of Legal Pluralism in Sub-Saharan Africa\nMedia & bookseller inquiries regarding review copies, events, and interviews can be directed to the publicity department at email@example.com or (608) 263-0734. (If you want to examine a book for possible course use, please see our Course Books page. If you want to examine a book for possible rights licensing, please see Rights & Permissions.)\nUrban Conflagration and the Making of the Modern World\nEdited by Greg Bankoff, Uwe Lübken, and Jordan Sand ....\nLC: 2011052830 GN\n248 pp. 6 x 9 1 table\nPaper $29.95 s\neBook $19.95 s\nAdobe Digital Edition\nAbout our e-books\nPrinting and cut/paste allowed, access on six different devices.\nMary Catherine Bateson, Kamari Maxine Clarke, Rebecca Hardin, Csilla Kalocsai, Macia Inhorn, George Marcus, J. Lorand Matory, Sidney Mintz, Melissa Remis.\n\"This multifaceted, energizing collection reminds us of the many reasons that ethnography’s sustained engagement with others is so vital--not just to anthropology but also for analysis and action in the contemporary world.\"\n—Kirin Narayan, author of Alive in the Writing: Crafting Ethnography in the Company of Chekhov\nHome | Books | Journals | Events | Textbooks | Authors | Related | Search | Order | Contact\nIf you have trouble accessing any page in this web site, contact our Web manager.\nUpdated February 22, 2012© 2012, The Board of Regents of the University of Wisconsin System","Jarvis, D., Stoeckl, N., Larson, S., Grainger, D., Addison, J. & Larson, A. The Learning Generated Through Indigenous Natural Resources Management Programs Increases Quality of Life for Indigenous People – Improving Numerous Contributors to Wellbeing. Ecological Economics, 2020, 106899,\nISSN 0921-8009. https://doi.org/10.1016/j.ecolecon.2020.106899.\nSee Hub research used to help inform the water plan on pages 49–51, 55.\nSee Hub research used to help inform the water plan on pages 24, 33, 36 & 37.\nMarshall, J.C., Blessing, J.J., Clifford, S.E., Negus, P.M., & Steward, A.L. (2020). Epigeic invertebrates of pig‐damaged, exposed wetland sediments are rooted: An ecological response to feral pigs (Sus scrofa). Aquatic Conservation: Marine and Freshwater Ecosystems. 2020; 1– 14. https://doi.org/10.1002/aqc.3468\nMilgin, A., Nardea, L., Grey, H., Laborde, S., & Jackson, S. Sustainability crises are crises of relationship: Learning from Nyikina ecology and ethics. People Nat. 2020; 00: 1– 13. https://doi.org/10.1002/pan3.10149\nCamp, E., Spencer-Smith, T., Chapple, R., Eccles, S., Spindler, R. and Varcoe, T., 2020. Healthy People in a Healthy Environment: Key Directions Statement. Australian Committee for IUCN, Sydney.\nRosemary Hill, Çiğdem Adem, Wilfred V Alangui, Zsolt Molnár, Yildiz Aumeeruddy-Thomas, Peter Bridgewater, Maria Tengö, Randy Thaman, Constant Y Adou Yao, Fikret Berkes, Joji Carino, Manuela Carneiro da Cunha, Mariteuw C Diaw, Sandra Díaz, Viviana E Figueroa, Judy Fisher, Preston Hardison, Kaoru Ichikawa, Peris Kariuki, Madhav Karki, Phil OB Lyver, Pernilla Malmer, Onel Masardule, Alfred A Oteng Yeboah, Diego Pacheco, Tamar Pataridze, Edgar Perez, Michèle-Marie Roué, Hassan Roba, Jennifer Rubis, Osamu Saito, Dayuan Xue. 2020. Working with Indigenous, local and scientific knowledge in assessments of nature and nature’s linkages with people. Current Opinion in Environmental Sustainability 43: 8-20, ISSN 1877-3435, https://doi.org/10.1016/j.cosust.2019.12.006.\nLarson, S., Stoeckl, N., Jarvis, D., Addison, J., Grainger, D., Watkin Lui, F., Walalakoo Aboriginal Corporation, Bunuba Dawangarri Aboriginal Corporation RNTBC, Ewamian Aboriginal Corporation RNTBC and Yanunijarra Aboriginal Corporation RNTBC. 2019. Indigenous Land and Sea Management Programs (ILSMPs) Enhance the Wellbeing of Indigenous Australians, International Journal of Environmental Research and Public Health, 17 (1),125. https://www.mdpi.com/1660-4601/17/1/125.\nDouglas, M.M., Jackson, S., Canham, C.A., Laborde, S., Beesley, L., Kennard, M.J., Pusey, B.J., Loomes, R. & Setterfield, S.A. (2019). Conceptualizing Hydro-socio-ecological Relationships to Enable More Integrated and Inclusive Water Allocation Planning, One Earth, Volume 1, Issue 3, 361-373, ISSN 2590-3322. https://doi.org/10.1016/j.oneear.2019.10.021.\nGrainger, D. & Stoeckl, N (2019). The importance of social learning for non-market valuation. Ecological Economics 164: 106339. https://doi.org/10.1016/j.ecolecon.2019.05.019\nNegus, P.M., Marshall, J.C., Clifford, S.E., Blessing, J.J. & Steward, A.L. (2019). No sitting on the fence: protecting wetlands from feral pig damage by exclusion fences requires effective fence management. Wetlands Ecology & Management https://doi.org/10.1007/s11273-019-09670-7\nThe University of Western Australia (UWA) will host a new Australian research hub to provide national leadership in threatened species […]\nAs the water at a wetland’s edge retreats, exposed sediments provide rich habitat for ground-dwelling invertebrates like beetles, mites, springtails […]\nMore than 100 Indigenous contributors have created the first Indigenous-led guidelines on how to best strengthen and share Indigenous knowledge […]\nQuestions from the Our Knowledge Our Way guidelines launch have been organised into the following categories: Questions to specific panel […]\neDNA reveals where endangered birds of a feather flock together Top image: Minden Photos/Alamy Scrolling image: NT DEPWS Hub […]\nThis new video from our project working in Kakadu with Bininj/Mungguy shares experiences of how technology can help monitor healthy […]\nNo one-size-fits-all approach to predators that plunder turtle nests. Protecting the nests of marine turtles from raids by pigs, dingoes […]"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:16eed71a-8db8-4472-868c-c6ccddb7d9cb>","<urn:uuid:c3f0d475-09bc-4b81-b170-3d872479dcf5>"],"error":null}
{"question":"Could you compare how Factor and Ada handle memory management? I'm interested in understanding the key differences in their approaches to memory allocation and management.","answer":"Factor and Ada have different approaches to memory management. Factor uses automatic memory management with a garbage collector, which is implemented in C++. In contrast, Ada provides more explicit control through features like access types for dynamic memory allocation and controlled types that help prevent memory leaks through user-defined finalization. Ada also allows for user-designed assignment and smart pointers to manage memory more precisely.","context":["This article relies too much on references to primary sources. (July 2019)\n|Paradigm||multi-paradigm: functional, concatenative, stack-oriented|\n0.98 / July 31, 2018\n|Typing discipline||strong, dynamic|\n|OS||Windows, macOS, Linux|\n|Joy, Forth, Lisp, Self|\nFactor is a stack-oriented programming language created by Slava Pestov. Factor is dynamically typed and has automatic memory management, as well as powerful metaprogramming features. The language has a single implementation featuring a self-hosted optimizing compiler and an interactive development environment. The Factor distribution includes a large standard library.\nSlava Pestov created Factor in 2003 as a scripting language for a video game. The initial implementation, now referred to as JFactor, was implemented in Java and ran on the Java Virtual Machine. Though the early language resembled modern Factor superficially in terms of syntax, the modern language is very different in practical terms and the current implementation is much faster.\nThe language has changed significantly over time. Originally, Factor programs centered on manipulating Java objects with Java's reflection capabilities. From the beginning, the design philosophy has been to modify the language to suit programs written in it. As the Factor implementation and standard libraries grew more detailed, the need for certain language features became clear, and they were added. JFactor did not have an object system where you could define your own classes, and early versions of native Factor were the same; the language was similar to Scheme in this way. Today, the object system is a central part of Factor. Other important language features such as tuple classes, combinator inlining, macros, user-defined parsing words and the modern vocabulary system were only added in a piecemeal fashion as their utility became clear.\nThe foreign function interface was present from very early versions to Factor, and an analogous system existed in JFactor. This was chosen over creating a plugin to the C part of the implementation for each external library that Factor should communicate with, and has the benefit of being more declarative, faster to compile and easier to write.\nThe Java implementation initially consisted of just an interpreter, but a compiler to Java bytecode was later added. This compiler only worked on certain procedures. The Java version of Factor was replaced by a version written in C and Factor. Initially, this consisted of just an interpreter, but the interpreter was replaced by two compilers, used in different situations. Over time, the Factor implementation has grown significantly faster.\nFactor is a dynamically typed, functional and object-oriented programming language. Code is structured around small procedures, called words. In typical code, these are 1–3 lines long, and a procedure more than 7 lines long is very rare. Something that would idiomatically be expressed with one procedure in another programming language would be written as several words in Factor.\nEach word takes a fixed number of arguments and has a fixed number of return values. Arguments to words are passed on a data stack, using reverse Polish notation. The stack is used just to organize calls to words, and not as a datastructure. The stack in Factor is used in a similar way to the stack in Forth; for this, they are both considered stack languages. For example, below is a snippet of code that prints out \"hello world\" to the current output stream:\n\"hello world\" print\nio vocabulary that takes a string from the stack and returns nothing. It prints the string to the current output stream (by default, the terminal or the graphical listener).\nThe factorial function can be implemented in Factor in the following way:\n: factorial ( n -- n! ) dup 1 > [ [1,b] product ] [ drop 1 ] if\nNot all data has to be passed around only with the stack. Lexically scoped local variables let you store and access temporaries used within a procedure. Dynamically scoped variables are used to pass things between procedure calls without using the stack. For example, the current input and output streams are stored in dynamically scoped variables.\nFactor emphasizes flexibility and the ability to extend the language. There is a system for macros, as well as for arbitrary extension of Factor syntax. Factor's syntax is often extended to allow for new types of word definitions and new types of literals for data structures. It is also used in the XML library to provide literal syntax for generating XML. For example, the following word takes a string and produces an XML document object which is an HTML document emphasizing the string:\n: make-html ( string -- xml ) dup <XML <html> <head><title><-></title></head> <body><h1><-></h1></body> </html> XML> ;\ndup duplicates the top item on the stack. The\n<-> stands for filling in that part of the XML document with an item from the stack.\nFactor includes a large standard library, written entirely in the language. These include\nA foreign function interface is built into Factor, allowing for communication with C, Objective-C and Fortran programs. There is also support for executing and communicating with shaders written in GLSL.\nFactor is implemented in Factor and C++. It was originally bootstrapped from an earlier Java implementation. Today, the parser and the optimizing compiler are written in the language. Certain basic parts of the language are implemented in C++ such as the garbage collector and certain primitives.\nFactor uses an image-based model, analogous to many Smalltalk implementations, where compiled code and data are stored in an image. To compile a program, the program is loaded into an image and the image is saved. A special tool assists in the process of creating a minimal image to run a particular program, packaging the result into something that can be deployed as a standalone application.\nEdited: 2021-06-18 18:12:58","Ada is a structured, statically typed, imperative, wide-spectrum, multi-paradigm, object-oriented high-level, ALGOL-like programming language, extended from Pascal and other languages. The language was developed in the late 1970s and early 1980s. Ada is named after Augusta Ada Byron (often now known as Ada Lovelace), daughter of the poet Lord Byron.\nAda has built-in language support for explicit concurrency, offering tasks, synchronous message passing, protected objects, and non-determinism. Ada incorporates the benefits of object-oriented languages without incurring the pervasive overheads.\nOther notable features of Ada include: strong typing, inherent reliability, modularity mechanisms (packages), run-time checking, parallel processing, exception handling, the ability to provide abstraction through the package and private type, and generics.\nAda is particularly strong in areas such as real-time applications, low-level hardware access, and safety-critical software, as it has specialized design features, and high reliability. Most errors are detected at compile time and of those remaining many are detected by runtime constraints. While Ada was originally targeted at embedded and real time systems, the Ada 95 revision added support for object-oriented (including dynamic dispatch), numerical, financial, and systems programming. With its readability, scalability, and being designed for development of very large software systems, Ada is a good choice for open source development.\n1. Ada 95: The Lovelace Tutorial by David A. Wheeler\nAda 95: The Lovelace Tutorial is an introduction to Ada 95. The book explains the basics of the Ada computer programming language and assumes that the reader has had some exposure to another algorithmic programming language (such as Pascal, C, C++, Fortran, or BASIC).\nLovelace is interactive and contains many short sections, most of which end with a question to help ensure the reader understands the material. Object-oriented facilities of Ada are covered in depth, and the essential features of Ada programming are given thorough treatment.\n- Brief Introduction to Ada – What is Ada, simple program, use clauses, simple variable, integers, parameters and exceptions.\n- Basic Ada Structure (Packages) – sections cover program units, declarations and bodies, packages, and compilation units.\n- Ada Lexical Elements – looks at Ada from the bottom up.\n- Procedures and Type Integer – the Ada type Integer is used to store integer values, declare a subprogram (procedure or function) declaration, subprogram Bodies and Local Variables.\n- Statements (if, loop) – If statements, case statements, simple loops, and loop iteration schemes.\n- Basic Types (Float, Boolean, subtypes, record) – type float, Boolean, creating types and subtypes, enumeration, arrays, records, and private and limited private types.\n- Object-Oriented Programming – includes inheritance, dynamic dispatching, encapsulation, standard object-oriented format, abstract types and subprograms, and more.\n- Introduction to String Types – examples types of strings, basics of type strings and more.\n- Basic Input/Output – learn how to use more capabilities of Text_IO, especially how to read and write text files, line and file endings.\n- Exceptions – learn how to define exceptions, how to raise exceptions, and how to handle exceptions.\n- Generics – defining generics, generic formal parameters and more.\n- Access Types – learn how to declare and use access types.\n- Tasks and Protected Types.\n- Ada-related Information.\n- Ada Program Structure.\n- Interfacing to Other Languages (including C and Java).\n- Miscellaneous Ada Subjects.\n- Sample Ada Program “Small”.\nThe electronic version of the Lovelace tutorial is released under the terms of the GNU General Public License (GPL).\n2. Ada 95: The Craft of Object-Oriented Programming by John English\nAda 95: The Craft of Object-Oriented Programming is an introduction to Ada 95. It uses an example-driven approach which gradually develops small trivial programs into large case-study type programs.\nThe main focus of this book is on maintenance problems, and using object-oriented technology to write maintainable, extensible programs. Program design is introduced throughout the book, with hypothetical maintenance scenarios used to show design shortcomings, and revise them to accommodate maintenance needs. Practical issues such as debugging programs are tackled, and important Ada features not found in other languages are dealt with practically and early on in the text. Each chapter ends with useful exercises.\n- Programming concepts.\n- Fundamentals of Ada – includes program layout, context clauses, strings, and the beginnings of a simple calculator which is extended in later chapters.\n- Statements – covers If statements, assignment statements, compound conditions, the case statement, range tests, the null statement, loops, and exception handling.\n- Procedures, functions and packages – Zeller’s Congruence, declaring procedures and functions. The chapter also examines packages including child packages.\n- Defining new data types – define data types which can be tailored fairly closely to the type of information that a particular program is concerned with modelling. The chapter covers standard data types, integers, subtypes, modular integers, real types, numeric literals, constants, and enumerations. The chapter ends with the type Boolean, and the type Character.\n- Composite data types – record types, strings, declaring array types, multidimensional arrays and more.\n- Exceptions – shows a programmer how to declare and re-raise exceptions, and obtain information about exceptions.\n- Program design and debugging.\nAbstract Data Types\n- Private types – examines full and partial views, deferred constants, overloaded operators, the Use type clauses, and more.\n- Designing with abstract data types – separate out the user interface, design the mode, define the view package, implement the ADT packages and more.\n- Dynamic memory allocation – access types, linked lists, doubly linked lists, iterators, and more.\n- Generics – generic packages, generic parameters and more.\n- Building a calculator.\nDesigning extensible software\n- Tagged types – variant records, tagged types, inheriting primitive operations and more.\n- Polymorphism and dispatching – class-wide types, dispatching, abstract types, and stream input/output.\n- Controlled types – memory leaks, user-defined finalisation, smart pointers, user-designed assignment, and testing controlled types.\n- An object-oriented calculator.\n- Designing a spreadsheet.\n- Multitasking – looks at active objects, task types, communicating with tasks, sharing data between tasks, and more.\n- Loose ends.\nPermission is given to redistribute this work for non-profit educational use only.\n3. Ada in Action by Do-While Jones\nAda in Action explores many selected advanced features and constructs of ADA and explains how to use them for the best results. It provides examples of how to write clear, correct, maintenance code and gives the reader re-usable components that can be used without modification in their own programs.\nThe book shows the reader to apply good software engineering principles and techniques to ADA programs through tested and proven methods. The text will be of benefit to military software and commercial software engineers and programmers, educational institutions and ADA programmers.\n- Numeric Considerations – examines the POOR_COORDINATES package, the STANDARD_INTEGERS package, the non-existent STANDARD_FLOATS package, DIM_INT_32 package, generic INTEGER_UNITS package, generic FLOAT_UNITS package, DIM_FLOAT package, and more.\n- IO Utilities – ASCII_UTILITIES package, TEXT_IO package, VIRTUAL_TERMINAL package, SCROLL_TERMINAL package, FORM_TERMINAL package, and more.\n- Programming isn’t software engineering – looks at the show tool, the more tool, the write tool, the line tool, the search tool, and more.\n- Testing software components and programs.\nThe book is released under an open source license.\n4. Ada 95 Rationale – The Language – The Standard Libraries by John Barnes\nAda 95 Rationale: The Language – The Standard Libraries describes the rationale for Ada 95, the revised International Standard. It introduces Ada 95 and its powerful new mechanisms, and explains the rationale behind them.\nThe first part is an Introduction to Ada 95; it presents a general discussion of the scope and objectives of Ada 95 and its major technical features. The second part contains a more detailed chapter by chapter account of the Core language. The third part covers the various Annexes which address the predefined environment and the needs of specialized application areas.\n- Evolution of Ada 95.\n- Highlights of Ada 95 – works through the major new features of Ada 95 and the consequential benefits as seen by the general Ada user.\n- Overview of the Ada Language – explores objects, types, classes and operations, statements, expressions and elaboration, system construction, multitasking, exception handling, low level programming, standard library, and application specific facilities.\nThe Core Language\n- Lexical Elements – reserved words and identifiers, program text, pragmas, and requirements summary.\n- Types and Expressions – types, classes, objects, and views, character types, numeric types, composite types, array types, record types, access types, type conversion, staticness, and more.\n- Object Oriented Programming – describes the various ways in which object oriented programming is achieved in Ada 95.\n- Subprograms – covers other relatively minor improvements to subprograms.\n- Packages – a number of important changes to the language are addressed in this chapter.\n- Visibility Rules – visibility and scope rules are rewritten to make them consistent and clearer, the use type clause is introduced for operators, renaming is now allowed for subprogram bodies, generic units and library units, and a number of minor improvements.\n- Tasking – protected types, the Requeue statement, timing and more.\n- Program Structure and Compilation Issues – provides more examples of important changes in the overall structural area of the language, and discusses other topics of a structural nature.\n- Representation Issues.\n- A Predefined Language Environment.\n- Interface to Other Languages.\n- Systems Programming.\n- Real-Time Systems.\n- Distributed Systems.\n- Information Systems.\n- Safety and Security.\nThis package may be freely copied and distributed, if accompanied by a statement, and provided that integral copies of all files are included (i.e. no change whatsoever is allowed).\nAll books in this series:\n|Free Programming Books|\n|Java||General-purpose, concurrent, class-based, object-oriented, high-level language|\n|C||General-purpose, procedural, portable, high-level language|\n|Python||General-purpose, structured, powerful language|\n|C++||General-purpose, portable, free-form, multi-paradigm language|\n|C#||Combines the power and flexibility of C++ with the simplicity of Visual Basic|\n|PHP||PHP has been at the helm of the web for many years|\n|HTML||HyperText Markup Language|\n|SQL||Access and manipulate data held in a relational database management system|\n|Ruby||General purpose, scripting, structured, flexible, fully object-oriented language|\n|Assembly||As close to writing machine code without writing in pure hexadecimal|\n|Swift||Powerful and intuitive general-purpose programming language|\n|Groovy||Powerful, optionally typed and dynamic language|\n|Go||Compiled, statically typed programming language|\n|Pascal||Imperative and procedural language designed in the late 1960s|\n|Perl||High-level, general-purpose, interpreted, scripting, dynamic language|\n|R||De facto standard among statisticians and data analysts|\n|COBOL||Common Business-Oriented Language|\n|Scala||Modern, object-functional, multi-paradigm, Java-based language|\n|Fortran||The first high-level language, using the first compiler|\n|Scratch||Visual programming language designed for 8-16 year-old children|\n|Lua||Designed as an embeddable scripting language|\n|Logo||Dialect of Lisp that features interactivity, modularity, extensibility|\n|Rust||Ideal for systems, embedded, and other performance critical code|\n|Lisp||Unique features - excellent to study programming constructs|\n|Ada||ALGOL-like programming language, extended from Pascal and other languages|\n|Haskell||Standardized, general-purpose, polymorphically, statically typed language|\n|Scheme||A general-purpose, functional language descended from Lisp and Algol|\n|Prolog||A general purpose, declarative, logic programming language|\n|Forth||Imperative stack-based programming language|\n|Clojure||Dialect of the Lisp programming language|\n|Julia||High-level, high-performance language for technical computing|\n|Awk||Versatile language designed for pattern scanning and processing language|\n|BASIC||Beginner’s All-purpose Symbolic Instruction Code|\n|Erlang||General-purpose, concurrent, declarative, functional language|\n|VimL||Powerful scripting language of the Vim editor|\n|OCaml||The main implementation of the Caml language|\n|ECMAScript||Best known as the language embedded in web browsers|\n|Bash||Shell and command language; popular both as a shell and a scripting language|\n|LaTeX||Professional document preparation system and document markup language|\n|TeX||Markup and programming language - create professional quality typeset text|\n|Arduino||Inexpensive, flexible, open source microcontroller platform|\n|Elixir||Relatively new functional language running on the Erlang virtual machine|\n|F#||Uses functional, imperative, and object-oriented programming methods|\n|Tcl||Dynamic language based on concepts of Lisp, C, and Unix shells|\n|Factor||Dynamic stack-based programming language|\n|Eiffel||Object-oriented language designed by Bertrand Meyer|\n|Agda||Dependently typed functional language based on intuitionistic Type Theory|\n|Icon||Wide variety of features for processing and presenting symbolic data|\n|XML||Rules for defining semantic tags describing structure ad meaning|\n|Vala||Object-oriented language, syntactically similar to C#|\n|Standard ML||General-purpose functional language characterized as \"Lisp with types\"|\n|D||General-purpose systems programming language with a C-like syntax|\n|Dart||Client-optimized language for fast apps on multiple platforms|\n|Markdown||Plain text formatting syntax designed to be easy-to-read and easy-to-write|\n|Kotlin||More modern version of Java|\n|Objective-C||Object-oriented language that adds Smalltalk-style messaging to C|\n|VHDL||Hardware description language used in electronic design automation|"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:f7e892b9-9abe-466e-8d5c-2471b57a5af1>","<urn:uuid:14a169b6-f4d5-4d0e-8b21-d9c19c9641e5>"],"error":null}