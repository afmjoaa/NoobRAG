{"question":"What are the guidelines for touchscreen interface design in mobile devices, and what vulnerabilities exist in their biometric security systems?","answer":"For touchscreen interface design, guidelines specify that interactive elements should be at least 44 points wide or high, with adequate spacing to accommodate finger-based input, which is much larger than a mouse pointer. Designers must consider that users often interact with thumbs, making the screen sides optimal interaction zones, and need to account for both indoor and outdoor usage with highly reflective screens. Regarding biometric security vulnerabilities, these systems can be compromised through various methods, particularly through biometric spoofing where hackers create fake fingerprints using materials like silicone or gelatine. They can obtain fingerprint data from surfaces like glasses or desks, or through data breaches. Additionally, hackers can perform man-in-the-middle attacks to intercept communication between the device and authentication server, or exploit vulnerabilities in device firmware.","context":["Creativity comes from rules\nThe Human Interface Guidelines feels very much like a technical document, which might be surprising coming from Apple, but the rules are there to help you understand the technical aspects of the platform and so design for it better. Recognise that they give you a baseline to work from and that, knowing those constraints, you can be more creative.\nElements are essential\nThe ‘iOS UI Element Usage Guidelines’ section provides a very useful breakdown of how and when to use the absolute core building blocks of the interface, such as the navigation bar, table lists and buttons. You need to know and appreciate the default interface in order to create truly great bespoke ones.\nConsider the screen\nDesigning for screens on iOS mobile devices is a different matter to designing for the web, as they’re very bright, very sharp and very reflective. They can be used both indoors and outdoors – and are also usually covered in fingerprints. So test ruthlessly on the actual screen during the design process.\nCreate custom icons\nIcons represent a user’s first experience of an app, so you should consider how they work at different sizes. For example, the larger version of the Contacts icon appears as an address book with ring binding and letters down the side; the smaller one just has a suggestion of these textures, rather than being a crunched-down image.\nResearch suggests that often when users rotate a device, they’re prepared to immerse themselves more deeply in that content. So you might want to use a switch to landscape orientation as a trigger for more features, such as a video clip. Consider the idea that users are less likely to be just using one finger to interact.\nGestures, not clicks\nThe input method should be fundamental to how you design your interface. A finger is much larger than a mouse pointer and you can’t always see past it, so leave space around the actual hit region. However, bear in mind that people also use their thumbs, making the sides of the screen a sweet spot for those.\nThe invisible grid\nThe HIG states that no interactive element, such as a button, should be less than 44 points wide or high. Those dimensions operate as a kind of standard metric for an unspoken grid that relies upon 44 points: buttons are 44 points, the icon bar and Facebook menu are 88 points. This gives you a consistency that underlies a lot of visual elements.\nFocus on the primary task: do one or two things really well rather than trying to offer lots of features to a lesser standard. It’s much more about short, focused experiences, or ‘snacking’ – doing things in five-minute bursts with interruptions – rather than just immersing yourself in one app.\nMake it stretchable\nStretchable views are similar to nine-slice scaling in Fireworks. For example, if you’re creating a button with rounded edges that you want to stay the same shape whatever size it is, you need to make sure it has stretchable views so that iOS can stretch the middle part of the button as much as it wants without warping the image.\nGet the basics right\nWhen you first read the section of the HIG titled ‘Human Interface Principles’, there’s a temptation to scan it because it can feel a little like preaching to the converted. Read it properly and you’ll discover that it’s full of solid advice, such as keeping your interfaces consistent and giving users feedback as they wait for tasks to complete.\nIcons: Jamie Jones","Fingerprint authentication has become a popular way to secure our devices and protect our personal information. However, the question remains: can fingerprint unlock be hacked? The short answer is yes. While fingerprint authentication has advantages in convenience and security, it is not foolproof and can be bypassed using hacking methods. In this article, we will explore how fingerprint authentication works, the risks and advantages associated with it, the various methods of hacking biometric systems, and ways to protect ourselves from potential security breaches.\nWhat is fingerprint authentication?\nFingerprint authentication is a biometric security feature that uses a person’s unique fingerprint to grant access to a device or application. It involves scanning the ridges and valleys of the fingerprint, which are unique to each person, and saving the data as a template. When a user attempts to unlock the device, the system compares the fingerprint scan to the stored template to verify the identity of the user.\nFingerprint authentication is becoming increasingly popular due to its convenience and security. Unlike traditional passwords or PINs, fingerprints cannot be easily forgotten or guessed by others. Additionally, fingerprints are unique to each individual, making it difficult for someone else to impersonate the user. Fingerprint authentication is now commonly used in smartphones, laptops, and even some payment systems.\nHow does fingerprint authentication work?\nFingerprint authentication works by using a sensor to take an image of the user’s fingerprint. The sensor then analyses the image to identify unique points on the fingerprint, including ridge endings, bifurcations, and ridge dots, among others. The system then creates a template of the fingerprint, which is stored in a secure location within the device. When a user attempts to unlock the device, the system compares the scanned fingerprint to the stored template. If a match is found, the device is unlocked.\nFingerprint authentication is considered to be one of the most secure methods of authentication, as each person’s fingerprint is unique and cannot be replicated. Additionally, the use of fingerprint authentication eliminates the need for passwords, which can be easily forgotten or hacked. Fingerprint authentication is now widely used in smartphones, laptops, and other devices, and is becoming increasingly popular in the field of biometric security.\nAdvantages of using fingerprint authentication\nFingerprint authentication has several advantages over traditional passwords, including convenience, accuracy, and security. With fingerprint authentication, users no longer need to remember complex passwords or worry about the security of their passwords. Additionally, fingerprint authentication systems are highly accurate, with a less than 1% false-positive rate. This makes them a reliable authentication method that can be used in a variety of devices and applications.\nAnother advantage of fingerprint authentication is that it is difficult to replicate or forge. Each person’s fingerprint is unique, making it nearly impossible for someone to fake or steal another person’s identity. This makes fingerprint authentication a highly secure method of authentication, especially in high-security environments such as government agencies or financial institutions.\nFingerprint authentication is also a fast and efficient method of authentication. With just a touch of a finger, users can quickly and easily access their devices or applications. This can save time and increase productivity, especially in workplaces where employees need to access multiple devices or applications throughout the day.\nRisks associated with using fingerprint authentication\nWhile fingerprint authentication has advantages, it also has risks associated with it. One of the primary risks is the potential for biometric data breaches. Unlike passwords, which can be easily changed, biometric data, such as fingerprint templates, cannot be changed once they are compromised. Additionally, biometric data can be stolen or copied without the user’s knowledge, leading to potential security breaches.\nAnother risk associated with fingerprint authentication is the possibility of false positives or false negatives. False positives occur when the system incorrectly identifies someone as an authorized user, while false negatives occur when the system fails to recognize an authorized user. This can lead to frustration and inconvenience for users, as well as potential security breaches if unauthorized individuals are granted access.\nFurthermore, there are concerns about the accuracy and reliability of fingerprint authentication technology. Factors such as dirt, sweat, or injuries to the fingers can affect the accuracy of the system, leading to potential errors or failures. This can be particularly problematic in high-security environments where access control is critical.\nHacking methods used to bypass fingerprint authentication\nThere are several methods that hackers use to bypass fingerprint authentication systems. These methods include biometric spoofing, which involves creating a fake fingerprint using a variety of materials, such as silicone or gelatine, and using it to unlock the device. Other methods include using latent fingerprints, or fingerprints left on surfaces, to create a fake fingerprint, or using photographs of a person’s fingerprints to gain access.\nHowever, there are also more advanced methods that hackers use to bypass fingerprint authentication. One such method is known as a “man-in-the-middle” attack, where the hacker intercepts the communication between the device and the authentication server, and alters the data being sent. Another method is to exploit vulnerabilities in the device’s firmware or software, allowing the hacker to gain access to the fingerprint data stored on the device.\nBiometric spoofing: The most common method used to hack fingerprint authentication\nBiometric spoofing is one of the most common methods used to bypass fingerprint authentication. It involves creating a fake fingerprint using materials such as silicone, gelatine, or even play-doh, and using it to unlock the device. Hackers can easily obtain fingerprint templates by taking photographs of fingerprints left on surfaces, such as glasses or desks, or by using publicly available fingerprint images.\nOne way to prevent biometric spoofing is to use multi-factor authentication, which requires users to provide additional forms of identification, such as a password or a security token. Another method is to use liveness detection, which checks for signs of life, such as blood flow or pulse, to ensure that the fingerprint being scanned is from a living person and not a fake. As technology advances, new methods of biometric authentication, such as facial recognition and iris scanning, are also being developed to provide more secure forms of identification.\nHow hackers use fake fingerprints to bypass biometric authentication\nOnce a hacker has created a fake fingerprint, they can use it to unlock the device by placing it on the scanner. The sensor will read the fingerprints, and if it matches the stored template, the device will unlock. Biometric spoofing is so effective because most fingerprint sensors cannot differentiate between a real fingerprint and a fake one made from silicone or gelatine.\nHowever, some newer fingerprint sensors have additional features that can detect fake fingerprints. For example, some sensors use ultrasound to create a 3D image of the fingerprint, making it more difficult to spoof. Additionally, some sensors can detect the temperature of the finger, which can help distinguish between a real finger and a fake one made from a material that does not conduct heat in the same way as human skin.\nCan hackers steal your fingerprints and use them for hacking?\nYes, hackers can steal your fingerprint and use it for hacking. This can be done by obtaining a copy of your biometric data, either through data breaches or by lifting your fingerprints from surfaces. They can then create a fake fingerprint and use it to gain access to your device or system.\nIt is important to note that unlike passwords, you cannot change your fingerprints. Once your biometric data is compromised, it is compromised for life. This makes it a particularly valuable target for hackers, as they can use it repeatedly to gain access to your personal information.\nHowever, there are ways to protect your biometric data. One way is to use multi-factor authentication, which requires a combination of something you know (like a password) and something you have (like a fingerprint or a security token). This makes it much harder for hackers to gain access to your accounts, even if they have your biometric data.\nHow to protect yourself from fingerprint authentication hacks\nThere are several ways to protect yourself from fingerprint authentication hacks. One of the most effective ways is to use a device that has multiple biometric authentication methods, such as facial recognition or voice recognition. This way, if the fingerprint data is compromised, the hacker will not be able to access the device without these additional authentication methods.\nAnother way to protect yourself is to regularly update your device’s software and security features. Manufacturers often release updates that address security vulnerabilities, so it’s important to stay up-to-date with these updates to ensure your device is as secure as possible.\nIt’s also important to be cautious about where and how you use your fingerprint authentication. Avoid using it on public devices or in public places where someone could easily access your fingerprint data. Additionally, make sure to properly secure your device with a strong password or PIN in case your fingerprint data is compromised.\nAlternatives to using fingerprint authentication for security purposes\nThere are several alternatives to using fingerprint authentication for security purposes, including passwords, PINs, and pattern locks. These alternatives, while not as convenient as fingerprint authentication, offer the advantage of being easily changeable, making them more secure in case of a breach.\nAnother alternative to fingerprint authentication is facial recognition technology. This method uses a camera to capture an image of the user’s face and compares it to a stored image to verify their identity. While this method is not foolproof, it is becoming increasingly popular and is often used in conjunction with other security measures.\nFinally, some companies are exploring the use of biometric authentication methods that go beyond fingerprints, such as iris or voice recognition. These methods offer even greater security than fingerprint authentication, but may require more specialized hardware and software to implement.\nHow companies are improving the security of their biometric systems\nAs biometric systems become more popular, companies are working hard to improve their security. This includes introducing additional biometric authentication methods, such as facial recognition or voice recognition, to make the systems more secure. Additionally, companies are investing in biometric sensors that can detect fake fingerprints, making it more difficult for hackers to bypass the systems.\nLegal and ethical concerns surrounding the use of biometric data for security purposes\nThe use of biometric data for security purposes raises several legal and ethical concerns. One of the primary concerns is the potential for data breaches, which can lead to the theft of sensitive personal information. Additionally, there are concerns about the use of biometric data for surveillance purposes, and the potential for abuse of this data by governments and corporations.\nThe future of biometric security: What’s next after fingerprints?\nWhile fingerprint authentication remains a popular method of biometric security, there are other biometric authentication methods on the horizon. These include technologies such as facial recognition, voice recognition, and even DNA authentication. As these technologies become more sophisticated and secure, they may replace fingerprint authentication as the preferred method of biometric security in the future.\nIn conclusion, while fingerprint authentication is a reliable and convenient method of securing our devices and protecting our personal information, it is not foolproof and can be bypassed using hacking methods. As such, it is important for users to be aware of the risks associated with biometric security and take steps to protect themselves from potential security breaches. By understanding the strengths and weaknesses of fingerprint authentication, and its potential for security breaches, we can better protect ourselves in an increasingly digital world."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"search_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:5a738c64-3d2d-4bea-adf4-eb58ae43d0e7>","<urn:uuid:553b26bc-5566-4455-a40f-1bb771bbd762>"],"error":null}
{"question":"Which safety issue affects more U.S. locations annually - water drowning incidents or prescription drug contamination?","answer":"Water drowning incidents affect more locations across the U.S. According to the data, drowning surpasses all other causes of death for children 14 and under in ten states (Alaska, Arizona, California, Florida, Hawaii, Montana, Nevada, Oregon, Utah, and Washington). In comparison, the prescription drug contamination issue, while widespread, was found in drinking water supplies of at least 41 million Americans according to the 2008 investigation. The drowning risk is particularly prevalent between May and August, with 40% of incidents occurring on weekends, while drug contamination is a year-round issue affecting water treatment facilities that cannot fully remove these substances.","context":["I get several questions a month about prescription drug disposal. For safety purposes, it’s very important to eliminate old prescriptions and unused drugs from your home, but you should do so in a responsible manner.\nAccording to the EPA, you should not “flush expired or unwanted prescription and over-the-counter drugs down the toilet or drain unless the label or accompanying patient information specifically instructs you to do so.” As a first-choice, the EPA recommends bringing your no longer used and/or expired drug products to a take-back program, generally offered by your local police or fire department.\nFor locations in Illinois, we recommend that you go to the Illinois EPA site and search for a drop-off location near you. Simply enter your zip code and the miles you are willing to travel. It’s a good idea to call the facility in advance just to make sure of what they accept, and to find out their hours for drop-off.\nThere are three main reasons that prescriptions drugs should not be placed in the garbage or flushed down your toilet or drain:\nAccording to The Watershed Council in Northern Michigan, wastewater treatment plants are not designed to treat all the substances contained in medications. Although treatment plants are currently equipped to remove chemicals, foreign materials, and microorganisms from the water prior to ejecting it into streams and lakes, active drug compounds from humans are not eliminated from the water in the treatment process. Treatment systems permit up to 93% of highly concentrated active drug compounds to leave a treatment plant. Therefore, most of these chemicals pass through the treatment facilities and accumulate in river, lakes, ground water, as well as aquatic organisms.\nA 2008 Associated Press five-month investigation concluded that a vast array of pharmaceutical products have been found in drinking water supplies of at least 41 million Americans. At current levels, pharmaceutical residues are unlikely to pose an immediate risk to human health, but the long-term consequences of individual chemicals, and combinations of chemicals, are unknown, especially as concentrations rise.\nTrash and Landfills\nBut dumping them in the trash without special measures also presents problems. Children or pets might accidentally find and get into them. Teenagers and/or adults who abuse prescription drugs also have easy access.\nAccording to Take Back Your Meds:\nUnwanted drugs are still chemically active when they are thrown in the trash, even if mixed with kitty litter or coffee grounds. Several studies have shown that medicines in a landfill can be released to the local environment through the landfill liquid – or “garbage juice” – that may be collected and sent to wastewater treatment plants.\nIn addition, they might leach out into the soil and contaminate groundwater.\nAccording to The Watershed Council, properly getting rid of unused medicines reduces the risk that prescription and over-the-counter medicines will be mishandled and end up on the street.\nKeeping medicines around the home can lead to possible poisoning from accidental ingestion, particularly among young children and pets, or illegal use or theft. When unused or expired drugs sit in the medicine cabinet, they become too easily available and appealing to potential drug abusers, especially young adults and youth. Based on a National Survey on Drug Use and Health, persons between the ages of 12 and 17 abuse prescription drugs more than cocaine, heroin, and methamphetamine combined; and prescription drug abuse is second only to marijuana use. Additionally, medications left unattended can have tragic accidental consequences. Each year in the U.S., over 71,000 children 18 and younger are seen in emergency rooms for unintentional overdoses of prescription and over-the-counter drugs.\nFeel free to contact us about prescription and over-the-counter drug disposal in your area. We can also address questions concerning the disposal of ‘sharps’ – medications that require injection with a needle or syringe. There are some locations in the area that will accept these types of medications as well.","Drowning Fact Sheet\nDrowning accidents are the leading cause of injury/deaths among children under five. A temporary lapse in supervision is a common factor in most drownings and near-drownings. Child drownings can happen in a matter of seconds–in the time it takes to answer the phone. There is often no splashing to warn of trouble. Children can drown in small quantities of water and are at risk in their own homes from wading pools, bathtubs, buckets, diaper pails, and toilets as well as swimming pools, spas, and hot tubs.\nDeath and Injuries\nA swimming pool is 14 times more likely than a motor vehicle to be involved in the death of a child age 4 and under. Each year, approximately 1,150 children ages 14 and under drown; more than half are preschoolers (ages 0-4). An estimated 5,000 children ages 14 and under are hospitalized due to near-drownings annually in the United States. Of children surviving near-drownings, 5-20 percent suffer severe and permanent disability.\nWhere Drownings Happen\nApproximately 50 percent of preschooler drownings occur in residential swimming pools. Each year, more than 2,000 preschooler near-drownings occur in residential pools. Of preschooler pool drownings, 65 percent occur in the child’s home pool and 33 percent at the homes of friends, neighbors or relatives. Each year, 350 drownings (for all ages) happen in bathtubs and approximately 40 children drown in five-gallon buckets. In ten states–Alaska, Arizona, California, Florida, Hawaii, Montana, Nevada, Oregon, Utah, and Washington– drowning surpasses all other causes of death to children ages 14 and under.\nHow and When Drownings Happen\nOf all preschoolers who drown, 70 percent are in the care of one of both parents at the time of the drowning. Of all preschoolers who drown, 75 percent are missing from sight for five minutes or less. Two-thirds of all drownings happen between May and August with 40 percent occurring on Saturdays and Sundays. It is the artificial method of circulating blood and oxygen through a body and attempting to keep the brain alive. CPR does work. When initiated within four minutes, the survival rate is 43 percent. When initiated within four to eight minutes, the survival rate is ten percent.\nWhy Learn CPR?\nOne in seven people will have the opportunity to use CPR in their lifetime. Ninety percent of the time, CPR will be done on a family member or close friend. More than 650,000 people die annually from heart attack in the United States each year. More than 350,000 die before reaching the hospital. When the brain starts to go four to six minutes without oxygen, brain damage/death begins. http://www.elcajonfirefighters.org/drownings.htm\nWater-Related Injuries: Fact Sheethttp://www.cdc.gov/ncipc/factsheets/drown.htm\nCenters for Disease Control and Prevention at Department of Health and Human Safety Overview\nIn 2003, there were 3,306 unintentional fatal drowning’s in the United States. averaging nine people per day. This figure does not include 473 drowning’s in boating-related incidents (CDC 2005).\nFor every child 14 years and younger who dies from drowning, five receive emergency department care for nonfatal submersion injuries.\nMore than half of these children require hospitalization (CDC 2005). Nonfatal drowning’s can cause brain damage that results in long-term disabilities ranging from memory problems and learning disabilities to the permanent loss of basic functioning (i.e., permanent vegetative state).\n- Drowning is the second leading cause of injury death of infants and children younger than 15 in the U.S.\n- Children less than 5 and adolescents between the ages of 15-24 yrs have the highest drowning rates\n- For every child who drowns, four children are hospitalized for near-drowning.\n- One third of near-drowning pediatric victims who are comatose on admission to the hospital will suffer significant neurologic damage\n- The annual cost of care per year in a chronic care facility for an impaired survivor of a near-drowning event is approximately $100,000.\n- The annual lifetime cost attributable to drowning and near-drowning in children less than 15 years of age is $384 million.\n- Children less than 1 year of age most frequently drown in bathtubs and buckets\n- Children between the ages of 1-4 years most often drown in home or apartment swimming pools. Most of these children drown by entering the pool from their home through the unprotected side of the pool. In the majority of cases, the children were last seen in the home, but were out of eye contact for only a moment and the immersion was silent (no screams or splashing was heard).\n- Children between 5-19 most often drown in lakes, ponds, rivers and pools.\nChildren are our precious love ones. You need a “layer system” of protection to injure their safety. Susan Reeves sent me the following information\nSecurity layers of protection (like protecting the President)\n1. Child‘s ability to swim (Rated on a scale)\n2. Device on the child (“Pool Turtles” or vest) (Pool turtles are worn\nlike a wrist watch with an alarm in the house. When the sensor gets wet –\nthe alarm sounds. The children wear these every moment they are not in\nwater, and that includes sleeping with them on.)\n3. Device in the water that detects ripples\n4. Fence layer – with lock and basic water safety equipment that consists of a pole, rope and personal flotation device (PFD). A swimming pool should always have this equipment in working condition nearby.\n5. House alarms on doors\n6. Parental Awareness/Sitter education/Visitors -“Pool Master” similar to\nlifeguard, does nothing but watch water at a party – can be rotated among\nattending adults or a teenager specially hired for the occasion. The poolmaster is needed even when the children are wearing floating devices. While personal flotation devices (PFD) are generally safe, the pool is still a place where children must be supervised. For example, if the device suddenly shifts position, loses air, or slips out from underneath, the child is left in a dangerous situation. A PFD is not a replacement for parental supervision. Never leave a child alone at the poolside\nI have also seen security cameras that monitor the pool, and the image\nappears in the corner of your computer screen.\nNever leave an infant or small child unattended in the bathroom, even for a few moments.\nTeach children water and swimming skills as early as possible\nTake special precautions if you own a pool:\nUse layers of barrier protection between the child and water to warn and impede. Pool and spa owners can take practical steps to make their pool and spa less dangerous by installing “layers of protection.” These include:\nKeep enticing objects that might attract a child out of the pool area\nDrain standing water off spa and pool covers: children can drown in as little as 2 inches of water!\nInstall a phone poolside with emergency numbers posted and programmed into speed dial. Also post CPR instructions poolside.\nJoan E writes, My sister had a pool in Melrose Park which my parents had built for their grandchildren. I would watch 11 of them all by myself! Both my sister and Henry’s sister worked, so they left their kids with me to watch in addition to our four. But one of the closest calls we had was when my sister and I were just sitting on the stairs in the her pool talking to each other, and all of a sudden my sister noticed our daughter gurgling under water looking at her, behind my back! She had taken off her safety belt and jumped in the pool!\nI was very lucky that I learned early on through all that, that DROWNING IS VERY QUIET! Since then, I have repeated that a million times to my kids and grandchildren and visitors! In the movies they always show someone sinking, splashing and screaming for help, not so! I can’t tell you the times we have had guests here, that let their little ones run around and the kids will take off their water wings or tube off, and then they forget to put them back on and then will start to get back in the pool without them.\nWhat is really scarey is that they assume the kids will know that they can’t go back in the pool without them, but there have been many times I’ve caught the children just climbing back in without their lifesavers! My own kids knew my rules and made their little ones keep the things on. That is why I always sat sitting in the direction of the large shallow kid’s end of the pool and talked to people without looking at them but kept my eyes on the children. Otherwise I was in the pool sitting and watching them. It always shocked me how easily people can forget about their little non-swimmers and be talking and looking away. I was so happy when my last of 14 grandchildren learned how to swim! I taught them all myself. It was quite an accomplishment, especialy since one grandson is a handicapped child but now is a great deep water swimmer! He loves the pool and even though he doesn’t talk much, he understood me enough to learn how to swim. I think I got them all swimming by 4 or less.\nI figured out a great method that worked so well. I would put a toy on the stairs under the water and ask them to get it. Then I put it on the next lower stair and they had to get it. I went down the stairs, and they eventually had to learn to put their faces in the water, and even open their eyes to look for it, especially because sometimes I would move it on them. THEN, finally I would get the toy on the bottom step and next the floor. Then their little rear ends would bob up trying to get down there for the toy. They found out they had to paddle and fight to get down to the bottom stair or floor, and that they didn’t just fall or sink to the bottom. And that taught them to hold their breath and keep their mouths shut. SO, then I moved the item a few feet in front of them away from the stairs, and they really had to use their arms and kick to get down there, and lo and behold, they were actually swimming! Then the farther away the item was, they naturally were diving down to get it! If you have any other little ones who still aren’t swimming, try this method. It won’t happen in one lesson, but it will pretty quickly!\nThe swimming motion actually comes very naturally. The key is they have to learn to hold their breath when they put their faces under water and open their eyes. Once they learn that, you are half way there! Your little George must have breathed in some water. It doesn’t take much, my friend lost a 3 year old who was sitting on a potty chair on the toilet while the tub was just beginning to be filled. She went to answer the door for the grocery delivery and he got off the toilet and dropped his bear into the tub and fell in after it and was gone by the time she got back upstairs and there wasn’t must water in the tub yet.!\nIn the event of a drowning–\nRemove the victim from the water, have someone call 9-1-1 or your local emergency number. Check consciousness and breathing.\nIf the victim is not breathing, open the airway and attempt\nrescue breathing.If breaths do not go in, re-tilt the head and attempt rescue breathing again. If air still does not go in, give abdominal thrusts (Heimlich maneuver) for children and adults to clear the airway.\nOnce the airway is clear, provide rescue breathing or CPR as needed.\n- Filer’s Files 37 2019 -Was a space alien killed in NJ in 1978? - September 9, 2019\n- Filer’s Files #10 – 2018 Planetary Defense - March 13, 2018\n- UFO Hunters ‘Discover’ Crashed Alien UFO Drone In NASA Mars Rover Photo - November 4, 2015"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:369b1bb9-27ab-40de-b839-82450c6fdc0f>","<urn:uuid:730adaab-db58-4d00-8e94-f561d774d575>"],"error":null}
{"question":"What are the IMO's emission reduction targets for ships, and how does NOx pollution specifically impact the environment?","answer":"The IMO has set a target to cut the shipping sector's overall CO2 output by 50 percent by 2050, with aims to begin reductions as soon as possible and eventually phase out carbon emissions entirely. This is in addition to the Energy Efficiency Design Index requirements for new ships. Regarding NOx pollution specifically, it contributes to several environmental issues: ground level ozone, acid deposition, particulate matter, and indirect effects on global warming. Studies show that shipping accounts for 18-30% of the world's nitrogen oxides emissions.","context":["\"This is a very important signal for the shipping industry and the maritime cluster to work full steam ahead to reduce greenhouse gas emissions\" said Panos Laskaridis, president of ECSA, The European Community Shipowners' Association.\nPanos Laskaridis was speaking after the IMO Marine Environment Protection Committee (MEPC) announcement in April 2018: member state delegates have agreed on a target to cut the shipping sector's overall CO2 output by 50 percent by 2050, aiming to begin reductions as soon as possible and pursue efforts to phase out carbon emissions entirely. This IMO announcement comes as an additional measure to strengthen the Energy Efficiency Design Index (EEDI) efficiency requirements already mandated for new ships. [ref.1: IMO Agrees to CO2 Emissions Target published in Marine Executive, April 2018: https://www.maritime-executive.com/article/imo-agrees-to-co2-emissions-target]\nShipyards now have no choice but to introduce new vessels which fit with these emission limit requirements. In addition to this environmental regulation, they also must respond to another expectation – coming from the market and fleet operators - that the less fuel the ship consumes (and therefore the less CO2 emitted) the lower the operational expenditure will be.\nImproving existing technologies is one obvious way to reach these targets. Focusing on the marine engine design and even on components of one system can be a way to fit with requirements. However, the development time is always constrained as to keep being competitive the shipyards have to introduce new technologies quickly to the market. In a fast-paced design environment, using simulation to analyze the efficiency of a product at an early stage of the design cycle can make a significant difference to development times.\nThat’s why Ganser Common Rail Systems (CRS) decided to opt for system simulation to develop and analyze an improved control hydraulic valve (the poppet valve) to reduce engine emissions of a vessel.Simcenter Amesim for marine propulsion systems\nGanser CRS chose to use Simcenter Amesim, the system simulation solution which is part of the Simcenter Portfolio, to improve the control (poppet valve) valve design to reach the very small injection quantities used in micro pilot applications.\nUsing Simcenter Amesim, the Ganser CRS engineering team improved the nozzle needle geometry through validating and implementing two new features:\na defined slope of the injection rate during the opening of the injector, which is favorable to reduce combustion noise and emissions\na fast injection rate decreases during the injection closing, which is favorable for the end-of-combustion conditions\nThe new technology developed by Ganser CRS was tested on a marine engine, replacing the previous mechanical injection system. New Ganser CRS systems are employing not only this unique poppet valve technology, but also a Waves Dynamics and Dampening System (WDD) and injector integrated accumulators.\nThe final achievements on the 6-cylinder dual fuel marine engine, without any modification of the cylinder head, are impressive. Tests proved a stable operation of the engine in gas mode with gas ignited by diesel and the achievement of IMO Tier 3 emission levels. More details can be found in the published paper.[ref.2 Common Rail Injectors With Poppet Valve by Ganser CRS published in Diesel&Gas Turbine Worldwide, September 2016 https://dieselgasturbine.com/common-rail-injectors-poppet-valve/]\nWhile this example shows how simulation can help to improve existing engine designs, some shipyards are eager to go one step further in the design of their vessel andstep directly into the hybridization of their machine, again using system simulation. Curious to know more about how it can be deployed? Read Vessel hybridization, too ambitious you said?","The continuous rise of Nitrogen Oxide (NOx) in the atmosphere is a matter of great concern. Several factors have been stated as the reason for the increase of this element in the air and toxic emission from ships is one of them.\nNitrogen Oxide (NOx) is formed in the atmosphere when fuels such as oil, gas, and coal are burned at a very high temperature. The pollution caused because of NOx is supposed to be known as one of the most dangerous forms, which eventually contributes towards global warming.\nNitrogen Oxide and its effects on the Environment\nA high-level of nitrogen oxide being released into the atmosphere can result in to:\n- Ground Level Ozone\n- Acid Deposition\n- Particulate Matter\n- Indirect Effect to Global Warming\nSignificant contributors to this toxic oxide are factories, coal-burning plants and emissions from motor vehicles and marine diesel engines.\nStudies show that shipping is the source of 18-30% of the world’s nitrogen oxides. Moreover, though several steps have been taken to reduce the emission of NOx from ships, there is still a long way to go to bring down emissions to zero or an acceptable level.\nIMO’s MARPOL Annex VI\nMARPOL (Marine Pollution) is one of International Maritime Organization (IMO)’s regulatory policies focuses on preventing different forms of marine pollution including oil, noxious liquid substances, harmful substances, waste water, garbage and emissions of sulphur oxides and nitrogen oxides at sea.\nMARPOL Annex VI Regulation 13 sets out the mandatory limitations on NOx. The regulation affects not only ships from signatory states but also ships entering MARPOL signatory-member waters.\nMARPOL Annex VI and ECAs\nApart from the mandatory NOx limitations for oceangoing vessels around waters of the member states, IMO also defines Emission Control Areas or ECAs where the MARPOL NOx emission standards will apply.\nThe ECAs includes:\n- Baltic Sea (2006)\n- North Sea (2007)\n- Waters in North American coasts that include waters adjacent to the Pacific coast, the Atlantic/Gulf coast extending to 200 nautical miles from US coast (2010)\n- Waters around Puerto Rico and the US Virgin Islands that are just recently designated by IMO (effective 2014)\n- Norway, Japan and Mediterranean areas are being considered for future ECA proposal.\nIn the past few years, IMO has become stricter in implementing norms to reduce shipping emissions and to minimize the effects of marine pollution.\nIs there a way to reduce NOx emission?\nIn the hope of complying with IMO’s global limits on nitrogen oxides and the enforcement of more stringent standards within the three Emission Controlled Areas marine diesel engine operators, ship owners and seafarers are left with no option but to find the best technology in reducing the amount of NOx from their ship’s exhaust systems and to take steps to make their ship “greener”.\nAmong the various emission control applications Selective Catalytic Reduction (SCR) System comes out to be the most efficient, effectively reducing ship’s NOx emission by 90-95%. By mixing a reagent (SCR 40 – 40% Marine Urea Solution) to the exhaust gas, nitrogen oxides are converted to Nitrogen (N2), water and Carbon Dioxide (CO2).\nGlobal Marine Urea Solution Suppliers\nSome marine SCR System providers offer the supply of urea solution with their application. A known SCR System provider in Japan is Hitachi Zosen while Miratech is from the United States. Both suppliers offer Marine SCR application that complies with Tier III NOx emission standards.\nFor marine urea solution requirements, there are growing urea markets in the US, China and recently in Singapore. Read here for more information regarding marine urea solutions in Singapore.\nSourced by ekomeri.com"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:b7b115ea-2203-4724-a352-d995e6b94904>","<urn:uuid:d61eb555-5360-4771-bb06-eedd73ac81f4>"],"error":null}
{"question":"Been noticing my toddler has speech issues and he's had several ear infections lately - is there a connection between these two things?","answer":"Yes, there is a connection. Since speech is learned by listening, frequent ear infections can slow down speech sound development in young children. Hearing loss can affect articulation development because children learn speech by listening.","context":["What is articulation?\nArticulation (are tih kyoo LAY shun) is the process of making speech sounds by moving the tongue, lips, jaw, and soft palate. Children learn speech by imitating the sounds they hear as you talk about what you are doing during the day, sing songs, and read books to them.\nChildren begin developing speech as an infant. By 6 months of age, babies coo and play with their voices making sounds like \"oo, da, ma, and goo.\" As your baby grows he will begin to babble, making more consonants like \"b\" and \"k\" with different vowel sounds. Your child will continue to imitate sounds and word shapes, and these will turn into natural, spontaneous speech.\nAlthough children begin to develop speech as infants, they do not learn to make all speech sounds at one time. Every sound has a different, but predictable, range of ages for when the child should make the sound correctly. Articulation errors are a normal part of speech development. Most children will make mistakes as they learn to say new words. By the time the child is 3 years old, speech should be understandable about 80 percent of the time. By the time the child is 4 years old, speech should be understandable almost all the time, although there may still be sound errors. Children should be able to make all of the sounds of the English language correctly by the time they are 8 years old.\nWhat is an articulation delay or disorder?\nAn articulation delay or disorder happens when errors continue past a certain age. Articulation errors can occur at the beginning, middle, or end of a word. The following are the 3 most common articulation errors:\n- Replacing one (1) sound for another – bacuum for vacuum.\n- Omitting a sound – bue for blue.\n- Distorting a sound – you recognize the sound, but it sounds funny. A lisp is a distortion of /s/ and is caused when the tongue sticks out past the teeth.\nNot all sound replacements and omissions are considered speech errors. Instead, they may be related to a dialect or accent.\nThe chart below gives general guidelines for the age when children learn to make certain speech sounds.\nWhy do some children have problems with articulation?\nMany articulation errors happen for reasons we do not understand. Your child may not learn how to make the sounds correctly or may not learn the rules of speech on his own. Physical problems can also affect articulation, such as the following:\n- Illness that lasts a long time – Stimulation may be limited\n- Hearing loss – Speech is learned by listening. For this reason, frequent ear infections can slow down speech sound development in young children.\n- Brain tumors – Tumors may affect speech centers of the brain or weaken muscles of the lips, palate, tongue, or vocal cords.\n- Developmental disorders (like autism)\n- Neurological disorders (like cerebral palsy)\nAn articulation delay or disorder is a problem when:\n- Listeners do not understand what the child is saying;\n- The child is frustrated and misbehaves because he cannot express what he wants;\n- The child avoids situations where he needs to speak; and\n- The child is embarrassed or worried about how he sounds or because others make fun of his speech.\nHow you can help\n- Talk to your child and play with him. This is a chance to make talking fun and model correct speech sounds for him.\n- Face your child when you talk to him and work at his level.\n- Do not interrupt or constantly correct your child.\n- Do not reinforce errors by imitating them. Instead, model the correct way to make the sound. For example, if your child says, “That’s a wellow duck,” you say, “Yes, that’s a yellow duck. A yellow baby duck. The sun is yellow, too.”\n- Reward your child with praise for saying the sound correctly or give him credit for trying.\n- Read to your child. Use reading as a way to surround your child with the targeted sound. For example, read Goodnight Moon if the child is working on the /g/ sound. Use meals, bath time, bedtime, playtime, and other daily routines to work on speech. These activities can be great learning moments.\nIf you have questions about articulation, call Rehabilitation Services at 901-595-3621. If you are inside the hospital, dial 3621. If you are outside the Memphis area, call toll-free 1-866-2ST-JUDE (1-866-278-5833), extension 3621.\nThis document is not intended to take the place of the care and attention of your personal physician or other professional medical services. Our aim is to promote active participation in your care and treatment by providing information and education. Questions about individual health concerns or specific treatment options should be discussed with your physician.\nSt. Jude complies with health care-related federal civil rights laws and does not discriminate on the basis of race, color, national origin, age, disability, or sex.\nATTENTION: If you speak another language, assistance services, free of charge, are available to you. Call 1-866-278-5833 (TTY: 1-901-595-1040).\nATENCIÓN: si habla español, tiene a su disposición servicios gratuitos de asistencia lingüística. Llame al 1-866-278-5833 (TTY: 1-901-595-1040).\n1-866-278-5833 تنبيه: إذا كنت تتحدث باللغة العربية فيمكنك الاستعانة بخدمات المساعدة اللغوية المتوفرة لك مجانا. .يرجى الاتصال بالرقم\n.(1-901-595-1040 الهاتف النصي:)"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:8ce8d438-7220-43e3-a532-bcd582a2a844>"],"error":null}
{"question":"How can 3D printing technology potentially change hardware development costs in the future?","answer":"3D printing could potentially eliminate the need for steel molds in plastic housing production, which would flatten the cost-of-change curve and remove a major inflection point. This could make changes economically feasible at points where they were previously prohibitive, and could reduce the overall product development cycle by weeks or months compared to traditional injection molding processes.","context":["Do you believe that agile can be applied to the development of non-software products? Well it can! Take Wikispeed, for instance, which builds a street-legal car every sprint. This one example alone should be enough to convince you that agile outside of software development is not only possible, but also is a great way to achieve competitive advantage.\nOver the past couple of years an increasing number of my training and coaching engagements have been for companies where hardware (HW) and/or firmware (FW) was a major focus of their agile efforts.\nMany people I encounter both within and outside of HW/FW environments assume that HW/FW development is better suited to a non-agile, sequential, phase-based approach. In my experience asking “Can agile be used in a HW/FW environment?” is the wrong question. A more appropriate question would be “Where and how can agile be used during HW/FW development?”\nOne effective way to start answering this question is to examine the cost of change characteristics of the organization’s current HW/FW development approach.\nCost of Change Is the Driver\nThe principal argument that I hear against the use of agile in HW/FW development is something like: “Hardware is not software, we aren’t as flexible as you software guys because the cost of change for us is different than it is for you.” I agree that the cost-of-change curve is different when developing HW/FW than when developing software, but that doesn’t mean that agile can’t play a role during HW/FW development. Let’s see why.\nA cost-of-change curve has time on the horizontal axis and cost on the vertical axis.\nA goal during product development is to keep the cost-of-change curve flat for as long as possible—making it economically sensible to embrace change for as long as possible.\nAll cost-of-change curves contain one or more inflection points (where the graph turns sharply upwards). Once you hit an inflection point, the cost of making (at least certain) changes goes up. In other words, making that same change earlier would have been cheaper and now it has gotten much more expensive.\nWhen HW/FW people say, “Our cost of change is different!” what they really mean is that their cost-of-change curve hits its first major inflection point sooner than a software cost-of-change curve. See the following graph.\nDon’t get too attached to the exact nature of each curve in this graph—I didn’t base the shape of each curve on a rigorous analysis of historical data. Rather, this graph is meant to be more intuitive. The basic takeaway should be that HW typically inflects up sooner than FW, and FW sooner than software.\nDraw the Cost-of-Change Curve for your Development Process\nTo make any discussion on this topic more real, you need to draw the cost of change curve for your specific development process. Before I do training or coaching for a HW/FW team, I ask team members to help me graph their cost-of-change curve. Below is a recent example of one my client’s cost-of-change curve.\nIn this graph I changed the names of the “phases” to make them generic (and not company identifiable), but the cost-of-change curve is the one I drew based on the input my client provided. (Again the fine-grain details are not what’s relevant; what is relevant is the general shape of the curve.)\nIn this example, the cost of change during definition and prototyping is pretty low. The goals of these phases are, respectively, to come up with a killer high-level product definition and to create prototypes of sufficient fidelity to provide confidence that the product can be cost-effectively manufactured at scale. During these phases the teams can operate (and, in my client’s case, currently are operating) in a very agile-like way.\nNotice at Phase3 the cost-of-change curve takes a steep climb upwards. For this particular company, Phase3 is when the teams actually start cutting steel for the injection molds that will be used to produce their product’s plastic housing. Cutting steel (taking a block of steel and cutting out a mold) is expensive in terms of both time and money—about six weeks and tens of thousands of dollars. So, when the teams get to this phase they really don’t want to be making any significant changes in areas where the cost of change has inflected radically upwards.\nCutting steel is just one example of an aggressive inflection in the cost-of-change curve. In other companies, the teams’ cost of change might inflect upward at this point if they are going into the foundry to fabricate custom ASIC chips. Making design changes and having to go back into the foundry to do another spin on the chip is expensive in terms of both time and money.\nTo further the example in the above graph, when the teams begin Phase5, the cost-of-change curve takes another steep upward turn. In this case, Phase5 is where the company places orders for long-lead components needed to manufacture its product. Order volumes could be in the millions of units. At this time, for example, the teams really don’t want to change their choice of microcontroller after the company has already placed a non-cancellable order for one million units!\nTo summarize, by drawing the cost-of-change curve, we can see the low-cost-of-change areas where agile might have obvious application in a HW/FW environment. My experience is that companies doing HW/FW are already operating in a very iterative and incremental manner in the areas where their cost of change is low.\nTools and Technologies Can Influence the Cost-of-Change Curve\nThe tools and technologies employed by teams can significantly affect the shape of the cost-of-change curve. Often when people comment that HW development can’t be agile, they are focusing on fabricating boards or chips. But in the early stages of their development effort, teams are not immediately building boards or chips. Today, teams commonly perform software simulation of electronic and mechanical components and assemblies first. When simulating circuits the cost of change is on par with software change. In other circumstances perhaps the teams are using FPGAs (field programmable gate arrays) either as their target deployment technology or to test-out their logical designs before moving on to more custom development. Changing FGPAs is more like changing software than it is like changing hardware.\nAs tools and technologies improve they tend to flatten the cost-of-change curve, and equally if not more important, such improvements can reduce the overall product development cycle. Revisit the injection-molding example I used earlier with a particular focus on where the cost-of-change curve first inflects upwards in the graph. That inflection point occurs because the technology (injection molding) requires a specific tool (a steel mold to hold hot plastic) to be fabricated.\nNow, fast forward into the future (maybe a decade or so from now). What if by then 3D printers can produce plastic housings at speed, cost, and quality levels that exceed today’s injection molding? If this becomes true, then a major step in the process can be eliminated along with the associated cost of cutting steel and the time required to cut it. More importantly, change at this point in the development cycle becomes economically sensible again (where in the past it was prohibitive). You would be more willing to consider a change to the plastic housing of your product if the majority of the work to create it was performed in a software CAD system and then sent to high-speed 3D printers for fabrication.\nSo not only do we change the nature of the cost-of-change curve by flattening out an inflection point, we very likely remove weeks or months of overall time for getting a new product to market. Remember, in the earlier example, it takes six weeks to cut steel and the company is planning to cut steel twice because the first cut, after inspecting it and trying out the resulting plastic housings, will likely require some adaptation for the final product.\nAdvance the Work Forward to Lower Cost-of-Change Areas\nThere is an additional approach that I use when working with HW/FW organizations to help them better utilize agile. The approach is based on leveraging the principle of “never let an unvalidated assumption live long before you validate it.”\nLet me use failure analysis to illustrate the point. Let’s say you are developing a device that can be dropped (e.g., phone, camera, watch) or can fail in a mechanical way (the door handles of a Tesla car automatically present and retract themselves electromechanically). At some point you need to test the various failure modes of your product. For example, can the Tesla door handles present and retract 100,000 times before there is a failure? Or can you drop the new Apple watch 10 times in a row from a height of one meter and not have the band disconnect from the watch?\nThe obvious (and very late) time to validate whether these characteristics are being met is when you actually have the final product (and before you manufacture at scale). But learning that you have a failure problem this late in the process might put you rather high up on the cost-of-change curve. So, much like we do with software development, we would try to advance forward at least some of the testing (the failure analysis work) to an earlier part of the development process when the cost of change (assuming we find a problem) would be easier to absorb.\nThe good news is that frequently we can advance such work to an early phase. In the specific case of failure analysis, there are some pretty good software tools that can be used to determine whether or not your design will likely have a problem under certain conditions (e.g., dropping the watch from a height of one meter). Using these tools won’t eliminate the need to do failure testing on physical instances of the product, but it would hopefully raise your confidence level that failure testing done late will confirm the robustness of your product, rather than exposing now-expensive-to-change problems.\nAgile can and is being used in HW/FW environments to develop products. To understand where and when your teams can use agile, first draw your cost-of-change curve based on your current process and annotate it with the tools and technologies that are drivers of the costs. Then start looking for opportunities to leverage agile approaches in all of the low-cost-of-change areas. Next, look at the high-cost-of-change areas and see if applying agile principles there can move work to areas of a lower cost of change."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:0fa627b3-98ea-4674-804d-813c9d67cda5>"],"error":null}
{"question":"How does establishing daily routines help in a preschooler's development and learning process?","answer":"Daily routines are essential for preschoolers' development because they provide a sense of control and set children up for success. Since young children have little control over their lives and are reliant on adults, predictable routines help them feel more secure. Routines themselves are learning opportunities where children practice transitioning between activities, cleaning up, and developing self-care skills like washing hands before snacks. Through regular routines like morning activities (going to the potty, getting dressed, eating breakfast), children learn essential life skills and independence. Additionally, giving specific tasks within these routines, such as feeding the dog or putting dirty clothes in the hamper, helps them feel important and empowered while teaching them about daily responsibilities.","context":["Your little toddler doesn’t seem so little anymore. They just celebrated their third birthday and you’ve lost track of how many months old they are. Your child isn’t quite a kid, but definitely no longer a baby. You’ve got a preschooler!\nA child of 3 to 5 years of age is considered a preschooler. So whether or not your child is attending a formal preschool program, they are no longer a toddler. Preschoolers are different from toddlers in that they are developing the basic life skills, independence, and knowledge that they will need as they enter their school years. Though they are maturing physically, emotionally, and cognitively, play remains one of the primary ways they will be learning and experiencing the world.\nWhat are developmental milestones for a preschooler?\nPreschoolers are learning many new skills and stretching their cognitive abilities. Though there are the major skills to look out for, be aware that every child develops differently, and yours might accomplish one skill earlier than others. Don't worry about small differentiations from the norm, but if you have concerns about the overall development, consult your pediatrician.\nAt 3 years-old, your child will probably have the fine motor skills to dress themself and the gross motor skills to pedal a tricycle. You’ll notice your child is more interested in interactive play rather than parallel play, and asking deeper questions about their environment. You may have already noticed an uptick in the number of times your child asks, “Why??”\nBy age 4, a child is likely able to dress and undress, cut basic figures out of paper and paste them on another piece of paper, draw little stick figures, name four or five colors, and understand simple joke structures. At age 5, kids will mostly be able to count, draw a person with the arms, legs, and body in the right places, exhibit imaginary and pretend play (sometimes with an imaginary friend), ride a two-wheel bicycle with training wheels, and articulate well enough to be understood.\nHow do preschoolers learn best?\nEveryday life skills\nThe preschool age is a time for rapidly growing independence; your child learns to separate from you. Young children, preschoolers included, have so little control over their lives– they are reliant on the whims of the adults around them. Having a predictable routine gives a child a sense of control over their days, and sets them up for success. In fact, a lack of routine can lead to a struggle with regulation that detract from everyone’s learning and happiness.\nBeyond this, the routine itself is part of the learning. Giving a child a chance to practice moving from one thing to another, cleaning up when it’s time to clean up, learning important self-care skills like washing hands before snack– these are all routines that help a child become simultaneously independent and a member of the group.\nDuring the preschool years, she will learn essential life skills, like dressing and feeding herself. Because children learn best when there are clear rules and expectations, establish regular routines. The morning routine can involve going to the potty, getting dressed, and eating breakfast -- all skills that your child will eventually be able to do on her own. Give some specific tasks that will make her feel important and empowered, like feeding the dog or putting dirty pajamas in the hamper. Simple chores can help her feel as though she has a daily contribution to make.\nIn addition to using life skills as learning opportunities, play is still a major player in supporting the way preschoolers naturally develop. The three main categories of development are cognitive, physical, and social-emotional. All three of these are developing simultaneously, and none of these areas develop in a silo. They are interconnected. A really clear example of two areas of development that might initially seem unrelated, but are, in fact, very related is language development and physical development. The development of verbal speech relies on the physical development of the mouth, tongue, and throat. Having the right oral muscle development allows a child to experiment with sounds that eventually become speech. By experimenting with sounds, infants and toddlers are also building the muscles they will need for speech.\nPlay allows for this holistic development because it naturally encompasses all three domains. And if that’s not convincing enough, the Journal of the American Academy of Pediatrics (AAP) has said, “Play is so important to optimal child development that it has been recognized by the United Nations High Commission for Human Rights as a right of every child.”\nWhat is sensory play?\nFor young children, play is often a full body activity that helps them develop skills they will need later in life. Running, dancing, climbing, rolling—these activities all foster muscle development and help fine-tune motor skills. Children also build their mental and emotional muscles as they create elaborate, imaginative worlds rich with a system of rules that govern the terms of play.\nSensory play includes any activity that stimulates a young child's senses of touch, smell, taste, sight and hearing, as well as anything which engages movement and balance. Children, and even adults learn best and retain the most information when they engage their senses. Providing opportunities for children to actively use their senses as they explore their world through ‘sensory play’ is crucial to brain development – it helps to build nerve connections in the brain’s pathways. This leads to a child’s ability to complete more complex learning tasks and supports cognitive growth, language development, gross motor skills, social interaction and problem solving skills.\nWhat are sensory play ideas for preschool aged children?\nWhen thinking about providing sensory play opportunities for preschoolers, here are some learning domains and play activities to consider:\nFine Motor is the development of the small muscles in the hands.\nGross Motor is the development of the large muscle groups in the body.\nPre-math is the development of knowledge, skills, and concepts related to more formal academic math.\nPlaying “I Spy”\nComparing sizes and shapes\nPre-literacy is the development of knowledge, skills, and concepts that lay the foundation for reading and language development.\nReading and listening to stories\nExploring writing (scribbling, making letters, etc.)\nConversations and questions, especially ones that help expand vocabulary\nSocial-emotional development is all about managing one’s emotions and building positive relationships with others.\nPaying attention to others’ emotions and talking about them\nAdult-supported conflict resolution\nListening to stories and talking about character’s emotional experiences\nWhether they engage at school or at home, sensory activities facilitate exploration and naturally encourage preschoolers to use scientific processes while they play, create, investigate and explore. They allow this special age group to refine their thresholds for different sensory information, helping their brain to create stronger connections to process and respond to sensory information that will ultimately prepare them not only for grade school academic learning but also nurture healthy social skills and foster good relationships with others, as well as their own selves. Contact us today to learn more!"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:7263697d-8bb8-4798-98a1-825a89f421fc>"],"error":null}
{"question":"What role do institutional checks and balances play in governance, and how does modern technology affect family communication patterns?","answer":"The Electoral College was established as part of the Constitution's system of checks and balances, preventing Congress from directly choosing the president while maintaining state representation. Each elector cast two votes, with federal officials banned from serving as electors, and meetings held at the state level to prevent centralized control. Regarding modern communication patterns, technology has significantly altered family interactions, with observations showing families immediately turning to mobile devices in social settings like restaurants, reducing face-to-face communication. Psychologists and experts note that traditional family time contributes to less depression, better academic performance, and happier children, but this is being compromised by excessive media use, requiring parents to establish parameters for appropriate media use.","context":["The Electoral College is one of the most controversial and ridiculed parts of the Constitution. It is not just for the obvious reasons – the “wrong winner” elections of 1876, 1888 and 2000. The college has also focused the nation’s attention on a few swing states, allowing candidates to ignore a large percentage of the voters in their campaign. Today, the three largest states get little attention, except as ATMs for campaign fundraising.\nCriticism of the Electoral College is rampant, and defense of it rarely goes beyond “why change?” What is rarely discussed is why it was created and what it was designed to do. The college was very much a part of the philosophy of the Constitution – an intelligent compromise between many competing interests, part of the system of checks and balances.\nWhat would eventually be called the Electoral College generally escaped criticism in the bitter ratification debates after the Constitution was written. Alexander Hamilton was particularly proud, noting that it was “almost the only part of the system, of any consequence, which has escaped without severe censure.” The “anti-Federalist” opponents of the Constitution tacitly agreed with the college concept. This may not be surprising because Congress, not the executive branch, was supposed to be the locus of power.\nHowever, an even more important reason to have an Electoral College was because the young nation had little experience in directly electing executives.\nIn most states, the governors were not chosen by the voters. Instead, in eight of the 13 original states, the legislature chose the governor. And in two of the other states, if no candidate received an absolute majority of votes, the legislature ended up making the choice. This was the model the constitutional conventioneers drew on. The original plan that was brought to Philadelphia, and the first outlines of a presidency adopted by the convention, provided for election of the chief executive by Congress.\nThe option of choosing a president by popular vote was voted on a number of times during the convention, but only two states were in favor of it. There were a number of reasons to oppose it. Many of the conventioneers believed the country was too large to directly elect the president. Some Southerners realized their states’ impact would be diluted, as the compromise that counted three-fifths of slaves as part of the population for representation and taxation purposes gave the slave states more impact in Congress than they would have in a popular vote.\nSome states’ rights advocates wanted the states to have more of a say. Small states were concerned that their votes would be drowned out, with a few large states able to gang up to select a candidate; the same impetus helped push forward the idea of equal representation in the U.S. Senate. And still others simply did not trust the people to choose properly.\nBut the idea that Congress would choose the president failed the checks-and-balances test. The conventioneers were worried that a congressionally chosen president would be basically owned by his selectors. So they created the Electoral College to act as an alternative Congress. It contained exactly the same number of members as Congress. As an added safeguard, federal officials were banned from serving as electors, and the electors did not meet as a national group but rather met in each state.\nEven with this alternative, Congress still retained a potentially large role. Each elector cast two votes, one of which had to be for someone from another state. The second-place finisher became vice president. There was a further expectation that the electors’ votes would be divided among favorite sons. If this came to pass, the electors would have served as a nominating committee. The top five candidates would be sent to Congress, which would select a president in a state-by-state vote of the House delegations. The winning candidate needed an absolute majority of the states.\nThe system was not perfect. Congress was called on to select a president in 1800 and 1824. In the first instance, Thomas Jefferson and his running mate, Aaron Burr, both got 73 electoral votes. That election led to the creation of the 12th Amendment, which divided up each elector’s vote into one for a presidential candidate and one for a vice presidential candidate and lowered the number of candidates sent to Congress from five to three. In 1824, there was a four-way split, which played a large role in helping to create our current two-party political system. Congress also played a role in the disputed election of 1876 – though that was ultimately decided by a commission.\nIf the vote Nov. 6 is close, the Electoral College is guaranteed to come under increased scrutiny and another avalanche of complaints. Although the likelihood of change is small, it bears considering how the college came into being and that it met the founders’ basic goal of a system of checks and balances.\nMCT Information Services\nJoshua Spivak is a senior fellow at the Hugh L. Carey Institute for Government Reform at Wagner College. He writes the Recall Elections Blog, and he wrote this for the Los Angeles Times.","Hansa Bhargava MD FAAP\nStaff Physician, Children's Healthcare of Atlanta\nMedical Editor, WebMD\nPediatricians are seeing more and more teens suffering from stress. Whether they are complaining of it or having somatic symptoms such as headaches and stomach aches, it seems that stress and anxiety are on the rise. We know that over scheduling, homework, and the pressures of getting into college can contribute to this. But can media also affect it? Is screen time and media a stressor or a remedy for stress?\nIn a recent WebMD survey published in their Teens and Stress report, 54% of teens were stressed according to parents. Interestingly, 40% of parents turned to the screen for family stress relief while 58% of teens did. Social media and texting was used as stress relief by almost half the teens. This is on the heels of the Common Sense Media survey reporting that US teens were using media for 9 hours a day. Other recent reports have shown that 94% of teens with mobile devices are online daily with many online constantly.\nSo it seems that stress is on the rise and media use is on the rise. Although there may not be a direct relationship, some real issues impact stress and anxiety. Consider this: 23 % of teens report cyberbullying, especially girls. There have been reports of “Facebook depression” and loneliness, as kids who aren’t in social media conversations may feel left out. Other negative consequences can also have an impact: many teens are in front of a screen late at night or ‘sleep text’, both of which can contribute to lack of sleep, which in turn can decrease focus and potentially cause irritability and depression. And last but certainly not least, what about the time media consumes?\nTime spent on media is time often not spent communicating with family. Lately, when I’ve gone into a restaurant, I’ve observed that as soon as a family sits down, everyone pulls out a mobile device. No one is really talking. So even the short amount of time not doing homework, playing soccer, or at school is being compromised. Psychologists, community leaders and experts have long reported that family time can contribute to less depression, less anxiety, better academic performance and generally happier kids. But what if that family time is on media??\nAs the AAP reviews our screen time recommendations, I feel that we, as pediatricians should continue to advise parents about basic principles.\nParents need to lay down some parameters about when and how media is used. Media is a centerpiece of teens’ lives and is not going away, but just as we don’t give our kids a set of keys to our car and say “just drive”, we need to enforce appropriate media use. And good modeling is also critical: parents need to put down their mobile devices and simply communicate with their kids. Old fashioned parenting and just talking to your kids can build the foundation to a less stressful childhood and hopefully a happier life."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:955898df-edb5-4162-b31d-490d456a07ea>","<urn:uuid:9ee5a44c-ac0e-4471-bc2a-f3b31e423c46>"],"error":null}
{"question":"What chemical compounds are responsible for the pleasurable effects in both tea and chocolate? I'm curious about the science behind their feel-good properties!","answer":"Both tea and chocolate contain methylxanthines, which are stimulant compounds. In tea, these include caffeine, theobromine, and theophylline, making up 2-5% of the dry weight of fresh leaves. In chocolate, there are over 300 different chemical compounds, including theobromine and methyl-xanthine (which are mildly addictive caffeine-like substances). Chocolate also contains phenylethylamine, a stimulant similar to dopamine and adrenaline that affects the brain's mood centers and induces emotions similar to falling in love.","context":["Tea chemistry is complex. Just how complex? Well, on the bush, tea leaves contain thousands of chemical compounds, when they are processed, these compounds break down, form complexes and form new compounds. When we steep tea leaves, our senses are tingled by the thousands of volatile compounds (collectively known as the “aroma complex”) from the tea liquor and the thousands of non-volatile compounds and the complexes between them, not all of which are water soluble, and the ones that are water soluble are soluble at a function of the properties of the water used for steeping like temperature, total dissolved solids, pH, etc.\nSo all of this makes it very difficult to generalize and say that x chemical is responsible for y taste. Many tea chemicals have been categorized into broad groups, and collectively we have some idea of what happens to these groups during processing and what flavors and aromas they are responsible for. As tea gains popularity, there is no doubt that more research will be done on tea chemistry and we’ll have a clearer picture of what is going on chemically from the field to the cup.\nPlant leaves are made up of mostly water, when they are removed from the plant they begin to wilt and lose water. Tea leaves are no exception to this. In the field, they are made up of mostly water, when they are plucked the leaves begin to lose water or wilt, a process called withering in the tea industry. As tea leaves wither, their cell walls begin to break down and the chemical components inside come in contact with oxygen and each another, spurring on a group of reactions we call oxidation. Over the years, tea producers have learned to control the natural tendency of tea leaves to wither and oxidize in order to produce a finished tea that has a desirable appearance, aroma, flavor, and taste using methods we’ll refer to as tea processing.\nAmazingly, for hundreds of years tea makers have produced drinkable teas using principles of withering and oxidation with no knowledge of the underlying chemistry. From what we know today, the most important compounds in fresh tea leaves responsible for producing teas with desirable appearance, aroma, flavor, and taste are: polyphenols, amino acids, enzymes, pigments, carbohydrates, methylxanthines, minerals and many volatile flavor and aromatic compounds. These components undergo changes during tea processing to produce what we’ll call a ‘finished’ or ‘made’ tea – one that has been processed and is ready for packaging or steeping. Let’s take a look at each of these compounds beginning with the most abundant, polyphenols.\nIn steeped tea, polyphenols are largely responsible for astringency. The term polyphenol simply refers to a categorization of compounds composed of many phenolic groups, hence the name poly-phenol. These compounds are plant metabolites produced as a defense against insects and other animals and are the most abundant compounds in tea comprising as much as 30-40% of both freshly plucked tea leaves and solids in tea liquor1. They are derived from amino acids via sunlight and therefore tea grown in the shade has a smaller concentration of polyphenols and a higher concentration of amino acids2. The bud and first leaf have the highest concentration of polyphenols and polyphenol levels decrease in each leaf moving down the plant3. There are an estimated 30,000 polyphenolic compounds in tea4, flavonoids are arguably the most important group of polyphenols in tea and are the source of the many health claims surrounding tea, and specifically tea antioxidants. Within the flavonoid group, flavanols (also known as flavan-3-ols) are the most prevalent. Flavanols are also referred to as tannins, and during oxidation are converted to theaflavins and thearubigins—the compounds responsible for the dark color and robust flavors notably present in black teas. The major flavanols in tea are: catechin (C), epicatechin (EC), epicatechin gallate (ECG), gallocatechin (GC), epigallocatechin (EGC), and epigallocatechin gallate (EGCG). EGCG is the most active of these catechins and is often the subject of studies regarding tea antioxidants. Tea flavanols are sometimes collectively referred to as catechins. Besides flavanols, tea flavonoids also include flavonols, flavones, isoflavones, and anthocyanins; all of which contribute to the color of a tea’s infusion and its taste.\nAmino acids give tea its brothiness, or umami taste. Tea leaves contain many amino acids, the most abundant of which is theanine. Camellia sinensis, a mushroom called Boletus badius, and an plant called guayusa (which is often processed made into a tisane) are the only three natural sources of theanine found thus far in nature. In the tea field, sunlight converts amino acids to polyphenols, and as such; shade grown tea contains more amino acids than tea grown in direct sunlight. Some tea bushes are even deliberately shaded for several weeks before harvest to enhance the tea’s amino acid content. Theanine, more specifically L-Theanine is responsible for promoting alpha brain wave activity which promotes relaxation. L-Theanine in concert with caffeine can induce a state of “mindful alterness” in the tea drinker. In steeped tea, amino acids make up 6% of the extract solids1.\nPolyphenol oxidase and peroxidase are the most important enzymes in tea leaves. They are responsible for the enzymatic browning of tea leaves that takes place when the cell walls in the leaves are broken and the polyphenols are exposed to oxygen – otherwise known as oxidation. These enzymes may be denatured or deactivated using heat so that browning cannot occur; this is one of the first steps in green tea production and is why finished green tea leaves remain green. The enzymes may also be denatured by simply depriving them of moisture for a time which is what happens during the long withering period in white tea production.\nPlant pigments are responsible for absorbing light for photosynthesis. Pigments also give leaves their color. There are two major groups of pigments in fresh tea leaves: chlorophylls and carotenoids. These pigments condense during withering and oxidation and become darker. During oxidation, the green color of tea chlorophylls is converted to black pigments known as pheophytins. This conversion leads to the dark appearance of finished oxidized teas. Tea carotenoids are another pigment group found in tea leaves and are mainly composed of carotenes which are orange and xanthophylls which are yellow and are also responsible for the color of finished tea leaves.\nAll plants store energy formed during photosynthesis in starches and sugars, otherwise known as carbohydrates. Plants later use this stored energy to fuel important reactions, in tea, carbohydrates help to fuel the enzymatic reactions that take place during oxidation and are also responsible for the creation of polyphenols in young tea leaves. Carbohydrates make up on average 11% of extract solids in steeped tea1 and lend to its sweetness.\nMethylxanthines in tea include the stimulant caffeine and two similar compounds: theobromine and theophylline. The tea plant creates these chemicals as a natural combatant towards insects and other animals. On average, methylxanthines in tea leaves make up 2% to 5% of the dry weight of the fresh leaves5. Methylxanthines also contribute to a bitter taste in the tea infusion. Levels of these compounds depend on the variety and cultivar of Camellia sinensis used, climate, age of the leaves, and the propagation method (seed vs. cutting) used on the plant.\n28 mineral elements have been found in the tea flush5. Compared to other plants, tea has a higher than average amount of: fluorine, manganese, arsenic, nickel, selenium, iodine, aluminum, and potassium5. Tea also has an unusually high amount of fluorine, which has been known to help prevent tooth decay in humans, however too much fluorine can be harmful. It is important to note that fluorine occurs in greater amounts in older tea leaves. Tea minerals vary greatly with each harvest and change greatly during processing.\nThe volatile substances in tea leaves are largely responsible for a tea’s flavor and aroma. The aroma complex of tea is made up of hundreds (maybe even thousands) of flavor and aroma compounds that exist in trace amounts. Many of these aromatic compounds do not exist in fresh tea leaves and are derived from other substances during processing. The flavor and aroma of each tea depends on a wide variety of combinations of these compounds, hence the name aroma complex. Compounds such as, linalool and linalool oxide are responsible for sweetness; geraniol and phenylacetaldehyde are responsible for floral aromas; nerolidol, benzaldehyde, methyl salicylate, and phenyl ethanol are responsible for fruity flavors; and trans-2-hexenal, n-hexanal, cis-3-hexenol, and b-ionone are responsible for a tea’s fresh flavor6. When studying tea’s aroma complex, it is sometimes broken into two parts: primary aroma (from fresh tea leaves) and secondary aroma (products of manufacture). Regardless, more and more research is being done on tea volatiles and how our olfaction system works in general, so we may expect some clarity on this issue in the coming years.\n- Harbowy, Matthew E., and Douglas A. Balentine. “Tea Chemistry.” Critical Reviews in Plant Sciences 16, no. 5 1997: 415–480\n- Ercisli, Sezai, Emine Orhan, Ozlem Ozdemir, Memnune Sengul, and Neva Gungor. “Seasonal Variation of Total Phenolic, Antioxidant Activity, Plant Nutritional Elements, and Fatty Acids in Tea Leaves Grown in Turkey.” Pharmaceutical Biology 46 (2008): 683–687\n- Bhatia, I.S. “Composition of Leaf in Relation to Liquor Characteristics of Made Tea.” Two and a Bud 83 (1961): 11–14.\n- Uncovering the secrets of tea – http://www.rsc.org/chemistryworld/2012/11/tea-health-benefits\n- Zhen, Yong-su. Tea: Bioactivity and Therapeutic Potential. London: Taylor & Francis, 2002\n- “Tea Chemistry – Tocklai”. Tocklai Tea Research Association, n.d. http://www.tocklai.org/activities/tea-chemistry/","A guide to chocolate – the journey from pod to palate.\nBy Sara Jayne Stanes, Founder, The Academy of Chocolate\nChocolate is unique. It is the only substance that melts in the mouth at body temperature, gently exploding into a warm, sensual liquid. This singularly hedonistic and deeply satisfying experience has earned chocolate a role in everything from seduction and the demise of slavery to a venerated staple of the herbal pharmacopoeia. Of course, we are talking really serious chocolate here.\nWhat is fine chocolate?\nChocolate is ‘cocoa mass’ or ‘cocoa liquor’ – a combination of the roasted and ground kernel of the ‘cacao’ bean, the principal part of which is cocoa butter (i.e. the fat released when the bean is ground) and sugar. This is then refined and processed. Chocolate may also contain lecithin – a natural emulsifier – as well as flavours like vanilla, and in the case of milk chocolate, milk solids.\nChocolate itself of course doesn’t grow anywhere. It is cocoa, or ‘cacao’, as it should be known, which grows on trees. Its name derives from ‘Theobroma Cacao’ (food for the gods) and is made from the seeds of the rainforest tree grown 20 degrees north and 20 degrees south of the Equator. There are three major varieties: Criollo, Trinitario, and Forastero. These varieties are generally separated into what is known as fine beans (the first two) and bulk beans (normally Forastero). Of these three varieties of cacao, there are over 280 hybrids. Cocoa beans are very susceptible to hybridisation and it is debatable whether there are any true Criollo beans still growing (with the exception of very remote parts of Venezuela’s borders with Colombia and Bolivia).\nThe history of chocolate\nThe name ‘chocolate’ most probably comes from the Olmec/Mayan/Aztec hybrid: xoco-atl (whoko-atll) meaning ‘bitter water’ – a fatty, grainy drink made from the crushed roasted and ground cocoa beans, sometimes with the addition of herbs and spices. Sugar (from the Caribbean) was not known in Central and South America until well after the Spanish invasion. When Hernan Cortes and his men invaded Mexico in 1518, they found cocoa was used also as currency. Montezuma was reputed to drink several cups of foaming ‘xoco-atl’ a day before visiting his harem of wives!\nWhile geographical discoveries point to the existence of ‘cacao’ around 6,000 years ago, it was the Olmecs (by Veracruz, Mexico) who were the first recorded people to have found uses for chocolate circa 1500 BC. Remains of cacao have been found in the graves of their priests and are thought to be gifts for the gods on their journey from earth to the afterlife. Cacao was used in ceremonial occasions as offerings to the gods just as communion bread and wine traditionally represent the body and blood of Christ.\nThe Mayans believed that for the sun to rise every morning, cacao had to be prepared and offered to replace the blood that the sun lost in its overnight fight with the jaguar. In the 9th century, the god of cacao, Queztalcoatl, introduced the cacao seeds to humans and showed them how to use it.\nBanished from earth by a party of jealous gods from ‘eden’, he vowed to return to earth disguised as a ‘fair-skinned, bearded man’ to save the people. This gave rise to the legend that Cortes was the resurrected god, Queztalcoatl, and gave him and his Spanish army free passage to capture and colonise Mexico City (then known as Tenochtitlan).\nMany other uses for chocolate have been recorded especially for medicinal remedies. Rather than illness being caused by disease, Native Americans viewed health as the state of being in balance with the environment. Losing that balance – perhaps through a ‘perturbed’ diet – could create sickness. Chocolate was viewed as one means for restoring lost balance. As we know now, they were more accurate in their analysis of chocolate than we gave them credit. Among the conditions cured by chocolate were tuberculosis, toothaches, and ulcers. It was also alleged to cure itches, repel tumours, and foster sleep.\nBy the 1680s, reports emerged that chocolate could restore energy after a day of hard labour, alleviate lung inflammation, and even strengthen the heart. By the 1800s, cocoa was being mixed with ground amber dust to relieve hangovers. Combined with other ingredients, it also became the basis of treatments for syphilis, hemorrhoids, and intestinal parasites.\nChocolate arrived in England around the middle of the 17th century – the same time as coffee and tea. Travellers brought home recipes for the strange drinks, together with tales of their reputed therapeutic prowess. It’s really no wonder that chocolate became such an important part of the apothecary’s medicine chest. It’s also no coincidence that many apothecaries were Quakers, who as part of the Quaker religion, were not permitted to enter certain trades and professions, including the armed forces, and were not allowed to go to university.\nAt the time of the industrial revolution in the early 19th century, many people across England and elsewhere in Europe moved from areas full of countryside known for their agriculture to towns. At this time, conditions became squalid and people turned to drink for relief. Names like Cadbury, Fry, and Rowntree were all devout Quakers and were familiar with and proficient at using cocoa.\nNaturally, with the advent of machinery, they discovered ways of making the fatty grainy drink into something far more palatable. Thus more work for the labourers and an alternative to the demon alcohol. It was this desire for the smooth drink that led to the discovery of chocolate – allegedly by the grandson of the founder, Joseph Fry in c. 1840, as a by-product in the form of a pastille to eat and enjoy.\nQualities and varieties\nCocoa grows on trees some 20 metres high in the wild and 3-8 metres under cultivation in parts of the world close to the Equator (20 degrees north and 20 degrees south). Its indigenous home is Central and South America, with the best beans coming from Venezuela, Grenada, Ecuador, Belize, Colombia, and the Caribbean. The very first cocoa beans were cultivated from Brazilian beans in Portuguese West Africa as late as 1840. Today, the largest producers are the Ivory Coast, Ghana, Indonesia, and Brazil.\nThe three main varieties of beans are as follows:\n‘Native’ or ‘of local origin’, this high-quality, aromatic chocolate, which is substantially lacking in bitterness, is grown in Central America and a few regions of Asia. It is likely originally from Mexico, but now represents less than 5% of the world’s production. Criollo is rarely used alone due to its scarcity and expense. It is finicky to grow and doesn’t adapt well to different climates.\n‘Foreigner’ or ‘stranger’, this is ordinary, everyday cocoa, which originates from the Amazon. Forastero represents 80% of the world’s cocoa and has a slightly bitter flavour. In the context of coffee, it is often referred to as ‘cocoa’s robusta beans.’\nTrinitario was developed as a hybrid of Criollo and Forastero in Trinidad as a result of the near-total destruction of the Criollo plantations by a hurricane in 1727. Seeds for new plantings were brought from Venezuela and cross-fertilised, with the final result encompassing characteristics of both varieties. Trinitario represents about 10-15% of the world’s total cocoa beans. Its blends are fine and rich in fats and it makes great chocolate.\nFrom this small parentage, there are thousands of hybrids, all affected by different soil and climates. This makes the identification of cocoa a complex process.\nThe flavours of the beans are affected by the location, climate and soil in which they are grown. It is important to remember that like fine wines, the vintages vary from year to year – sometimes subtly, and sometimes dramatically.\nThe journey from pod to palate\nCocoa trees produce thousands of tiny, delicate flowers that develop into pods. Resembling rugby balls, pods grow directly from the lower branches and the trunk. They contain between 25-40 seeds, roughly the size of olives, which are formed like a giant corn on the cob. The pods weigh between 200-800g and ripen after 5-6 months.\nThe taste of the juice and the freshly-picked beans is similar to tropical fruits such as sweet milky lychees, pineapple, and melon. After a few hours, they become very bitter and almost inedible. One tree produces only enough beans for 1kg chocolate per year.\nIn the next stage of the process, pods are harvested and cut open with a machete. Fresh beans are then scooped out and laid on the ground, or put in special boxes to ferment for between 5-9 days, depending on the variety. It’s the fermentation process that develops the full flavour profile. After that, seeds (or beans) are laid out on the ground to dry in the sun for several days. They are evenly and regularly raked over before being shipped and transported to the factory.\nThe chocolate-making process\n1) To make the majority of commercial chocolate, ‘anonymous’ beans from different countries are blended. The exception is for origin or single bean chocolate varieties (although these high-quality products represent a tiny percentage of the chocolate produced).\n2) The next stage of the process is cleaning, which removes foreign bodies like unwanted twigs and stone.\n3) Roasting time and temperature are also important to the end product. The higher the temperature, the more bitter the chocolate and the more need for sugar generally. The best chocolate is made from beans roasted at a lower temperature for a longer time, thus creating a richer flavour. In the case of the latter, less sugar is generally needed.\n4) Kibbling/shelling/winnowing separates the nib from the shell.\n5) Grinding of the cocoa nib then produces cocoa mass (or liquor). An average cocoa bean contains 55% cocoa butter.\n6) Now the mass (liquor) takes one of two journeys. It’s either used to make chocolate, or it is pressed to make cocoa cake or cocoa butter.\nThe ground cocoa mass is mixed with sugar and kneaded into a ‘dough’. The dough is then refined through five revolving steel rollers which reduces all the solid particles to 20 microns (less for top-quality chocolate). The ‘mixture’ that comes off at the top of the fifth roller is in powder or flake form which is barely perceptible to the touch.\n‘Conching’ is essential at this stage for smoothness. The process uses paddles that resemble shells or ‘conches’ (the origin of its name) to thoroughly mix and refine the chocolate mixture to perfection. Depending on the producer’s recipe, more cocoa butter can be added at this stage. Generally, the longer the ‘conch,’ the better, but not so long ago, it was a sign of good chocolate when it was conched for four or five days. Today’s technology has changed that to 8 or 10 hours. The conching time should be long enough to drive off the unwanted volatile odours and bitterness, but not too long to damage some of the more complex chocolate flavours.\nLiquid chocolate is then loaded onto thermostatically-controlled tankers for national and international distribution, or chocolate is tempered to ensure proper crystallisation of cocoa butter and its homogenous dispersal throughout the chocolate. Chocolate is then moulded into the familiar blocks or pistoles (chips) that we buy.\nFor cocoa cake or cocoa butter:\nThe second route for the liquor is ‘pressing’ in which immense hydraulic pressure is exerted to produce cocoa cake and cocoa butter. Cocoa cake is crushed into a fine powder, before being alkalised (also known as ‘Dutching’ after Van Houten, who invented it at the beginning of the 19th century). Alkalisation mellows the flavour, makes it more digestible because it becomes soluble, and, importantly, darkens the colour. The process was ‘invented’ based on the customs of the Aztec medicine men who used to add wood ashes from the fire to chocolate to make it more palatable.\nCocoa butter is deodorised and refined for use in white chocolate and also added to some dark chocolate.\nSo is chocolate good for us?\nPeople should certainly not feel guilty about eating it. Below are just some of the ways in which the consumption of chocolate can benefit health.\nChemicals called flavanols, present in cocoa drinks, and to a lesser extent, in chocolate, can boost the production of nitric oxide which is crucial to the regulation of blood pressure. Research by Harvard Medical School has shown that the benefits can be as great as those of aspirin.\nDeep vein thrombosis\nThe chances of developing this condition can also be offset by the flavanols, according to research at the University of California Davis. A 50g bar of chocolate contains the same concentration of chemicals as two glasses of red wine, four-and-a-half cups of tea, six apples, or seven onions.\nFlavanols are also known to improve the cardiovascular system and are said to help prevent coronary heart disease.\nScientists at the University of California have also detected that many of chocolate’s naturally occurring chemical compounds, such as opiates, are also found in cannabis. Opiates and cannabinoids trigger different receptors in the brain, which is “therapeutically interesting.”\nIn the meantime, individuals wishing to self-medicate with non-prescription-strength chocolate should reach for cocoa or dark chocolate, which can contain 2-3 times more of these compounds per ounce than milk chocolate. The chocolate you find in industrial newsagents is also much less compound-rich, while containing far more sugar.\nTasting: the senses\nThe cocoa bean naturally contains over 400 distinct aromas – at least twice as much as any other gems of nature. The rose has only 14, and the gastronome’s staple, the onion, only half a dozen.\nThe taste of chocolate is equally complex as a result of the presence of over 300 different chemical compounds, including theobromine and methyl-xanthine – two mildly addictive caffeine-like substances – and phenylethylamine, a stimulant similar to the body’s own dopamine and adrenaline. Phenylethylamine strikes the brains’ mood centres and induces the emotion of falling in love, a matter of only partly understood brain chemistry. Then there is the actual physical pleasure of feeling the chocolate melt in the mouth, a perculiarly seductive sensation.\nMany of these chemical compounds are identical or similar to those found in fruits, vegetables, spices, herbs, and other substances. That’s why we chocolate enthusiasts compare the aromas of\ndifferent chocolates to those in toasted bread, grass, wood (bark), leather, melon, mango, citrus, cherry, red berry, plum, raisin, honey, peach, vanilla, butterscotch, herbs, and wine, to name but a few. We’re not being fanciful; there’s a chemical correlation underlying the comparison, and this fact explains the rich metaphorical language used to describe a chocolate’s sensory characteristics.\nWe all taste things differently according to our mood, our own individual physiological make-up, and the time of day, which is why ‘discovering’ chocolate is so much fun and tells us not only about the chocolate but also about ourselves!\nAnd who said chocolate was junk food?\nAbout Sara Jayne Stanes\nSara Jayne Stanes is the founder of The Academy of Chocolate and CEO of the Royal Academy of Culinary Arts, a food writer, author of award-winning book, ‘Chocolate – the Definitive Guide’, and a Chocolate ‘Evangelist’.\nSara was awarded an OBE in the Queen’s Birthday Honours in June of 2007, was made an Honorary Freeman of the Worshipful Company of Cooks in 2006, and was awarded the Special Award in the 2011 Catey Awards. She was also awarded an Honorary Doctorate by the University of West London where she is also a Visiting Professor.\nThe Royal Academy is Culinary Arts is Britain’s leading association of head chefs, pastry chefs and restaurant managers whose principal aims include raising the standards of food, cooking, and service through education and training, and providing career opportunities for young people. The Academy runs awards schemes, specialised apprenticeships, and ‘Chefs Adopt a School’ (of which Sara is a Trustee). As well as being the Academy’s charity, it is a schools project that teaches children about food in a holistic sense, where it comes from, and the importance of cookery through ‘discovering tastes’ sessions. The RACA supports the Gold Service Scholarship (of which Sara is also a Trustee) started in 2012 to reward talented young professionals in hotel and restaurant food service.\nAs a food writer and chocolate truffle maker, Sara’s love affair with fine chocolate stretches over 20 years. She evangelises about it whenever she can and to whoever will listen! She is passionate about the provenance of ingredients, origins, varieties, breeds, organics, and, above all else, chocolate. She has written ‘Chocolate – The Definitive Guide’, which contains a foreword by Michel Roux OBE. It is the story of 3,500 years of cocoa to chocolate from pod to palate, covering its history, geography, social, and religious applications, nutritional values, tastings, wines pairings, ‘do’s and don’ts, and recipes. ‘Chocolate’ won the Guild of Food Writers Jeremy Round Award in 2000. Sara regularly broadcasts and writes about chocolate, as well as giving lectures, seminars, and tastings.\nSara campaigns to encourage people to ‘look beyond the label’ of their chocolate confectionery. Currently, like much of agriculture across the world, cacao farmers are paid a pittance (average $1.4 per kilo). Between $4-6 would ensure that they maintain a decent standard of living, and would ensure that they would get healthcare and a proper education for their children. Subsequently, she and a group of fellow chocolate lovers started an Academy of Chocolate in 2005. It aims to promote the story of chocolate and ensure that everyone knows the difference between ‘real’ chocolate and chocolate-flavoured confectionery. It is a long journey from pod to palate!\nSara lives in Clapham, London with her chocolate-loving, wine merchant husband, Richard, and their Battersea dog Montezuma."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"format_constrained"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:339a10d6-29f3-4b7b-b8bf-d718e2eaf797>","<urn:uuid:f79962c1-cf29-4978-8d15-5d3055a454be>"],"error":null}
{"question":"I've noticed some faint glows in the night sky this month. Could it be the zodiacal light or northern lights, and how can I tell the difference?","answer":"The zodiacal light and northern lights have distinct visual characteristics that can help you identify them. The zodiacal light appears as a cone-shaped glow along the ecliptic (the sun's path), tilted to the left and visible near the western horizon, particularly in February and March for Northern Hemisphere viewers. You might also see a faint glow called the gegenschein opposite to the sun's location. In contrast, the northern lights (Aurora Borealis) appear as colorful, moving displays, typically showing green and pink colors when particles collide with oxygen at lower altitudes, and blue and purple colors when interacting with nitrogen at higher altitudes. The northern lights are primarily visible in high-latitude regions near the Arctic Circle, while the zodiacal light can be seen from various locations with dark skies.","context":["Over the next two weeks, you have an excellent chance to spot one of the most rarely observed objects in the sky, the zodiacal light.\nThe zodiacal light takes its name from the ancient band of 12 constellations through which the sun, moon, and planets pass, the Zodiac. Modern astronomers prefer to use the name of the sun's path, the Ecliptic, and add a 13th constellation, Ophiuchus.\nMost of the material in the solar system is concentrated in a flat disk coinciding with the ecliptic. Besides the eight planets and their accompanying moons, there are thousands of asteroids and millions of smaller particles, ranging all the way down to fine grains of interplanetary dust.\nThese millions of particles cannot be seen as individual objects, but like the distant stars in the Milky Way, their tiny inputs of light combine to create a faint glow along the plane of the ecliptic. This is what we call the zodiacal light.\nMost people nowadays have never seen the Milky Way because its light is so faint that it is wiped out by light pollution except under dark country skies. The zodiacal light is even fainter than the Milky Way.\nVeteran night sky photographer Jeffrey Berkes managed to capture the zodiacal light in March 2014 during a trip to Death Valley National Park, where he photographed the celestial light from Badwater Basin.\n\"It was simply incredible here with the unworldly landscape at the lowest point in the US (232 feet below sea level),\" Berkes told Space.com in an email, adding that he will return to Death Valley this year to make another attempt while running an astrophotography workshop.\nSo to see the zodiacal light you first of all need to get away from light polluted urban skies. You need to observe at the dark of the moon, such as we will experience over the next two weeks. Finally you have to choose the right time of the year, when the ecliptic is close to rising vertically in the sky, normally in February and March in Northern Hemisphere evenings. You also need to give your eyes at least 20 minutes in total darkness so that they reach their maximum sensitivity.\nUse the bright planets Venus and Mars low on the western horizon to locate the base of the cone of zodiacal light, which will be tilted over to the left. The similar cone of the Milky Way, about 20 degrees to the right, will give you an idea of the shape to look for.\nIt will also help to use what astronomers call \"averted vision.\" This takes advantage of the fact that the most sensitive cells in our eyes' retinas are not in the central part of the visual field, but slightly away from it. The trick is to look slightly away from where you expect the zodiacal light to appear. This will put it on the most sensitive part of your eye, and improve your chance of seeing it.\nOnce you manage to spot the zodiacal light, try to follow it upward in the sky towards the zenith. You may see a faint glow directly opposite the sun's location, called the gegenschein, German for \"counterglow.\" This is a slight concentration of sunlight reflected off the interplanetary dust, just as traffic signs reflect back the light of car headlights.\nSpotting the zodiacal light and gegenschein are among the rarest astronomical observations any stargazer can accomplish. Good luck, and let us know if you succeed.\nEditor's note: If you capture an amazing view of the zodiacal light or any other night sky sight and want to share it with Space.com for a story of gallery, you can send images and comments in to managing editor Tariq Malik at: email@example.com.\nThis article was provided to SPACE.com by Simulation Curriculum, the leader in space science curriculum solutions and the makers of Starry Night and SkySafari. Follow Starry Night on Twitter @StarryNightEdu. Follow us @Spacedotcom, Facebook and Google+. Original article on Space.com.","The Aurora Borealis, often known as the Northern Lights, is one of nature’s most mesmerizing and intriguing occurrences. For ages, this beautiful display of colors moving across the night sky has been a source of awe and inspiration. In this post, we’ll look at the physics underlying the Northern Lights, their cultural importance, and the ethereal beauty they bestow on the arctic areas.\nTable of Contents\nThe Science of the Aurora Borealis\nWhen charged particles from the Sun, principally electrons and protons, hit with the Earth’s magnetic field, the Northern Lights appear. These particles are transported by the solar wind and interact with the gases in our atmosphere to produce a colorful display. The colors of the Aurora Borealis are controlled by the type of gas they collide with and the height at which these collisions occur.\nGreen and pink are the most prevalent hues seen in the Northern Lights, which are formed when charged particles clash with oxygen at lower altitudes. When these particles mix with nitrogen at higher altitudes, they produce hypnotic blue and purple colors.\nThe Northern Lights have long been a source of inspiration and mysticism for those who live in the northern regions. Indigenous civilizations, such as the Inuit and Sami, have created rich mythology and stories revolving around the Northern Lights. These natural wonders are frequently interpreted as spirits or heavenly manifestations, and their existence has profound spiritual and cultural significance.\nSeeing the Northern Lights is considered a good omen or a sign of good fortune in many cultures. It is also thought that children conceived under the Aurora Borealis would be blessed or have particular skills. These cultural beliefs lend another degree of fascination to an already incredible occurrence.\nBest Places to See the Northern Lights\nIf you want to see the Northern Lights, you’ll need to go to high-latitude places around the Arctic Circle. The following are some of the greatest places to see the Northern Lights:\n1. Troms, Norway: Known as the “Gateway to the Arctic,” Troms is a renowned Northern Lights location.\n2. Fairbanks, Alaska: During the winter months, Alaska provides various opportunities to see the Aurora Borealis.\n3. Yellowknife, Canada: Yellowknife, located in the Northwest Territories, has bright sky and great viewing conditions.\n4. Reykjavik, Iceland: Iceland’s isolated places make for spectacular Northern Lights photography.\n5. Abisko, Sweden: This little settlement in Swedish Lapland is famous for its constant displays of the Northern Lights.\nFrequently Asked Questions (FAQs) About the Northern Lights\nThe Northern Lights, commonly known as the Aurora Borealis, are a hypnotic natural phenomena that has enthralled mankind for ages. If you have any queries concerning this incredible display of lights in the night sky, you’ve come to the correct spot. Here are some often asked Northern Lights questions:\n1. What exactly are the Northern Lights?\nThe Northern Lights are a natural light display that occurs predominantly near the Arctic Circle in high-latitude countries. They are created by charged particles from the Sun hitting with the Earth’s magnetic field, resulting in a spectacular display of colors in the night sky.\n2. When and where will I be able to see the Northern Lights?\nThe Northern Lights are most typically seen in places such as Norway, Sweden, Finland, Canada, Alaska, and Iceland during the winter months, from late September to early April. The precise date and location are determined by solar activity and geographical considerations.\n3. What produces the Northern Lights’ various colors?\nThe Northern Lights’ hues are influenced by the type of gas in the Earth’s atmosphere with which the charged particles interact. Green and pink colors are produced by oxygen, whereas blue and purple colors are produced by nitrogen.\n4. Is there a best time to watch the Northern Lights?\nThe Northern Lights may be seen both at night and early in the morning. Look for them in dark, clean sky away from artificial light sources.\n5. Do the lights in the Southern Hemisphere and Antarctica seem the same?\nYes, the Aurora Australis, often known as the Southern Lights, may be seen in the Southern Hemisphere. It happens at the Antarctic Circle and has similarities to the Northern Hemisphere’s Aurora Borealis.\n6. Is it possible to anticipate when the Northern Lights will appear?\nWhile predicting the precise timing of the Northern Lights is difficult, there are aurora forecasting systems that can offer information on the possibility of auroral activity in certain places. You can arrange your Northern Lights viewing with the aid of websites and applications.\n7. Do you think you can hear the Northern Lights?\nNo, the Northern Lights are a visual spectacle that make no sound. It’s a frequent misperception that they generate noise.\n8. Are there Northern Lights trips available?\nYes, numerous tour companies in Aurora Borealis hotspots provide guided trips tailored exclusively for seeing the Aurora Borealis. These trips frequently include transportation, expert guides, and the greatest viewing places.\n9. What is the best way to shoot the Northern Lights?\nPhotographing the Northern Lights may be difficult but extremely rewarding. A camera with manual settings, a tripod, a wide-angle lens, and patience are required. Long exposures and low ISO settings will assist in capturing the beauty of the lights. Before attempting to shoot the Northern Lights, it’s a good idea to learn about astrophotography techniques.\n10. Can you see the Northern Lights all year?\nBecause of the longer darkness in northern locations, the Northern Lights are more commonly seen throughout the winter months. They can, however, appear at any time of year, but the odds of sighting them during the summer are substantially smaller.\nThe Northern Lights are a natural phenomenon that continues to awe and inspire those who are fortunate enough to experience them. If you’re planning a journey to view the Aurora Borealis, do your homework on the best dates and locations for your excursion and be ready for an unforgettable experience.\nThe Northern Lights are a breathtaking reminder of the natural world’s beauty and magnificence. For decades, this celestial light display, caused by the interplay of solar particles and the Earth’s magnetic field, has grabbed the hearts and minds of people all over the world. Whether you pursue the Northern Lights for their scientific intrigue, cultural importance, or simple visual splendor, they are a really amazing experience. So pack your belongings, go north, and let the Northern Lights charm you."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:081fa401-559b-4bc6-a2b1-a551f24ee1e0>","<urn:uuid:282ef433-2ba7-4c63-b27c-8f8e3b9ec243>"],"error":null}
{"question":"What are early treatment options for melanoma skin cancer, and how does treatment change once it becomes advanced (stage 4)?","answer":"For early melanoma limited to the skin, treatment options include surgery to remove the entire melanoma with surrounding normal skin, chemotherapy to slow cancer cell growth, immunotherapy to boost immune system response, and targeted therapy that blocks cell signals when mutation is present. However, once melanoma advances to stage 4 and spreads to other body parts, treatment becomes more complex, involving chemotherapy, radiotherapy, biological therapies like ipilimumab and vemurafenib (for those with BRAF V600 gene changes), or surgery to remove metastases. The choice of advanced treatment depends on where the cancer has spread, symptoms present, and previous treatments received.","context":["|Article Views: 127|\n|Skin cancer melanoma: Symptoms, diagnosis and treatment|\n|Posted on Oct 15, 2015|\n|Melanoma is a form of skin cancer that develops in the cells that produce the pigment that gives your skin its color- melanin. It is by far the most aggressive forms of skin cancer and can also form in your eyes and, uncommonly, in internal organs, such as your intestines. |\nThe precise reason behind all melanomas is unclear, but persistent exposure to ultraviolet radiation from sunlight or tanning lamps and beds increases your risk of developing melanoma. Restricting your exposure to UV rays can help reduce your risk of melanoma.\nMelanoma skin cancer occurs in people over 40 years of age and especially in women.\nSymptoms related to Melanoma skin cancer\nMelanomas can appear anywhere on your body, mostly on the back, legs, arms and face which are directly exposed to harmful sun rays. They can also develop on less exposed areas such as soles of your feet, palms of your hands and fingernail beds. Signs and symptoms of melanoma skin cancer include:\n- Any change in size, color, shape, or texture of a mole or other skin growth\n- An open or inflamed skin wound that won't heal\n- A change in an existing mole\n- A small, dark, multicolored spot with irregular borders that may bleed\n- A cluster of shiny, firm, dark bumps\n- A mole larger than a pencil eraser\n- A flesh-colored oval bump with a rolled border, which may develop into a bleeding ulcer\n- A reddish, brown, or bluish black patch of skin on the chest or back\n- A firm, reddish, wart-like bump that grows gradually\n- A flat spot that becomes a bleeding sore that won't heal\nTests and diagnosis for Melanoma skin cancer\nIn order to check for melanomas, your skin specialist may:\n- Perform a physical examination of your skin.\n- Do a skin biopsy which involves taking a sample of your skin and have it tested for melanomas.\n- Examine your lymph nodes to see if they are unusual in any way. This may be followed by an examination to check whether melanomas have spread to the lymph system.\n- Use imaging tests like PET scan, CT scan, and MRI to see whether cancer has spread to other parts of your body, such as the lungs, brain, or liver.\n- Full body photography to look for changes in any mole and to watch for new moles appearing in normal skin.\nTreatment for Melanoma skin cancer\nMelanoma skin cancer can be effectively cured if it's found and treated in its early stages when its impact is limited to skin only. Several treatment options are there to cure melanoma skin cancer including:\n- A surgery where entire melanoma is removed along with a margin of normal-appearing skin.\n- Chemotherapy in which medicines are used to stop or slow the growth of cancer cells.\n- Immunotherapy in which medicines are used to help your body's immune system fight the cancer.\n- Targeted therapy in which medicines are used to prevent cancer by blocking signals in the cell. The therapy is given only when a patient shows changed cell mutation.\nRegular follow-up consultations are important after you have been diagnosed with melanoma. Follow guidelines from your skin doctor.\n|Written by : Lazoi Team |","Advanced melanoma (Stage 4)\nThis page is about treatment for advanced melanoma skin cancer. There is information about\nWhat advanced melanoma is\nAdvanced melanoma means the cancer has spread to another part of the body. Your melanoma may have already spread when it is diagnosed. Or it may come back in another part of the body some time after you were first treated. This is called recurrent melanoma. Cancer that has spread to another part of the body is called secondary cancer or metastatic cancer.\nTreatments for advanced melanoma\nFor melanoma that has spread, you may have chemotherapy, radiotherapy, biological therapy or surgery. Chemotherapy and radiotherapy may help to shrink melanoma and relieve symptoms. The biological therapies ipilimumab (Yervoy), vemurafenib (Zelboraf), interferon and interleukin 2 can help to shrink and control melanoma for a time. Vemurafenib only works for people who have a change in a gene called BRAF V600. Sometimes surgery can remove tumours that have spread to other parts of the body.\nWhich treatment should I have?\nWhich treatment is right for you will depend on where your cancer has spread, the symptoms it is causing, and the treatment you have already had. Your doctor or specialist nurse will discuss the options for treatment with you.\nThe treatment team\nA team of health professionals working together will plan your treatment. They are specialists in the treatment of advanced stage melanoma. The team is called a Specialist Skin Cancer Multidisciplinary Team (SSMDT).\nExperimental techniques using new drugs or combining chemotherapy with biological therapy are also being tried. If you would like to be part of a clinical trial into a new treatment, talk to your doctor.\nView and print the quick guide for treating advanced melanoma.\nAdvanced melanoma means the cancer has spread from where it started to another part of the body. It is also called stage 4 melanoma. Your melanoma may have already spread when it is diagnosed. Or it may come back in another part of the body some time after you were first diagnosed and treated. Cancer that has spread to another part of the body is called secondary cancer or metastases.\nMelanoma can spread to almost anywhere in the body but the most common places for it to spread are the\nClick on the links to find out more about secondary cancers and lymph nodes.\nThese are also shown in the diagram below.\nRemember that melanoma may cause symptoms depending on where in the body it has spread to. But you are also likely to have aches and pains and off days in the same way as anyone else. So if you get aches or pains, they may not always be due to the melanoma. But do check with your doctor or specialist nurse about any symptom that is worrying you. It may not be caused by your cancer but if it is, the sooner you get treatment the better.\nFor advanced melanoma the treatment that is right for you will depend on\n- Where your cancer has spread to\n- The symptoms you have\n- The treatment you have already had\n- Changes in the genes within the melanoma tumour\nIt can be difficult to decide which treatment to try, and in what order. And this decision may be different for each person with advanced melanoma. It can be difficult to decide whether to have treatment at all when you have an advanced cancer. The treatment will not cure the melanoma but cancer drugs or radiotherapy may shrink it or control it for a time. Most people who choose to have treatment for advanced melanoma, will usually try more than one treatment.\nYou will need to think about how the treatment will affect your day to day life. This includes whether it may cause side effects, as well as stresses such as travelling back and forth to the hospital. Most importantly, you will need to understand what the treatment you are offered can do to control the melanoma.\nYour doctor will discuss the options for treatment with you. There may also be a counsellor or specialist nurse at the hospital you could talk to. You may also want to talk things over with a close relative or friend. It can be helpful to talk over difficult decisions with someone outside your circle of family and friends. You can look at our melanoma organisations page for organisations that can give you information and tell you about how to get counselling and emotional support.\nA biological therapy called ipilimumab (Yervoy) can shrink advanced melanoma and control it for a time. You have it as a drip (intravenously).\nA drug called vemurafenib (Zelboraf) can shrink melanoma in people who have a change in a gene called BRAF V600. About half the people with melanoma have this gene change. Vemurafenib is called a BRAF inhibitor and you take it as a tablet.\nThere is information about these treatments on the page about biological therapies for melanoma in this section.\nChemotherapy can help to shrink advanced melanoma and reduce symptoms in some people. Doctors have to balance the benefit of the chemotherapy against the possibility that the chemotherapy will cause side effects. The drug most commonly used is dacarbazine (DTIC). Sometimes it is used with other chemotherapy drugs, such as carmustine (BCNU), vinblastine or cisplatin. Temozolomide is also used. It is a similar chemotherapy drug to dacarbazine, but you take it as a tablet.\nYou can find information about chemotherapy for melanoma and the side effects in this section.\nSurgery may sometimes be used to remove tumours that have spread to other areas of the body. This is called metastectomy. If all the metastases can be removed it is called a complete metastectomy. After this type of surgery people can often stay well for months or perhaps years.\nIn one study, 4 out of 10 people (40%) who had all their secondary melanoma removed were still alive 5 years later. But this operation is only suitable for a small number of people with stage 4 melanoma. More trials need to be done to see just how well it works. Other treatments may be given before or after surgery. Research tells us that some cancer treatments such as chemotherapy will work better for smaller tumours than larger ones. So sometimes surgery can be used before other treatments.\nIf the secondary tumours are in the brain, they may be difficult or impossible to remove and the outlook is then not so good.\nA team of health professionals work together to plan your care. They are specialists in the care of people with advanced stage melanoma. The team is called a Specialist Skin Cancer Multidisciplinary team (SSMDT). It includes specialist surgeons and cancer doctors, a specialist nurse, an occupational therapist, camouflage make up specialist, and a counsellor or psychologist.\nMany clinical trials are going on with advanced melanoma patients to try to find out which treatments work best.\nSome experimental techniques are combining chemotherapy with biological therapy. Information about these and other new treatments for melanoma is included on our melanoma research page, further on in this section.\nIf you would like to be part of a clinical trial into a new treatment, talk to your specialist or GP. They may be able to find a trial going on in, or near, your hospital. You can also find clinical trials in melanoma by using our clinical trials database.\nRated 5 out of 5 based on 72 votes\nQuestion about cancer? Contact our information nurse team"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:5567a842-75b8-43bb-919e-666a6f1d879b>","<urn:uuid:23c3fcc6-52cb-4450-93ce-c4b3df2a6331>"],"error":null}
{"question":"Is computer control more critical for precision in CNC machining or waterjet cutting systems?","answer":"Computer control is more critical for precision in CNC machining than in waterjet cutting systems. In CNC machining, the computer breaks down complex shapes into thousands of minute instructions to create incredibly complex objects, with the entire process dependent on precise computer numerical control. While waterjet cutting also uses computer control, its precision depends more on physical factors like water pressure, nozzle condition, abrasive quality, and material thickness rather than purely computational control. Waterjet cutting speed and accuracy can be maintained through proper maintenance and parameter adjustments, even with simpler control systems.","context":["CNC machining and conventional machining are a lot of things. In this blog, we will help you see the difference between these two and which one can be used as an aid to do your work easier.\nWhat Is CNC Machining?\nCNC machining, also known as computer numerical control, is a manufacturing process used to produce parts. The process uses a computer to control the movement of the machine and cutting tools. CNC machining can be done with various tools, including milling machines, lathes, and drill presses.\nCNC machining is often used to make metal parts out of solid metal blocks using drills or mills. It’s also used to make plastic parts on CNC routers. The advantage of CNC machining is that it can produce precision parts quickly and at a low cost compared with other methods like handcrafting.\nCNC machining has roots in older technologies such as manual lathe turning and milling machines from the 1800s powered by compressed air or water pressure instead of electricity. These early machines required skilled operators who could program them by hand using instructions written on paper for each part they wanted to make.\nModern CNC machines use electronic controls that can be programmed remotely by computer software or manually by an operator with a simple interface such as a computer keyboard or touch screen display panel. The software then sends instructions directly to the machine controller over a network connection such as an Ethernet cable or WiFi link.\nWhat Is Conventional Machining?\nConventional machining is the process of creating a p material from a solid block of material, also known as stock.\nConventional machining consists of several different processes:\nTurning: A turning operation is done on a lathe. Material is removed from the workpiece using specialized tools such as cutting tools, broaches, reamers and taps. The tool is held in a chuck at one end of the lathe while the other is rotated at high speed by an electric or hydraulic motor. Turning can be used to produce internal features such as interior slots, external features such as threads and grooves, and profiles such as tapers and shoulders.\nMilling: In milling, material is removed using rotating cutters that ride along an axis parallel to the bed of the machine. A milling machine has multiple axes of movement, so it can perform complex operations like contouring surfaces or creating three-dimensional shapes from blocks of solid stock material. Milling machines commonly have three axes that allow them to move in any direction relative to the workpiece or vice versa (XYZ). Most CNC machines are capable of milling operations because they can move multiple axes simultaneously without human intervention. Milling is performed on various materials and with a variety of tools. The most common materials include wood, plastic, aluminum, brass and steel. Common milling operations include drilling holes, creating slots or grooves in flat surfaces, shaping circular components like wheels or gears and many others.\nAdvantages Of CNC Machining Over Conventional Machining\nThe advantages of CNC machining over conventional machining are many. The following are some of the main advantages:\nThe cost savings in CNC machining is due to several factors. The first is that it eliminates the need for manual labor. Secondly, there is no need to have expensive tooling and jigs. Thirdly, using automation means you can work on multiple parts at once instead of just one at a time. You can also save on storage costs as you don’t need to store tools or jigs. Finally, there are no other costs associated with conventional machinings like sludge removal or dust collection systems.\nThe accuracy of CNC machining is much better than that of conventional machining because it eliminates human error and reduces the chances of mistakes being made during the machining process. It also allows for greater flexibility in terms of tolerances and precision required by customers.\nCNC machining is much faster than conventional machining since it uses computer-controlled machines and robots which can work 24 hours a day without stopping (although this depends on your business). You also don’t have to wait for an operator or worker to be available before starting work on a project. This means you can finish your work much faster than before because there is no waiting time involved. You can also set up multiple CNC machines and have them run simultaneously, which reduces the amount of time it takes to produce parts in bulk (if this is what you need).\nCNC Machining VS Conventional Machining\nConventional machining is the oldest form and has been used for hundreds of years. It involves using hand tools to move material through a series of cutting operations, typically using dies and punches to create parts from bar stock or other pieces of metal. One advantage of this method is that it’s usually cheaper than CNC machining since you don’t need expensive tooling or complex machinery. However, it’s less precise than CNC machining so more scrap can be produced during the cutting process. Conventional machining also takes longer than CNC because it’s done manually rather than automatically by a computer-controlled machine tool.\nCNC stands for computer numerical control, which means that machines are controlled by computers rather than people controlling them manually, like in conventional machining. This allows machines to perform repetitive tasks much faster than people can manually, so they can be used in high-volume production lines where hundreds or thousands of parts need to be made at once.\nIs CNC Machining Better Than Conventional?\nCNC machining is a process in which computer numerical control (CNC) machines create parts. CNC machining can be done with a variety of different tools, but it is often used to produce high-quality parts that are accurate and consistent.\nThere are two main types of CNC machinery: vertical and horizontal. Vertical CNC machines use a rotating spindle to cut materials like metal, plastics, or ceramics into the desired shapes. Horizontal CNC machines use a cutting bit that moves back and forth along the material’s surface while being cut by the spindle. Both machines require toolpath programming to generate instructions for how they should operate to complete their tasks effectively.\nConventional manufacturing methods involve using large machines called lathes to turn raw materials into finished products using a series of complex motions and cuts that can take hours or even days for one piece. These machines are very expensive (upwards of $1 million or more), which makes them impractical for small businesses or hobbyists who may only need them infrequently.\nCNC machining allows businesses to create custom parts with greater accuracy and consistency than traditional methods at lower costs by using computers instead\nDespite all the apparent differences, achieving the result is usually the same. The primary reason CNC machining gets its classification is the computer behind it. It breaks down a complex shape into thousands upon thousands of simple, minute instructions to create an incredibly complex object. Like conventional machines, CNC machines require a lot of time and patience to create these machines and achieve these results.","The cutting speed of a process is a good way to measure its efficiency. Many professionals often ask about the waterjet cutting speed since it is one of the most popular commercial cutting processes.\nAn idea of waterjet cutting speed can better show how it fares against other cutting alternatives. Additionally, manufacturers can estimate their productivity output based on the speed parameter.\nTherefore, this article will go into detail about waterjet cutting speed. You will also learn about the various parameters that affect this value.\nHow Fast Can a Water Jet Cut?\nWaterjet cutters can cut at a speed of 15 cm per minute on average. The speed of cutting is variable and dependent on many different factors. For a better perspective, an alternative cutting technology, such as Electric Discharge Machining (EDM), cuts at around 1 cm per minute.\nHow many inches per minute can a water jet cut?\nThe average cutting speed of a water jet cutter is 12 inches per minute. The common range falls between 12 to 15 inches per minute. However, that is not the maximum speed of these cutters. For softer materials like marble and glass, water jet cutters can cut at a speed of astonishing 90 inches per minute.\nWhat are the Factors That Influence Water Jet Machine Cutting Speed?\nMany factors play a crucial role in determining waterjet cutting speed. These factors are:\nMaterial thickness is the primary factor that determines the waterjet cutting speed. Cutting speed is inversely proportional to the material thickness. Therefore, when the material thickness increases, the cutting speed decreases. For instance, consider a waterjet cutter cutting stainless steel sheet of 6mm thickness at 6 inches per minute. The same cutter with the same orifice diameter will cut a stainless steel sheet of 20 mm thickness at around 2 inches per second.\nType of Material\nThe type of material also dictates the cutting speed of the waterjet. Different materials have different physical properties. Some materials are soft and porous. Other materials are extremely hard. Materials with poor machinability lead to slower cutting speeds of the water jet.\nTo take the variation of the type of material on cutting speed, let us compare stainless steel and marble. A common waterjet cutter can cut half an inch thick glass at 12 inches per minute. At the same time, the cutter will cut half an inch thick titanium sheet at the rate of 6 inches per minute.\nThe abrasive’s quality, amount, and size influence the cutting speed. Abrasives with a high hardness provide a faster cutting speed. The standard abrasive is garnet with a mesh size of 80. There are many different varieties within garnet itself. Fine-size garnet with high uniformity provides the best cutting results.\nThere is a wide range of water pressure used in industrial waterjet machines. A lower pressure does not mean you cannot cut harder materials. However, a lower pressure makes cutting slower. High-pressure machines speed up the cutting process for harder materials.\nAn important thing to keep in mind is that beyond a certain point, the cost of the equipment will increase exponentially with an increase in pressure. This is why 90,000 psi pumps are available in the market but not as common as 60,000 psi. Higher pressure also leads to faster wear and tear of the waterjet equipment.\nHigher nozzle diameter leads to a faster cutting speed. A larger nozzle diameter allows for more water volume to pass. This leads to a greater eroding force of the water. The nozzle diameter can be increased within certain limits. This is because the ultra-high pressure of the waterjet stream will be difficult to maintain with a wide nozzle.\nTo compare the effect of nozzle diameter on cutting speed, let us take the example of aluminum cutting. A 0.007″ diameter nozzle will cut a 5 mm sheet of aluminum at around 15 inches per minute. A 0.014″ diameter nozzle can provide a cutting speed of around 40 inches per minute.\nA wider nozzle will also mean more water and abrasives flowing per minute. This will pile up the cost of the entire operation.\nWater quality also affects the cutting speed of a dynamic waterjet system. This fact is unknown even to many professionals. Poor quality water contains impurities which can get clogged in the waterjet tubes and nozzles. This reduces the speed of the waterjet system and increases the equipment breakdown rate. Therefore, using treated water is a good way to ensure faster cutting and longer equipment life.\nWater Flow Rate\nModern waterjet machines come with a flow rate adjustment system. A higher flow rate leads to a faster cutting operation. This is because more water passes out of the nozzle every minute, leading to a higher eroding force. However, a higher flow rate also means that there will be accelerated wear and tear of the components. It can also increase the operational cost of the cutting process. Using various flow rates on different use cases to find the sweet spot is the best way.\nStraight cuts are the fastest to make. 90-degree sharp corners decrease the cutting speed because the waterjet cutting head needs to stop and then move again to make the turn. The best way to optimize cutting speed is to use rounded corners instead of sharp corners. This ensures that the machine can keep cutting through the corner without stopping.\nFocusing Tube Length\nThe cutting speed is inversely proportional to the focusing tube length. A longer tube reduces the speed of the waterjet stream. This, in turn, lowers the accuracy and the cutting speed. A shorter tube maintains both the accuracy and the cutting speed.\nThe taper angle plays a slight role in the cutting speed. Steep angles increase the cutting speed. Meanwhile, low taper angles reduce the speed. It is important to note that steep angles reduce the accuracy of waterjet machining. Therefore, you will have to find a sweet spot between speed and accuracy regarding the taper angle.\nSpending a little time ensuring proper workpiece fixtures can improve the cutting speed and quality. Proper fixtures eliminate any chances of movement of the workpiece and secure it firmly. Besides a faster cutting speed, this also increases the safety of the cutting equipment.\nCutting perpendicular to the material’s grain can lead to a faster cutting speed. This is due to the internal geometry of the materials.\nNozzle wear is a common phenomenon in waterjet cutting. This is due to the extremely high pressure handled by the nozzle. A worn-out nozzle reduces the waterjet operation’s accuracy and cutting speed. This is due to the poor focusing ability of the worn-out nozzle. This can be easily eliminated by following proper maintenance measures for your waterjet equipment.\nAbrasive Flow Rate\nA higher abrasive flow rate leads to a significantly faster cutting operation. This is because, in abrasive waterjet cutting, the cuts are done by abrasive instead of water. Therefore, increasing the abrasive flow rate leads to a higher cutting rate. However, this also increases the cost of the operation due to higher abrasive consumption.\nWaterjet nozzles are designed with advanced engineering to suit different manufacturing goals. For instance, there are specific nozzles for faster cutting operation and nozzles for high-precision cutting. If the cutting speed is your primary concern, using a nozzle designed for faster cutting could be better.\nCutting Head Wear\nCutting head wear and tear has a similar effect as nozzle wear. A worn-out cutting head will reduce the accuracy and speed of the waterjet operation. Therefore, following proper maintenance measures can be a great way to ensure the fastest possible cutting speed.\nWorkpiece Material Variability\nEvery material has a degree of internal imperfection. This could be little voids or major internal cracks. These imperfections can affect the cutting speed. Calculating the exact speed varies based on internal geometries can be hard and unpredictable. Using better quality material is a good idea to ensure consistent cutting speed and quality.\nPower Grid Stability\nIt is common for manufacturers to see a drop in cutting speed even though the waterjet equipment is working perfectly. This occurs due to fluctuations in the power supply to the equipment. These fluctuations not only drop the cutting speed but can also damage the equipment itself. Make sure you have a stable power grid for a faster and smoother waterjet operation.\nExperienced operators know how to set up the machine for optimized cutting speed and quality. Inexperienced operators unfamiliar with the process often take time to set up and learn the outcomes along the way. It can take some time and a lot of hit and trial for unskilled operators to optimize the operation for speed. Therefore, if high-speed cutting is your priority, ensure that you have a skilled operator working the equipment.\nHow Does Water Jet Cutting Speed Impact Cut Quality and Surface Finish?\nA slower cutting speed usually leads to better cutting accuracy. The relationship between waterjet cutting speed and surface quality is also the same. A slower cutting process can provide a better surface finish. Therefore, while faster waterjet cutting can lead to higher productivity, the outcome might not be as accurate as you desire. Try the machine at different settings and use a speed-accuracy parameter that perfectly meets your requirements.\nHigh-speed cutting is one of the characteristics of waterjet technology that makes it preferred by manufacturers. An important end note regarding cutting speed in waterjet applications is using stacking. You can stack multiple workpieces and cut them together, exponentially increasing speed. Additionally, a high-quality waterjet cutter ensures you get a good and consistent speed throughout the operation.\nTechni Waterjet machines are the best in this regard. Multiple models are available, so you can choose one that meets your requirements. All Techni Waterjet machines come with complimentary software to provide cutting processes at the click of a button."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:ec94b4d1-4108-46d2-9f24-b733711146aa>","<urn:uuid:11417f29-907f-4099-933a-eaa9efa45730>"],"error":null}
{"question":"Want to know breeding cycle of Boer goats - when can they reproduce during the year?","answer":"Unlike most goats, Boer goats can breed all-year-round, meaning they could theoretically breed more than once per year. However, breeding multiple times per year is not recommended as it's hard on the dam's health. The pregnancy period lasts about 150 days, and twins are the norm, though they can have singles or even quadruplets.","context":["Do you want to produce your own meat? Homegrown meat is a great way to be independent of the supermarket and self-sufficient.\nBoer Goats is an attractive meat goat breed option for homesteaders who don’t have acreage for cattle.\nAmong the several meat breeds available, the Boer goat has the fastest growth and best meat-to-bone conversion – a valuable quality for homesteaders!\nBoer goats are an African breed though there is disagreement as to whether we can thank the Hottentots, Europeans, and even East India Indians for it's earliest stages of development. What we do know is in the early 1900s South African farmers began intentionally selecting traits favoring a fast-growing, adaptable, prolific meat goat.\nIn 1993 the first Boer goats were imported to the United States. Since then the demand for goat meat has skyrocketed. The American Boer Goat Association states they register over 45,000 head of Boer goats annually with all of the goat meat industry being estimated to value 150 million-400 million yearly.\nBreed Standards of Boer Goats\nThe American Boer Goat Association is the main registry for Boer goats in the U.S. Since Boers are meat goats a sturdy frame, well-sprung ribcage, and fleshy, compact appearance are the breed standard. Registries are also looking for the following characteristics:\n1. Meat Production\nBoth bucks and does should be sturdy, with a deep chest, wide topline, and a fleshy appearance without being overweight.\nA large frame with well-sprung ribs is especially important since this is a sign of a carcass that can support their meat, gives adequate respiratory, rudimentary, and, in does, kid carrying capacity.\n2. Height and Weight\nThe American Boer Goat Association states bucks weigh 200-340 lbs. And does between 190-230 lbs.\nAlthough there are no rigid requirements for color, the traditional Boer is white with a brown head, but there are some that are black and white or mostly brown.\n4. Other Breed Specific Standards\nSome common features registries also require are:\n- Well-defined Roman nose\n- Long drooping ears\n- Well-proportioned fleshy neck\n- Overall masculine appearance in bucks\n- Overall feminine appearance in does\n1. Meat Production Details\nHow much meat can you expect from one Boer goat? Although it depends on age, nutrition, diet, and genetics, on average, you can count on 40-50lbs of meat per goat raised to market age, which is roughly 6-9 months.\nBoer goats have the highest meat-to-bone ratio of all goat breeds, so if meat production is important to you they are the way to go.\nThere are mixed experiences about the hardiness of Boer goats. They were bred to be adaptable and many African breeds hold true to their hardy reputation.\nYet, I personally know several individuals whose Boer goats succumbed to parasites faster than other breeds. Parasites are the number one health concern with goats, so always err on the side of stocking your herd with the healthiest animals possible coupled with an effective deworming action plan.\nBoer goats have been called “gentle giants” and are a very pleasant docile breed. They are known for being good mothers and enjoys attention.\nBreeding Boer Goats\nSince Boer goats are a fast-growing breed it is recommended to breed them according to weight instead of age. 80lbs. is considered the best weight for doelings with 70lbs or 6 months for bucklings.\n2. Breeding Season\nUnlike most goats, Boers breed all-year-round meaning you can theoretically breed them more than once a year. This is hard on the dam’s health, however, and is not considered good practice.\nBoers ability to bred year-round does give you the advantage of customizing your breeding program for kids to be sale-ready at peak market times. These will vary according to your area and customer base. Overall, this flexibility is a great perk in owning Boers.\nLike all goats, Boer does carry their kids for about 150 days. Boers have a very high fertility rate – another one of their amazing perks.\nBoer does make great mothers though like all goats they need extra TLC when pregnant. Does whose own needs are met will focus on their kids if you support their mothering with good quality feed, minerals, hay, and fresh water.\nAs a rule of thumb twins are the norm for Boer goats, though singles are not uncommon and they can even have quadruplets.\n4. Genetic Defects\nA common defect among Boer does is extra teats. Goats normally have two teats with a firm upper udder. It is common to find more with Boers ranging from an extra orifice (opening) that may or may not give milk to spur teats that branch off one of the main two or is a separate teat altogether.\nIn the past when the U.S. Boer gene pool was small, multiple teats was overlooked. However, customers are now becoming more concern with quality.\nThe American Boer Goat Association does have requirements on how deformed teats may be. Overall, the tendency is those looking for registered breeding stock are putting more emphasis on genetic quality, while your strict meat customer likely will only be concerned with meat quality.\nCaring For The Boer Goat\n1. Feeding and Nutrition\nThe best way to keep feed cost down is to feed Boers on high-quality pasture. By encouraging high-protein legumes and good pasture management like rotational grazing, the only times you should need to feed grain is during poor pasture condition like winter or drought.\nSo when grain feeding is necessary, what should you look for? Being meat goats Boers have higher protein needs, yet not so high as to interfere with rumen health. A high-quality feed for meat goats should meet what Boers require.\n2. Housing and Fencing\nGoats need a loafing shed or draft-free barn to protect them from weather and predators. Boer goats especially need to shelter from freezing temperatures to avoid frostbite to their long ears.\n3. Health Issues and Care\nAs mentioned before parasites are the top health concern with goats. A deworming protocol, rotational grazing methods, and small herd numbers are three crucial aspects of preventing parasite infestations.\nBoer goats also need to be kept in shape. Their fleshy tendencies make them prone to being overweight which stresses their bodies and lowers their resistance to disease.\nGoats tend to see to their own bathing needs, but an occasional bath is sometimes in order.\nAlso, goats need their hooves trimmed every 4-6 weeks.\nSince most homesteaders handle their goats, training your Boer goats to a lead rope will make your life much easier. Also, if you want to show your Boers, there are certain stands and positions your goats will need to learn.\n1. Savannah Goats\nFor couples or smaller families, the Savannah goat might be a better alternative to Boer goats. Also an African breed, Savannahs are beautiful goats with similar personalities as Boers.\n2. Kiko Goats\nFor a breed that gives similar meat amounts yet is known for being parasite resistant, Kikos meet the bill. Kikos do not grow as fast as Boers but they are very hardy animals.\nDid You Know?\nThe word Boer comes from the Dutch word for “farmer”. The Dutch established trading routes along the South African posts and found raising their own livestock to be essential to their survival.\nWhile historians don't quite agree on who combined European imports with native goats, the Dutch left their mark in this popular breed.\nAlso, the first registry was established in South Africa 64 years before the American registry. Today Boer goats are one of the few double-muscled meat goat breeds, explaining why they are the top breed in the meat market.\nSo are you ready for a big step away from dependence on the grocery store? If raising your own meat is next on your bucket list, meet a few Boer goats to see if they are for you.\nThe double-muscle meat production of Boers and their gentle personalities make them a good choice for homesteaders. Are you ready to give them a go?"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:7b350cdb-818a-46d9-92c8-0fa37aa3734e>"],"error":null}
{"question":"Could you tell me what's special about yeast compared to other fungi? I'm trying to understand its unique characteristics.","answer":"Yeast is unique among fungi because while most fungi are multicellular with a mycelium, yeast is unicellular. It's a saprophytic Ascomycetes fungus that grows in sugary media and has a round, spherical, or oval shape. It contains normal eukaryotic organelles (except chloroplasts), has a single nucleus, and reproduces asexually through budding. Unlike other fungi, its cell wall doesn't contain chitin.","context":["Bacteria vs Yeast\nMicroorganisms are a taxonomically diverse group of organisms. Microbes include bacteria, cyanobacteria, protozoa, some algae, fungi and viruses.\nBacteria were first observed in 1674. The name originated from the Greek word “small stick”. Bacteria are unicellular and typically few micrometers long. They have a diversity of shapes. They may occur as attached to surfaces. They form biofilms having different species. Their thickness can be a few micrometers to several centimeters. There are many shapes such as cocoid, bacilli, spiral, comma and filamentous. There is no membrane bound organelles. They lack a nucleus, mitochondria, chloroplasts, golgi bodies and ER. DNA is present in the cytoplasm, in an area called nucleoid. DNA is highly coiled. 70+ type ribosomes are present. Cell wall consists of peptidoglycans. Gram positive bacteria possess a thick cell wall with several layers of peptidoglycan. Gram negative bacteria cell wall has few layers surrounded by a lipid layer.\nA smaller DNA molecule may also be present. It is called a plasmid. The plasmid is circular and contains extra chromosomal material. It undergoes self replication. They carry genetic information. However, the plasmid is not essential for the survival of the cell. Flagella are rigid protein structures used in motility. Fimbriae are fine filaments of the protein involved in attachment. Slime layer is a disorganized layer of extra cellular polymers. Capsule is a rigid polysaccharide structure. It is also called the glycocalyx. The capsule provides protection. It contains polypeptides. Hence it resists phagocytosis. Capsule is involved in recognition, adherence and formation of biofilms. Capsule is associated with the pathogenesis. Some produce endospores which are highly resistant dormant structures.\nYeast is a fungus. Fungi are eukaryotes. Most of which are multicellular with a vegetative body forming a mycelium, but yeast is unicellular. Fungi are always heterotrophic, and they are the major decomposers living on dead organic matter. Decomposers are saprophytes. They secrete extra cellular enzymes to digest organic matter and absorb the simple substances formed.\nClassification of fungi is based on 2 main characteristic features. Those are morphological features of vegetative mycelia and characteristics and organs and spores produced in sexual and asexual reproduction. Fungi are classified into 3 main divisions Zygomycetes, Ascomycetes and Basidiomycetes. Yeast is a unicellular Ascomycetes fungus. It is a saprophytic fungus growing in sugary media. It is round or spherical or oval in shape. It contains a single nucleus. At the centre of the cell is a well marked vacuole with granular substances suspended in it. Normal eukaryotic organelles except chloroplasts are found within the cells. Lipid and volutene granules too are present. Surrounding the cell is a cell wall. No chitin is found in the cell wall. The common mode of asexual reproduction is budding. During sexual reproduction ascusspores within asci are formed, but no ascocarps are formed.\nWhat is the difference between Bacteria and Yeast?\nBacteria are prokaryotes and yeasts are fungi which are eukaryotes. The 2 types of organisms are fundamentally different.\n• In bacteria there is no organized nucleus and in yeast there is an organized nucleus.\n• In bacteria there is only a single circular DNA. In yeast, there are several linear DNA.\n• In bacteria nucleolus is absent and in yeast nucleolus is present inside the nucleus.\n• In bacteria 70s ribosomes are present. In yeast 80s ribosomes are present."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:b83b6143-936b-4feb-8e1c-49edc8f53d60>"],"error":null}
{"question":"What is the key difference between how LIME analyzes black box AI models when applied to image classification versus structured data?","answer":"When analyzing images, LIME creates perturbed versions by modifying pixels and blurring/removing parts of the image (like removing a dog's body), then observes how the classification probability changes. With structured/tabular data, LIME creates perturbed examples by modifying individual data fields (like car model, person age, area) and observes how the prediction probability changes. In both cases, LIME identifies important features by seeing which modifications cause significant changes in the model's output, but the perturbation approach is adapted to the specific data type being analyzed.","context":["What do we do every time we’re about to cross a road and there’s a car approaching? We look into the driver’s eyes and we think: Oh, he sees me, I’m safe. Or he doesn’t, and I’m not. We make this conclusion because we know that he or she is human and we think alike. We know how he would react because we know how we would react.\nBut what if the next time you’re about to cross a road and the car is approaching there are no eyes to look into? What if it’s a self driving car? You cannot make eye contact with an algorithm. And you may wanna think about these kind of scenarios because they are much closer than what we think.\nSoon, self driving, fully autonomous cars will be occupying our roads. And instead of humans behind the wheel there are these powerful AI driven machines. They make sense of the complex world. And they make split second decisions we bet our lives on.\nSuch AI innovations are also trickling down into other areas as well. In the form of a smart assistant for example. How many of us are already daily interacting with smart assistants such as Siri. Or Amazon’s Alexa. Or Google Duplex.\nThe interactions that we have with such machines will become indistinguishable from human interaction. And this means we must be able to trust them. Having trust is easy when you can see that the decisions that the machines makes are correct. For example when a complex system is predicting the weather, we can clearly see that often it is correct. And even when it isn’t, often the worst thing that can happen is you getting wet.\nTrust is much more difficult when we cannot directly observe the outcome. The consequences of their decisions. Think about the banks when they have to grant a loan. Or the insurance companies when they have to accept or not accept a policy application at underwriting. In such circumstances it becomes paramount to trust the machines. Because by not accepting a loan, you’re affecting someone else’s life. A whole family future. In these circumstances we must be able to trust the machines.\nLet me tell you a short story. It’s 2015. It’s New York. And a man called Jason, when he was walking home, got jumped by 4 guys. They broke his eye socket and his jaw. And only because he was lucky enough, it didn’t affect his hearing or he didn’t become blind. A few days later, his partner, Virginia, went to the pharmacy to pick up the painkillers for her partner.\nAnd, when she was about to receive them, the pharmacist told her that her insurance was cancelled. The insurance was denied. She start panicking. She started asking why. But the pharmacist surely couldn’t tell her why she was denied. So she started calling the insurance company staff. Started asking why it happened. And even the insurance company on the other side of the phone could not tell her why she was denied.\nOnly because she had the resources and the time to investigate by herself, she later found out that what happened, was that a few weeks earlier, before the incident, they just moved to New York. Because she had a new job. And right after that they claimed thousands of dollars for the facial surgery for Jason. And we all know that shortly claiming after entering a policy, it’s an indicator for fraud. We use it on a daily basis at FRISS.\nBut the issue here is not the indicator itself. The issue here is that the insurance company in this case was not able to provide an explanation. That that was why this whole cancellation happened.\nThis is a real story. And the Virginia in this story is Virginia Eubanks who later after this incident\nwrote a book called Automating Inequality where she explores more extreme cases where this automated system are used by the police to target and to punish the poor.\nSo what can we do? Well a step in the right direction is to make this AI machine, this black box model, more transparent. More interpretable. Because we are in front of 2 kind of situations. Either we observe that the decisions they make are systematically correct. Like the weather forecasting. Not always, but often. Or we know how such decisions are made. This leads me to talk to you about the AI and interpretability in AI.\nBecause when we talk about interpretability in general, it was very well defined by Miller is, “the degree to which a human can understand the cause of a decision”. So how does this fit into the AI concept? Well, AI, it’s no more than a model which takes data as input and provides predictions as output.\nSo interpretable AI in this context means interpretable models. Transparent models. And some models are interpretable by design. Think about the decision trees. Decision trees were a very popular model in use for classification. And in this example we tried to predict whether a person is a male or a female based on the height first.\nSo we asked: Is she or he taller than 180 centimeters? Yes: we classify as a male. No: we ask another question. Does he or she weigh more than 80 kilograms?\nYes: we classify as a male.\nNo: we classify as a female.\nSo we can clearly see in each step, at each node, how the prediction was made. What was the kind of question and decision that was made at each step. The issue with this is that it’s a very simple model.\nDecision trees such as also linear regressions are very simple models. And simple models can not answer more complicated questions. For other complicated questions such as image recognition or weather forecasting, the AI came to the rescue with more complicated solutions. Deep Neural Networks.\nDeep Neural Networks are really complex but highly performant models which are at the core of the smart assistants. At the core of the self driving cars or the weather forecasting. These are highly performant models which are a black box to us. We don’t really know how each prediction is made. And this can become a problem. Because, automated black box models can have several severe consequences.\nFirstly, a hazard in the context of automated systems, is that one should not confuse correlation with causation. For example: We can very well predict the number of shark attacks based on the number of ice cream sales. As the ice cream sales go up the number of shark attacks are also found to rise.\nBut obviously this is a spurious correlation. The reality behind this is that the common causal effect is that it’s summer. So that’s when people go more often to the beach. And they’re more susceptible to shark attacks. Why does this happen? Well…data to model AI, if collected properly, reflects society. And there’s a catch. Every society has its own fundamental flaws. Its own challenges. Which can create patterns hidden into data. And if not careful, this pattern can lead to potential poisonous prejudices.\nLet’s analyze a couple of examples where this went completely wrong. This can lead for example to the phenomenon of redlining. Which is the systematic denial of health services, or bank loans, or even the building of a supermarket to people living in certain areas which are often associated with ethnic minorities or a particular race.\nAnother even more disturbing example happened back in 2016 when an Israeli startup called Faception, claimed they could have flagged and classify criminals based only on the analysis of the facial treats, with the deep learning model.\nThe explanation behind the system was really scarce. And we can clearly see where this can go wrong. This can just reinforce our stereotype of a criminal or a terrorist, and can lead to a much more and even worse controversial kind of discrimination. So we are in front of a dilemma because we want both highly performant models, but we also want transparent AI.\nWhat can we do? Several techniques have been deployed in the last years, which allow you to take your black box model and sort of explain the predictions, so that humans can understand it. And one of these techniques is LIME.\nLIME is the acronym for Local Interpretable Model-Agnostic Explanation. Which sounds really fancy, but the idea behind this technique, is really simple but very clever. So I’m gonna try now to show you how it works.\nLet’s consider for example a very famous black box model. Which is the Google Inception Neural Network which classifies images. And let’s consider the picture of this majestic Wiener dog. If we run our model for this image we get a probability of 88% that this picture represents a Wiener dog.\nNow we want to know what made this black box model make such a decision. What LIME does, it randomly creates perturbed images. Like modifying the pixels of the image. For example in the first one you can see that the mouth of the Wiener dog was blurred out. And in the second one the head was taken out. If we rerun these 2 examples through the system we see that the probability of being a Wiener dog is sort of stable. It stays the same. So these were not really influential to make the prediction.\nBut what if we take out the body of the Wiener dog. And we rerun the example through the model. We see hat the probability of being a Wiener dog decreased significantly to 12%. What does this mean? This means that the shape of the body of the Wiener dog in this case was one of the most important factors in our model to make the prediction of a Wiener dog.\nThis was with images. But with tabular data, with structured data, it behaves pretty much the same way. So let’s say we have this strip of data which is about a claim. So we have the car model, the claim type, the person age, the area and the time of which the claim occurred. LIME creates 3 perturbed examples where we perturb first the model, then the person age and then the time of the claim. And we see that if we rerun this through our fraud model the probability remains pretty much the same.\nBut we have a fourth example where we perturb the area in which the claim occurred. And we see that the probability decreased to 32%. A huge decrease. What does this mean? This means that the area was one of the top predictors of our model to predict fraud. This is a clue of a redlining phenomenon going on.\nDoes this mean we solved the problem? Does explainable AI mean trustable AI? Unfortunately it’s not that simple. As Cassie Kozyrkov explains here: “Trust based only on explainability is like trust\nbased on a few pieces out of a giant puzzle.” This means that these explanation techniques are just an intuition of what’s going on. They can help us, but they’re not in any way complete for us to trust AI.\nSo what’s let’s say the correct path, a good principle for us, to trust AI. We just saw LIME. And even much more important is to have a reliable testing framework. Also with constant monitoring. You want to check regularly that your model is doing what it is supposed to do. And finally reliable data collection. You cannot trust your model if you don’t trust the guy who actually collected the data.\nAs data becomes bigger and models become more complex, the gap between humans and machines becomes wider. And explainable AI, trustable AI, aims to close this gap. To build a bridge between humans and machines. So that humans can understand machines better.\nAnd by researching and developing new model’s interpretation techniques we can have a better understanding of intelligence. We can have a better understanding of the world’s phenomena. Which, ultimately, is the real goal of science.","This example shows how to use locally interpretable model-agnostic explanations (LIME) to understand why a deep neural network makes a classification decision.\nDeep neural networks are very complex and their decisions can be hard to interpret. The LIME technique approximates the classification behavior of a deep neural network using a simpler, more interpretable model, such as a regression tree. Interpreting the decisions of this simpler model provides insight into the decisions of the neural network . The simple model is used to determine the importance of features of the input data, as a proxy for the importance of the features to the deep neural network.\nWhen a particular feature is very important to a deep network's classification decision, removing that feature significantly affects the classification score. That feature is therefore important to the simple model too.\nDeep Learning Toolbox provides the\nimageLIME function to compute maps of the feature importance determined by the LIME technique. The LIME algorithm for images works by:\nSegmenting an image into features.\nGenerating many synthetic images by randomly including or excluding features. Excluded features have every pixel replaced with the value of the image average, so they no longer contain information useful for the network.\nClassifying the synthetic images with the deep network.\nFitting a simpler regression model using the presence or absence of image features for each synthetic image as binary regression predictors for the scores of the target class.The model approximates the behavior of the complex deep neural network in the region of the observation.\nComputing the importance of features using the simple model, and converting this feature importance into a map that indicates the parts of the image that are most important to the model.\nYou can compare results from the LIME technique to other explainability techniques, such as occlusion sensitivity or Grad-CAM. For examples of how to use these related techniques, see the following examples.\nLoad the pretrained network GoogLeNet.\nnet = googlenet;\nExtract the image input size and the output classes of the network.\ninputSize = net.Layers(1).InputSize(1:2); classes = net.Layers(end).Classes;\nLoad the image. The image is of a retriever called Sherlock. Resize the image to the network input size.\nimg = imread(\"sherlock.jpg\"); img = imresize(img,inputSize);\nClassify the image, and display the three classes with the highest classification score in the image title.\n[YPred,scores] = classify(net,img); [~,topIdx] = maxk(scores, 3); topScores = scores(topIdx); topClasses = classes(topIdx); imshow(img) titleString = compose(\"%s (%.2f)\",topClasses,topScores'); title(sprintf(join(titleString, \"; \")));\nGoogLeNet classifies Sherlock as a\ngolden retriever. Understandably, the network also assigns a high probability to the\nLabrador retriever class. You can use\nimageLIME to understand which parts of the image the network is using to make these classification decisions.\nYou can use LIME to find out which parts of the image are important for a class. First, look at the predicted class of\ngolden retriever. What parts of the image suggest this class?\nimageLIME identifies features in the input image by segmenting the image into superpixels. This method of segmentation requires Image Processing Toolbox; however, if you do not have Image Processing Toolbox, you can use the option\n\"Segmentation\",\"grid\" to segment the image into square features.\nimageLIME function to map the importance of different superpixel features. By default, the simple model is a regression tree.\nmap = imageLIME(net,img,YPred);\nDisplay the image of Sherlock with the LIME map overlaid.\nfigure imshow(img,'InitialMagnification',150) hold on imagesc(map,'AlphaData',0.5) colormap jet colorbar title(sprintf(\"Image LIME (%s)\", ... YPred)) hold off\nThe maps shows which areas of the image are important to the classification of\ngolden retriever. Red areas of the map have a higher importance — when these areas are removed, the score for the\ngolden retriever class goes down. The network focuses on the dog's face and ear to make its prediction of golden retriever. This is consistent with other explainability techniques like occlusion sensitivity or Grad-CAM.\nGoogLeNet predicts a score of 55% for the\ngolden retriever class, and 40% for the\nLabrador retriever class. These classes are very similar. You can determine which parts of the dog are more important for both classes by comparing the LIME maps computed for each class.\nUsing the same settings, compute the LIME map for the\nLabrador retriever class.\nsecondClass = topClasses(2); map = imageLIME(net,img,secondClass); figure; imshow(img,'InitialMagnification',150) hold on imagesc(map,'AlphaData',0.5) colormap jet colorbar title(sprintf(\"Image LIME (%s)\",secondClass)) hold off\nLabrador retriever class, the network is more focused on the dog's nose and eyes, rather than the ear. While both maps highlight the dog's forehead, the network has decided that the dog's ear and neck indicate the\ngolden retriever class, while the dog's eye and nose indicate the\nLabrador retriever class.\nOther image interpretability techniques such as Grad-CAM upsample the resulting map to produce a smooth heatmap of the important areas of the image. You can produce similar-looking maps with\nimageLIME, by calculating the importance of square or rectangular features and upsampling the resulting map.\nTo segment the image into a grid of square features instead of irregular superpixels, use the\n\"Segmentation\",\"grid\" name-value pair. Upsample the computed map to match the image resolution using bicubic interpolation, by setting\nTo increase the resolution of the initially computed map, increase the number of features to 100 by specifying the\n\"NumFeatures\",100 name-value pair. As the image is square, this produces a 10-by-10 grid of features.\nThe LIME technique generates synthetic images based on the original observation by randomly choosing some features and replacing all the pixels in those features with the average image pixel, effectively removing that feature. Increase the number of random samples to 6000 by setting\n\"NumSamples\",6000. When you increase the number of features, increasing the number of samples usually gives better results.\nBy default the\nimageLIME function uses a regression tree as its simple model. Instead, fit a linear regression model with lasso regression by setting\nmap = imageLIME(net,img,\"golden retriever\", ... \"Segmentation\",\"grid\",... \"OutputUpsampling\",\"bicubic\",... \"NumFeatures\",100,... \"NumSamples\",6000,... \"Model\",\"linear\"); imshow(img,'InitialMagnification', 150) hold on imagesc(map,'AlphaData',0.5) colormap jet title(sprintf(\"Image LIME (%s - linear model)\", ... YPred)) hold off\nSimilar to the gradient map computed by Grad-CAM, the LIME technique also strongly identifies the dog's ear as significant to the prediction of\nLIME results are often plotted by showing only the most important few features. When you use the\nimageLIME function, you can also obtain a map of the features used in the computation and the calculated importance of each feature. Use these results to determine the four most important superpixel features and display only the four most important features in an image.\nCompute the LIME map and obtain the feature map and the calculated importance of each feature.\n[map,featureMap,featureImportance] = imageLIME(net,img,YPred);\nFind the indices of the top four features.\nnumTopFeatures = 4; [~,idx] = maxk(featureImportance,numTopFeatures);\nNext, mask out the image using the LIME map so only pixels in the most important four superpixels are visible. Display the masked image.\nmask = ismember(featureMap,idx); maskedImg = uint8(mask).*img; figure imshow(maskedImg); title(sprintf(\"Image LIME (%s - top %i features)\", ... YPred, numTopFeatures))\n Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. “‘Why Should I Trust You?’: Explaining the Predictions of Any Classifier.” In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 1135–44. San Francisco California USA: ACM, 2016. https://doi.org/10.1145/2939672.2939778."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:f8dfeef1-ff6f-43c3-8d2f-98c02c742d86>","<urn:uuid:1b74a121-327a-4e62-8b67-f71bed79d47c>"],"error":null}
{"question":"How did Red Bull use its beam wing arrangement differently from other teams?","answer":"Red Bull was the first to use a bi-plane style arrangement in their beam wing, where one element is positioned above the other. They only used the traditional arrangement when maximum downforce was needed, such as at the Hungarian circuit. The team had different configurations to help enhance their setup and fine-tuned the design.","context":["Given that the upper section of the endplate had been removed, a contoured transition was required to connect the mainplane and upper flap, and as a result, a complete reevaluation of the concept was necessary.\nAlthough the design limitations still offered multiple possibilities, teams had to find a compromise between generating downforce and the negative effect of drag.\nAs predicted, the spoon-shaped mainplane design was widely used throughout the grid, as it provided more flexibility in the transition area.\nAs the season progressed, teams began to take more risks with their designs. Some chose a more traditional, flat-profile mainplane, while others made notable changes to the shape of the upper corner of the endplate and wingtip, including the presence or absence of cutouts.\nRed Bull did not make as many changes to its rear wing as some of its competitors. It had a limited number of rear wing designs to suit the different circuit types and weather conditions on the calendar.\nRed Bull had more options for design variations with its beam wing arrangement. It not only fine-tuned the design but also had different configurations to help enhance its setup.\nThe team based in Milton Keynes was the first to use the bi-plane style arrangement, in which one element is positioned above the other. The more traditional arrangement was only employed when maximum downforce was necessary, for example at the Hungarian circuit.\nAs expected, Ferrari also had various designs to suit the different downforce needs of each circuit. But it also made adjustments to optimize its design in relation to the rest of the car. For example, two different rear wing configurations were used in Canada, with Carlos Sainz using a design for higher downforce, and Charles Leclerc using a new arrangement that emphasized the shape of the upper wing elements and beam wing.\nMercedes took a distinct approach to its initial design, choosing an upturned leading edge in the central part of the wing and a more compact transition to the endplate.\nThe team soon discovered that the wing was not performing as expected and made substantial reductions to the upper flap, and on several occasions, added a Gurney to the trailing edge to improve the balance of the car.\nA rear wing with low downforce was first introduced at the Miami Grand Prix, which eliminated the upturn on the leading edge of the mainplane and reduced the transition to the endplate.\nThe team continued to use similar methods, such as reducing the upper flap and adding a Gurney when necessary, to improve the balance of the car throughout the later stages of the Mercedes’ campaign.\nMercedes also had the ability to interchange panels in the wing tip section to optimize the required downforce and drag. The team also adopted the fully enclosed wing tip design, which was first implemented by Alpine in Saudi Arabia.\nAlfa Romeo used an innovative design in the tip section cutout (as indicated by the red arrow) to reach its drag and downforce goals in specific conditions and a conventional design for other situations (as shown in the inset).\nAston Martin was recognized for its unique rear wing design for 2022, which featured a rolled endplate extension that created an illusion of an endplate above the mainplane’s surface, violating FIA’s regulations. Despite its innovative design, no other team adopted this design, and it was banned by FIA from 2023 onwards.\nAston Martin also had the most unconventional rear wing design in Monza, featuring a twisted version of the spoon-shaped design."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:12c1434c-30db-4727-8991-1c635fbfa09d>"],"error":null}
{"question":"What is the connection between Alan Turing's wartime code-breaking work and his later contributions to artificial intelligence testing?","answer":"During World War II, Turing worked at Bletchley Park breaking Nazi Germany's Enigma codes using the Turing-Welchman Bombe machine, which performed logical deductions to decipher encrypted messages. This experience with mechanical problem-solving later influenced his approach to artificial intelligence testing. In 1950, he developed the Turing Test, which similarly relied on logical deduction but applied to the challenge of determining machine intelligence through conversation. Both endeavors involved creating systematic methods to process and evaluate complex information - decrypting enemy codes in one case, and assessing machine intelligence in the other.","context":["In the first of a two-part essay, writer and tech philosopher Tom Chatfield looks at an early progenitor of artificial intelligence.\nIn 1770, the inventor Wolfgang von Kempelen displayed a mechanical marvel to the Viennese court. Watched by the Archduchess Maria Theresa and her entourage, he opened the doors of a wooden cabinet four feet long, three feet high and just over two feet deep, illuminating its interior by candlelight to display glistening cogs and gears. Seated at the cabinet was a life-size model of a man in Turkish dress—a turban and fur-trimmed robe. In front of the Turk, on top of the cabinet, was a chessboard.\nKempelen closed his cabinet and asked for a volunteer to play a game of chess against the Turk. It was astonishing request. Finely crafted automata had been entertaining royalty for centuries, but the idea that one might undertake an intellectual task such as chess was inconceivable—something for the realm of magic rather than engineering. This was precisely the point. Six months earlier, Kempelen had claimed to the Archduchess that he was utterly unimpressed by magic shows, and could build something far more marvelous himself. The Turk was his proof.\nCount Ludwig von Cobenzl, the first volunteer, approached the table and received his instructions: the machine would play white and go first; he must ensure he placed his pieces on the centre of each square. The count agreed, Kempelen produced a key and wound up his clockwork champion, and with a grinding of gears the match began. To its audience’s astonishment, the machine did indeed play, twitching its head in apparent thought before reaching out to move piece after piece. Within an hour, the Count had been defeated, as were almost all the Turk’s opponents during its first years of growing renown in Vienna.\nHumanity, for so long self-defined as the pinnacle of nature, had begun to feel less than mighty in the face of its own creations.\nA decade later, Maria Theresa’s son, Archduke Joseph II, asked Kempelen to bring his creation to a wider public. The Turk visited Paris, London and Germany, inviting fervent speculation wherever it went. Among its losing opponents were Benjamin Franklin, visiting Paris in 1783, and—under its second owner after Kempelen’s death—the emperor Napoleon in 1809. Napoleon tested the machine with illegal moves, only to see the Turk sweep the pieces off the board in apparent protest.\nIt was, of course, a fraud—a magic trick masquerading as a mechanism. Behind the cogs and gears lay a secret compartment, from within which a lithe grandmaster could follow the game via magnets attached to the underside of the board, moving the Turk’s arm through a system of levers. In his book The Turk, the British author Tom Standage tells the story in captivating detail—noting that even the unmasking of its workings in the 1820s scarcely diminished the age’s fascination with the Turk. The image of man and machine locked in combat across the chessboard was simply too perfect—and too perfectly matched to a growing unease around technology’s usurpation of human terrain.\nHumanity, for so long self-defined as the pinnacle of nature, had begun to feel less than mighty in the face of its own creations. The Industrial Revolution brought fire and steam as well as clockwork into the public imagination, together with anxieties that have echoed across society since: of human redundancy in the face of automation, and human seduction by new kinds of power.\nKempelen’s desire to make not simply a machine but also a kind of magic trick was no accident. The Turk set out to inspire belief, and had picked the perfect arena for persuasion: a bounded zone within which complex questions of ability and intellect were reduced to a single dimension. Sitting down opposite a modern reconstruction of the Turk in Los Angeles, Standage found himself surprised by how “remarkably compelling” the illusion remained, speaking to “its spectators’ deep-seated desire to be deceived.” Tools that can master a task are one thing—but it’s when they are also able to engage and enthrall that enchantment begins.\nLong before digital computers had gained a genuine mastery of chess, one man devised a twentieth-century game with some remarkable similarities to Kempelen’s scenario. “I propose to consider the question, ‘Can machines think?'” wrote Alan Turing in his 1950 paper “Computing Machinery and Intelligence.” The trouble with such a question, he observed, was that answering it was likely to involve splitting hairs over the meaning of the words “machine” and “think.” Thus, he continued, “I shall replace the question by another, which is closely related to it and is expressed in relatively unambiguous words. The new form of the problem can be described in terms of a game which we call the ‘imitation game.’”\nTuring’s imitation game entailed a conversation between a human tester and two hidden parties, each communicating with the tester via typed messages. One hidden party would be human, the other a machine. If a machine could communicate in this way, such that the human tester could not tell which of their interlocutors was machine or human, then the machine would have triumphed. “The game may perhaps be criticized,” Turing noted, “on the ground that the odds are weighted too heavily against the machine. If the man were to try and pretend to be the machine he would clearly make a very poor showing. He would be given away at once by slowness and inaccuracy in arithmetic.” Pretending to be human meant embracing limitations as much as showing strengths.\nThe answer was likely to involve splitting hairs over the meaning of the words “machine” and “think.”\nTuring’s thought experiment suggested that, if the impression created were robust enough, the means of its achievement became irrelevant. If everyone could be fooled all the time, whatever was “really” going on inside the box ceased to matter. A game was the perfect test of intelligence precisely because it abandoned definitions in place of a challenge that could be passed or failed, as well as endlessly restaged. By excluding the world in favor of a staged performance, it made the ineffable conceivable.\nTuring’s game is today played for real during the annual Loebner Prize, which since 1991 has promised $25,000 for the first AI that judges cannot distinguish from an actual human—and that convinces its judges that their other, human conversation partner must be a machine. No machine has got close to winning this award, but competing AIs are ranked in order of achievement. To the frustration of many AI specialists, the most successful chatbots tend to use tricks based on stock responses and emotional impact rather than understanding. Much like the grandly dressed Turk two centuries ago, the setup rewards the use of distractions and deceits: bogus bios, pre-programmed typing errors, hesitations, colloquialisms and insults. To win an imitation game, in certain circumstances at least, is not so much about perfect reproduction as targeted mimicry.\nThis is echoed in the world at large. If and when people are fooled by modern AIs, something more like stage magic than engineering is going on—a fact emphasize by the importance that companies like Apple, Amazon, and Google attach to quirky features which make their creations more appealing. Ask Amazon’s digital personal assistant Alexa whether “she” can pass the Turing test and you’ll get the reply, “I don’t need to pass that. I am not pretending to be human.” Ask Apple’s Siri if “she” believes in God and you’ll be told, “I would ask that you address your spiritual questions to someone more qualified to comment. Ideally, a human.” The responses have been pre-scripted both to amuse and to disarm. They’re meant to fool us into perceiving not intelligence but innocence—products too charming and too useful to provoke any deeper anxiety. The game is not what it claims to be.\nCheck out part two of Tom Chatfield’s essay here.","Last Updated on November 10, 2020 by Sean B\nAlan Turing is one of the biggest names in the field of computer sciences, crypto analysis, artificial intelligence, and even the philosophy of artificial intelligence and mechanic consciousness.\nAlan Mathison Turing left an enormous legacy behind him and is an inspiration for many in the field of artificial intelligence and computer science. Turing not only lent his contributions towards the development of computers and artificial intelligence, but he also saved millions of lives during World War II.\nIn this blog, we shall be looking at a brief biography of Alan Turing and his contributions towards the development of computer science, Artificial Intelligence, and modern-day Chatbots.\nAlan Mathison Turing was born in Maida Vale on 23rd of June 1912, in London. Alan’s father, Julius Mathison Turing, was part of the Indian Civil Service (ICS) and was stationed in India’s Odisha state. Alan Turin’s mother was Ethel Sara Turing.\nDue to Turing’s father’s position in ICS, the family of Turing spent a lot there in India. However, Turing’s parents wanted to raise their son in Britain, so the family moved to Maida Vale in London.\nTuring was the second son of Julius and Ethel. Since his adolescence, Alan Turing showed signs of genius. The job of Julius requires him to visit and travel to different places. Parents of Turing would leave their two sons with a retired army couple.\nAlan Turing was enrolled in St Michael’s, a day school at 20 Charles Road, St Leonards-on-sea when he was six. Since his adolescence, he was known by his teachers for his remarkable genius. Alan Turing was regarded as a child prodigy.\nBetween 1922 and 1926, Turing was admitted to Hazelhurst Preparatory School. Then in 1926, Turing was admitted to Sherborne School at the age of 13. Sherborne school was a boarding school in the town of Sherborne in Dorset.\nTuring was enormously inclined towards the subjects he was interested in. Alan Turing once rode to his school on a bicycle for 97 kilometers to attend the first day of his term, which coincided with the 1926 general strike.\nTuring was able to accomplish many impressive feats in school life. Turing was able to solve advanced problems in 1927 without taking courses on even the elementary versions of Calculus. At age 16, Alan Turing was able to grasp Einstein’s work, specifically Einstein’s criticism of Newtonian physics.\nUniversity Life and Research\nAfter completing his studies in Sherborne, Turing started studying at King’s College, Cambridge. Turing received many first-color honors in mathematics. At age 22, Turing proved the Central Limit Theorem.\nIn 1936, Turing spent his time studying under the church at Princeton University. Turing also studied cytology along with his mathematical works. Turing also built three stages of an electro-mechanical binary multiplier. Turing received his Ph.D. from the Department of Mathematics in Princeton in June 1938.\nThe Entscheidungsproblem was a problem in mathematics and computer sciences first posed by David Hilbert and Wilhelm Ackermann in 1928. The German word for the problem translates to a decision problem.\nThe problem asks for an algorithm that can respond to our logical questions in yes or no answers. The questions will consist of universally accepted notions such as axioms. Turing published his work “On Computable Numbers, with an Application to the Entscheidungsproblem.”\nTuring’s work put forth the idea of his “Universal Turing Machine” and promised that his machine would be able to perform the feats the problem demands it to. If an algorithm is provided to the machine, then the machine will be able to perform any mathematical operation.\nTuring’s paper has been known as the most influential work in mathematics. Ad mist his work, Turing gives the abstract idea of the Universal Turing Machine, which acted as an antecedent towards the development of modern computers.\nAlan Turing accomplished many feats throughout his career, which includes providing an abstract idea of a Universal Turing Machine, breaking the Enigma Code, and proposal of the Turing test.\nUniversal Turing Machine\nUniversal Turing machine was an abstract idea of a machine capable of stimulating the description of the machine as well as the input on that machine, which is stored in a tape. Alan Turing introduced his idea in 1936.\nThe Universal Turing machine acted as an inspiration for a stored-program computer created by John Von Neuman in 1946. These Turing ideas helped the computer sciences take their intuitions about automata one step further and opened limitless probabilities of advancement in the field of computer science.\nThe Code Breaker and Cryptoanalysis of the Enigma Machine\nAlan started working part-time with the British Government before the events of World War II at the Code and Cypher School (GC&CS;), the British code-breaking organization. Alan became a full-time participant in the cryptanalysis of the German ciphers at Bletchley Park in 1939.\nAlan’s work was focused on deciphering the encrypted messages of the Enigma Machine used by Nazi Germany. His work was in collaboration with his partners Dilly Knox and a senior code breaker at GC&CS;.\nThe polish cipher bureau was successful in deciphering the Nazi codes before it. However, Germans had strengthened their safety protocols, which made breaking the enigma code a harder task than before.\nWith the sharing of information given by the polish cipher bureau, Alan Turing and his team at the Bletchley Park was able to create a bombe machine used for decryption of the enigma codes. Alan Turing also lent his help deducing the indicator procedures used by Nazi Germany.\nThe Turing-Welchman Bombe was an electromechanical machine specified by Alan Turing. It was based on the Polish Bomba Kryptologizna. The Bombe was highly effective at breaking and deciphering of Nazi Germany’s messages encrypted by the Enigma Machine.\nThe Bombe worked using a crib-based decryption system. It searched for possible settings used by the Enigma Machine, such as the rotor order, rotor settings, and plugboard settings using a crib. The crib performed logical deductions and was effective at reducing the work of the cryptanalysts at Bletchley Park.\nAlan Turing broke the code of the Enigma Machine and the code of the German navy. The British departments were facing increasing difficulties in breaking the German navy code, so Alan decided.\nAlan solved the German navy indicator and further advised the use of Banburismus, a cryptanalytical process that was essential in breaking the code of the German navy. Alan Turing, along with Us navy cryptanalysts, worked on the naval Enigma and his efforts were effective in reducing the work of the cryptanalysts in deciphering the codes.\nThe Turing Test and Antecedents Towards the Development of Early Computers and Artificial Intelligence\nAlan Turing initially called his test the Imitation Game when he created it in 1950. The fact that it has come to be known as the Turing Test shows his importance in the field of Artificial Intelligence. The Turing Test revolves around testing a machine’s capability to exhibit intelligent behavior akin to a human being. Turing wrote a paper in 1950 titled “Computer Machinery and Intelligence,” where he first proposed his test.\nThe Turing Test can be summed up in one question, “Can machines think?” Alan Turing pondered upon this profound question and advised a test to determine the machines’ ability to think.\nA Turing Test is carried out between two participants and an interrogator. One of the participants is human, while the other is a machine. The interrogator’s job is to simultaneously hold a conversation between the two participants and determine who is human and who is a machine.\nTuring Test was profoundly inspirational in the sense that it opened up new possibilities for automata and was a precedent to early developments in the field of Artificial Intelligence. The Turing test’s real-world implication can be seen in the form of the Loebner Prize, which announces a prize for whoever wins the Turing test.\nA reversed version of the Turing Test is used in the form of CAPTCHA as a safety protocol on websites to find out whether the person interacting with the website is human or a bot.\nDeath and Legacy\nAlan Turing died early because he was homosexual; he and his 19-year-old sexual partner Murray were charged with indecency after Turing admitted to being in a sexual relationship with Murray. At the time, being homosexual was illegal in Britain, and sadly in much of the world. Under section 11 of the Criminal Law Amendment Act 1885, both Alan and Murray were charged with gross indecency. Turing was convicted and was given a choice between imprisonment and probation with chemical castration.\nAlan chose the second option; Alan would be given Diethylstilbestrol injections, a synthetic estrogen designed to eliminate his libido. The doses also resulted in the development of breast tissues in Alan Turing.\nAlan Turing was found dead by his housekeeper on 8 June 1954. The cause of death was regarded as Cyanide poising, and it is believed that he committed suicide by taking lethal doses. Turing’s body was cremated, and the ashes were scattered in Working Crematorium.\nAfter his death, the prime minister of Britain, Gordon Brown, issued an apology to Turing following a petition by John Graham-Cumming. Britain made a policing and crime act law in 2017, which included an Alan Turing Law, which pardons men who were historically convicted under homosexual acts.\nAlan Turing has left an enormous legacy behind him, his contributions in the field of computer sciences and Artificial Intelligence are limitless, and there’s no doubt that he is a pioneer in mathematics, logic, and cryptoanalysis.\nFirst of all, Alan Turing’s contributions to the world were astounding, and in retrospect, I believe that all decent human beings can see that his treatment simply because of who he loved was horrible and even criminal. Now that I have made that statement clear, let’s talk about his work. Turing, along with the others at Bletchley Park, solved the encryption of the Enigma Machine using an earliy computing device he helped design and build. It is estimated that this saved the lives of more than 14 million people. That act alone makes him one of the most important figures of the 20th Century.\nBut his contributions to the developments of Computing and Artificial Intelligence may have had an even greater impact. How many lives have been saved by the use is AI in the fields of Medicine? How many lives have been saved by the use of Computers to predict the paths of Hurricanes and Tornadoes? And those are just two fields that Computers have had an impact in.\nThe creation of chatbots is a small part of what Alan Turing had a hand in creating, but that’s how it is with great minds, they touch so many different small things with their minds that we tend to forget how big they were as a whole.\nAlan Turing was a giant.\nWe hope that the blog was informative.\nWe look forward to reading your thoughts in the comments."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:dd94a016-fe0c-467d-8427-69bf132d6857>","<urn:uuid:94ce1616-26de-4ebf-b7c8-dfcab87c00ce>"],"error":null}
{"question":"How many Allied sailors died in the Battle of Savo Island during World War II?","answer":"1,200 Allied sailors died in the Battle of Savo Island near the Solomon Islands during World War II.","context":["Space to play or pause, M to mute, left and right arrows to seek, up and down arrows for volume.\nSole survivor fights to clear WWII shadow\nThe sole survivor of an Australian air crew wrongly blamed for a naval disaster near the Solomon Islands in World War Two is determined to keep up the fight to have the history books corrected.\nLEIGH SALES, PRESENTER: For more than 70 years American historians have wrongly blamed an Australian air crew for contributing to a Second World War naval disaster.\nTwelve hundred allied sailors died in the battle of Savo Island in the Solomons.\nThe sole survivor of the air crew is now 94-years-old and he's never stopped fighting to have history corrected.\nAdam Harvey reports.\n(Archival newsreel footage in background)\nADAM HARVEY, REPORTER: August 1942 - desperate days in World War 2. After the disaster at Pearl Harbour, US forces are finally striking back against the Japanese at Guadalcanal in the Solomon Islands.\nEXCERPT NEWSREELS (Archival): Under cover of the fleet's concentrated fire, the marines land. The attack took the Nips completely by surprise but Japanese reinforcements are on their way and the allied fleet stands by.\nADAM HARVEY: The marines have a toehold but they're vulnerable to counter-attack. An Australian air crew is on the lookout - four young men, average age 22.\nERIC GEDDES: When we joined up we became friends and we were friends for the rest of our lives. I'm the sole survivor and the last man standing.\nADAM HARVEY: Eric Geddes is radio operator and gunner. His battle didn't end in 1945.\nFor 70 years he's been fighting a terrible slur against his crew.\nERIC GEDDES: If I printed in tomorrow's newspaper that you were responsible for the deaths of 1,023 sailors, how would you feel?\nWe were angry and we couldn't believe that this could be. We just couldn't believe it.\nADAM HARVEY: Eric Geddes and the crew of the Lockheed Hudson were part of an RAAF squadron at Milne Bay, in New Guinea.\nERIC GEDDES: They were all low level patrols and our purpose was to gather intelligence about Japanese movements in the area, whether or not they were active or whether they just weren't about or what was happening.\nADAM HARVEY: His immaculate logbook records each patrol, including one on August 8th 1942, after the marines land at Guadalcanal.\nERIC GEDDES: I could nominate that particular event from take-off to landing and I wouldn't miss a trick.\nADAM HARVEY: The Hudson crew tracked north from Milne Bay, over Goodenough Island, and spotted a Japanese convoy steaming towards Guadalcanal.\n(to Eric Geddes) How far away were they?\nERIC GEDDES: Well, we didn't need the binoculars to work it out, let's put it that way.\nThey then sent up two fighters to take care of us so we thought well, we can't stand and fiddle around with these people because we've got to deliver this intelligence. And I pulled out the radio and tried to contact Milne Bay.\nADAM HARVEY: There was no answer. Eric Geddes kept tapping away in Morse Code on his radio transmitter.\nERIC GEDDES: When we arrived back at Milne Bay they were surprised we were back early and the intelligence officer arrived at the end of the runway when we landed. We were debriefed, and we gave them precisely what I've just told you.\nADAM HARVEY: Thanks to the Australians' work, the Allied ships defending Guadalcanal should have known what was coming, but Eric Geddes' warning was not acted upon, with devastating consequences at a place called Savo Island.\nThe HMAS Canberra was part of the fleet.\nEXCERPT NEWSREEL (Archival): Then night, and a counterattack by Japanese warships.\nMAC GREGORY, HMAS CANBERRA: I was officer watch on the bridge of Canberra, but we were just sitting there when suddenly this cruiser force of five heavy cruisers, 1 destroyer and two light cruisers suddenly attacked us.\nWe were out of the war after two and a half, three minutes.\nADAM HARVEY: The Allies were routed. More than 1,000 sailors died. Three US ships and the Canberra were lost.\nRICHARD FRANK, ASIAN PACIFIC WAR HISTORIAN: The battle of Savo Island was undoubtedly the most humiliating defeat at sea suffered by the US Navy during the entirety of the Second World War.\nADAM HARVEY: And the Aussies got the blame.\nDR CHRIS CLARK, NAVAL HISTORIAN: To the Americans it looked like they hadn't done their job properly and allowed the Japanese to obtain an element of surprise that resulted in such a disastrous outcome.\nADAM HARVEY: Harvard historian Samuel Morrison was commissioned by president Roosevelt to write a history of the Pacific battles and he attributed the Allies' defeat at Savo Island in part to the Australian Hudson crew.\nSAMUEL MORRISON: The pilot of this plane, instead of breaking silence to report, as he had orders to do in an urgent case, or returning to base which he could have done in two hours, spent most of the afternoon completing his search mission, came down at Milne Bay, had his tea, and then reported the contact.\nMAC GREGORY: He was accused by Morrison, the naval historian, of not breaking silence, of dilly-dallying on the way back to Port Moresby and having his tea before he was debriefed, all of which was complete nonsense.\nRICHARD FRANK: We don't really know exactly where Morrison got the notion that the Hudson crew had stopped to have tea before they turned in the report. We do know that he was, like most of the Americans who had been familiar with the operation, was very humiliated by what had happened.\nERIC GEDDES: We were pretty upset about this and disturbed about the situation.\nADAM HARVEY: You can understand why Eric Geddes is still so unhappy more than 70 years later when you come here, to the Australian War Memorial.\nSamuel Morrison's 14 volume work on World War II is part of the collection so it's one of the places where his claims about the Hudson crew live on.\nSamuel Morrison has long since been proved wrong.\nDR CHRIS CLARK: Ship for ship the Japanese had a superior navy and that may be unpalatable to the United States but they were simply out fought.\nRICHARD FRANK: Well I would say to Mr Geddes that his mission has been accomplished, through his efforts and also through the work of other historians. The failures that did occur that led to the defeat at Savo Island were multiple and shared many levels, primarily by Americans, and no one now writing about this would single out and certainly write in such demeaning terms about this particular Australian air crew.\nADAM HARVEY: But Eric Geddes wants official US recognition that the Australians did their best.\nERIC GEDDES: My problem is not our history. It's American history, and that has, that has a lot to answer for.\nADAM HARVEY: He's taken his campaign to the top.\nERIC GEDDES: My ambition is that president Obama actually gets to read the letter I wrote to him. I think he's a man who really would understand the truth and facts when he reads them.\nAnd to be respectful of my other three comrades who are no longer here to talk for themselves, I feel that I have the liability of doing this and I suppose the day I drop off the perch will be the day that I'll forget about it.\nLEIGH SALES: What an amazing 94-year-old. Adam Harvey reporting.\nSee how this story turns out, as Adam Harvey catches up with Eric Geddes at the conclusion of his long battle for justice.\nAuthor Adam Harvey"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:ba7eedd9-55c9-4ec7-917f-c8a4091d9a8d>"],"error":null}
{"question":"IOT vs blockchain cost savings food industry?","answer":"IoT and blockchain technologies both offer cost savings in different ways. IoT implementation helps reduce operational expenses by automating temperature monitoring, decreasing waste through predictive analytics, and improving efficiency in task management and facilities maintenance. Blockchain, particularly through blockchain-as-a-service (BaaS) platforms, offers cost-effective deployment through monthly subscription-based models that make it affordable for even small to mid-sized food brands to implement full traceability. Both technologies help minimize costly product quality issues and improve operational efficiency by providing real-time monitoring and data analysis capabilities.","context":["Whether driven by regulatory factors or brand protection, the food industry has adopted advanced monitoring and management technologies to maintain and support modern day food safety cultures and operations within companies. This type of technology utilizes checklists and sensors designed to monitor and gather data. Typically, these are built into handheld devices that store collected information in the cloud. The stored data is instantly accessible for management to monitor. Additionally, the FDA demands that two years worth of records be on hand during an inspection. Instead of sorting through copious piles and ﬁle drawers of paper, the information can be pulled directly from the database and presented to the inspectors.\nIn an effort to improve the operation, food companies should adopt new processes to be proactive and ditch the old days of manually tracking and recording temperature data. Utilizing this type of technology ensures consistency, transparency and quality. Not to mention the increase in eﬃciency and savings over time.\nPerhaps the most powerful technological methodology to implement is using the Internet of Things (IoT) to improve processes in the supply chain. How is IoT relevant for a food safety strategy? In an integrated approach to food safety, IoT temperature sensors dispersed throughout the cold and hot food chain, coupled with a food safety/task management system for taking HACCP (Hazard Analysis and Critical Control Points) required food safety temperature measurements, provides unprecedented visibility and traceability, for an end-to-end food safety strategy for grocery stores or restaurants. It’s worth noting that customers are not necessarily looking for an IoT solution when they start the process for acquiring a solution for monitoring coolers/freezers or grills, but for an automated temperature monitoring system whose data is cloud-based and just so happens to be available via the internet.\nData from IoT sensors dispersed through the food chain is continually collected and analyzed to ensure temperatures do not exceed pre-deﬁned limits. These limits are based upon HACCP guidelines. The collected data is then subsequently stored electronically for a period speciﬁed by the user, typically up to two years from the collection date per FSMA regulations. If a temperature measurement falls outside pre-deﬁned limits, an alert via text or SMS can be sent to the end-user for corrective actions. Recent developments in IoT have also coupled active monitoring with predictive analytics to determine appliance health.\nShould an issue occur in the food chain, food safety data would then be correlated with transactional data to not only deﬁne when a limit was exceeded, but to potentially trace the impact to the consumer or in- store sales/proﬁtability. Additionally, high or low sales of a speciﬁc item could also be equated to how the item is prepared.\nUtilizing checklists that guide operational eﬃciency, powered by IoT technologies is not only limited to food safety. The capabilities of IoT can be deployed for task management or facilities maintenance practices such as entry/exit applications, facility maintenance/sweep logs, CO2 sensing (beverage and condiment), customer queue length for ordering or check out and incident reporting—when the documentation of an incident is required should a customer or employee incur an injury within the facility.\nThe implementation of comprehensive end-to-end food safety and task management strategy utilizing remote monitoring based upon IoT promises to provide businesses with a new cornerstone for building a comprehensive and preemptive food safety and facilities plan. By meeting the strict requirements of HACCP regulations, companies can continually reduce operational expenses, decrease waste and potentially predict events that could aﬀect the food chain and subsequently the consumer. An integrated approach to food safety utilizing a food safety/task management system with IoT can positively inﬂuence all consumers within the restaurant, grocery and food chain realms.","Traceability is essential in the snack and baking industry. Ingredients always need to be tracked, and this is especially important in the case of a recall. These days, implementing a comprehensive software suite is often an essential step in establishing a strong food safety program.\n“When it comes to traceability, distributed ledger technology, or ‘blockchain,’ is often the first thing that comes to mind for the modern food producer,” says Pratik Soni, founder and CEO, Omnichain Solutions, Los Angeles. Many are keen to implement the technology, as it provides a decentralized, digital record of every step in a product’s lifecycle—from raw ingredients, through production, to distribution to the retail shelf and in the consumer’s hands, he says. “Once logged on the ledger, the data are immutable and shared with everyone across the supply chain, ensuring full traceability, transparency, and accountability.”\nSome companies may think that blockchain for the food supply chain is still something in its infancy, but there are very mature solutions available today, explains Soni. “Namely, blockchain-as-a-service (BaaS) platforms, like the one offered by Omnichain, are available now, that enable manufacturers to go fully live and functional with their own distributed ledger—connecting the many disparate systems, records and databases across their supply chain—through simply an internet browser in under 90 days,” he notes.\nSince users only need an internet browser to connect to the network, BaaS platforms can scale along with a business as it grows, says Soni. “A monthly subscription-based payment model also makes deployment affordable so that small to mid-sized food brands can reap the same traceability benefits as a large corporation. In today’s society, where consumers now demand more information about the goods that they purchase, definitive insight into product provenance can be a powerful differentiator.”\nTraceability software should be a standard in any ERP package that deals with the food and beverage industry, says Erin Schlee, marketing communications manager, SYSPRO USA, Costa Mesa, CA. “SYSPRO Software has had lot traceability built into the core of the system for years, enabling companies to track ingredients from raw materials all the way through to finished product.”\nChristine C. Akselsen, CEO, Kezzler, Oslo, Norway, says that Kezzler has recently enhanced its platform’s traceability capabilities. “While previously focused on enabling product authentication and production control, we have now moved to integrate our platform deeper into the complete lifecycle of products.” This includes deeper integration of the platform into supply-chain systems, enhancing track-and-trace functionalities, and support for global interoperable product identification standards like GS1’s Digital Link, she says.\nThis interoperability and capability of integration lends support for global traceability initiatives, such as Russia's “cryptocode” serialization mandate (Federal Law No. 488-FZ), which becomes effective this January—and which will extend to almost all product areas, notes Akselsen.\nFurther regulation around food safety has moved manufacturers toward much more detailed lot tracking and traceability, says Wayne Ortner, senior product consultant, FlexiBake, Vancouver, British Columbia. “ERP and WMS like FlexiBake have needed to adapt to keep up with the new regulations and trends. FlexiBake’s warehouse management system has detailed lot tracking on both ingredients and on packaging, palletization by lot numbers, intelligent placement of finished goods, and detailed raw material picklists generated from a live inventory.”\nOne of the major barriers FlexiBake has seen when it comes to the idea of scanning lot numbers into inventory is that most major suppliers will supply product with lot numbers on their ingredient packages, but barcoded lot numbers are not yet included, meaning there is no way to scan lot codes into any system, Ortner adds. “The only way to integrate lot code scanning into an automated production floor is to manually write input the lot number, create the barcode and label, and then stick it on the ingredient yourself. Once suppliers include scannable lot numbers on barcodes, it will make integrating scanning devices much more attainable for manufacturers.”\nElemica, Wayne, PA, has a new quality solution that integrates and simplifies multiple challenging facets of today’s supply chain, says Kurt Nusbaum, VP, quality management solutions. “Ranging from multitier traceability through raw material product quality and supplier nonconformance management, this new solution provides integrated visibility previously not available to manufacturers and distributors.”\nStreamlining communication among suppliers, customers, and manufacturing sites creates an inter-enterprise traceability, quality, and compliance solution that minimizes costly product quality and delivery issues, notes Nusbaum. “Extending supply chain data gives manufacturers and participants the upstream visibility and control they need to provide improved downstream customer service. Elemica offers an end-to-end digital supply network that not only provides traceability well beyond first-tier suppliers and throughout the manufacturing process, but also captures quality data in the form of an electronic certificate of analysis (e-COA).”\nBy linking supplier COA data with shipment information, you not only gain traceability, but you also receive real-time alerts to risky material variation within the supply chain, Nusbaum adds. Elemica’s traceability and quality management solution also provides supplier nonconformance and regulatory tracking and automation, he says. “The combination of visibility, traceability, and transparency into supply chain processes means you gain molecular insights into what is happening within your supply chain, from raw materials to end customers. This includes all documentation, transaction, data exchange, messaging, etc.—all shared on a networked platform so that all trading partners have accountability and understand escalation paths, if needed.”\nThe deep knowledge and insights gained from information within the supply chain, along with quality information shared on COAs, means that risk can be minimized, saving costs and improving customer satisfaction, he explains.\nMatt Brown, CEO, Wherefour, Petaluma, CA, says that they’ve been impressed with the ruggedized tablet and phone-sized products that Zebra Technologies has introduced. “These are laser scanners with highly accurate scanning capability. Zebra has 8- and 10-inch Android tablets with built-in laser barcode scanners. They are a convenient and easy-to-hold size with a catcher’s mitt style grip.”\nThe tablets are ruggedized to work in cold and wet environments, which is common in many food applications, Brown notes. “Having a built-in laser barcode scanner is a big feature, because laser scanning is more accurate than phone or tablet cameras. Wherefour recently announced a partnership to integrate its traceability and ERP software with Zebra devices. This means a user can have everything they need in one tablet.”\nOrtner says that he’s seen a major shift in the hardware that’s being used to run today’s food manufacturing software. “Historically, manufacturers would opt for an industry-specific or manufacturing-specific device to run their software. Today, more and more companies are using smartphones, tablets, and mobile printers that are not necessarily designed for the food manufacturing industry.”\nTechnology is also growing more predictive. Schlee says that RFID software and handheld scanners are standard products, but AI and machine learning can assist with this in the sense of contamination and allergies. “For example, with allergies, you might use artificial intelligence (AI) to predict contamination. And AI tied with IoT sensors could figure out temperature fluctuations in the environment or moisture content that could cause mold, bacteria, and contamination,” she notes.\n“Kezzler has greatly enhanced its traceability capabilities with the advent of our latest release,” Akselsen says. “Our client base is now leveraging those capabilities to support their customer demands for farm-to-fork and grass-to-glass traceability. One great example of this is FrieslandCampina, one of the world’s largest dairy cooperatives, which uses our traceability connectivity suite to link every single serialized product back to its farm of origin, aggregate production information, track distribution from production to consumption, and finally engage with their consumers all through one single unified touchpoint, which is a Kezzlerized GS1 Digital Link.”\nBrown says that Wherefour has made some recent enhancements to its ability to trace ingredients from the loading dock through production and onto the delivery truck into customers’ hands. “Our users now are able to upload virtually any document they want to store with purchase orders such as certificates of analysis, photos of received goods and bills of lading,” he comments. “We have made some changes to our production trace report to delve even deeper for forward or backward ingredient searches. The report can be run to show all events for a specific lot code in greater detail, such as material that was produced in a separate lot but included in the current batch. Reports also can show a complete breakdown of the inventory used for a product, along with the vendors who supplied those ingredients and to which customers that product was shipped.”\nOrtner says that tablets are being used for inventory counts, palletization, and picking product for shipments. “Technology is evolving to become more user-friendly and the majority of manufacturers are opting for a cloud-based software solution that can be used on any commercially available device, rather than deploying specific hardware configurations used for locally installed software.”\nTracking incoming ingredients is not a new technology, and SYSPRO USA has offered it for quite a while, says Schlee. “These technologies continue to become more advanced as things like IoT and blockchain, and machine learning mature, but robust ERP technology made for food and beverage should have this offering as part of its core.”\nWith the maturation of distributed ledger technology, Soni says he’s seeing it naturally converge with other impactful technologies like AI and machine learning. “After all, if you have holistic data logged in a single digital ledger, that makes it even easier for AI to get in and start going to work, finding connections and correlations that would otherwise be unknown, difficult or time consuming for decision makers,” he notes. “At Omnichain, we’re introducing AI and machine learning into our platform to process the vast amounts of supply chain data on our customers’ distributed ledgers, enabling more predictive supply chain management. From these data, the platform can generate deliver proactive insights to users, such as verifications of product provenance or identification of the source of a quality or safety issue in the supply chain.”"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:daee02e5-a496-4c83-a41e-8824082c73e8>","<urn:uuid:d2910b28-2e97-4ead-b04e-1db2d01ab420>"],"error":null}
{"question":"Which is more influenced by genetic factors: nicotine content in vegetables or nicotine addiction in humans?","answer":"Nicotine addiction in humans is more influenced by genetic factors. Scientific studies have shown that genetic factors account for about 60.3% of nicotine dependence heritability. In contrast, the nicotine content in vegetables like tomatoes, potatoes, and eggplants is primarily determined by their natural composition as members of the Solanaceae family, with consistent amounts ranging from 2-7 micrograms/kg in fresh fruits, rather than being influenced by genetic variation.","context":["Results show that all the peppermint plants contain minor amounts of nicotine before treatment, but the experiments revealed that the plants also incorporate nicotine considerably from the soil as well as from tobacco smoke. … The incorporated nicotine was subsequently metabolised by the plants.\nWhat herbs contain nicotine?\nNicotine is an alkaloid found in the nightshade family of plants (Solanaceae), predominantly in tobacco, and in lower quantities in tomato, potato, eggplant (aubergine), and green pepper. Nicotine alkaloids are also found in the leaves of the coca plant.\nWhat foods contain nicotine?\nFoods That Contain Nicotine\n- Tomatoes. An average tomato has a nicotine concentration of 7.1 -7.3 ng/g. …\n- Potatoes. An average potato has a nicotine concentration of 15 ng/g. …\n- Eggplants. Eggplants (aubergines) have a concentration of 100 ng/g of nicotine. …\n- Teas. …\n- Peppers and Capsicums. …\nWhat products have nicotine in them?\nAll tobacco products contain nicotine, including cigarettes, non-combusted cigarettes (commonly referred to as “heat-not-burn tobacco products” or “heated tobacco products”), cigars, smokeless tobacco (such as dip, snuff, snus, and chewing tobacco), hookah tobacco, and most e-cigarettes.\nIs nicotine present in tea?\nEndogenous nicotine was confirmed to be present in tea plants (Camellia sinensis L.) by liquid chromatography-tandem mass spectrometry of tea samples from tea-producing regions in six Asian countries. All samples contained nicotine (0.011–0.694 μg g⁻¹ dry weight).\nDoes ketchup have nicotine?\nKetchup generally contains a significant amount because it is processed tomato. Tea often (but not always) contains nicotine. In fact it generally contains five alkaloids, which of course is why it works – if it didn’t then you would just drink water, it’s quicker, easier and cheaper.\nDoes chocolate have nicotine?\nCaffeine and nicotine were detected in all samples of chocolate, whereas myosmine was not present in any sample. The caffeine content ranged from 420 to 2780 mg/kg (relative standard deviation 0.1 to 11.5%) and nicotine from 0.000230 to 0.001590 mg/kg (RSD 2.0 to 22.1%).\nDoes fruit contain nicotine?\nThe edible Solanaceae fruit analyzed in this investigation were found to contain relatively consistent amounts of nicotine in the range of 2-7 microg/kg for fresh fruits.\nDoes Lipton contain nicotine?\nTea is a popular beverage worldwide, but you may be surprised to learn that it contains nicotine. … Despite being present in tea, it’s absorbed differently than the nicotine in cigarettes and poses very little risk to your health.\nWhich vegetable has the most nicotine?\nIt starts with plants\nWhere does nicotine come from? The simple answer is: plants. More specifically: the Solanaceae family, commonly known as nightshade. This family includes tomatoes (~332 ng of nicotine each on average), potatoes (~675 ng), and eggplants/aubergines (~525 ng).\nWhat is in nicotine that makes it so addictive?\nBut what makes nicotine so addictive? Consuming nicotine—through regular cigarettes or vaping—leads to the release of the chemical dopamine in the human brain. As with many drugs, dopamine prompts or “teaches” the brain to repeat the same behavior (such as using tobacco) over and over.\nIs nicotine a stimulant or depressant?\nThe nicotine in tobacco smoke travels quickly to the brain, where it acts as a stimulant and increases heart rate and breathing. Tobacco smoke also reduces the level of oxygen in the bloodstream, causing a drop in skin temperature. People new to smoking often experience dizziness, nausea and coughing or gagging.\nDoes nicotine damage your brain?\nBrain risks: Nicotine affects your brain development. This can make it harder to learn and concentrate. Some of the brain changes are permanent and can affect your mood and ability to control your impulses as an adult.\nWill smoking green tea get you high?\nSome may claim green tea gives you a marijuana-like high. No studies or science support this.\nIs there nicotine in coffee?\n8.5 g of coffee beans is typically a sufficient amount to brew a 6 oz. cup of coffee. It is understood that the amount of coffee to nicotine can be varied as desired. 3.5 g of instant coffee crystals and 2 mg of nicotine are added to a single-serving coffee packet and sealed.\nIs nicotine harmful to the body?\nNicotine is a toxic substance. It raises your blood pressure and spikes your adrenaline, which increases your heart rate and the likelihood of having a heart attack.","Traditional quantitative genetics studies have revealed nicotine dependence is heritable and molecular genetics studies are providing increasing evidence that the genes responsible for nicotine’s pharmacokinetics and pharmacodynamics are particularly important.\nDoes nicotine addiction run in family?\nTwin and family studies have shown that there is not one specific gene that determines who will develop a smoking addiction but rather several genes that cause an individual to become more susceptible to being addicted to nicotine.\nCan smoking be genetic?\nTobacco smoking is believed to be a complex, multifactorial behaviour with both genetic and environmental determinants. While early reports suggested that the influence of heredity on smoking was modest, more recent studies have found significant genetic influences on several aspects of smoking behaviour.\nWhat is the heritability of nicotine addiction?\nResults The heritability of nicotine dependence was 60.3% (95% confidence interval [CI], 55.4%-65.2%); that of alcohol dependence, 55.1% (95% CI, 49.7%-60.5%).\nWho is most at risk for nicotine addiction?\nAdults that Live in Rural Areas\nThey are also more likely to be heavier smokers, smoking 15 or more cigarettes per day, compared to smokers in urban areas. Kids in rural areas are also more likely to start smoking at a much younger age and smoke daily, making addiction more severe and smoking harder to quit.\nDo genetics play a role in addiction?\nWhile the environment a person grows up in, along with a person’s behavior, influences whether he or she becomes addicted to drugs, genetics plays a key role as well. Scientists estimate that genetic factors account for 40 to 60 percent of a person’s vulnerability to addiction.\nIs there a genetic test for addiction?\nOffering the GARS test to a person’s family in treatment is the best way to confirm the risk of addiction in the family to help confirm the genetic basis of the Genogram.\nDoes father smoking affect baby?\nThe new analysis, published in the European Journal of Preventive Cardiology, found that parental smoking was significantly associated with risk of congenital heart defects in newborns, with an increased risk of 25 percent when mothers smoked while pregnant. The link was even stronger when fathers smoked.\nCan you be immune to nicotine?\nIf nicotine makes the person nauseous they will be immune to nicotine addiction. There are others who are simply immune to the addictive effects of nicotine, usually people who are genetically not prone to addiction of any kind.\nHow are some people not addicted to nicotine?\nSaid simply, a small cluster of genes on Chromosome 15 seems to be able to lessen our addiction to nicotine. People lucky enough to inherit certain versions of these genes can smoke up a cloud and never become addicted.\nHow does nicotine affect your genes?\nCigarette smoking activates oxidative and inflammatory response and leads to uncontrolled structural changes in airways and alters gene expression. Such changes have a characteristic similar to that for COPD patients. Therefore, smoking is determined as a key risk factor for chronic respiratory disease development.\nDoes nicotine help with stress?\nWhen a person smokes, nicotine reaches the brain within about ten seconds. At first, nicotine improves mood and concentration, decreases anger and stress, relaxes muscles and reduces appetite.\nIs smoking genetic or environmental?\nPrevious studies based on data from the NTR have shown that smoking in is influenced both by shared environmental (51-56%) and by genetic factors (36-44%) [4–6]. The estimates for the importance of those factors are comparable with other twin studies worldwide [7–10].\nWhat are signs of vaping addiction?\nOlorunnisola, these are some signs parents can look for that may suggest if a child is vaping:\n- Increased thirst. Vaping removes hydration from the skin, especially around the mouth and throat. …\n- Nosebleeds. …\n- ‘Vaper’s tongue. …\n- Skin damage. …\n- Sleep disturbance. …\n- Emotional problems. …\n- Passing on caffeine.\nWhat does being addicted to Nic feel like?\nYour attempts at stopping have caused physical and mood-related symptoms, such as strong cravings, anxiety, irritability, restlessness, difficulty concentrating, depressed mood, frustration, anger, increased hunger, insomnia, constipation or diarrhea. You keep smoking despite health problems.\nHow long does nicotine withdrawal last?\nNicotine withdrawal symptoms usually begin a few hours after your last cigarette. They are usually strongest in the first week. For most people, nicotine withdrawal fade and are gone after about 2 to 4 weeks. Chat to your doctor or a Quitline counsellor if you find that nicotine withdrawal is lasting longer."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:aadfa7bc-550a-462a-b45a-f70fe55f413d>","<urn:uuid:f680cf01-edb6-491f-8b10-bf0a732c18e5>"],"error":null}
{"question":"Working on study design - need help! Which factors matter more: the experimental setup in clinical drug trials or the data collection approach in recreational fishing studies?","answer":"In clinical drug trials using GLM, the key factors are the controlled experimental variables - drug dosage levels (e.g., 'not used', 'low dose', 'high dose'), their interactions, and measurable covariates like age. The setup allows for precise measurement of both individual drug effects and their combinations. In recreational fishing studies using zero-inflated negative binomial models, the important factors are both observational variables (like number of people in group, presence of children, camping equipment) and the underlying data structure - particularly accounting for why zeros occur in the data. The fishing studies must consider that zeros can come from two different sources: unsuccessful fishing attempts and non-participation in fishing, requiring a more complex modeling approach to handle this data structure.","context":["Glossary of statistical terms\nGeneral Linear Model:\nGeneral (or generalized) linear models (GLM), in contrast to linear models, allow you to describe both additive and non-additive relationship between a dependent variable and N independent variables. The independent variables in GLM may be continuous as well as discrete. (The dependent variable is often named \"response\", independent variables - \"factors\" and \"covariates\", depending on whether they are controlled or not).\nConsider a clinical trial investigating the effect of two drugs on survival time. Each drug is tested at three levels - \"not used\", \"low dose\", \"high dose\", and all the 9 (=3x3) combinations of the three levels of the two drugs are tested. The following general linear model might have been used:\nwhere Y is survival time (response), i and j correspond to the three levels of drug I and drug II respectively, X is age, Ci are additive effects (called \"main effects\") of each level of drug I, Dj are main effects of drug II, Rij are non-additive effects (called interaction effects or simply \"interactions\") of drugs I and II, N is random deviation.\nWe have here three independent variables: two discrete factors - \"drug I\" and \"drug II\" with three levels each, and a continuous covariate \"age\".\nIn this particular case, because each of the two factors (drugs) has a zero level i,j=1 (\"not used\"), main effects C1, B1, and interactions R1j, j=1,2,3; Ri1, i=1,2,3 are zeros. The remaining unknown coefficients - A, B, Ci, Dj, Rij - are estimated from the data. The main effects Ci, Dj of the two drugs and their interaction effects Rij are of primary interest. For example, their positive values would indicate a positive effect - longer survival time due to use of the drug(s).\nWant to learn more about this topic?\nStatistics.com offers over 100 courses in statistics from introductory to advanced level. Most are 4 weeks long and take place online in series of weekly lessons and assignments, requiring about 15 hours/week. Participate at your convenience; there are no set times when you must to be online. Ask questions and exchange comments with the instructor and other students on a private discussion board throughout the course.\nThis course will explain the theory of generalized linear models (GLM), outline the algorithms used for GLM estimation, and explain how to determine which algorithm to use for a given data analysis. GLM allows the modeling of responses, or dependent variables, that take the form of counts, proportions, dichotomies (1/0), positive continuous values, as well as values that follow the normal Gaussian distribution. Logistic, Poisson, and negative binomial regression models are three of the most noteworthy GLM family members.\nNote: Detailed study of model specification and the interpretation of software output is handled in statistics.com's individual courses on regression, logistic regression, count data modeling, etc.\nThis course will cover the analysis of contingency table data (tabular data in which the cell entries represent counts of subjects or items falling into certain categories). Topics include tests for independence (comparing proportions as well as chi-square), exact methods, and treatment of ordered data. Both 2-way and 3-way tables are covered.","Version info: Code for this page was tested in Mplus version 6.12.\nZero-inflated negative binomial regression is for modeling count variables with excessive zeros and it is usually for overdispersed count outcome variables. Furthermore, theory suggests that the excess zeros are generated by a separate process from the count values and that the excess zeros can be modeled independently.\nPlease note: The purpose of this page is to show how to use various data analysis commands. It does not cover all aspects of the research process which researchers are expected to do. In particular, it does not cover data cleaning and checking, verification of assumptions, model diagnostics or potential follow-up analyses.\nExamples of zero-inflated negative binomial regression\nSchool administrators study the attendance behavior of high school juniors at two schools. Predictors of the number of days of absence include gender of the student and standardized test scores in math and language arts.\nThe state wildlife biologists want to model how many fish are being caught by fishermen at a state park. Visitors are asked how long they stayed, how many people were in the group, were there children in the group and how many fish were caught. Some visitors do not fish, but there is no data on whether a person fished or not. Some visitors who did fish did not catch any fish so there are excess zeros in the data because of the people that did not fish.\nDescription of the data\nLet’s pursue Example 2 from above. The associated dataset can be found here.\nWe have data on 250 groups that went to a park. Each group was questioned before leaving the park about how many fish they caught (count), how many children were in the group (child), how many people were in the group (persons), and whether or not they brought a camper to the park (camper). The outcome variable of interest will be the number of fish caught. Even though the question about the number of fish caught was asked to everyone, it does not mean that everyone went fishing. What would be the reason for someone to report a zero count? Was it because this person was unlucky and didn’t catch any fish, or was it because this person didn’t go fishing at all? If a person didn’t go fishing, the outcome would be always zero. Otherwise, if a person went to fishing, the count could be zero or non-zero. So we can see that there seemed to be two processes that would generate zero counts: unlucky in fishing or didn’t go fishing.\nLet’s first look at the data. We will start with reading in the data and the descriptive statistics and plots. This helps us understand the data and give us some hint on how we should model the data.\nLet’s look at the data.\nData: File is C:fish.dat; Variable: Names are nofish livebait camper persons child xb zg count; Missing are all (-9999); Usevariables are camper persons child count; Analysis: type = basic; plot: type is plot1; ESTIMATED SAMPLE STATISTICS Means CAMPER PERSONS CHILD COUNT ________ ________ ________ ________ 1 0.588 2.528 0.684 3.296 Covariances CAMPER PERSONS CHILD COUNT ________ ________ ________ ________ CAMPER 0.242 PERSONS -0.026 1.233 CHILD -0.014 0.515 0.720 COUNT 0.730 2.856 -1.670 134.832 Correlations CAMPER PERSONS CHILD COUNT ________ ________ ________ ________ CAMPER 1.000 PERSONS -0.048 1.000 CHILD -0.034 0.546 1.000 COUNT 0.128 0.221 -0.170 1.000\nAnalysis methods you might consider\nBefore we show how you can analyze this with a zero-inflated negative binomial analysis, let’s consider some other methods that you might use.\n- OLS Regression – You could try to analyze these data using OLS regression. However, count data are highly non-normal and are not well estimated by OLS regression.\n- Zero-inflated Poisson Regression – Zero-inflated Poisson regression does better when the data is not overdispersed, i.e. when variance is not much larger than the mean.\n- Ordinary Count Models – Poisson or negative binomial models might be more appropriate if there are not excess zeros.\nZero-inflated negative binomial regression\nIn the syntax below, we have indicated that count is a count variable by using the count statement. The (nbi) option is used to indicate 2 things: that we are modeling our count variable with a negative binomial distribution, and that we are specifying a zero-inflated model. Without the (nb) option we would be specifying a (zero-inflated) poisson model, and without the (i) option, we would be estimating a negative binomial model without zero-inflation. Also, we use the usevariables statement to indicate that we are not using all of the variables in the data set in the current model. We have omitted the missing statement because we have no missing data in this data set. The default estimation method is MLR – maximum likelihood parameter estimates with standard errors and a chi-square test statistic that are robust to non-normality and non-independence of observations when used with type = complex. The MLR standard errors are computed using a sandwich estimator. This is what we generally call robust standard errors. To get the \"regular\" standard errors, we use the estimator = ml on the analysis statement. Two regression equations are specified in the model statement: the first equation is the negative binomial model, predicting the count of fish using child and camper. The second equation is the logit model, indicated by count#1, predicting membership to the zero generating process using persons.\nData: File is C:fish.dat; Variable: Names are nofish livebait camper persons child xb zg count; Count is count(nbi); Usevariables are camper persons child count; Analysis: estimator = ml; Model: count on child camper; count#1 on persons; MODEL RESULTS Two-Tailed Estimate S.E. Est./S.E. P-Value COUNT ON CHILD -1.515 0.196 -7.747 0.000 CAMPER 0.879 0.269 3.265 0.001 COUNT#1 ON PERSONS -1.666 0.679 -2.454 0.014 Intercepts COUNT#1 1.603 0.836 1.916 0.055 COUNT 1.371 0.256 5.353 0.000 Dispersion COUNT 2.679 0.471 5.683 0.000\nIn the MODEL FIT INFORMATION portion of the output, you will find the log likelihood for the final model as well as a number of fit statistics. In the MODEL RESULTS section of the output you will find the negative binomial regression coefficients (estimates) for each of the variables, standard errors and the ratio of the estimate to its standard error. This can be used as a Z test, where values greater than 2 are considered to be statistically significant. Following these are logit coefficients for predicting excess zeros. In the above output, we see that both child and camper are significant predictor of count, and persons is a significant predictor in the logit model. Thus for each additional child, the log count of number of fish count decreases by 1.515. For each additional person, the log odds of membership to the excess zero-generating process decreases by1.666.\nNow let’s rerun the model without the analysis statement in order to obtain robust standard errors.\nData: File is C:fish.dat; Variable: Names are nofish livebait camper persons child xb zg count; Count is count(nbi); Usevariables are camper persons child count; Model: count on child camper; count#1 on persons; MODEL FIT INFORMATION Number of Free Parameters 6 Loglikelihood H0 Value -432.891 H0 Scaling Correction Factor 1.762 for MLR Information Criteria Akaike (AIC) 877.782 Bayesian (BIC) 898.911 Sample-Size Adjusted BIC 879.890 (n* = (n + 2) / 24) MODEL RESULTS Two-Tailed Estimate S.E. Est./S.E. P-Value COUNT ON CHILD -1.515 0.241 -6.280 0.000 CAMPER 0.879 0.470 1.869 0.062 COUNT#1 ON PERSONS -1.666 0.431 -3.871 0.000 Intercepts COUNT#1 1.603 0.665 2.410 0.016 COUNT 1.371 0.389 3.520 0.000 Dispersion COUNT 2.679 0.577 4.645 0.000\nThe robust standard errors attempt to adjust for heterogeneity in the model. Robust standard errors tend to be larger than \"regular\" standard errors for parameters in the negative binomial part of the model and smaller for parameters in the logit part of the model. We see that now camper is not statistically significant.\nThings to consider\nHere are some issues that you may want to consider in the course of your research analysis.\n- Question about the over-dispersion parameter is in general a tricky one. A large over-dispersion parameter could be due to a miss-specified model or could be due to a real process with over-dispersion. Adding an over-dispersion problem does not necessarily improve a miss-specified model.\n- The zinb model has two parts, a negative binomial count model and the logit model for predicting excess zeros, so you might want to review these Data Analysis Example pages, Negative Binomial Regression and Logit Regression.\n- Since zinb has both a count model and a logit model, each of the two models should have good predictors. The two models do not necessarily need to use the same predictors.\n- Problems of perfect prediction, separation or partial separation can occur in the logistic part of the zero-inflated model.\n- Count data often use exposure variable to indicate the number of times the event could have happened.\n- It is not recommended that zero-inflated negative binomial models be applied to small samples. What constitutes a small sample does not seem to be clearly defined in the literature.\n- Pseudo-R-squared values differ from OLS R-squareds, please see FAQ: What are pseudo R-squareds? for a discussion on this issue.\n- Long, J. Scott (1997). Regression Models for Categorical and Limited Dependent Variables. Thousand Oaks, CA: Sage Publications."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:5895de3f-f9c6-4d36-bbd1-efd5fdcbd39b>","<urn:uuid:bdabc1ca-404f-47df-b7be-2a06e8a6e2e9>"],"error":null}
{"question":"I'm fascinated by historical jewelry ownership. Could you compare how the Hope Diamond and the Wittelsbach Diamond changed hands through royal families?","answer":"The Hope Diamond and the Wittelsbach Diamond both have rich histories of royal ownership. The Hope Diamond was first purchased by French King Louis XIV in 1669, who had it recut into a 69-carat heart-shaped stone known as the French Blue. It was later stolen from the French royal treasury in 1792. The Wittelsbach Diamond, on the other hand, was owned by King Philip IV of Spain and Leopold I of Austria, before passing to Maria Amalia who married into the royal Wittelsbach family of Bavaria, giving the diamond its name. Both diamonds eventually left royal possession - the Hope Diamond went through various owners before being donated to the Smithsonian by Harry Winston in 1958, while the Wittelsbach Diamond was sold at auction to jeweler Laurence Graff in 2008 for $23.4 million.","context":["Best for Last: Hope Diamond Is the Final Stop on 2020’s Gem Gallery Virtual Tour\nMore than 200 million visitors to the National Museum of Natural History in Washington, DC, have marveled at the beauty and majesty of the Hope Diamond since jeweler Harry Winston donated it to the Smithsonian Institution in 1958. In yesterday’s column, we recounted how and why Winston decided to use the US Postal Service to ship the 45.52-carat gem from New York to DC.\nThe weathered brown paper mailing wrapper — showing $2.44 in postage, but also $142.85 for $1 million worth of insurance — is a popular exhibit at the Smithsonian’s National Postal Museum. But, 1.3 miles away on the National Mall, the Hope Diamond is the prize of National Gem Collection.\nWhen the Smithsonian’s gem gallery was renovated in 1997, the Hope Diamond necklace was moved onto a rotating pedestal inside a case made of 3-inch-thick bulletproof glass. The display sits in the center of an expansive rotunda, adjacent to the main entry of the Janet Annenberg Hooker Hall of Geology, Gems, and Minerals. The 7,500-plus gemstones in the collection range in size from less than a half-carat to 23,000 carats.\nIn a normal year, 4.2 million people would pass through the Smithsonian’s most popular museum, but this has not been a normal year. The Smithsonian museums remain closed in an effort to contain the spread of COVID-19.\nDuring the pandemic, we have hosted 13 virtual tours utilizing 360-degree viewing technology provided by the Smithsonian. Previous stops have included the “Zuni Tribe Turquoise,” “Picasso Kunzite Necklace,” “Marie Antoinette Earrings,” “Hall Sapphire Necklace,” “Victoria-Transvaal Diamond,” “Carmen Lúcia Ruby,“ “Chalk Emerald,“ “Gifts from Napoleon,“ “Stars and Cat’s Eyes,“ “Logan Sapphire,“ “Dom Pedro“ aquamarine, “Steamboat“ tourmaline and a grouping of enormous topaz.\nHere’s how to navigate to the Hope Diamond.\n— First, click on this link…\nThe resulting page will be a gallery called “Geology, Gems & Minerals: Precious Gems 1.”\n— Next, click the double-left-arrow two times to navigate to the gallery called “Geology, Gems & Minerals: Hope Diamond 1.”\nWhen you arrive, you will see a single, glass-encased exhibit at the center of a rotunda.\n– Touch the Plus Sign to zoom in.\n(You may touch the “X” to remove the map. This will give you a better view of the exhibit. You may restore the map by clicking the “Second” floor navigation on the top-right of the screen.)\nResearchers believe the Hope Diamond’s origin can be traced back to 1642, with the discovery in India of a beautiful blue rough diamond. It was crudely finished and weighed 115 carats when it was purchased in 1666 by French merchant Jean Baptiste Tavernier, at which time it became known as the Tavernier Diamond.\nFrench King Louis XIV bought the Tavernier Diamond in February 1669 and ordered it to be recut. The result was a 69-carat heart-shaped stone that would be known as the French Blue.\nIn 1792, the French Blue was stolen from the royal treasury in Paris. Its whereabouts remained unknown until a large blue diamond appeared in 1839 in the collection of Henry Philip Hope, a London banker and gem collector. Gem historians believe the French Blue had been. once again, recut. The 45.52-carat gem became known as the Hope Diamond.\nAfter going through numerous owners, it was sold by French jeweler Pierre Cartier to Washington socialite Evalyn Walsh McLean in 1911. In 1949, McLean’s heirs sold the stone to Winston, who exhibited it throughout the US for a number of years. In 1958, he decided to donate it to the Smithsonian.\nAccording to the Smithsonian, Winston envisioned the institution assembling a gem collection to rival the royal treasuries of Europe — “crown jewels” that would belong to the American public.\n“Other countries have their Crown Jewels,” Winston reportedly said. “We don’t have a Queen and King, but we should have our Crown Jewels, and what better place than here in the nation’s capital at the Smithsonian Institution.”\nCredits: Hope Diamond photo by Studio Kanji Ishii, Inc. / Smithsonian. Screen captures via naturalhistory2.si.edu.","Famous Blue Diamonds\nProfiles of eight of the largest and most famous (and infamous) blue diamonds. Sorry to say you won't stumble across any of these in your local pawnshop or on eBay - these gemstones are all in museums or private collections!\n- The Hope Diamond\n- Probably the best know blue diamond (and one of the most famous diamonds in the world), the 45.52 carat Hope Diamond is a fancy deep grayish blue diamond, presumed to have been mined in India. The first documented owner of this breathtaking diamond was a French merchant named Jean Baptiste Tavernier. At the time, it is said to have been a 112 carat diamond with a crude, triangular cut. In 1668, Tavernier sold it to King Louis XIV of France, who had it recut by court jeweler Sieur Pitau into a 67.12 carat stone and set in gold. It was nicknamed The French Blue, and was worn by King Louis on a ribbon around his neck on special occasions. During the French Revolution, the stone (along with the rest of the Crown Jewels) was turned over to the French government and subsequently stolen during looting in 1792.\nIn 1812 the Hope surfaced again, this time in the hands of London diamond merchant Daniel Eliason, who is assumed to have sold it to King George IV. In 1839 it was documented in the collection of Henri Louis Hope, from whom the diamond gets it's name. It remained in the Hope family until 1901, after which it was sold several times.\nYou may be wondering about the rumored Curse of the Hope Diamond - the claims that misfortune and tragedy have befallen all of the Hope's owners. In reality, the \"curse\" was just a clever marketing ploy - a combination of half-truths and out-and-out fiction, dreamt up by Pierre Cartier - presumably to enhance the diamond's mystique (and subsequent sale price!)\nIn 1912, the Hope diamond was purchased by the eccentric diamond mining heiress Mrs. Evalyn Walsh McLean. The exquisite diamond got little respect during it's time in the McLean household, where it was stored in a plain shoebox and occasionally worn by one of Mrs. McLean's dogs!\nThe Hope Diamond was purchased from the McLean estate in 1949 by Harry Winston Inc. After a decade of exhibitions, Harry Winston donated the Hope Diamond to the Smithsonian Institute, where it remains to this day - once again getting the royal treatment as one of the most visited exhibits in the museum.\n- The Terschenko Diamond (the Mouawad Blue)\n- The Terschenko diamond is a 42.92 pear shaped fancy blue believed to be of Indian origin. Originally owned by the Terschenko family (sugar barons in pre-communist Russia) it was smuggled out of the country just prior to the Russian revolution and ended up in the hands of a private owner. It remained in obscurity until 1984 when it resurfaced for auction at Christies, where it was purchased by Saudi Arabian diamond dealer Robert Mouawad for $4.5 million.\n- The Wittelsbach Diamond\n- At 35.56 Wittelsbach diamond is the 3rd largest blue diamond, as notable for it's unique dark blue color as for it's impressive size. It broke diamond auction records in December, 2008 when it was sold by Christies to London jeweler Laurence Graff for a jaw-dropping 23.4 million dollars. Former owners of this diamond include King Philip IV of Spain, Leopold I of Austria, and his granddaughter Maria Amalia - who married into the royal Wittelsbach family of Bavaria, from which the diamond takes it's name. [ Photo of the Wittelsbach diamond ]\n- The Sultan of Morocco\n- At 35.27 carats, the grayish-blue Sultan of Morocco is the 4th largest blue diamond. Once owned by Cartier, this cushion cut gemstone is believed to have been last sold in 1972, possibly by jeweler Laykin et Cie, to a private American collector. It was last seen on public display in 1969 at the New York State Museum's World of Gems exhibition.\n- The Eugenie Blue (the Blue Heart)\n- The 30.82 carats Eugenie Blue, also known as the Blue Heart diamond, is a heart-shaped fancy vivid or fancy deep blue diamond. Legend has it the the Blue Heart was once owned by Empress Eugenie (Eugénie de Montijo) - wife of Napolean III - but there's no real evidence of this and many experts doubt it to be true. The Blue Heart's country of origin is not known, but the diamond was cut into it's present-day heart shape around 1909 or 1910 by Atanik Ekyanan of Paris. It was then purchased by Cartier, and sold to an Argentinian woman (Mrs. Ungue). It remained in her possession until the mid 20th century when it was sold to Van Cleef and Arpels who subsequently sold it for $300,000 to a European family. A few short years later it was again purchased, this time by jeweler Harry Winston. It's last private owner was Marjorie Merriweather Post, who donated it to the Smithsonian museum , where it remains to this day - on display as part of the museum's diamond collecton.\n- The Blue Lili Diamond\n- A 30.06 carat, tapered cushion-cut blue diamond of unknown color grade. Not much is known about the Blue Lili, but it is believed to be from the Premier diamond mines in South Africa. It was purchased and cut by the William Goldberg Diamond Corporation.\n- The Heart of Eternity Diamond\n- This heart shaped fancy vivid blue diamond weighs in at 27.64 carats and originates, as do many other exquisite blue colored diamonds, from the Premier mines in South Africa. The Heart of Eternity is owned by a private collector and, as of this writing, is on loan to the Smithsonian National Museum of Natural History.\n- The Blue Magic Diamond\n- According to it's GIA (Gemological Institute of America) certificate, the Blue Magic is a 12.02 carat modified pear-shaped diamond of an exquisite fancy vivid (the highest color grade) blue color with a clarity of VVS-2.\nIn general, fancy blue diamonds continue to attract a lot of attention. Here are some notable, recent auction sales:\n- May 2009 - an internally flawless, 7.03 carat cushion-cut, fancy vivid blue diamond sold for $9.48 million dollars - a record-setting high price-per-carat ever at Sotheby's Geneva auction.\n- The Millennium Blue - a 5.16 carat pear-shape, internally flawless, fancy vivid blue, sold for $6.4 million dollars at Sotheby's Hong Kong auction in April, 2010.\n- In October 2010, the largest known triangular fancy vivid blue diamond ever auctioned (10.95 carats), aka the Bulgari Blue, set yet another auction record of $15.7 million dollars, making it top gemstone sale of 2010.\nPrevious: Blue Diamonds - FAQ\nResources: Learn more about these and other famous diamonds"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:b1930af5-a475-44fe-bad2-26277315ab7a>","<urn:uuid:e5da9d4a-98b3-4ded-8010-6f791f376844>"],"error":null}
{"question":"I'm teaching crafts to kids and want to know: between making a vinyl record purse and a glowing holiday card, which project requires more specialized tools?","answer":"The vinyl record purse requires more specialized tools. It needs specific equipment like a drill press, leather punch, wide-mouth pliers, and sewing machine. In contrast, the glowing holiday card only requires basic crafting tools like scissors/hobby knife, hole punch (which is optional), and regular household items like tape.","context":["Turn Vinyl Records Into a Unique Purse\nCrafter Kristin Hansen demonstrates how to create a vibrant, wild purse from records and pink leopard print fabric.\n- two 45-rpm records\n- label covers (printed on computer)\n- 6\" x 60\" fabric (for body and handles)\n- 13-1/2\" x 44\" fabric (for lining)\n- 6\" round elastic cord\n- four 1\" metal rings\n- flat button\n- large rhinestone\n- fast-drying glue\n- glue gun\n- hot glue\n- two 1\" rhinestone buckles\n- 6\" metal beading wire\n- two 2\" squares of black fabric\n- two 72\" pieces of 1/8\" ribbon\n- crewel sewing needle\n- 1/8\" and 3/16\" Hanson drill bits\n- drill press\n- drilling template\n- sewing machine\n- straight pins\n- leather punch\n- 2 pair of wide-mouth pliers\n- image design software\n- inkjet paper\nPrep the Records\n1. Select two 45-rpm records. Using design software on the computer, create a 3-1/2 inch diameter circle with a 1-1/4 inch black circle in the center. Create the label design and print out two copies onto white paper. Glue a label on each record.\n2. Place one 45-rpm record in the drilling template. Drill holes with 1/8-inch drill bit around the perimeter and at the top for the closure. Repeat with the other record.\n3. Using a 3/16-inch drill bit, enlarge two holes along the top of each record for attaching handles.\n4. Glue a 2-inch black fabric square to the back of each 45-rpm record with hot glue to cover the hole (Image 1).\n5. Glue the flat part of the button to the back of the large rhinestone using fast-drying glue (Image 2).\n6. Fold a 6-inch piece of beading wire in half and thread it through the button.\n7. Thread the wire ends through the large hole you drilled in the record. Twist the wire to secure.\n8. Thread elastic through the other record’s large hole and tie a knot.\nLining and Body Fabric\n1. Cut the 60-inch body fabric into two pieces: 15\" x 60\" and 45\" x 60\".\n2. Cut the 45-inch piece of body fabric into two 3\" x 45\" strips to make the handles.\n3. Cut the 44-inch piece of lining fabric into two pieces: 6\" x 15\" and 7-1/2\" x 44\".\n4. Fold the 7-1/2-inch piece of lining fabric into 6 pieces. Lay a record on top, trace the circumference and cut out the circles, leaving 1/4 inch around all sides for the seam allowance. Make 6 7-1/2-inch circles.\n5. Place two circles on top of one another, right sides together, and fold the top circle in half.\n6. Set a third circle on top, right sides together. Repeat with the other three circles. When sewn together, the folded circle will form the pocket on the inside of the purse.\n7. Machine-stitch around the circumference of the first three circles, leaving a 1-inch opening at the bottom so you can turn the circles right side out. Repeat with other set of circles. Turn the circles right side out through the opening in the bottom. Hand stitch the openings closed.\n8. Machine-stitch the 6\" x 15\" pieces of body fabric and lining fabrics, right sides together, along the long sides, leaving the ends open. Turn the pieces right side out, tuck the ends to the inside and topstitch the ends.\n9. Find the center bottom of the circle and attach it to the center of the 15-inch body piece. Pin and topstitch together.\n10. Repeat with the other circle. This completes the shell of the purse with the pockets inside.\n11. Fold each 45-inch strip of fabric for the handles into quarters lengthwise, tucking the raw edges of the fabric inside. Topstitch along the outer edges.\n12. Thread 1 inch of one end of the handle through the rhinestone buckle and sew the handle to itself to hold the buckle. Thread the other end of the handle through the buckle again, fold the end of the handle over 1 inch and sew to itself. Do the same for the other handle.\n13. Use a leather punch to make a hole in the bottom center of one circle of the shell. Make sure the hole goes through all layers of the circle and through the layers of the body material.\n14. Using a crewel needle and the 1/8-inch ribbon, thread the ribbon through the hole in the fabric shell, through the bottom center hole in one record and back through the hole in the fabric shell.\n15. Continue to punch holes along the circumference of the fabric shell, matching them to the holes in the record.\n16. Whip stitch ribbon through the holes around the entire purse and stitch the final stitch back through the original hole. Tie a knot. Repeat with the other record.\n17. Using two pair of pliers, open the metal rings and thread one through the 3/16-inch holes at the top of each record, two per record.\n18. Slide the handle loop over the metal ring and close the metal ring using pliers.\n19. Loop the elastic over the rhinestone button of your purse.","Let It Glow Holiday Cards\nCraft a glowing card for friends and family this holiday season with paper circuits - no soldering required! This tutorial will guide you through how to create simple paper circuitry using only copper tape, a coin cell battery, a LilyPad Button Board, and an LED, and it will leave you with a basic understanding of how circuits work.\nPaper engineer and pop up book designer Robert Sabuda allowed us to adapt some of his free templates for use with electronics. We'll be covering the electronics build in this tutorial and linking to Robert's instructions for the pop up cards.\nWhy Aren't We Soldering?\nYou may have seen Nick's awesome Father's Day Card tutorial and are wondering why this one is different. In classrooms or homes where supplies or budgets are limited, using tape and craft supplies helps keep complexity down. The drawback is that the connections aren't as sturdy/permanent with tape vs solder. You can always use these templates and solder components to the copper tape if you have the supplies on hand.\nIf you are brand new to working with electronics, here's some helpful reading to check out:\nMaterials and Tools\nHere is a list of all the materials and tools you'd need to follow along:\n- Copper Tape - 5mm width (~18\" of tape per card)\n- LED (2 for Christmas Tree Design) *see note below about LED choices\n- Coin Cell Battery - 12mm (CR1225)\n- LilyPad Button Board -or- LilyPad Slide Switch\n- Cardstock (2-3 pieces)\n- Vellum or Parchment Paper (optional) - creates a nice diffused effect for LEDs in window cut outs\n- Clear Tape\n- Scissors/Hobby Knife\n- Hole Punch/Screw Punch (optional) - to cut out holes in Christmas Tree Design\n- Decorating Supplies - stickers, markers, white out pen (for Gingerbread House 'icing')\nA note on LEDs:\nWe recommend using the smallest LED you can find - 3mm work well because they don't add too much bulk to your card when folded. For extra flair try using cycling RGB LEDs. We've also found that cutting individual LEDs from a set of LED String Lights works well - you will have to use a hobby knife to scrape the coating off of the wires before using. We'll cover that process later in the tutorial. Feel free to experiment with different LEDs and find what works best for your project.\nStep 1: Print Templates\nRight-click the images below and choose “Save Link As” to download the templates to your computer. Each file has two (or three) pages which includes all pop up pieces and the circuit template.\nPrint your templates out on cardstock. If needed, adjust your printer's margins or choose 'Fit to Page' in the print settings. The card template is slightly smaller than the paper, make sure to cut along the black border for the final card size.\nSet the pop up pages aside for now. We'll build our circuit first and then assemble the pop up once the electronics are all installed.\nGingerbread House Template - 3 pages\nPrint page 2 (pop up pieces) on brown cardstock for a great gingerbread house base, or print all on a light colored cardstock and color in when you are finished. Can also be used to create a winter cottage pop up.\nChristmas Tree Template - 2 pages\nLooks great if printed on green cardstock, or use light colored cardstock and decorate after assembling.\nWindow Template - 2 pages\nUse stickers or paper cut outs on vellum to create a festive silhouette scene in the window frame.\nStep 2: Create Copper Traces\nTime to create a path for our electricity with copper tape. The templates for both cards are fairly similar, so we'll be demonstrating with the Gingerbread House template. Each has icons to help guide you in constructing the circuit.\nTake a look at the template and find the circle marked A. Peel away a few inches of the paper backing from the copper tape and stick down along the grey line.\nCut when you reach the scissors icon.\nNext we'll place tape along Line B - but wait, what's this corner?! To keep a solid connection of copper around corners, we'll be using a folding technique to press the tape into shape.\nStart by sticking the copper tape down until you reach the corner, then fold the tape backward on itself. Use a fingernail or pen to give it a good crease at the edge.\nThen carefully move the tape down around the corner - you should see the fold forming - and press down flat against the paper. The neatness of the fold doesn't matter that much, it will be covered by your pop up in the end. Finally, cut the tape when you reach the scissors icon.\nThe last copper tape line will also form a battery holder. We'll start by folding 1/2\" of copper tape onto itself, sticking the adhesive sides together to form a flap.\nThis allows the top of the copper to fold down over the coin cell battery - the positive side of the battery is the top and negative side is the bottom, which allows us to create a 'battery sandwich' with copper tape touching each side.\nSee the diagrams below to explore how this method works. We won't be installing the battery until the end of our project, so set that aside for now. Fold the card in half along the dotted center line before moving onto the next step.\nStep 3: Prepare and Place LED\nNow that our copper is in place, time to add the LED. Each template has an LED symbol which shows a shaped wire - we use this method to help us remember which side is positive and negative on the LED.\nHere's excerpt from our Light-emitting Diodes (LEDs) Tutorial about LED polarity:\n\"In electronics, polarity indicates whether a circuit component is symmetric or not. LEDs, being diodes, will only allow current to flow in one direction. And when there’s no current-flow, there’s no light. Luckily, this also means that you can’t break an LED by plugging it in backwards. Rather, it just won’t work. The positive side of the LED is called the “anode” and is marked by having a longer “lead,” or leg. The other, negative side of the LED is called the “cathode.”\nHere are directions for bending a standard LED (as shown in the image above) to prepare it for our circuit.\nUsing pliers (or your finger) bend the longer leg of the LED flat. Then form the wire into a zig zag shape. Be careful not to break the wire by bending back and forth over the same joint too many times.\nNext, bend the other leg flat and curl into a spiral. Use the end of the pliers to lightly grab the end of the wire and curl around the tool.\nOnce all shaping is complete, place the LED on a table or flat surface to make sure it sits flat and upright. If not, make any adjustments now.\nThe Christmas Tree design looks best if the LEDs are pointed at a slight angle toward the middle of the tree and each other, see picture below.\nUsing LED String Lights\nWe've been experimenting with cutting up LED string lights (also known as fairy lights) because they use tiny LEDs that are great for flat surfaces like greeting cards. Cut one LED off of the strip, making sure to leave ~1/2\" of wire on either side. Then use a hobby knife to scrape away the coating from the wire to expose it. Make sure to scrape all around the wire, not just the top or bottom side, to ensure that you'll have a good connection with the tape. Sandpaper will also work, if you don't want to use a knife.\nEach LED will have four wires coming from it - two positive and two negative because the LEDs are wired in parallel. It's hard to see immediately which is which (they don't have the handy longer/shorter trick like normal LEDs) - but we can quickly check them against a battery. Once we know which side is positive - mark the wire with a sharpie to help identify it. It's okay to just leave the wires straight rather than shaping them like the other LED example.\nIf you have super eyesight - you can check for a green marking on the LED, this is the negative side.\nThis picture shows an LED from a string light prepped by identifying and marking the positive wire and trimming the extra wires so that they don't accidentally short circuit each other.\nTape Down LED\nRegardless of which LED type is going into the card, line up the positive lead with the copper tape marked + and the negative with -. Use clear tape over it to hold down to the copper.\nStep 4: Attach Button\nNext, we'll place the LilyPad button over the oval icon on the template facing up. It doesn't matter which side touches postive and negative. Make sure the conductive pads on the bottom of the button touch the copper tape, then tape down the ends with clear tape. Be careful not to tape directly over the push part of the button or it may interfere with the ability to press it. You can also use a LilyPad switch instead of a button - the installation is the same.\nStep 5: Insert Battery\nOnce all the components are installed, it's time to test our circuit by adding a battery. Carefully slip the battery underneath the copper tape flap we made earlier, and center it inside the circle icon. Make sure the positive side of the battery (top, marked with the battery model and +) is facing up. Press the copper over the battery, and tape with clear tape.\nNow, press the button, and the LED should light up!\n- Check the tape connections - use your nails or a pencil to make sure the tape is firmly adhering the components to the copper tape.\n- Check the battery - make sure it is sandwiched firmly between the top and bottom copper tape lines and that the top copper is not accidentally touching the bottom of the battery.\n- Check the wires of the LED - double check that they weren't accidentally broken while bending them into shapes with pliers.\nHere are what the finished circuits should look like:\nStep 6: Prepare Pop Up\nTime to cut out our pop up pieces. Click the links below to visit Robert Sabuda's website for full instructions on assembling the Gingerbread and Christmas Tree pop ups.\nNote: There is an additional edit to this template - use a hobby knife to cut windows in the house so that the LED can shine through. Cut out the small circle in the base template (page 3 of the pdf) to let the LED shine through before you glue the house down.\nNote: To create a string of lights effect after the pop up is cut and folded, use a hole punch or small pair of scissors to cut out the small circles marked on the back of the template. For a three dimensional look, use a glue gun to adhere a small dab of glue on the back of each hole (be careful not to glue the card to your work surface). The glue will press through and create a bulb-like shape. This may take some practice to get the right amount of glue for each 'bulb' - we recommend trying on scrap paper first.\nBegin folding the window card along the center crease, be careful not to bend the frame.\nNext, carefully fold the bottom of the window frame.\nTo fold the top creases of the window frame, press the card flat against a table.\nYour final pop up will look like this:\nUse a piece of parchment paper or vellum behind the frame to create a frosted look. Tape or glue on silhouette shapes to create a holiday scene.\nStep 7: Assemble and Admire\nWhen your pop ups are constructed and ready to light, carefully place them over your copper tape circuit, and cut holes, if necessary, to allow the LED to poke through the paper. Glue or tape the corners down to adhere to the backing. Gently fold down the pop up to close the card.\nFinally, use a marker or stickers to indicate where the button should be pressed.\nAdd any extra decorations to make the card extra special. We used a white out pen to create icing on the Gingerbread House and some candy stickers.\nNow, your glowing holiday greeting is ready to mail to a friend or keep for yourself!\nResources and Going Further\nThings to try:\n- Try these techniques with other pop up designs or create your own - can you figure out how to adapt the copper tape path to fit your card choice?\n- Get crafty with stickers, paint, or markers after you've built your card to customize it even more"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:9e99ab27-dd52-4f88-bbef-7b7126b7288f>","<urn:uuid:1512e63f-b679-439a-94a6-7de4e02573e0>"],"error":null}
{"question":"Hi! I'm working on my thesis about signal processing in cardiology - what kind of heart-related signals do researchers analyze for medical diagnostics? 👨‍⚕️","answer":"Researchers analyze heart signals particularly related to arrhythmia, such as atrial fibrillation, and signals connected to dialysis. They also study biological rhythms, specifically focusing on heart frequency variability.","context":["The department´s educational and research programmes covers vast areas of the IT field: from telecommunication via antennas to artificial intelligence, from basic theories via algorithms to implementations on silicone and in biomedical systems. Digital information is currently used in nearly any context; digital broadcast of radio and television, internet and mobile telephony are just some examples. Future devices and systems will increasingly be based on a complicated collaboration between different techniques. It is in this light the department´s breadth should be seen.\nIn Electroscience, about 60 employees are working in four research groups. The groups work together focusing on applications in, for example, wireless communication and medical engineering.\nResearch covers most of the aspects of analogue and digital technology, from a theoretical and comprehensive system level, to specific algorithms and circuits. Courses are given for a number of programmes, but mostly for the programme in Electrical Engineering. Research is divided into the four main areas given below:\nIn Electronic Design, integrated circuits (microchips) with specific functions are designed and developed, mainly for wireless communication, image processing and medical applications, e.g. pacemakers. The demands vary depending on the area of application, but high computational capacity and low energy consumption are central. Research is carried out in both analogue and digital circuits, as well as conversion between these kinds of signals.\nKeywords: circuit design, analogue, digital, integrated circuits, FPGA, field-programmable gate array\nResearchers in Radio Systems are working on the basic issues of radio communication. They are studying the propagation of radio waves from transmitters to receivers, and developing technical solutions as well as complete systems. Their main aims are to develop quick, efficient and robust systems for future wireless communication.\nKeywords: radio systems, radio channels, propagation of electromagnetic waves, antennae, wireless networks, mobile telephony, systems with multiple input and output (MIMO), multi-carrier technology (OFDM), ultra-wideband (UWB), communication with medical implants\nWithin the field of Signal Processing, the electrical activity of the human body is being investigated with the aim of developing methods and algorithms for medical diagnostics and treatment. Special emphasis is placed on methods for the analysis of heart signals resulting from arrhythmia, such as atrial fibrillation, or in connection with dialysis; and also the modelling and analysis of biological rhythms, mainly heart frequency variability.\nKeywords: medical signal processing, mathematical modelling, cardiology, arrhythmia, pacemakers, audiology, oto-acoustic emissions, epilepsy\nTheoretical Electrical Engineering is a basic scientific subject, with many applications. Methods are being developed in electromagnetic wave propagation and the investigation of materials (i.e. how electromagnetic waves, such as radio waves, are reflected from or propagate in different materials) and efficient antennae. Apart from mobile telephony, applications include surveillance systems and non-intrusive sample analysis.\nKeywords: electromagnetic field theory, antennae, wave propagation, mobile telephony, scattering theory, optical fibres, non-intrusive sample analysis\nThe activities in Information Technology cover large areas of what is generally termed IT: from telecommunication, via image compression, to artificial intelligence: from basic theory, via algorithms, to implementation on chips.\nDigital information is used in almost everything these days: digital radio and TV, the Internet and mobile telephones are just a few examples. In the future, equipment and systems will be based more and more on the complicated interaction between various techniques. This involves everything from household goods, computer networks at home and multimedia systems, to huge communication networks and information systems.\nCurrent research projects include data and image compression, coding theory, cryptology, modulation and access methods, signal processing, model-based diagnostics (artificial intelligence, AI) and computer systems for search engines and other information systems.\nSome thirty-five employees work at the division. Courses are given in the programmes for Computer Science, Electrical Engineering, Industrial Management and Engineering, Engineering Mathematics and Information & Communication Engineering.\nKeywords: information technology, communication systems, coding theory, ASDL technology, data security, cryptology, data compression, image compression, computer systems, search engines, data transmission, model-based diagnostics and alarn clean-up and maintenance\nCommunication Systems applies an all-embracing systems perspective to digital communication systems that include a substantial amount of software. Research follows two main directions, telecommunication systems and software systems.\nAbout 30 people are employed at the division. Courses are given mainly in the programmes for Computer Science, Electrical Engineering and Information and Communication Engineering, and some in Industrial Management and Engineering.\nTelecommunication Systems saknas i listan focuses mainly on issues of capacity in the design of various systems, e.g. fixed telephony, data transmission, the Internet, mobile telephony, wireless networks, broadband, etc.\nKeywords: mobile systems, GSM, 3G, 4G, data transmission, broadband, the Internet, fixed telephony, IP telephony, communication protocols, Bluetooth, wireless networks"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:2265a6b8-b49d-4dd8-9fcd-d07aab235d00>"],"error":null}
{"question":"How do the principles of torsional analysis differ between bonded particles in granular solids and single angle beam structures?","answer":"In granular solids, torsional analysis focuses on the interaction between bonded particles where bonds resist stretching/compression, shear, bending, and torsion, with forces and torques calculated from potential energy. The V-model allows for independent torsional stiffness calculations and is applicable for large particle turns. In contrast, torsional analysis of single angle beams involves calculating angles of rotation and shear stresses along the beam length, with specific assumptions including pinned ends for torsional loading and uniform torsion. The beam analysis ignores shear stresses and normal stresses due to warping, and assumes no continuous lateral-torsional restraint along its length.","context":["Simulation of granullar solids using the V-model\nThe article is based on the papers:\n- V.A. Kuzkin and I.E. Asonov \"Vector-based model of elastic bonds for simulation of granullar solids\"// Phys. Rev. E, 86, 051301, 2012 (Download: pdf).\n- V.A. Kuzkin Addendum to \"Vector-based model of elastic bonds for simulation of granular solids\" // arXiv:1507.06957 [cond-mat.soft], 2015 (download ArXiv version)\nThe Discrete (or Distinct) Element Method (DEM) is widely used for the computer simulation of solid and free-flowing granular materials. Similarly to classical molecular dynamics, in the framework of DEM the material is represented by a set of many interacting rigid body particles (granules). The equations of the particles' motion are integrated numerically. In free-flowing materials the particles interact via contact forces, dry and viscous friction forces, electrostatic forces, etc. Computer simulation of deformation and fracture of granular solids, such as rocks, concrete, ceramics, particle compounds, agglomerates, nanocomposites, etc. is even more challenging. Particles in granular solids are usually connected together by some additional bonding material such as cement or glue. The example of composite material consisting of PbS nanoparticles bonded together by a copolymer is shown in figure above. The copolymer (bonding material) resists the relative translation and rotation of neighboring PbS particles. In DEM simulations bonding material is usually taken into account implicitly using the concept of so-called bonds. Neighboring particles are connected by the bonds that resist to stretching/compression, shear, bending, and torsion. The bonds cause forces and torques acting on the particles along with contact forces. Mass of the bonding material is usually neglected. The assumption does not influence static properties of the granular material. The influence on the dynamic properties is not so straightforward and should be considered separately. However let us note that in many practical applications the mass of bonding material is much smaller than the mass of the particles (see, for example, figures below). Therefore the mass of bonding material can be neglected.\nIn the paper V.A. Kuzkin and I.E. Asonov \"Vector-based model of elastic bonds for simulation of granullar solids\"// Phys. Rev. E, 86, 051301, 2012 vector-based model(the V-model) of elastic bonds in solids is developed. The V-model is based on the combination of approaches proposed in works of P.A. Zhilin, E.A. Ivanova, A.M. Krivtsov, N.F. Morozov ,  and works of M.P. Allen , S.L Price. Equations describing interactions between two rigid bodies in the general case are summarized. The general expression for the potential energy of the bond is represented via vectors rigidly connected with bonded particles (see figures below).\nThe vectors are used for description of different types of bond's deformation. The expression for potential energy corresponding to tension/compression, shear, bending, and torsion of the bond is proposed. Forces and torques acting between particles are derived from the potential energy. Two approaches for calibration of V-model parameters for bonds with different length/thickness ratios are presented. Simple analytical formulas connecting geometrical and elastic characteristics of the bond with parameters of the V-model are derived. Main aspects of numerical implementation of the model are discussed.\nAdvantages of the V-model[править]\nLet us summarize advantages of V-model:\n- longitudinal, shear, bending, and torsional stiffnesses of the bond are independent\n- applicable in the case of large turns of the particles\n- conservation of energy (bonds are perfectly elastic)\n- any non close packed structure, rods and shells can be simulated(see figures above)\n- forces and torques are calculated as a functions of particles' positions and orientations (more accurate integration of motion equations than in the case of incremental algorithm used for BPM)\n- description of bonds of any length/thickness ratio. It is shown that in the case of small deformations the behavior of the bond can fit the behavior of Bernulli-Euler rod or Timosheko rod or short cylinder connecting particles (depending on length/thickness ratio)\n- simple analytical expressions connecting parameters of V-model with geometrical and mechanical characteristics of the bond\n- bonds can connect points inside the particles or lying on particles surfaces (not only particle centers)\n- fracture criterion for the bond can be used if required\nJava Script example[править]\nTwo dimmensional example: deformation of a discrete rod. Author: Ruslan Lapin\nPull the last particle using mouse.\nPress the links under the picture to see the animation.\nThe simplest way of debugging is a solution of the following four test problems for the system of 2 bonded particles. In all four cases forces and torques acting on the particles can be calculated both analytically and numerically. Obviously, the results should coincide.\n- Pure stretching. Porition of one particle is fixed. Another particle is moving along the line connecting particles with constant velocity. The force acting on the particles is calculated.\n- Pure shear. Orientations of the particles are fixed. Position of one particle if fixed. Another particle is moving (with constant velocity) along the line, orthogonal to initial direction of the bond. The force and torques acting on the particles are calculated.\n- Pure bending. Positions of both particles are fixed and the particles are rotated along the same axis, orthogonal to the bond, but in opposite directions. The torque acting on the particles is calculated.\n- Pure torsion. Positions of both particles are fixed. Orientation of one particle is fixed. Another particle is rotating around the bond. The torque acting on the particles is calculated.\nDEM packages using the V-model[править]\nHistory and acknowledgements[править]\nThe idea underlining V-model was first formulated by Vitaly Kuzkin during communication with Michael Wolff in Technical University of Hamburg (March, 2011). The first formulation was very simple and coarse, but it works! The results of some test simulations were presented by Vitaly Kuzkin on APM 2011 conference (July, 2011). At the present moment V-model is much more flexible and physically meaningful than its first version. Now it is developed jointly by Vitaly Kuzkin and Igor Asonov. The V-model is implemented in DEM package LIGGGHTS by Patrick Fodor.\n- E.A. Ivanova, A. M. Krivtsov, N. F. Morozov, A. D. Firsova. Decsription of crystal particle packing considering moment interactions // Mechanics of Solids. 2003. Vol. 38. No 4, pp. 101-117.\n- E. A. Ivanova, A. M. Krivtsov, N. F. Morozov, Derivation of macroscopic relations of the elasticity of complex crystal lattices taking into account the moment interactions at the microlevel // J. App. Math. and Mech.,Vol. 71, Is. 4, 2007, pp. 543-561.\n- M.P. Allen, D.J. Tildesley, Computer simulation of liquids, Clarendon Press, Oxford, 1987, p. 385.","Single Angle Bending with Torsion\nIn practice it is common for angles to be loaded offset from their shear center. However, navigating the calculations for these seemingly simple elements can prove to be quite time consuming. This is an integrated approach to designing angles subjected to both a uniform bending load and a torsion induced as a result of a uniform load being offset from the shear center. The design can be done for either equal or unequal length angles. Furthermore, the vertical leg can be either longer or shorter than the horizontal leg - pointed up or down. Calculations are in accordance with the allowable stress method and the AISC 13th Edition Specification as well as the AISC Design Guide 9 - Torsional Analysis of Steel members. Section Properties\n- All angle section properties are calculated about both the geometric and principle axes. This includes properties that are listed in the AISC manual as well as those that are not - such as minor principle axis section modulus, radius of gyration and the value of bw which is tabulated for a limited number of angles in the AISC Commentary. Torsional resistance factors (J) are also calculated. Bending Stresses\n- The maximum tip bending stressses are calculated and a stress factor is also calculated to compare the principle axis solution to that when considering just the geometric axis\n- Allowable moments for both major and minor principle axes are calculated as per the AISC Specification. Torsion\n- Angles of rotation and shear stresses are calculated along the length of the beam\n- Allowable torsional shear strength is calculated as per the AISC Specification. Unity Check\n- A unity check for combined bending and torsion loading is evaluated at multiple points along the length of the angle and plotted. Deflections\n- Tip deflection of the angle is plotted for both the rotational component as well as the direct shear component of the applied loads.\n- The vertical deflection ratio (to beam length) is calculated for reference. Users are left to determine their own acceptable design guidelines in terms of the allowable deflection. End Restraint Reactions\n- Angles which are loaded about their geometric axes are subject to movement in two directions. Such movement requires an end restraint in both directions as well. The magnitude of the horizontal restraint is not insignificant and in certain circumstances can exceed the vertical reaction. The magnitude of the end restraints is calculated in both directions. User Note: These reactions ignore the stiffness of the elements attached to the angle. The analysis is limited to the following beam angles:\n- Simply supported at ends and subjected to a uniform load\n- Pinned ends for torsional loading and subject to a uniform torsion\n- Beam is without continous lateral-torsional restraint along its length\n- Shear stresses and normal stresses due to warping are ignored\n- No axial loads\n- Cb = 1.0\nThe worksheet is protected but without a password\nGuide to Stability Design Criteria for Metal Structures\nSteel Designers Manual\nThis download is for Registered Users Only.\nPerhaps you need to login or register."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:2c12c58b-2e34-49e6-bd7b-44f200991f28>","<urn:uuid:a345148a-2526-4d77-aa55-f07b59cf0014>"],"error":null}
{"question":"I'm a medical researcher looking into publication impact. What was the 2015 impact factor of the Cochrane Database of Systematic Reviews, and what specific contraceptive research was published that year?","answer":"The Cochrane Database of Systematic Reviews (CDSR) had an impact factor of 6.103 in 2015. During that year, significant contraceptive research was published, including studies on combined oral contraceptives and their relationship to venous thromboembolism (VTE). This research showed that all combined oral contraceptives increase VTE risk, with certain formulations containing desogestrel, drospirenone, gestodene, and cyproterone acetate showing 50-80% higher risk compared to those containing levonorgestrel.","context":["Cochrane Database of Systematic Reviews\nThe Cochrane Database of Systematic Reviews (CDSR) is the leading resource for systematic reviews in health care. The CDSR includes Cochrane Reviews (the systematic reviews) and protocols for Cochrane Reviews as well as editorials. The CDSR also has occasional supplements. The CDSR is updated regularly as Cochrane Reviews are published ‘when ready’ and form monthly issues; see publication schedule.\nCochrane Reviews are prepared by review author teams, working with CRGs, which are led by one or more Co-ordinating Editors. The Co-ordinating Editors are members of an Editorial Board. Dr David Tovey is the Editor in Chief. Each CRG takes responsibility for a specific area of health care or policy; see the list of CRGs.\nCochrane's editorial and publishing policies, as well as general information about the editorial and publishing processes, and publications is available in the Cochrane Editorial and Publishing Policy Resource.\nThe Cochrane editorial process follows a consistent and structured path. It is unique in two ways: (1) CRGs monitor the process of review development throughout the editorial life cycle, beginning with registration of a title, through preparation and publication of the protocol and completed review; (2) Cochrane Reviews are updated to take account of emerging evidence, to provide the best and most current evidence to guide decision-making.\nEditorials and supplements\nEditorials aim to stimulate discussion and ideas around the development of evidence synthesis to promote good decision-making in clinical care and health policy. The Editor in Chief may commission editorials linked to Cochrane Reviews of interest or on topics likely to be of interest to a broad readership. Proposals for editorials are welcome and should be submitted to the Editor in Chief for consideration.\nSince 2009, the Cochrane Colloquium abstracts (for oral presentations and posters) have been published as a CDSR supplement and, since 2010, Cochrane Methods (ISSN: 2044-4702), the official annual newsletter for methodological issues within Cochrane, has been published as an annual CDSR supplement. See Supplements for more information.\nInformation for authors\nView How to prepare a Cochrane Review for further information, including author resources and training information.\nThomson Reuters publish the impact factors of all journals indexed in the ISI Journal Citation Report. The Cochrane Database of Systematic Reviews (CDSR) impact factor describes the ratio of the number of Cochrane Reviews published, for example, during 2013 and 2014 to the number of citations these reviews received in 2015. The CDSR received its first impact factor in 2007.\nThe 2015 Impact Factor for the Cochrane Database of Systematic Reviews (CDSR) is 6.103. The release of the Journal Citation Reports (JCR) on the 13th June, 2016 did not include the CDSR due to incomplete indexing of content by Thomson. Wiley have worked with Thomson to resolve the indexing problem. The CDSR and full citation data related to the CDSR will be included in the JCR in September when the corrected version of the JCR is published.\n|Year||Impact factor (IF)||Downloads (where available)|\n|2015||6.103||IF fact sheet | IF and usage report|\n|2014||6.035||IF fact sheet | IF and usage report|\n|2013||5.939||IF fact sheet | IF and usage report|\n|2012||5.785||IF fact sheet | IF and usage report|\n|2011||5.912||IF fact sheet | IF and usage report|\n|2010||6.186||IF fact sheet | IF and usage report|\n|2009||5.653||IF fact sheet | -|\n|2008||5.182||IF fact sheet | -|\nAltmetric tracks social media sites like Twitter, Facebook, Google+, and Pinterest as well as blogs, newspapers, magazines and online reference managers like Mendeley and CiteULike for mentions of the published Cochrane Protocols and Reviews.\nThe CDSR is published online monthly with quarterly DVDs. As outlined on the access page, access is free for many people in low-income and middle-income countries via a number of initiatives. This is in addition to general access options, including national licenses and subscriptions. There are “green” and “gold” open access options for Cochrane Reviews.\nHow to submit comments and feedback\nWe welcome and encourage feedback on individual Cochrane Reviews, and there is a dedicated process for this. Readers of a Cochrane Review can do this by clicking on the \"Submit comments\" button, under article tools (access via the 'standard' version of the Cochrane Review), not the enhanced version.\nMost accessed Cochrane Reviews (2015)\nCochrane Reviews are ranked by access and full-text downloads during the year; therefore a Cochrane Review published in January may have a higher ranking than one published in September.\n- Exercise for depression Gary M Cooney, Kerry Dwan, Carolyn A Greig, Debbie A Lawlor, Jane Rimer, Fiona R Waugh, Marion McMurdo, Gillian E Mead\n- Interventions for preventing falls in older people living in the community Lesley D Gillespie, M Clare Robertson, William J Gillespie, Catherine Sherrington, Simon Gates, Lindy M Clemson, Sarah E Lamb\n- Early skin-to-skin contact for mothers and their healthy newborn infants Elizabeth R Moore, Gene C Anderson, Nils Bergman, Therese Dowswell\n- Interventions for preventing obesity in children Elizabeth Waters, Andrea de Silva-Sanigorski, Belinda J Burford, Tamara Brown, Karen J Campbell, Yang Gao, Rebecca Armstrong, Lauren Prosser, Carolyn D Summerbell\n- Interventions to improve hand hygiene compliance in patient care Dinah J Gould, Donna Moralejo, Nicholas Drey, Jane H Chudleigh\n- Midwife-led continuity models versus other models of care for childbearing women Jane Sandall, Hora Soltani, Simon Gates, Andrew Shennan, Declan Devane\n- Honey as a topical treatment for wounds Andrew B Jull, Nicky Cullum, Jo C Dumville, Maggie J Westby, Sohan Deshpande, Natalie Walker\n- Screening for breast cancer with mammography Peter C Gøtzsche, Karsten Juhl Jørgensen\n- Repositioning for pressure ulcer prevention in adults Brigid M Gillespie, Wendy P Chaboyer, Elizabeth McInnes, Bridie Kent, Jennifer A Whitty, Lukmann Thalib\n- Electronic cigarettes for smoking cessation and reduction Hayden McRobbie, Chris Bullen, Jamie Hartmann-Boyce, Peter Hajek\nNumber of Cochrane reviews and protocols published by issue\n|2015/16||Total reviews||Total protocols||Total reviews and protocols|\n|Issue 1 '16||6741||2460||9201|\n|Issue 2 '16||6791||2433||9224|\n|Issue 3 '16||6822||2460||9282|\n|Issue 4 '16||6872||2413||9285|\n|Issue 5 '16||6906||2431||9337|\n|Issue 6 '16||6931||2458||9389|\n|Issue 7 '16||6961||2459||9420|\n|Issue 8 '16||7004||2516||9520|\n|Issue 9 '16||7038||2520||9558|\n|2015/2016||New reviews||Updated reviews||Withdrawn reviews||Conclusions changed|\n|2015/16||New protocols||Updated protocols||Withdrawn protocols|\n|Issue 1 '16||42||6||9|\n|Issue 2 '16||38||7||20|\n|Issue 3 '16||27||2||7|\n|Issue 4 '16||34||6||5|\n|Issue 5 '16||43||2||39|\n|Issue 6 '16||49||6||11|\n|Issue 7 '16||27||2||8|\n|Issue 8 '16||34||1||6|\n|Issue 9 '16||35||10||9|\n|Issue 10 '15||33||4||1|\n|Issue 11 '15||49||4||10|\n|Issue 12 '15||38||6||6|","Cochrane for Clinicians\nPutting Evidence into Practice\nRisk of Venous Thromboembolism with Use of Combined Oral Contraceptives\nAm Fam Physician. 2015 Mar 1;91(5):287-288.\nAuthor disclosure: No relevant financial affiliations.\nWhich combined oral contraceptives carry the greatest risk of venous thromboembolism (VTE)?\nAll combined oral contraceptives increase VTE risk. The risk is greater for those containing desogestrel, drospirenone, gestodene (not available in the United States), and cyproterone acetate (not available in the United States) when compared with levonorgestrel. All combined oral contraceptives are effective in preventing pregnancy. (Strength of Recommendation: B, based on inconsistent or limited-quality patient-oriented evidence.)\nThe first combined oral contraceptives debuted in 1960 and are now used by 17% of women 15 to 44 years of age. In the United States, more women use combined oral contraceptives than any other contraceptive method.1 However, studies have demonstrated an up to fourfold increase in the risk of VTE among combined oral contraceptive users compared with nonusers (pregnancy carries a slightly greater than fourfold risk).2,3 Over time, the hormone formulations and dosages of combined oral contraceptives have changed in an effort to decrease thrombogenic risk. The authors of this review looked at studies featuring multiple combined oral contraceptive formulations and dosages to determine the relative risk associated with each.\nThis Cochrane review included nine cohort and 17 case-control studies. The authors found no pertinent randomized controlled trials. Only five studies objectively confirmed VTE in all study patients, raising concern that ascertainment bias influenced the outcomes of the other studies. The absolute risk of VTE in nonusers was 0.19 to 0.37 per 1,000 woman-years. The risk of VTE with combined oral contraceptive use (15 studies) was 3.5 times greater than with nonuse (95% confidence interval [CI], 2.9 to 4.3).\nCompared with that of nonusers, the risk of VTE was 3.2 times greater (95% CI, 2.0 to 5.1) with first-generation progestins, 2.8 times greater (95% CI, 2.0 to 4.1) with second-generation progestins, and 3.8 times greater (95% CI, 2.7 to 5.4) with third-generation progestins. This corresponds to absolute risk increases of 0.61 to 1.18 per 1,000 woman-years for first-generation progestins, 0.55 to 1.04 per 1,000 woman-years for second-generation progestins, and 0.72 to 1.41 per 1,000 woman-years for third-generation progestins. Risk of VTE was similar among the third- and fourth-generation progestins desogestrel, drospirenone, gestodene, and cyproterone acetate, each of which carried a risk of VTE that was 50% to 80% higher than that associated with the second-generation progestin levonorgestrel.\nThe Centers for Disease Control and Prevention (CDC) recommends against combined oral contraceptive use in those who smoke more than 15 cigarettes per day, who have a blood pressure equal to or greater than 160 mm Hg systolic or 100 mm Hg diastolic, or who have multiple risk factors for or a history of vascular disease. Likewise, the CDC strongly recommends against the use of combined oral contraceptives in those who have a history of VTE or known thrombophilia.4\nAll women should be counseled on the risk of VTE with combined oral contraceptive use vs. the risk of VTE in pregnancy (1.4%; 95% CI, 1.0% to 1.8%).5 As much as possible, physicians should try to use lower-dose hormone formulations to decrease the risk of VTE. Based on this review, levonorgestrel has a lower risk than desogestrel, drospirenone, gestodene, or cyproterone acetate.\nThe practice recommendations in this activity are available at http://summaries.cochrane.org/CD010813.\nThe views expressed here are those of the author and do not necessarily reflect those of the U.S. Army or the Department of Defense.\nde Bastos M, Stegeman BH, Rosendaal FR, et al. Combined oral contraceptives: venous thrombosis. Cochrane Database Syst Rev. 2014;(3):CD010813.\nREFERENCESshow all references\n1. Hall KS, Trussell J. Types of combined oral contraceptives used by US women. Contraception. 2012;86(6):659–665....\n2. Venous thromboembolic disease and combined oral contraceptives: results of international multicentre case-control study. World Health Organization Collaborative Study of Cardiovascular Disease and Steroid Hormone Contraception. Lancet. 1995;346(8990):1575–1582.\n3. Heit JA, Kobbervig CE, James AH, Petterson TM, Bailey KR, Melton LJ III. Trends in the incidence of venous thromboembolism during pregnancy or postpartum: a 30-year population-based study. Ann Intern Med. 2005;143(10):697–706.\n4. Centers for Disease Control and Prevention (CDC). U.S. Medical Eligibility Criteria for Contraceptive Use, 2010. MMWR Recomm Rep. 2010;59(RR-4):1–86.\n5. Meng K, Hu X, Peng X, Zhang Z. Incidence of venous thromboembolism during pregnancy and the puerperium: a systematic review and meta-analysis. J Matern Fetal Neonatal Med. 2014 May 7:1–9. [Epub ahead of print] http://informahealthcare.com/doi/abs/10.3109/14767058.2014.913130. Accessed December 23, 2014.\nCopyright © 2015 by the American Academy of Family Physicians.\nThis content is owned by the AAFP. A person viewing it online may make one printout of the material and may use that printout only for his or her personal, non-commercial reference. This material may not otherwise be downloaded, copied, printed, stored, transmitted or reproduced in any medium, whether now known or later invented, except as authorized in writing by the AAFP. Contact firstname.lastname@example.org for copyright questions and/or permission requests.\nWant to use this article elsewhere? Get Permissions\nMore in AFP\nMOST RECENT ISSUE\nAug 15, 2019\nAccess the latest issue of American Family Physician"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:b906d812-9fe7-458f-b783-b9edc804d013>","<urn:uuid:e87014cc-6689-4f7d-a969-03fc414ce2a6>"],"error":null}
{"question":"What are the key metals that deep sea mining aims to extract due to depleting terrestrial deposits?","answer":"Deep sea mining aims to extract metals such as copper, nickel, aluminium, manganese, zinc, lithium and cobalt. These metals are in high demand for high-tech applications like smartphones and green technologies including wind turbines, solar panels and electric storage batteries.","context":["There is growing interest in the mineral deposits of the deep sea . This is largely due to depleting terrestrial deposits for metals such as copper, nickel, aluminium, manganese, zinc, lithium and cobalt, coupled with rising demand for these metals to produce high-tech applications such as smartphones and green technologies such as wind turbines, solar panels and electric storage batteries.\nShallow sea mining already occurs in a number of locations around the world. These operations are targeting different resources, e.g. gold, diamonds (Namibia), iron ore (New Zealand) or sand and aggregates (Denmark, Netherlands and the UK). However, recent advancements in subsea technology have enabled greater access to seabed resources, and consequently Deep Sea Mining (DSM) is becoming increasingly feasible. Deep Sea Mining therefore relates to operations that occur below the continental shelf margins, approximately 200 metres water depth or more, and does not relate to the extraction of bulk sand and gravel. The lower limit of DSM is constrained only by the depth tolerances of subsea technology.\nThe marine dredging community are a natural choice of partner for the extraction of deep sea minerals, given the community’s experience with extracting bulk materials from the seabed. However DSM presents new challenges, in terms of the deposit types and extraction technology, the environments within which it occurs, and how its impacts can be managed. Coupled to this, the scientific evidence and associated permitting regime is evolving rapidly around the world, especially in international waters beyond national jurisdictions.\nFollowing a discussion with the WODA Board, it has been agreed to convene a WODA Working Group on the Management of the Environmental Effects of Deep Sea Mining to promote the use of robust scientific evidence in the management of the environmental effects from the dredging community, within the policy and regulation of Deep Sea Mining.\n2. Aims of the Environmental Effects of DSM working group\nThe aim of the Working Group is to promote the use of robust scientific approaches to the management of environmental effects, within the policy and regulation of Deep Sea Mining. It is intended as a technical forum for dredging community specialists with an interest in the management of environmental effects, and is not seeking to advocate the development of the sector or particular organisational interests, or address other aspects of deep sea mining.\nThe Working Group shall present the joint views of the CEDA, EADA, and WEDA members, through WODA.\n3. Terms of reference for Environmental Effects of DSM working group\nThe working group on the Management of the Environmental Effects of DSM will have the following terms of reference:\n- To contribute to policy formulation by bodies including the International Seabed Authority (ISA) and OSPAR such that they are informed with robust scientific and technical information on the management of the environmental effects of deep sea mining activities, including what is practically and technically achievable..\n- To coordinate and respond to requests for dredging community technical input to consultation on technical guidance documents on the management of environmental effects of DSM, issued by the ISA, OSPAR etc.\n- To act as a channel for scientific and technical evidence on the management of environmental effects collected by the dredging community back into relevant national and international scientific fora on deep sea mining. All formal technical responses of the Working Group on the management of environmental effects will be submitted to the ISA via WODA. Inputs to OSPAR will be by CEDA.\n- To prepare a report on the ‘state of the art’ relating to the management of environmental effects that have been discussed and agreed by the WG.\n- The working group is expected to have a duration of 3-years, to be reviewed.\n4. Membership of DSM working group on environmental effects\nCEDA, EADA and WEDA shall propose approximately 4 members each to the WODA Working Group. Members shall agree to provide independent evidence-led input, bearing in mind that the aims of the Group do not include advocacy of DSM.\nThe chair and secretary of the working group shall be elected by the whole Working Group. Other operating procedures shall be consistent with those adopted by other dredging association Working Groups.\nThe working group can include members with another formal national role on DSM, however the focus of this group is confined to providing technical input on the management of environmental effects from the dredging community.\nAll working group members undertake to provide their inputs free of charge, and will endeavour to all requests for input within the shortest possible timeframe.\nThe World Organisation of Dredging Associations (WODA) is composed of three independent sister associations with separate corporate structures: the Western Dredging Association (WEDA) serving the Americas, the Central Dredging Association (CEDA) serving Europe, Africa, and the Middle East, and the Eastern Dredging Association (EADA) serving the Asian and Pacific region. www.woda.org\n|Thomas P. Cappellino\n|Mieke van Loenen\n|Capt. K. Subramaniam\n|P. O. Box 1393\n|c/o Port Klang Authority\n|Bonsall, CA 92003, USA\n|2629 HD Delft, Netherlands\n|42009 Port Klang.\n|P: 1 (949) 422-8231\n|P: +31 (0)15 268 2575\n|P: +60 03-31688358\nThe Technical Co-ordination Committee of WODA"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:a1d05d59-c664-44c0-8e32-440c6b9a6e74>"],"error":null}
{"question":"How long is the Great Rift Valley, and which regions does it traverse?","answer":"The Great Rift Valley is over 8,700km in length, constituting nearly one-third of earth's circumference. It extends from the Jordan Valley in the north, through the Red Sea, down to South Omo Valley of Ethiopia, through Lake Turkana in Kenya, across Tanzania, Mozambique, and ends near the Zambezi delta.","context":["Two Women, Two Tribes, and a Journey of a Lifetime is a 9-part series penned by Lim Ka Ea about her one year stint in Addis Ababa, Ethiopia where she accompanied her husband on his 9th humanitarian mission. No stranger to travel and humanitarian missions herself, she learned that Ethiopia is not really Africa and Africa is not really all about national parks or long distance-runners. She also learned that being a “tai-tai” is so overrated unless there is another “tai-tai” to get into mischief with. This 9-parter tells the story of how two “tai-tais” explored Ethiopia and discovered their life as both an individual and a woman. This weekly series started with Part I: My first encounter with Africa and Part II: The faces, sounds and smell of Addis Ababa.\nAs I was about to turn my back against Ethiopia, I was given an unexpected gift. A few months later, I met a young woman, whom I had a previous brief encounter with when I made a short trip to Kenya. She has just arrived in Ethiopia from Azerbaijan with her husband who is working in the same organisation as mine.\nI soon learned that she has taken up photography while she was living in Nairobi, accompanying her husband in his previous job. Like me, she had gone through longer period of lifestyle adjustment, depression and isolation as a result of moving from places to places, in support of her husband’s work.\nThis amazing photo of the elephants of Amboseli, Kenya won Irada an award. Click to enlarge image.\nIrada started developing her interest in photography after realising that in order for her to bounce back into life, she needed something which would serve a greater purpose in her life. I find Irada to be a remarkable and inspiring woman. Not only is she a devoted wife and a mother of two adorable children, she finds time to nurture her own personal interest. She intends to pursue a career in photography with the hope that one day, she will be able to discover and share her own vision of the world through her own lens.\nHer story of courage, strength, determination and optimism provided me with renewed hope and enthusiasm about my own role in Ethiopia. I realised that I had been too busy drowning myself in personal discontentment which had in turn blinded my ability to discover a higher sense of purpose in my life there.\nInstead of doing something, I had reduced myself to just being “the unemployed wife.”\nThe opportunity for me to do something arrived when Irada suggested a trip to the South-Omo Valley, homes to the many traditional isolated tribes of Ethiopia. In the beginning, I was seduced by my sense of adventure but subsequently, my inner self reminded me that I would never forgive myself for not achieving anything while being in Ethiopia. So the idea of writing this story was conceived, thanks to this destined encounter, as Irada would have me believe.\nSouth Omo Valley – home to the forgotten tribes of Ethiopia\nAlthough South Omo Valley is home to many traditional tribes such as the Tsemay, Banna, Konso, Ari, Dassanitch, Arbore, Karo, Bumi and Surma, the Mursi were the ones attracting us the most due to their unique practices of lip plates, face painting, elaborate hairstyles and other ceremonious traditions.\nOur exciting journey began in Addis Ababa on an otherwise typical bright sunny day. In the early morning hours of 30 May 2008, we loaded our rented and chauffeured Toyota Cobra with basic camping necessities such as mosquito domes, sleeping bags, three days of food supply consisted of canned and dried food, rolls of toilet paper, a torch light, bottles of mineral water, hand sanitiser, mosquito repellent, wet wipes and a first aid kit.\nAs any seasoned travellers would do, three jerry cans of fuel were strapped securely on top of our car to prevent the possibility of being stuck in the middle of nowhere. We were informed that we would need to camp in Mago National Park as they are no hotel facilities.\nA calf looking up curiously while chewing on dried twigs\nFinally, the rare opportunity to camp in the tribal wilderness of Africa greeted us both with great trepidation as well as excitement.\nOf course being women, our travel bags included miscellaneous female hygiene and personal care products, not forgetting luxury items such as mobile phones, mp3 player, 5litres of South African white wine, a bag of mini Toblerone and a large Nestle chocolate bar.\nHalf of the car’s backseat was occupied by Irada’s photographic equipment while I settled with only a notebook and a copy of my dog-eared Lonely Planet guidebook on Ethiopia and Eritrea. We were rather amazed by Jalalem, our driver’s lonely and evidently self-contained duffel bag. Though we were only three, Irada and I decided to squeeze ourselves in the backseat so that we could talk easily.\nWe brought bags of candies to be distributed along the way with hopes of earning some smiles from the local children. This turned out to be handy since we were often greeted by emaciated-looking boys and girls demanding for “caramella” or candies in Italian. It didn’t take long for us to dispense all the candies much to the children’s delight and appreciation.\nAs we bid farewell to Addis Ababa, we could not help but feel like a saner version of Thelma and Louise, leaving behind our mundane domestic lives in search of hopefully, a melodramatic adventure.\nThe journey through the Great Rift Valley\nThe Great Rift Valley, measuring more than 8,700km in length, constitutes nearly one-third of the earth’s circumference. It extends from the Jordan Valley in the north, through the Red Sea, down south to the South Omo Valley of Ethiopia, through Lake Turkana in Kenya, across Tanzania, Mozambique and finally ends near the Zambezi delta. Situated south-west of Ethiopia, the South Omo Valley itself boasts six lakes, Ziway, Abiata-Shala, Langano, Awassa, Chamo and one of the largest of the Rift Valley; Lake Abaya measuring at 1160sqm.\nIn order to reach the Mursi, we had to make overnight stops at Arba Minch, Jinka and Mago National Park, across more than 1,000km of wide asphalt roads often proceeded with very narrow, winding and bumpy gravelled paths, as we moved towards more and more remote and isolated areas.\nAs we began to drift slowly away from Addis Ababa, we started to see the real beauty and charm of the Ethiopian countryside in all its organic splendour. Our first overnight stop, Arba Minch is approximately 550km from Addis Ababa, of which the last 110km consists of unpaved roads through mountainous terrain.\nWe were impressed by Jalalem’s driving skills as he confidently swerved and manoeuvred the car to avoid big potholes, cattle and pedestrians along the roads. In the beginning, it felt like a fun roller-coaster ride but after awhile, our buttocks and legs felt sore from the constant bumpy stretch of roads. It almost felt like we were sitting on massage chairs for hours, except that soothing vibrations were replaced by violent poundings.\nThe journey took us about eleven hours with occasional brief stopovers in the towns of Shashemene, Wolayta and Weyto to ease our full bladders, stretch our legs and refuel our vehicle and stomachs.\nKings and queens of the roads\nNotoriously known for sleeping in long car rides, I did not even doze off for a second throughout the whole journey. Neither did Irada. We were both constantly fascinated by almost everything, right down to the herds of cattle which seemed to rule the roads in almost every village we passed.\nThe local cows are shaped rather oddly, with backs protruding like humps and skins sweeping loosely beneath their necks. There was a lot of “Look! Look!” exclamations, as each of us pointed out what seemed to have amused or tickled us from both sides of the car. Jalalem was completely unperturbed, except for his strange fascination for banana plantations as he often pointed out to us.\nDespite a lot of frustrations without a local guide, we relied on Jalalem with his limited command of English to explain things. We soon picked up some basic Amharic during the course of our journey.\nSince it was already close to the month of June, we began to see the sign of rainy season albeit the seasonal rain being late at this time of the year. The weather was slightly grey and melancholy followed by occasional soft drizzle. There was already flooding in certain lower areas.\nDonkeys huddled together under the flat canopy of acacia trees, waiting stubbornly for the rain to stop. I began to notice how endearing these white-snouted four-legged creatures are as I often saw them in pairs, facing each other, as if having a private conversation with only God privy to it. Sometimes, they even nuzzled affectionately against each other’s necks. The sluggish pace of countryside often creates time for love even for animals, as compared to city life where people seem to hurry on with their inconsequential affairs.\nWhile it was unfortunate for the people who were beginning to be affected by the flood, it provided us with some cool respite from the summer heat of Addis Ababa.\nNext: Part IV – Getting in sync with nature\nKa Ea used to be a globe trotter. She has lived in Timor Leste and Afghanistan while working as a civic education and human rights officers for the United Nations. She then tried to be a full time housewife in Ethiopia and Cambodia but failed miserably. Now, she works with lawyers and human rights activists by day and watches Discovery Travel & Living by night. She writes for The Malaysian Insider during her dwindling free time. She longs for the day when someone would pay her to travel, eat and write.\nIrada Humbatova was born in Azerbaijan’s capital Baku on 12 July 1974. She trained and worked as a midwife from 1994 to 1997, later assisting the International Federation of the Red Cross/Red Crescent with maternal health work by training and supporting traditional birth attendants in rural areas. Since then she has followed her husband on Red Cross missions around the world, developing her love for photography into a passion and profession. Inspired by Africa’s immense beauty and its people’s suffering she moved from art photography to photojournalism. She has since grown to become Reuters’ stringer for Ethiopia and work on assignments for other news outlets and magazines. Irada is currently back in Baku continuing her work with Reuters. She contributes most of the photographs in this series."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:48f89fef-1e08-4b3e-bda3-5bdbca826c8c>"],"error":null}
{"question":"I'm a nursing student interested in hospital medication systems. What is a unit-dose drug distribution system, and what are the nursing practices to prevent medication errors when using it?","answer":"A unit-dose drug distribution system is a pharmacy-coordinated method where medications are packaged and distributed in single unit packages containing predetermined amounts of drugs, identified up to the point of administration. Each dose is individually packaged and labeled with the patient's name and information. To prevent medication errors in this system, nurses must follow several practices: ensure the five rights of medication administration (right patient, medication, dosage, route, and time), double or triple check procedures with other nurses, properly document everything including medication labeling and administration records, ensure proper storage of medications, and have drug guides available. The system permits better drug identification and control while reducing medication errors through standardized packaging and administration processes.","context":["2 edition of development of a unit dose distribution system in an active teaching hospital found in the catalog.\ndevelopment of a unit dose distribution system in an active teaching hospital\nDeborah I. Johnston\n|The Physical Object|\n|Pagination||iv, 49, ii leaves :|\n|Number of Pages||49|\nUnit-Dose Drug Distribution System. A unit-dose drug distribution system, managed primarily by pharmacy technicians, is a system which permits identification of the drug up to the point of administration and is the primary distribution system for all inpatient areas. Medications dispensed are contained in single unit packages and. 45 / Hospital pharmacy management Responsibilities of hospital staff The hospital pharmacist should be an expert on medicines who advises on prescribing, administering, and monitoring, as well as a supply manager who ensures that medicines are available through procurement, storage, distribution, inven-tory control, and quality assurance.\nCase Study: Unit-Dose Implementation at the Ross Memorial Hospital “Slow but Sure, Through Small Cycles of Change” Prepared by S. Fockler, RPh, Director of Pharmacy Decem Updated INTRODUCTION The Ross Memorial Hospital (RMH) is a . Outline of the procedure entailed in a Decentralized Unit-Dose Distribution system: Outline of the procedure entailed in a Decentralized Unit-Dose Distribution system Upon administration to the hospital, the patient is entered into the system. Diagnosis, allergies and other pertinent data are entered on to the Patient Profile card.\nThis study describes the conformity of commercial oral single solid unit dose packages in hospital pharmacy practice in Quebec. A large proportion of unit dose packages do not conform to a set of nine criteria set out in the guidelines of the American Society of Health-System Pharmacists and the Canadian Society of Hospital Pharmacists. The introduction of a unit-dose medicines packing, distribution and administration system at University Hospital of Leicester has streamlined the work of nurses on .\nfield study of human response to traffic noise\nHerbs, spices and medicinal plants, that have potential in northern Ontario: literature review\nInterview with the vampire\nThe master-piece of round-heads, or, An explanation and declaration of the right round-heads indeed\nThe Journal of pain\nTransactions (architectural installations, 1980-2005)\nHeaven, hell, and everything in between\nFactors that affect choice of shelving materials in single-family homes\nnote on Eric Gills Pilgrim type.\nHealing for damaged emotions\nVoyager encounters Jupiter.\nA house of kings\nLewis Carrolls Alice in Wonderland\nRhythm in prose illustrated from authors of the nineteenth century\nThe 2000-2005 Outlook for Fuel Dealers in Latin America\nStart studying UNIT DOSE DISTRIBUTION. Learn vocabulary, terms, and more with flashcards, games, and other study tools. Search. unit dose system. one day dispensing system. adv of unit dose syst. accurate dosing, pt charged only for med reciev, recycling, better control of inventory, increased commun, increased time for clinical RPH.\nChapter Unit-Dose Drug Distribution Systems Michael D. Murray, PharmD, MPH Purdue University School of Pharmacy Kaveh G. Shojania, MD University of California, San Francisco School of Medicine Background Medication errors are especially likely when health professionals engage in multiple tasks within a short time Size: 25KB.\nWHAT IS UNIT DOSE SYSTEM. Unit-dose medications have been defined as: “Those medications which are ordered, packaged, handled, administered and charged in multiples of single dose units containing a predetermined amount of drugs or supply.\n3 types of Hospital Drug Distribution Systems. Ward stock/floor stock 2. Traditional (combination of ward stock and individual patient orders) 3. Unit-dose system. Ward stock. Medications are stocked on nursing unit; nursing staff is responsible for prep and admin of meds.\n- clinical documentaion tool lists active meds and administration. Drug Distribution Systems in the Nursing Home 1. Floor Stock - only OTC items permitted (example: irrigating solutions and certain IV supplies cannot be floor stocked if it contains a “Rx Only” warning statement) 2.\nTraditional Bottles - Rarely used in Nursing homes 3. Unit Dose a. hour supply (AutoMed type packaging) Size: KB. Unit Dose Solutions Inc. (UDS) is a company based in Morrisville, NC, near Research Triangle Park, NC and conveniently located near RDU International Airport.\nFormed inthe Company’s mission is to provide packaging services to healthcare facilities (hospitals, long-term healthcare facilities, nursing homes, etc.) and small to mid-sized. A unit-dose (unit-of-use) drug distribution system is in place for all dosage forms to provide patient-specific, individually packaged medications, which minimizes nurse/caregiver drug product manipulation (e.g., cutting in half) in order to arrive at the correct dose prior to administration.\nA comparative review of decentralized versus centralized unit dose drug distribution systems for teaching hospitals is discussed. A drug distribution system Task Force, established at the Toronto Western Division of The Toronto Hospital, identified key characteristics of a safe, accountable and efficient drug distribution system.\nThis article focuses on unit dose drug distribution system used for inpatients. Its historical development is shortly described. The text highlights the goals which may be obtained by implementing. Drug Distribution and Control: Distribution–Technical Assistance Bulletins Drug control (of which drug distribution is an important part) is among the pharmacist’s most important responsi-bilities.\nTherefore, adequate methods to assure that these re-sponsibilities are met must be developed and implemented. The unit dose system of medication distribution is a phar-macy-coordinated method of dispensing and controlling medications in organized health-care settings. The unit dose system may differ in form, depending on the specific needs of the organization.\nHowever, the following distinctive elements are basic to all unit dose systems: medica. The solution to hospital needs is Athena: the modular and flexible system for pharmacies for the management of unit doses.\nThis tool allows the distribution in pharmacies of personalized formulas, or to group and identify, with the name of the patient, the prescribed medications. The aim of this study was to compare the incidence of medication errors and the stages of the drug distribution system at which they occur in a United Kingdom (UK) hospital using the ward pharmacy system, a German hospital using the unit dose system and a.\nCENTRALISED UNIT DOSE DRUG DISTRIBUTION SYSTEM • All in-patient drugs are dispensed in unit doses and all the drugs are stored in central area of the pharmacy and dispensed at the time the dose is to be given to the patient • To operate the system, delivery devices such as medication carts or pneumatic tubes are required.\nComments on French unit-dose packaging specifications 10/ - 2/5 Strong support from AFSSAPS to unit dose drug dispensing systems. By supporting the use of unit-dose packaging, the AFSSAPS contributes to the development of a unit-dose based medication distribution and control system.\nThis is a positive step since the. In the unit dose system, each dose of medication is individually packaged and identified. according to name, strength, and the patient for which it is intended.\nThis has greatly. reduced the number of medication errors. Central Location of Drugs. Under the unit dose system, the size of ward. Results and conclusioris of a study of a unit-dose drug distribution system in a private hospital are presented in this second of two articles.\nThe objectives of the study were to compare the time/cost data of the present traditional drug distribution system and the unit-dose concept of drug distribution.\nBackground The unit dose system of medication distribution (UDDS) is a pharmacy coordinated method of dispensing and controlling medications in organised healthcare settings. In our hospital, medications contained in single unit packages are delivered during the morning for a 24 h period.\nHowever, after delivery, many drugs are requested throughout the day for different reasons. Institutional pharmacies are used to buying in bulk and repackaging and debulking onsite into Unit Dose Packaging, but if the quantities, packaging specifications or federal requirements are too much for an onsite pharmacy facility, our contract packaging team can help meet all the regulations and issues.\nA unit dose is the amount of a medication administered to a patient in a single dose. If drugs are packaged as unit doses, it makes it easier and safer to administer the required dose. Each one of the capsules contains a single dose, or unit dose, of the medication.Unit dose system means a system in which multiple drugs in unit dose packaging are dispensed in a single container, such as a medication drawer or bin, labeled only with patient name and location.\nDirections for administration are not provided by the pharmacy on the drug packaging or container but are obtained by the person administering directly from a prescriber's order or medication.Unit-dose drug distribution, when all doses are dispensed by the X X pharmacy in the exact dose the physician prescribed, in final, ready-to-administer form, labelled for a particular patient.","Medication errors remain one of the most common causes of unintended harm to patients. They contribute to adverse events that compromise patient safety and result in a large financial burden to the health service. The prevention of medication errors, which can happen at every stage of the medication preparation and distribution process, is essential to maintain a safe healthcare system. One third of the errors that harm patients occur during the nurse administration phase: administering medication to patients is therefore a high-risk activity. This article highlights factors that contribute to medication errors, including the safety culture of institutions. It also discusses factors that relate specifically to nurses, such as patient acuity and nursing workload, the distractions and interruptions that can occur during medication administration, the complexity of some medication calculations and administration methods, and the failure of nurses to adhere to policies or guidelines\nIt is important for all nurses to become familiar with various strategies to prevent or reduce the likelihood of medication errors. Here are ten ways thtato help you do just that.\n- Ensure the five rights of medication administration.\nNurses must ensure that institutional policies related to medication transcription are followed. It isn’t adequate to transcribe the medication as prescribed, but to en\nsure the correct medication is prescribed for the correct patient, in the correct dosage, via the correct route, and timed correctly (also known as the five rights).\n- Follow proper medication reconciliation procedures.\nInstitutions must have mechanisms in place for medication reconciliation when transferring a patient from one institution to the next or from one unit to the next in the same institution. Review and verify each medication for the correct patient, correct medication, correct dosage, correct route, and correct time against the transfer orders, or medications listed on the transfer documents. Nurses must compare this to the medication administration record (MAR). Often not all elements of a medication record are available for easy verification, but it is of paramount importance to verify with every possible source—including the discharging or transferring institution/unit, the patient or patient’s family, and physician—to prevent potential errors related to improper reconciliation. There are several forms for medication reconciliation available from various vendors.\n- Double check—or even triple check—procedures.\nThis is a process whereby another nurse on the same shift or an incoming shift reviews all new orders to ensure each patient’s order is noted and transcribed correctly on the physician’s order and the medication administration record (MAR) or the treatment administration record. Some institutions have a chart flag process in place to highlight charts with new orders that require order verification.\n- Have the physician (or another nurse) read it back.\nThis is a process whereby a nurse reads back an order to the prescribing physician to ensure the ordered medication is transcribed correctly. This process can also be carried out from one nurse to the next whereby a nurse reads back an order transcribed to the physician’s order form to another nurse as the MAR is reviewed to ensure accuracy.\n- Consider using a name alert.\nSome institutions use name alerts to prevent similar sounding patient names from potential medication mix up. Names such as Johnson and Johnston can lead to easy confusion on the part of nursing staff, so it is for this reason that name alerts posted in front of the MAR can prevent medication errors.\n- Place a zero in front of the decimal point.\nA dosage of 0.25mg can easily be construed as 25mg without the zero in front of the decimal point, and this can result in an adverse outcome for a patient.\n- Document everything.\nThis includes proper medication labeling, legible documentation, or proper recording of administered medication. A lack of proper documentation for any medication can result in an error. For example, a nurse forgetting to document an as needed medication can result in another dosage being administered by another nurse since no documentation denoting previous administration exists. Reading the prescription label and expiration date of the medication is also another best practice. A correct medication can have an incorrect label or vice versa, and this can also lead to a med error.\n- Ensure proper storage of medications for proper efficacy.\nMedications that should be refrigerated must be kept refrigerated to maintain efficacy, and similarly, medications that should be kept at room temperature should be stored accordingly. Most biologicals require refrigeration, and if a multidose vial is used, it must be labeled to ensure it is not used beyond its expiration date from the date it was opened.\n- Learn your institution’s medication administration policies, regulations, and guidelines.\nIn order for nurses to follow an institution’s medication policy, they must become familiar with the content of the policy. This is where education comes into play whereby the institution’s educator or education department educates nurses on the content of their medication policy. These policies often contain vital information regarding the institution’s practices on medication ordering, transcription, administration, and documentation. Nurses can also familiarize themselves with guidelines such as the Beers’ list, black box warning labels, and look alike/sound alike medication lists.\n- Consider having a drug guide available at all times.\nWhether it’s print or electronic is a matter of personal (or institutional) preference, but both are equally valuable in providing important information on most categories of medication, including: trade and generic names, therapeutic class, drug-to-drug interactions, dosing, nursing considerations, side effects/adverse reactions, and drug cautionaries such as “do not crush, or give with meals.”\nUtilizing any or all of the above strategies can help to prevent or reduce medication errors. Nurses must never cease to remember that a medication error can lead to a fatal outcome and it is for this reason that med safety matters."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:53f1a9d5-ad37-43f0-8150-80bc62e4d565>","<urn:uuid:7ffb0bd6-e8c5-40fe-96bb-ef325f3d9910>"],"error":null}
{"question":"What are the common mistakes to avoid when performing shoulder-in versus travers?","answer":"For shoulder-in, common mistakes include pulling the horse's head too much to the inside while shoulders remain on track, excessive neck bend, the horse's quarters swinging out, rider not turning their body to follow the horse's shoulders, lack of impulsion and rhythm, and the horse's head being tilted. For travers, key mistakes to avoid include looking behind at the horse's quarters (which causes the horse to bring head and shoulders outside), collapsing the inside hip, allowing the upper body to become crooked, swinging the outside leg too far back, losing impulsion, asking for too much angle, allowing the horse to look outside, and most commonly, the rider leaning away from the movement which sends conflicting signals to the horse.","context":["Shoulder-in is a fantastic exercise to incorporate into your everyday schooling. It supples, straightens and is a super exercise for collecting work. In this feature, International Dressage Rider, Sarah Rogers shares her top tips for riding the perfect shoulder-in. Masterclass In Shoulder-In With Sarah Rogers was published in print by Horse & Rider.\nBefore riding any lateral work, I like to give my horses a good warm-up: At least 10 minutes of walk work then warming up in trot and canter, on both reins and then focusing on incorporating transitions both within the pace and direct transitions with changes of rein to really get the horse listening to my seat and aids.\nWhen you are teaching a horse any lateral work, aim for a few steps at a time, rather than attempting the entire length of the school. Riding a few correct steps without the horse falling in and then riding forward in a straight line is better than the horse losing his balance and falling in.\nYou can ride shoulder-in both in the walk and trot, and I like to start teaching the babies in the walk before moving up into trot. A correct shoulder-in means the correct positioning of the shoulder: Be careful not to use too much inside leg, as this will only push the horse’s quarters to the outside and will turn this exercise into leg yield, not shoulder-in.\nI always teach the horse along the fence line to give them a little help, but shoulder-in can be ridden on another line, such as three-quarter or centreline, but this is harder to ride correctly, so best left until the horse understands the movement correctly.\nTo prepare for shoulder-in, ride in a straight line and start to incorporate some 15-metre circles on the line, reducing them down to 10 metres. This will get the horse flexing around your inside leg and off your seat aids. As you come around the corner, think about the bend. However, instead of riding a circle, keep the three-track position and ride down the long side in this position. Your body needs to be positioned slightly as if turning but your aids ask the horse to move forward on the line. Your shoulders should be parallel to your horse’s shoulders.\nYou don’t want to over flex your horse to the inside, but there should be some neck bend. Too much neck bend to the inside and the horse will start to fall in through the shoulder.\nWhen you are riding this exercise, maintain the rhythm and quality of step. If your horse gets stuck or becomes stuffy, put him back onto a straight line and ride him forward then attempt it again. When riding this movement in trot, the pace should always be collected.\nYou should be able to see your horses’ inside eye, but the neck bend is very subtle and too much neck bend can also lead to loss of control of your horse’s shoulder.\nEnsure that you ride this exercise on both reins but remember, just like us, they may be stronger or more supple on one side of their body than the other.\nCommon issues that arise when attempting this exercise are:\n• The rider pulling the horse’s head to the inside when the shoulders still remain on the track.\n• Too much neck bend.\n• The horse’s quarters swinging out as this isn’t a correct shoulder-in.\n• Rider not turning their own body to follow the horse’s shoulders or blocking the movement due to stiffness.\n• Lack of impulsion and rhythm.\n• Horse’s head tilted.\nRemember this is a gymnastic exercise, so gradually start to incorporate it into your schooling and ensure you warm-up and cool your horse down correctly.\nAbout Sarah Rogers:\nSarah is a British dressage rider who rides and competes for the Bechtolsheimer family. Previously, Sarah was based in Germany for five years, where she trained and competed for Klaus Balkenhol. Working with Klaus and Annabel Balkenhol (Klaus’s daughter) Sarah broke and produced numerous young horses, as well as training the more advanced horses and competing up to Grand Prix. Sarah also spent three years in Paris competing for International Dressage Judge Marietta Almasy where she had a lot of success also.\nSarah is sponsored by Aloeride www.aloeride.com, a pure organic aloe-vera supplement harnessing the power of nature to support healthy hooves, digestion, coat and skin. Feel free to forward Masterclass In Shoulder-In With Sarah Rogers to your friends.\nAbout Viva Lotta\nViva Lotta, 8yr old homebred mare by Vivaldi owned by Dr Bechtolsheimer and Mrs Bechtolsheimer. Viva was competing at Advanced Medium at the time of this shoot, but also training towards PSG and showing a lot of potential for piaffe and passage.","In this article I thought we would look at the second of the lateral movements – travers (also known as quarters-in, haunches-in or croup-in). As with my article on shoulder-in, I thought we should start with a quick glimpse of the history of this highly controversial movement before looking at the reasons for doing (or not doing) travers and then finishing with the ‘how-to-dos’.\nTravers was first written about by the Duke of Newcastle (1592 – 1676). Newcastle introduced haunches-in along the wall to work the horse’s haunches. In his book ‘William Cavendish, A General System of Horsemanship’ he said that “The inside hand combined with the outside leg works the croup because the inside rein pushes the inside hind leg outwards, at the same time that the outside leg pushes the outside leg inwards. By combining these aids, the horse must bring his hind legs underneath himself.” He went on to recommend that the horse should be worked in both directions in this manner before starting haunches-in on a volte (small circle) at a walk.\nSalomon de la Broue (1630-1610) and François Robichon de la Guérinière (1688-1751) both had doubts about the value of travers and recommended the use of renvers (haunches-out) instead. In fact, Guérinière stated that he thought a horse trained in haunches-in relied on the wall to prevent the outside front shoulder falling out rather than the rider’s aids. De la Broue was of the same opinion but thought that haunches-in was beneficial for horses that were heavy in the hand. These two masters had a profound effect on the French school, and neither Boucher, whom mentions the movement in passing or Fillis wrote much about haunches-in.\nGustav Steinbrecht (1808 – 1885) felt that travers should be taught once shoulder-in was established. He believed that travers was extremely beneficial in developing true collection as correct travers “requires greater flexion of the forehand than the shoulder-in” and that travers was unsurpassed for improving the trot.\nAlois Podhajsky (1898 – 1973) on the other hand, questioned the use of travers. He thought that travers encouraged the horses’ natural tendency towards crookedness and that any advantages obtained from the exercise were outweighed by the disadvantages. He advocated the use of renvers, as he felt this offered all the advantages of travers without the disadvantages.\nSo if the disadvantages of travers are to encourage crookedness and to overweight the outside shoulder, what are the advantages of travers? Firstly the movement teaches the horse to engage it’s outside hind leg through correct flexion of all 3 joints. Secondly it lightens the horse’s shoulders and finally by stretching the outside of the horse it increases suppleness. In fact like shoulder-in, quarters-in is the equivalent to a Pilates work out for the horse and helps develop the horse for collection.\nObviously if you are competing in dressage you will need to perform travers by the time you get to advanced medium level in BD or second level in the States. The FEI Rule Book states in article 412, that the “main aim of the exercise is to develop and increase the engagement of the hindquarters and thereby also the collection”, and this is fundamentally why I believe we should all teach our horse the movement, as it helps the horse develop the correct muscles to carry a rider.\nThe rules go on to describe the movement as follows: “The Horse should be slightly bent round the inside leg of the Athlete but with a greater degree of bend than in shoulder-in. A constant angle of approximately thirty five (35) degrees should be shown (from the front and from behind one sees four tracks). The forehand remains on the track and the quarters are moved inwards. The horse’s outside legs pass and cross in front of the inside legs. The horse is bent in the direction in which it is moving”.\nUnder FEI rules travers can be performed either in Collected trot or Collected canter, however, the walk should be used for introducing the movement and one should refrain from practising the movement in canter as much as possible as it can lead to a crooked strike-off.\nYet another important benefit of travers is that quarters-in prepares the horse for the half pass. Basically travers and half pass are almost the same movement; it is just that they are done on different lines and with a bit more angle required for half pass.\nSo having considered the pros and cons of the movement, how do we go about teaching travers to our horse? As with shoulders-in I like to teach travers to the horse and rider separately. With the horse I teach the movement either on-line (using a cavesson) or in-hand. There are a number of methods that cover teaching travers to the horse with simple ground work exercises. I will be producing an article on this shortly but details can be found on the Straightness Training website. If you are not a Straightness Training student and would like more information Manolo Mandez has some great ground work exercises that can be rented through Vimeo and Bent Branderup has some downloads for purchase in his on-line shop.\nAs far as the rider goes, I believe the best way to learn what do with our body is to ‘play act’ travers without the horse. At the Rider Biomechanics clinics I often use pool noodles with handles to help the students feel the movement. These work brilliantly as they are able to give the student an immediate visual reference as to whether they are getting the movement right or wrong. If you want to give this a try and haven’t got a pool noodle don’t worry, just try walking the movement anyway. Make sure your hips are pointing forward and that your head and shoulders are facing forward and are level. Ensure that your upper arms are close to your torso and bent at the elbows as though you are holding an imaginary tray of drinks. Now start to walk in a straight line (preferably by a wall). Then bring your inside hip forward fractionally and take your outside hip back, ensuring that you keep your upper body pointing straight ahead and your head over your sternum. Be very careful not to collapse to the inside as you turn your lower body, or as is more common turn your shoulders to the outside. Try walking a couple of steps in this position. Feel how your outside leg starts to step further under. That’s the equivalent of your horse starting to engage his outside hind leg more. We need to be able to do the movement easily, without tension and without turning our torso to the outside or collapsing to the inside before we think about trying to do it on horseback!\nOnce we can do the above exercise without thinking about it and our horse can happily perform travers down the entire long side of the arena on-line or in-hand without losing rhythm, then it is time for us to try to ride travers.\nYou should always begin asking for travers in walk. Ensure that you have a relaxed horse and that the walk is active and free-flowing. It is important to be conscious of this so that you do not lose the rhythm or tempo and the gait is not be impaired in any way as the travers begins.\n- Use the corner before the long side of the school to help you set up. As you start down the long side check that your horse’s head is pointing straight ahead. Look forward yourself, so you are looking between your horse’s ears.\n- Bring your inside leg and hip forward and move your outside leg (and hip) back slightly to ask the horse’s hindquarters to move inwards towards a 35 degree angle. At this stage it doesn’t matter if you get the correct angle, just the try.\n- Keep your inside leg on the girth to maintain impulsion and flexion to the inside.\n- Your inside rein will maintain a soft contact and flexion.\n- If necessary use a small touch of the dressage whip on the horse’s outside hip.\n- Look for one or two strides only and then continue straight and reward your horse. Do not ask for too much, too soon.\n- DO NOT …Look behind at the horse’s quarters – although this is a cheat’s way to initiate the movement and can be deliberately utilised to get the feel of quarters-in, it should not be necessary if both you and your horse have practised this movement separately. If you look over your shoulder at your horse’s quarters your horse will bring his head and shoulders to the outside, and you will need to correct with more inside rein.\n- DO NOT …Collapse your inside hip.\n- DO NOT …Allow your upper body to become crooked.\n- DO NOT …Swing your outside lower leg too far back.\n- DO NOT …Allow the impulsion to wane. This can and does happen when you are teaching the movement. It probably means you are asking for too much, too soon. Straighten up, reward your horse and next time ask for just one step.\n- DO NOT …Ask for too much angle. If this happens the chances are you have looked to the outside, turned your shoulders to the outside or taken your outside leg too far back. It is important that the horse continues to step under his point of mass with his outside hind and not behind it.\n- DO NOT …Allow the horse to look to the outside. This will move his whole body into a sideways movement.\nA COMMON MISTAKE is for the rider to lean away from the movement in travers. The horse (unless specifically trained to the contrary) follows the rider’s bodyweight and it will naturally go in the direction the rider’s body leans towards. If the rider’s weight and his rein aids are giving conflicting signals to the horse, the rider is setting up the horse to fail. Instead, ride the horse in travers and if you need to, weight your inside stirrup as the inside hind leg comes forward. That way, you and your horse will move forward in the movement TOGETHER. This creates more expression and better flow and movement.\nLook for a few quality strides only, then straighten your horse and ride away. By asking little and rewarding frequently you will soon be able to maintain travers along the whole long side of the arena, easily and in rhythm.\nOnce you can do this try:\n- Travers on a circle\n- Travers in trot\n- Ride shoulder-in along the long side to E or B, then do a 10m circle followed by travers for the rest of the long side.\nThe travers (haunches-in) is the first movement we teach a horse in which he bends in the direction of the line of travel. Learning travers is a prelude to teaching half pass, which requires quite a lot of lateral suppleness and cadence. These movements are the only two in dressage where the forehand is on the line of travel with the haunches displaced."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:844f8a84-2b0c-4db2-9d5f-bf9f2b05af4c>","<urn:uuid:58b4f819-2cdb-4b9c-b61a-c9f4eb30f0b9>"],"error":null}
{"question":"How does the Marlin Firmware's safety features protect during 3D printing operations, and what are the recommended health and safety controls for different printing environments?","answer":"The Marlin Firmware includes safety features like software end-stops that prevent axis crashes and limit travel, though these can be disabled for specific configurations. The firmware also has EEPROM support for storing critical settings and travel limits to prevent hardware damage. Regarding safety controls, NIOSH recommends a hierarchy of protective measures: first, substituting hazardous materials when possible; second, implementing engineering controls like fume hoods or local exhaust ventilation with HEPA/carbon filtration; third, using administrative controls like limiting access to printer areas; and finally, as a last resort, using personal protective equipment like respirators. The appropriate controls depend on the setting - workplace, school, or home - and factors such as room size, ventilation, and occupant characteristics.","context":["This is a guide to using g-code command M206 to set the Z axis home offset in the Marlin Firmware, a guide to a method of adjusting the gap distance between the hot end nozzle and the build platform. To set the Z axis home offset on the 3d printer, you will use g-code commands including M206 for the home offset, M500 and M501 for the Marlin Firmware EEPROM feature, and G1 for controlled move to Z axis zero position.\nFirstly, this guide may not be suitable for all 3d printers, probably those printers that are the delta type and those with the auto bed levelling feature. Apart from that, for this guide to work for you, you will need a 3d printer configured with a Z+ end stop.\nOn a lot of 3d printer set-ups, the gap between the nozzle tip and the build platform surface is just a fraction of a millimetre, so it does not take much to upset the gap distance. Things like levelling the build platform, changing the hot end nozzle and using different filament types can cause the first layer height to be out of calibration.\nUsually, as accurately as possible, you would only adjust the final travel limit for Z axis in the Marlin Firmware, then upload the firmware to the controller motherboard. However, if adjustments are going to be made more often, it would be more convenient to adjust the final travel limit using the home offset feature.\nThe guide will explain a method of applying an offset to the Z axis to extend the maximum travel limit, initially set in the Marlin Firmware, using Pronterface. A combination of g-codes will be used through Pronterface so that the home offset can be set, saved and tested. An initial edit in the Marlin Firmware configuration.h file is required, but beyond that, you would only need to change the home offset value to change the gap distance between the hot end nozzle and the 3d printer build platform.\nRemoved Safeguards – Important\nThis guide, an extension to the Marlin Firmware v1, Basic Configuration Set-up Guide, covers editing the Marlin Firmware configuration.h file to enable EEPROM support and to disable software end-stops, and as a result of editing the configuration file, it will be important to note that some operational safeguards will be disabled; it will be possible to manually jog an axis beyond its travel limits, probably resulting in an axis crash and possibly causing damage. Accidentally pressing the 100mm jog button instead of the 10mm jog button, is an example of what could cause a 3d printer axis crash; this would normally be prevented by enabled software end-stops. Attempting to print models outside the physical print area could also cause an axis crash also.\nOnly use this method to set home offset, as described in this guide, if the users of your 3d printer are aware of the manual jog limits and the risk of crashing an axis when exceeding the limits. It would be recommended to include axis homing to the g-code compiler start file so that homing is automatically applied to the model g-code files at compile time. It would be good practice to manually home the 3d printer, using the printer interface such as Cura or a printer control interface, before starting each print.\nMarlin Firmware Configuration\nIf you are attempting to configure the Marlin Firmware for the first time you will need to head over to the Marlin Firmware set-up guide here to get started with the basics.\nIt’s basically going to be a quick edit of the Configuration.h before we get started with the main guide to configuring the home offset. Use the Arduino IDE search tool to quickly find the lines of code needed for editing.\nDisable Software End-Stops\nTo configure the home offset successfully, we’ll need to be able to travel beyond the fixed travel limits set in the Marlin Firmware. When software end-stops are enabled, the home offset will not work outside the axis travel limits. If we want the axis to travel to maximum position plus home offset, we will need to disable software end-stops.\n#define min_software_endstops false\n#define max_software_endstops false\nSoftware end-stops are enabled by default. To disable software end-stops, find the above lines of code in Marlin Firmware Configuration.h file and set each line to false as shown.\nEnable EEPROM Suport\nAfter setting the Z axis home offset on the 3d printer, we want to store the setting in EEPROM so that the home offset value we want to use is available automatically when the printer is started.\nTo enable EEPROM support in the Marlin Firmware, uncomment the above code snippets by removing the forward slashes at the start of each line of code.\nTravel Limits After Homing\nIdeally, we want to set a maximum travel limit that stops the hot end a good safe distance above the build platform with home offset set to zero, and then fill the gap between the nozzle and the build platform with home offset. If you change the build platform thickness by adding a glass surface for PLA and then remove glass surface for ABS, you will have to allow for the thickness of the glass also. A guide to clearing the current home off set is included further down this article.\n#define Z_MAX_POS 80\nNormally, you will only need to change the value for Z_MAX_POS; just edit the above line of code to the maximum travel limit you want to set for your 3d printer. On my 3d printer for example, I have around 90mm of travel on the Z axis, as shown in the above code, I’ve set the Z_MAX_POS to 80, that leaves around 10mm to play with when setting the home offset.\nMarlin Firmware Home Offset Guide\nAfter the Marlin Firmware is configured as above, the build platform needs to be levelled before attempting to set the Z axis home offset. You will need to be prepared to fine tune the final offset measurement while the hot end and the heated build platform are up to working temperature. You can practice setting the home offset while the 3d printer is cold, this will avoid trial and error while the nozzle is hot and not extruding for long length of time.\nSetting home offset and storing to EEPROM is done through the 3d printer interface software such as Cura and Pronterface. Both Cura and Pronterface have a terminal interface that allow the user to send g-code commands to the Marlin Firmware. For this guide, Pronterface will be used because manual jog controls and terminal are in the same application window for convenience.\nTerminal inputs may be case sensitive in some 3d printer software interfaces, if you get an error or no response in the terminal feedback window, check that you are typing upper-case g-code commands.\nSetting & Testing Home Offset\nThere are seven steps to follow in this guide, first two steps will be to check and clear existing offset for the Z axis, followed by five steps to set and test new home offset. If you are using the Marlin Firmware home offset feature for the first time, please be sure to read through the whole guide first before changing any settings.\nThe guide describes a set-up that is similar to my 3d printer only, so the Z axis measurements used in this guide are there as a set-up example and not meant to be copied for use in other 3d printer set-ups. If you’ve read the guide in full, you’ll have an idea of what measurements to use on your 3d printer to set your own home offset.\nChecking & Clearing Existing Home Offset\nSaving a new home offset setting will replace a previously saved offset in EEPROM, so if you’re using a 3d printer you’re not familiar with, avoid unexpected results by first checking for existing offset setting. The next two steps will help to discover and clear an existing offset.\nStep 1. This is a simple check to see if an offset has been set.\n- Not an essential step to clearing home offset, put the 3d printer in a safe position by homing each axis after powering up the printer.\n- Enter the g-code M501 in the terminal interface text box.\n- Press the send button to send the g-code to the 3d printer.\n- Data stored in EEPROM is then read to the terminal window. Look for the line with M206 to find the current Z axis home offset.\nStep 2. You can fine tune existing offset by jumping to later steps, or you can start a fresh by setting offset to Zero.\n- An illustration of what an existing home offset looks like, which can be compromised after build platform re-levelling.\n- Clear the current Z axis home offset by sending g-code M206 Z0 through the terminal; we set the Z axis home offset to zero.\n- Save the new home offset to EEPROM by sending g-code M500.\n- Confirm that the new offset was saved to EEPROM by sending g-code M501.\n- An illustration of what zero home offset looks like, the 3d printer should be homed after home offset changes.\nSetting The Initial Home Offset – 3D Printer Cold\nThis part of the guide describes setting up an initial home offset while the printer is cold. Basically, we are setting a new offset that will be a centimetre or two short of what we need, we will get the hot end nozzle close to the build platform while the printer is cold. Then, later in this guide, we heat up the 3d printer for fine tuning the final offset.\nStep 3. We prepare the printer for the next step so that an initial offset can be measured.\n- First, home the 3d printer. The illustration shows that Z_MAX_POS is much less than Z axis physical travel distance, this should give us room to set an offset.\n- Send the Z axis to the zero position by sending g-code G1 Z0 through the 3d printer software interface terminal.\n- The Z axis should now be positioned at zero. We are now ready to measure the initial offset in the next step.\nStep 4. So, moving forward from Step 3. c, we are now going to set a rough home offset value with the 3d printer cold, no heaters switched on. If you are just fine tuning the final home offset value, you could probably skip to Step 5.\nIf you have a mirrored or glass build platform, slide a sheet of paper over the platform to avoid hot end nozzle reflection that can make you think the gap between the nozzle and platform is bigger than it actually is. If you prefer, centre X and Y axis over the build platform before measuring the gap between hot end nozzle and build platform.\n- With the Z axis at zero position, as in Step 3. c, use the Z axis manual jog control to bring the hot end nozzle closer to the build platform in 1mm steps. Keep a count of how many 1mm steps, and stop when you get the nozzle about 1 to 2mm away from the platform. Make a note of the total of 1mm steps made for the home offset value. If fitted, the 3d printer LCD control interface will show Z as a negative number; this can be used as the home offset value, with the value changed to positive.\n- The 3d printer build platform and nozzle is about a millimetre or two apart and we now know the initial offset value we want to start with. For my 3d printer, the initial home offset will be 8mm; because I allowed about 10mm for home offset when setting Z_MAX_POS in the Marlin firmware.\n- We save the initial offset value to EEPROM, send the new offset value, using the g-code command M206 Z8, through the terminal; setting 8mm as the new home offset.\n- Immediately save the new offset to EEPROM by sending the g-code M500.\n- Check that the new offset was saved to EEPROM by sending g-code M501. Look for the line with M206 in it.\n- With the offset now added and saved to EEPROM, -8 position becomes the new zero position, giving the Z axis a total of 88mm of travel, as the case with my 3d printer. The offset will be updated after homing the Z axis, in the next step.\nStep 5. A new home offset value has been saved to EEPROM, and confirmed. Now it’s time to mechanically test the new offset before moving on to fine tuning. The 3d printer is still cold at this point, however, the build platform can be preheated now if preferred, especially if the platform takes a long time to heat up.\n- To avoid disturbing the X and Y axis centred over the build platform, using the 3d printer software jog controls, home only the Z axis. When the Z axis is at the end stop, the gap between the hot end nozzle and the platform should be slightly more than both Z_MAX_POS and home offset added together.\n- If the measurements check out as above, then it should be safe to send the Z axis to zero position, send the g-code command G1 Z0 through the terminal.\n- The 3d printer Z axis should now be at zero position, leaving a millimetre or two gap between the hot end nozzle the the build platform as expected.\nFine Tuning The Initial Home Offset – 3D Printer Hot\nStep 6. If you are jumping straight into fine tuning, you need to start from step 5. Right, we’re on to fine tuning the home offset now. This is were you need to be careful, because it is recommended to have the hot end and the build platform at working temperatures while setting the first layer height or gap between the nozzle and platform.\nI use A4 photocopy or printer paper as a gap feeler for setting the gap between the nozzle and the platform. You may need to cut the A4 sheet to fit inside the printer, but have the sheet at a size so that it can be handled and positioned while avoiding hands and fingers touching the hottest parts of the 3d printer.\n- Use the -Z axis jog control to fine tune the ideal offset, get the nozzle close enough to the platform to lightly grab the A4 sheet of paper. Keep a count of each jog move size for totalling later.\n- Using the A4 sheet as a feeler gauge, in the case of my 3d printer example, the jog moves total is 2.5mm. This would show as -2.5 on the 3d printer LCD.\n- An offset has already been saved to EEPROM, so we need to add 2.5mm to the existing offset, this would make the total home offset value 10.5mm. Send g-code M206 Z10.5 through the terminal.\n- Store the new offset by immediately sending g-code M500.\n- Check that the offset has been saved by sending g-code M501.\n- After homing the 3d printer again, the Z axis will be updated, and the -2.5 position will become the new zero position. Go to step 7 to test the new settings.\nPart 7. By now, the home offset should be ready for the first 3d print test, all we need to do now is test the offset setting, like in step 5, just to confirm we are ready to go. Once the following test is complete, home the 3d printer, and switch off the heated bed and the hot end nozzle heater.\n- Home the 3d printer. The illustration shows how the settings look. The 3d printer LCD would show 90.5 at Z+.\n- If you have set the home offset carefully, and there is no risk of a Z axis crash, send g-code G1 Z0.\n- The hot end nozzle should now be A4 paper thickness away from the platform. Congratulations, home offset set.\nI hope you found this guide useful, A lot of care was made to avoid mistakes, but if you find any please let me know.\nThe graphical illustrations should provide a quick guide for return visits to jog the memory when needing to set a new home offset. If you are feeling confident and you find the 3d printer interface software jog controls don’t give you enough fine tuning, use the G1 controlled move command.\nMarlin Firmware Home Offset Guide Using G-code M206","The NIOSH on 3D Printer Fumes and Health. Your Guide to 3D Printers and Health, Best Practices.\nThe NIOSH is a part of America’s CDC (Centers for Disease Control). The NIOSH itself is The National Institute for Occupational Safety and Health for the United States. It is the part of the government tasked with researching into the safety of workers in many professions. At 3DPrint.com we noticed a number of very interesting articles come out by NIOSH researchers about 3D printing. We were especially impressed with their thoughtful and thorough research on carbon nanotubes in 3D printer filaments. There is also a very informative post about 3D printers and safety on the NIOSH website. We’ve always been worried about 3D printing safety including fine particles and especially fumes from 3D printers. At 3DPrint.com we think that we are potentially creating significant health issues with some 3D printing practices. We, therefore, reached out to the NIOSH for some guidance. A group of NIOSH researchers took the time to respond to us with some best practices for 3D printer safety. We’re very thankful for their well thought out and clear answers to our questions. We must, as they have, qualify their statements as an initial response but we do believe that this is the clearest and most extensive look into 3D printing safety online.\n“It is important to note that there is a current lack of data on 3D printer emissions. In addition, the rapidly shifting description of the “workplace/production environment,” the availability of this technology beyond industrial applications, and the tremendous variety of feedstock polymers that are commercially available or can be made by consumers mean that additional research is needed to evaluate these emissions’ possible health effects.”\n1) If I 3D print with FDM at home should I get a fume hood or HEPA/Carbon filtration just in case?\n‘NIOSH focuses on worker health and our research is performed in the laboratory and in occupational settings, which can be quite different from homes. Consideration of whether to use a fume hood or filtration will depend on several factors, including the design of the 3-D printer, the type of filament being extruded (filaments are materials (plastic, nylon or other) that are fed into the printer in order to create the final object), the size and air movement in the room in which it is being used, and who is occupying the room (children, adults, people with pre-existing health problems).\nWhile there are no occupational exposure limits for the small particles emitted by 3-D printers, there are some exposure limits for specific chemical vapors that are emitted during printing. For occupational settings, these chemical exposure limits can be used to guide the selection of appropriate controls to reduce exposures to a safe level. In workplaces, NIOSH research has shown that appropriately designed and operated local exhaust ventilation with HEPA/carbon filtration reduces the amounts of particles and chemicals in air. It is important to understand that occupational exposure limits are intended to protect adults in workplace settings and, at this time, we do not know what levels of particles or chemical vapors would be safe for children and others in homes. Given this uncertainty, it is difficult to recommend specific levels that should be achieved when trying to reduce emissions in homes, though use of a printer in a well-ventilated area could help lower emissions.”\n2) What are the risks of 3D printing?\n“For FDM 3-D printers, there are risks related to the printer itself and potentially from the emissions. Risks related to the printer are similar to those associated with working with other types of machines and may include electrical shock from damaged power cords, burns from touching hot surfaces such as the extruder nozzle, and injury such as cuts from contact with sharp edges or contusions from contact with moving parts. At this time, our understanding of risks from particle and chemical vapor emissions from 3-D printers is limited.\nIn one study done by NIOSH, rats exposed for 1 hour to particle and vapor emissions from a FDM 3-D printer using ABS filament (a type of plastic material) developed acute hypertension, indicating the potential for cardiovascular effects. In another NIOSH research study, lung cells exposed to FDM 3-D printer emissions from printing with ABS and polycarbonate for about 3 hours showed signs of cell damage, cell death, and release of chemicals associated with inflammation, suggesting potential for adverse effects to the lungs if emissions are inhaled. These in vitro findings need to be confirmed with more extensive in vivo studies. It is important to understand that exposures used in toxicology studies may not be the same as those encountered by workers or in homes for a number of reasons, including the use of ventilation in workplaces or the amount of fresh air brought into homes by the heating and cooling system.”\n3) How would I best protect myself against 3d printing risks?\n“Risks related to the printer itself can often be eliminated by safe work practices and the design of the 3-D printer. For example, as with any electrical device used at work or in the home, daily inspection of the electrical cord can help to identify if the cord is damaged and should not be used. After an object is printed, allowing sufficient time for the extruder nozzle to cool down before removing the object from the build chamber will reduce the risk of burns. NIOSH researchers often observe smaller 3-D printers being used in workplaces that are also purchased by consumers for private use. Using a 3-D printer with a cover or doors that prevent the user from reaching in while machine parts are moving will help reduce the risk of injury.\n- At this time we do not know what levels of exposure causes adverse health effects, so we can’t recommend safe levels of exposure to 3-D printer emissions whether in the workplace or in homes. In occupational settings, we use the “hierarchy of controls” to protect workers from risks on their jobs. The hierarchy of controls specifies, from most preferred to least preferred, the types of controls that should be used to reduce occupational exposures:\n- The most preferred method is to substitute or eliminate the hazard. For example, in the case of FDM 3-D printing with filaments that contain carbon nanotubes, the emission of plastic-particles that contain carbon nanotubes can be eliminated by not using that type of filament if it is not necessary for the performance of the built object.\n- If a risk cannot be eliminated, engineering controls such as a fume hood or local exhaust ventilation (a system that specifically ventilates the printer rather than the air in a room) with HEPA/carbon filtration would be the next preferred method to reduce emission levels. Some 3-D printers are now being sold with built-in filtration units.\nAlternatively, a printer owner may purchase an after-market fan/filter systems to reduce emissions. However, NIOSH researchers have not yet evaluated how well these built-in or after-market filtration systems work. It is important to understand that for engineering controls such as fume hoods or local exhaust ventilation with filtration to be effective, these systems must be properly designed, built and operated.\nIn one workplace, NIOSH researchers showed that an appropriately designed and operated local exhaust ventilation with HEPA/carbon filtration reduced the amounts of particles and chemicals in air. NIOSH researchers have also observed that in some workplaces where the ventilation system is not built correctly that the chemicals are released back into the room air. Additionally, systems that use carbon filters to remove organic chemical vapors need to be monitored over time because the charcoal has a finite capacity to adsorb chemicals. Once this capacity is reached, the charcoal filter needs to be replaced or it will not capture additional organic vapor emissions.\n- If engineering controls cannot reduce the risk to an acceptable level, administrative controls may be used. An example of an administrative control is that NIOSH researchers have observed in some workplaces that employees do not enter the room where 3-D printers are operating unless it is necessary (e.g., to perform maintenance or to retrieve a built object).\nFinally, if none of these controls can reduce emissions to an acceptable level, the least preferred control is the use of personal protective technologies such as respirators or dust masks. In workplaces, respirators are the least preferred means of control because they do not remove the exposure, they only reduce the amount that might be inhaled; this depends on the proper selection of filters and cartridges that remove contaminants while breathing. Additionally, to be effective, respirators rely on the worker to properly wear and use the mask. To wear a respirator, a user must be medically cleared by a physician and it must be properly fitted and retested each year to ensure fit. The user must be properly trained on how to wear, remove, and maintain the respirator. NIOSH researchers have observed in some workplaces where 3-D printers are used that some employees with facial hair will put on a respirator, but the hair prevents the respirator from forming a tight seal with their face so the mask does not provide any protection to the worker.”\n4) If I had a 3D printer at a school what should my safety precautions be?\n“NIOSH focuses on worker health and our research is performed in the laboratory and in occupational settings, which can be quite different from environments such as homes or schools. For example, 3-D printers may be used with different frequency in schools and there may be only one printer operating in a large classroom as opposed to many printers in a small workspace. These differences will influence the types of controls implemented to reduce emissions.\nThere are no occupational exposure limits for the small particles emitted by 3-D printers but there are some exposure limits for specific chemical vapors that are emitted during printing. For occupational settings, these chemical exposure limits can be used to guide the selection of appropriate controls to reduce exposures to a safe level.\nIt is important to understand that occupational exposure limits are intended to protect adults in workplace settings. At this time, we do not know what levels of particles or chemical vapors would be safe for children in schools. Given this uncertainty, it is difficult to recommend specific levels that should be achieved when trying to reduce emissions in schools. In workplaces, NIOSH research has shown that appropriately designed and operated local exhaust ventilation with HEPA/carbon filtration reduces the amounts of particles and chemicals in air. If exhaust ventilation is not feasible, use of a printer in a well-ventilated area could help lower emissions.”\nYou May Also Like\nShould Vented Enclosures Become A Mandatory Safety Standard for FFF 3D Printers?\nWith innovation always comes unintended consequences. There’s been much-to-do with the possible health repercussions of 3D printing, particularly when it comes to the fine particles and fumes produced by the...\nHow Safe are the Titanium Powders Used in 3D Printing?\nHow safe is additive manufacturing? That’s a question that has been asked multiple times, and has been the subject of multiple research studies. In a recent study entitled “Titanium Powders...\nUL and Georgia Tech Continue Research Into Impact of 3D Printing Emissions on Indoor Air Quality\nIn 2015, non-profit safety science company Underwriters Laboratories (UL) and its Chemical Research Initiative, the Georgia Institute of Technology (Georgia Tech), and Emory University Rollins School of Public Health, worked together to conduct a two-year study...\nNew 3D Printing Safety Enclosure and Filtration System Designed for Ultimaker 3D Printers and XSTRAND\nThe Ultimate 3D Printing Store (U3DPS), based in Florida, was one of the first resellers in the US to carry engineering-grade XSTRAND 3D printing filament by Owens Corning. This exclusive material is...\nView our broad assortment of in house and third party products."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:c0733e54-c7b0-4445-a202-b16a9a26468c>","<urn:uuid:0a1cd8a3-01e6-42f3-83fd-9ed19b38da65>"],"error":null}
{"question":"When did Johnny Cash receive the Grammy Legend Award?","answer":"Johnny Cash received the Grammy Legend Award in 1991, alongside Aretha Franklin, Billy Joel, and Quincy Jones.","context":["Johnny Cash was born on February 26, 1932 in Kingsland, AR. His older brother, Jack, died when he was 12 years old.\nJohn R. \"Johnny\" Cash (February 26, 1932 – September 12, 2003) was an American singer-songwriter, actor, and author who was considered one of the most influential musicians of the 20th century. Although he is primarily remembered as a country icon, his songs and sound spanned other genres including rock and roll and rockabilly —especially early in his career—and blues, folk, and gospel. This crossover appeal won Cash the rare honor of induction in the Country Music Hall of Fame, the Rock and Roll Hall of Fame, and the Gospel Music Hall of Fame.\nCash was known for his deep, distinctive bass-baritone voice, for the \"boom-chicka-boom\" sound of his Tennessee Three backing band; for a rebelliousness, coupled with an increasingly somber and humble demeanor; for providing free concerts inside prison walls;]page needed[ and for his dark performance clothing, which earned him the nickname \"The Man in Black\". He traditionally began his concerts with the phrase \"Hello, I'm Johnny Cash.\", followed by his standard \"Folsom Prison Blues\". Music\nCountry music is a genre of American popular music that originated in the rural regions of the Southern United States in the 1920s. It takes its roots from the southeastern genre of American folk music and Western music. Blues modes have been used extensively throughout its recorded history. Country music often consists of ballads and dance tunes with generally simple forms and harmonies accompanied by mostly string instruments such as banjos, electric and acoustic guitars, fiddles, and harmonicas.\nThe term country music gained popularity in the 1940s in preference to the earlier term hillbilly music; it came to encompass Western music, which evolved parallel to hillbilly music from similar roots, in the mid-20th century. The term country music is used today to describe many styles and subgenres. In 2009 country music was the most listened to rush hour radio genre during the evening commute, and second most popular in the morning commute in the United States.\nThe Grammy Legend Award, or the Grammy Living Legend Award, is a special award of merit given to recording artists by the Grammy Awards, a ceremony that was established in 1958 and originally called the Gramophone Awards. Honors in several categories are presented at the ceremony annually by the National Academy of Recording Arts and Sciences of the United States for outstanding achievements in the music industry.\nThe first Grammy Legend Awards were issued in 1990 to Andrew Lloyd Webber, Liza Minnelli, Smokey Robinson and Willie Nelson. The honor was inaugurated to recognize \"ongoing contributions and influence in the recording field\". The next year four more musicians (Aretha Franklin, Billy Joel, Johnny Cash and Quincy Jones) were acknowledged with Grammy Legend Awards. The award was given to Barbra Streisand in 1992 and Michael Jackson in 1993.\nThe Pine Bluff Metropolitan Statistical Area, as defined by the United States Census Bureau, is a three-county region in southeast Arkansas, anchored by the city of Pine Bluff. As of the 2010 census, the MSA had a population of 100,258. It is also a component of the larger Little Rock-North Little Rock, AR Combined Statistical Area.\nThe United States of America (USA), commonly referred to as the United States (US), America, or simply the States, is a federal republic consisting of 50 states, 16 territories, a federal district, and various overseas extraterritorial jurisdictions. The 48 contiguous states and the federal district of Washington, D.C., are in central North America between Canada and Mexico. The state of Alaska is the northwestern part of North America and the state of Hawaii is an archipelago in the mid-Pacific. The country also has five populated and nine unpopulated territories in the Pacific and the Caribbean. At 3.79 million square miles (9.83 million km2) in total and with around 316 million people, the United States is the fourth-largest country by total area and third largest by population. It is one of the world's most ethnically diverse and multicultural nations, the product of large-scale immigration from many countries. The geography and climate of the United States is also extremely diverse, and it is home to a wide variety of wildlife.\nPaleo-indians migrated from Asia to what is now the US mainland around 15,000 years ago, with European colonization beginning in the 16th century. The United States emerged from 13 British colonies located along the Atlantic seaboard. Disputes between Great Britain and these colonies led to the American Revolution. On July 4, 1776, delegates from the 13 colonies unanimously issued the Declaration of Independence. The ensuing war ended in 1783 with the recognition of independence of the United States from the Kingdom of Great Britain, and was the first successful war of independence against a European colonial empire. The current Constitution was adopted on September 17, 1787. The first 10 amendments, collectively named the Bill of Rights, were ratified in 1791 and guarantee many fundamental civil rights and freedoms.\nKingsland is a city in Cleveland County, Arkansas, United States. Its population was 447 at the 2010 U.S. census. It is included in the Pine Bluff, Arkansas Metropolitan Statistical Area. It is famous as the birthplace of Johnny Cash.\nValerie June Carter Cash (June 23, 1929 – May 15, 2003) was an American singer, dancer, songwriter, actress, comedian and author who was a member of the Carter Family and the second wife of singer Johnny Cash. She played the guitar, banjo, harmonica and autoharp, and acted in several films and television shows. Carter Cash was inducted into the Christian Music Hall of Fame in 2009. She was ranked No. 31 in CMT's 40 Greatest Women in Country Music in 2002.\nJohn Gale \"Johnny\" Horton (April 30, 1925 – November 5, 1960) was an American country music and rockabilly singer most famous for his semi-folk, so-called \"saga songs\" which began the \"historical ballad\" craze of the late 1950s and early 1960s. With them, he had several major successes, most notably in 1959 with the song \"The Battle of New Orleans\" (written by Jimmy Driftwood), which was awarded the 1960 Grammy Award for Best Country & Western Recording. The song was awarded the Grammy Hall of Fame Award, and in 2001 ranked No. 333 of the Recording Industry Association of America's \"Songs of the Century\". His first hit, a number #1 was in 1959, with When It's Springtime in Alaska (It's Forty Below)\nDuring 1960, Horton had two other successes with \"North to Alaska\" for John Wayne's movie, North to Alaska, and \"Sink the Bismarck\". Horton is a member of the Rockabilly Hall of Fame and the Louisiana Music Hall of Fame\nIn journalism, a human interest story is a feature story that discusses a person or people in an emotional way. It presents people and their problems, concerns, or achievements in a way that brings about interest, sympathy or motivation in the reader or viewer.\nHuman interest stories may be \"the story behind the story\" about an event, organization, or otherwise faceless historical happening, such as about the life of an individual soldier during wartime, an interview with a survivor of a natural disaster, a random act of kindness or profile of someone known for a career achievement. Law Crime"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"format_constrained"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:27dc7628-5ea0-4ed8-9aa7-2a2e68e71175>"],"error":null}
{"question":"What's the key difference between traditional 2D design approaches and modern 3D visualization in infrastructure projects?","answer":"Traditional 2D design approaches have been the standard way of designing and representing the world, while modern 3D visualization has become the preferred standard for design, visualization, and analysis throughout an asset's lifecycle. The shift to 3D is enabled by advancements in technology and hardware, allowing for better use of spatial information to inform and guide the design process. While 2D drawings remain essential in many fields, 3D modeling provides enhanced capabilities for creating accurate and functional designs. AutoCAD designers now work extensively with both 2D and 3D modeling techniques, with 3D becoming increasingly important for creating complex designs and helping projects achieve economic, sustainability, and performance goals.","context":["As software leaders, we believe that working together to create direct, faster, and more transparent data flow between our systems will enable our customers to positively impact the way we build things. By streamlining information exchange early in the design phase, we can responsibly consider the natural environment and set achievable goals for sustainability and resiliency.\nCreating an interface between geographic information systems (GIS) and building information modeling (BIM) will help to lower costs, reduce waste and coordinate logistics scheduling. Authoritative information shared between builders and cities can ensure that projects finish on time, within budget and with less negative impact on the community.\nThe collective set of information generated from these efforts can then be available to future projects, for regulation reporting, impact and risk analysis.\nIn a world where sensors and devices providing real-time status create increasingly big data, the combined GIS and BIM systems will ingest this information for improved project context. Users can then process and visualize this data to monitor the health of the build, make adjustments, and inform ongoing maintenance requirements.\nCombining our innovative technology and the creativity of the Esri and Autodesk customer and partner communities, we look forward to the next generation of building with ‘Data at the Center’. This vision paper frames the focus areas of the Esri and Autodesk partnership. We look forward to incorporating input from our customers and partners.\nFocus on the Future\nIn the next 30 years, the number of people living in cities will double while the global population will increase by 3 billion people. This growth will require construction of a thousand new buildings every day. Urbanization will result in 75 percent of the world’s population living in cities, with 95 percent of the population within a day’s drive from an urban center.\nThe infrastructure that exists today is already failing to meet the needs of our current population, let alone scale to meet vastly accelerating and expanding demands. The way we plan, design, build and operate will go through a major transformation to meet these requirements. The same will need to happen with the software that is used to help achieve project success.\nEsri founder and president, Jack Dangermond, and Andrew Anagnost, the president and CEO of Autodesk, met on stage at Autodesk University in November to discuss how we must do, ‘More, Better . . . with Less’. Communities must become smarter, more sustainable, and grow at a pace that has never been seen before. Massive growth will happen while communities strive to become continuously more efficient; using the data and context around them to make better decisions. To achieve a sustainable world, all of this growth and change must have less net impact on the world around us.\nWhen Jack Dangermond spoke, he said that while this partnership could not have stopped Hurricane Harvey [for example], the next-generation Houston [or other urban area] will be more resilient thanks to improved understanding. Achieving more resilient infrastructure that can better respond to major catastrophes is an overriding necessity to accommodate the massive increase in urbanization in the next 30 years. Freeing the flow of data between GIS and BIM will help our best planners, designers, engineers and operators consider the interface between our built environment and natural environment––and the impact that design decisions have on this balance.\nFor our customers, obvious wins from our partnership can come from removing slow and inefficient data conversion between systems, but that is just the start. Before the partnership, both companies focused on the next major changes to our respective software and work process to meet the heightened demand for improved infrastructure. We realized that neither of us was likely to achieve this transformation alone, and that we could accelerate community success by working together. Our partnership is committed to helping discover and create the next evolution of how we do ‘smart’—smart cities, smart utilities, smart transportation, smart logistics, smart infrastructure.\nThroughout our initial meetings, our focus has always started with understanding the complete workflow of how data moves between planning, designing, building, and operating buildings and infrastructure. The key has been to figure out how we can more easily integrate BIM and GIS data to improve this process. Autodesk and Esri are focused on the following key areas to help make you more successful in the work that you do:\n- Transforming the Project Lifecycle – Improving data integration workflows will bring immediate value to both GIS and BIM users. We know we can achieve more than just data integration and are already researching opportunities to bring context to the design and building workflows that lead to improved construction and renovation of facilities and infrastructure.\n- Building Site Context with the Environment – Buildings and assets will be planned, designed, and built in a way that considers project context in depth by making relevant information available when it is needed most. By integrating GIS and BIM, planners and designers will better understand projects in relative context: how the natural and existing built environment will be impacted by and interact with new projects. Geodesign, designing with the natural environment in mind, provides a new approach that capitalizes on the power of spatial knowledge to address complex environmental design problems. Geodesign practitioners will benefit from a free flow of information to better inform the design. By pulling these insights up the project lifecycle, project owners will be able to predict potential issues, streamline the project lifecycle, and reduce costly delays.\n- Sensing Site Change – Thanks to the many technological innovations of drones, sensor input, and data processing, we have achieved the ability to rapidly scan, photograph, and sense the three-dimensional world around us. Every project can start with a realistic and accurate ‘picture’ of the original site that can be updated with regular scans to record how the site changes with every phase of development. We are focused on delivering and improving site context and visualization for all project stakeholders.\n- Designing and Visualizing the Real World in 3D – Traditionally, designing and representing the world has been done in 2D. With advancements in technology and hardware, 3D is quickly becoming the standard people want to use for design, visualization, and analysis of assets throughout their lifecycle. Our focus will be on tools and apps that utilize 3D to facilitate better use of spatial information to inform and guide the design process so that projects can achieve economic, sustainability, and performance goals.\n- Optimizing Infrastructure Operation Intelligence – The Internet of Things (IoT) makes it possible to monitor every tremor and temperature change of an infrastructure asset such as a building or bridge using embedded sensors. The world is going to be tracked with billions of sensors all around us, many of which will be designed into the assets we use and then tracked and analyzed in 3D experiences. This data will fuel machine learning that will generate new insights when they are most relevant. By sharing Esri’s expertise in The Science of Where™ and Autodesk’s leadership in design analysis, we look to discover new opportunities for enabling customers to plan, deploy and consume sensor information to improve operational performance of large systems of assets.\n- Open and Extensible – Recognizing that ‘Data is at the Center’ of our customers’ businesses and organizations, we are committed to creating extensible platforms that enable our users to innovate. We understand that the future of our platforms depends on users extending our capabilities, creating new tools from our software building blocks, and inventing new workflows to become more productive.\nFuture releases of technology will be driven by these key themes. As we focus on helping you solve the problems of tomorrow, your feedback and direction are essential to achieving our shared mission.\nWe are excited about what the future holds and look forward to shaping that future with you. More information pertaining to software functionality will be announced in 2018.","What is an AutoCAD designer?\nAn AutoCAD designer is a professional skilled in using AutoCAD software, a prominent tool in engineering, architecture, and construction, for creating precise digital drawings and models. The role is instrumental in converting ideas, sketches, or specifications into detailed plans and blueprints that serve as the foundation for real-world projects.\nBeyond drafting, these designers are pivotal in conceptualizing and optimizing designs, which is vital for project efficiency, sustainability, and safety. Their work serves as the blueprint for what gets built, making them integral to planning, problem-solving, and execution in various industries.\nDuties and responsibilities\nAutoCAD designers are responsible for creating and modifying two-dimensional (2D) and three-dimensional (3D) computer-aided design models per a project’s specifications. They work closely with engineers, architects, or designers to understand the design requirements and translate these needs into technical drawings. These drawings may include layouts, schematics, or other types of technical illustrations, depending on the industry.\nTheir responsibilities often extend to revising existing drawings, making adjustments based on changes in plans or corrections, and ensuring that all design documentation is updated. In addition to drafting, they may also review designs to ensure they comply with legal regulations and quality standards.\nThe work environment for an AutoCAD designer is predominantly office-based, where they spend most of their time on computers with specialized software. Given the collaborative nature of design and construction projects, they often work as part of a team and may need to attend meetings with other professionals involved in a project, such as engineers, project managers, or clients.\nAs design software continues to evolve, many designers have the tools to work remotely. However, this can depend on the industry and the complexity of the projects they handle.\nTypical work hours\nAutoCAD designers usually work standard office hours, from 9 AM to 5 PM, although the exact timing can vary depending on the employer and the project deadlines. Overtime may be required during critical phases of a project to meet deadlines. Some industries that require round-the-clock operations, like manufacturing or utilities, might offer shift work for designers.\nWith the increase in remote working capabilities and freelancing opportunities, some designers also have the flexibility to set their own hours, provided they meet project timelines and expectations.\nHow to become an AutoCAD designer\nIn order to become an AutoCAD designer, you will need a combination of education, training, and experience. In this career guide section, we cover the steps you’ll need to take to achieve your goal:\nStep 1: Obtain a high school diploma\nA high school diploma or equivalent, with courses in mathematics, computer science, graphic design, and drafting, provides a solid foundation for future studies in AutoCAD.\nStep 2: Earn a degree or certificate in a related field\nWhile only sometimes required, a degree or certificate in a related field, such as architecture, engineering, design, or computer science, can be beneficial. Many community colleges, vocational schools, and universities offer programs that include training in AutoCAD.\nStep 3: Learn AutoCAD\nYou can learn AutoCAD through various online platforms, community colleges, or technical schools. These courses typically cover the basics of this software’s interface, commands, and advanced 2D and 3D modeling techniques.\nHere are a few excellent online course options we recommend:\n- Take Udemy’s Complete AutoCAD course to learn how to draw and make layouts for floor plans, circuit diagrams, and mechanical drafting from scratch.\n- Learn basics and take practice quizzes in design with the Complete AutoCAD 2D & 3D From Beginners to Expert Course.\n- Continue to practice designing with Udemy’s 72 AutoCAD 2D & 3D Drawings and Practical Projects from different engineering disciplines.\n- With Skillshare’s Rendering AutoCAD Drawings in Photoshop course, you’ll learn how to convert CAD floor plans into beautiful, colored layouts.\n- Watch video lessons, complete practice exercises, and take a mock exam with the AutoCAD Certification Exam Preparation course.\nStep 4: Gain hands-on experience\nPractical experience is crucial in the field. Consider internships or entry-level civil engineering, architecture, or manufacturing design positions that allow you to apply AutoCAD knowledge in an authentic setting.\nStep 5: Get certified\nAutodesk, the creator of AutoCAD, offers certification programs that can boost credibility and demonstrate proficiency. To get certified, you must pass an exam that assesses feature and capability knowledge.\nStep 6: Apply for AutoCAD positions\nWith education, experience, and certification, you can apply for drafting, design, engineering, or architecture jobs.\nStep 7: Engage in continuous learning\nComputer-aided design is constantly evolving, so staying updated with the latest versions of AutoCAD and other relevant software is important. Take additional courses or training sessions and learn from online tutorials and forums.\nHow much do AutoCAD designers make?\nVarious factors, including geographic location, level of education, years of experience, industry, and the complexity of projects, can influence AutoCAD designer salaries. Those with extensive experience or knowledge in specific industries may command higher compensation. The size of a company and the demand for AutoCAD skills can also impact pay.\nHighest paying industries\n- Aerospace Manufacturing – $73,540\n- Architectural and Engineering Services – $71,160\n- Control Instruments Manufacturing – $70,210\n- Computer Systems Design – $69,180\n- Building Equipment Contractors – $68,880\nHighest paying states\n- California – $78,110\n- Washington – $76,540\n- Massachusetts – $74,620\n- Texas – $73,480\n- New York – $72,850\nTypes of AutoCAD designers\nIn this career guide section, we will explore the various types of AutoCAD designers, shedding light on their unique responsibilities and areas of focus.\nArchitectural AutoCAD designer\nArchitectural designers draft detailed plans for buildings and structures. They create floor plans, elevation views, and cross-sections used by architects and construction crews.\nMechanical AutoCAD designer\nMachinery, tools, and mechanical systems are developed from detailed 2D and 3D drawings of parts and assemblies. These designers ensure accurate dimensions and specifications for use in manufacturing.\nCivil AutoCAD designer\nCivil drawings must conform to industry standards and regulations. Designers in this role work to create precise drawings for infrastructure projects like bridges, highways, and sewer systems.\nElectrical AutoCAD designer\nElectrical designers draft schematics for electrical systems, including wiring diagrams, circuit layouts, and other plans. Building, machinery, or equipment projects rely on these designs for installations or repairs.\nInterior design AutoCAD designer\nThese AutoCAD designers create plans for rooms and interior areas, detailing furniture placement, fixtures, and other elements so clients can visualize spaces.\nAeronautical AutoCAD designer\nAeronautical designers work in the aerospace industry to create complex component or system designs. Their work is integral to developing and manufacturing aircraft and related technology.\nStructural AutoCAD designer\nArchitects and engineers need to ensure that structures are safe and sound. To create detailed plans, these designers focus on structural components of buildings and construction projects, such as foundations, beams, columns, and trusses.\nTop skills for AutoCAD designers\nDesigners must master using AutoCAD software to create precise and detailed technical drawings. This includes understanding features and functions, from basic drawing tools to complex 3D modeling. Technical proficiency produces high-quality designs that meet project specifications and industry standards.\nReal-world measurements and proportions are translated into 3D models for manipulation and visualization on a smaller scale. Spatial understanding is critical for creating accurate and functional designs for construction or products.\nAutoCAD designers must ensure that every project element is accurate and precise, from dimensions and scales to specific materials or processes. This meticulousness prevents costly mistakes that could affect a project’s feasibility.\nProjects often encounter challenges, such as space constraints or structural issues. Good problem-solving skills ensure that innovative solutions meet both functional and aesthetic requirements. The ability to think critically and creatively helps these designers overcome obstacles and deliver quality designs.\nLastly, communication skills are essential for these professionals. They must liaise with various stakeholders, including clients, engineers, and construction teams, to understand needs and relay design ideas. Good communication helps ensure that designs align with project objectives and stakeholder expectations, leading to successful outcomes.\nAutoCAD designer career path\nThe career journey typically begins with a solid educational foundation in architecture, engineering, or industrial design, where AutoCAD designers learn crucial principles and gain familiarity with the software. Some may pursue programs or certifications focusing on AutoCAD or similar CAD software.\nStarting roles often include draftsperson, CAD technician, or junior designer, where hands-on experience can be gained through creating technical drawings, developing plans, and modifying existing designs. These positions, often supervised by experienced designers or engineers, may include contributing to larger projects, learning to navigate industry standards, and meeting client specifications.\nWith several years of experience and a strong portfolio of work, an AutoCAD designer can advance to a senior designer role, overseeing projects from concept to completion, coordinating with team members, and meeting design standards and requirements.\nBeyond this, many choose a specific field, such as architecture, civil engineering, mechanical engineering, or electrical engineering. They may become design managers or transition to project managers, where they leverage expertise to guide larger projects.\nSimilar job titles\nAutoCAD designer position trends and outlook\nThe increasing complexity of design projects and the continuous technological evolution drive the demand for designers with advanced skills. Proficiency in the latest versions of AutoCAD and other CAD software is a must. The rise of Building Information Modelling (BIM) has affected the field significantly, as it allows for more coordinated and efficient design and construction processes.\nIntegrating virtual reality (VR) and augmented reality (AR) technologies in design and presentation processes is another growing trend. Those who can adapt to these technological changes and continuously update their skills will likely have better job prospects.\nEmployment projections for AutoCAD designers\nAccording to the U.S. Bureau of Labor Statistics, employment of AutoCAD designers is projected to decline by 3% through 2031, slower than the average for all occupations. This decline in employment is due to automation and the increasing use of 3D printing. CAD software is increasingly sophisticated and can automate many of the tasks that designers once did. The growing use of 3D printing makes it possible to create prototypes and finished products without drawings.\nDespite the decline in employment, the role is still needed for complex drawings or prototypes and to maintain or update existing drawings. Additionally, engineers and architects continue to use these professionals for new products and structures.\nAutoCAD designer career tips\nStay current with software updates\nAutoCAD regularly releases updates and new software versions. Stay current with these updates and use the most advanced tools or features to make workflows more efficient.\nMaster 3D modeling\nWhile 2D drawings are still essential in many fields, 3D modeling is becoming increasingly important. Mastering this skill can make you more versatile and valuable as a designer.\nDevelop strong communication skills\nIn this role, you’ll often need to communicate with clients, engineers, or other stakeholders. Develop strong communication skills to ensure that designs accurately reflect project requirements.\nUnderstand industry standards\nDifferent industries have varying standards and conventions for CAD designs. Understanding your industry’s standards can ensure that designs are compliant and usable.\nPrioritize continuous learning\nAutoCAD and other CAD software are complex and constantly evolving. Continuous learning can help you stay up-to-date and develop more sophisticated design skills. Consider the following:\n- AutoCAD certification programs\n- Workshops on specific features or techniques\n- Online tutorials and forums\nExpand your software knowledge\nWhile AutoCAD is prevalent, many other CAD software packages are available. Knowing how to use SolidWorks or Revit can make you more versatile as a designer.\nBuild a professional network\nNetworking can provide opportunities for learning, mentorship, and job opportunities. Here are a few professional associations to consider:\n- Autodesk User Group International (AUGI)\n- American Design Drafting Association (ADDA)\n- Designers of Things (DoT)\nDevelop a strong portfolio\nA strong portfolio showcasing your best work can be a powerful tool when looking for jobs or freelance opportunities. Make sure to include a variety of designs that demonstrate skills and versatility.\nKeep an eye on emerging trends\nThe use of CAD software is changing due to virtual reality and 3D printing. Keep an eye on these and other emerging trends to stay ahead of the curve.\nWhere the AutoCAD designer jobs are\n- Fluor Corporation\n- Jacobs Engineering Group\n- New York\nTop job sites\n- CAD/CAM Recruiters\nWhat qualifications are needed to become an AutoCAD designer?\nThe role often requires a bachelor’s degree in a related field, such as drafting, architecture, or engineering. However, some individuals may begin to work with an associate’s degree, a certificate in drafting or CAD, and a strong portfolio. Proficiency in AutoCAD software is essential, and knowledge of other industry-related software can be beneficial.\nWhat kind of training does an AutoCAD designer typically undergo?\nFormal education often provides training in how to use AutoCAD software, create and modify designs, and apply drafting principles. After graduation, they may continue to learn on the job under the supervision of more experienced designers or engineers. Continuing education through software updates and new versions is typical, as is keeping current with the latest technology.\nWhat are the essential skills for an AutoCAD designer?\nThey need strong technical skills to create detailed drawings and designs using AutoCAD software. Good problem-solving skills resolve design issues, and attention to detail ensures accuracy. Additionally, communication skills are essential for understanding project requirements and collaborating with team members.\nWhat does a typical day look like for an AutoCAD designer?\nA typical day often involves working on various design projects using AutoCAD software, such as creating new designs, modifying existing ones, or converting plans into finished work. They often collaborate with other professionals, like architects or engineers, to discuss requirements and ensure they meet goals.\nWhat are the main responsibilities of an AutoCAD designer?\nDesigners create and modify technical drawings and plans using AutoCAD software. They typically work from rough sketches and engineers’ notes to ensure projects adhere to specifications and standards. Other duties may include maintaining a database of drawings and providing cost and material estimates.\nWhat industries employ AutoCAD designers?\nVarious industries require technical drawings, including architecture, engineering, construction, manufacturing, and interior design. They might work for architectural firms, engineering consultancies, construction companies, or manufacturing plants, or they may choose to freelance.\nWhat is the role of an AutoCAD designer in project planning?\nProject planning includes the creation of technical drawings to guide the project. These drawings visually represent the scope, specifications, dimensions, procedures, and materials. They may also contribute to estimating project costs and timelines.\nHow does an AutoCAD designer collaborate with other professionals?\nThey often collaborate with architects, engineers, project managers, and construction professionals to understand requirements and incorporate feedback into designs. Close communication occurs throughout a project’s lifecycle to make necessary adjustments and implement plans accurately.\nWhat challenges do AutoCAD designers often encounter?\nChallenges include managing complex design requirements, keeping up with advancements in CAD technology, and ensuring design accuracy and precision. They might also face challenges related to project deadlines, design modifications, and communication with various stakeholders.\nHow does an AutoCAD designer stay updated with new methods and technologies in design?\nStaying updated with new methods and technologies is crucial. They can attend workshops, seminars, or training programs to learn about the latest advancements in CAD technology. Subscribing to industry publications, participating in relevant professional organizations, or engaging in online forums and communities can also be helpful."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:b6874ba1-b4ca-47df-88e0-bb54bd2ade38>","<urn:uuid:5a365c0a-52f1-4513-a469-cc0b477d9899>"],"error":null}
{"question":"Which mathematical relation is similar to how the periodic table groups elements: < or ≤?","answer":"The relation ≤ is more similar to how the periodic table groups elements. This is because ≤ is an antisymmetric relation, meaning it can go both ways only when the elements are equal, which parallels how elements in the same group of the periodic table share similar properties based on their electron configurations. For example, Group 1 elements all end with ns¹ configuration and Group 2 elements end with ns² configuration, establishing a clear pattern. In contrast, the relation < is asymmetric, meaning it cannot go both ways at all, which doesn't reflect the periodic patterns where elements in the same group share common properties.","context":["We will explore relations that are antisymmetric and asymmetric in both a real-world context and a mathematical context. We will examine properties of each of these types of relations, and determine how best to tell them apart by their differences.\nSuppose that Riverview Elementary is having a father son picnic, where the fathers and sons sign a guest book when they arrive.\nHere’s something interesting! This list of fathers and sons and how they are related on the guest list is actually mathematical! In mathematics, a relation is a set of ordered pairs, (x, y), such that x is from a set X, and y is from a set Y, where x is related to y by some property or rule.\nIf we let F be the set of all fathers at the picnic, and S be the set of all sons, then the guest book list, call it G, is a relation from set F to set S. That is, G consists of all the ordered pairs (f, s), such that f is related to s by the rule that f is the father of s.Let’s consider another example of a relation in the real world that wouldn’t seem mathematical at first glance. Consider the relation A that is defined by the rule ‘is a relative that came before that individual (an ancestor), or is that individual’. In other words, A is the set of ordered pairs (a, b), such that a is a relative of b that came before b, or is b. Once again, one wouldn’t think a list of pairs such as this would be mathematical, but it is!\nAsymmetric and Antisymmetric Relations\nWhen it comes to relations, there are different types of relations based on specific properties that a relation may satisfy.\nTwo of those types of relations are asymmetric relations and antisymmetric relations. Ot the two relations that we’ve introduced so far, one is asymmetric and one is antisymmetric. Let’s take a look at each of these types of relations and see if we can figure out which one is which.An asymmetric relation, call it R, satisfies the following property:\n- If (x, y) is in R, then (y, x) is not in R.\nTherefore, if an element x is related to an element y by some rule, then y cannot be related to x by that same rule. In other words, in an asymmetric relation, it can’t go both ways.\nAn antisymmetric relation, call it T, satisfies the following property:\n- If (x, y) and (y, x) are in T, then x = y.\nThat is, if an element x is related to an element y, and the element y is also related to the element x, then x and y must be the same element. Thus, in an antisymmetric relation, the only way it can go both ways is if x = y.\nOkay, similar names, but we can see that an asymmetric relation is different from an antisymmetric relation in that an asymmetric relation absolutely cannot go both ways, and an antisymmetric relation can go both ways, but only if the two elements are equal.\nLet’s think about our two real-world examples of relations again, and try to determine which one is asymmetric and which one is antisymmetric. First, consider the relation G consisting of ordered pairs (f, s), such that f is the father of s. Hmmm…for this relation to be asymmetric, it would have to be the case that if (f, s) is in G, then (s, f) cannot be in G. This makes sense! If f is the father of s, then s certainly can’t be the father of f. That would be biologically impossible! Therefore, G is asymmetric, so we know it is not antisymmetric, because the relation absolutely cannot go both ways.Now, consider the relation A that consists of ordered pairs, (a, b), such that a is the relative of b that came before b or a is b.\nIn order for this relation to be antisymmetric, it has to be the case that if (a, b) and (b, a) are in A, then a = b. Again, this makes sense! If a is a relative of b that came before b or is b and b is a relative of a that came before a or is a, then it must be the case that a and b are the same person, because it can’t be the case that a came before b and b came before a. Therefore, the only possibility is that a is b. Since it is possible for it to go both ways in this relation (as long as a = b), the relation is antisymmetric, but can’t be asymmetric.If you’re wondering about some examples that actually seem more mathematical.\nConsider the relations < and ≤, where (a, b) is in < only if a is strictly less than b, and (c, d) is in ≤ only if c is less than or equal to d. The relation < is asymmetric, because it can’t be the case that for two numbers, a and b, a ; b and b ; a, so if (a, b) is in ;, then (b, a) can’t be in <. It absolutely can’t go both ways.On the other hand, the relation ; is antisymmetric, because if for two numbers c and d, both c ; d and d ; c, then it must be the case that c = d.\nThe only way for it to go both ways is if c = d.\nA relation is a set of ordered pairs, (x, y), such that x is related to y by some property or rule. Two types of relations are asymmetric relations and antisymmetric relations, which are defined as follows:\n- Asymmetric: If (a, b) is in R, then (b, a) cannot be in R.\n- Antisymmetric: If (a, b) and (b, a) are in R, then a = b.\nThe easiest way to remember the difference between asymmetric and antisymmetric relations is that an asymmetric relation absolutely cannot go both ways, and an antisymmetric relation can go both ways, but only if the two elements are equal.As we’ve seen, relations (both asymmetric and antisymmetric) can easily show up in the world around us, even in places we wouldn’t expect, so it is great to be familiar with them and their properties!","Presentation on theme: \"The Periodic Law Notes (Chapter 5) – Part 2. I. History of the Periodic Table About 70 elements were known by 1850 (no noble gases) but there didn’t appear.\"— Presentation transcript:\nI. History of the Periodic Table About 70 elements were known by 1850 (no noble gases) but there didn’t appear to be a good way of arranging or relating them to study.\nA. Mendeleev and Chemical Periodicity Mendeleev placed known information of elements on cards (atomic mass, density, etc…). He arranged them in order of increasing atomic masses, certain similarities in their chemical properties appeared at regular intervals. Such a repeating pattern is referred to as periodic.\n1. Mendeleev’s table was published in 1869. 2. He left blanks in his periodic table for undiscovered elements and he predicted their properties. Later elements were discovered with properties he predicted! 3. Problems with his table – a few elements did not fit – the atomic mass arrangement did not match with other similar properties. 4. Recognition – Mendeleev never received the Nobel Prize – the importance of the Periodic Table was not realized in his lifetime!\nB. Mosleley and the Periodic Law 1. Moseley, with Rutherford, in 1911 discovered a new pattern. The positive charge of the nucleus increased by one unit from one element to the next when elements are arranged as they are now on the Periodic Table 2. This discovery led to definition of atomic number and the reorganization of the Periodic Table based on atomic number not atomic mass. 3. The Periodic Law – the physical and chemical properties of the elements are periodic functions of their atomic numbers. 4. Moseley died at the age of 28 – victim of WWI\nC. The Modern Periodic Table 1. An arrangement of the elements in order of their atomic numbers so that elements with similar properties fall in the same column (or group). Groups: vertical columns (#1-18) Periodic: horizontal rows (# 1-7) 2. Periodicity – the similarities of the elements in the same group is explained by the arrangement of the electrons around the nucleus.\nII. Electron Configuration and the Periodic Table\nA. The s-block Elements: Groups 1 and 2 1. Group 1: Alkali metals -soft silvery metals -most reactive of all metals, never found free in nature -reacts with water to form alkaline or basic solutions – store under kerosene -whenever you mix Li, Na, K, Rb, Cs, or Fr with water it will explode and produce an alkaline solution -ns 1 (ending of all electron configurations for this group)\n2. Group 2: Alkaline earth metals -less reactive than Alkali, but still react in water to produce an alkaline solution -never found free in nature -harder, denser, stronger than alkali - ns 2 (ending of all electron configurations for this group), because they have 2 electrons in the s sublevel, this makes them a little less reactive then the Alkali metals in group 1\nB. The d-Block Elements: Groups 3-12 -are all metals with metallic properties (malleability, luster, good conductors, etc…); are referred to as the Transition Metals -Harder and denser than alkali or alkaline -Less reactive than alkali or alkaline -For the most part their outermost electrons are in a d sublevel -Exceptions to the electron configuration are found in these groups (Ex: Ni, Pd, Pt)\nExamples – draw the orbital notation (lines and arrows) for the predicted electron configuration for Cr #24: However, the real E. C. is [Ar]4s 1 3d 5. The 4s 1 electron has been moved to achieve greater stability. The elements with “irregular” electron configurations are Cr, Cu, Nb, Mo, Ru, Rh, Pd, Ag, La, Pt, Au, Ac, Gd, Th, Pa, U, Np, and Cm. We will also use the actual electron configuration (found on the periodic table at the back of your book) instead of the predicted configuration for these elements. You do NOT have to memorize these, they will be highlighted or marked on your periodic table.\nC. The p – Block Elements: Groups 13 – 18 -Contain metals and nonmetals -Metalloids, along zigzag line, have characteristics of both metals and nonmetals (many are good conductors but are brittle). The metalloids are boron, silicon, germanium, arsenic, antimony, and tellurium.\n1. Group 17 - Halogens – most reactive nonmetals -7 electrons in outermost (s and p) energy levels (that is why so reactive – only need one electron to have 8) -called the salt formers (they react vigorously with metals to form salts) A salt is a metal and a nonmetal bonded together. -most are gases\n2. Group 18 - Noble gases – unreactive - 8 electrons in outermost s and p energy levels - all are gases The s and p blocks are called the main group or representative elements!\nD. The f-Block Elements: Inner Transition Metals -final electrons fill an f sublevel 1. Lanthanides – shiny reactive metals; Ce-Lu (fill the 4f sublevel) 2. Actinides – unstable and radioactive; Th-Lr (fill the 5f sublevel)\nHydrogen and Helium - Oddballs -Hydrogen is NOT an Alkali metal, it is a very reactive gas. It is placed with the Alkali metals because 1s 1 is its electron configuration. -Helium is a Noble gas, it is unreactive, but it does not have 8 electrons in outermost energy level, because it only has 2 total electrons!\nIII. Electron Configuration and Periodic Properties A. Valence Electrons – electrons in the outermost s and p orbital’s; electrons available to be lost, gained, or shard in the formation of chemical compounds When all of the valence orbital’s are full (have 8 electrons), the atom tends to be unreactive (like the Noble Gases)\n# of Valence ElectronsGroup #Ending Configuration 11ns1 very reactive 22ns2 313np1 414np2 515np3 616np4 717np5 very reactive 818np6 very unreactive\nB. Atomic Radii 1. The size of an atomic radius cannot be measured exactly because it does not have a sharply defined boundary. However the atomic radius can be thought of as ½ the distance between the nuclei of identical atoms joined in a molecule Atomic Radii Trend\n2. Period trend – atomic radii decrease as you move across a period. As you move across a period, from left to right, the size of an atom decreases because the nucleus is getting larger and more positive, but it is still pulling on the same number of energy levels. Which atom is larger? (A) Zr or Sn (B) Li or Cs\n3. Group trend - atomic radii decrease as you move up a group (or increase as you move down a group). Shielding effect - an invisible barrier made of core electrons serve to decrease the pull of the nucleus on the outer (valence) electrons. Shielding increases as you go down a group because there are more core electrons. Shielding is considered to be constant as you move across a period because the number of energy levels is staying the same. Which atom has more shielding? (A) K or Ca (B) Na or K Which atom is smaller? (A) N or P(B) Li or K\nC. Ionization Energy (IE) – a measure of the ease with which an electron can be removed from a gaseous atom or ion (sometimes called ionization potential). 1. First ionization energy – the energy required to remove one outermost electron from a gaseous atom. Second ionization energy – the additional amount of energy needed to remove an outermost electron from the gaseous +1 ion. IE Trend\n2. Period trend – ionization energy increases as you move across the period. As you move across a period the ionization energy increases because the atoms get smaller. Another way to think of it: the number of valence electrons increases (the amount of energy needed to remove one electron is less then what is needed to remove 7 or 8 electrons). 3. Group trend – ionization energy increases as you move up a group (or decreases as you move down a group). In general, as you do down a group the ionization energy decreases because the size of the atom is increasing and the outermost electrons are further from the nucleus. 1. Which atom has the higher first ionization energy? (A) Ba or At(B) Cl or Ar 2. In removing 3 electrons from boron, which electron would require the most energy to remove?\nD. Electronegativity (EN) – the tendency of an atom to attract electrons to itself when it is chemically combined (bonded) to another element. Electronegativity Trend\nIn general, metals have low EN and nonmetals have high EN. The actual amount of EN an atom has is indicated by a number of the Pauling Electronegativity Scale that goes from 0 to 4. Dr. Linus Pauling set up this scale and gave the element having the greatest EN an arbitrary number of 4, and he assigned numbers to the others relative to this element. Flourine is the most electronegative element at 4. (3.98) and Francium is the least electronegative at 0.7.\n1. Period trend - EN increases as you go across a period (excluding the noble gases) because size decreases. 2. Group trend - EN increases as you go up a group (or decreases as you go down a group) because there is less pull from the nucleus as the electrons get further away. Which would have the greater EN? (A) Ca or Se(B) Be or O Electronegativity enables us to predict what type of bond will be formed when two elements combine.\nE. Ionic Radii 1. Period trend – The size of ions decreases as you move across a period because you have more protons pulling on the same number of energy levels. 2. Group trend – the size of the ions decrease as you move up a group (or increase as you move down a group) because the number of energy levels increases. Example: Which would be larger? K+1 or Ge +4 Which is larger? P ion or Cl ion Ionic Radii Trend\n3. Metals – metal ions (cations) always have a smaller radii than its corresponding atom because: a. it loses its outer energy level electrons (valence electrons) b. the proton to electron ratio is greater in the ion than in the atom sodium atom 11p+/11e- = 1.0 sodium ion 11p+/10e- = 1.1 The value of p+ ro e- ratio varies inversely to the size of the ion. The sodium ion is smaller because it has a larger proton to electron ratio. Which would be larger? Potassium atom or potassium ion\n4. Nonmetals - nonmetal ions are always larger than their corresponding atoms because: a. repulsion between electrons b. the p+ to e- ratio is less in the ion chlorine atom 17p+/17e- = 1.0 chlorine ion 17p+/18e- = 0.94 The chlorine ion is larger because it has a smaller proton to electron ratio. Which would be larger? Sulfur atom or sulfur ion"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:e70c7b65-fa5f-4e5a-a089-46a979d9f952>","<urn:uuid:ff3753be-40fe-43bb-ad85-af93d9590869>"],"error":null}
{"question":"How does AI/ML-driven utilization forecasting help IT teams in practical scenarios?","answer":"AI/ML-driven utilization forecasting can detect minor anomalies in daily usage trends, such as increased office bandwidth usage during sporting events, even with substantial unused capacity. It helps IT teams know when to upgrade infrastructure by considering business factors like recruitment and attrition rates. The technology enables automated device procurement requests, assists in planning internet billing and IT infrastructure budgets, and helps optimize virtual memory allocation, allowing teams to address potential issues before they become problems.","context":["Optimizing Utilization Forecasting with Artificial Intelligence and Machine Learning\nContextual blindness is one major risk that IT teams dealing with huge volumes of data are either unaware of or overlook, assuming it to be “the nature of their work.” Most IT teams use multiple tools for network analytics, which can cause the data obtained from these tools to have many missing links due to different formats or names for the same metrics. Without contextual data, it’s difficult to model a set of IT infrastructure resources, examine their dependencies, understand the overall business impact, analyze based on priority, troubleshoot, and resolve issues that crop up.\nWhat IT team wouldn’t want a crystal ball that could predict the future of their organization’s IT infrastructure, letting them fix application and infrastructure performance problems before they arise?\nWell, the current shortage of crystal balls makes the union of artificial intelligence (AI), machine learning (ML), and utilization forecasting the next best thing to anticipate and avoid issues that threaten the overall health and performance of your IT infrastructure components.\nUtilization forecasting is a technique that applies ML algorithms to produce daily usage forecasts for all utilization across CPUs, physical and virtual servers, disks, storage, bandwidth, and other network elements, enabling networking teams to manage resources proactively. This technique helps IT engineers and network administrators prevent downtime caused by overutilization.\nAI/ML-Driven Forecasting for IT infrastructure\nAn AI/ML-driven forecasting solution produces intelligent and reliable reports, taking advantage of the current availability of ample historic records and high-performance computing algorithms.\nWithout AI and ML, utilization forecasting relies on reactive monitoring. You set predefined thresholds for given metrics such as uptime, resource utilization, and network bandwidth, as well as hardware metrics like fan speed and device temperature. When a threshold is exceeded, an alert is issued.\nHowever, this reactive approach will not detect the anomalies that happen below the set threshold, causing indirect issues that could be detrimental to your network’s health to go undetected. Moreover, traditional utilization forecasting can’t tell you when you will need to upgrade your infrastructure based on current trends.\nHow AI/ML-Driven Utilization Forecasts Are Better\nA closer look at predictive technologies reveals the fundamental difference between proactive and reactive forecasting.\nWithout AI and ML, utilization forecasting uses linear regression models to extrapolate and make predictions based on existing data. This method involves no consideration of newly allocated memory or anomalies in utilization patterns. Also, pattern recognition is a foreign concept. Although useful, linear regression models do not give IT admins complete visibility.\nUtilization forecasting driven by AI and ML, on the other hand, utilizes seasonal-trend decomposition using the Loess (STL) method. STL allows you to study the propagation and degradation of memory, as well as analyze pattern matching, whereby periodic configuration changes in a metric will automatically be adjusted. In simple terms, STL dramatically improves the accuracy of AI/ML-driven forecasting thanks to those dynamic, automated adjustments. Moreover, if any new memory is allocated, or if the memory size is increased or decreased for a device, the prediction will change accordingly; this is not possible with linear regression.\nTo forecast utilization proactively, you need accurate algorithms that can analyze usage patterns and detect anomalies in daily usage trends—without generating any false positives. Let’s examine a simple use case.\nWith AI/ML-driven utilization forecasting, you can find a minor increase in your office bandwidth usage during the World Series, the FIFA World Cup, or other sporting events. Even if you have a huge amount of unused internet bandwidth, this anomalous usage can be detected.\nLikewise, proactive utilization forecasting lets you know when to upgrade your infrastructure by factoring your business’ new recruitment and attrition rates. With AI aiding intent-based networking, the IT admin is the gamemaster as well as the umpire, well-equipped with an extra pair of artificially intelligent eyes, ready to spot violations. Fueled by AI-based forecasting techniques, an organization’s mean time to repair (MTTR) is also bound to reduce.\nData-Driven Decision-Making with AI/ML-Driven Utilization Forecasts\nIncorporating AI-based forecasting consequently means fewer bottlenecks and increased productivity for IT admins. While previously there was a need for constant, manual network monitoring, AI and ML can now be used to generate a forecast report so an IT admin has a clearer picture of the usage levels of various devices in the network. AI can provide predictions that play a pivotal role in an IT admin’s evaluation process, thus eliminating possible human error or biases. In other words, ML coupled with human intervention is possibly the best way to go about making all important, resource-dependent IT decisions.\nIT admins can get a helping hand from an AIOps-enabled network performance monitoring (NPM) tool that offers data-driven recommendations in terms of consumable data formats of only useful information. This results in less experience-based problem-solving and more data-influenced problem avoidance. An AIOps-enabled NPM solution can alert the IT admin in advance about possible performance degradation. AI/ML-based forecasting enables organizations to raise automated device procurement requests well in advance, plan their internet billing and IT infrastructure budget, know if they need to reduce their internet usage, and modify the allocation and reallocation of virtual memory as required. This essentially means you can roll out the solution to a networking issue before it even knocks at your data center’s door.\nBeyond forecasting, ML can also be used to improve anomaly detection. Here, adaptive thresholds for different metrics are established using ML and analysis of historic data, revealing anomalies and triggering appropriate alerts. Furthermore, application and infrastructure monitoring functions will also be improved when enhanced with AI and ML technologies."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:6ac4ca63-48b4-41af-8d98-f84add71d25e>"],"error":null}
{"question":"¿Cuántos tipos de fuentes se recomienda usar en el diseño de apps vs sitios web? 🤓","answer":"Both app and web design follow the same recommendation of using 2 fonts (3 at most) and contrasting a serif with a sans-serif font. For apps specifically, the main operating systems have default fonts - Android uses Roboto while iOS uses the Helvetica family and San Francisco typography. For websites, the rule of thumb is also to use 2 fonts, typically one for body copy and one for headings, with the key being to maintain consistency to help readability.","context":["The importance of typography in app design\nApp design not only affects graphic design, but also the structure and organization of the content of the application. This is the reason why design can not stupidly follow trends. Indeed, the interaction logic that the application will have must obey invariable rules.\nTypography is one of the elements that have changed most over time. Practically always neglected in the past, it now has a prominent place in app design.\nAesthetics is not the only reason for the choice of typography. We will discuss the technical reasons for paying attention to the font used in an application. Beyond making it more beautiful, your typography can improve:\nReadability: Choosing a good typography assumes that the user will read your application more comfortably, especially if it contains a lot of text. The app design of a media such as the New York Times is complicated because the content targets many different types of users, ages and tastes. To adapt, some applications allow the user to set the size and style of characters. There will always be a typography by default, but this one will adapt to the needs of each user.\nAll User Experience (UX): Typography affects it directly. If the font is incorrectly chosen, the user will spend more time decrypting the contents of an application, at the risk of getting tired of it and of removing your application. The design app must therefore focus on the font in order to propose the best UX possible.\nProfessionalism: A font that does not match your brand’s philosophy undermines the consistency of appearance. Professional applications of large commercial enterprises will have a different character from entertainment applications.\nObviously, there are other reasons to pay attention to typography, some of which are related to subjective factors such as personal preferences. In all cases, the choice of typography for your design must satisfy the three conditions mentioned above. Probe users to identify the typography that will maximize the conversion rate and generate the most interactions.\nTop 4 Tips to Improve your App’s Typography\nIt should be noted that 95% of digital communication goes through the text. Despite the importance of a logo or an icon for the app design, it is indeed the text that conveys the communication with the user.\nThe ease with which texts are read – in other words, the legibility of an application – affects the UX and ease of use of your app. Typography is an essential part of this process; easy to decipher characters will always be your best option. But how to choose a “good” typography?\nThe key elements of good typography in app design\nNow identify the characteristics of a good font. The choice or design of typography is very important for app design. Do you know how to make your typography attractive? Take note :\n1. The size of the letter\nApple says on its web page of help to the developers that it is useless that your typography is the most refined if it is illegible on your application. The font size recommended by the US firm to design applications for iOS devices is 11 to 17 points. The recommendation is similar for Android. It should be noted that an average of 60 to 75 characters per line of a web page must be reduced between 35 to 50 characters per line for a mobile application. The ideal is to write short sentences and to minimize their complexity in order to facilitate reading. Although there are applications that allow you to adjust the size of characters from the device, this is not always the case, so it is advisable to be careful.\n2. The spaces\nWe have very little room to organize content in app design, so we must make intelligent use of white space in the application. This means that, contrary to what most users may believe, it is better to avoid displaying too much text, images or buttons on each screen. On the contrary. We have to leave spaces between the lines and use margins to bring more clarity to the design. In short, do not be afraid to leave empty spaces. It is essential to take care of the particularly important pages (payment, subscription, etc.) which are the support of the conversion. In these cases, it is best to limit the amount of content and leave space.\n3. The contrast\nA strong contrast between the text and the background is necessary for the user to see the information at a glance. The default choice is a black text on a white background. Obviously, if all app designers followed these guidelines, all applications would look the same. It is possible and advisable to play with the colors to identify the correct combination of colors and style that will make the mark of your mobile application. Keep in mind that there must at least be a contrast between two colors (one for the background and one for the letters).\n4. The serif of characters\nThe character wheelbase is the subject of a great debate which is still current for websites, and of course extends to app design or graphic design of any kind. The prevailing operating systems, Android and iOS, are very clear on the recommendations. By default, Android uses the Roboto font for display, while Apple uses the Helvetica font family. These fonts are sans serif. In both cases, we can say that they have chosen to use simple and fine lines. In the case of iOS, the arrival of the typography San Francisco in 2015 has further improved the readability, required by the small format of Apple Watch.","Is the type on your site legible? Is it readable? Do people have to put in a lot of effort to read your prose or does your type get out of the way so that your words are easily understood?\nA few days ago Scrivis started a new thread on the Drawer forums (a interesting place for discussions about design by the way) with a complaint about how some site designs have text that simply can’t be read. In his own words:\nI can’t tell you how often this happens. I get linked to a website with a great article or a website where someone tells me it has a great design and when I visit it I can’t read a damn thing. Having bad aesthetics is acceptable. Misplacing stuff on a grid I can live with. However, how do some people design a site, look at the font and walk away thinking that it is legible?\nIf web design is 95% typography how can people design a site with unreadable copy?\nThe designer of the Inventory Magazine site showed up defending his choices. He made some good points including the client’s wishes and some approval from the site’s audience. Some of us in the thread had a hard time reading the text. Others read it perfectly fine.\nMy first thought was simply the font is too small (for my aging eyes anyway). As the thread continued I began to wonder what is it that makes type legible? What makes the text on one site easy to read and hard to read on another site? Is it simply the size of the font or is there something more?\nThe Difference Between Legibility and Readability\nThe first question we should ask is does type even need to be legible? I’ll let Alan Haley answer the question.\nNot all typefaces are designed to be legible. Many are drawn to create a typographic statement, or provide a particular spirit or feeling to graphic communication. Some are even designed just to stand out from the crowd. To the degree that a typeface has personality, spirit, or distinction, however, it often suffers proportionally on the legibility scale.\nThere are actually two types of type.\n- Text Type is designed to be legible and readable across a variety of sizes\n- Display Typeis designed to attract attention and pull the reader into the text. It can be more elaborate, expressive, and have a stylish look.\nConsider the image below, a screenshot of the site of designer Ardo Ayoub. The type above the navigation can be made out as the designer’s name, but it’s main purpose isn’t to be read. It’s there to set a mood. It doesn’t need to be legible. That’s not its point.\nNote: The text is much easier to read at the smaller size of the image here. Click through to the site to see it larger. Had you not seen the image here first, you might not have quickly recognized the text as the designer’s name.\nI’ve been interchanging the words legibility and readability so far, but they’re actually two different things.\n- Legibility – a measure of how easy it is to distinguish one letter from another in a given typeface. Legibility describes the design of a typeface. How legible a typeface is designed to be depends on its purpose. Legible typefaces usually have larger closed or open inner spaces (counters). They generally have a larger x-height, though not too large.\n- Readability – how easy words, phrases, and blocks of text can be read. Readability describes how a typeface is used on the page. Good typography (more readable) encourages a desire to read the copy and reduces the effort required to read and comprehend the type. The reader shouldn’t even notice the type. She should simply understand the words.\n“Legibility” is based on the ease with which one letter can be told from the other. “Readability” is the ease with which the eye can absorb the message and move along the line.\n—J. Ben Lieberman “Types of Typefaces”\nType must be legible to make it readable, but making type more legible doesn’t necessarily increase readability. Many other things go into creating readable type.\nHow to Make Your Type More Readable\nThe common reaction when coming across text you can’t read online is to make it larger. More goes into designing text that is readable, some of which has little to do with the type itself.\nLet’s walk through some of the factors that affect readability starting with your overall design. This post will focus on the design terminology. I’ll have a followup on how to control things with css.\nLayout: The use of grids, whitespace, and images can all have an impact on readability. Images can help create a flow through your text and give readers a place to rest. A lack of space around text blocks and in your design in general can make elements blend into each other. Grids help align your type across the page.\nAlignment: Text can be centered, justified, left-justified, or right-justified. Each has it’s appropriate place in a design. Left-justified is generally best for long blocks of copy. Having a strong left edge gives the eye an easy place to come back to after reaching the end of the line.\nNew paragraphs: Should you indent paragraphs or start a new paragraph with a line space? Either can be fine as long as you’re consistent. The line space is more common online and probably a little easier to read on a monitor. Other ways to indicate new paragraphs include drop caps, ornamentation, and outdenting. Pick one or combine two ways to signify a new paragraph and stick with it.\nMeasure: refers to the length of a line of text.Long lines of text tire the eye and make it hard to find your way back to the next line. Your measure should be 45 – 75 characters long with 66 characters as an often been cited ideal. If your design uses multiple columns of text you probably want to keep your measures shorter (40 – 50 characters).\nLeading: (line-heght in css) is the vertical space between lines of text. It’s the distance between one baseline of text and the next. Common advice says your leading should be at least 25% to 30% larger than your font size, however I find that 50% larger is most readable online. You can use negative leading for shorter phrases and text blocks, but make sure the ascenders and descenders of the type don’t collide.\nKerning: refers to adjusting the space between specific character pairs. Letter combinations where one or more characters have an open space or angle out (T, A for example) require an adjustment (when paired with some letters) to make the space between them appear more uniform. This is usually taken care of in the typeface and not something you would often do manually Monospaced fonts have no kerning.\nTracking: is related to kerning. It’s the overall space between letters instead of specific pairs of letters (letter-spacing in css). Tracking can be used for effect as well as readability. Script typefaces often require a positive tracking as do all caps and small caps.\nCase: Type can be lowercase, uppercase, and mixed case. Lower case (with capitalization for grammar) is the easiest to read in longer blocks of text. Upper case (all caps) is more difficult to read, though it can work fine in short blocks of text.\nFont Style and Font Weight: Roman face fonts are the easiest to read. Italics and bolding can become difficult in long blocks of copy and are best use in small doses.\nColor: Typographic color refers to the space between letters, words, and lines of text as well as the weight of the font. Leading, and tracking among other things help control typographic color and can be used to control the hierarchy of your text. When text gets too dense (has too much color) it becomes harder to read.\nContrast: relates to the actual colors chosen for text and the background on which the text sits. The more contrast between text and background the more readable the text will be. Black text on a white background is the easiest to read. From there every combination makes text less readable.\nNumber of different fonts: Using too many fonts in a single design can be a struggle to adjust to. Consistency helps readability. Rule of thumb advice is to use 2 fonts (3 at most) and to contrast a serif and sans-serif font. Perhaps one for body copy and one for headings.\nTypeface: We started this discussion with the idea that not all type needs to be legible. Your first choice for readability is therefore to choose a legible typeface. Your choice for body copy is going to come down to either a serif or sans-serif typeface.\nSerifs help lead the eye and they’re typically chosen in print. However they can become difficult to read at smaller sizes as the serifs collide, which is why sans-serif fonts have become more common online. Either serif or sans-serif fonts can work well online as long as you do most of the other things in this post right.\nAt smaller font sizes you want to find a typeface with a heavier stroke weight and less contrast between thick and thin parts otherwise strokes can overwhelm the thinner strokes. You’d typically want to choose a font with a wide-set width and a larger x-height. Often you’ll need to add tracking at smaller font sizes.\nI purposely saved font size for last in order to show you how many other things play a role in readability. Some fonts read better when small and others when large. Verdana for example is designed slightly larger than other fonts making it very readable at small sizes and explains why it’s so often used in online body copy.\nThe question you probably most want to know is what size should you use. There isn’t a simple answer. Compare the following two fonts.\nThis is 12px Verdana\nSame 12px, but a different size.\nSomewhere around 10pt or 11pt is ideal for reading in print. Of course there’s no such thing as a point online even if css lets us set fonts in pt sizes. There isn’t an exact conversion between px and pt either, however using the conversion chart at Reed Design we see that 10 to 11pt corresponds to 13 to 15 px, probably a bit larger than what many sites use.\nYou also need to take your audience into account. I can tell you that the older I get, the larger I want to see a font-size set.\n12px is a common setting for online body copy. If you go below this be prepared for people to comment how small your font is. At 14px you’ll probably start getting comments about your large font. 12px to 14 px though is probably a good range to choose for body copy, though again it will depend on other factors.\nFor things like captions you would usually go a little smaller and naturally for headings you would typically go bigger.\nOne last point about font sizes is the idea of scaling, which can be used to develop a hierarchy in your type. The basic idea is to choose font sizes that relate to one another for different elements in your design. I’ll let Mark Boulton, the author of the post behind the previous link, give the details.\nWhile this post has covered quite a bit of information there’s plenty more to know about typography in regards to legibility and readability. The following posts can serve as your next step in acquiring a greater understanding of typography.\n- Web Typography\n- In search of the perfect font\n- Typography Part 3: It’s All About Legibility\n- Typography on the information highway\n- Legibility Guidelines\n- Typography and the Aging Eye: Typeface Legibility for Older Viewers with Vision Problems\nHopefully I’ve convinced you that there’s a lot to designing legible and readable type. My main goal was to get you to see past the idea that it all comes down to your choice in font and font size. Like a lot of people when I see type I can’t read my instinct is to think it simply needs to be bigger. Hopefully now we’re both aware of the many of things that make type more readable.\nI also hope this post has convinced you to spend more time planning out the typography in your next design. Again I think most of us choose a typeface or two, set some sizes and are done with it.\nI’d like to tell you I spent a lot of time with the typography on this site, but I can’t. Face and size were pretty much it, though I think I did ok when it comes to the overall layout making the text easier to read. With the next design of this site I will be paying a lot more attention to typography and again I hope you will too.\nNext time let’s see if we can both take a step in that direction and look at the css properties that deal with typography. We’ll go through them thinking about the terms we’ve discussed here and see if we can set up a basic typographic stylesheet. With a little luck we’ll come up with something we can reuse from design to design.\nDownload a free sample from my book, Design Fundamentals."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:82a8cee5-d0d3-4525-8728-1d83b19e7c9b>","<urn:uuid:61ee5b32-f772-43bf-8564-a9f2f133fdf2>"],"error":null}
{"question":"How do natural satellites like Earth's moon compare to NASA's educational platforms for learning about space?","answer":"While Earth's moon is a natural satellite that orbits our planet, NASA provides interactive educational platforms to learn about such celestial bodies. The Space Place website offers interactive games and activities for upper-elementary children to learn about space science, while natural satellites like the Moon serve as actual objects of study in our solar system. There are 240 known natural moons in the solar system, including Earth's Moon, which is one of the largest at over 3,000 kilometers across. NASA's educational resources help explain these concepts through hands-on activities, games, and videos designed to make space science accessible to young learners.","context":["NASA’s award-winning Space Place website engages upper-elementary-aged children in space and Earth science through interactive games, hands-on activities, fun articles and short videos.\nTo learn more, view the interactive Reach Map of Science Activation activities.\nPaper models of the great space observatories and explorers of the universe. The … Read More play Links out; StarChild. This is NASA's official eclipse Web site. Use the links above to explore the site's topics. This link takes you away from NASA Space Place.\nLaunch SATERN. A learning center for young astronomers. play; Mission to Jupiter! The current count orbiting our star: eight.. Get the latest updates on NASA missions, watch NASA TV live, and learn about our quest to reveal the unknown and benefit all humankind. Links out; Sun for Kids. This is NASA's official eclipse Web site. This link takes you away from NASA Space Place. play; Mission to Jupiter! NASA’s Universe of Learning provides resources and experiences that enable youth, families, and lifelong learners to explore fundamental questions in science, experience how science is done, and discover the universe for themselves. We are in the … Read More January 15, 2020 New releases for the WorldWind Java and Web platforms are ready, however they are being delayed due to new NASA software release processes. Build a model spacecraft to explore the universe!\nThis link takes you away from NASA Space Place. Agency Training Administration. February 26, 2020 Dear WorldWind Community, Here is a quick update on our progress with the latest WorldWind release which is now just a few weeks away. During that time, NASA projects have come and gone and the intended audience has variously changed from high school students to college students and back to middle school students. Space and science activities you can do with NASA at home, including video DIY tutorials for making rockets, Mars rovers and Moon landers out of materials you have lying around – or templates you can print out. Pick any two planets and see their sizes side by side.\nIf you are looking for something a bit more advanced (ages 14+), then head on … This ecosystem of teams and NASA Infrastructure projects meets learners’ needs in a variety of venues throughout their lifelong learning journey. Learn more about carbon's role in climate change on Earth with these features, multimedia, interactives and other resources from NASA. print Links out; What is infrared?\nThere are more planets than stars in our galaxy. view Links out; StarChild. Watch en Español : Seleccione subtítulos en Español bajo el ícono de configuración. Top 10 facts about the Sun and lots more.\nHelp Juno reveal Jupiter's true nature.\nExplore Earth and space with these STEM activities, projects, videos and games for kids and students from NASA-JPL. NASA Official: Benjamin Reist.\nWe send spacecraft to take pictures, gather information, and find out more about them.\nNASA’s Universe of Learning provides resources and experiences that enable youth, families, and lifelong learners to explore fundamental questions in science, experience how science is done, and discover the universe for themselves. The inner, rocky planets are Mercury, Venus, Earth and Mars.The outer planets are gas giants Jupiter and Saturn and ice giants Uranus and Neptune.. The System for Administration, Training, and Educational Resources for NASA (SATERN) is NASA's Learning Management System (LMS) that provides web-based access to training and career development resources. Play and Learn: Climate Kids NASA’s Climate Kids website brings the exciting science of climate change and sustainability to life, providing clear explanations for the big questions in climate science. This link takes you away from NASA Space Place.\nNASA scientists opened an untouched rock and soil sample from the Moon returned to Earth on Apollo 17 for scientific study.\nplay Links out A learning center for young astronomers. Do you want to know more about our solar system's eight planets?\nplay; Compare the sizes of the planets. Your browser does not appear to support HTML5. NASA’s Climate Kids website brings the exciting science of climate change and sustainability to life, providing clear explanations for the big questions in climate science. NASA.gov brings you the latest images, videos and news from America's space agency.\nSee the universe in a whole new light! What is a browser? play; Compare the sizes of the planets.\nplay Links out StarChild is a learning center for young astronomers ages 5-13 to learn about the solar system, the Milky Way galaxy, and the universe beyond. NASA has a portfolio of programs and opportunities dedicated to attracting, engaging and educating students and to support educators and educational institutions across the nation. Click a planet on the sidebar or choose from below to get started.\nThe Training Administration website has a … Help Juno reveal Jupiter's true nature.\nCollared Lizard Range\nIlayaraja Folk Songs List\nHow To Message Someone On Bambino\nSika Deer Delaware\nWolf Spider Florida Babies\nMark Rothko Collection\nChrono Cross Steal List\nBad Emotions Quotes","A natural satellite is an object that orbits a planet or other body larger than itself and which is not man-made. Such objects are often called moons. The term is normally used to identify non-artificial satellites of planets, dwarf planets, or minor planets. There are 240 known moons within the solar system, including 163 orbiting the planets, four orbiting dwarf planets, and dozens more orbiting small solar system bodies.\nThe large gas giants have extensive systems of natural satellites, including half a dozen comparable in size to the Earth's moon. Of the inner planets, Mercury and Venus have no moon at all; Earth has one large moon (the Moon); and Mars has two tiny moons: Phobos and Deimos. Among the dwarf planets, Ceres has no moons (though many objects in the asteroid belt do), Eris has one: Dysnomia, and Pluto has three known satellites: Nix, Hydra, and a large companion called Charon. The Pluto-Charon system is unusual in that the center of mass lies in open space between the two, a characteristic of a double planet system.\nThe orbital properties and compositions of natural satellites provides us with important information on the origin and evolution of the satellite system. Especially a system of natural satellites orbiting around a gas giant can be regarded as a miniature solar system that contains precious clues for studying the formation of solar systems.\nNatural satellites orbiting relatively close to the planet on prograde orbits (regular satellites) are generally believed to have been formed out of the same collapsing region of the protoplanetary disk that gave rise to its primary. In contrast, irregular satellites (generally orbiting on distant, inclined, eccentric and/or retrograde orbits) are thought to be captured asteroids possibly further fragmented by collisions. The Earth-Moon and possibly Pluto-Charon systems are exceptions among large bodies in that they are believed to have originated by the collision of two large proto-planetary objects (see the giant impact hypothesis). The material that would have been placed in orbit around the central body is predicted to have reaccreted to form one or more orbiting moons. As opposed to planetary-sized bodies, asteroid moons are thought to commonly form by this process.\nMost regular natural satellites in the solar system are tidally locked to their primaries, meaning that one side of the moon is always turned toward the planet. Exceptions include Saturn's moon Hyperion, which rotates chaotically because of a variety of external influences.\nIn contrast, the outer moons of the gas giants (irregular satellites) are too far away to become 'locked'. For example, Jupiter's moon Himalia, Saturn's moon Phoebe and Neptune's Nereid have rotation period in the range of ten hours compared with their orbital periods of hundreds of days.\nSatellites of satellites\nNo \"moons of moons\" (natural satellites that orbit the natural satellite of another body) are known. It is uncertain whether such objects can be stable in the long term. In most cases, the tidal effects of their primaries make such a system unstable; the gravity from other nearby objects (most notably the primary) would perturb the orbit of the moon's moon until it broke away or impacted its primary. In theory, a secondary satellite could exist in a primary satellite's Hill sphere, outside of which it would be lost because of the greater gravitational pull of the planet (or other object) that the primary satellite orbits. For example, the Moon orbits the Earth because the Moon is 370,000 km from Earth, well within Earth's Hill sphere, which has a radius of 1.5 million km (0.01 AU or 235 Earth radii). If a Moon-sized object were to orbit the Earth outside its Hill sphere, it would soon be captured by the Sun and become a dwarf planet in a near-Earth orbit.\nTwo moons are known to have small companions at their L4 and L5 Lagrangian points, which are about sixty degrees ahead of and behind the body in its orbit. These companions are called Trojan moons, because their positions are comparable to the positions of the Trojan asteroids relative to Jupiter. Such objects are Telesto and Calypso, which are the leading and following companions respectively of Tethys; and Helene and Polydeuces, which are the leading and following companions of Dione.\nThe discovery of 243 Ida's moon Dactyl in the early 1990s confirms that some asteroids also have moons. Some, like 90 Antiope, are double asteroids with two equal-sized components. The asteroid 87 Sylvia has two moons.\nNatural satellites of the solar system\nThe largest natural satellites in the solar system (those bigger than about 3,000 kilometers across) are Earth's moon, Jupiter's Galilean moons (Io, Europa, Ganymede, and Callisto), Saturn's moon Titan, and Neptune's captured moon Triton. For smaller moons see the articles on the appropriate planet. In addition to the moons of the various planets there are also over 80 known moons of the dwarf planets, asteroids and other small solar system bodies. Some studies estimate that up to 15 percent of all trans-Neptunian objects could have satellites.\nThe following is a comparative table classifying the moons of the solar system by diameter. The column on the right includes some notable planets, dwarf planets, asteroids, and trans-Neptunian objects for comparison.\n|Satellites of planets||Dwarf planet satellites||Satellites of\n|(136472) 2005 FY9\n|Charon||(136108) 2003 EL61\n2 Pallas, 4 Vesta\nmany more TNOs\n|Dysnomia||S/2005 (2003 EL61) 1\nS/2005 (79360) 1\nand many others\n|S/2005 (2003 EL61) 2\nmany more TNOs\nS/2000 (90) 1\nmany more TNOs\nS/2000 (762) 1\nS/2002 (121) 1\nS/2003 (283) 1\nS/2004 (1313) 1\nand many TNOs\n|less than 10||at least 47||at least 21||many||many|\nThe first known natural satellite was the Moon (Luna in Latin). Until the discovery of the Galilean satellites in 1610, however, there was no opportunity for referring to such objects as a class. Galileo chose to refer to his discoveries as Planetæ (\"planets\"), but later discoverers chose other terms to distinguish them from the objects they orbited.\nChristiaan Huygens, the discoverer of Titan, was the first to use the term moon for such objects, calling Titan Luna Saturni or Luna Saturnia—\"Saturn's moon\" or \"The Saturnian moon,\" because it stood in the same relation to Saturn as the Moon did to the Earth.\nAs additional moons of Saturn were discovered, however, this term was abandoned. Giovanni Domenico Cassini sometimes referred to his discoveries as planètes in French, but more often as satellites, using a term derived from the Latin satelles, meaning \"guard,\" \"attendant,\" or \"companion,\" because the satellites accompanied their primary planet in their journey through the heavens.\nThe term satellite thus became the normal one for referring to an object orbiting a planet, as it avoided the ambiguity of \"moon.\" In 1957, however, the launching of the artificial object Sputnik created a need for new terminology. The terms man-made satellite or artificial moon were very quickly abandoned in favor of the simpler satellite, and as a consequence, the term has come to be linked primarily with artificial objects flown in space – including, sometimes, even those which are not in orbit around a planet.\nAs a consequence of this shift in meaning, the term moon, which continued to be used in a generic sense in works of popular science and in fiction, has regained respectability and is now used interchangeably with satellite, even in scientific articles. When it is necessary to avoid both the ambiguity of confusion with the Earth's moon on the one hand, and artificial satellites on the other, the term natural satellite (using \"natural\" in a sense opposed to \"artificial\") is used.\nThe definition of a moon\nThere has been some debate about the precise definition of a moon. This debate has been caused by the presence of orbital systems where the difference in mass between the larger body and its satellite are not as pronounced as in more typical systems. Two examples are the Pluto-Charon system and the Earth-Moon System. The presence of these systems has caused a debate about where to precisely draw the line between a double body system, and a main body-satellite system. The most common definition rests upon whether the barycentre is below the surface of the larger body, though this is unofficial and somewhat arbitrary. At the other end of the spectrum there are many ice/rock clumps that form ring systems around the solar system's gas giants, and there is no set point to define when one of these clumps is large enough to be classified as a moon. The term \"moonlet\" is sometimes used to refer to extremely small objects in orbit around a larger body, but again there is no official definition.\n- ↑ Canup, R. and E. Asphaug (2001). Origin of the Moon in a giant impact near the end of the Earth's formation. Nature 412: 708-712.\n- ↑ Stern, S., H. Weaver, A. Steffl, M. Mutchler, W. Merline, M. Buie, E. Young, L. Young, and J. Spencer (2006). A giant impact origin for Pluto’s small moons and satellite multiplicity in the Kuiper belt. Nature 439: 946-949.\n- ↑ Marchis, F., P. Descamps, D. Hestroffer and J. Berthier (2005). Discovery of the triple asteroidal system 87 Sylvia. Nature 436: 822-824. Retrieved July 2, 2007.\n- ↑ This column lists objects that are moons of small solar system bodies, not small solar system bodies themselves.\n- ↑ Sometimes referred to as \"Luna\".\n- ↑ 6.0 6.1 Diameters of the new Plutonian satellites are still very poorly known, but they are estimated to lie between 44 and 130 km.\n- ↑ (617) Patroclus I Menoetius\n- ↑ (22) Kalliope I Linus\n- ↑ (87) Sylvia I Romulus\n- ↑ (45) Eugenia I Petit-Prince\n- Karttunen, H., et al. (eds.). 2003. Fundamental Astronomy, 4th ed. Helsinki: Springer-Verlag. ISBN 3540001794\n- Bakich, Michael E. 2000. The Cambridge Planetary Handbook. New York: Cambridge University Press. ISBN 0521632803\n- Beatty, J. Kelly, et al. (eds.). 1999. The New Solar System, 4th ed. New York: Cambridge University Press. ISBN 0521645875\nAll links retrieved July 2, 2007.\n- Moons in our Solar System – Windows to the Universe, University Corporation for Atmospheric Research\n- Natural Satellite Physical Parameters – NASA Jet Propulsion Laboratory\n- [http://www.planetary.org/explore/topics/compare_the_planets/moon_numbers.html Moons of the Solar System – The Planetary Society\n- JPL Solar System Dynamics – NASA Jet Propulsion Laboratory\n- USGS Astrogeology: Gazetteer of Planetary Nomenclature – Planetary Body Names and Discoverers\n- “Upper size limit for moons explained” by Kelly Young, NewScientistSpace\n- Asteroids with Satellites by William Robert Johnston\n|Natural satellites of the Solar System|\n|The Sun · Mercury · Venus · Earth · Mars · Ceres · Jupiter · Saturn · Uranus · Neptune · Pluto · Eris|\n|Planets · Dwarf planets · Moons: Terran · Martian · Asteroidal · Jovian · Saturnian · Uranian · Neptunian · Plutonian · Eridian|\n|SSSBs: Meteoroids · Asteroids (Asteroid belt) · Centaurs · TNOs (Kuiper belt/Scattered disc) · Comets (Oort cloud)|\n|See also astronomical objects and the solar system's list of objects, sorted by radius or mass.|\nNew World Encyclopedia writers and editors rewrote and completed the Wikipedia article in accordance with New World Encyclopedia standards. This article abides by terms of the Creative Commons CC-by-sa 3.0 License (CC-by-sa), which may be used and disseminated with proper attribution. Credit is due under the terms of this license that can reference both the New World Encyclopedia contributors and the selfless volunteer contributors of the Wikimedia Foundation. To cite this article click here for a list of acceptable citing formats.The history of earlier contributions by wikipedians is accessible to researchers here:\nNote: Some restrictions may apply to use of individual images which are separately licensed."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"format_constrained"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:5590333f-ade0-46bf-bb47-2085257488ad>","<urn:uuid:23e6318b-c1be-47b1-8389-b7f957c43c4d>"],"error":null}
{"question":"What's the difference between John Williams' film scores in elementary school music curricula versus his overall film composition career?","answer":"In elementary school curricula, John Williams is primarily studied through specific compositions like Liberty Fanfare and selections from Star Wars, Jaws, Superman, Harry Potter, Raiders of the Lost Ark, and E.T. the Extra-Terrestrial, particularly in fourth grade. In his broader career, Williams has composed over 100 film scores, earning 51 Academy Award nominations, with works ranging from large orchestral scores like Star Wars to more intimate pieces like Schindler's List with its Hebraic melodies and violin solos. His versatility spans from heroic orchestral scores to adaptations for different types of films and stories, while maintaining the ability to create standalone concert music.","context":["Music Curriculum by Grade Level\nFirst graders use their bodies as an important way to internalize the musical beat. Playing barred pitched and non-pitched percussion instruments builds on kindergarten skills as students learn variety of Jewish and American folk songs. First graders will:\n- Write and perform individual rhythmic compositions\n- Learn about three instrument families: brass, percussion, and keyboards\n- Respond to music using visual art and movement\nStudents continue to expand their ability to write and perform their own rhythmic compositions. Second graders are also introduced to the woodwinds and strings while also learning about instruments in a marching band. The music of Gioacchino Rossini, a significant operatic composer, is introduced. Second graders will:\n- Compose and play simple rhythms using combinations of quarter notes, two eighth notes, quarter rests and other rhythms in standard meter signatures\n- Identify by sight and sound instruments from the four different families of the orchestra: strings, woodwinds, brass, and percussion\n- Distinguish expressive features: contrast in loud/soft, fast/slow, strong/weak, and smooth/detached\nVocal ranges continue to be expanded by singing in melody and beginning harmony songs. Third graders also study museums that are dedicated to music. Students describe different musical elements by listening to pieces composed by Antonin Dvorak, Gustav Holst, and Camille Saint-Saëns. Collaborative work is emphasized as students compose and perform body percussion and non-pitched percussion pieces. Third graders will:\n- Sing, alone and with others, a varied repertoire of songs from memory\n- Explore expressive musical elements which may create visual imagery, impact our emotions, and affect our interpretation of music\n- Study the Rock Hall of Fame and Museum-Cleveland; Country Music Hall of Fame and Museum-Nashville; Sun Studio-Memphis; Stax Records-Memphis; Motown-Detroit; Experience Music Project-Seattle\nStudents find out how radio shaped musical styles and tastes in the United States. Fourth graders learn how music and language arts are tied together as they listen to a radio dramatization. Student compositions at this grade level become longer and the children learn more advanced playing techniques on the Orff instruments.\nFourth graders will:\n- Sing songs from the 1920s, 30s, 40s, 50s, 60s, and 80s\n- Improvise and play melodies or accompaniments within a given framework of pitch, rhythm, and form\n- Listen and analyze the music of Plink, Plank, Plunk! – Leroy Anderson; Niagara Symphony – William Henry Fry; C Jam Blues – Duke Ellington; Hit the Road Jack – Ray Charles; Liberty Fanfare – John Williams; Production Number from Hollywood Suite– Ferde Grofé; A Tribute to John Williams – Star Wars, Jaws, Superman, Harry Potter, Raiders of the Lost Ark, E.T. the Extra-Terrestial; Beethoven Lives Upstairs\nIn addition to expanding vocal music opportunities, students in fifth grade study Jewish-American composers and musicians. Musicians studied may include Irving Berlin, who wrote a great deal of music for many Broadway classics, and conductors/composers like Leonard Bernstein and Aaron Copland. Students continue to refine their composition skills and use more advance techniques while playing instruments.\nUnits of study include:\n- Vocal Music\n- Music Literacy\n- Playing Instruments: Non-Pitched Percussion Instruments, Pitched Percussion Instruments, Pitched Melodic Instruments (Recorder)\n- Music Listening and History\nStudents in grades two through eight are eligible to be in the choir. Choir rehearsals are held during lunch periods and before school with additional practices for special performances. The choir performs at Holidays Under Glass each December at the IDS Crystal Court in downtown Minneapolis. In addition, the choir sings at a variety of community events in Twin Cities Jewish community, including Empty Bowls, a fundraiser for ending hunger in St. Louis Park and the community-wide Yom Ha'atzmauut (Israeli Independence Day) festivities.\nHeilicher fourth through eighth graders can take band with music school instructors who give individual and ensemble lessons. Instruments offered are woodwinds, brass, and percussion. Students can choose from flute, clarinet, alto saxophone, trumpet, trombone, tuba, and percussion instruments. The band performs several times throughout the school year.","John Williams & American Film Music\nHaving written over 100 film scores, as well as other symphonic and chamber works, John Williams is among the most prolific and celebrated composers of our time. His film scores, particularly those involving large-scale, orchestral forms, have earned him 51 Academy Award nomination and countless other honors. In addition, he has greatly influenced American symphonic repertoire and practices, as his work as both composer and conductor has helped bridge the gap between popular culture and classical music.\n- Music has been part of film since the beginning of cinema in the 1890s. Silent films were accompanied by live performance, and synchronized sound in film emerged in the 1920s. By the 1930s, music was an important part of the Hollywood film industry, with classical Hollywood film music involving large, orchestral scores, typically in the style of 19th century Romanticism.\n- By the time John Williams began working in TV and film in the late 1950s, film music had branched out to include all styles of popular music and modernism.\nWilliams quickly demonstrated versatility as a composer; his first Academy Award nomination was in 1967, and his first win was for his musical adaptation of Fiddler on the Roof (1971). His second Oscar came for Jaws (1975), directed by Steven Spielberg, with whom Williams has collaborated ever since. A departure from trends of the time, Jaws was scored for large orchestra with music structured around the now-iconic, two-note theme that represents the shark.\n- Williams became known for such narrative, large-scale, orchestral film scores that reference the classical Hollywood style. This can be heard in Star Wars (1977) and other films in the series. Often called a “space opera,” the Star Wars films feature recurring musical themes for characters and ideas (leitmotifs), musical effects to complement the action (called “Mickey-Mousing”), and vivid orchestration.\n- He has written several other heroic, orchestral scores (e.g., Superman, Raiders of the Lost Ark, E.T.), yet has a remarkable capacity to adapt his music to different types of films and stories. One such notable achievement is Schindler’s List (1993), for which he won his fifth Oscar and features poignant, Hebraic melodies and violin solos performed by Itzhak Perlman. Also noteworthy are the first three Harry Potter films (2001, 2002, and 2004), which musically transport the audience to a magical and mystical world.\n- Even though some criticize Williams’ music for being rooted in 19th century idioms, a particular hallmark of his film music is how well it can stand on its own as concert music. His compositions as well as work as a conductor (Boston Pops) have brought symphonic music into popular American culture, and also advanced and revitalized symphonic repertoire.\n- What are some of the musical devices that Williams uses to create musical narratives in films?\n- How does Williams adapt his musical style to different types of films and narratives? For example, what are some of the stylistic differences between Star Wars and Schindler’s List?\n- In what ways do the musical themes in the Star Wars movies represent the characters and ideas? How does Williams interweave these themes throughout each film, and across films? How does he do this in other films, such as Jaws or Harry Potter?\n- Even though Williams’ orchestral style is based on 19th century musical idioms, why does his music resonate so well with audiences today?\n- What are some other trends in contemporary film music? Who might follow in Williams’ footsteps?\nMore to Explore\n- Biography of John Williams Click here\nBooks for Further Reading & Listening\n- Audissino, Emilio. John Williams's Film Music: 'Jaws,' 'Star Wars,' 'Raiders of the Lost\nArk,' and the Return of the Classical Hollywood Music Style. University of Wisconsin Press, 2014. 346 pages. While there are many popular articles about John Williams and his music, this is the first (and currently only) scholarly book in English. Audissino provides an overview of film music, particularly the classical Hollywood style of the 1930s-40s, and discusses Williams’s tremendous contribution to film music from that perspective. He provides insight on Williams as a composer with in-depth discussion of some of his most successful film scores, and examines his music’s wide reaching influence.\nClick here to order\n- Kalinak, Kathryn. Film Music: A Very Short Introduction. Oxford University\nPress, 2010. 143 pages. As the title suggest, this is a brief overview of film music – it’s history from cinema’s beginning, how music functions in film, and different musical approaches and styles across the world of film. While not specifically detailing John Williams’s music, Kalinak’s general discussion puts his music into this larger perspective.\nClick here to order"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"search_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:0bc1cd2b-3cf0-4db2-8a46-76b71aac6919>","<urn:uuid:bbed682e-50ce-49c8-85a5-a8508e1e4439>"],"error":null}
{"question":"How do the peer review and editorial processes differ between Comptes Rendus Biologies and Archives of Natural History?","answer":"Comptes Rendus Biologies employs a rigorous peer review committee of eminent specialists in the subject matter of submitted articles. In contrast, Archives of Natural History requires papers to be submitted via email to the Honorary Editor, who works with Associate Editors and suggests that authors provide names of two potential referees. Archives of Natural History specifies that all submitted papers must be original and undergo peer review before acceptance, with a detailed peer-review process explained on their website. The journal also requires strict adherence to style and format requirements, with papers potentially being returned to authors for modification if they don't meet these standards.","context":["Comptes Rendus Biologies (CR BIOL)\nArticles in French and English, Abstracts in both Languages. Abridged Version in the other Language. Former title: Comptes Rendus de l'Académie des Sciences - Series III - Life Sciences. Les Comptes Rendus Biologies couvrent l'ensemble des domaines des sciences de la vie, présentés notamment au travers des rubriques suivantes: Approches théoriques et modélisation / Biologie et génétique moléculaires / Génétique / Biologie du développement et de la reproduction / Biologie cellulaire / Biochimie / Neurosciences / Physiologie / Biologie et pathologie animales / Biologie et pathologie végétales / Immunologie / Microbiologie : bactériologie, mycologie, parasitologie, virologie / Pharmacologie, toxicologie / Sciences médicales / Biophysique / Biologie des populations / Epidémiologie / Ecologie / Evolution / Taxinomie / Agronomie / Anthropologie. Les articles sont proposés sous la forme de notes originales relatant brièvement une découverte importante. Pour toute validation, un comité de lecture rigoureux s'assure de l'arbitrage d'éminents spécialistes sur le sujet de l'article présenté. Les articles sont rédigés en langue française ou anglaise et font obligatoirement l'objet d'une version abrégée d'une page au moins dans l'autre langue. Les textes proviennent de l'entière communauté internationale et la diffusion s'élargit régulièrement à travers le monde, avec un nombre croissant de lecteurs et d'auteurs, surtout dans les pays dans lesquels la contribution aux avancées scientifiques est particulièrement active.\n- Impact factor1.53Show impact factor historyHide impact factor history\n- WebsiteComptes Rendus Biologies website\nOther titlesComptes rendus., Biologies, C.R. biologies\nMaterial typeDocument, Periodical, Internet resource\nDocument typeInternet Resource, Computer File, Journal / Magazine / Newspaper\n- Author can archive a pre-print version\n- Author can archive a post-print version\n- Voluntary deposit by author of pre-print allowed on Institutions open scholarly website and pre-print servers\n- Voluntary deposit by author of authors post-print allowed on institutions open scholarly website including Institutional Repository\n- Deposit due to Funding Body, Institutional and Governmental mandate only allowed where separate agreement between repository and publisher exists\n- Set statement to accompany deposit\n- Published source must be acknowledged\n- Must link to journal home page or articles' DOI\n- Publisher's version/PDF cannot be used\n- Articles in some journals can be made Open Access on payment of additional charge\n- NIH Authors articles will be submitted to PMC after 12 months\n- Authors who are required to deposit in subject repositories may also use Sponsorship Option\n- Pre-print can not be deposited for The Lancet\nPublications in this journal\nArticle: Sexual size dimorphism in the ontogeny of the solitary predatory wasp Symmorphus allobrogus (Hymenoptera: Vespidae)[show abstract] [hide abstract]\nABSTRACT: Sex-specific patterns of individual growth, resulting in sexual size dimorphism (SSD), are a little studied aspect of the ontogeny related to the evolutionary history and affected by the ecology of a species. We used empirical data on the development of the predatory wasp Symmorphus allobrogus (Hymenoptera, Vespidae) to test the hypotheses that sexual differences of growth resulting in the female-biased SSD embrace the difference in (1) the egg size and the starting size of larva, (2) the larval development duration, and (3) the larval growth rate. We found that eggs developing into males and females have significant differences in size. There was no significant difference between the sexes in the duration of larval development. The relative growth rate and the food assimilation efficiency of female larvae were significantly higher than compared to those of male larvae. Thus, the SSD of S. allobrogus is mediated mainly by sexual differences in egg size and larval growth rate.Comptes Rendus Biologies 04/2013; 336(2):57-64.\nComptes Rendus Biologies 01/2013; 336:1-12.\nArticle: Effort d’échantillonnage et atlas floristiques – exhaustivité des mailles et caractérisation des lacunes dans la connaissance[show abstract] [hide abstract]\nABSTRACT: Résumé Les atlas floristiques sont des outils centraux pour les politiques de conservation de la flore même si l’effort d’échantillonnage est rarement homogène sur les territoires concernés. Ce travail vise à développer une méthode pour estimer le taux d’exhaustivité des unités géographiques d’échantillonnage d’atlas floristiques. Il propose également un outil d’aide à la prospection pour parer à l’hétérogénéité de l’effort d’échantillonnage et optimiser l’efficacité de nouveaux inventaires. Une synthèse bibliographique couplée à des tests sur un jeu de données a abouti au choix d’un estimateur non paramétrique, le Jackknife 1, pour estimer la richesse réelle des unités géographiques d’échantillonnage. Le nombre de données de chaque unité géographique d’échantillonnage est utilisé comme estimateur de l’effort d’échantillonnage. Le rapport entre la richesse observée et cette richesse estimée donne le taux d’exhaustivité de l’inventaire de chaque unité géographique d’échantillonnage. Dix-huit variables ont ensuite été sélectionnées pour décrire les inventaires et orienter les futures prospections ; ces variables renseignent les localisations, les périodes et les espèces à privilégier pour de futurs inventaires. Abstract Floristic atlases have an important input to flora conservation planning even though their data quality varied greatly across countries. This study aimed to assess survey completeness of cells of floristic atlases. Then, a surveying guide is designed to overcome as efficiently as possible sampling biases. A review and analyses on a wide dataset were carried out to select an estimator of the true species richness of surveyed cells. The Jackknife 1, a non-parametric estimator, appeared as the best compromise for regional floristic atlases. The number of records in each cell was used as an estimator of sampling effort. The ratio between the observed species richness and the estimated species richness measures the completeness of inventories in each surveyed cell. Eighteen variables were selected to describe current inventories and design new surveys. These variables highlight locations, periods and species to be given priority in future studies.Comptes Rendus Biologies 12/2012; 335(12):753-763.\n[show abstract] [hide abstract]\nABSTRACT: The field of epigenetics is young and quickly expanding. During the last year alone, thousands of research articles considered epigenetic mechanisms and their phenotypic consequences in different animal and plant species. Various definitions have been given, though, as to what precisely is epigenetics. Recent ones take into consideration that chromatin at genes and chromosomal regions can be structurally organised by covalent modifications and nuclear proteins, and via RNA molecules, in order to achieve defined expression states that can be perpetuated. Such somatically and meiotically heritable effects on gene function have diverse biological and medical implications. In particular, they are known to be important in development. A recent discussion meeting in Paris at the French Academy of Sciences reviewed our current understanding of 'Epigenetics and Cellular Memory' and where this novel discipline in life sciences is heading.Comptes Rendus Biologies 12/2008; 331(11):837-43.\nArticle: Influence of biological, environmental and technical factors on phenolic content and antioxidant activities of Tunisian halophytes.[show abstract] [hide abstract]\nABSTRACT: Halophyte ability to withstand salt-triggered oxidative stress is governed by multiple biochemical mechanisms that facilitate retention and/or acquisition of water, protect chloroplast functioning, and maintain ion homeostasis. Most essential traits include the synthesis of osmolytes, specific proteins, and antioxidant molecules. This might explain the utilization of some halophytes as traditional medicinal and dietary plants. The present study aimed at assessing the phenolic content and antioxidant activities of some Tunisian halophytes (Cakile maritima, Limoniastrum monopetalum, Mesembryanthemum crystallinum, M. edule, Salsola kali, and Tamarix gallica), depending on biological (species, organ and developmental stage), environmental, and technical (extraction solvent) factors. The total polyphenol contents and antioxidant activities (DPPH and superoxide radicals scavenging activities, and iron chelating and reducing powers) were strongly affected by the above-cited factors. Such variability might be of great importance in terms of valorising these halophytes as a source of naturally secondary metabolites, and the methods for phenolic and antioxidant production.Comptes Rendus Biologies 12/2008; 331(11):865-73.\n[show abstract] [hide abstract]\nABSTRACT: Various methods of suberin extraction have been used in order to identify monomers of this complex polymer. Pre-extraction of waxes has allowed us to identify for the first time 3-friedelanol as a terpen from cork. Moreover, the wax chemical composition found here varied from previous results since cerin was not identified while friedelin and betulin were. Three fractions were obtained: a polymeric, a monomeric and a low molecular weight fraction, the last of which has never before been described. 2,6-heptanediol was found to be the main compound of this fraction. Furthermore, depolymerisation at room temperature gives the same yields as those obtained at reflux, defining an easier and cheaper methodology.Comptes Rendus Biologies 12/2008; 331(11):853-8.\nArticle: [Help yourself, and heaven will help you. Initiatives that society can give to allow aged people to remain included].Comptes Rendus Biologies 12/2008; 331(11):878-80.\nArticle: Observations on the life history of Chaerilus philippinus Lourenço & Ythier, 2008 (Scorpiones, Chaerilidae) from the Philippines.[show abstract] [hide abstract]\nABSTRACT: Biological observations on Chaerilus philippinus were based on specimens from the region of Appari, North of Luzon in the Philippines. The total duration of embryonic development was estimated as being between 110 to 136 days, while the moults between successive juvenile instars and adulthood took place at ages that averaged 7, 39, 73, 190 and 327 days. These developmental periods are shorter and different from those previously observed among species of non-buthid scorpions. They prove to be rather similar to those observed in buthid scorpions, however. Morphometric growth values of the different instars are similar or smaller than those of other species of scorpions that have been studied. Aspects of maternal care and social behaviour are also commented.Comptes Rendus Biologies 12/2008; 331(11):896-900.\nArticle: [The phytoclimates of France: probabilistic classification of 1874 bio-indicators of the climate].[show abstract] [hide abstract]\nABSTRACT: This article presents a synthesis of the relationships between plants and climates at the scale of France, based on a probabilistic classification of 1874 bio-indicators. This classification defines plants groups that indicate the climate, named phytoclimates, expressing the climatic gradients in France. This classification shows 210 phytoclimatic groups distributed into ten cluster levels. The analysis of the various hierarchical levels shows two main phytoclimates testifying the importance of the marine masses and the altitude. The analysis of the third hierarchical level underlines particular phytoclimates which would not be easily recognizable by only analysing the overlapping of floristic and climatic territories. This classification allows one to select taxa that are indicators of the climate. The distribution monitoring or modeling of these taxa should show the effects of the global change on the ecosystems.Comptes Rendus Biologies 12/2008; 331(11):881-95.\nArticle: Lead accumulation in the roots of grass pea (Lathyrus sativus L.): a novel plant for phytoremediation systems?[show abstract] [hide abstract]\nABSTRACT: Eleven day-old grass pea plants (Lathyrus sativus L.) were grown hydroponically for 96 h in the presence of 0.5 mM lead nitrate (Pb(NO(3))(2)). The survival rate was 100%. The mean lead content (measured by ICP-OES) in root tissues was 153 mg Pb g(-1) dry matter. Over three quarters of the lead was not labile. Compared with control plants, lead-exposed plants showed a six-fold, two-fold and three and a half-fold reduction in their root calcium, zinc and copper contents, respectively. Together, these results suggested that Lathyrus sativus L. was tolerant to a deficiency in essential nutrients and able to store large amounts of lead in its root tissues. Therefore, it could be used for the development of new rhizofiltration systems.Comptes Rendus Biologies 12/2008; 331(11):859-64.\n[show abstract] [hide abstract]\nABSTRACT: The aim of this work is to develop and study a fully continuous individual-based model (IBM) for cancer tumor invasion into a spatial environment of surrounding tissue. The IBM improves previous spatially discrete models, because it is continuous in all variables (including spatial variables), and thus not constrained to lattice frameworks. The IBM includes four types of individual elements: tumor cells, extracellular macromolecules (MM), a matrix degradative enzyme (MDE), and oxygen. The algorithm underlying the IBM is based on the dynamic interaction of these four elements in the spatial environment, with special consideration of mutation phenotypes. A set of stochastic differential equations is formulated to describe the evolution of the IBM in an equivalent way. The IBM is scaled up to a system of partial differential equations (PDE) representing the limiting behavior of the IBM as the number of cells and molecules approaches infinity. Both models (IBM and PDE) are numerically simulated with two kinds of initial conditions: homogeneous MM distribution and heterogeneous MM distribution. With both kinds of initial MM distributions spatial fingering patterns appear in the tumor growth. The output of both simulations is quite similar.Comptes Rendus Biologies 12/2008; 331(11):823-36.\n[show abstract] [hide abstract]\nABSTRACT: The seed constitutes the main vector of plant propagation and it is a critical development stage with many specificities. Seed longevity is a major challenge for the conservation of plant biodiversity and for crop success. Seeds possess a wide range of systems (protection, detoxification, repair) allowing them to survive in the dry state and to preserve a high germination ability. Therefore, the seed system provides an appropriate model to study longevity and aging.Comptes Rendus Biologies 11/2008; 331(10):796-805.\n[show abstract] [hide abstract]\nABSTRACT: Many organisms among the different kingdoms store reserve lipids in discrete subcellular organelles called lipid bodies. In plants, lipid bodies can be found in seeds but also in fruits (olives, ...), and in leaves (plastoglobules). These organelles protect plant lipid reserves against oxidation and hydrolysis until seed germination and seedling establishment. They can be stabilized by specific structural proteins, namely the oleosins and caleosins, which act as natural emulsifiers. Considering the putative role of some of them in controlling the size of lipid bodies, these proteins may constitute important targets for seed improvement both in term of oil seed yield and optimization of technological processes for extraction of oil and storage proteins. We present here an overview of the data on the structure of these proteins, which are scarce, and sometimes contradictory and on their functional roles.Comptes Rendus Biologies 11/2008; 331(10):746-54.\n[show abstract] [hide abstract]\nABSTRACT: In legume plants, the determination of individual seed weight is a complex phenomenon that depends on two main factors. The first one corresponds to the number of cotyledon cells, which determines the potential seed weight as the cotyledon cell number is related to seed growth rate during seed filling. Since cell divisions take place between flowering and the beginning of seed filling, any stress occurring before the beginning of seed filling can affect individual seed growth rate (C and N reserve accumulation in seeds), and thus individual seed weights. The second factor concerns carbon and nitrogen supply to the growing seed to support reserve accumulation. Grain legume species produce protein-rich seeds involving high requirement of nitrogen. Since seed growth rate as determined by cotyledon cell number is hardly affected by photoassimilate availability during the filling period, a reduction of photosynthetic activity caused by nitrogen remobilization in leaves (e.g., remobilization of essential proteins involved in photosynthesis) can lead to shorten the duration of the filling period, and by that can provoke a limitation of individual seed weights. Accordingly, any biotic or abiotic stress during seed filling causing a decrease in photosynthetic activity should lead to a reduction of the duration of seed filling.Comptes Rendus Biologies 11/2008; 331(10):780-7.\nData provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable.\nISSN: 2156-4574, Impact factor: 0.71\nISSN: 1936-4776, Impact factor: 1.96\nPublic Library of Science, Public...\nISSN: 1932-6203, Impact factor: 4.09\nISSN: 1768-3238, Impact factor: 1.71\nRoyal Society (Great Britain)\nISSN: 1744-957X, Impact factor: 3.76\nDeutsche Zoologische Gesellschaft\nISSN: 1742-9994, Impact factor: 4.46\nInstituto Internacional de Ecologia...\nConsejo Superior de Investigaciones...\nISSN: 1600-0633, Impact factor: 1.57","Archives of Natural History\nArchives of Natural History publishes peer-reviewed papers on the history and bibliography of natural history in its broadest sense, and in all periods and all cultures.\nThis is taken to include botany, general biology, geology, palaeontology and zoology, the lives of naturalists, their publications, correspondence and collections, and the institutions and societies to which they belong. Bibliographical papers concerned with the study of rare books, manuscripts and illustrative material, and analytical and enumerative bibliographies are also published.\nProduced twice a year, ANH contains refereed, illustrated papers and book reviews and is published for the Society by Edinburgh University Press. The current issue of ANH is ANH 46, issue 1 (2019). The next issue is due in October 2019.\nArchives of Natural History is freely available to members of SHNH – Society for the History of Natural History (print and online). Members have full online access to all previous issues dating back to 1936. To learn more about membership of the Society please visit our membership page. Individuals who are not members of the Society may access articles from Archives of Natural History on payment of an access fee.\nSubscriptions to Archives of Natural History are welcomed from libraries and institutions. For more information, please visit the Subscriptions page on the EUP website.\nA cumulative index is available of the contents for the Journal of the Society for the Bibliography of Natural History and Archives of Natural History, volumes 1- 38.\nANH Editorial team\nHonorary Editor: Mr Herman Reichenbach (email@example.com)\nAssociate Editors: Dr Isabelle Charmantier, Dr Arthur MacGregor and Dr Charles Nelson\nBook Reviews Editor: Ms Maggie Reilly (firstname.lastname@example.org)\nPostal address for Book Reviews: Ms Maggie Reilly, Zoology Curator, The Hunterian (Zoology), Graham Kerr Building, University of Glasgow, Glasgow G12 8QQ, UK\nThe Honorary Editor welcomes the submission of articles to Archives of Natural History on the history and bibliography of natural history in its broadest sense, and in all periods and all cultures. Full instructions for submitting an article can be found here on the Edinburgh University Press ANH submission page.\nAll papers submitted must be original and will be subject to peer review before acceptance for publication. The names of two suggested referees should be added to the submission. The peer-review process for Archives of Natural History is explained here.\nThe Honorary Editor can advise authors about the suitability and format of papers. Papers must be submitted via email to the editor using the following email address: email@example.com. Detailed instructions on preparing a paper for email submission can be downloaded here – Instructions for authors.\nAny paper that does not follow the style and format required for Archives of Natural History, as detailed in these instructions for authors may be returned to the author(s) for modification before being considered for possible publication.\nArchives of Natural History online\nVolumes 1 – 45 published since 1936 are available online free of charge to members of the Society who have renewed their subscription for 2019. Individuals who are not members of the Society may access articles from Archives of Natural History on payment of an access fee. Book Reviews are freely available."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:50c6da9a-1964-4c81-a60e-88df367b854c>","<urn:uuid:922578bc-cc41-43d5-a7ef-b40048add8c0>"],"error":null}
{"question":"How do the soil conditions in Washington State vineyards compare to those affected by the 1860s phylloxera crisis in France?","answer":"In Washington State, vineyards benefit from well-drained sandy loam soils, and phylloxera is not currently a problem due to cold winters and sandy soils that slow its spread. In contrast, French vineyards were devastated by phylloxera in the 1860s, leading to widespread vine death and economic collapse. As a result, nearly all French wines today come from vines grafted onto American rootstocks, while Washington vines can be planted on their own roots.","context":["Did you know that Argentine malbec is one of the last truly “French” wines?\nMost European wines today, even those famous French ones, come from vines grafted onto American rootstocks. The practice dates back to a plague that changed the world of wine forever.\nMore on that in a moment. But first... Are “older” vines really better?\nThis week, we revisit Julien Miquel’s lesson on just how much vines change as they age… including the trap first-time winemakers often fall for… the potential payoff of waiting for the vines to grow old… and why many wineries replant before that payoff comes…\nBorn in the Plague of ‘65 (Continued)\nHow Phylloxera Destroyed European Wine\nHistorians guess that the bug probably entered France, brought over unknowingly on boats from the new world, sometime around 1863.\nIt would not be discovered until the end of the decade. By that time, French wine, as it had been known for hundreds of years, was mostly extinct. Thousands of acres of vines had died, leaving vintners to puzzle and mourn over yellowed leaves and black, rotting roots.\nPhylloxera is a yellowish aphid-like bug that uses a long, thin proboscis to suck sap out of vine roots, not unlike a mosquito on your arm. It is not the feeding that damages the vine, but the chemicals excreted from the bug in the process, which inhibit the vine from healing itself. Multiply that by several thousand microscopic phylloxera on a single rootstock and death is certain.\nIt takes time for the vine to wilt and die, by which time the bugs are long gone. In the early years of the European plague, scientists could not agree on whether the cause was external (a bug or fungus) or due to some inherited flaw (in 1866, Mendel had created the modern science of genetics by publishing his experiments on inherited traits in plants).\nThe epidemic spreads\nEven when phylloxera finally emerged as the culprit, vintners had no easy cure, which led to a series of desperate efforts including flooding vineyards with water (drowning their vines in the process), and transplanting their vines in plots by the sea (where several vineyards were lost to high tides). An impotent hysteria took hold and entire communities coated their vines in toxic carbon disulfide.\nIn France’s wine growing regions, the economy collapsed. Families were unable to pay their bills. Businesses failed. Train lines shut down.\nHow American Rootstock Saved the Day\nSalvation began in secret, a tip passed on strictement entre nous from one vigneron to the next... “le fléau ne touche pas aux Américains.” Rumor had it some French vintners were having success grafting their old French vines onto American rootstocks. The French are proud, but not proud enough to go broke.\nToday, according to journalist Levi Gadye, “nearly all French wine, including expensive French wine, comes from vines grafted onto American rootstocks.”\nTo wit, the vines producing today’s vintages are not, genetically speaking, the same kinds of vines that produced famous vintages like the 1846 Bourgogne, 1865 Montrachet, and 1870 Lafite Rothschild (look ‘em up)... the vintages that made France’s reputation.\nThose original vines, which include the pre-phylloxera variety of malbec, are known as France’s lost grapes.\nToday, they only persist in just a few places on Earth where phylloxera never managed to take hold.\nArgentina’s Calchaquí Valley is one of those places.\nAt the Bonner vineyard at Gualfín, those old pre-phylloxera malbec vines remain still, the last vestige of what was once the pride of France.\nUntil next time,\nThe Wine Explorer","American Viticulture Areas\nAmerican Viticultural Areas, or AVAs, are geographical wine grape growing regions in the United States. Their boundaries are defined by the Alcohol and Tobacco Tax and Trade Bureau (TTB) and established at the request of wineries or other petitioners. Washington State currently has 13 AVAs.\nWashington’s premium wine industry began in the 1960s. The majority of the state’s wine grapes are planted east of the Cascade Range in the Columbia Valley appellation, which encompasses the Yakima Valley and Walla Walla appellations. The climate and soils produce grapes with intense fruit flavors and high natural acidity.\nWashington's vineyards straddle the 46th and 47th parallels, at approximately the same latitude as Bordeaux and Burgundy.\nBecause of its northerly location, Washington receives up to two more hours of sunlight per day during the growing season than California's North Coast. More sun means more flavor development in the grapes.\nIt can pour in Seattle, which has an average rainfall of 46“ per year, but east of the Cascades annual rainfall averages less than 10 inches. The Cascade Range creates a rain shadow that protects Eastern Washington from Pacific storms and allows for warm, dry days during the growing season. Low precipitation and low humidity minimize rot, mildew, disease and pest problems in the vineyards.\nGrowers control the amount of moisture the vines receive during the growing season. This provides for better canopy management and controls berry size to produce concentrated flavorful grapes. Growers irrigates only when necessary to dial in and concentrate the flavor balance in the grapes. The Columbia, Yakima and Snake rivers provide plenty of water via an extensive aqueduct system.\nDaily temperatures can fluctuate as much as 40-50 degrees during the growing season. This swing allows the retention of the grape's natural acidity and fresh fruit flavors which produce lively and fresh wines. Chilly nights (40-45 degrees F) lock in the acids and flavors; warm (but not-too-hot) days (85-90 degrees F) ensure that the grapes ripen slowly without excessive sugar development.\nThe Columbia Valley's cold winters force grapevines into dormancy. Once or twice a decade, sub-zero temperatures can damage some vines in the coolest parts of the valley. However, careful and on-going matching of grape varieties to vineyard sites lessens the impact. Vines are planted on their own roots rather than on rootstock, so in the event of severe winter damage, the vine can be trained up from the root system and produce a crop the next year. At this time the root louse, Phylloxera is not a problem in Washington, probably because the cold winters and sandy soils slow its spread.\nAbout 15,000 years ago a series of cataclysmic geologic flooding events occurred that allowed the formation of present day soils in eastern Washington. The floods converged on the Pasco Basin and were slowed by constriction of the Wallula Gap before draining into the Columbia River. The constriction caused back flooding of local rivers and valley basins and deposited fine grain slack water sediments (silt/sand) over the surrounding area. The floods deposited immense gravel bars and ice-rafted huge granite boulders (erratics) to higher elevations.\nThe resultant slack water deposited sediments and the subsequent wind-blown loess sediments make up the majority of the present day soils and are the backbone of agriculture in all of eastern Washington. These sandy loam soils are very well drained—and a perfect medium for grapevines.\nThe Columbia Valley covers 18,000 square miles and provides a huge range of geographical and climatic conditions for grape growing. This diversity creates distinctive fruit characteristics from individual sites and offers a myriad of stylistic options to winemakers.\nBecause of the diverse growing conditions in Eastern Washington, a large number of grape varieties do well here. When planted in the right locations, Chardonnay, Sauvignon Blanc, Riesling, Gewurztraminer, Semillon, Pinot Grigio, Viognier, Cabernet Sauvignon, Merlot, Syrah, Malbec, Petit Verdot, Barbera, and many others thrive."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"content_constrained"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:e2502817-2faa-4209-ab85-493be3cac905>","<urn:uuid:4a12ff9c-8fc4-4e61-a745-152271d452e3>"],"error":null}
{"question":"Looking for synthesis types in SynthMaster - what methods are available?","answer":"SynthMaster includes multiple synthesis methods: VA (Virtual Analog), Additive, Wavetable, Phase Modulation, Frequency Modulation, Pulse Width Modulation, Ring Modulation, Amplitude Modulation, Physical Modeling, and SFZ Sample Playback synthesis. The synthesizer features multi-algorithm oscillators, analog modeled zero delay feedback filters, flexible effects routing with 11 types of high quality effects, and a massive modulation architecture with 48 separate modulation sources and hundreds of modulation targets.","context":["KV331 Audio SynthMaster || Your Digital Software Registration Code and Instructions Will Be Sent to You, Along With an URL Connecting You Directly to The Manufacturer, Who Will Provide You With Your Software Digitally. Please Be Aware That Software Is Non-Cancelable and Non-Returnable. If you have any questions about this product please do not hesitate to contact us. Guildwater Gear is an Authorized KV331 Audio Dealer through the Xchange Market. There, You Can Find Us Among Their List of Authorized Resellers.\nSynthMaster is an 'all-around' semi-modular software synthesizer and effect plug-in that features many different synthesis methods including VA, Additive, Wavetable, Phase Modulation, Frequency Modulation, Pulse Width Modulation, Ring Modulation, Amplitude Modulation, Physical Modeling and SFZ Sample Playback synthesis. With its multi-algorithm oscillators, analog modeled zero delay feedback filters, flexible effects routing with 11 types of high quality effects and a massive modulation architecture with 48 separate modulation sources and hundreds of modulation targets; SynthMaster is a 'must-have' for all synthesizer enthusiasts!\n- Cross-platform VST, AU and AAX: SynthMaster runs as a VST, AAX instrument on both Windows and Mac OSX, and also as an Audio Unit instrument on Mac OSX.\n- 1800 Factory Presets: SynthMaster comes with 1800 factory presets from a world class team of sound designers: Arksun, Aelyx Design, Aiyn Zahev, Bluffmunkey, BigTone, Frank 'Xenox' Neumann, Gercek Dorman, Michael Kastrup, Nori Ubukata, Rob Lee, Umit 'Insigna' Uy, Ufuk Kevser, Teoman Pasinlioglu, Vandalism, Vorpal Sound and Brian 'Xenos' Lee.\n- Semi-Modular Architecture: For each SynthMaster instance, there are 2 layers followed by 2 global effect send busses. Each layer has its own: Arpeggiator, 2 Oscillators, 4 Modulators, 2 Filters, 4 ADSR Envelopes, 2 Multistage Envelopes, 2 2D Envelopes, 2 LFOs and 4 Keyscalers. The modulators can modulate frequency, phase, amplitude or pulse width of the oscillators or any other modulators at audio rate, or they can be used as regular oscillators.\n- Massive Modulation Architecture: SynthMaster has more hundreds of modulation targets and 48 modulation sources including ADSR Envelopes, 2D Envelopes, Multistage Envelopes, LFOs, KeyScalers, Easy Parameters, Vocoder Bands, MIDI Velocity, Aftertouch, Pitch Bend and MIDI CC. The modulation matrix, which has 64 available slots, has visual filtering as well so that targets for a specific source, or sources for a specific target can be filtered and shown on the user interface.\n- Easy Parameters: SynthMaster features 8 easy knobs and 2 XY pads that can be freely assigned as modulation sources, so that the complexity of the synth engine could be hidden away and only most important parameters of a preset can be controlled by the user. The easy parameters can be globally linked to MIDI controllers, and they can be assigned automatically by SynthMaster as well.\n- Powerful Arpeggiator: The arpeggiator in SynthMaster features classic arpeggiator modes such as Up, Down, UpDown, DownUp, UpDown2, DownUp2, AsPlayed as well as Sequence, Chord and Arpeggiate modes. Each of the 32 steps of the arpeggiator has its own Velocity, Note Number, Note Length, Slide and Hold parameters.\n- Basic Oscillators: 'Basic' oscillators in SynthMaster are capable of synthesizing many different types of waveforms: Sine, Square, Triangle, Sawtooth, Pulse, Noise, single cycle waveforms and multisampled WAV/AIFFs defined in SFZ files. Each oscillator comes with 17 different algorithms in the following categories: Spectral (LP, HP, LS, HS, BP, BS), Bend (Bend+, Bend-, Bend+/-), Sync (Rect Window, Half Cos Window, Cos Window, Tri Window, Saw Window), Pulse (Pulse1, Pulse2) and Quantize.\n- Additive Oscillators: An 'Additive' oscillator is actualy 8 'basic' oscillators running together. Each 'basic' oscillator has its own detune, tone, phase/pulse width/algorithm parameter, frequency, waveform type and algorithm parameters.\n- Vector Oscillators: A 'Vector' oscillator consists of 4 'basic' oscillators, mixed at different ratios. The mix ratios are determined by 2 orthogonal parameters in 2 dimensions: 'X Index' and 'Y index'.\n- Wavetable Oscillators: A 'Wavetable' oscillator is similar to basic oscillator, except that the waveform can be scanned (interpolated) through up to 256 different waveforms shapes. The position of the waveform can be adjusted using the 'wave index' parameter. Starting with version 2.9, SynthMaster now supports loading wavetables from wave files!\n- Stereo Oscillators with Unison/Voice Stacking: Oscillators in SynthMaster have stereo output. Using the \"voices\", \"voices mix\", \"detune curve\", \"detune spread\", \"pan stread\", \"tone spread\" and \"phase spread\" parameters, each basic/wavetable oscillator can generate a rich \"supersaw\" type sound.\n- Zero Delay Feedback Filters: All of the 4 new filter categories in SynthMaster are developed using the zero delay feedback filter technology. With advanced filter parameters like input gain, drive and acid, you can get that \"analog\" sound from the filters!\n- VAnalog Filters: The 'VAnalog' filters are modelled after the famous ladder filter, so they self oscillate when the filter resonance is maxed out. They have continuosly variable slope, which is unique to SynthMaster. They have 3 different CPU settings: \"Basic\", \"Normal\" and \"High\". The Basic setting sounds similar to the other algorithms in most cases and consumes at least 50% less CPU.\n- Multimode Filters: With the new 'multimode' filter type, it is possible to switch from Lowpass to Bandpass to Highpass filter types continuously. For analog multimode filters, it is also possible to change the slope of the filter continously from 0 db/oct to 24 db/oct.\n- Dual Filters: With the new 'dual' filter type, two multimode filters can be run simultaneously, either in parallel, or in series. The mix ratios between the filters, and the topology (parallel/series) between them can be changed continuosly, as well as the modes and cutoff frequencies of the filters.\n- Comb Filters: Comb filters are digital filters used in physical modeling synthesis.\n- Before/Inside/After Filter Distortion: A distortion stage can be inserted before, after, or even inside the filters. For analog filters, the distortion is applied for each of the 4 filter stages in 'inside' mode.\n- Rich Set of Effects: SynthMaster features 11 different effect types: Distortion, LoFi, Ensemble, Phaser, 6 Band EQ, Compressor, Vocoder, Delay, Chorus, Tremolo, Reverb.\n- Flexible Effects Routing: Each of the 11 effect types can be inserted on any layer insert or on any of the 2 global effect bus inserts.\n- Microtuning: SynthMaster supports Scala tuning, so tuning can be set either for each preset or globally by loading from a Scala tuning file.\n- Preset Browser: SynthMaster features a comprehensive preset browser with separate search criterias for instrument type, preset attributes, music style or preset author.\n- Online Preset Browser: Registered users can upload their presets to the online preset library, or browse and download presets created by other registered users; using the preset browser right inside the plug-in window.\n- Multiple Skins: SynthMaster comes with 3 different skins in different color variations. Using the included user interface editor, users can not only customize the existing skins but also can create their own custom interfaces as well.\n- Importing MIDI patterns as Arpeggiator Sequence: Monophonic or even polyphonic (chord) MIDI patterns can be imported into the arpeggiators in SynthMaster by just drag and drop of the MIDI file onto the arpeggiator view on the plugin window.\n- Importing WAV/AIFF Multisamples as SFZ definitions: Multisamples in WAV/AIFF formats could be imported into SynthMaster as SFZ definitions, by simply drag and drop of the WAV/AIFF files onto the oscillator waveform view on the plugin window."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:a33928ec-05d3-4c5d-88b5-d3d2fb7b9293>"],"error":null}
{"question":"I'm worried about my garden's sandy soil. What are the main disadvantages of having soil with low Cation Exchange Capacity?","answer":"Soil with low CEC has two main disadvantages: limited availability of mineral nutrients to plants and inefficient ability to hold applied nutrients. Plants must spend extra energy searching for minerals, and soluble mineral salts cannot be held efficiently because the cation warehouse is too small. Additionally, water retention is poor, which affects soil moisture-dependent functions and can limit organic matter accumulation.","context":["Cation Exchange Capacity (CEC) – First, what are cations (pronounced CAT-eye-ons)? Simply put, they are positively charged ions. The cations in the soil that concern us the most are calcium (Ca), magnesium (Mg), potassium (K), and hydrogen (H). The capacity of the soil to hold and exchange cations is determined by the amount of clay and/or humus that is present. These two colloidal (negatively charged) substances are essentially the cation warehouse or reservoir of the soil. Sandy soils with very little organic matter (OM) have a low CEC, but heavy clay soils with high levels of OM have a much greater capacity to hold cations.\nThe disadvantages of a low CEC include the limited availability of mineral nutrient to the plant and the soil’s inefficient ability to hold applied nutrient. Plants can exhaust a fair amount of energy (which might otherwise have been used for growth, flowering, seed production or root development) scrounging the soil for mineral nutrients. Soluble mineral salts (e.g. potassium sulfate) applied in large doses to soil with a low CEC cannot be held efficiently because the cation warehouse is too small.\nWater also has a strong attraction to colloidal particles. All functions that are dependent on soil moisture are also limited in soils with low CEC. Organisms such as plants and microbes that depend upon each other’s biological functions for survival are inhibited by the lack of water. Where there is little water in the soil, there is often an abundance of air, which can limit the accumulation of organic matter (by accelerating decomposition) and further perpetuate the low level of soil colloids.\nHigh levels of clay with low levels of OM would have an opposite effect (a deficiency of air), causing problems associated with anaerobic conditions. The CEC in such a soil might be very high, but the lack of atmosphere in the soil would limit the amount and type of organisms living and/or growing in the area, causing dramatic changes to that immediate environment. Oxidized compounds such as nitrates (NO3) and sulfates (SO4) may be reduced (i.e., oxygen is removed) by bacteria that need the oxygen to live, and the nitrogen and sulfur could be lost as available plant nutrients. Accumulation of organic matter is actually increased in these conditions because the lack of air slows down decomposition. Eventually, enough organic matter may accumulate to remedy the situation, but it could take decades or even centuries.\nThe CEC of a soil is a value given on a soil analysis report to indicate its capacity to hold cation nutrients. CEC is not something that is easily adjusted, however. It is a value that indicates a condition, or possibly a restriction that must be considered when working with that particular soil. Unfortunately, CEC is not a packaged product. The two main types of colloidal particles in the soil are clay and humus and neither is practical to apply in large quantities. Compost, which is an excellent soil amendment, is not necessarily stable humus. Over time compost may become humus, but the end product might only amount to 1-10 percent (in some cases, less) of the initial application.\nRemember that each percent of organic matter in the soil is equal to over 450 pounds per 1,000 square feet (20,000 lbs/acre). Compost normally contains about forty to fifty percent OM on a dry basis, and weighs approximately 1,000 pounds per cubic yard (depending on how much moisture it contains). If the moisture level is fifty percent, it would take two cubic yards of compost per thousand square feet to raise the soil OM level one percent (temporarily). Large applications of compost to the surface of the soil, however, can do more harm than good. Abrupt changes in soil layers can inhibit the movement of water and restrict the soil’s capacity to hold moisture. Obviously, building organic matter in the soil is not something that can or should be done overnight. Natural/organic nitrogen sources, in general, will do more than synthetic chemicals to raise or preserve the level of OM, because of the biological activity they stimulate. Colloidal phosphate contains a natural clay and is often used to condition sandy soils with a low CEC. Low phosphorus conditions should be present, however, to justify its use.\nIf a soil has a very low CEC, adjustments can and should be made, but not solely because of the CEC. A soil with a very low CEC has little or no clay or humus content. Its description may be closer to sand and/or gravel than to soil. It cannot hold very much water or many cation nutrients; plants, therefore, cannot grow well. The reason for the necessary adjustment is not the need for a higher CEC, but because the soil needs conditioning. A direct result of this treatment will eventually be a higher CEC. During the process of soil building, the steward must be aware of the soil’s limitations. Soil with a low CEC cannot hold many nutrients, so smaller amounts of fertilizer should be applied more frequently. Feeding a crop growing on soil with a low CEC is analogous to feeding an infant. It doesn’t eat a lot but must be fed often. As the CEC of the soil improves, larger doses of fertilizers can be applied less frequently."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:f87f48e9-cfd4-4086-9d05-12115f368b81>"],"error":null}
{"question":"Como investigador de enfermedades neurodegenerativas, necesito entender: ¿cómo se compara el rol de la microglía en la depresión inducida por estrés versus el envejecimiento cerebral?","answer":"In stress-induced depression, microglia in the medial prefrontal cortex becomes activated through TLR2/4 immune receptors, triggering inflammation-related cytokines IL-1α and TNFα, which leads to neuronal atrophy and depressive behavior. In aging, microglia shows decreased function and flexibility, while expressing elevated pro-inflammatory molecules (MHC-II, CD16/32, CD86) and increased secretion of inflammatory cytokines (TNF-α, IFN-γ, IL-6, IL-1β), resulting in a sensitized state that contributes to the inflammatory condition of the aging brain.","context":["Neural inflammation caused by the innate immune system plays an unexpectedly important role in stress-induced depression, a team of Japanese researchers has revealed.\nPrevious research had already hinted at the link between inflammation and depression, including increased levels of inflammation-related cytokines in the blood of patients suffering from depression, activation of microglia (inflammation-related cells in the brain) in depressive patients, and a high percentage of depression outbreaks in patients suffering from chronic inflammatory disease. However, the exact relationship between depression and inflammation still contains many unknowns.\nThe joint study was led by Professor Tomoyuki Furuyashiki and Assistant Professor Shiho Kitaoka (Kobe University Graduate School of Medicine) in collaboration with Project Professor Shuh Narumiya (Kyoto University Graduate School of Medicine).\nSocial Defeat Stress\nPsychological stress caused by social and environmental factors can trigger a variety of changes in both mind and body. Moderate levels of stress will provoke a defensive response, while extreme stress can lower cognitive functions, cause depression and elevated anxiety, and is a risk factor for mental illnesses.\nThe research team focused on repeated social defeat stress - a type of environmental stress - with the aim of clarifying the mechanism that causes an emotional response to repeated stress.\nFirst, they looked at changes of gene expression in the brain caused by repeated social defeat stress and found that repeated stress increased a putative ligand for the innate immune receptors TLR2 and TLR4 (TLR2/4) in the brain. Their next step was to investigate the role of TLR2/4 in repeated stress using a mouse with the TLR2/4 genes deleted.\nThey found that TLR2/4-deficient mice did not show social avoidance or extreme anxiety when exposed to repeated stress. Repeated stress usually triggers microglial activation in specific areas of the brain such as the medial prefrontal cortex, causing impaired response and atrophy of neurons, but these responses were not present in the TLR2/4-deficient mice.\nThe research team then developed a method to selectively block the expression of TLR2/4 in the microglia of specific areas of the brain. By blocking the expression of TLR2/4 in the microglia of the medial prefrontal cortex, they managed to suppress depressive behavior in response to repeated social defeat stress.\nThey found that repeated stress induced the expression of inflammation-related cytokines IL-1α and TNFα in the microglia of the medial prefrontal cortex via TLR2/4. The depressive behavior was suppressed by treating the medial prefrontal cortex with neutralizing antibodies for the inflammation-related cytokines.\n[caption id=“attachment_96440” align=“aligncenter” width=“680”] Credit: Kobe University[/caption]\nThese results show that repeated social defeat stress activates microglia in the medial prefrontal cortex via the innate immune receptors TLR2/4. This triggers the expression of inflammation-related cytokines IL-1α and TNFα, leading to the atrophy and impaired response of neurons in the medial prefrontal cortex, and causing depressive behavior.\n“These findings demonstrate the importance of neural inflammation caused by the innate immune system for stress-induced depression. This could lead to the development of new antidepressant medication targeting innate immune molecules,\"\nProfessor Furuyashiki said.\nXiang Nie, Shiho Kitaoka, Kohei Tanaka, Eri Segi-Nishida, Yuki Imoto, Atsubumi Ogawa, Fumitake Nakano, Ayaka Tomohiro, Kazuki Nakayama, Masayuki Taniguchi, Yuko Mimori-Kiyosue, Akira Kakizuka, Shuh Narumiya, Tomoyuki Furuyashiki The Innate Immune Receptors TLR2/4 Mediate Repeated Social Defeat Stress-Induced Social Avoidance through Prefrontal Microglial Activation Neuron DOI: https://doi.org/10.1016/j.neuron.2018.06.035\nTop Image: Christopher Burns on Unsplash","Population aging that we are currently witnessing has led to an increase in chronic age-related diseases, with dementia and depression being highlighted. Several studies establish a relationship between dementia and depression, although without defining the mechanism that links them. Some studies establish depression as a prodrome of dementia, while others consider it a risk factor for dementia. One of the events that is common between dementia and depression is the inflammatory process. In depression, an increase in inflammatory cytokines has been described, which would justify the serotonergic, noradrenergic and dopaminergic dysfunction of depression. This increase entails altering the activity of the hypothalamic–pituitary–adrenal (HPA) axis, thus linking chronic stress to depression, and the consequent weakening of the blood–brain barrier (BBB), facilitating the passage of pro-inflammatory factors. In this line, recent studies suggest that inflammation could direct the development of the pathogenesis of dementia, particularly Alzheimer’s disease (AD), once the pathology has begun. In addition, sustained exposure to pro-inflammatory cytokines characteristic of aging could alter the microglial function and the expression of enzymes responsible for amyloid peptide metabolism, aggravating the pathological process. In view of the involvement of the inflammatory process in both conditions, it is necessary to investigate the events which both conditions share, such as the inflammatory process, to know the involvement of the inflammatory process in both dementia and depression, possible relationship of these 2 conditions, and consequently, to establish the clinical approach to both conditions.\nKey words: dementia, Alzheimer’s disease, inflammation, depression\nAging is an important contributing factor in the onset and development of various neurological disorders, such as cognitive impairment or dementia. However, dementia is not a natural or inevitable consequence of aging. In fact, other clinical conditions, in this case pathological processes, have been described as being associated with an increased risk of cognitive impairment/dementia, including depression, hypertension, diabetes, hypercholesterolemia, and obesity.1\nDuring aging, the brain undergoes a progressive decline in energy use,2 and according to the free radical theory of aging, free radicals and related oxidants, both environmental and derived from cellular metabolism, would be the main cause of cellular damage, also due to their accumulation over time. Thus, the changes in energy metabolism associated with aging would be responsible for the associated functional and structural cellular problems. In other words, during the last third of our lives, our brain accumulates structural and functional damage that reduces our adaptive homeostatic capacity,3 which possibly makes it more susceptible to harmful stimuli.\nIn this context, one of the most affected cell types that are susceptible to such lesions are neurons, as well as different types of glial cells. Thus, in the aging brain, there is an increase in microglia, associated with a decrease in their function. Indeed, with aging, they lose their flexibility to move, which decreases their efficiency in defending the central nervous system (CNS),4 as well as their ability to block exogenous invasion or endogenous metabolites such as β-amyloid peptides.5, 6 In this regard, one of the most relevant facts about aged microglia is the elevated expression of pro-inflammatory molecules, such as MHC-II, CD16/32 and CD86. Even the secretion of pro-inflammatory cytokines, such as tumor necrosis factor alpha (TNF-α), interferon gamma (IFN-γ), inducible nitric oxide synthase (iNOS), interleukin-6 (IL-6), and interleukin-1β (IL-1β), increases significantly in response to harmful stimuli.7\nA possible interpretation of this shift from a microglial profile to an inflammatory or sensitization profile is based on 3 factors: 1) the increase in inflammatory markers and mediators; 2) the decrease in threshold and activation time; and 3) the increase in response and inflammation after this activation.8, 9\nIn this regard, Chung et al.10 established how age increases this sensitized state10, 11 – microglia develop an “alert, primed” phenotype, which contributes to the increased inflammatory state of the aging brain, as indicated by the increased inflammatory mediators and altered microglia phenotype (that occurs with age/aging). In this situation, results obtained in aged rodents following immune challenge, i.e., infection, show depressive-like behavioral complications and cognitive deficits.12\nIn the case of astrocytes, during aging, they also change their secretory phenotype to a pro-inflammatory phenotype under chronic stress. Even the aforementioned oxidative stress could induce astrocytes to secrete pro-inflammatory factors, such as IL-6, monocyte chemoattractant protein (MCP)-1 and metalloproteinase (MMP)-9, contributing to the inflammation in the senile brain, and altering the integrity of the blood–brain barrier (BBB).13\nAt this point, with the disruption of BBB integrity, it is important to note the enormous importance of the BBB in maintaining metabolic homeostasis in the CNS14 and, consequently, the increased exposure of brain tissue to toxic molecules or inflammatory signals that circulate in the blood when BBB is disrupted.\nConditions associated with impaired BBB integrity include oxidative stress,15 the presence of advanced glycation end products (AGEs) and their receptor (RAGE),16 increased production of pro-inflammatory cytokines,17 and vascular dysfunction, as well as chronic stress, depression or dementia.18, 19\nHaving described the role of aging as a contributing factor in various neurological disorders, such as cognitive impairment or dementia, it is necessary to understand its relationship to various clinical situations or pathological processes, including depression and dementia.\nIn the case of depression and dementia, we must bear in mind that there is no single mechanism that explains both pathologies, although similar neurobiological changes or even a similar pattern of neuronal damage have been described for both conditions, thus deepening our understanding of a complex relationship between both pathologies. Cognitive changes are common in the context of depression, and mood-altering symptoms of this condition often accompany cognitive disorders of dementia.20, 21, 22, 23 Our research group has found that the presence of depression increases the risk of dementia by 16%. However, we have also noted factors that condition this relationship, such as age or the presence of other diseases, for example, type II diabetes.24\nBoth dementia and depression present biological mechanisms that link them, such as vascular disease, atrophy of the hippocampus, a larger deposit of β-amyloid plaques, and inflammatory alterations.25, 26 In this sense, according to the latest studies, and as we have explained throughout this section, the inflammatory process is an important key effector in both processes.27, 28\nThis common point between depression and dementia is a promising research focus with clear clinical applicability for addressing both conditions.\nDepression and inflammation\nAlthough the main approach to depression is based on the historically accepted “monoamine depletion hypothesis,”29, 30 this hypothesis is not sufficient to explain the depressive disorder; especially in the last 20 years, several studies are pointing to the involvement of the inflammatory process in the disorder. This fact would justify the serotonergic, noradrenergic and dopaminergic dysfunction31, 32 inherent to depression; thus, we can speak of an “inflammatory hypothesis”. Authors such as Liu et al.28 link depression to the inflammatory process through increased levels of pro-inflammatory cytokines such as TNF-α and IL-6, decreased circulating levels of IL-1β and IL-8 in blood and cerebrospinal fluid,33 and increased corticotropin-releasing hormone levels; the latter results in an increase in the activity of the hypothalamic–pituitary–adrenal (HPA) axis, which in turn introduces stress into the process.\nChronic stress induces the weakening of BBB (described in animal experiments) and the consequent passage of circulating pro-inflammatory mediators.34 Therefore, authors such as Dudek et al.34 describe how stress-induced alteration of BBB permeability is linked to the inflammation of endothelium and involvement of tight junctions.\nFurthermore, as noted above, increased IL-6 and C-reactive protein (CRP) levels could predict the development of depressive symptoms.35, 36 Both molecules are predictive, indicating that inflammation precedes depression, but are also associated with cognitive symptoms of depression.37\nTherefore, the passage of peripheral myeloid cells into the brain in depressive processes would constitute an important clue supporting the existence of a central inflammatory response in depression that would be mainly driven by peripheral inflammatory events.38\nThe verification of this inflammatory response in depression suggests the possible existence of other causal biological pathways/processes in depressive processes39 and opens the door to the improvement of the response of current antidepressant therapies since, as reported by authors such as Miller and Raison,31 30–50% of depressed people do not respond to commonly prescribed antidepressant treatments and only 30% of patients remit.\nDementia and inflammation\nIn recent years, the inflammatory process has become important in the neurodegenerative pathology of Alzheimer’s disease (AD). Inflammation can “conduct” the pathogenesis of AD once the pathological process has begun.40, 41 Even the studies conducted by Lee et al.42 highlight the ability of pro-inflammatory microglia activation to aggravate and initiate the pathological process. At present, AD is considered to be a tauopathy initiated by β-amyloid peptide accompanied by neuroinflammation, thus connecting the 3 pathophysiological and anatomopathological events typical of AD.42, 43, 44, 45\nIn elderly population, age affects the microglial function and is associated with an alteration in amyloid metabolism, aggravated by sustained exposure to pro-inflammatory cytokines, such as TNF-α, and the whole process can inhibit microglial function.46\nIn this regard, the disruption of BBB by inflammatory mediators during the progression of AD has been described. In the BBB, the neurovascular unit (NVU) is responsible for neurovascular coupling, i.e., the interaction of neuronal (neurons and glia) and vascular tissues (endothelial cells, pericytes and adventitial cells).14 Several authors show how this coupling is impaired in AD,47 suggesting the important role of NVU in the progression of cognitive impairment. Other situations in which this coupling is also altered, and which are also related to AD, are hypertension48 and ischemic stroke49 (postmortem studies emphasize the important role of vascular pathology in a significant percentage of AD patients).50\nIn this way, aging appears to be an aggravating factor in the development of neurodegenerative diseases such as AD. In addition, and based on the studies reviewed, aging also contributes to an increase in vulnerability to certain conditions such as depression,51, 52 through sustained activation of pro-inflammatory signals (Figure 1).\nGiven this knowledge, it is necessary to develop new research lines in order to establish the link between depression and dementia; and, based on what is known, to establish strategies of modulation of pro-inflammatory states that could modify the prevalence of neurodegenerative diseases such as dementia."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"search_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:9a77479b-e12a-4a5b-8b44-e335662d61ae>","<urn:uuid:913fa090-40f3-43ba-bf80-b9868da12ccc>"],"error":null}
{"question":"How can prevent freezer burn in frost-free freezer?","answer":"In frost-free freezers, it is especially important to double or triple wrap foods to prevent exposure to the circulating air, which can dry out foods and make them more susceptible to freezer burn. Freezer burn appears as grayish-brown leathery spots and is caused by air coming in contact with the surface of the food. While freezer burn doesn't make food unsafe, it makes it dry in spots and heavily freezer-burned foods may need to be discarded for quality reasons.","context":["Tips for spring cleaning the freezer\nFrozen foods in the home may be stored in the freezer compartment of the refrigerator. Others elect for a larger upright or chest-style freezer. Many foods freeze well and are great to have on hand for preparing family meals. There are, however, some basic tips for maximizing the use of freezer space and reducing waste. It may take a spring cleaning event to get freezer compartments in shape but it will be worth it in the long run. Consider the following suggestions for getting the most out of your frozen foods from the Food Safety and Inspection Service (.fsis.usda.gov ).\nChoose appropriate and high quality foods\nJust as in other forms of home food preservation, when you choose freezing as a method to store/preserve foods, choose foods at optimum freshness. If you have fresh meat, for example, that has been sitting in the refrigerator for several days before the decision is made to place it in the freezer, the quality, once thawed, may well be marginal; at best. Keep in mind that not all foods freeze well. Mayonnaise, cream sauce and lettuce, for example may be frozen but once thawed, are not appetizing.\nProper packaging helps maintain quality and prevent freezer burn. It is safe to freeze meat or poultry directly in its original packaging, however this type of wrap is permeable to air and quality may diminish over time. For prolonged storage, overwrap these packages as you would any food for long-term storage. It is not necessary to rinse meat and poultry. Freeze unopened vacuum packages as is. If you notice that a package has accidentally been torn or has opened while food is in the freezer, the food is still safe to use; merely overwrap or rewrap it.\nFreeze food as fast as possible to maintain its quality. Rapid freezing prevents undesirable large ice crystals from forming on and throughout the food. This may require dividing food up into smaller containers or packages.\nWatch for freezer burn\nCounty Extension offices often receive customer calls asking how long certain foods will store well in the freezer. There are some basic guidelines found in chart form at foodsafety.gov/keep/charts/storagetimes.html but shelf life also depends on how well the food is packaged and the temperature of the freezer compartment (0 degrees F. or lower is recommended).\n•A note here about frost-free freezers. These types of freezers are a convenience to those who are not physically able or those who choose not to manually defrost their freezer. However, it is especially important to double or triple wrap foods to prevent exposure to the circulating air, which will dry out foods and makes them more susceptible to freezer burn.\nFreezer burn does not make food unsafe, merely dry in spots. It appears as grayish-brown leathery spots and is caused by air coming in contact with the surface of the food. Cut freezer-burned portions away either before or after cooking the food. Heavily freezer-burned foods may have to be discarded for quality reasons.\nDate and Rotate\nWriting a storage date on foods and rotating packages regularly (placing newest/freshest foods in the back and moving older foods to the front or top) protects against finding older food inside the freezer that is no longer usable and must be thrown away. Freezing does not stop foods from ripening or stop enzymes — it does substantially slow down the process.\nNever thaw foods in a garage, the basement or out on the kitchen counter. These methods can leave your foods unsafe to eat. There are three safe ways to thaw food: in the refrigerator, in cold water, or in the microwave. It's best to plan ahead for slow, safe thawing in the refrigerator. Small items may defrost overnight; most foods require a day or two. And large items like turkeys may take longer, approximately one day for each five pounds of weight.\nFor faster thawing, place food in a leak-proof plastic bag and immerse it in cold water. (If the bag leaks, bacteria from the air or surrounding environment could be introduced into the food. Tissues can also absorb water like a sponge, resulting in a watery product.) Check the water frequently to be sure it stays cold. Change the water every 30 minutes. After thawing, cook immediately.\nWhen microwave-defrosting food, plan to cook it immediately after thawing because some areas of the food may become warm and begin to cook during microwaving.\nFreezers are indeed a wonderful convenience to have food on hand and store large quantities of foods purchased during sales or after a hunting event. See the FSIS site or contact your local Extension Office for answers to specific questions.\nKathleen Riggs is the Utah State University Extension family and consumer sciences professor for Iron County. Questions or comments may be sent to firstname.lastname@example.org or call 435-586-8132."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:60e6598e-8287-4de7-9210-157ca363d63e>"],"error":null}
{"question":"I have clay-heavy soil in my garden. What's the best approach to improving it - should I till it, and what role do earthworms play in soil improvement?","answer":"For severely compacted, clay-heavy soil, tilling can be beneficial when done properly. The soil should be tilled 10-12 inches deep and combined with 2-3 inches of organic matter like compost or rotted manure to maintain its loosened texture. However, a more sustainable long-term solution would be to support earthworm activity, as these organisms naturally improve soil structure. Earthworms create thousands of tunnels that increase soil porosity, improve water absorption, and provide channels for plant roots. They also continuously mix soil layers and create nutrient-rich casts that enhance soil fertility, making them valuable allies in improving clay soil naturally.","context":["There are some gardening terms that are often taken for granted that not everyone knows a good definition for. One of these is tilling the soil. Understanding what it is and its advantages and disadvantages will help you decide whether you should add it to your list of gardening chores.\nWhat Tilling Is\nTilling is simply turning over and breaking up the soil. Exactly how deep you till and how fine you break up the soil depends on your reason for tilling. To make the task of tilling easier, you can purchase or rent an engine-powered tiller from a garden supply center. These tillers are great for large areas, but if you only have a small area, try a cultivator fork or a deep spader or cultivator to loosen up compacted soil.\nTilling is needed if you are mixing amendments into the soil. It is also helpful if you have severely compacted soil that needs to be broken up or any type of soil that needs to be broken into finer bits for planting seeds. This process also removes weeds and undesirable roots from the soil. If you would like to turn a section of your lawn into a garden bed, tilling turns the sod over, which mixes the organic matter from the grass into the soil, producing the base for a ready-to-plant garden bed.\nDisadvantages of Tilling\nTilling is not always beneficial, and you may want to think twice before reaching for a rotary tiller every spring. Earthworms, which naturally aerate and fertilize the soil through their digging and castings, are often killed by the chopping action of a tiller. Dormant weed seeds deep under the ground get stirred up by a tiller, where they can germinate and produce new weeds, even if the old weeds on the surface are now dead. Humus in the soil gets burned up by the addition of too much air from tilling. The plants cannot benefit from the nutrients in the humus if this happens.\nTilling for Severely Compacted Soil\nCompacted soil that is heavy in clay makes it difficult for roots to push through and grow. To break up compacted soil, you will need to till the soil and then work organic matter into the soil so it retains its loosened texture. Powered tillers make the job easier. Set it to till down to 10 to 12 inches deep. Leave the overturned soil for two to three days to dry out. Any roots, rocks, sticks or other foreign matter must be removed. You should then combine the soil with 2 to 3 inches of organic matter -- such as compost or rotted manure. The organic matter will help to naturally loosen and aerate the soil.\n- University of California Berkely Friends of the Botanical Garden: Serpentine and Its Plant Life in California\n- Under the Solano Sun: Make Friends With Your Clay\n- Fine Gardening: Tilling is One Chore You Might Be Able to Skip\n- Fine Gardening: Four Ways to Remove Sod\n- Fine Gardening: Essential Tools for Working the Soil\n- Creatas/Creatas/Getty Images","Earthworms & Soil Formation\nSoil formation is a continuous process, impacted by physical factors including wind, rain and temperature, as well as biological forces such as plants and animals. Earthworms significantly influence the formation of soil, helping to shape soil structure, content and fertility. With thousands of species worldwide, earthworms are found in most temperate and tropical zones around the globe. Although healthy soil can be formed without earthworms, the presence of earthworms is usually an indicator of productive soil. Through their seemingly tireless activity, earthworms benefit soil formation in a number of ways.\nEarthworms play an important role in mixing and aggregating soil. In their quest for food, earthworms continuously rise to the surface and tunnel down again, swallowing bits of soil and organic matter along the way. Earthworms excrete this mixture of minerals and plant materials into the soil in the form of nutrient-dense casts. In the process, they bring soil from the top layer to the lower strata and drag soil from below to the surface. This constant churning action can turn over a half foot of topsoil in approximately 10 to 20 years, according to experts at the Natural Resources Conservation Service.\nAs earthworms burrow through the dirt, they improve soil structure by loosening compacted soil and creating thousands of tunnels below the surface. These tunnels increase soil porosity, providing pathways for water and air to penetrate into the earth. As a result, soil is better able to absorb and retain water and less prone to erosion and water run-off. In addition, these tunnels provide channels into which plant roots can easily spread, adding stability as well as organic material to soil.\nEarthworms contribute to soil formation by assisting in the decomposition and incorporation of organic materials into the soil. Earthworms eat leaves and dead roots found on or near the soil’s surface. They mix this organic material into the soil by tearing off portions of plant material and burying it deep within the earth. As they eat, they grind bits of plant material in their gizzards and excrete the organic matter into the soil through their casts. Earthworm casts also pile up on the surface of the soil, adding to soil content. In addition, earthworms stimulate the activity of beneficial bacteria and fungi. These microorganisms feed on organic material, breaking it down into humus.\nSoil Food Web\nEarthworms are important members of the soil food web, a complex community of organisms that impact the process of soil formation. The soil food web consists of soil-dwelling organisms, ranging from bacteria, algae and fungi to insects, small mammals, reptiles and plants. These organisms affect soil formation by burrowing, breaking down plant and animal materials and eventually contributing their own organic matter to the soil at death. Earthworms are an important food source for many members of this community. In addition, earthworm tunnels and casts help provide the water, nutrition and oxygen needed to create an environment in which these organisms can thrive.\n- University of California Cooperative Extension: Earthworms and Soil Productivity\n- U. S. Department of Agriculture, Natural Resources Conservation Service: Soil Formation and Classification\n- U. S. Department of Agriculture, Natural Resources Conservation Service: Soil Biology: Earthworms\n- University of California, Oak Woodland Management: Earthworm Ecology in California\nBased in the Atlanta area, Charlene Williams has been writing and editing since 1988. She has over 15 years of experience working as a technical writer in the software industry. She has worked as a freelance writer for the past five years, and is a contributing writer for eHow and Answerbag. Williams holds a Bachelor of Arts in English from Kennesaw State University."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:d1e37670-2157-441f-9439-9ecafc49a0a0>","<urn:uuid:8a0a257f-1b9a-42d2-bfbc-6ccd0b5b9aed>"],"error":null}
{"question":"What physical tests do police recruits need to pass?","answer":"Police recruits must pass several physical tests to demonstrate their fitness for duty. The requirements include: having good eyesight and hearing, possessing adequate upper body strength, and being able to run at least a mile and a half. Candidates are also tested for endurance and dexterity. These physical requirements are crucial for ensuring officer safety and their ability to perform their duties effectively.","context":["Understanding what one is about to embark on as a police officer recruit occurs the day the academy received the application. Those who have been in this job for years will question the physical as well as emotional health of the candidate. Expectations are high for todays recruits, so should be the screening. Follow a candidate through the initial application process through to the end of the process as he or she is accepted into the next academy.\nKeywords: job analysis, performance appraisal methods, reliability and validity\nJob Analysis Paper\nPolice Officer Analysis\nThe job of any type of police officer comes with tremendous responsibility as well as many risks. There are many avenues that follow on the career path of a law enforcer. There are mental challenges one faces, so it is imperative for someone who’s looking to begin a journey in this field knows exactly what he or she will be faced with. There are several requirements that they must go through to be considered a candidate. Some of these requirements are mental capacity, strength, and endurance. A candidate run thought these assessments will guarantee that he or she is psychologically and physically capable of doing the job as a police officer.\nMany organizations use various types of job analysis to define a specific job. At the beginning of a new job the employee should receive a list of the job duties. Receiving this list will not only aid the new employee in knowing what tasks need to be completed. The job selected is that of a police officer and how the job-analysis will define the right candidate.\nA work-orientated analysis examines the abilities, skills, knowledge and other knowledge or KSAO’s (Spector, 2008). The physical side of a police officer compensates the majority of duties. Using KSAO’s method, those incapable of maintaining any level of the rigorous training will be excluded from the position of police officer.\nAn applicant needs to be physically fit when considering the position of police officer. A candidate must pass a written examination, physical abilities test, background investigation alongside a polygraph examination, be reviewed before a prescreening board, have an oral review, and a psychological examination and a physical exam before even considered a slot in the academy (City of Virginia Beach, 2011). The written exam consists of a bunch of questions that will test the knowledge of the candidate in his communication skills.\nEvaluate the reliability and validity of job analysis\nThere is special equipment someone who’s interested in law enforcement needs to be well trained on. Items like two way radios, handcuffs, baton as well as the completion of reports and forms all help making his or her job safer and easier. By using the KSAO this ensures that a potential officer will know what is expected of the officers during their shift. Securing the right data, and the information is dependable is solely based on person responsible for collection methods (Competency and Position Analysis Questionnaires, 2012). After the data is collected, discussing the results with the employee will ensure that he or she is in agreement with his or her observations. It is often noted that not very many supervisors know all that an employee handles on a daily or weekly basis to provide adequate answers to various questions that may arise during an analysis. Someone who studies how to assemble information related to various jobs have to be trained. He or she needs to know what questions to ask, what information needs to be imputed, over other items that someone may feel needs to be ignored. Most analysts have a vast background. They receive training from already trained job analysts (Competency and Position Analysis Questionnaires, 2012). Some organizations can supply training to someone who is looking to be the in-house analyst.\nPerformance Appraisal Methods\nThere are several avenues that can be used when looking to evaluate someone seeking employment as a police officer. The physical evaluation is crucial for a police candidate. The aspirant needs to not only be physically fit, but also needs great eyesight and hearing which will aid the officer in remaining safe on the streets. The applicant also needs to have upper body strength and the ability to run at least a mile and a half based on the Occupational Handbook for Police and Detectives, 2009. Endurance, dexterity and, strength are all physical tests given to the potential officer to ensure he will excel at his position (Office of Occupational Statistics and Employment Projections, 2009). Staying physically fit throughout his career will prevent him from emotional or physical trauma.\nCommunication assessment is also another form of appraisal. This method is used during the prescreening interview as well as oral review board. Communication is vital when dealing with the public manner. Not just verbal communication, but written communications are just as important. The officer will be required to write reports based on encounters he or she has had with victims, suspects, and witnesses. Having someone who is an excellent writer will help prosecutors when cases are tried in court (Office of Occupational Statistics and Employment Projections, 2009). Another valuable skill that an officer needs to hold is that of decision-making and judgment skills. Listening skills do not fall to far behind these two either because it will be needed when faced with the myriad of situations one comes across in a career in law enforcement. Being able to communicate both verbally and manually is highly effective in law enforcement.\nThe other two processes that will accurately evaluate a police officers performance are objective measures, and subjective measures. Objective measures are used to count a specific behavior, such as how many times does an officer respond to a call for service. Information collected by called received at dispatch can be transferred into statistics kept for reference. Subjective measures are evaluations performed by someone who has extensive knowledge of that person’s job performance, such as his shift supervisor (Spector, 2008). Evaluations done by an officers supervisor is based on performance in common practices.\nThe demands of a police officer are physically as well as psychologically challenging. Because of the psychological impact an officer faces it is imperative that an officer candidate undergo as many evaluations as possible to make sure he or she is not a risk to himself, or the community the officer serves. The physical tests ensure that the safety of the officer will attempt to keep him or her as well as the public from harm. Having a job analyst come in to review policies and procedures ensure that each new recruit is on top of his game will not be a sad statistic of those caught in the life of fire.\nCity of Virginia Beach. (2011). Virginia Beach Government. Retrieved February 11, 2012, from What are the Steps to Becoming a Police Officer?: http://www.vbgov.com/government/departments/police/profstanddiv/Documents/stepstoa pplyingtobecomeaVBPDpoliceofficer.pdf\nCompetency and Position Analysis Questionnaires. (2012). PAQ Services. Retrieved February\n11, 2012, from JOB ANALYSIS : http://www.paq.com/index.cfm?FuseAction=bulletins.job- analysis\nOffice of Occupational Statistics and Employment Projections. (2009, December 17). Occupational Outlook Handbook, 2010-11 Edition. Retrieved from Police and Detectives: http://www.bls.gov/oco/ocos160.htm#training\nSpector, P. E. (2008). Industrial and Organizational Psychology 5th Ed. . Hoboken, New Jersey: John Wiley and Sons, Inc. ."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:091b4bd3-a35a-49a6-9a14-6f264f5fe6a6>"],"error":null}
{"question":"What temperature is milk heated to during pasteurization, and what symptoms can people experience from drinking raw milk?","answer":"During pasteurization, milk is heated to about 161°F and then cooled to 40°F. As for symptoms from drinking raw milk, people commonly experience vomiting, diarrhea (sometimes bloody), abdominal pain, fever, headache and body aches. In some cases, people can develop more serious conditions including pneumonia, blood-stream infections, kidney failure, meningitis, chronic liver disease or chronic heart disease.","context":["Raw milk cheese is made with milk that has not been subjected to the pasteurization process. According to USDA food consumption laws, raw milk cheese is prohibited for sale unless it has been aged for a minimum of 60 days at a temperature of 35°F. Some see this law as completely unnecessary since a number of countries do allow raw milk cheese to be sold in the market without restrictions.\nRaw milk cheese\nThe 60-day law is aimed at protecting consumers from harmful bacteria that might potentially be present in raw milk. After 60 days, the salts and acids in raw milk naturally prevent the growth of microbes like E. coli, Salmonella, and Listeria.\nSubscribe to our Newsletter!\nReceive selected content straight into your inbox.\nAccording to a study, the chances of contracting listeriosis, a foodborne disease, is much higher when eating cheese made from raw milk than from those made from pasteurized milk. Organizations like the American Academy of Pediatrics recommend that children, pregnant women, and people with weak immunity avoid raw milk cheese to protect themselves against bacterial infections.\nDuring pasteurization, the raw milk is heated to about 161°F and is then cooled to 40°F. The good news is that pasteurization does not affect the nutritional value of milk and neither does it negatively affect the potency of the health benefits. However, cheese made from pasteurized milk pales in comparison to raw milk cheese when it comes to flavor.\nThis is because pasteurization also ends up killing the good bacteria that are responsible for infusing cheese with natural flavors. Some types of cheese only require a month or so to be properly aged. If you age them further, their texture and depth of flavor suffer.\nAmerica can take inspiration from Europe where raw milk cheese is allowed in the market with no extra restrictions. European cheeses get tested for harmful bacteria before they are approved for sale. If such testing standards were followed, the U.S. could potentially get rid of the 60-day rule for raw milk cheese.\nThe downside is that if a batch of milk or cheese were to be found contaminated, then all of it would have to be discarded. This is what prevents American cheese businesses from following European test standards. They would rather pasteurize the milk and ensure that they do not have to suffer any waste.\nPasteurization and safety\nEven with pasteurization, there is no guarantee that the cheese will be free from harmful microbes. Some studies have discovered that most of the milk-related illnesses actually came from milk or milk products that have been pasteurized. Food scientist Catherine Donnelly believes that the FDA purposefully overlooks these findings as it is being influenced by industrial cheese-making companies. She favors small businesses over corporations when it comes to ensuring the safety of cheese.\n“A small-scale producer has much more control over the safety of that process than some of these large-scale industrial plants, where there are lots of post-pasteurization processes like shredding, and cutting, and repackaging, and lots of opportunities for exposure to contamination.\nThat’s why we see more outbreaks associated with industrially processed products, as opposed to artisan,” Donnelly said to Eater. At present, American citizens have no option but to settle for pasteurized or 60-day aged raw milk cheeses. And the regulation looks like it will be staying with us for a long time to come.","The Dangers of Drinking Raw Milk\nWhat is raw milk?\nRaw milk is milk from cows, sheep or goats that has not been pasteurized to kill harmful bacteria.\nWhat does the New York State Department of Health recommend about the consumption of raw milk or raw milk products?\nThe New York State Department of Health strongly recommends that people DO NOT CONSUME any raw milk or raw milk products. The New York State Department of Health recommends consuming only milk and milk products that have been pasteurized.\nWhat are the recommendations from other agencies about consuming raw milk or raw milk products?\nThe federal Centers for Disease Control and Prevention (CDC), the Food and Drug Administration (FDA), the United States Department of Agriculture (USDA), the American Academy of Pediatrics and other medical, veterinary and scientific organizations recommend consuming only pasteurized milk and milk products.\nWhat is pasteurization?\nPasteurization is the process of heating milk to a high enough temperature for a long enough time to kill disease-causing bacteria contained in the milk and has been used safely for over 100 years. Pasteurization of milk became widespread in the United States by 1950. Pasteurization is the only way to ensure that milk products do not contain harmful bacteria.\nDo products made from raw milk carry dangerous bacteria?\nYes. Other foods made with raw milk include soft cheeses, sour cream, yogurt and ice cream. Any food made with raw milk can contain dangerous bacteria and consuming these foods can make you very sick.\nWhat kinds of harmful bacteria can raw milk contain?\nCan these bacteria be especially dangerous to certain people?\nYes. The bacteria in raw milk can seriously affect the health of anyone who drinks raw milk or eats foods made from raw milk. The bacteria can be especially dangerous to pregnant women, children, the elderly and people with weakened immune systems.\nWhat symptoms can people have if they become ill from drinking raw milk with these bacteria?\nMost commonly, bacteria in raw milk can cause vomiting, diarrhea (sometimes bloody), abdominal pain, fever, headache and body aches. In some instances, people can develop pneumonia, blood-stream infections, kidney failure, inflammation of the nervous system (meningitis), chronic liver disease or chronic heart disease from drinking raw milk. While some people will have mild illness from bacteria in raw milk, others are at much higher risk of life-threatening illness, especially pregnant women, children, the elderly and people with weakened immune systems (such as people with cancer or HIV infection or recipients of organ transplants).\nHow many people in the United States have become ill from consuming raw milk or raw milk products?\nSince 1993, over 70 outbreaks of human illness from consumption of raw milk have been reported nationwide, affecting over 1,500 people and causing 185 hospitalizations and 2 deaths. The actual number of illnesses associated with raw milk likely is far greater.\nWhat should I do if I become ill after drinking raw milk or eating a product made with raw milk?\nIf you or someone you know becomes ill after consuming raw milk or products made from raw milk, see a doctor or health care provider immediately, especially if you are pregnant or at higher risk for developing severe illness.\nDoes pasteurizing milk reduce its nutritional value?\nNo. The pasteurization process does not change the nutritional value of milk.\nDoes pasteurization cause disease, developmental problems or behavioral problems?\nNo. Pasteurization has never been shown to cause disease or other health problems.\nDoes pasteurization cause lactose intolerance or allergic reactions?\nNo. Pasteurization has never been shown to contribute to lactose intolerance or other allergic health problems.\nHow does New York State regulate the sale of raw milk?\nIn New York State, raw milk may only be sold at dairy farms that hold a permit from the New York State Department of Agriculture and Markets. Permitted farms are required to maintain proper sanitation, animal health, packaging procedures, monthly inspections, routine sampling and testing and post signs that warn that raw milk does not provide the protection of pasteurization.\nWhere can I see a complete copy of the rules and regulations regarding raw milk sales in New York?\nRequirements for the Production, Processing, Manufacturing and Distribution of Milk and Milk Products: www.agriculture.ny.gov/DI/Laws%20in%20PDF/PART2_milk_control_law.pdf.\nWhere can I get additional information about the risks of consuming raw milk?\nVisit the following resources:\n- The Dangers of Raw Milk: Unpasteurized Milk Can Pose a Serious Health Risk - U.S. FDA\n- Raw Milk Fact Sheet - FDA & CDC\n- Health Benefits, Risks, and Regulations of Raw and Pasteurized Milk, Valente B. Alvarez and Francisco Parada-Rabell - The Ohio State University Extension\n- Milk, Cheese, and Dairy Products, Myths About Raw Milk - FoodSafety.gov\n- Food Safety and Raw Milk - Food Safety at CDC"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"format_constrained"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:cd1676cd-a357-4122-8511-b88d6397c3eb>","<urn:uuid:67336dd8-d6e3-4310-9720-3303eef98581>"],"error":null}
{"question":"What's the connection between composing music and creating a dish?","answer":"Composing music and creating a dish share similar creative principles. In cooking, different elements of taste like sour lemon and sweet honey are combined to make something unique. Similarly, in music composition, different elements like strings, clarinet, and percussions are combined to create a unique sound. Both require time to be properly appreciated - time to enjoy beautiful music and time to taste a fantastic dish.","context":["Meet a Renaissance Man\nFrom folk to classical, music has always played an important role in Italy, inventing instruments such as the piano and violin. But of course, the Italian culture is also one steeped heavily in food, popular for its regional diversity and abundance of different tastes. Sicily, in particular, is a cross-section of both of these arts, home to the country’s largest opera house, Teatro Massimo, and often nicknamed “God’s Kitchen” for their variety of noted cuisines and wines. So it’s only natural for Messina-born Roberto Scarcella Perino, who now lives in New York City, to be torn between them. “I have two passions in my life: good music and good Italian cuisine,” Perino shares.\nThe art of music...\nSince he was a child, he knew music was going to be a big part of his life. His nonno, a surgeon who could speak Latin and Greek, played the violin and sung as a tenor. Moreover, his aunt was a soprano, who frequently sang with Franco Corelli—an Italian opera singer celebrated universally for his powerhouse voice—and another aunt also played the piano very well. Surrounded by the trade, he followed in his loved ones’ footsteps and started taking piano lessons at seven years old, subsequently writing music. “I wanted to write music for different reasons. For one, I needed to create, the other because I didn’t like to study so much or to practice scales. To me, writing music was like drawing for kids.”\nAt the age of 20, he moved north to study at the Conservatory Giovanni Battista Martini and at the University of Bologna. Afterwards, he entered the workforce as a composer, commissioned by the Fondazione Arturo Toscanini in Parma, later having operas performed in Pisa (A Caval Donato, 1999), Busseto (Merli Verdi e Cucù, 2001), and Torino (Blackout, 2008). Repeatedly touched by his music, Perino feels that hearing the sounds of his own creations are often the best moments of his life. “Whenever I listen to the performer or an orchestra play my music, I realize that what was once just an idea became something that I can really touch, that I can really enjoy.”\n... and the art of food\nSimilar to the craft of song, he was also exposed to the culinary arts at a young age, as his father was fond of hosting his big family and cooking for them. During his time in Emilia-Romagna, which he considers the capital of food, he first learned how to cook fresh pasta. “That was the moment my creativity took two different paths,” Perino recalls. Upon this discovery, though still pursuing a career in music, he immediately immersed himself into cooking. He threw parties, made meals for his friends, and asked his parents about recipes. However, in terms of recipes, he never follows them the “right way.” This ensures variation so that no two ever dishes are ever the same. Akin to listening to his music, Perino is gratified by the process of preparing food because when it’s finally ready he feels a sense of accomplishment.\nMoving to America\nIn 2001, he moved to New York, where he presented himself as a composer and, to his surprise, was very well received. He found a job at New York University and is now a Senior Language Lecturer of Italian. It was here where he premiered his first ballet, Colapesce (2004), at Casa Italiana Zerilli-Marimò. Perino went on to write two more ballets, Constellations (2005) and Basket-Dance (2007). Impressively, the latter was performed both in Pisa and at the Weill Recital Hall at Carnegie Hall. He continues to write music, presenting two world premieres this last spring at Casa Italiana, Piano Sonata No. 2 and String Quartet No. 1.\nCooking in New York\nPerino also finds the time to cater private parties and to teach others how to cook. “It is beautiful to share all of my knowledge about Italian cuisine,” he says. Proud of his Sicilian roots, he always utilizes the region’s ingredients, like eggplant, almonds, and “lots of tomatoes, of course.” Although he’s removed from his motherland’s Mediterranean climate, he takes advantage of places like the Union Square and Chelsea markets to buy the freshest ingredients. While, for example, he can’t find Sicilian eggplant, he adapts to the American-grown alternative. Fortunately, he loves the result “because it’s like tasting Sicily from New York.”\nAnother aspect he misses from home is collecting almonds from the beautiful mandorlo in fiore (almond trees). This makes tasting his semifreddo alle mandorle (almond parfait) even sweeter because it reminds him of his childhood. “It’s a beautiful moment and very nostalgic at the same time.”\nHis most popular dish that he likes to cook and that others always request is Parmigiana di Melanzane (Eggplant Parmesan). “When I prepare dinner for my friends, I offer a part of Italy to them, especially a part of Sicily, of my culture, of my family, my family’s recipes. It’s a beautiful way to share what I am,” he says. This happens also with music. “I write my music in a small room with just the piano. Then in a big hall, I present all of my music, which is actually my story translated with notes.”\nThe two arts as one...\n“Composing a dish is like putting together different elements of the taste, like the sour of the lemon or the sweet of the honey. All of the elements are different from each other but when combined they make something unique. The same thing happens when I write my music. When you do an orchestration you have the string, the clarinet, and the percussions together. You invent and you create a new sound that is unique just for the precise moment. Just as you need time to enjoy beautiful music, you need time to taste a fantastic dish.”"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:75d28a4f-2a16-40da-85e0-2fe5c5b120fb>"],"error":null}
{"question":"What electronic entertainment systems are suitable for marine use, and what personal safety equipment is mandatory for the crew during ocean voyages?","answer":"For marine entertainment, the Garmin Fusion Entertainment system offers a compact CD and DVD player suitable for cabin use. For crew safety, mandatory equipment includes immersion suits (survival suits) to prevent hypothermia - these must maintain body temperature above 35 degrees Celsius for six hours in 0-degree water. SOLAS requires one immersion suit per person onboard, except for ships in warm water voyages. Additional personal safety equipment includes life jackets with whistles and water-activated lights, and self-contained breathing apparatus for entering compartments with toxic gases.","context":["There are lots of different approaches to take when it comes to a marine adventure on the ocean. There are some experienced sailors that like the old-fashioned methods and can set off in their sailboat with the bare minimum. Others need a few more electronics onboard to turn their multi-hull boat into a sophisticated vessel.\nWhether boat owners buy a few essential items, or go crazy and deck out the entire boat, there are some important places to start.\nThis marine electronics guide points out the most important items to have onboard and explores 5 categories: navigation, communication, safety, sporting pursuits, and entertainment. Within those categories are three products that should prove useful to anyone heading out on an ocean voyage.\nThere are some brand names that come up a few times in this guide – such as Simrad and Garmin. This frequency of results is down to the quality offered, the breadth of their range and buyer recognition. There is also the fact that some marine companies build devices for a compatible set-up.\nSafety considerations and entertainment equipment are all important in their own right. But, there is no way to get to a destination and use these tools without the right navigation equipment. GPS systems can help sailors plot a course, but there are other helpful tools like compasses, charting devices and other apps.\nThere are lots of different mapping devices on the market that aim to provide clear navigation tools. Garmin is one of the most popular brands in this area because of their work on GPS and the quality of the tools. The Garmin ECHOMAP Plus uses SideVu, ClearVu, and traditional CHIRP sonar channels. The images and data all appeal in a clear 9-inch touchscreen so they are easy to read.\nAnother benefit to this mapping system is the use of the ActiveCaptain app. This app provides more information and connectivity between the boat and the marina. There is also the opportunity to use this device as a fish finder. Still, there are other dedicated products for that, such as the one below.\nDedicated fishfinders are a great idea for any boat owner with the desire to land the perfect catch on their marine adventure. Fishfinders give sailors the upper hand with a clearer idea of where the fish are and the terrain beneath the surface.\nThe Simrad S2016 Fish Finder is a great example because of the use of sonar and the quality of the display. The S2016 system uses a CHIRP-enabled Broadband sounder to scan the water. Results come back via a clear, audible ping and strong imaging on the 16’’ display.\nThis updated model has a much larger screen than previous options for a better field of vision.\nAuto-pilot systems can offer some peace of mind to sailors as they head out on a steady course. Solo travelers, or small groups with a lot to do, can attach these systems to the hydraulic steering for a little assistance. The Ev-150 Hydraulic system from Raymarine has everything that users need in one box. This package includes the sensor, control unit, control head, and cabling kit. The control head has a clear backlit screen with intuitive controls. This ease of use means that first time users shouldn’t have too many problems staying on course.\nMarine Communication Tools\nCommunication is also important on these adventures. Sailors need a strong line of communication between the boat and their port in case of emergencies. This connection allows for information relays on weather, incidents and anything else that might be vital to the project. A communication device also lets the crew contact families and bases for updates on positions and progress.\nA good VHF system is essential for anyone heading out on an ocean voyage. This two-way system provides a clear link between the vessel and the land. The importance of a device like this means that boat owners need a reliable, well-designed system.\nThe Simrad RS20 VHG Radio fits that bill for many sailors. It has multiple scan model and clear channels, with easy to use controls. The first microphone has four buttons for easy operation and there is a high visibility display.\nThe other benefit to this Simrad model is the size. This model is small enough to sit neatly on a bracket without taking up much space. There is also the option of a flush-mount installation.\nThere is little point in taking the latest smartphone out on the ocean in order to stay in touch with family and friends. A lack of coverage and dying batteries limit the potential of these cell phones.\nA satellite phone like the Garmin inReach Explorer+ is much more reliable with its 100% global iridium satellite coverage and 2-way text messaging. This handheld Garmin device is pretty user-friendly, even if it isn’t as sleek and smart as an iPhone.\nPartners at home approve of the ease of communication, which is the most important benefit here.\nMarine Electronics For Security & Safety Needs\nSafety and security are essential when traveling on your vessel for prolonged periods, even for those with experience. No sailor ever knows when danger will strike, or if a crewmate will have an accident. Basic essentials like flares and life jackets are a must. But, there are some electronic marine gadgets that can also help. In addition to this, many boat owners will worry about security. An unattended boat, full of expensive tech like this, could be a magnet for thieves.\nPersonal Locator Beacon\nStarting with the safety aspect, there is the personal locator beacon. These beacons are a great tool for those worst case scenarios and accidents. The beacon connects with local search and rescue operations to alert them to a life-threatening emergency in the location of the boat.\nThis ACR model is preferable to some other options because of the buoyancy. The beacon will float and emit a strong ling source to alert crews. Therefore, the beacon is more effective in extreme cases where boats may have sunk or capsized.\nBattery Operated Flares\nThose that don’t want to rely on the signals of a beacon for rescue may also like the idea of an additional flare. Flare guns have their benefits, but can also be difficult to use and unreliable. A battery operated handheld flare is a great alternative.\nThe Ocean Signal LED Flare uses a bright orange LED with 360-degree visibility, a range of 7 miles and a life of 6 hours. The settings on the device allow users to switch between a steady light source and a strobe effect. There is also the benefit that this model is waterproof, so safe for those in the water too.\nNight Vision Camera\nNight vision cameras are an important tool for safety on the boat. Keen sailors know how dark it gets out on the ocean at night, and how difficult it is to judge the water. A thermal night vision camera like the FLIR M232 Thermal Night Vision Camera can scan the scene for any potential hazard.\nSource: FLIR Systems\nA device like this is also helpful in case of accidents and persons overboard. Users will appreciate the 320×240 resolution, digital zoom and the ability to pan and tilt. Another helpful benefit of this model is the use of audible and visual alerts about navigation markers and obstacles.\nOnboard Security System\nThere are many boat owners that will turn to a device like this SirenMarine MTC system for the security features alone. The unit hooks up to alarm systems with motion sensors to alert users about potential break-ins. The great thing about the updated Connected Boat system is that the benefits go beyond a basic burglar alarm.\nThe device can also connect to the light and other sensors around the boat. Alerts inform users about water levels, engine temperatures and more. This is a versatile system with a lot of promise. The all-in-one hub approach is ideal for those that want a more streamlined tech system onboard the boat.\nSports and Entertainment\nFinally, there are plenty of marine electronics with a less serious purpose. Once sailors have navigated their course with the mapping tech and informed family members of their position, it is time for some fun.\nThose that are heading out for research purposes may need some extra gadget to help. Divers can take pictures and log progress with the right tech.\nThen there are entertainment systems for the voyage and long nights at sea.\nScuba diving computers\nExplorers that head out on the ocean for snorkeling expeditions will appreciate a good diving watch. However, consumers need to be aware that there is a big difference between the diver’s watches and scuba diver computers.\nDiving watches are good for keeping track of time while swimming and snorkeling but fail past a designated distance. Scuba watches like the Cressi Giotto Dive Computer provide clear data on the dive, with audible alarms and a log book.\nBuyers say that if the system had a compass it would be close to perfect. This system is much more than a water that happens to be waterproof. Instead, the computer is a sophisticated system with great potential.\nTravelers keen on swimming and diving may also want to have a camera to record what they see. Some scuba divers will do so for fun, others may find it helpful for data entries and research.\nThe GoPro is the camera that many consumers immediately think of for outdoor pursuits. The GoPro Hero 6 is waterproof up to 10 meters, so perfectly safe when snorkeling. It is also easy to use, mounts to other hardware and has the option of video capture.\nOthers will prefer waterproof DSLR-type devices with more options. Yet, none of these traditional options have the hands-free versatility of a GoPro.\nStereo Music System\nFinally, there are plenty of well-designed music systems that suit life on a multi-hull vessel on the open ocean. Music can help pass the time between destinations and life the spirits during bad weather.\nThe Garmin Fusion Entertainment system is a popular compact model from a well-known brand. This multifunctional CD and DVD player is a great addition to a cabin and pretty easy to use. There are some users that have had connection issues with the accompanying app.\nStill, a few bug fixes should sort that problem out in the future. Otherwise, this entertainment system is a good alternative to a more expansive home cinema set-up.\nSelecting products for Your voyage\nThere are quite a few factors to consider when choosing the best devices for a boat. The purpose is obviously the most important aspect. The item chosen has to suit the boat and have a practical purpose. The device also has to be easy to use, reliable and on budget.\nSailors need to remember that an impractical item left unused may take up valuable space, power, and finances. These problems lead to two other vital considerations when choosing electrical marine equipment.\nThe first issue is the power supply. Is there enough power on-board to power everything for the duration of the voyage? If not, is there a means of generating power, such as a solar generation system.\nThe second is the cost. Marine electronics don’t come cheap and some of the best options are thousands of dollars. Therefore, it pays to shop around for the best deal.\nAs long as sailors deck out their boat with researched purchases, there should be no problem.\nThe types of products and examples listed above are not the limit of the options available. There are other types of safety equipment, navigation tool, communication devices and entertainment options, click here.\nThis list simply provides a starting point and prime examples. Sailors looking to deck out their boat need to take the time to compare different models, price points, and specifications. No boat owners should settle for something that isn’t quite right, especially something as important as a GPS system, satellite phone, or beacon.","Types of Life-Saving Equipment Onboard Ships\nWhat happens when you are met with an accident on land? Rescue work begins in a couple of minutes (or hours depending on where you are) and soon experienced professionals will be rescuing and assisting you for further treatment.\nBut what if an accident occurs at sea? What is a cruise ship is in a distressing situation?\nWe’re talking about 6680 people on board (not to forget a 2200 people crew which equals to a total of around 8800, approx. 9000 people!) a cruise ship now (Let’s say onboard Symphony of the Seas- the world’s largest cruise liner or we all remember Costa Concordia, right?).\nHow do authorities even allow for such vessels to operate in notorious ocean waters where the waves and weather conditions are highly unpredictable? Does the ship have certain means to save lives in event of a disaster by the time they are aided by a rescue team?\nApart from the stringent rules and regulations followed during ship design and construction, it is mandatory to carry onboard certain life-saving equipment and appliances, which are helpful in saving the lives of people in the event of a disaster.\nThis article focuses on such life-saving equipment present onboard ships. One thing to keep in mind is not all ships have all the life-saving equipment mentioned herein.\nIt depends on the ship type, number of passengers (and crew) on board and the requirement pertaining to safety regulations that determine the type and quantity of life-saving equipment available.\nListed below are the main life-saving equipment used for evacuating and saving lives of people when a ship is under distress.\nThese are the primary life-saving equipment used when the crew and passengers are supposed to ‘abandon’ the ship and need out of water support. They must be available in sufficient quantity and support the required capacity and size such that the total number of persons on board can be evacuated from either port or starboard. ( This is done so that in case the ship is capsizing to one side, say port, the lifeboats can be lowered from the starboard side and everyone on board can be saved.)\nIn small vessels, such as harbour and river crafts open lifeboats and semi-enclosed lifeboats are used.\nFor all large vessels plying in oceans totally enclosed lifeboats are provided. This is done to ensure better protection against the weather and sea. They are fitted with small diesel engines for self-propulsion at a speed of around 6 knots and carry fuel for 24 hours of operation.\nIMO adopted Resolution MSC.402(96) highlighting the necessary requirements for the maintenance, repair, overhaul and thorough examination of lifeboats along with testing of their release gear.\nMoreover, following items ought to be thoroughly inspected wherever fitted and checked for satisfactory condition and operation-\n- Sprinkler System\n- Bailing System\n- The fender/skate arrangements\n- Rescue boat righting system\n- Boat structure condition (with a visual examination of external boundaries)\n- Engine and propulsion system\n- Manoeuvring System\n- Power Supply System\n- Air Supply System\nTypes of lifeboats used on ships\nMeasures to stop accidents on lifeboats\nLife Rafts and Inflatable Buoyant Apparatus\nThese are the secondary means of life-saving equipment on ship. Inflation of life rafts is done with carbon dioxide from the storage cylinder packed within the raft inside a container.\nThey may be launched via davits, cradles or free-fall racks (Davit launched rafts are launched usually from a single davit). They are first inflated on-board, boarded and then lowered into the water.\nLife rafts are subjected to a number of tests such as drop test, jump test, weight test, towing test etc. Some additional tests like damage tests, inflation test, pressure test, seam strength test etc. are peculiar only to inflatable life rafts.\nTo read more about each of these tests refer to Resolution MSC.81(70) Part 1-5\nThey are buoyant, rigid and ‘non-inflatable’ platforms with a similar function as life rafts but having downside of occupying more deck space. They are also subjected to various tests like material test, buoyancy test etc. to assess their reliability.\nThese are small, lightweight boats designed with the objective of rescuing people in distress and towing the survival crafts (such as life rafts and buoyant apparatus). They are designed as such to be launched in minutes and must remain stable when recovering a person in the water from either side of the boat. They are usually davit launched and come in different shapes and sizes.\nThe material used for construction is usually fibreglass with the addition of inflated rubber buoyancy chambers for extra stability.\nThey are subjected to tests like towing test, rigid rescue boat seating test, overload test, operation tests, righting tests, manoeuvrability etc.\nTo read in detail about these tests refer to Resolution MSC.81(70) Part 1-7\nA Guide to Different Types of Boats\nLife Preservers or Personal Floatation Devices\nLike rescue boats, personal floatation devices also come in a variety of shapes, sizes and designs. They may be either of solid buoyancy type with closed-cell foam or may be inflatable.\nInflation can be done either orally or carbon dioxide cartridge or combination of both. (We’ve all seen such life jackets present under the seats of every commercial airliner).\nLife Jackets are fitted with whistles to grab the attention of rescue people as well as a light which illuminates as soon as it touches the water to help in easy spotting people in distress. Alternatively, a chemical light stick and a reflective material may be used.\nThe life jackets are also subjected to various tests like temperature cycling, buoyancy, fire, stability, strength etc.\nTo know more about these tests, refer to Resolution MSC.81(70) Part 1-2\nThese are the common life-saving equipment present in all small and large vessels (You may have spotted them even on the sides of swimming pools). They are fitted around the perimeter of ship’s weather deck and are meant to be thrown rapidly to a person overboard.\nA man overboard is a common occurrence and these buoys provide the fastest aid for floatation in such a case.\nThe following table states the minimum number of lifebuoys (based on ship’s length) in accordance with SOLAS requirements.\nThe SOLAS requirements necessitate the below specifications ( for each lifebuoy)\n- The buoy’s outer diameter should not exceed 800mm with the inner diameter measuring at least 400mm\n- It should be installed as such to withstand free fall into the water above the ship’s waterline\n- Should weigh a minimum of 2.5 kg and be able to maintain 14.5 kg of iron afloat in freshwater for 24 hours.\n- It should be made of non-fading material\n- Burning shouldn’t be sustained with no melting after being enveloped in a fire for 2 seconds.\n- The lifebuoys are also subjected to a number of mandatory tests including temperature cycling test, drop test, test for oil resistance, the fire test, floatation test, strength test, test for operation with light and smoke signal and self-activating smoke signal tests.\nTo read more about each of these tests refer to Resolution MSC.81(70) Part 1-1\nWhy did people die despite wearing life jackets after the Titanic sank? The answer is hypothermia.\nWell, that’s where the survival suits become increasingly important. They are also called immersion suits and are used as protection overalls.\nTheir main function is to reduce the body-heat loss of a person in cold water and hence prevent death due to hypothermia.\nTypical designs allow the body temperature to prevent falling below 35 degree Celsius for six hours for a person in the water at 0 degrees Celsius.\nSome of the important requirements highlighted in SOLAS Chap.III/32 is\n- An immersion suit should be provided for every person onboard the ship. For ships other than bulk carriers it can be obviated provided the ship is continuously engaged in warm water voyages.\n- In case of a remotely located watch or work stations, an appropriate number of immersion suits should be provided at these locations itself.\n- They should be readily accessible with a clear indication of their positions\nTo read more about immersion suits, anti-exposure suits and thermal protective aids refer to Resolution MSC.81(70) Part 1-3\nWhen we have around 8800 people on board, it becomes a mammoth (and very important) task to communicate and alert each and every passenger and crew members of the situation and dictate the necessary steps and instructions to be followed.\nAny miscommunication or lack of communication can have disastrous ramifications. Hence these systems also form a part of life-saving equipment. All ships are fitted with general alarm systems so as to alert and summon the crew to their fire stations or boat stations.\nPassenger vessels are to be equipped with public address systems. Communication systems also include Portable Very High Frequency (VHF) Radios (commonly referred to as walkie-talkies) and are provided for emergency crew communications.\nEmergency Position Indicating Radio Beacons (EPIRB)\nHow does a rescue ship reach the exact location of a disaster? It’s quite obvious it cannot be done visually as the distances are too large for visual navigation. Then how does the rescue team reach the exact site of the disaster?\nThis happens with the aid of geographic co-ordinates sent via radio signals to a satellite receiver by EPIRB so that rescue efforts can be initiated at the earliest at the exact location of the disaster. These are buoyant electronic devices which float on water when a ship sinks and begin transmitting radio signals with geographic coordinates.\nTo know about the general requirements, distress function etc. of EPIRB click the below link referring to Resolution A.810(19)\nIf you’ve watched the movie Titanic you must’ve seen the crew sending distress signals with the use of flares. Distress Signals are typically parachute flares which can be spotted by nearby vessels and rescue personnel and determine the location of the ship in distress.\nResolution MSC.81(70) Part 1-4 outlines the necessary recommendations for various pyrotechnics.\nTests like temperature tests, water and corrosion resistance, handling etc. are of prime importance.\nSince we’ve talked about Titanic let’s shed some light on rocket parachute flares test and their specifications\nRocket parachute flares test\n3 rockets should be fired(vertically). It must be ensured that the parachute flare’s ejection happens after a height of 300 m. Also, the rate of descent should not exceed 5m/s and shall sustain burning for at least 40 seconds. They should also function efficiently when projected at an angle of 45 degrees to the horizontal.\nIn case the rocket is handheld, its recoil should be minimum\nIt should be established by laboratory tests that the average luminous intensity of flame material (which should burn uniformly) must be a minimum of 30,000 cd with the colour being vivid red.\nVideo: How to Use Maritime Distress Flares & Signals (Pyrotechnics)?\nRequirements Regarding PyroTechnics Put Forth by SOLAS\nFREE eBook: Introduction To Pyrotechnic\nSelf- Contained Breathing Apparatus\nWhat happens if a closed compartment is compromised due to malfunction of equipment and toxic gases such as carbon dioxide are present in it. How will a person diagnose and try to repair and minimise the damage when his breathing is compromised due to the presence of toxic gases.\nNo doubt the person will soon suffer asphyxiation which may lead to unconsciousness and ultimately death. To prevent such a mishap self-contained breathing apparatus are used by personnel entering a closed compartment infiltrated with toxic gases so as to facilitate oxygen supply in that hazardous environment.\nThese were important life-saving equipment which allows to save the lives of crew and passengers in case of an accident and make a ship safe from the point of view of safety of people.\nA ship sailing in the middle of the ocean has to be self-sustainable by all means as external aid might take hours to reach the designated site. And when we talk about self-sustainability, safety becomes one of the primary areas of concern.\nOver to you..\nWhich are other important life saving equipment which you think must be added to this list?\nLet’s know in the comments below.\nImages Reference- Wikipedia Creative Commons. All Images are under Public Domain unless otherwise stated\nGas Detectors. Oxygen and multi gas type.\nLife-Saving Device like Defibrillators"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:5be9549b-c924-419a-8295-1a0e13b72350>","<urn:uuid:02245253-d549-4051-a0fd-501167d9b86f>"],"error":null}
{"question":"What genetic variations affect cell development in Down syndrome and X-linked retinal disorders?","answer":"In Down syndrome, the genetic variation involves an extra copy of chromosome 21 in every cell's nucleus, resulting from nondisjunction during cell division. This extra chromosome is replicated in every cell of the body as the embryo develops. In X-linked retinal disorders, the genetic variation occurs specifically on the X-chromosome. In males, who only have one X-chromosome, a single copy of the mutated gene leads to the retinal condition. In females, who have two X-chromosomes, individual cells decide which X-chromosome to express, leading to varying degrees of disease severity. Both conditions involve chromosomal variations, but they affect cellular development in different ways.","context":["Understanding female IRD carriers\nResearch into X-linked inherited retinal diseases often focuses on males, but a study at CERA is aiming to make sure everyone benefits from newly developed treatments.\nInherited retinal diseases (IRD), a broad group of genetic eye conditions caused by changes in a person’s genetic code, do not always affect men and women in the same way.\n“A majority of female carriers of X-linked IRDs have a near-normal retina, while at the other end of the spectrum is quite severe disease, similar to what males with these conditions experience,” says Sena Gocuk, optometrist, CERA researcher and PhD candidate at the University of Melbourne.\nThe condition of many female carriers can often be so mild that it is only identified after a male relative is diagnosed with an inherited disorder.\nThis lesser severity has led past research to focus only on males with these diseases, but a new study at CERA is seeking help change that.\nChromosomes, the thread-like structures located inside the nucleus of living cells, contain the instructions for how those cells should operate.\nThey also determine a person’s biological sex; females have two X-chromosomes and males have one X-chromosome and one Y-chromosome.\nFor IRDs caused by changes in the DNA code of eye genes found on the X-chromosome, the sexes can be affected in very different ways.\n“Conditions like choroideremia, and some forms of retinitis pigmentosa, are X-linked conditions, meaning they are caused by faults in the X-chromosome,” says Gocuk.\n“For males, who only have one X-chromosome, a single copy of the mutated gene means that they will develop the retinal condition.”\nFor females, who have two X-chromosomes, an X-linked retinal condition may present quite differently – with some women having no symptoms, and others noting changes in their vision.\n“If a female has one mutated and one normal gene, individual cells decide which of the two X-chromosomes are expressed,” says Gocuk.\n“Because of that, you get a wide spectrum of disease severities.”\nSurvey and study\nThe study involves clinical visits, where Gocuk will learn more about the eye health of female carriers of these X-linked retinal diseases.\nParticipants will receive genetic testing and counselling, undergo a comprehensive eye examination to assess their retinae, and also have the opportunity to continue in the study to monitor how their vision changes over time.\nIn another part of her study, Gocuk is asking mothers, sisters and daughters of males with X-linked IRDs to complete an online survey, in order to develop a better understanding of how the condition affects women, and also their opinion of whether future gene therapies might potentially benefit them.\n“Questionnaires that have looked at female carriers of other genetic conditions have found a lot of women experience shock, guilt and anxiety when their male relatives are diagnosed,” says Gocuk.\n“One of my participants became quite teary when speaking with me about her experiences, because she has a number of male relatives with retinitis.\n“She knows that this research might not benefit her or her children, but she wants to contribute.\n“It’s been quite rewarding to hear these stories.”\nGocuk is a recipient of the University of Melbourne’s Harold Mitchell Postgraduate Travelling Fellowship, and will use the funds to travel to the University of Oxford to examine 10-year data from a UK longitudinal study as part of the project.\nHer work is also running alongside another ongoing study at CERA, surveying people who are living with an IRD.\nGocuk says she hopes her research might help lay the groundwork to see whether more people can benefit from emerging genetic treatments.\n“I would be pleased if we were able to determine if some of these women would be eligible for emerging treatments, like gene therapy.\n“It would be quite rewarding, and potentially life changing for this group of people.”\nIf you think you’re a female carrier of an X-linked inherited retinal disease or would like further information, please email Sena Gocuk: email@example.com.","What causes Down syndrome?\nIn every cell in the human body there is a nucleus, where genetic material is stored in genes. Genes carry the codes responsible for all of our inherited traits and are grouped along rod-like structures called chromosomes. Normally, the nucleus of each cell contains 23 pairs of chromosomes, half of which are inherited from each parent.\nDown syndrome is usually caused by an error in cell division called \"nondisjunction.\" Nondisjunction results in an embryo with three copies of chromosome 21 instead of the usual two. Prior to or at conception, a pair of 21st chromosomes in either the sperm or the egg fails to separate. As the embryo develops, the extra chromosome is replicated in every cell of the body. This type of Down syndrome, which accounts for 95% of cases, is called Trisomy 21.\nThe two other types of Down syndrome are called mosaicism and translocation. Mosaicism occurs when nondisjunction of chromosome 21 takes place in one-but not all-of the initial cell divisions after fertilization. When this occurs, there is a mixture of two types of cells, some containing the usual 46 chromosomes and others containing 47. Those cells with 47 chromosomes contain an extra chromosome 21. Mosaicism accounts for about 1% of all cases of Down syndrome. Research has indicated that individuals with mosaic Down syndrome may have fewer characteristics of Down syndrome than those with other types of Down syndrome. However, broad generalizations are not possible due to the wide range of abilities people with Down syndrome possess.\nWhat is Alzheimer’s disease?\nAlzheimer’s disease is the most common form of dementia in the elderly (accounting for 60-80% of patients) and in people with Down syndrome. Currently, in the United States over 5.3 million people have the disease. Alzheimer’s disease is a dementia that includes the loss of memory and other thinking skills. People diagnosed with dementia usually show:\n- Progressive loss of the ability to generate coherent speech or understand written or spoken language.\n- Difficulties recognizing or identifying objects (even with corrected vision).\n- Decline in ability to do motor activities (even if one understands what needs to be done).\n- Problems thinking abstractly, making sound judgments, planning and carrying out complex tasks.\n- Importantly – these changes in thinking must be severe enough to interfere with daily life.\nThere are several warning signs of Alzheimer’s disease but it is important to remember that change in function is the most important indicator:\n- Memory loss that disrupts daily life.\n- Problems solving problems or in planning.\n- Difficulty completing familiar tasks at home, at work or at leisure.\n- Confusion with time or place.\n- New problems with words in speaking or writing.\n- Losing or misplacing things.\n- Withdrawal from work or social activities.\n- Changes in mood or personality.\nWhat are the brain changes that happen with Alzheimer’s disease?\nSeveral changes must occur in the brain for a final diagnosis of Alzheimer’s disease. Two types of neuropathology are thought to cause dementia in this disease including neurofibrillary tangles and beta-amyloid plaques.\nInside the brain are nerve cells (neurons). Inside of neurons there is a normal protein called tau that is important for proper functioning. In Alzheimer’s disease, tau becomes abnormal (hyperphosphorylated) and this leads to a twisting up of long filaments (strings) that fill up the neuron and prevent it from functioning properly. Eventually, neurons die from these changes leading to many disconnections between neurons and between different parts of the brain. When this happens, the clinical signs of dementia can be seen.\nPlaques form in between neurons in the brain as big clumps of a protein called beta-amyloid. This beta-amyloid protein is very toxic to neurons and in high enough concentrations can change how they work and also lead to their death. The beta-amyloid protein is cut out of a longer protein called beta-amyloid precursor protein.\nWhy are people with Down syndrome more vulnerable to Alzheimer’s disease?\nThe gene for the beta-amyloid precursor protein that is cut to make beta-amyloid protein is on chromosome 21, which is present in triplicate in most people with Down syndrome. This means that people with Down syndrome are making more of the precursor protein than people without Down syndrome and leads them to being more vulnerable to the development of Alzheimer’s disease. Also, the age of onset of Alzheimer’s disease in adults with Down syndrome is earlier than those without Down’s and can start as young as in the 40’s. As they get older, the risk of getting Alzheimer’s disease gets higher.\nNot all adults with Down’s syndrome develop dementia\nImportantly, not all older adults with Down syndrome develop dementia despite making too much beta-amyloid protein. This means that some people may tolerate or compensate for these changes in the brain. This also means they can still function well on a day to day basis without the clinical signs of dementia. If, through research, we can learn how this happens, then it may be possible to develop ways to prevent or treat Alzheimer’s disease in people with Down syndrome. This is one of the long term goals of our longitudinal aging study – to discover new approaches to prevent Alzheimer’s disease in Down syndrome as people get older."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:10ff4be0-4275-4b2e-832c-f7836c13208b>","<urn:uuid:82b41c25-f881-43a0-b550-645fc033c8d4>"],"error":null}
{"question":"How can someone use a burial shroud with different casket options?","answer":"For natural burial with no casket, a burial shroud with carrying handles is recommended to help carry the body. When using a casket, the shroud is recommended but not necessary, and handles aren't needed since the casket has its own. With a willow carrier, the shroud can be used without handles since the carrier is designed with handles for this purpose. For cremation, the shroud is not necessary when using a casket, can be used with a willow carrier without handles, or can be used alone with handles recommended for carrying.","context":["This lovely and eco-friendly Biodegradable Casket for Burial or Cremation in Woven Willow is sustainably crafted by hand and ships free within the USA. Available to the public, this is a beautiful, affordable, and earth-conscious resting place for your loved one.\nYou can use our biodegradable willow caskets for the funeral or memorial service followed by a natural ground burial or cremation. Each casket includes a full length lid, and you can also choose to add a 3/4 lenth lid to allow viewing of the body during the service or a private family visitation. There are no metal components in the casket or shroud, so these are all suitable for cremation or natural ground burial.\nThe Willow Casket is lined with a natural unbleached cotton interior, matching pillow and privacy shield. Available in two sizes, 5' 9\" or the larger 6' 5\". These sizes are suitable for an individual up to 5'9\" or 6'5\" in height, respectively.\n- Crafted by hand from sustainable and eco-friendly willow\n- 100% natural, biodegradable, and eco-friendly\n- Rigid base and six sturdy carrying handles\n- Tested to carry a weight of up to 350 lbs\n- Lined with natural cotton and includes matching pillow and privacy shield\nCasket sizes and dimensions\n- 5' 9\" size inside dimensions: 70\"L x 20\"W x 12\"H\n- 5' 9\" size outside dimensions: 77\"L x 25\"W x 15\"H\n- 6' 5\" size inside dimensions: 77\"L x 24\"W x 12\"H\n- 6' 5\" size outside dimensions: 85\"L x 30\"W x 16\"H\nShipping & delivery\n- Ships free to any address in the continental USA\n- You can ship to your home or to the funeral home/cemetery\n- Please allow up to 5 business days for delivery\n- Expedited shipping can be arranged, call 877-900-5309\n- Please note that rush delivery is very expensive since this is a large item, and delivery times cannot be 100% guaranteed\n- Casket comes in two sizes, for individuals up to 5'9\" tall or 6'5\" tall\n- Includes matching full-length lid, optional 3/4 length viewing lid\n- Optional burial shroud to wrap the body - see details below\n- You can personalize the casket with an 8\"x5\" or 12\"x3\" engraved bamboo plaque\n- Plaque includes up to 3 lines of text, 40 characters max per line, choice of 5 fonts\nShroud options and details\n- Natural, eco-friendly bamboo, which is a highly sustainable resource\n- Available in two sizes, large and XL\n- Large: Approx. 112” long x 73” wide (ideal for just about anyone)\n- X-Large: Approx. 118” long x 78” wide (designed for individuals well over 6'5\")\n- Available with carrying handles or without\n- See usage suggestions below\nShroud use suggestions\n- Shroud is ideal for natural burial with no casket - handles are recommended to help carry the body.\n- Shroud is recommended (but not necessary) for burial with casket - no handles needed, as the casket will be carried and already has handles.\n- Shroud is recommended for burial with willow carrier - no handles needed, as the willow carrier has handles and is designed for this function.\n- Shroud is not necessary for cremation when casket is used\n- Shroud can be used for cremation with a willow carrier - no handles needed\n- Shroud can be used for cremation with no casket/carrier - handles recommended to help carry.\n- Our eco-friendly caskets are crafted by skilled weavers as a small cottage industry, not in factories. They are created from sustainable materials such as fast-growing willow, seagrass or bamboo which don’t require heavy machinery for harvesting, giving these funeral products a small carbon footprint. Burial of these caskets also help reduce your own carbon footprint, as most caskets you find at the funeral home feature metal components and/or heavy chemical finishes."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"format_constrained"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:6b231a81-0733-4986-b16d-7fedc2c77b15>"],"error":null}