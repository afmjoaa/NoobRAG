{"question":"How do photographers and Norman Rockwell draw inspiration from their surroundings when creating visual stories?","answer":"Both photographers and Norman Rockwell drew significant inspiration from their immediate surroundings, but approached it differently. Photographers use their surroundings to create stories through visual language and cultural meaning, capturing both intentional narratives and simple daily life documentation. Meanwhile, Rockwell was specifically influenced by his family, friends, neighbors, and community - particularly in Arlington, Vermont, where locals would often serve as models for his work. His surroundings inspired him to paint scenes of everyday American life for over 60 years, eventually expanding to include broader social themes. Both creative approaches transform ordinary moments into meaningful visual narratives, though through different artistic mediums.","context":["What is storytelling in photography?\nStorytelling in photography, or storytelling in any visual medium for that matter, is at the heart of any good visual piece.\nBut what exactly is it, and how does it differ to how we know storytelling to be traditionally told—through words?\nWell, the good thing is that stories, both visual and auditory, are an exercise of the imagination.\nWhen you’re told a story, it’s a recount of events that have taken place. Someone telling you what happened, you imagining that story in your mind’s eye, not only thinking about what might have happened to the storyteller but also infusing your own previous experiences with it as well.\nThis is important because good storytelling isn’t a one-shot affair. It’s not just a piecing together of what happened; it’s more than that.\nStorytelling is also a recall of our own experiences and imagination, fused with a recount of events.\nIt’s our own experiences and imagination that give emotion to the story, and it’s the story that invites us to do that.\nNot everyone sees the same story\nBut the thing is because we all have our own unique experiences, our own unique imagination, regardless of whether a story is told directly, or indirectly, no two interpretations will ever be precisely the same.\nEven after watching the same movie as someone else, no two people will have exactly, scene-for-scene, line-for-line, the same opinion as another.\nThis is an important fact to remember as it pertains to photography specifically, as many people think that stories are a “one-and-done” scenario; that every image results in the same message being told to everyone, and that’s just not true.\nThe role of the photographer is to provide a message to the viewer; a suggestion of how to interpret the incoming stimuli.\nThe role of the viewer is to understand that image and infuse emotion, feeling, meaning, and value; if it calls for it successfully.\nWhat is a successful story?\nBetter than asking what is a “good” story, is asking what is a “successful” story.\nHow successful a story might be determined by how much emotion it invokes, or how unanimoustory-tells the overall consensus of what is being conveyed, how hard it makes people think, or how much it stops people in their tracks, etc.\nIt’s the same difficulties as we face when we think about what makes a “good” image; what exactly are the constituent parts of what makes an image “good”, and what even is “good”, anyway?\nAll these things are ultimately up to the photographer to decide. Still, if we take a common success metric of a successful story to be one where viewers understand the image and can form a story on their own, we can make good progress towards how to craft more compelling stories in our images.\nHow to story-tell in photography\nAnd so the question arises; how do we story-tell in photography successfully?\nWell, for better or for worse, we have only two ways to do it in photography.\nThose two ways are Single images and Multiple images and both have their places and times.\nStorytelling in photography with single images\nThe main focus of single images is what I call insinuated stories.\nThat is, stories that are presented, but left with an open interpretation; an invitation for you as the viewer to create the story yourself through, aided by your own experiences and imagination.\nWith a single image, there is no setup. There is simply what is presented to you, no more, no less. As such, the use of strong visual language cues helps photographers direct viewers into a particular path of emotion and feeling.\nVisual language is the way we use single or a combination of elements that come with pre-established cultural meaning.\nFor example, the use of leading lines tells viewers which part of the image to focus on. A small figure in a giant landscape can spark a feeling of grandiosity or loneliness. The use of the colour red sparks passion, excitement, and action.\nIt’s through the masterful combination of these visual language cues that the photographer can guide the viewer into a narrower path of understanding, usually leading to more thoughtful and thus more compelling images that say more than your average snapshot.\nThis is what we call “vision” in photography; the ability to see these elements come together in the field and to capture them successfully.\nStorytelling in photography with multiple images\nUsing multiple images is a little more straight-forward.\nThe focus of using multiple images for storytelling in photography can be more around the idea of a narrative arc.\nEvery good story at a baseline has a beginning, middle, and end, and it’s our job as photographers to answer the question: how might we best show this arc from end to end via a series of images?\nThere’s a lot of leeway here, and we have a lot of traditional narrative arcs to draw from: the hero’s journey, rags to riches, riches to rags.\nBut it could even be as simple as introduction, climax, resolution.\nFor example, if you wanted to tell the story about your first time visiting a new location, you could set up your sequence in the following way:\n- Shot 1: Tight shot of packing bags\n- Shot 2: Medium shot of navigation in the car on the way to the destination\n- Shot 3: Medium shot of walking at the trailhead\n- Shot 4: Epic, wide drone shot at the summit\n- Shot 5: Dronie of you and your buddies during sunset\nThe possibilities are endless.\nBut, not every image needs a deliberate story\nThat is, you as the photographer don’t need to be intentional to tell a story with every image you create.\nHowever, that won’t stop viewers from creating one anyway. We’re human, that’s just what we do. We’re wired for story.\nWhat we don’t need to do, however, is to create tremendous pressure on ourselves to purposefully tell stories with every shutter we click.\nThe art of documenting even simple, daily life, is one of photography’s purest pleasures. And a lot of times, there is no intention behind these images than to simply record what’s in front of us. That, in and of itself, is the “story”.\nThere’s absolutely nothing wrong with this; in fact, most of my images are these types of images.\nBut of course, that doesn’t mean that we can’t also make those images the best we possibly can, using all the techniques at our disposal to preserve our memories in the best shape they could potentially be.\nStorytelling is an exercise in imagination.\nThe stories we are told require our imagination to both create and to fill the space between.\nAs such, no two interpretations of the same story between two different people will ever truly be exactly the same, because we all come from different experiences and have different imaginations.\nUnlike mediums like video, photography and other 2-dimensional, still, visual mediums have just two ways to story-tell…\nA single image provides an opportunity for an insinuated story; one where the imagination of the viewer has more of a part to play in understanding the image and assigning an emotion or feeling to it.\nMultiple images provide photographers with an opportunity to story-tell around a narrative arc; to tell a story that flows from A to B.\nIn both instances and in all of photography, we use Visual language techniques and the power of pre-established cultural meanings to help “say more” in our images and gently coerce our viewers into feeling a particular way, or experiencing a specific emotion generated from their imagination.","Meet Norman Rockwell, American Illustrator\nThis lesson will introduce students to Norman Rockwell, a professional artist and illustrator who had been inspired by the daily events in his everyday life in America. Images from books, the internet, written articles and a group discussion will be used to demonstrate how the American culture influenced Norman Rockwell in communicating his ideas as visual narratives.\nThis lesson is designed for one or two 20 minute class periods.\nEnduring Understandings/ Essential Questions:\n- Norman Rockwell is a well-known, famous American illustrator.\n- Illustrators are visual storytellers; they are artists and create pictures to tell a story.\n- Rockwell’s paintings show what American life was like from the early 1900’s to the 1970’s. His paintings progress from the portrayal of ordinary, everyday events to social inequalities.\n- We can learn about Norman Rockwell by reading books, talking about and looking at artwork. We can see authentic pieces of art created by Rockwell at museums, studying his paintings and visiting websites that have information about him.\n- Norman Rockwell was inspired by his surroundings (nature, politics but also ordinary events of his friends and neighbors.)\n- Who is Norman Rockwell?\n- What is an illustrator?\n- What was the subject of Norman Rockwell’s artwork? What was he telling a story about in his paintings?\n- How might you learn more about Norman Rockwell?\n- What things influenced Norman Rockwell’s work?\n- Four Freedoms\n- This lesson is designed for one or two 20 minute class periods.\n- National Standards for Visual Arts\n- Advertisements; Art critics; Art Director; Book publishers; Boy Scouts; Charles Dickens; Culture; Familiar; Golden Age of Illustration; Illustrator; Museums; Saturday Evening Post; Self-Portrait; Symbol\n- Students will share any prior knowledge regarding Norman Rockwell that they may have.\n- Students will discuss the information presented in text(s) about Norman Rockwell and selected illustrations painted by Norman Rockwell.\n- Using prior knowledge, the information from the book(s) and internet resources, the students will tell about Norman Rockwell, his paintings and the stories illustrated in the paintings.\n- Students will listen attentively to one another as they share personal responses about the specific artworks.\nNorman Rockwell was born in New York City in 1894. He was a super-skinny kid and was terrible at sports, but he always knew that he wanted to be an artist. His paintings would tell a story without words. His work was influenced by family, friends, neighbors and vacations. He worked for more than sixty years painting scenes of people in their everyday life.\nRockwell was a teenager when he was hired to work as the art director of Boys’ Life, the official magazine of the Boy Scouts of America. In this job, Rockwell had to make all of the decisions about how the magazine should look. When he was 22 years old, Rockwell painted his first cover for a popular American magazine, The Saturday Evening Post, and continued to paint 323 covers over the next 47 years.\nIn 1916, the same year his work was on the cover of The Saturday Evening Post, Rockwell married Irene O’Connor. Their marriage lasted 14 years, then they divorced. In 1930 he married a teacher, Mary Barstow. Norman and Mary Rockwell had three sons, Jarvis, Thomas, and Peter. Nine years after they were married, the family moved to the small town of Arlington, Vermont. The community of people in Arlington were supportive of Norman Rockwell and his work. Their neighbors and friends were often eager to be models for his work.\nIn 1943, President Franklin Roosevelt’s spoke to the American people in a speech called “The Four Freedoms.” Norman Rockwell felt the President’s message was important and wanted to illustrate it. The Four Freedoms paintings were Norman Rockwell’s interpretations of the Freedom of Speech, Freedom to Worship, Freedom from Want, and Freedom from Fear. These four paintings became tremendously popular.\nThe Rockwell family moved to Stockbridge, Massachusetts in 1953. Sadly, six years later his wife, Mary, died unexpectedly. Shortly after, Rockwell met a retired teacher, Molly Punderson, at the library and they became close friends and eventually married. It was at this time that Rockwell began to paint pictures illustrating some of his most worrisome concerns and deepest interests, including civil rights, poverty, and the exploration of space.\nIn 1977, the President presented Rockwell with the nation’s highest civilian honor, the Presidential Medal of Freedom! Rockwell became one of America’s all-time favorite artists before he died in 1978.\nAmerican Chronices: The Art of Norman Rockwell\nNorman Rockwell Museum\n- Getting to Know the World’s Greatest Artist: Norman Rockwell by Mike Venezia\nAdditional Teaching Resources:\nThe Norman Rockwell Museum at Stockbridge by The Norman Rockwell Museum\nNorman Rockwell, My Adventures as an Illustrator by Norman Rockwell\nNorman Rockwell: Behind the Camera by Ron Schick\nNorman Rockwell’s America by Christopher Finch\nNorman Rockwell’s Four Freedoms: Images that Inspire a Nation by Stuart Murray and James McCabe\nAmerican Chronicles: The Art of Norman Rockwell by Linda Szekely Pero\nNorman Rockwell’s Counting Book by Gloria Tabor\nNorman Rockwell: Storytelling with a Brush by Beverly Sherman\nMy Adventures as an Illustrator by Norman Rockwell\nA Rockwell Portrait: An Intimate Biography by Donald Walton\nEnduring Ideals: Rockwell, Roosevelt & the Four Freedoms, edited by Stephanie Haboush Plunkett and James J. Kimble\n- Ask the students to listen attentively to one another as they share personal responses throughout the lesson.\n- Read Getting to Know the World’s Greatest Artists: Norman Rockwell by Mike Venezia to the students, pausing to reinforce information and allowing for questions to be asked when something is unclear.\n- Show the students Norman Rockwell’s Triple Self Portrait and ask the students to respectfully share with the group what they see in the image. The class will collectively take a visual inventory. As each student contributes, restate their observation. You might be able to elaborate on what they have said to add more visual detail or you might ask them for clarification. You might encourage them to look more closely and carefully.\n- Ask the students to think about themselves and what objects they would include in their own self-portrait to help show others who they are as individuals. Distribute paper and ask each student to draw their own self portrait including at least one symbol about themselves.\n- Students will be evaluated on demonstrating appropriate listening and speaking skills during their participation in the group discussion.\n- Students will be evaluated on their participation in analyzing the text read to them.\n- Students will be evaluated on their participation in analyzing the visual imagery through informal checks of understandings.\n- Understand that people from different places and times have made art for a variety of reasons.\n- Compare images that represent the same subject.\n- Describe what an image represents.\n- Interpret art by categorizing subject matter and identifying the characteristics of form.\n- Interpret art by identifying subject matter and describing relevant details."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:345e3be9-c9ef-4efc-aefb-9483c3fb1690>","<urn:uuid:99975ef2-b2da-482d-9f98-6938b49430dc>"],"error":null}
{"question":"I'm researching 20th century scientists - what are the key differences between Waddington's approach to developmental biology and current discussions about biological individual mechanisms?","answer":"Waddington approached developmental biology by focusing on gene regulatory products and mechanisms underlying development, as demonstrated through his work on Drosophila wing mutations and cell phenotypes. He introduced concepts like canalization and genetic assimilation. In contrast, current discussions about biological individuality focus on identifying 'individuating mechanisms' - traits that increase among-unit selection or decrease within-unit selection. While Waddington was primarily concerned with how genes regulate development within organisms, modern debates about biological individuality examine broader questions about what constitutes an individual across different biological scales and how selection operates at various levels.","context":["Pierrick Bourrat has reviewed Scott Lidgard and Lynn Nyhart’s book Biological Individuality: Integrating Scientific, Philosophical, and Historical Perspectives for Notre Dame Philosophical Reviews.\nDerek Skillings from University of Bordeaux/CNRS has a new article at Aeon about biological individuality:\nFor millennia, naturalists and philosophers have struggled to define the most fundamental units of living systems and to delimit the precise boundaries of the organisms that inhabit our planet. This difficulty is partly a product of the search for a singular theory that can be used to carve up all of the living world at its joints.\nSkillings reviews the deep historical roots of the question, touching on the views of Charles Darwin and his grandfather, both Huxleys (T. H. and Julian), Herbert Spencer, and other 19th and early 20th century thinkers, as well as some more recent authors, including Daniel Janzen and Peter Godfrey Smith.\nA couple of weeks ago, I indulged in a little shameless self-promotion, writing about my new chapter on volvocine individuality in Biological Individuality, Integrating Scientific, Philosophical, and Historical Perspectives. Now two graduate students in the Michod lab at the University of Arizona, Erik Hanschen and Dinah Davison, have published their own take on volvocine individuality in Philosophy, Theory, and Practice in Biology (“Evolution of individuality: a case study in the volvocine green algae“). The article is open-access, and Hanschen and Davison are listed as equal contributors.\nAs I mentioned previously, I have a chapter in the newly published book Biological Individuality, Integrating Scientific, Philosophical, and Historical Perspectives. The chapter was actually written nearly five years ago, but things move more slowly in the philosophy world than that of biology. Finally, though, both the print and electronic versions are now available; here is the electronic version of my chapter. The book currently has no reviews on Amazon, so if you want to give it a read, yours could be the first. If you’re interested in current and historical views on individuality, there is a lot of good stuff in here, including contributions by Scott Lidgard & Lynn Nyhart, Beckett Sterner, Andrew Reynolds, Snait Gissis, Olivier Rieppel, Michael Osborne, Hannah Landecker, Ingo Brigandt, James Elwick, Scott Gilbert, and Alan Love & Ingo Brigandt.\nIn the Major Transitions class, the students keep pointing out that the transitions on Maynard Smith and Szathmáry’s list come in two flavors with very different properties. Sure, there are some important similarities between multicellular organisms and social insects, but they are quite different from cellular slime molds and the conspiracy of prokaryotes that make up eukaryotes.\nThe question of what constitutes a biological individual is intimately entangled with questions about levels of selection. Many authors implicitly or explicitly treat individuals as units of evolution or some variation on this theme. A recent appreciation for the complexity of bacterial biofilms has led to comparisons with multicellular organisms. A recent paper by Ellen Clarke bucks this trend by claiming that multispecies biofilms are not evolutionary individuals.\nAt the Philosophy of Science Association meeting in Chicago, I attended an interesting talk by Karen Kovaka, “Biological Individuality and Scientific Practice” (the abstract of her talk is here). Now the paper arising from that talk is out in the journal Philosophy of Science. It argues that biologists do not need to resolve the question of what constitutes an individual in order to do good empirical work, with which I agree. She contrasts two views of the relationship between individuality and scientific practice, the “quality dependence” account and the “content sensitivity” account:\nQuality dependence: the quality of empirical work in biology depends in part on the resolution of the debate about biological individuality…\nContent sensitivity: Biologists’ understanding of biological processes is sensitive to the individuals they take to be participants in those processes.\nPreviously, I introduced Beckett Sterner’s new paper comparing and critically evaluating the views of Ellen Clarke and Peter Godfrey-Smith on biological individuality. For Clarke, individuality is recognized by the presence of ‘individuating mechanisms’: traits that increase the capacity for among-unit selection or decrease the capacity for within-unit selection. Godfrey-Smith recognizes different kinds of individuals, but at a minimum, populations of individuals must have Lewontin’s criteria of phenotypic variation, differential fitness, and heritability of fitness, i.e. be capable of adaptive change.\nIn grad school I wound up hanging around with John Pepper (yeah, Dr. Pepper) a good bit. I think I disagreed with him more than I agreed with him, sometimes to the point of exasperation, but conversations with him were never boring.\nOne of John’s most annoying refrains was “is it an organism?” I was studying (and still study) a group of algae for which this question can be genuinely confusing. Most people would say a Chlamydomonas cell is a single-celled organism, and most would agree that Volvox is a multicellular organism, but what about the four-celled species Tetrabaena? A four-celled organism or a collection of four single-celled organisms? What about an undifferentiated colony of 32 cells, such as Eudorina? Or Pleodorina, which is around the same size but with two cell types? Somewhere between a unicellular ancestor and Volvox, a new kind of individual emerged. Among the extant species*, where do we draw the line between organisms and groups of organisms, or can we (or should we) draw a line at all?\nOne of my pet topics is the concept of biological individuality, which I’ve written about quite a lot here. One question that comes up often, in fact what I initially asked Dr. Pepper when he used to carry on about it, is why does it matter?\nSo much ink has been spilled trying to define what an individual is, in the peer-reviewed literature of philosophy and of biology, as well as several books dedicated to the topic. What is the point of all this, to justify so much intellectual effort and so many dead trees?","Conrad Hal Waddington\n|Birthplace:||Evesham, Worcestershire, UK|\n|Death:||Died in Edinburgh, City of Edinburgh, UK|\n|Managed by:||Carlos Federico (Cantarito) Bung...|\nMatching family tree profiles for Conrad Hal Waddington\nAbout Conrad Hal Waddington\nFrom Wikipedia, the free encyclopedia\nConrad Hal Waddington Born 8 November 1905 Evesham, Worcestershire, England Died 26 September 1975 Edinburgh, Scotland\nFields Developmental biology, Genetics, Paleontology Institutions Cambridge University, Christ's College University of Edinburgh Wesleyan University Centre for Human Ecology Alma mater Cambridge University Known for Epigenetic landscape, canalisation, homeorhesis, genetic assimilation, chreod Influences Alfred North Whitehead Influenced Jean Piaget, Sanford Kwinter, Gregory Bateson, Margaret Mead\nConrad Hal Waddington CBE FRS FRSE (1905–1975) was a developmental biologist, paleontologist, geneticist, embryologist and philosopher who laid the foundations for systems biology. He had wide interests that included poetry and painting, as well as left-wing political leanings.\n* 1 Life * 2 Epigenetic landscape * 3 Waddington as an organiser * 4 References * 5 Selected works o 5.1 Books o 5.2 Papers * 6 External links\nWaddington, known as \"Wad\" to his friends and \"Con\" to family, was born to Hal and Mary Ellen (Warner) Waddington, 8 November 1905. Until nearly three years of age, Waddington lived with his parents in India, where his father worked on a tea estate in the Wayanad district. In 1910, at the age of four, he was sent to live with family in England including his aunt, uncle, and Quaker grandmother. His parents remained in India until 1928. During his childhood, he was particularly attached to a local druggist and distant relation, Dr. Doeg. Doeg, who Waddington called \"Grandpa\", introduced Waddington to a wide range of sciences from chemistry to geology. During the year following the completion of his entrance exams to university, Waddington received an intense course in chemistry from E. J. Holmyard. Aside from being \"something of a genius of a [chemistry] teacher,\" Holmyard introduced Waddington to the \"Alexandrian Gnostics\" and the \"Arabic Alchemists.\" From these lessons in metaphysics, Waddington first gained an appreciation for interconnected holistic systems. Waddington reflected that this early education prepared him for Alfred North Whitehead's philosophy in the 1920s and 30s and the cybernetics of Norbert Wiener and others in the 1940s.\nHe attended Clifton College and Sidney Sussex College, Cambridge. He took the Natural Sciences Tripos, earning a First in Part II in geology in 1926. He took up a Lecturership in Zoology and was a Fellow of Christ's College until 1942. His friends included Gregory Bateson, Walter Gropius, C. P. Snow, Solly Zuckerman, Joseph Needham, and J. D. Bernal. His interests began with palaeontology but moved on to the heredity and development of living things. He also studied philosophy.\nDuring World War II he was involved in operational research with the Royal Air Force and became scientific advisor to the Commander in Chief of Coastal Command from 1944 to 1945. After the war he became Professor of Animal Genetics at the University of Edinburgh. He would stay at Edinburgh for the rest of life with the exception of one year (1960–1961) when he was a Fellow on the faculty in the Center for Advanced Studies at Wesleyan University in Middletown, Connecticut. His personal papers are largely kept at the University of Edinburgh library.\nWaddington was married twice. His first marriage produced a son, C. Jake Waddington, professor of physics at the University of Minnesota, but ended in 1936. He then married Justin Blanco White, daughter of the writer Amber Reeves, with whom he had two daughters, mathematician Dusa McDuff and anthropologist Caroline Humphrey.\nIn the early 1930s, Waddington and many other embryologists looked for the molecules that would induce the amphibian neural tube. The search was, of course, beyond the technology of that time, and most embryologists moved away from such deep problems. Waddington, however, came to the view that the answers to embryology lay in genetics, and in 1935 went to Thomas Hunt Morgan's Drosophila laboratory in California, even though this was a time when most embryologists felt that genes were unimportant and just played a role in minor phenomena such as eye colour.\nIn the late 30's, Waddington produced formal models about how gene regulatory products could generate developmental phenomena, showed how the mechanisms underpinning Drosophila development could be studied through a systematic analysis of mutations that affected the development of the Drosophila wing (this was the essence of the approach that won the 1995 Nobel prize in medicine for Christiane Nüsslein-Volhard and Eric F. Wieschaus). In a period of great creativity at the end of the 1930s, he also discovered mutations that affected cell phenotypes and wrote his first textbook of developmental epigenetics, a term that then meant the external manifestation of genetic activity.\nWaddington also coined other essential concepts, such as canalisation, which refers to the ability of an organism to produce the same phenotype despite variation in genotype or environment. He also identified a mechanism called genetic assimilation which would allow an animal’s response to an environmental stress to become a fixed part of its developmental repertoire, and then went on to show that the mechanism would work. He thus demonstrated that the ideas of inheritance put forward by Jean-Baptiste Lamarck could, in principle at least, occur.\nIn 1972, Waddington founded the Centre for Human Ecology.\nWaddington's epigenetic landscape is a metaphor for how gene regulation modulates development. One is asked to imagine a number of marbles rolling down a hill towards a wall. The marbles will compete for the grooves on the slope, and come to rest at the lowest points. These points represent the eventual cell fates, that is, tissue types. Waddington coined the term Chreode to represent this cellular developmental process. This idea was actually based on experiment: Waddington found that one effect of mutation (which could modulate the epigenetic landscape) was to affect how cells differentiated. He also showed how mutation could affect the landscape and used this metaphor in his discussions on evolution—he was the first person to emphasise that evolution mainly occurred through mutations that affected developmental anatomy.\nWaddington as an organiser\nWiki letter w cropped.svg This section requires expansion.\nWaddington was very active in advancing biology as a discipline. He contributed to a book on the role of the sciences in times of war, and helped set up several professional bodies representing biology as a discipline.\nA remarkable number of his contemporary colleagues in Edinburgh became Fellows of the Royal Society during his time there, or shortly thereafter.\nWaddington was an old-fashioned intellectual who lived in both the arts and science milieus of the 1950s and wrote widely. His 1960 book \"Behind Appearance; a Study Of The Relations Between Painting And The Natural Sciences In This Century\" (MIT press) not only has wonderful pictures but is still worth reading.\nWaddington was, without doubt, the most original and important thinker about developmental biology of the pre-molecular age and the medal of the British Society for Developmental Biology is named after him.\n1. ^ Robertson, Alan. 1977. \"Conrad Hal Waddington. 8 November 1905–26 September 1975.\" Biographical Memoirs of Fellows of the Royal Society 23, 575-622. Pp. 575-76. 2. ^ Waddington, C. H. 1975. The Evolution of an Evolutionist. Ithica, NY: Cornell University Press. Pg. 2. 3. ^ Robertson, Alan. 1977. \"Conrad Hal Waddington. 8 November 1905 — 26 September 1975.\" Biographical Memoirs of Fellows of the Royal Society 23, 575-622. Pg 577. 4. ^ Robertson, Alan. 1977. \"Conrad Hal Waddington. 8 November 1905 — 26 September 1975.\" Biographical Memoirs of Fellows of the Royal Society 23, 575-622. Pp. 579-580. 5. ^ Yoxen, Edward. 1986. \"Form and Strategy in Biology: Reflections on the Career of C. H. Waddington.\" In A History of Embryology, edited by T. J Horder, J. A Witkowski, and C. C Wylie. Cambridge: Cambridge University Press. Pp. 310-11. 6. ^ \"Guide to the Center for Advanced Studies Records, 1958 - 1969\". Wesleyan.edu. http://www.wesleyan.edu/libr/schome/FAs/ce1000-137.html. Retrieved 2010-04-04. 7. ^ Robertson, Alan. 1977. Conrad Hal Waddington. 8 November 1905 — 26 September 1975. Biographical Memoirs of Fellows of the Royal Society 23, 575-622. P. 578 8. ^ Goldberg, A. D., Allis, C. D., & Bernstein, E. (2007). Epigenetics: A landscape takes shape. Cell, 128, 635-638. 9. ^  Hall BK. 2004. In search of evolutionary developmental mechanisms: the 30-year gap between 1944 and 1974. J Exp Zool B Mol Dev Evol. 2004 Jan 15;302(1):5-18. 10. ^ Robertson, Alan. 1977. Conrad Hal Waddington. 8 November 1905 — 26 September 1975. Biographical Memoirs of Fellows of the Royal Society 23, 575-622. P. 585.\nSelected works Books\n* Waddington, C. H. (1939). An Introduction to Modern Genetics. London : George Alien & Unwin Ltd. * Waddington, C. H. (1940). Organisers & genes. Cambridge: Cambridge University Press. * Waddington, C. H. (1941). The Scientific Attitude, Pelican Books * Waddington, C. H. (1946). How animals develop. London : George Allen & Unwin Ltd. * Waddington, C. H. (1953). The Epigenetics of birds. Cambridge : Cambridge University Press. * Waddington, C. H. (1956). Principles of Embryology. London : George Allen & Unwin. * Waddington, C. H. (1957). The Strategy of the Genes. London : George Allen & Unwin. * Waddington, C. H. (1959). Biological organisation cellular and subcellular : proceedings of a Symposium. London: Pergamon Press. * Waddington, C. H. (1960). The ethical animal. London : George Allen & Unwin. * Waddington, C. H. (1961). The human evolutionary system. In: Michael Banton (Ed.), Darwinism and the Study of Society. London: Tavistock. * Waddington, C. H. (1966). Principles of development and differentiation. New York: Macmillan Company. * Waddington, C. H. (1966). New patterns in genetics and development. New York: Columbia University Press. * Waddington, C. H., ed. (1968–72). Towards a Theoretical Biology. 4 vols. Edinburgh: Edinburgh University Press. * Waddington, C. H., Kenny, A., Longuet-Higgins, H.C., Lucas, J.R. (1972). The Nature of Mind, Edinburgh: Edinburgh University Press (1971-3 Gifford Lectures in Edinburgh, online) * Waddington, C. H., Kenny, A., Longuet-Higgins, H.C., Lucas, J.R. (1973). The Development of Mind, Edinburgh: Edinburgh University Press (1971-3 Gifford Lectures in Edinburgh, online) * Waddington, C. H. (1977) (published posthumously). Tools for Thought. London: Jonathan Cape Ltd.\n* Waddington C. H. 1942. Canalization of development and the inheritance of acquired characters. Nature 150:563-565. * Waddington, C. H. 1953. Genetic assimilation of an acquired character. Evolution 7: 118-126. * Waddington, C. H. 1953. Epigenetics and evolution. Symp. Soc. Exp. Biol 7:186-199. * Waddington, C. H. 1956. Genetic assimilation of the bithorax phenotype. Evolution 10: 1-13. * Waddington, C. H. 1961. Genetic assimilation. Advances Genet. 10: 257-290. * Waddington, C. H. 1974. A Catastrophe Theory of Evolution. Annals of the New York Academy of Sciences 231: 32-42.\n* NAHSTE Project Record of C.H. Waddington * Induction and the Origin of Developmental Genetics - works by Salome Gluecksohn-Schoenhimer and Conrad Hal Waddington * Epigenetics News\nConrad Hal Waddington's Timeline\nNovember 8, 1905\nEvesham, Worcestershire, UK\nSeptember 26, 1975\nEdinburgh, City of Edinburgh, UK"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:ee117239-2b09-4c2b-b764-81862545b6ca>","<urn:uuid:c44521fb-90eb-4fb8-918d-e1e93a4530b0>"],"error":null}
{"question":"What are the basic rules about outs and strikes in softball game?","answer":"In softball, each team gets 3 outs per inning. A batter is out after 3 strikes, and receives a walk after 4 balls. These are part of the fundamental rules of softball, along with having 9 players per team and requiring pitchers to throw underhand.","context":["A regulation game consists of 7 innings unless extended because of a tie score or unless shortened because the home team needs none or only a fraction of its 7th inning or unless 1 team is leading by 10 runs after 5 innings.\nIs college softball 7 or 9 innings?\nNCAA college softball games are seven innings for a complete game. The inning consists of two halves, where the top of the inning has the away team hit while the bottom has the home team hit. There are nine softball players out on the field, just like you would see in baseball games.\nDoes softball have 9 innings?\nThe game is played in usually seven innings. Each inning is divided into a top half, in which the away team bats and tries to score runs, while the home team occupies the field and tries to record three outs; then a bottom half, when the teams’ roles are reversed.\nHow long is a softball game?\nA softball game typically last an hour or two, although longer and shorter games are possible. The length depends on the number of innings in the game. The typical game has seven innings, but the rules allow anywhere between three and seven innings.\nHow many quarters are in softball?\nLength of Games A regulation game shall consist of four quarters. Quarters shall be 8 minutes in length. 2.\nWhy is softball only 7 innings and baseball is 9?\nMost games do not last more than six innings. But with time, pitching is getting better that makes it very difficult to score runs, and the game started to last longer. So, baseball bosses decided to set an innings limit, and it is set to nine players and nine innings.\nWhat are innings in softball?\nAn inning is made up of two rounds, where both teams take a turn each to bat and field. Each half of the inning will not end till three outs occur. The home team will usually field first. At the end of all seven innings, if the score is tied, there will be extra innings played until a winner emerges.\nWhat are 5 rules of softball?\nThe Top 10 Rules Of Softball Running The Bases. 3 outs per team per inning. 3 strikes and you’re out. 4 Balls is a walk. 7 Innings in a game. 9 players per team. Pitchers must throw underhand. Foul Balls are softballs hit out of play.\nWhat inning means?\nDefinition of inning 1a : a division of a baseball game consisting of a turn at bat for each team also : a baseball team’s turn at bat ending with the third out. b innings plural in form but singular or plural in construction : a division of a cricket match. c : a player’s turn (as in horseshoes, pool, or croquet).\nWhat are the 10 positions in softball?\nTen defensive players are allowed on the field: Pitcher, catcher, first baseman, second baseman, third baseman, shortstop, right fielder, right center fielder, left center fielder, and left fielder.\nHow long are softball games innings?\nAll games will be limited to 6 innings or fifty minutes – no new inning start after 50 minutes. Make sure to request the game starting time.\nWhy is a softball yellow?\nThe biggest reason for making softballs bright yellow is that they are easier to see. The distance between the pitching mound and the batter’s box is approximately 14 feet shorter in softball than in baseball, giving the hitter less time to react to the pitch.\nIs a baseball harder than a softball?\nIt’s scientifically proven that fastpitch softball is harder than baseball. On average, there is less reacting time in softball batting than in baseball when you consider both pitching speed and distance. Also, a softball field is smaller, basepaths are shorter, and pitching style is harder.\nHow long is an inning in baseball?\nOfficial ‘Length’ of Baseball Innings Explored Officially the “length” of an inning in baseball is a total of six consecutive outs, or three per team.\nHow long is a NCAA softball game?\nEach university softball game has seven innings, yet if the score is tied, the game may feature extra innings. Generally, a game could have a duration of around two hours.\nWho invented softball?\nSoftball began in 1887 when George Hancock, a reporter for the Chicago Board of Trade, invented “indoor baseball”. By the spring of 1888, the game had spread outdoors. It was originally called either mushball, kittenball or indoor baseball, but by the 1920s it had acquired the name of softball.\nWhy are there 9 innings?\nIn baseball’s infancy, teams would play until one team scored 21 times. So, Baseball bosses decided to set an inning limit. As baseball was played with nine players, nine innings seemed like a good way to go for the number of innings. Major league baseball has been experimenting with ways to speed up the game.\nWhat does 9 innings mean in baseball?\nIn baseball, the 9th inning is the last inning in the game. Sometimes the bottom of the ninth inning is not played or ends before three outs are made. The game is over and play will end immediately in the 9th inning if: The home team is in the lead at the end of the top-half of the 9th inning.\nHow many innings do baseball have?\nA full baseball game is typically scheduled for nine innings, while softball games consist of seven innings; although this may be shortened due to weather or extended if the score is tied at the end of the scheduled innings."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:415a260b-67c2-4290-a247-433859345cca>"],"error":null}
{"question":"Which facility offers more comprehensive mental health services - Southside Health Clinic or C.E. King High School?","answer":"Southside Health Clinic, part of Harris Health System's school-based clinics, offers comprehensive mental health services including evaluation and assessment, psychiatric therapy, individual and group counseling, family counseling, alcohol and drug abuse treatment, and has both a behavioral health specialist (5 days/week) and psychiatrist (1 day/week). The information provided about C.E. King High School does not mention any mental health services, only describing it as a replacement high school focused on personalized learning.","context":["About the School-Based Clinics Program\nThe first Harris County Hospital District (now the Harris Health System) School-Based Clinic was established at Jackson Middle School in 1994 with the help of a grant from the Texas Department of Health. In 1995, the Harris County Hospital District Board of Managers approved a district-wide health promotion program called the \"Make a Difference Program\". This program would bring preventative and early detection services closer to those who would utilize them. The school-based clinics were recognized as an integral part of this project for improving the health status of our community's children.\nToday, the Harris Health System continues to operate eight permanent school-based clinics in conjunction with six independent school districts: Houston ISD, Channelview ISD, Galena Park ISD, North Forest ISD, Deer Park ISD and Sheldon ISD. The clinics include:\n- Robert Carrasco Health Clinic at Marshall Middle School (opened 1995 at Sherman Elementary, but relocated to Marshall in March 2006); Houston ISD\n- Smiley Health Clinic (opened 1995); North Forest ISD\n- Patrick Henry Health Clinic (formerly opened in 1996 at Scarborough Elementary, but relocated to Patrick Henry Middle School in July 2003); Houston ISD\n- Jerry Neal Health Clinic at Channelview Annex Campus (opened 1997); Channelview ISD\n- Almatha Clark Taylor Clinic at Cloverleaf Elementary School (opened 1998); Galena Park ISD\n- Southside School-Based Health Clinic (formerly known as Nuestra Clinica, it was located at Jackson Middle School but closed in August 2003 and relocated in March 2004); Galena Park ISD\n- Deepwater Clinic (opened February 2005); Deer Park ISD\n- Sheldon Clinic (opened October 2006); Sheldon ISD\nEach school-based clinic is linked with a Harris Health System Community Health Center and is staffed with an advanced practice nurse, a licensed vocational nurse and a clinical clerical specialist. A specially trained registered nurse provides well child evaluations at Almatha Clark Taylor, Southside and Patrick Henry Health Clinics. Community outreach eligibility workers go to each clinic on a once weekly or twice monthly schedule in order to screen and provide Harris Health System Gold Cards to patients and their qualified immediate families. The program is overseen by a faculty pediatrician of the Baylor Department of Family and Community Medicine.\nServices provided by the school-based clinics include Texas Health Steps Well Child Exams, family education, evaluation and treatment of minor acute and chronic health problems, referrals to secondary and tertiary treatment centers, basic laboratory procedures, vision/hearing screening, appropriate referral for outside services and immunizations. Each school-based clinic has a Class D pharmacy, which allows for the provision of medications.\nIn March 2003, the Harris Health System Mobile Health Program was combined with the School-Based Clinics Program. The Harris Health System Mobile Health unit provides vaccinations to children in most Harris County Independent School Districts.\nThe program expanded services in November 2005 to include behavioral health care at Southside Health Clinic. A behavioral health specialist is on site five days a week and a psychiatrist one day a week. Services at the clinic include evaluation and assessment, psychiatric therapy, individual and group counseling, family counseling, treatment for alcohol and drug abuse and referrals for more intensive treatment, as needed. An InSight specialist rotates between two school-based clinics, providing free alcohol, drug and tobacco screening and brief intervention to adolescents. Mental health services were added to Patrick Henry School-Based Clinic in the summer of 2006.\nClinic services are available to the pediatric population of Harris County. The School-Based Clinic Program operates Monday through Friday, during daytime hours. Clinics are open year round, thus providing a medical home for the community's children.","Scale and relationship of spaces, use of materials, building organization and flow, adaptability and flexibility, instructional function supporting a variety of learning and teaching styles and educational appropriateness are all elements of design excellence. Show examples of where the educational program and design goals informed the site development and facility design with an enhanced student learning experience.\n|Austin ISD—Doss Elementary School\nPreserving and Growing a Community Culture. This replacement school with an owl mascot—known as the “School in the Trees”—shares its site with a city park and embraces sustainable, biophilic design to further reinforce its setting in nature. With clearly stated project requirements and a strong existing cultural foundation, the new school environment delivers operational value and state-of-the-art learning spaces, allowing each student the opportunity to find their own unique path to knowledge.\n|Community ISD—Community High School\nA new high school was designed with three East-West wings: a two-story Academic Wing, a Fine Arts and Food Service Wing, and an Athletic Wing, closest to the stadium. The three wings are connected by a “Main Street” corridor through the center. The building orientation plays an important role in energy efficiency, minimizing openings on the East and West facing walls, significantly reducing utility costs. Outdoor spaces between the wings are secure Outdoor Learning Areas for students and faculty.\n|Crowley ISD—Bill R. Johnson CTE Center\nCollaboration & Innovation—Real world leader’ experiences (medical, automotive, entrepreneurship, etc.) influenced the design to deliberately combine early college and career preparation programs in one facility. The new district-wide facility, designed with students and community for the community, intentionally does not resemble any typical school. Partnerships informed the design to replicate the industries and learning environments to prepare students for their next phase of life.\n|Houston ISD—Lamar High School\nFusing historic & new to create a campus honoring the past, present, & future. The 30-acre urban site houses a renovated original high school building, an 80-year-old art deco city landmark, poorly planned 80’s additions that were removed to make room for new fields, & a new parking garage to increase site capacity. A context-sensitive, 300,000 SF addition houses academic neighborhoods, arts, athletics, & student services to align with the educational philosophy of the IB program & PBL model.\n|Humble ISD—Centennial Elementary School\nBuild a brand new elementary school to serve the community.\n|Katy ISD—Gerald D. Young Agricultural Sciences Center\nThis Agricultural Sciences facility was born with the realization of increasing expectations for student preferences, graduation requirements, and growth in the district and community usage. Serving the district’s K-12 population of over 87,000 students, this facility was built on the important historical aspects of the agriculture industry that has existed in this community for decades. This modernized structure incorporates instruction, rodeo, and a venue for community events.\n|Sheldon ISD—C.E. King High School\nThe district is in the petrochemical corridor in unincorporated Houston. It serves a richly diverse population. Known for industry, it traditionally has not been recognized for its schools. As a tight-knit community, the replacement of their high school (a 50-year-old building) on a new site presented bold opportunities. The rebirth of the high school was deeply embraced. It is iconic, a destination for personalized learning, and a symbol that all students deserve high-quality education."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:c6cf5c98-2135-4c35-a656-ba2f14c6f46a>","<urn:uuid:e6244457-6c3e-4b73-bb70-b00cc508013f>"],"error":null}
{"question":"What are the current legislative efforts to address pay equity, and how do workplace discrimination laws protect against educational requirements?","answer":"The Paycheck Fairness Act aims to combat pay discrimination by banning employers from seeking salary history and holding them accountable for systemic discrimination. The Act also requires transparency in wage reporting to help workers advocate for fair pay. Additionally, there are proposals for family and medical leave, affordable childcare, and minimum wage increases. Regarding educational protections, federal laws including Title VII of the Civil Rights Act of 1964 prevent employers from setting unnecessary educational requirements or language fluency unless essential for job performance. The Age Discrimination in Employment Act ensures access to training programs regardless of age, and employers cannot discriminate based on learning disabilities - hiring decisions must be based solely on job qualifications and capabilities.","context":["Equal Pay Day was originated by the National Committee on Pay Equity in 1996 as a public awareness event to illustrate the gap between men’s and women's wages.\nWomen lose thousands of dollars each year, and hundreds of thousands over a lifetime, because of the gender and racial wage gap. In 2019, the typical woman who worked full time took home just 82 percent of the typical man’s pay.\nThe disparities are even greater for Black, Native American, and Hispanic women, who earned 63 percent, 60 percent, and 55 percent of white men’s wages, respectively.\nWhile Asian American women make 87 percent of what white men make, the gap for Asian women varies significantly depending on subpopulation, with some Asian women – for example, Cambodian and Vietnamese women – earning among the lowest wages.\nFor Latinas’ Equal Pay Day doesn’t occur until October because it takes that long for the average Latina to earn what the average white man made in the previous year.\nSince the covid-19 pandemic began, women, especially women of color, disproportionately are working on the frontlines, in caregiving, and working to combat the virus, but they continue to earn less than their male counterparts.\nIn a proclamation on Equal Pay Day, President Joe Biden said the nation needs to begin by passing the Paycheck Fairness Act, which will take steps towards the goal of ending pay discrimination.\nFor example, Biden said, it will ban employers from seeking salary history – removing a common false justification for under-paying women and people of color – and it will hold employers accountable who engage in systemic discrimination.\n“The bill will also work to ensure transparency and reporting of disparities in wages, because the problem will never be fixed if workers are kept in the dark about the fact that they are not being paid fairly,” he said in the proclamation. “Relying on individuals to uncover unfair pay practices on their own will not get the job done; when pay data is available, workers can better advocate for fair pay and employers can fix inequities.”\nBiden said family and medical leave also is needed to make schedules more predictable and childcare more affordable, and build pipelines for training that enable women to access higher-paying jobs.\nAnd, he added, this commitment also means increasing pay for childcare workers, preschool teachers, home health aides, and others in the care economy – and taking additional steps to increase wages for American workers, such as raising the minimum wage and empowering workers to organize and collectively bargain, both of which are important to reducing the wage gap for women.\n“There is still significant work to be done to make sure our daughters receive the same rights and opportunities as our sons, and that work is critical to ensuring that every American is given a fair shot to get ahead in this country,” Biden said in concluding his proclamation. “Today, on Equal Pay Day, we recognize the role that equal pay plays in building back better for everyone.”","Many employers require you to have a specific type and level of education to qualify for certain jobs. Workplace educational discrimination happens when an employer requires that you have a level of education that isn’t necessary for the job. While there are laws to prohibit educational discrimination, some biases can still emerge in the workplace.\nCalling It Even\nEducation discrimination involves academic-related requirements set by employers that may violate the law. For example, discrimination might be the case if an employer is advertising for “college graduates” to apply because such advertising can deter otherwise qualified non-graduates from applying. Any tests of your skills or knowledge that an employer requires you to take must be necessary and job-relevant as well.\nCivil rights laws in the U.S. prohibit educational institutions from discriminating against you on the basis of race, gender, national origin, disability or age. You can’t be denied the opportunity to get your GED because you’re 50-years old any more than the opportunity to complete a Ph.D. because you’re a woman. Still, discrimination can happen when an employer understands the educational demographics of people in the job market, or educational backgrounds of particular individuals, and sets requirements for hiring or promotion that are meant to exclude certain individuals or groups.\nLaws and Learning\nMany federal laws focus on preventing education discrimination. For example, the Age Discrimination in Employment Act requires that employers let you enter training or apprenticeship programs despite your age. Title VII of the Civil Rights Act of 1964 prevents employers from requiring you to be fluent in English before hiring you for a job unless the employer can prove that speaking English is essential to performing that job. Employers also can’t hold a learning disability against you. They can only decide whether to hire you or not based on your qualifications and capability to do the job.\nMore women than men attribute educational credentials, along with hard work and long hours, as keys to getting promotions at work, according to a 2010 study by the Center for Work-Life Policy. Still, even though the study found that women held 34 percent of senior management positions, men outnumbered women in top executive positions by four to one. Bias against promoting educated and hard-working women all the way to the top is still prevalent in workplaces throughout the U.S. despite widespread improvements to workplace equality in general.\n- FindLaw: How to File a Discrimination Complaint With the Government\n- U.S. Equal Employment Opportunity Commission: Federal Laws Prohibiting Job Discrimination Questions and Answers\n- Center for Work-Life Policy: Harvard Business Review Research Report: The Sponsor Effect: Breaking Through the Last Glass Ceiling\n- U.S. Equal Employment Opportunity Commission: Prohibited Employment Policies/Practices\n- Jupiterimages/Stockbyte/Getty Images\n- What Does EOE Mean in Reference to a Job?\n- The Objectives for a Teaching Resume\n- Define Workplace Discrimination\n- Strategy to Stop Discrimination in the Workplace\n- Promotion Discrimination in the Workplace\n- Age Requirements for the U.S. Army Rangers\n- Can My Boss Force Me to Go to Employee Counseling?\n- Security Guard Training Requirements"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:cba0a24f-85f4-4c08-9cc9-a680e3d65fc4>","<urn:uuid:0255e196-c5f6-420f-bc4c-da342e27ac03>"],"error":null}
{"question":"What are the historical and current demographic challenges facing the US and Japan in terms of life expectancy fluctuations and population sustainability?","answer":"The US historically experienced volatile life expectancy changes, including a dramatic decline during the 1918 influenza pandemic from 50.9 to 39.1 years, but has since stabilized with only rare declines. Japan, on the other hand, faces a current demographic crisis with a low birth rate of 1.4 children per woman, a rapidly aging population, and a projected population decline of almost one-third by 2060, creating an unsustainable situation for their pension and healthcare systems.","context":["This week, NCHS published the latest “life tables” for the United States, through the year 2018. Life expectancy estimates for 2019 are expected to be released in the coming weeks. The new report, “United States Life Tables 2018,” features a rich collection of historical data, extending back to the beginning of the 20th century, when life expectancy at birth in the U.S. was only 47.3 years (see Table 13). The tables in the report document how life expectancy for the total population has increased over time and is now 78.7 years for the U.S. as a whole.\nOver the past 50 years, declines in life expectancy have been relatively rare. However, in the early part of the 20th century, life expectancy was quite volatile, declining 13 times between 1902 and 1928 – including a staggering 11.8 year decline in life expectancy during the height of the influenza pandemic of 1918, in which life expectancy at birth dropped from 50.9 years in 1917 to 39.1 years in 1918. As the pandemic subsided, life expectancy reversed course and increased 15.6 years in 1919 to 54.7 years.\nThis phenomenal two-year period illustrates that historically, declines in life expectancy can be usually traced to significant health events occurring nationally and globally. During the years of the Great Depression and World War II, for example, there were four years when life expectancy in the U.S. declined. In 1957, as the so-called “Asian Flu” began to take a foothold globally, life expectancy in the U.S. ticked down.\nMost often, declines in life expectancy have been only one-year events, before resuming an upward trend. Remarkably, between 1924 and 2018, only once did life expectancy decline for two consecutive years – in 1962 and 1963. In 2015 and 2017, life expectancy at birth declined, but remained unchanged in 2016 and increased slightly in 2018.\nDisparities in life expectancy at birth between racial groups have long existed, and continue today. Life expectancy is higher for non-Hispanic whites than for non-Hispanic blacks – a gap of 3.9 years in 2018 (see Table A). At the same time, life expectancy is higher for U.S. Hispanics than for non-Hispanic whites (a gap of 3.2 years). With age, these gaps start to narrow – at age 65 the gap in life expectancy between non-Hispanic whites and non-Hispanic blacks was 1.4 years in 2018, and the gap between Hispanics and non-Hispanic whites was 2.0 years. By age 75, the gap in life expectancy narrowed to 0.3 years between non-Hispanic whites and non-Hispanic blacks, and to 1.7 years between Hispanics and non-Hispanic whites. By age 79, the gap in life expectancy disappears between non-Hispanic whites and non-Hispanic blacks and by age 80, life expectancy is actually slightly higher for non-Hispanic blacks than non-Hispanic whites (9.2 years vs. 9.1 years). The gap in life expectancy between Hispanics and non-Hispanic whites and non-Hispanic blacks remains at least one year throughout the life tables.\nPossible explanations for the gap in life expectancy and the reduction in the gap as people age includes the fact that death rates for external or “premature” causes of death are higher among younger non-Hispanic blacks vs. non-Hispanic whites and Hispanics, though there are no data in this particular report on possible reasons for the disparities. Other NCHS reports, however, do address causes of death that contribute to life expectancy.","Tackling Japan’s Demographic Time Bomb\nJapan’s demographic time bomb is not merely ticking—it is already on the brink of exploding. The country needs more tax-paying, young citizens to offset its ever-increasing number of pensioners if retirees wish to maintain their current standards of living.\nThe writing is clearly on the wall: Japan needs more tax-paying, young citizens to offset its ever-increasing number of pensioners if these retirees wish to maintain their current standards of living. The question, however, is whether there are public policies that can provide a solution to this impending crisis, and what may happen if the number of Japanese retirees continues to surge past the number of workers.\nThanks to a heavily subsidized public healthcare system, not to mention a diet naturally low in fat and a lifestyle that encourage lots of walking, the Japanese are living ever longer. The nation boasts the world’s longest life expectancy of nearly 85 years, compared to 79.6 years in the United States (a number that puts it in 42nd place globally). While living long and living well is certainly enviable, it comes with a heavy price tag for Japan when the number of children born annually continues to decline steadily.\nThe Japanese government estimates that the current population of 127 million will shrink by almost a third to about 87 million by 2060. Given that the birth rate averages 1.4 children per woman, such dire projections are hardly surprising. Indeed, Japan’s population has been on the decline for the past decade. Today, nearly a quarter of the population is over 65 years old, and that figure is projected to rise to 40 percent over the next quarter century. Coupled with the fact that Japan has the unhappy title of being one of the most indebted countries in the world, with a debt-to-GDP ratio above 240 percent, it hardly takes a degree in economics to figure out that the current situation is unsustainable. Japan’s demographic time bomb is not merely ticking—it is already on the brink of exploding.\nDespite all indications that Japan needs to take action now to boost its population and cut back on pension spending, proposals by the ruling government of Prime Minister Shinzo Abe have been far too tepid to bring about the necessary changes. A graying society will not only place a heavy burden on the country’s finances but will also put Japan’s national security at risk and reshape its political landscape. Although Abe has repeatedly acknowledged this demographic challenge, and although a governmental advisory panel earlier this year suggested setting a numerical target for the birth rate to over two children per woman by 2034, nothing short of a social revolution will be sufficient to combat the problem of Japan’s rapidly ageing society. Numerous tax breaks, revised welfare spending measures, and more child-care centers are of course well-intentioned measures that should be enacted. What is truly needed, however, are four critical measures that would fundamentally change Japan’s social values in order to respond effectively to the country’s looming demographic crisis.\nFirst, the Abe government should allow for career on-off ramps that would help retain the country’s most talented citizens. Today, increasingly fewer Japanese companies are able to provide lifetime employment guarantees that were readily available in the past. Yet most blue chip companies, as well as the civil service, continue to adhere to corporate Japan’s traditional practice of recruiting undergraduate students once a year at the same time. Career-track employees are usually the ones who joined the company straight out of college, and opportunities for those who have taken time away from school or who have chosen to change careers remain limited. The same is true for retirees and individuals above the age of 60. Last year, Japan’s mandatory retirement age was raised to 61, and will continue to go up by one year every three years until 2025, when the retirement age reaches 65. This logic, however, flies in the face of the reality confronting Japan’s different age groups. Many people in their sixties, who have accumulated more contacts and face reduced pressures at home than their younger counterparts, are better able to contribute to the workplace. Greater flexibility in when each person retires, not to mention how workers young and old shape their careers, could bolster Japan’s tax revenue at this critical demographic juncture.\nEmpowering and increasing the representation of women at all levels of the workplace, a key policy objective for Prime Minister Abe, could also help resolve the country’s impending crisis. Abe’s rally to boost the number of daycare centers and invest in after-school activities for students has raised the political visibility of this issue. Given that Japan still ranks 123rd out of 189 countries in terms of the percentage of women serving in parliament, however, there is still a long way to go before even the appearance of equality between genders emerges. Encouraging and supporting a greater number of working women will be crucial to increasing Japan’s tax base. Given that Japanese women are usually well-educated, keeping them active in the workplace could potentially both increase workplace diversity and enhance the global competitiveness of Japanese corporations.\nThird, the Abe government should encourage romance—or at least get more of its people to marry and have children. Many Japanese women, particularly those who value having a career of their own, are currently hesitant about marriage because it would lead to a drastic change in their lifestyles. Many recognize that even the most understanding of husbands may not help offset the social norms that pressure married women with children to stay home and look after the family. As a result, last year the number of marriages hit a post-World War II low of 661,594 couples countrywide. For many women, the choice is between getting married and losing what they have achieved professionally or retaining their independence by remaining single. Such a diametrical decision, however, should be softened for the benefit of all Japan. A determination by the Abe government to offer more tax breaks and other financial incentives would certainly help counteract this trend.\nFourth and finally, Japan must embrace immigration as a solution to its impending fiscal and demographic woes. A declining birthrate is a global trend, as is an ageing population. It will therefore be increasingly difficult for any country to meet all of its labor needs relying solely on the population that exists within its borders. In the case of Japan, more caregivers, nurses, and other providers catering to a graying society will be needed, especially if more women choose to go back to work full-time. The Japanese government will thus inevitably need to consider seriously the possibility of opening its doors to more immigration, rather than just to the highly skilled workers it currently courts. Although the Japanese have been at the forefront of developing robots designed to meet the mounting tsunami of elderly people’s needs, there is a limit to what can be expected from technology, especially when the psychological as well as physical needs of an ageing society are considered.\nJapan stands on the edge of a precipice. The Abe government has at its disposal numerous policy initiatives to offset or mitigate the impact of the crisis it will increasingly confront in the coming years. Until it takes concrete action, however, Japan’s demographic time bomb will tick on.\nThe opinions expressed here are solely those of the author.\nAbout the Author\nThe Asia Program promotes policy debate and intellectual discussions on U.S. interests in the Asia-Pacific as well as political, economic, security, and social issues relating to the world’s most populous and economically dynamic region. Read more"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:96768bf7-6325-4e6a-84ae-25f230cd46ef>","<urn:uuid:81756617-3f15-42e9-965d-3e987440734f>"],"error":null}
{"question":"How is Buddhist Samadhi different from Christian worship practices? Pourriez-vous expliquer la différence?","answer":"Buddhist Samadhi and Christian worship differ fundamentally in their nature and purpose. In Buddhism, Samadhi does not mean absorption in the eternal self, but rather refers to absorption within the mind or entering unified states of awareness where no thought, emotion, or other mental modification remains active. In contrast, Christian worship involves elements like supplication, adoration, expressions of dependence, humility, subservience, and proclamations of praise. Christian worship practices can vary widely, from elaborate Catholic rituals with sensory elements like incense and vestments, to simpler Protestant services, while Buddhist practice focuses on achieving mental states through concentration and meditation techniques like Jhanas.","context":["Religious beliefs and their attendant moral values usually find accommodation within organizational structures, set procedures, and their expression in particular symbols. In western society, the forms of Christian institutions have become so well-established that it is often easy even for secularized lay people to assume that a religion must have analogous structures and symbols to those of Christianity. The model of the separated worship building, a stable congregation, served by a resident priesthood which has power to mediate or counsel, are all items for which analogues are expected of other religions. Yet even a cursory review must make it clear that religion need not look like this model. The major religions of the world manifest a variety of diverse arrangements, from, on the one hand, sacerdotalism, the practice of sacrifice, and sacramentalism with profuse use of auxiliary aids to faith (such as incense, dance, and imagery) to, on the other hand, keen asceticism and singular dependence on verbal expression and prayer. Both extremes may be encountered within one major tradition, in Hinduism or Christianity, while, in its orthodox expression, Islam is more uniformly ascetic—its ecstatic manifestations occurring at the fringes.\nReligious worship differs greatly in form and frequency among the various religions. It has different implications and takes a distinctive form in non-theistic systems such as Buddhism. Since there is no transcendent deity, there is no point in supplication, no place for adoration, no need for expressions of dependence, humility, and subservience, no purpose in proclamations of praise—all of which form a part of Christian worship. Yet contemporary Christian worship is itself the product of a long process of evolution. The Judaeo-Christian tradition has changed radically over the centuries. Old Testament demands for animal sacrifice for a vengeful God are far removed from the devotional practice of, say, nineteenth-century mainstream Protestantism. The replacement of chanting and metrical psalm-singing by popular hymns gave a quite different appearance to Christian worship in the course of a couple of centuries. Today, the concept of an anthropomorphic God has waned in Christianity, and from the point of view of modern theology, contemporary Christian worship, in which anthropomorphic imagery is abundant, is distinctly anachronistic. It can hardly be surprising that some modern denominations, unburdened by old traditions (in which the patina of antiquity is easily mistaken for the aura of sanctity) should have reduced, if not altogether relinquished, traces of the anthropomorphism of the past. Even apart from such evolutionary trends, however, there is abundant diversity among Christian denominations to establish the point that any stereotyping of what worship implies betrays the many-sided diversity of religion in today’s world. Thus, the Roman Church developed the elaborate use of auditory, visual, and olfactory sensation in the service of faith. Catholic liturgy, whilst abjuring the use of dance and drugs, which have been employed in other religions, has elaborate ritual, sacraments, and vestments, a great wealth of symbolism, and a profusion of ceremonies marking the calendar and hierarchy of the Church and the rites of passage for individuals. In sharpest contrast to Roman Catholicism stand the Quakers, who reject any concept of a priesthood, any enactment of ritual (even of the unsacramental commemorative patterns of ritual common in some of the Protestant denominations) and the use of imagery and vestments. Emphasis on the adequacy and competence of lay performances, the rejection of sacrality, whether of buildings, places, seasons or ceremonies, and such aids as talismans and rosaries, is a characteristic in greater or lesser measure of Protestant religion. Evangelicals reject the idea of a priesthood, and Quakers, Brethren, Christadelphians, and Christian Scientists function without a paid ministry. While most Protestant denominations retain a breaking of bread ceremony they do so often as a commemorative act in obedience to scripture, and not as a performance with any intrinsic power. Thus, whilst in some instances different actions have similar purposes, in other cases, as with the breaking of bread, an apparently similar act acquires, in accordance with a denomination’s teaching, a distinctive meaning. Where, as in Christian Science, the deity is regarded as an abstract principle, acts of worship, whilst having a familiar religious purpose of bringing the believer into rapport with a divine mind, take on a quite different complexion from the supplicatory practices of denominations which retain an anthropomorphic view of deity.\nNew religions—and all religions were new at some time—are likely to ignore or to jettison some of the traditional practices and institutions of older and established faiths. They are all the more likely to do so if they arise in periods of accelerated social and technical development when the life-patterns of ordinary people are undergoing radical change, and when assumptions about basic institutions—family, community, education, the economic\norder—are all changing.\nNew religions—and all religions were new at some time—are likely to ignore or to jettison some of the traditional practices and institutions of older and established faiths. They are all the more likely to do so if they arise in periods of accelerated social and technical development when the life-patterns of ordinary people are undergoing radical change, and when assumptions about basic institutions—family, community, education, the economic order—are all changing. In a more dynamic society, with increasingly impersonal social relations, and the influence of new media of communication, and a wider diffusion of all sorts of information and knowledge, the increased diversity of religious expression is entirely to be expected. New religions in western society are unlikely to find congenial the structures of the churches that originated two, three, four or fifteen or more centuries ago. To offer one example, given the intensified degree of social, geographic, and diurnal mobility of modern population, it would not be appropriate to suppose that new religions would organize themselves congregationally as stable and static communities. Other techniques of communication have superseded the pulpit and the printing press, and it would be surprising, in this area of activity as in others, if new religions were not to embrace the enhanced facilities of the era in which they emerge. That they do things differently from the traditional stereotype of religion, that they look outside western society for their legitimation, or that they employ new techniques for spiritual enlightenment does not disqualify them as manifestations of human religiosity.","Jhana, The Rapturous State of Stillness in Buddhist Meditation Practice\nSummary: This is a comprehensive essay on Buddhist Jhanas, the meditative states experienced by the Buddhist monks during their practice of Right Concentration on the Eightfold Path\nBuddhism is a practical religion. Its philosophy and practice are rooted in the mundane lives of common people who are subject to transience and suffering. The Buddha presented his teachings in clear terms, and set achievable and verifiable goals so that there was little scope for ambiguity or speculation. Therefore, Buddhism sets itself apart from the speculative philosophies and complex theologies of other religions.\nFor the same reason, although you may find identical words in Hinduism and Buddhism, they may not carry the same meaning, intent or purpose. Hinduism is primarily focused upon the eternal Self (Atman) and Buddhism upon the objective reality or the not-Self (Anatma). Hence, they fundamentally differ in their interpretation of various spiritual concepts and experiences.\nFor example, liberation in Buddhism does not mean immortality. It only means liberation from the cycle of births and deaths. Buddhism does not believe in the eternality of anything, nor does it view the world as unreal or illusory. The illusion arises in the mind due to unwholesome thoughts, desires and expectations. Buddhism also acknowledges the existence of devas (gods) in heavenly realm, but does not consider them to be immortal or immutable.\nAnother important concept where Buddhism differs from others is Samadhi. Samadhi in Buddhism does not mean absorption in the eternal self, but absorption within the mind or entering the unified states of awareness in which neither any thought nor emotion nor any other modification of the mind remains active.\nTo achieve this state, Buddhism prescribes the attainment of Jhanas through the transformative practices such as right actions, right thinking, right awareness, right concentration, etc., on the Eightfold Path. In the following discussion we focus upon the importance of Jhanas in Buddhist meditative practices and their distinguishing features.\nJhana is the Pali equivalent of the Sanskrit word Dhayna. In Hinduism Dhyana refers to the practice of meditation or contemplation, which can be done with or without concentration. However, in Buddhism Jhana refers to both concentrated meditation and any meditative state which arises from it. In Buddhism, the practice of Jhana forms part of the Right Concentration on the Eightfold Path. By practising it one overcomes the hindrances of unwholesome and evil states of the mind and attains one pointedness, peace and happiness.\nThe practice of Jhana has a great significance in Buddhism. It is central to many Buddhist Practices. The Buddha constantly advised his followers to practice Jhanas to attain Nirvana. Jhana is also frequently mentioned in many Buddhist texts. They attest to its popularity and importance in Buddhist spiritual practices.\nThe Buddha identified four progressive states of Jhanas, which arise from the practice of Right Concentration and which lead to meditative absorption (Samadhi). He also explained their characteristic features and how to identify them and distinguish them. From the Buddhist perspective, Jhana (Concentrated meditation) is neither an abstract concept nor an aesthetic, mental exercise. It is a transformative practice, which is difficult but practicable. Regular practice of jhana leads to verifiable and distinguishable mental states, which can be discerned to know the progress one has made.\nThe Jhanas are attained by a two-pronged approach. One is by eliminating the unwholesome factors, which need to be removed to purify the mind such as negative thoughts and emotions, and the other is by cultivating factors that need to be acquired such as positive thoughts and states of mind. However, as we see later, when a monk advances into the higher Jhanas, he has to abandon even the positive aspects of the mind to end up with just one unified mental state in which one-pointedness, equanimity, discernment, sameness, etc., work in tandem.\nTo enter the first state of Jhana, the initiate has to begin the journey by abandoning the five hindrances (pañcanivarana) so that he can experience five positive states or modifications. These too he has to abandon as he advances into higher states, so that in the end he remains with nothing but sheer discernment (buddhi). The five hindrances are sensual desire (kama), ill will (vyapada), sloth and torpor ((thina and middha), restlessness and worry (uddhaca and kukkucca), and doubt (vicikiccha). Detailed description of the Jhanas is found many Buddhist texts. The Buddha himself described them to the monks for guidance. A brief description of the same is provided in the following section.\nThe first Jhana\nThe Buddha: “There is the case where a monk — quite withdrawn from sensuality, withdrawn from unskillful qualities — enters and remains in the first jhana: rapture and pleasure born from withdrawal, accompanied by directed thought and evaluation.”\nExplanation: The practitioner enters the first jhana when he wholly gives up the aforementioned five hindrances or unwholesome states of mind and cultivates the five “factors of possession” or the mental features that need to be cultivated. They are the directed thought (vitarka), evaluative thought (vichara), one pointedness (ekagrata), rapture (priti) and happiness (sukham). To attain this the practitioner has to follow a strict monastic discipline and live in a suitable dwelling place amidst likeminded people. It may be noted that vitarka and vicara are differently defined by different scholars. As their names suggest vitarka is associated with controlled thought, and vicara with analysis and evaluation. Together, they represent the whole thought process.\nThe second Jhana\nThe Buddha: \"Furthermore, with the stilling of directed thought and evaluation, he enters and remains in the second jhana: rapture and pleasure born of composure, unification of awareness free from directed thought and evaluation — internal assurance.”\nExplanation: When he is fully stabilized in the first Jhana and perfected the five factors of possession, with mastery in strengthening, attaining and sustaining those factors (which may involve considerable time), the practitioner qualifies to enter the second Jhana. In this stage, he has to abandon both types of thought so as to silence the mind and free it from all thoughts, and remain stabilized in the remaining three factors of possession namely rapture, pleasure and one pointedness. With the mind freed from thoughts, with unified and one-pointed awareness, and with the feelings of assuredness, composure, confidence and tranquility which arises from it, he experiences rapture and pleasure as they permeate and pervade his whole body. Thus, in this Jhana out of the five factors of possession, only three are retained.\nThe third Jhana\nThe Buddha: “And furthermore, with the fading of rapture, he remains in equanimity, mindful and alert, and physically sensitive to pleasure. He enters and remains in the third jhana, of which the Noble Ones declare, 'Equanimous and mindful, he has a pleasurable abiding.'”\nExplanation: After mastering the second Jhana in different ways and having succeeded in attaining, strengthening, sustaining the three factors of it, the practitioner is now ready to practice and enter the third Jhana. In this state, he comes to realize that rapture is also a form of unwholesome disturbance only and needs to be abandoned or eliminated to advance further into the deeper states of stability and tranquility. Thereby, he cultivates indifference towards it. To achieve mastery in its practice, he resorts to mindfulness (sati), sameness or equanimity (upeksha) and discernment (samprajnata). Thus in this state, out of the five factors of possession, only two are retained and the rest are abandoned as unwholesome.\nThe Fourth Jhana\nThe Buddha: \"And furthermore, with the abandoning of pleasure and stress — as with the earlier disappearance of elation and distress — he enters and remains in the fourth jhana: purity of equanimity and mindfulness, neither-pleasure-nor-pain.”\nExplanation: The Fourth Jhana begins with the realization that one cannot completely abandon rapture without abandoning happiness. Since both are interlinked and have close affinity, the initiate realizes that since it is difficult not to experience rapture when one is happy, he is vulnerable to the risk of falling back to the previous Jhana and remaining stuck there. Further, he also realizes that abiding in prolonged happiness may lead to habitual clinging, which in itself is an unwholesome hindrance. Therefore, contemplating upon the state of equanimity or sameness and abandoning happiness along with its accompanying physical and mental feelings, he enters the four Jhana. With the neutral feeling of sameness firmly established in his mind in the place of happiness, with peace and stability reigning his mind, he abides in unified awareness more than ever. Having abandoned happiness, pleasure and pain, he practices unwavering concentration. Thus, in this Jhana, he retains only one pointedness, with equanimity and pure mindfulness (parisuddha sati), and abandons the remaining factors of possession as gross and unwholesome.\nThe subtle Jhanas\nThe Jhanas are usually four, since it is not practically possible to abandon one pointedness also and practice discernment. Once the four Jhanas are attained, there is nothing else to be abandoned or attained. The practitioner’s unified mental state becomes stabilized, without the risk of reversal or falling back. He cannot easily be disturbed or tempted with unwholesome thoughts and distractions.\nHowever, it is not the end of the road. The Buddha described the fifth state of concentration as the practice of one pointed concentration which is refined, improved and perfected with discernment. It arises from the attainment of the four Jhanas. The Buddha also spoke about the supernatural powers that arise from the perfection one achieves in the fivefold practice of Right Concentration.\nApart from the four Jhanas, the Buddhist texts also refer to four subtle Jhanas or non-material Jhanas, which are named after their respective objects of concentration. They are, mindfulness of space (akasa chetana), mindfulness of consciousness (Vijnana chetana), mindfulness of emptiness (akincana chetana), and mindfulness of neither perception nor non-perception (nevasaññana saññayatana). Some describe them as variations or modes of the fourth Jhana.\nFrom the names ascribed to them, it becomes clear that in these Jhanas the objects of the concentration are fixed, unlike in the previous Jhanas where one may choose different objects to practice concentration. The factor of possession, one-pointedness, also remains constant since it cannot be abandoned. The four objects of concentration are also progressively subtler. For example, consciousness is subtler than space, emptiness subtler than consciousness, and that which is neither perception nor non-perception is even more subtler than emptiness. By practising concentration on the subtle objects, one reaches the end of the objective world, the not-Self and attains the indescribable state of Nirvana.\nSuggestions for Further Reading\n- Right Concentration on The Eightfold Path of Buddhism\n- What is Dhyana? Definition and Significance\n- Right Mindfulness on The Eightfold Path of Buddhism\n- The Meaning and Practice of Mindfulness\n- Buddhism - Objects of Meditation and Subjects for Meditation\n- Buddhism - Right Concentration\n- Right Mindedness Or Right Resolve\n- Meditation on Anicca or Impermanence in Buddhism\n- Awakening and Enlightenment in Buddhism on the Path to Nirvana\n- Is Buddhism a Spiritual Religion?\n- Concentration and Mindfulness Meditation\n- Eight Realizations of the Great Beings\n- An Analysis of Hindu Buddhist Meditation Techniques\n- Buddhism - Right Concentration\n- The Buddhist Meditation\n- Buddhist Meditation and Depth Psychology\n- Buddhism: A Method of Mind Training\n- A Modern Treatise on Buddhist Satipatthana Meditation\nTranslate the Page"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:4a5ae455-eb74-4cbb-9d34-1b85b29d25bd>","<urn:uuid:98346f1f-031c-4cda-bc0b-ef67e58bf114>"],"error":null}
{"question":"Which offers faster real-time results: solar water-splitting technology or nanoscale disease testing devices?","answer":"Nanoscale disease testing devices offer faster real-time results compared to solar water-splitting technology. While solar water-splitting processes require time to convert sunlight into hydrogen through photoelectrochemical systems, nanoscale testing devices can provide results in about 10 minutes for disease detection, allowing for immediate on-site testing of samples. These devices can test for various conditions, from antibiotic resistance to protein markers indicating cancer or other diseases, providing much quicker real-time feedback than solar energy conversion processes.","context":["Harnessing the Power of the Sun\nWith the looming energy crisis in Egypt and the depletion of petroleum and natural-gas reserves, searching for sources of alternate fuel becomes all the more important. Inside AUC’s nanotechnology labs, researchers are working to find ways to create “smart energy” to produce renewable fuels and electricity from sunlight on a sustainable basis. Using nanotechnology techniques, or the science of the very small, scientists are making big advances in combating chronic problems like reducing greenhouse gases and lessening the burden on natural resources for generations to come.\n“The international community is aware of the crisis arising from the use of fossil fuels and is increasingly focusing on the development of zero-carbon emission technologies using renewable-energy sources,” said Nageh Allam, assistant professor of nanotechnology and renewable energy in the Department of Physics, who joined AUC two years ago from the Massachusetts Institute of Technology.\nWorking on harnessing solar energy, Allam’s research is divided into several stages. The first stage involves the use of nanomaterials to harvest sunlight for the production of clean fuel, such as hydrogen, to reduce pollution and greenhouse gases and, in turn, global warming. The second stage includes the use of nanomaterials to harvest sunlight and convert it into electricity via solar cells. The third stage involves the use of sunlight to convert the problematic carbon dioxide into natural gas. The last stage includes the utilization of solar energy for water desalination.\n“Solar energy is one of the renewable energy sources under consideration because it is the most abundant and, if harnessed efficiently, is capable of meeting global energy needs for the foreseeable future,” Allam said. Hydrogen is one of the lightest and cleanest gases. When used as a fuel, no emissions are produced. Its only byproducts are water, heat and energy. However, it takes a lot of energy –– and money –– to extract hydrogen from water. Currently, hydrogen gas is mostly created using fossil fuels, which cause harmful carbon-dioxide emissions. Working to develop technologies that can efficiently and cheaply make use of hydrogen as an energy source, not an energy carrier, Allam’s research involves the fabrication of nanostructured arrays of highly oriented semiconductors. These arrays are adapted to create nanostructured materials, which can be efficiently used to convert sunlight into energy. “Sunlight is used to split water molecules into oxygen and hydrogen atoms, which can be used to produce clean energy,” Allam explained. “In theory, sunlight can be used to excite a semiconducting material, which in turn acts as a catalyst for the water-splitting reaction in an electrochemical cell.”\nPutting his theory into practice, Allam is currently designing photoelectrochemical systems to carry out the solar-driven, water-splitting process. “This is one of the most promising technologies being developed by scientists in the 21st century,” said Allam, whose team secured $2 million in funding from the Qatar Foundation for their work on solar hydrogen production and the conversion of carbon dioxide into useful fuels.\n“Our focus is on the production of hydrogen as a clean and efficient fuel because it can be used to power everything, from houses and electrical devices to cars and airplanes. We are also working on using solar energy to produce electricity and to convert carbon dioxide into useful fuels.”\nIn order to properly conduct this research, Allam established the Energy Materials Laboratory on campus. The lab is chiefly used for the design and assembly of nanomaterials utilized for solar-energy conversion, a practical application of Allam’s research. It includes fabrication tools for materials synthesis, a solar simulator and equipment to test the efficiency of the devices created for solar-energy conversion, among others.\n“The lab is open for everyone in the AUC community as well as those outside it,” said Allam. “In fact, we are working to establish a diploma in solar energy, and the students enrolled in such a program will spend almost 50 percent of their time in the lab — developing, designing and testing solar-energy devices.” Through this lab, Allam aims to provide undergraduate and graduate students with the proper skills to conduct advanced research. He hopes that undergraduate students, in particular, could garner solid research skills that would make them competitive applicants at research institutes.\nIn recognition of his work, Allam recently received the State Award in Advanced Technological Sciences, which is given to two individuals who are at the top of their fields, from the Egyptian Academy of Scientific Research and Technology. In addition, Allam was awarded the 2012 Misr El Kheir (MEK) award for his research on solar-energy conversion. MEK is a nongovernmental organization that aims to empower Egyptians to develop self-sustainable development models. Allam received first place in the category of Physical Sciences for his article on nanoscale control of metal oxides to develop new materials and systems with unique physical and chemical properties, which can have promising applications. The article was published in ACS Nano, a nanotechnology research journal produced by the American","Digital Devices to Enhance Nanotechnology\nAs technology advances, so does the need for more powerful and efficient power sources that can keep up with computing demands while remaining scalable and inexpensive. New nanotechnology innovations are opening the door to the technology of the future.\nArtificial neural networks are vital to developing computing abilities such as pattern recognition at levels on par with humans. Nanoscale devices called memristors (rhymes with ancestors) might be the answer to creating truly functional artificial brains. Memristors control power flow, remember charge, and are tiny and inexpensive. Scientists at the University of Southampton have shown that memristors can “learn” information without assistance and process data in real time, making them a potential foundation for the next generation of the Internet of Things.\nThe silicon carbon transistors found in conventional computer chips aren’t efficient enough to keep up with the performance requirements demanded by such chips. Nearly 20 years ago, nanotubes—minuscule rolls of carbon sheets—were discovered to be much more efficient. However, there have been several roadblocks to manufacturing functional nanotubes. To connect the nanotubes with the metal contacts needed to conduct energy, he created a new way to fuse them together at the nanotubes’ ends. IBM plans to replace silicon carbon transistors with nanotube transistors within the next decade, banking on much better performance at a fraction of the power use.\nMicrocable Power Textile\nThe next wave of electronics will require lightweight, efficient, and inexpensive power sources. Researchers at the Georgia Institute of Technology have created a potential solution: a textile that can produce power using nothing but the sun and human body motion.\nThe researchers used polymer fibers to make solar cells, and then wove the cells with fiber-based triboelectric nanogenerators (materials that become electrically charged when in contact with each other), which create energy from motion. The resulting fabric is only 320 micrometers thick—approximately one-third of a millimeter—and could be integrated into items such as tents and clothing to power devices like phones and wearables.\nTesting for disease is often a race against time, especially when trying to prevent potential outbreaks, or when quick treatment is crucial. These nanotechnology devices enable testing that’s efficient, inexpensive, and quick, producing real-time results that could help to stop epidemics in their tracks.\nModern medicine has made many once-endemic diseases, like polio and tuberculosis, rare. But sometimes bacteria and microorganisms mutate into “superbugs,” rendering conventional treatments ineffective. The World Health Organization calls antibiotic resistance one of the biggest threats to humans.\nQuickly testing bacteria for drug resistance is crucial, but the usual ways to grow and test bacteria samples in a lab take time. Researchers at the University of Alberta have created a nanoscale device that can test samples on site, in real time.\nThe device captures bacteria from a sample using a minuscule cantilever, which sends the sample through a channel where receptors identify the bacteria type. The bacteria are then exposed to antibiotics, and the reaction indicates whether they are treatable or antibiotic-resistant. Another advantage, given that sometimes only minuscule samples are available: the device can be used to test samples millions of times smaller than a raindrop.\nTesting for cancer and infectious diseases can currently mean waiting for hours before lab results are available. But researchers at the Henry Samueli School of Engineering and Applied Science, the California NanoSystems Institute, and the David Geffen School of Medicine collaborated to discover a faster method to test for the presence of proteins in body fluids that indicate cancer or other diseases.\nThe new test uses DNA nanotechnology—which exploits DNA’s chemical and physical attributes rather than its genes—to trigger a molecular chain reaction if disease-related proteins are present. The results appear in about 10 minutes, and the test can be done in a doctor’s office, removing the need for a separate hospital visit. The researchers have successfully tested for flu, with plans to test for diseases characterized by more complex protein structures. Eventually, they want to integrate the technology into a handheld reader, which could become the go-to device in every doctor’s office."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:48c6442e-3849-4f6c-92df-cb68645a524d>","<urn:uuid:2dd36a51-1467-4963-bd0e-4d6c1e4e72bd>"],"error":null}
{"question":"Compare the eligibility requirements for genetic evaluation between inherited bone marrow failure syndrome patients at NIH versus breast cancer risk assessment at Mayo Clinic - what documentation is needed from family members?","answer":"For NIH's bone marrow failure syndrome study, families must have a member with either a diagnosed inherited bone marrow failure syndrome, abnormal test results (chromosome breakage, red cell adenosine deaminase, or telomere length), or clinical features typical of known IBMFS. For Mayo Clinic's breast cancer risk assessment, patients need to gather 3-4 generations of family health history on both sides, including detailed information about parents, grandparents, aunts, uncles, cousins and children. The Mayo Clinic approach emphasizes documenting patterns of multiple conditions beyond just cancer, while NIH focuses specifically on confirming bone marrow failure syndrome diagnosis through clinical tests and family history.","context":["Individuals with one of the [glossary term:] inherited bone marrow failure [glossary term:] syndromes, and their unaffected or affected parents, brothers, sisters, and children, as well as some other relatives, are invited to participate.\nIndividuals who choose to participate in the study will be asked to complete a family history questionnaire and an individual information questionnaire. Physical examinations and samples of blood, bone marrow (from those affected with the disorder), and other tissues, including tumors, may be requested for research studies.\nWe may offer participants in this study the opportunity to visit the NIH Clinical Center where we can offer comprehensive examinations by specialists, laboratory tests, and age-appropriate [glossary term:] cancer screening tests. At the same time, we also will collect information and samples needed for the research portion of this study.\nThe purpose of this study is to identify the relationship between [glossary term:] genes, physical examination and laboratory findings, and the risk of getting cancer. Treatment is not being offered as part of this project. However, we will discuss treatment and disease prevention options with participants and their physicians (at the participant's request). We will also provide assistance in establishing care with appropriate physicians if needed. We expect our study participants to remain under the care of their regular doctors while they are participating in our study. If emergency medical problems develop while visiting the NIH Clinical Center as part of this study, we will, of course, provide the care needed to deal with that emergency.\nFor a family to be considered eligible to participate in this study, someone in the family must have must have one of the following:\n- A diagnosis of one of the named inherited bone marrow failure syndromes described on this website, or have been told they had a bone marrow failure syndrome without a specific name attached to it.\n- An abnormal [glossary term:] chromosome breakage test (the test for Fanconi’s anemia).\n- An abnormal red cell adenosine deaminase (ADA) test (the test for Diamond-Blackfan anemia).\n- An abnormal telomere length test (very short) (the test for dyskeratosis congenita).\n- The clinical and laboratory features typical for one of the known IBMFS (see individual descriptions).\nIf my family participates at the NIH Clinical Center, what will we receive?\nWe invite affected individuals as well as their parents, siblings, and children to come to the NIH for a one week evaluation.\n- Detailed personal medical history along with family history, including [glossary term:] cancer.\n- Complete physical examinations, blood and bone marrow studies (as appropriate), x-ray/imaging tests, and age- and diagnosis-appropriate cancer screening.\n- Specialty consultations that are age- and diagnosis-appropriate, including audiology (hearing tests), dentistry, dermatology (skin examination), endocrinology (hormone studies), gastroenterology (intestines), genetic counseling, gynecology, hepatology (liver evaluation), neurology, ophthalmology (eye examination), otolaryngology (ears, nose and throat examination), neuro-otology (nerve-related hearing studies), physiatry (rehabilitation, physical medicine), pulmonology (lung studies), urology (kidneys, bladder, prostate), and others as needed.\n- Information regarding genetic test results, when possible, for those who choose to learn their mutation status.\n- Advice about the management of existing clinical problems and, occasionally, information related to medical issues that may not have been previously recognized.\n- Education and advice regarding possible ways to reduce cancer risk.\n- Results of clinical tests and cancer screening evaluations, along with summary reports.\n- Referral back to the primary physician or other consultants at home for medical interventions based on results of tests performed at the NIH.\n- Reporting of scientific results to the family at the end of the study, which they may share with other physicians involved in the family’s care.\n- Reimbursement for costs related to traveling to the NIH Clinical Center.\nThe following link will take you to the description of the protocol: https://www.cc.nih.gov/home/clinicalstudies.html.","By Cynthia Weiss\nDEAR MAYO CLINIC: My grandmother and mother, as well as an aunt and a cousin, have had breast cancer. Another cousin was diagnosed with colon cancer recently. It has been suggested that I undergo genetic counseling to determine my cancer risk. As a young man, is genetic testing necessary for me? What benefit would I gain from visiting a genetic counselor?\nANSWER: Although it can be daunting to have a loved one diagnosed with cancer, having a family history does not mean that you will automatically get cancer. This is one of the reasons why having a discussion with a genetic counselor can be valuable.\nA genetic counselor is someone who reviews your personal health history and your family's health history to identify your personal risk for certain conditions. A genetic counselor can try to determine if there is a pattern or connection among family members' diagnoses and how that may affect you.\nFor instance, you mentioned that your grandmother, mom and a cousin have had breast cancer, but it's unclear if all of these women are on the same side of the family. If they are all related — for example they are all on the maternal side of your family — then that suggests more of a pattern that could potentially increase your risk, even as a man. Breast cancer affects men, too, though it occurs more infrequently.\nPeople seem to be most aware of genetic counseling when it comes to breast cancer. This likely is due to the fact that the most common genes associated with increased breast cancer risk — BRCA1 and BRCA2 — have received a lot of media attention over the years. In general, though, only about 5%–10% of breast cancers have a hereditary cause that can be identified. If a hereditary cause for breast cancer is discovered, this condition may increase the risk for other types of cancer, as well.\nOther cancers that may have a hereditary connection include ovarian, colon, prostate, uterine and pancreatic cancers. Concerns for a hereditary cancer syndrome rise if people are diagnosed at younger ages, have a personal history of more than one cancer, or have multiple family members with the same or associated cancers. This information may be useful for you if you decide to have children. You'll be able to determine the likelihood that you might pass along a gene and increase the risk to your offspring for certain cancers.\nMeeting with a genetic counselor doesn't immediately mean that you need genetic testing. Rather, the goal of the appointment is to have a discussion that can guide you toward making an informed decision regarding genetic testing. Discussing the potential risks and limitations of genetic testing are just as important as reviewing potential benefits of testing.\nAnother benefit of meeting with a genetic counselor is to learn more about your family risk for certain conditions, which could be valuable in the future. For instance, understanding your risk for cancer is important, but learning about certain hereditary heart and neurologic conditions, as well as more rare genetic conditions like cystic fibrosis, might help with family planning down the road.\nSometimes reviewing all of this information together allows things to be put in a new context. For some families, it illustrates a clear pattern of increased risk for certain conditions. In other cases, though, it may lower your concern.\nI recall a young woman who came to see me to discuss her significant family history of cancer. But as we began charting her family tree, we realized that only a few relatives had developed skin cancer, and they worked outdoors on a farm or in construction. In reality, her cancer risk was minimal given her lifestyle.\nAlthough it can be challenging to learn about the details of your family's health tree, especially if prior generations didn't share as much or document health concerns, it is important to talk with your family prior to meeting with a genetic counselor if you can.\nFamily history should be gathered for three or four generations on both sides, and include parents, grandparents, aunts, uncles, cousins and children.\nHelpful information to gather includes:\nA good resource to help you get started is a free online tool called My Family Health Portrait. This tool is available through the surgeon general's office. It allows you to collect the information and create a family pedigree that can be printed and shared with health care professionals and your family.\nAs you discuss your family history, don't forget to talk about conditions that may not have a strictly genetic cause but may have a genetic link. Although there are conditions such as diabetes where a genetic test is not available, it's important to document the patterns in your family and share them with your primary care provider. — Sarah Mantia, Clinical Genomics, Mayo Clinic, Jacksonville, Florida\nRead more stories about advances in individualized medicine.\nRegister to get weekly updates from the Mayo Clinic Center for Individualized Medicine blog.\nTags: breast cancer, Cancer, cancer, center for individualized medicine, DNA Testing, gene sequencing, genetic testing, Genetics, genomic medicine, genomic medicine, individualized medicine, Lynch syndrome, Multi-omics, Precision Medicine, predictive genomics"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:8d07abe1-6d94-40be-91d7-44be68bacb39>","<urn:uuid:c23a6a16-ada2-4a57-87a9-a93a998dba16>"],"error":null}
{"question":"What is the difference between how puncture wounds and phlebotomy procedures break the skin - can you explain such as with examples?","answer":"Puncture wounds and phlebotomy procedures break the skin differently. A puncture wound is caused by an object accidentally piercing the skin, creating a small hole that can force bacteria and debris deep into tissue. Examples include nail penetration or splinters. In contrast, phlebotomy is a controlled medical procedure where a trained technician deliberately draws blood using sterile, single-use tools in a clinical setting. Phlebotomists must confirm their equipment is sterile before collection and follow proper labeling and documentation procedures to ensure patient safety.","context":["Cuts, Scrapes and Puncture Wounds (cont.)\nJohn P. Cunha, DO, FACOEP\nJohn P. Cunha, DO, is a U.S. board-certified Emergency Medicine Physician. Dr. Cunha's educational background includes a BS in Biology from Rutgers, the State University of New Jersey, and a DO from the Kansas City University of Medicine and Biosciences in Kansas City, MO. He completed residency training in Emergency Medicine at Newark Beth Israel Medical Center in Newark, New Jersey.\nWilliam C. Shiel Jr., MD, FACP, FACR\nDr. Shiel received a Bachelor of Science degree with honors from the University of Notre Dame. There he was involved in research in radiation biology and received the Huisking Scholarship. After graduating from St. Louis University School of Medicine, he completed his Internal Medicine residency and Rheumatology fellowship at the University of California, Irvine. He is board-certified in Internal Medicine and Rheumatology.\nIn this Article\n- Cuts, scrapes (abrasions), and puncture wounds facts\n- What is the best first aid for a cut or scrape?\n- Who should seek medical care for a cut?\n- What are the signs and symptoms of a wound infection?\n- How are puncture wounds different?\n- Will I need a tetanus shot for a cut, scrape, or puncture wound?\nHow are puncture wounds different?\nA puncture wound is caused by an object piercing the skin, creating a small hole. Some punctures can be very deep, depending on the source and cause.\nPuncture wounds do not usually bleed much, but treatment is necessary to prevent infection. A puncture wound can cause infection because it forces bacteria and debris deep into the tissue, and the wound closes quickly forming an ideal place for bacteria to grow.\nFor example, if a nail penetrates deep into the foot, it can hit a bone and introduce bacteria into the bone. This risk is especially great if an object has gone through a pair of sneakers or tennis shoes. The foam in sneakers can harbor bacteria that can lead to serious infection in the tissues.\nFirst aid for puncture wounds includes cleaning the area thoroughly with soap and water. These wounds are very difficult to clean out. If the area is swollen, ice can be applied and the area punctured should be elevated. Apply antibiotic ointments (Bacitracin, Polysporin, Neosporin) to prevent infection. Cover the wound with a bandage to keep out harmful bacteria and dirt. Cleanse the puncture wound and change the bandage three times a day, and monitor for signs of infection (the same signs as in the cuts section). Change the bandage any time it becomes wet or dirty.\nPeople with suppressed immune systems or any particularly deep puncture wounds should be seen by a doctor. If it is difficult to remove the puncturing object, it may have penetrated the bone and requires medical care.\nMost puncture wounds do not become infected, but if redness, swelling or bleeding persists, see your doctor.\nPuncture wounds to the feet are a particular concern. Wear shoes to minimize the risk of a puncture wound from a nail or glass, especially if the affected person has diabetes or loss of sensation in the feet for any reason.\nAdditional common causes of puncture wounds can include animal or human bites, or splinters from wood or other plant material, which carry a high risk of infection and should be treated by a physician.\nFind out what women really need.","Choosing a Phlebotomy Technician Training Program near Canyon Country California\nChoosing the right phlebotomy technician school near Canyon Country CA is an important initial step toward a gratifying career as a phlebotomist. It might seem like a challenging undertaking to assess and compare all of the school options that are accessible to you. However it’s important that you do your due diligence to ensure that you get a quality education. In reality, most potential students start the process by considering two of the qualifiers that initially come to mind, which are location and cost. An additional factor you might consider is whether to attend online classes or commute to a nearby campus. We’ll review more about online schools later in this article. What you need to remember is that there is far more to checking out phlebotomy training programs than locating the closest or the cheapest one. Other factors such as reputation and accreditation are also important considerations and should be part of your selection process also. Toward that end, we will supply a list of questions that you need to ask each of the phlebotomy schools you are reviewing to help you choose the right one for you. But prior to doing that, let’s address what a phlebotomist is and does, and afterwards continue our conversation about online schools.\nPhlebotomist Job Description\nA phlebotomist, or phlebotomy technician, draws blood from patients. While that is their main task, there is in fact far more to their job description. Before collecting a blood sample, a phlebotomist has to confirm that the tools being employed are sterile and single use only. Following the collection, the sample has to be correctly labeled with the patient’s information. Afterward, paperwork must be properly filled out in order to track the sample from the time of collection through the laboratory testing process. The phlebotomist then delivers the blood to either an in-house lab or to an outside lab facility where it can be tested for such things as infectious diseases, pregnancy or blood type. Some phlebotomists actually work in Canyon Country CA area laboratories and are responsible for making sure that samples are tested properly utilizing the highest quality assurance procedures. And if those weren’t enough duties, they can be asked to train other phlebotomists in the drawing, transport and follow-up process.\nWhere do Phlebotomists Practice?\nThe easiest answer is wherever patients are treated. Their work environments are numerous and diverse, including Canyon Country CA hospitals, medical clinics, nursing homes, or blood banks. They may be tasked to draw blood samples from patients of of every age, from infants or toddlers to senior citizens. Some phlebotomy techs, based on their practice and their training, specialize in collecting blood from a certain type of patient. For example, those working in a nursing home or assisted living facility would exclusively be collecting blood from elderly patients. If they are practicing in a maternity ward, they would be collecting blood from newborns and mothers solely. In contrast, phlebotomy technicians practicing in a general hospital environment would be collecting samples from a wide range of patients and would work with different patients every day.\nPhlebotomy Education, Certification and Licensing\nThere are primarily 2 kinds of programs that furnish phlebotomist training, which are degree and certificate programs. The certificate program normally takes less than a year to finish and provides a general education along with the training on how to draw blood. It offers the fastest route to becoming a phlebotomy tech. An Associate of Science Degree in Clinical Laboratory Science, even though it’s not exclusively a phlebotomy degree, will include training on becoming a phlebotomy tech. Offered at Canyon Country CA junior and community colleges, they normally take two years to complete. Bachelor’s Degrees are less accessible and as a four year program furnish a more expansive foundation in lab sciences. When you have finished your training, you will no doubt want to be certified. Although not mandated in the majority of states, most Canyon Country CA employers look for certification prior to hiring technicians. A few of the primary certifying organizations include:\n- National Phlebotomy Association\n- National Healthcareer Association (NHA).\n- American Society for Clinical Pathology (ASCP).\n- American Medical Technologists (AMT).\nThere are a few states that do require certification in order to practice as a phlebotomist, like California and Nevada. California and a few additional states even require licensing. So it’s essential that you select a phlebotomist training program that not only furnishes a premium education, but also readies you for any licensing or certification exams that you elect or are required to take.\nPhlebotomy Online Training\nTo start with, let’s resolve one likely misconception. You can’t obtain all of your phlebotomist training online. A substantial component of the curriculum will be clinical training and it will be conducted either in an approved healthcare facility or an on-campus lab. Many courses also require completing an internship in order to graduate. However since the non-clinical part of the training may be accessed online, it may be a more practical option for some Canyon Country CA students. As an additional benefit, many online colleges are more affordable than their traditional counterparts. And some expenses, including those for commuting or textbooks, may be reduced as well. Just confirm that the online phlebotomist program you enroll in is accredited by a regional or national accrediting agency (more on accreditation to follow). With both the comprehensive clinical and online training, you can obtain a superior education with this approach to learning. If you are disciplined enough to study at home, then attaining your degree or certificate online may be the right choice for you.\nTopics to Ask Phlebotomy Schools\nSince you now have a general idea about what is involved in becoming a phlebotomist, it’s time to start your due diligence process. You might have already selected the type of program you wish to enroll in, whether it be for a certificate or a degree. As we previously mentioned, the location of the school is relevant as well as the cost of tuition. Possibly you have decided to enroll in an phlebotomy online college. All of these decisions are a critical part of the procedure for selecting a program or school. But they are not the sole considerations when making your decision. Following are some questions that you need to ask about each of the Canyon Country CA schools you are considering prior to making your ultimate decision.\nIs the Phlebotomy Program State Specific? As previously mentioned, each state has its own regulations for practicing as a phlebotomy technician. Several states require certification, while a few others mandate licensing. Every state has its own requirement regarding the minimum amount of practical training performed before working as a phlebotomist. Consequently, you may need to pass a State Board, licensing or certification examination. Therefore it’s extremely important to enroll in a phlebotomist program that fulfills the state specific requirements for California or the state where you will be working and prepares you for all examinations you may have to take.\nIs the College Accredited? The phlebotomist program and school you pick should be accredited by a respected national or regional accrediting agency, for example the National Accrediting Agency for Clinical Laboratory Sciences (NAACLS). There are many advantages to graduating from an accredited program aside from a guarantee of a quality education. To begin with, if your program has not received accreditation, you will not be able to take a certification examination offered by any of the previously listed certifying organizations. Next, accreditation will help in getting financial aid or loans, which are typically unavailable for non-accredited schools. Finally, graduating from an accredited school can make you more attractive to prospective employers in the Canyon Country CA job market.\nWhat is the School’s Reputation? In a number of states there is minimal or no regulation of phlebotomist schools, so there are those that are not of the highest quality. So in addition to accreditation, it’s essential to check the reputations of all schools you are reviewing. You can begin by requesting references from the schools from employers where they refer their graduates as part of their job placement program. You can research online school reviews and rating services and ask the accrediting agencies for their reviews as well. You can even contact several Canyon Country CA hospitals or clinics that you might have an interest in working for and find out if they can provide any insights. As a closing thought, you can check with the California school licensing authority and ask if any complaints have been submitted or if the schools are in total compliance.\nIs Plenty of Training Included? First, check with the state regulator where you will be working to find out if there are any minimum requirements for the length of training, both classroom and practical. As a minimum, any phlebotomy program that you are looking at should provide at least 40 hours of classroom training (most require 120) and 120 hours of clinical training. Anything lower than these minimums may signify that the Canyon Country CA training program is not comprehensive enough to furnish adequate training.\nAre Internship Programs Included? Ask the colleges you are reviewing if they have an internship program in collaboration with Canyon Country CA medical facilities. They are the ideal means to obtain hands-on clinical training frequently not obtainable on campus. As an added benefit, internships can assist students develop contacts within the local Canyon Country health care community. And they look good on resumes as well.\nIs Job Placement Support Available? Getting your first phlebotomist position will be a lot easier with the support of a job placement program. Ask if the programs you are looking at provide assistance and what their job placement percentage is. If a college has a higher rate, signifying they place most of their students in jobs, it’s an indication that the program has both an excellent reputation together with an extensive network of professional contacts within the Canyon Country CA medical community.\nAre Classes Offered to Fit Your Schedule? Finally, it’s crucial to verify that the ultimate school you choose provides classes at times that are compatible with your hectic schedule. This is particularly important if you choose to still work while going to college. If you can only attend classes in the evenings or on weekends near Canyon Country CA, make certain they are available at those times. Additionally, if you can only attend on a part-time basis, verify it is an option as well. And if you have decided to study online, with the clinical training requirement, make sure those hours can also be completed within your schedule. And ask what the make-up protocol is should you need to miss any classes due to emergencies or illness.\nConsidering Phlebotomy Training near Canyon Country CA?\nCanyon Country, Santa Clarita, California\nCanyon Country is north of the San Fernando Valley via Newhall Pass through the Santa Susana and San Gabriel Mountains. Canyon Country is located in the upper watershed of the Santa Clara River in the Santa Clarita Valley and Sierra Pelona Mountains foothills.\nThe area was the ancestral homeland of the Tataviam people for over five hundred years, and other tribes before then, such as the Tongva, Kitanemuk, and Serrano people. After the Spanish invasion, the valley first became grazing lands of the Mission San Fernando Rey de España around 1790. In 1834, after Mexican Independence, it became part of the Rancho San Francisco land grant centered on the confluence of the Santa Clara River and Castaic Creek.\nIn the 1880s the rancho become the Newhall Ranch empire of Henry Newhall, now the present day Newhall Land and Farming Company. In 1928 the St. Francis Dam collapsed, suddenly flooding and washing away settlements and people along the Santa Clara River section of present-day Santa Clarita not including Canyon Country. Canyon Country was originally to be called Solemint. It later absorbed the community to the west called Honby. In the 1960s and 1970s the Newhall Land company's suburban developments transformed Canyon Country and the surrounding towns into a focused residential and cultural city.\nThe house of The Crandall family used in the movie Don't Tell Mom the Babysitter's Dead is located in this town. A home here was also featured in the climax of Real Genius, but it no longer exists.\nEnroll in the Ideal Phlebotomy Training Program near Canyon Country CA\nMaking certain that you select the right phlebotomist training is an essential first step toward your success in this rewarding healthcare field. As we have discussed in this article, there are a number of factors that go into the selection of a quality school. Phlebotomist certificate or degree programs can be offered in a number of academic institutions, including junior or community colleges, trade schools, and colleges and universities that provide a wide assortment of courses in medical care and health sciences. Training program offerings may differ slightly from state to state as every state has its own prerequisites when it concerns phlebotomy training, licensing and certification. The most critical point is that you need to thoroughly evaluate and compare each college prior to making your final decision. By addressing the questions that we have provided, you will be able to fine tune your choices so that you can pick the ideal college for you. And with the proper education, you can realize your goal of becoming a phlebotomy technician in Canyon Country CA.\nOther Cool Cities in California\nBusiness Results 1 - 10 of 472\nBing: California Phlebotomy Training Schools Search results\nNational Phlebotomy Certification | Phlebotomy Certification Renewals | CE\nHigh school graduation or equivalent and completion of a NAACLS-approved phlebotomy program within the last five years that consists of 80 hours of classroom training and 40 hours of clinical training in an accredited lab with a minimum of 200 successful blood draws.. Two full years as a part-time phlebotomist or one year as a full-time phlebotomist, or; A letter from a health care supervisor ...\n40 Hour Home Health Aide Training Programs - California\nCalifornia Conference of Local Health Officers. CCLHO Board and Committee Information; ... Phlebotomy Certificate; Recent; ... ADDRESS CITY ZIP CODE PHONE NUMBER; LOS ANGELES: A+ HEALTH CARE TRAINING: 14540 VICTORY BLVD. STE. 217A: VAN NUYS: 91411 (818) 387-6844: FRESNO: ACCLAIM EDUCATION, INC. 3636 NORTH FIRST STREET, SUITE 139: FRESNO: 93726\nRenewal of Clinical Laboratory Personnel Licenses ... - California\nContinuing Education Requirements Phlebotomy Certificates. Limited Phlebotomy Technician (LPT) and Certified Phlebotomy Technicians (CPT I and CPT II ) must complete a total of 6 contact hours of continuing education provided by a Department-approved accrediting agency or an accredited academic institution. All Other Clinical Laboratory Personnel Licenses\nBest 15 Law Schools in California in 2022 - Best Value Schools\nThe school ranks among one of the top 10 U.S. Schools for diversity. It also ranks in the top 3 for required experiential learning credits (15) among U.S. Schools. The faculty has been published extensively in a number of law reviews and specialty journals, including California Law Review, Boston College Law Review, Iowa Law Review, among others.\nEmergency Medical Technician (EMT) vs. Paramedic\nEMT training programs can usually be completed in 150 hours. Students wishing to become successful EMTs must also pass the National Registry Emergency Medical Technicians (NREMT) cognitive exam. A paramedic is the highest level of EMT (Emergency Medical Technician) certification.\nBest 20 Colleges in Southern California in 2022 - Best Value Schools\nOur Ranking Methodology We ranked the schools on this list using a formula that produces a rating for each school. Our formula takes two vital statistics (yearly tuition rate and graduation rate) into account. Several of the schools on this list have different tuition rates for California residents and non-residents. If this is the case, … Best 20 Colleges in Southern California in 2022 Read ...\nPhlebotomy Technician (NCPT) Certification - National Center for ...\nYour Path to becoming a Phlebotomy Technician. Phlebotomy technicians are healthcare professionals responsible for obtaining patient specimens. They can perform various functions such as venipuncture, micro-collection, and specimen processing and handling while maintaining patient safety and confidentiality.\nNAACLS - National Accrediting Agency for Clinical Laboratory Science ...\nIn order to provide complete functionality, this web site needs to store browser cookies. If you continue to use this site, you agree to store cookies on your device.\nOnline LVN Programs in California: See CA Approved Schools\nEarning your LVN credential can be a great entry point into a rewarding nursing career. Licensed vocational nurse positions in California are on the rise with an estimated growth rate of 15 percent during the 2018-2028 decade, according to Projections Central, and 11,600 new jobs expected to come open.. To practice as an LVN in California you’ll need to complete your credential and pass the ..."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:600b6bbb-ac1e-444a-8fe3-b558405c098a>","<urn:uuid:eea934bc-0e16-4103-8817-23cccb5c9b66>"],"error":null}
{"question":"What are the key benefits of patch management software, and what potential security risks can arise when integrating access control systems with networks?","answer":"Patch management software offers several key benefits including automating the discovery and delivery of missing patches, keeping servers and network hardware up-to-date, and helping companies stay ahead of software threats by monitoring vulnerabilities. It enables patches to be pushed to servers and validated quickly, with features like automatic scans, multi-patch deployments, and scheduled deployment. However, when access control systems are connected to networks, they become vulnerable to cyber intrusions. These systems can be compromised through malware, man-in-the-middle attacks, brute force attacks, dictionary attacks, IP spoofing, and denial of service attacks. Attackers might target these systems not only to gain physical access by unlocking doors but also to infiltrate the organization's network and access sensitive information or spread ransomware.","context":["Patch Management Software\nPatch Management software keeps servers and network hardware up to date by automating the discovery and delivery of missing patches and updates. Patches can be pushed to servers and validated in a short period of time. Companies and programmers can stay ahead of software threats by monitoring vulnerability and automating the response of exploits launched into a system. Patch Management software is related to Server Management software, Requirements Management software, and Website Monitoring software.\nFilter Results (28)\n- Cloud, SaaS, Web (21)\n- Installed - Mac (9)\n- Installed - Windows (18)\n- Mobile - Android Native (6)\n- Mobile - iOS Native (7)\n- Automatic Scans (22)\n- Custom Patches (17)\n- Multi-Patch Deployments (23)\n- Network Wide Management (20)\n- Patch Management (12)\n- Remote Access (11)\n- Remote Protection (17)\n- Scheduled Deployment (23)\n- Subscription Services (11)\n- Vulnerability Scanning (20)\nPatch Everything! Cloud Management Suite allows you to automatically keep desktops, laptops, servers and remote users up-to-date with the latest security patches and software updates from Microsoft, MacOS, Linux and third-party vendors like Adobe, Java and Chrome. Patch Virtual Machines, Legacy OS and IoT devices too! Automatic and Pre-built Patch commands, such as Critical, Top 10 Windows, and Third-Party will have you looking like you earned a PhD in Patch Management. Learn more about Patch Manager Cloud-based Patch Management automatically deploy required patches to endpoints. Produce reports for management showing patch coverage. Learn more about Patch Manager\nThe SolarWinds RMM remote monitoring and management platform is designed to make managing, maintaining, and protecting IT assets easy for IT service providers and in-house IT professionals. From a single web-based dashboard, you get access to security features, like antivirus, patch management, email protection, and integrated backupin addition to drag-and-drop automation that doesnt require you to learn a scripting language. In short, RMM is built to provide efficiency and grow as you grow. Learn more about SolarWinds RMM SolarWinds® RMM is a remote monitoring and management platform designed to make managing, maintaining, and protecting IT easy. Learn more about SolarWinds RMM\nKaseya VSA is an integrated Patch Management platform that can be leveraged to easily deploy software updates to end-points and ensure overall system compliance. Configure, automate, deploy and report on Patch within your network. Kaseya VSA makes your IT staff more productive, your services more reliable, your systems more secure, and your value easier to show. VSA capabilities include: Patch Management, Remove Monitoring, Remote Control, AV/AM, Process Automation, and more. Learn more about Kaseya VSA Easily deploy Patches with the power of Kaseya VSA - the IT Systems Management platform designed to help you manage all of IT. Learn more about Kaseya VSA\nNinjaRMM is a cloud-based patch management platform that lets you patch your endpoints to ensure safety and compliance. You can mass-configure and automate patches based on a schedule or categories you approve or reject. See every aspect of patching across all your endpoints with robust dashboards and reports. In addition to patching, NinjaRMM has all-in-one features like remote control, scripting, and antivirus management, all from a single pane of glass. Learn more about NinjaRMM NinjaRMM is the easiest, all-in-one remote IT management and monitoring solution that IT professionals love for patch management. Learn more about NinjaRMM\nPulseway is a mobile-first patch management software allowing you to use OS or WSUS patch management to stay on top of updates for all your IT systems. Never again have updates interrupt your or your clients¿ workday with the ability to schedule for a time that suits everyone best and receive instant notifications the second that update has been completed. Learn more about Pulseway Pulseway is the ultimate patch management software for sysadmins and MSPs allowing you to scale when you see fit. Learn more about Pulseway\nAtera is a cloud-based IT management platform that provides an all-in-one solution built for MSPs and IT Support personnel. Atera offers RMM, PSA & Helpdesk, and Remote Access in one powerful solution. Atera's innovative platform is simple to use, allowing quick on-boarding. Atera's disruptive pricing model allows users to pay per technician with unlimited agents, offering transparent and predictable pricing. We offer a free 30-day trial, so sign up now and get started right away! Learn more about Atera Remote Monitoring Management (RMM), Professional Service Automation (PSA) and Remote Access - IT happiness in one place. Learn more about Atera\nA web-based windows desktop management software that helps in managing 1000s of desktops from a central location. It can manage desktops both in LAN and across WAN. It automates regular desktop management activities like installing software, patches, and service packs, standardize the desktops by applying uniform configurations such as wallpapers, shortcuts, printer, etc. In simple terms, it helps administrators to automate, standardize, secure, and audit their windows network. Learn more about ManageEngine Desktop Central Helps administrators to automate, standardize, secure, and audit their windows network. Desktop Central now supports MDM also. Learn more about ManageEngine Desktop Central\nPatch management wrapped in a single, All-in-One solution, Naverisk provides tools for device and network monitoring, alerting, auditing, patching, reporting, ticketing, workflow, automation, scripting, and much more. Plus, our partners enjoy exceptional 24/7 global support for the platform at no additional cost. Get a Free Trial today! Learn more about Naverisk RMM & PSA Naverisk provides tools for patching, device and network monitoring, alerting, auditing, reporting, ticketing, scripting + more Learn more about Naverisk RMM & PSA\nJamf Pro is a complete device management solution for IT pros to simplify the deployment, inventory and security of Macs, iPads, iPhones and Apple TVs. Designed to automate vital tasks such as patch management while driving end-user productivity, Jamf Pro is the EMM tool that delights IT pros and the users they support by delivering on the promise of unified endpoint management for Apple devices. The IT experience you want is within reach. Request a free trial and become an IT superhero today. Learn more about Jamf Pro A complete Mobile Device Management solution for IT pros to simplify the deployment, inventory, and security of Apple devices. Learn more about Jamf Pro\nIntegrated into SysAid IT Asset Management, SysAid Patch Management keeps Windows-based servers and PCs up-to-date with the latest security patches and updates. SysAid harnesses OEM technology to provide a full and seamless patch management solution from within SysAid Help Desk and SysAid ITSM. SysAid offers an audited patching process, through SysAid Change Management, to ensure that all patch-related changes are properly documented, correctly performed and comply with regulations. Learn more about SysAid Offered in both cloud & on-premise, SysAid combines all the essentials, so you can easily manage patches in one place. Learn more about SysAid\nPatch management is one of the largest concerns of IT service providers and their clients. With new vulnerabilities being discovered almost daily, keeping systems up-to-date with patches is often a full-time job, especially in larger environments. ConnectWise Automate, formerly LabTech, integrates with industry-leading patch management solutions and contains numerous built in scripts & automated patch deployment. Learn more about ConnectWise Automate Automate integrates with industry-leading patch management solutions & contains numerous built in scripts & automated patch deployment. Learn more about ConnectWise Automate\nPatch Manager Plus is an automated patch management software that provides enterprises with a single interface for all patch management tasks. Works across platforms, helping you patch Windows, Mac, Linux & 300+ third-party applications. You can automate the scan for missing patches, test & approve for hassle-free patching, customize deployment policies to meet business needs, decline patches & generate vulnerability reports. Patch Manager Plus is now available both on cloud and on-premise. Learn more about Patch Manager Plus Patch Manager Plus is an automated patch management software to patch Windows, Mac, Linux updates and 300+ third-party applications. Learn more about Patch Manager Plus\nSolarWinds Patch Manager makes it easy to perform 3rd-party patch management across tens of thousands of servers and workstations, and enables you to leverage and extend the capabilities of Microsoft WSUS or SCCM to report, deploy, and manage 3rd-party patches as well as Microsoft patches. Learn more about Patch Manager Automated patch management software for Microsoft servers, workstations, and third-party applications. Learn more about Patch Manager\nFileWave's hybrid approach, including traditional client management for Mac and Windows alongside modern Unified Endpoint Management (UEM,) makes endpoint management simple today, and in the future. FileWave includes IT asset inventory; software packaging and distribution; patch management; and OS imaging and deployment for Mac and Windows. Manage all of your devices, apps, and settings on computers and mobile devices. Supports Windows, Mac, iOS, Android and Chrome OS. Learn more about FileWave Endpoint Management Inventory and manage devices, apps, and settings on computers and mobile devices. Supports Windows, Mac, iOS, Android, and Chrome OS. Learn more about FileWave Endpoint Management\nKenna Security integrates vulnerability scan data with big data analytics and multiple exploit feeds to predict the risk of a vulnerability being exploited. It calculates risk scores for your enterprise, organizations and critical asset groups. Remediation guidance prioritizes patches that have the greatest risk reduction. Actionable results are achieved within hours of installing scan data. Kenna Security uses exploit intelligence feeds to identify vulnerabilities being exploited in the wild and put your company at risk.\nGFI LanGuard is a complete vulnerability management solution, which allows you to scan, detect, assess and rectify security vulnerabilities on your network. It also provides the tools to remediate vulnerabilities and install missing patches on the network. GFI LanGuard gives you a complete picture of your network set-up and helps you to maintain a secure network state faster and more effectively. We scan your network and ports to detect, assess and correct security vulnerabilities with minimal administrative effort.\nControl the levels of automation required for patches, images and updates, using a single console for operating systems from different vendors. Control the levels of automation required for patches, images and updates, using a single console for operating systems.\nPatch management is one of the largest concerns of IT service providers and their clients. With new vulnerabilities being discovered almost daily, keeping systems up-to-date with patches is often a full-time job, especially in larger environments. ConnectWise Automate, formerly LabTech, integrates with industry-leading patch management solutions and contains numerous built in scripts & automated patch deployment. Automate integrates with industry-leading patch management solutions & contains numerous built in scripts & automated patch deployment.\nAutomoxs cloud-based solution simplifies patching and configuration management across Windows, Linux, Mac OS X, and 3rd party software. Automox provides IT managers and sysadmins with a patching system of record to track, control, and manage their patching process, providing greater security, improved productivity, and significant time savings. Automox is designed to complement your environment, whether you need a new patching solution or want to improve your existing patch workflow. Automox Provides Automated Patch Management, Real Time Inventory Visibility, Software Deployment, and Full Configuration Control.\nSnaPatch connects and interacts with both SCCM and VMware. By bridging both platforms it allows us to remove the inherent risk associated with patching and updating servers. This is achieved by taking snapshots of the virtual server prior to deploying any patches. SnaPatch also connects to your SCCM SQL database to pull vasts amount of information to provide you with the clearest and most up to date picture of your environment. SCCM is a fantastic tool for deploying updates to your Windo Automate snapshotsof Virtual Servers prior to deployment of Windows updates from Microsofts SCCM all from one easy to use console.\nAllows for cross platform capabilities, reporting on every device within the enterprise, and deploying custom applications. Allows for cross platform capabilities, reporting on every device within the enterprise, and deploying custom applications.\nKeep pace with rapid changes in technology, software licensing, and regulatory issues using a rich set of integrated application modules you access from an integrated console. Simplify asset management and software distribution, and secure complex computing environments with automated, proactive system monitoring and updates. Keep pace with rapid changes in technology, software licensing, and regulatory issues using integrated application modules.\nAutomated or customized manual, rule-based, secure patch deployment. At-a-glance risk assessment based on current CVEs. Automated patch and update management for Microsoft products and most common third-party applications using MSI, InstallShield, Wise, .exe or other installers. Comprehensive control over when, where and how patches and updates are deployed to minimize user disruption. Easy to operate, short training period. Automated or manual rule-based OS and application patching/updating. At-a-glance risk assessment. Fine control of patch deployments.\nPanda Adaptive Defense 360 is an advanced cybersecurity suite that integrates Endpoint Protection and Endpoint Detection and Response (EDR) solutions, with 100% Attestation and Threat Hunting & Investigation Services, all delivered via a single lightweight agent. The combination of these solutions, modules and services provides a highly detailed visibility of all endpoint activity, n absolute control of all running processes, and the reduction of the attack surface. Panda Patch Management reduce the risk and complexity of vulnerabilities in systems and third party applications","Assessing Cyber Risks to Your Access Control System\nPrint Issue: March 2020\nAround lock sat in the front of Joseph Bramah’s shop in London with a challenge displayed on the window: whoever could pick the Bramah Precision lock would win 200 guineas (roughly $30,000 today). That challenge would remain for 67 years until A.C. Hobbs—an American locksmith—took up the gauntlet.\nHobbs brought a great deal of experience to the table. He had gained recognition in America for demonstrating to bank managers that their locks could be picked, so they should be replaced with locks of his own invention.\nAt the Great Exhibition hosted in London in 1851, Hobbs announced after successfully picking a Chubb “Detector” lock that he would open Bramah’s creation. Bramah’s sons set Hobbs up with a workspace above their shop. For 52 hours, Hobbs worked at the lock until he successfully picked it.\nHobbs’ success became known as The Great Lock Controversy, striking fear into the hearts of everyone who had previously used the Bramah lock—including the Bank of England—because they believed it could not be picked. Their sense of security was shattered.\nSince then, methods for locking doors and controlling access have changed with the times and technology advancements. Now, instead of having a guard monitor and log when a door is unlocked and opened in a facility, and then verify that that individual is allowed to do so, most organizations rely on access control systems. And often, these systems are connected to the Internet—making them vulnerable to cyber intrusions.\n“Older access control systems were not meant to be tied to the building network or the organization’s network,” says Coleman Wolf, CPP, CISSP, senior security consultant for Environmental Systems Design, Inc., (ESD) and a member of the ASIS International IT Security Council. “There are adapters that can be used to put those on the network. They function just fine. I can access the control panel from my desk, but the security isn’t always the best.”\nThe access control system is “meant to provide a function, but either the device was not built to have password protection or the person who installed it wanted to get it up and running, so they didn’t put in the effort to install the security with it,” Wolf adds.\nBy connecting an access control system to the Internet, the system becomes part of the Internet of Things (IoT). Typical IoT devices include thermostats, electrical outlets, light switches, refrigerators, smart speakers, and doorbells. They also now include—in the security arena—cameras, alarm systems, smoke detectors, locks, and other access control devices, says David Feeney, CPP, PMP (Project Management Professional), and advisory manager of cyber and physical security risk services at Deloitte.\n“Before IoT, everything that was connected to a network was a network device in the traditional sense,” explains Feeney, who is past chair of the ASIS Physical Security Council. “Now, almost anything can be a network device. And while the computer industry has had decades to incorporate security into its products, services, and overall DNA, IoT is essentially a toddler—growing rapidly but with most of its maturation still ahead.”\nAll of these IoT devices face a “gauntlet of cyber threats,” Feeney says, including malware, man-in-the-middle attacks, brute force attacks, dictionary attacks, IP spoofing, denial of service and distributed denial of service (DDoS) attacks, session hijacks, and more.\n“The difference that IoT brings is that the attack surface—the aggregation of all points at which an attacker can gain access—is now exponentially larger once access control and other IoT devices are added to the network,” Feeney adds.\nIt might seem obvious why someone would want to compromise an access control system: to unlock the doors to a building to gain entry.\n“The first thing that people think about is that once they’re inside the system, they have control over the system so they can unlock doors or disable sensors—things that are part of the actual mission of the access control system itself,” Wolf says.\nFor instance, in a worst-case scenario at a highly controlled environment like a hospital, a compromised access control system could be used to lock surgeons out of an operating room or open doors to the pharmacy.\nBut there’s another equally concerning reason someone might want to hack an access control system, Feeney adds.\n“Your natural first thought might be that access control systems are attacked because attackers want to gain access to an area, and the system is standing in their way,” explains Feeney. “That is one reason. But the reason is often that an attacker simply wants access to the network, and an access control system is as good an entry point as any other.”\nRegardless of the method of infiltrating an organization, attackers are often looking to infiltrate the network and then move within it to gain access to more sensitive or valuable information.\nHackers used this method during the infamous Target breach in 2013. They compromised a third-party vendor, obtained valid credentials from an unknowing authorized user, and connected to Target’s network using its vendor-portal process. The malicious actors then leveraged this access to obtain payment card data and personally identifying information about Target customers.\n“Maybe there are employee databases where they could steal information,” Wolf says. “Or they could use that access to spread ransomware, where files and systems could be encrypted and held hostage—forcing the organization to pay to free up that information.”\nLeveraging an intrusion into the access control system to the organization’s building system could also pose safety risks to employees—such as setting off a fire alarm—or equipment.\n“If you’re able to control the HVAC system, you could prevent cooling of data center space, so servers start to overheat and fail,” Wolf says. “And that can cause interruption of business or operations.”\nMitigating Existing Risk\nDespite the numerous vulnerabilities that exist, there are myriad ways to mitigate the risk of compromise to an access control system.\n“I work with a lot of clients who don’t have any drawings of where their devices are—they are flying blind,” Wolf says. “They don’t know, if something goes wrong, where to go and what component to look at.”\nThe first step for security professionals with an existing access control system that is connected to the network is to fully understand the system—where the readers are, how it works, how it is connected to the network, who has access to the system, and who has administrative privileges over it. Then, all that information should be documented.\n“Identify where everything is and, probably most importantly, how those devices intercommunicate with each other and the outside world,” Wolf adds. “An Internet connection is one thing, but with older systems we’ll see a DSL line or dial-up modem connections to systems so a contractor can log in and make changes to the system.”\nThese systems may have been installed decades ago. People often forget about those connections, which could be used by malicious actors to infiltrate access.\nWolf also recommends security professionals working with an existing access control system connected to the network assess if it meets the organization’s current security requirements.\nStarting from Scratch\nFor those in the fortunate position of installing a new access control system, the process should start with a “soul-searching discussion” on the risks and benefits of connecting that system to the Internet, Feeney says.\n“If there isn’t a significantly compelling benefit to essentially adding a door to your network, it is arguably not worth doing,” he explains. “In the case of access control, there may be a strong case for doing this—especially if the desired end goal is moving to the cloud. In this case, be sure to leverage best practices to incorporate security into your new network architecture.”\nThe organization should consider if the access control system should be on a network separated from other assets. Doing this will help mitigate the risk that an intruder will use the access control network to obtain corporate information.\n“If the ultimate goal is to move your access control system to the cloud, this network separation can still be done at the organization level,” Feeney says. “The separate access control or IoT network will connect to the cloud infrastructure. The original corporate network will separately protect all other assets. So, if the access control network’s connectivity is compromised, the attacker will not get access to the corporate network.”\nOnce a decision is made about what network the system should reside on, the organization should designate who is responsible for that network and the day-to-day management of it. This is critical because the system will require regular patching and updates to mitigate new security threats.\n“Often an organization’s IT department is better equipped to maintain the system because—if they’re a good IT organization—they will have a patch management process in place to make sure that the network switches and all the network servers are up to date,” Wolf says.\nWhen purchasing the actual access control system, the individual responsible—such as the physical or IT security representative—should ask vendors how data from the reader to the master console is protected, says Darrell Brown, CISSP, information security program manager at La-Z-Boy Incorporated and member of the IT Security Council.\n“Is that data in transit encrypted? At what level? And what is the right fit for my company?” Brown adds.\nOrganizations should also ask how often the vendor itself issues patches to its products, and what the process for issuing those patches is.\n“Proactively query your providers about patches and security updates to your hardware,” Feeney recommends. “Many access control devices traditionally get patches because customers request a feature or report an error that requires the patch. Instead, patch these devices like you do your computer—proactively as part of a comprehensive security strategy.”\nOrganizations should also have a robust master service agreement that outlines expectations and the responsibilities the vendor has to the organization.\n“Have clear lines that delineate who owns what part of the system,” Brown adds. “Who’s responsible? Where’s the backup? Is there a backup? How do we ensure failover to it?”\nAnd while the system is being installed and implemented, security professionals should ensure that the process follows best practices for maintaining good cyber hygiene. This starts with disabling default passwords to create strong, unique passwords for the system, and limiting administrative privileges.\nESD frequently encounters operating systems set up to automatically give administrator privileges to any users.\n“Most people don’t need that, and by restricting that, you’re ensuring that if a bad guy were to gain access using one person’s credentials, they wouldn’t have the ability to have administrative rights over the whole operating system,” Wolf says.\nAccess control systems, like all locks, can be compromised by motivated actors given the right circumstances. Security practitioners should not assume that the system itself is secure.\n“Security is ideally a shared responsibility between consumer and provider,” Feeney says. “You’ll find this to typically be the case. But where the separations of responsibilities lie can differ greatly. For that reason, always check your service level agreement to understand what security responsibilities your provider has and what is left to you as the consumer.”\nMegan Gates is senior editor at Security Management. Contact her at [email protected]. Follow her on Twitter @mgngates."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:8973de82-923a-4a45-836b-d98c10616fd0>","<urn:uuid:e2d4eaf1-f92e-4f65-8645-3672e13810ad>"],"error":null}
{"question":"How does hunting contribute to wildlife conservation efforts for both turkeys and bison?","answer":"For both species, hunting plays a crucial role in conservation. In turkey conservation, over 90% of state wildlife agency funding comes from hunters through license and tag purchases, plus an 11% excise tax on hunting equipment (Pittman-Robertson Act), which has raised $4 billion since 1937 for public land purchase and habitat improvement. Similarly, modern bison hunting supports conservation through lottery drawings in states like Alaska, Arizona, Montana, Utah, and Wyoming, with most funds going toward maintaining the health and longevity of wild herds. In both cases, regulated hunting helps maintain species populations while generating essential funding for conservation efforts.","context":["By Brent Lawrence\nA person recently posed the question: “You’re with the National Wild Turkey Federation and I see your tagline of ‘Conserve. Hunt. Share.’ How do you conserve wildlife by hunting?”\nI love questions like this because they show an open mind and an interest to learn.\nHunters are indeed the champions of conservation in many ways.\n- The NWTF has conserved and improved 17 million areas of essential wildlife habitat with its partners by raising and investing $372 million since 1985. In South Carolina alone, we have invested $20 million to improve 337,985 acres across the state. These improvements come in the form of prescribed fires, timber management, tree planting, streamside projects, land and easement purchases, and many other critical habitat initiatives. The habitat projects that help wild turkeys, also improve land for deer, quail, rabbits, songbirds and many other species in the fields and forests.\n- We have helped successfully reintroduce turkeys to 99.5 percent of suitable habitat in North America, thanks to an innovative method to transfer wild turkeys between states drawn up by Edgefield’s Dr. James Earl Kennamer. Through the efforts of Dr. Kennamer and the NWTF, we have worked with state wildlife agencies to move more than 200,000 wild turkeys across North America.\n- In the early 1900s there were fewer than 30,000 wild turkeys left in the United States due to uncontrolled hunting, and detrimental agricultural and forestry practices. Today there are more than 7 million wild turkeys in North America. It truly is the greatest wildlife success story in North America, and hunters paid the way.\nBut there is another component to how the NWTF and hunters conserve wildlife and habitat. It is a fact that anti-hunting organizations don’t want you to know.\nNationwide, more than 90 percent of the funding for state wildlife agencies comes from hunters. It comes through our hunting licenses and tag purchases, and through a self-imposed tax on the purchase of hunting-related items.\nThrough the Pittman-Robertson Act, an 11-percent excise tax is placed on firearms, ammunition and archery equipment. This act has raised $4 billion since its inception in 1937. The funds are distributed to states through the Secretary of the Interior and can only be used by state wildlife agencies to purchase public land, improve essential habitat and create additional outdoor opportunities. These opportunities also benefit hikers and bikers, canoeists and campers.\nWithout funding from hunters’ license and tag sales and the Pittman-Robertson Act, there wouldn’t be a Sumter National Forest, much less the bike trails, campgrounds and other facilities. There would be no active management of the land to decrease wildfire risks, keep trails clear of brush and limbs, and there certainly wouldn’t be the multitude of deer, turkey and the many non-game species that are your constant companions on a trip through the forest.\nRevenue from hunters also supports two essential groups of people: wildlife biologists and game wardens. Biologists conduct the scientific research that determines hunting seasons and the appropriate harvest limits to keep species healthy and vibrant for future generations. Our game wardens protect wildlife from poachers and other people who aren’t looking out for the future of wildlife.\nHunters support hunting seasons and limits and we respect the game laws. We want to pass on our hunting heritage to the next generation … and we put our money where our heart is.\nWhether you’re a hunter or simply an outdoor enthusiast, there are two great ways you can do your part for wildlife.\n- First, buy a hunting license every year even if you don’t hunt. The South Carolina Department of Natural Resources will put the money to good use.\n- Second, attend an NWTF banquet. NWTF volunteers hold two conservation fundraisers in Edgefield every year, one in the spring and a second in November. In addition to a great meal, tremendous fun and opportunities to win great prizes, you’ll also be helping conservation in South Carolina.\nTo learn more about the history of conservation, the wild turkey and the NWTF, please stop by our Winchester Museum at the Wild Turkey Center at 770 Augusta Road in Edgefield. Admission is free for NWTF members, and for non-members admission is only $5 for adults, $2 for children under 17 and free for children under age 2. At the museum, you’ll be able to learn about the history of hunting, conservation and the NWTF.\nYou’ll learn why the NWTF’s tagline of “Conserve. Hunt. Share.” is far more than just a motto on the bottom of a business card. Without hunting, there is no conservation. Without conservation, there is no hunting. And we owe it to future generations to share this message.\nPlease, help pass on the tradition.\nBrent Lawrence is Public Relations Director with the NWTF, which has proudly called Edgefield home since 1973.","The American West once had its own sea: A great, sweeping, endless ocean of waving grasslands, expanding across the western United States from Canada to Mexico. It was a vast land of pristine and fertile ground, stretching off from horizon to horizon, unspoiled by man’s limitless ambitions. And like every sea, it teemed with life. Rabbits and grouse hid in the grass, wary of the foxes and coyotes prowling in the darkness. Antelope bounced across the vast expanses of open land, staying ahead of the packs of wolves that swept down from the islands of plateaus and ridges standing along the plains. And then, of course, there were the bison. The whales of the grasslands, the American buffalo once traveled the prairie in mobs whose numbers rivaled the number of stars in the sky. They were at one time in history the most populous big game animal in the entire country, with herds stretching for miles, the dust from their movements often blocking out the sun itself. And then, one day, they were just gone.\nThe fall of the great American bison herds has been well documented, and it is well known that they were hunted to near-extinction. As man moved west in search of new land, the inevitable tide of civilization clashed with the buffalo. The land the creatures occupied was needed for development. That massive sea of grass they lived on and the soil from which it grew was perfect for the incoming cattle and agriculture the pioneers needed to feed the oncoming masses of humanity looking to settle the West. What was more, the bison themselves were valuable. Their hides made clothing, leather goods, and blankets; their tongues and bone marrow were considered delicacies; and their gall bladders were thought to have medicinal qualities. Buffalo hunting, then, became not only a sport but an occupation.\nFrontiersmen moved across the prairie in search of the animals, shooting as many of them as they could and cutting the hides and tongues from the carcasses. Although the animals had always been hunted, the Plains Indians—who previously hunted them only for subsistence and had vast knowledge of the herds and their movements—with the coming of the fur trade in the United States, also began to hunt buffalo for profit. Natives in the West had acquired horses and improved weapons, and quickly became involved in the “robe trade”—trading buffalo robes for manufactured trade items such as cooking pots, knives, and guns, as well as cloth, glass, tobacco, sugar, and flour. Though the natives respected and loved the bison, it was a classic case of rising material expectations. Around 1830, as the demand for beaver pelts was replaced by silk, buffalo robes became the currency of trade for almost every trapper, mountain man, cowboy, and Indian seeking their fortune in the “New World.” Over the next 60 years the bison herds, which at one time numbered almost 50 million animals, were cut down to fewer than 1,000 animals by 1880. The few remaining animals hid in distant corners of the country. The West was “won,” and the mighty bison had nearly vanished with the victory, never to return again—or so it was thought.\nIn 1905, with bison numbers still dwindling, President Theodore Roosevelt, arguably the greatest conservationist of our nation’s history, along with William Hornady and Ernest Baynes, formed the American Bison Society (ABS). Working with both the Bronx Zoo—where Hornady served as director—and more prominently Yellowstone National Park, which had already established a bison preserve in 1902 containing two dozen animals, the American Bison Society set out to prevent the bison from going completely extinct and create awareness of the importance of the animal. They banned all hunting of bison and emplaced protective measures on areas they inhabited. They worked with Congress to make the American bison the country’s national mammal and then set about repopulation efforts.\nIn 1907, they shipped 15 buffalo from the Bronx Zoo to the Wichita Mountains in Oklahoma, which became the first official animal reintroduction in North American history. They also founded and established herds in South Dakota and Nebraska. Two years later, ABS successfully petitioned Congress to establish a permanent national bison range in northern Montana. Thanks to ABS’s efforts, by 1920, the bison’s population had grown from fewer than 1,000 animals to nearly 12,000. Once all of the herds were established and secure, the American Bison Society disbanded in 1935, but their actions served as a catalyst for the country, sparking a desire to bring the bison back.\nToday there are more than half a million American bison living in North America. Although most of these animals are semi-domesticated livestock whose meat has created its own niche market, or fenced-in animals on private ranches, there are around 20,000 wild, free-roaming bison in the country—a population that continues to grow. Alaska, Arizona, Montana, Utah, and Wyoming all have wild buffalo populations that one can hunt via lottery drawings. These hunts support future bison conservation, with most of the funds for the lottery draws going toward shaping the health and longevity of the wild herds.\nHunting bison today is ambitious: although the North American bison’s population has returned to substantial numbers, hunting them has changed from stalking massive herds in relatively small areas to hunting small groups of animals throughout massive areas of land. Hunters lucky enough to draw a tag find the bison a worthy and challenging game animal. In most states the hunters’ success rate for bison is less than 50 percent, but the volume of delicious, high-quality meat, along with the robe and massive skull more than make up for the possibility of coming up empty in the hunt. By hunting bison today we are contributing to and enjoying a great American success story. The American bison stands not only as a symbol of the country but as a symbol of the power of conservation, proving that a species can be brought back from the brink."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:132e658f-7169-4b21-b69c-eaf72e074db0>","<urn:uuid:f769a7f4-88bd-4030-840e-3a6a3c78aafc>"],"error":null}
{"question":"What happens in the brain during the development of Alzheimer's disease?","answer":"During the development of Alzheimer's disease, two major changes occur in the brain. First, a normal protein called tau becomes abnormal (hyperphosphorylated) and forms twisted filaments that fill neurons, preventing proper function and eventually causing cell death. Second, protein clumps called beta-amyloid plaques form between neurons. These beta-amyloid proteins are toxic to neurons and can alter their function and lead to their death. The beta-amyloid protein is derived from a longer protein called beta-amyloid precursor protein. These changes result in disconnections between neurons and different parts of the brain, leading to the clinical signs of dementia.","context":["What causes Down syndrome?\nIn every cell in the human body there is a nucleus, where genetic material is stored in genes. Genes carry the codes responsible for all of our inherited traits and are grouped along rod-like structures called chromosomes. Normally, the nucleus of each cell contains 23 pairs of chromosomes, half of which are inherited from each parent.\nDown syndrome is usually caused by an error in cell division called \"nondisjunction.\" Nondisjunction results in an embryo with three copies of chromosome 21 instead of the usual two. Prior to or at conception, a pair of 21st chromosomes in either the sperm or the egg fails to separate. As the embryo develops, the extra chromosome is replicated in every cell of the body. This type of Down syndrome, which accounts for 95% of cases, is called Trisomy 21.\nThe two other types of Down syndrome are called mosaicism and translocation. Mosaicism occurs when nondisjunction of chromosome 21 takes place in one-but not all-of the initial cell divisions after fertilization. When this occurs, there is a mixture of two types of cells, some containing the usual 46 chromosomes and others containing 47. Those cells with 47 chromosomes contain an extra chromosome 21. Mosaicism accounts for about 1% of all cases of Down syndrome. Research has indicated that individuals with mosaic Down syndrome may have fewer characteristics of Down syndrome than those with other types of Down syndrome. However, broad generalizations are not possible due to the wide range of abilities people with Down syndrome possess.\nWhat is Alzheimer’s disease?\nAlzheimer’s disease is the most common form of dementia in the elderly (accounting for 60-80% of patients) and in people with Down syndrome. Currently, in the United States over 5.3 million people have the disease. Alzheimer’s disease is a dementia that includes the loss of memory and other thinking skills. People diagnosed with dementia usually show:\n- Progressive loss of the ability to generate coherent speech or understand written or spoken language.\n- Difficulties recognizing or identifying objects (even with corrected vision).\n- Decline in ability to do motor activities (even if one understands what needs to be done).\n- Problems thinking abstractly, making sound judgments, planning and carrying out complex tasks.\n- Importantly – these changes in thinking must be severe enough to interfere with daily life.\nThere are several warning signs of Alzheimer’s disease but it is important to remember that change in function is the most important indicator:\n- Memory loss that disrupts daily life.\n- Problems solving problems or in planning.\n- Difficulty completing familiar tasks at home, at work or at leisure.\n- Confusion with time or place.\n- New problems with words in speaking or writing.\n- Losing or misplacing things.\n- Withdrawal from work or social activities.\n- Changes in mood or personality.\nWhat are the brain changes that happen with Alzheimer’s disease?\nSeveral changes must occur in the brain for a final diagnosis of Alzheimer’s disease. Two types of neuropathology are thought to cause dementia in this disease including neurofibrillary tangles and beta-amyloid plaques.\nInside the brain are nerve cells (neurons). Inside of neurons there is a normal protein called tau that is important for proper functioning. In Alzheimer’s disease, tau becomes abnormal (hyperphosphorylated) and this leads to a twisting up of long filaments (strings) that fill up the neuron and prevent it from functioning properly. Eventually, neurons die from these changes leading to many disconnections between neurons and between different parts of the brain. When this happens, the clinical signs of dementia can be seen.\nPlaques form in between neurons in the brain as big clumps of a protein called beta-amyloid. This beta-amyloid protein is very toxic to neurons and in high enough concentrations can change how they work and also lead to their death. The beta-amyloid protein is cut out of a longer protein called beta-amyloid precursor protein.\nWhy are people with Down syndrome more vulnerable to Alzheimer’s disease?\nThe gene for the beta-amyloid precursor protein that is cut to make beta-amyloid protein is on chromosome 21, which is present in triplicate in most people with Down syndrome. This means that people with Down syndrome are making more of the precursor protein than people without Down syndrome and leads them to being more vulnerable to the development of Alzheimer’s disease. Also, the age of onset of Alzheimer’s disease in adults with Down syndrome is earlier than those without Down’s and can start as young as in the 40’s. As they get older, the risk of getting Alzheimer’s disease gets higher.\nNot all adults with Down’s syndrome develop dementia\nImportantly, not all older adults with Down syndrome develop dementia despite making too much beta-amyloid protein. This means that some people may tolerate or compensate for these changes in the brain. This also means they can still function well on a day to day basis without the clinical signs of dementia. If, through research, we can learn how this happens, then it may be possible to develop ways to prevent or treat Alzheimer’s disease in people with Down syndrome. This is one of the long term goals of our longitudinal aging study – to discover new approaches to prevent Alzheimer’s disease in Down syndrome as people get older."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:82b41c25-f881-43a0-b550-645fc033c8d4>"],"error":null}
{"question":"How is a ventricular assist device (VAD) surgically implanted?","answer":"During VAD implant surgery, several steps are followed: First, general anesthesia is administered and a ventilator supports breathing. The surgeon then makes an incision through the sternum to open the pericardium. After administering heparin, the patient is connected to a cardiopulmonary bypass machine. The VAD driveline is tunneled through the abdominal wall. The surgeon then inserts the VAD into the proper heart locations - for an LVAD, the inflow cannula typically connects to the left ventricle apex and the outflow graft to the aorta. The VAD operation is gradually increased while weaning off bypass. Finally, protamine is given to reverse the heparin, the sternum is closed with metal wires, and the superficial layers are closed with absorbable sutures.","context":["A ventricular assist device (VAD) is a medical device designed to facilitate the circulation of blood from a heart chamber (ventricle) to the rest of the body.\nVarious types of VADs exist:\n- Left ventricular assist device (LVAD): This is the most prevalent type, aiding the left side of the heart in pumping oxygen-rich blood from the heart to the body.\n- Right ventricular assist device (RVAD): It is responsible for pumping oxygen-poor blood from the right side of the heart to the lungs.\n- Biventricular assist device: This device supports both sides of the heart.\n- Pediatric VADs: These are smaller devices equipped with smaller cannulas and pumps. They can be tailored to accommodate individuals ranging from newborns to young adults.\nReasons for undergoing the procedure\nA Ventricular Assist Device (VAD) may be necessary for individuals with advanced heart failure who are either awaiting a heart transplant or need ongoing assistance to ensure sufficient blood circulation in their bodies.\nVentricular assist devices offer various benefits:\n- Bridge to recovery: Provides cardiac support during the period when the heart is recovering from weakness, serving as a temporary measure.\n- Bridge to transplant: Designed for individuals eligible for a heart transplant, the VAD offers essential support while waiting for a suitable donor heart.\n- Destination therapy: Intended for those ineligible for a heart transplant, this option provides permanent cardiac support, enhancing longevity and improving overall quality of life.\nPossible complications linked to the surgical placement of a ventricular assist device (VAD) and the long-term utilization of the device comprise:\n- Malfunction of the device.\n- Formation of blood clots.\n- Cerebrovascular accident.\n- Right-sided heart failure.\nBefore the procedure\nWhen someone has severe heart failure, not everyone qualifies for a ventricular assist device (VAD). After being diagnosed with advanced heart failure, individuals undergo thorough medical and psychological evaluations. These evaluations check the current function of the heart and other organs and screen for various health issues such as breast cancer, prostate problems, and infectious diseases. The evaluation includes:\n- Electrocardiogram (EKG)\n- Computed Tomography (CT) scan\n- Exercise testing\n- Both left and right cardiac catheterizations\nBesides medical assessments, there are discussions about what it’s like to live with a VAD, and patients receive detailed education on the day-to-day tasks involved in caring for the device. The healthcare team also helps patients identify nearby loved ones or a support system that can offer assistance when needed.\nDuring the procedure\nDuring VAD implant surgery, the following steps occur:\n- General anesthesia is administered to induce sleep and momentarily impair feeling. Patients won’t remember the process. Breathing is supported by a ventilator during the procedure and in the early postoperative period in the intensive care unit (ICU).\n- The surgeon opens the sac (pericardium) surrounding the heart by making an incision in the middle of the chest through the sternum.\n- The surgeons put the patient on a cardiopulmonary bypass machine after administering heparin to maintain blood flow throughout the body and stop blood from passing through the heart while the procedure is being performed.\n- The VAD driveline is then tunneled through the abdominal wall.\n- The VAD is then inserted by the surgeon into the proper locations within the heart. Although there are many different configurations available, the inflow cannula of an LVAD normally connects to the apex of the left ventricle, and the outflow graft connects to the aorta.\n- After the VAD is implanted, the surgeon speeds up its operation to support the patient’s circulatory needs while weaning them off the bypass machine.\n- The surgeon uses a medication called protamine to reverse the heparin and encourage blood clotting once the VAD’s function is stable. After that, the surgeon reattaches the sternum using metal wires. After applying a sterile dressing, they use absorbable sutures to seal the more superficial layers.\nAfter the procedure\nFollowing implant surgery, you will spend a minimum of a few days in the intensive care unit (ICU). Once your health improves and you no longer require ICU-level care, you will be transferred to the cardiac step-down unit for the remainder of your hospital stay. During your postoperative stay, healthcare professionals will closely monitor your organ functions, including your heart, kidneys, brain, lungs, and liver. You will also collaborate with physical and occupational therapists and other specialists to learn how to manage your life with the new device. Your providers will prescribe a blood-thinning medication called warfarin sodium to prevent blood clots from forming inside the VAD.\nA nurse or VAD coordinator provides instruction on maintaining your ventricular assist device, covering the following aspects:\n- Properly cleaning the equipment and driveline.\n- Regularly monitoring the incisions, including the driveline exit site, to ensure they remain healthy and are healing properly.\n- Learning how to replace the batteries and addressing any troubleshooting needs for various alarms that may arise from the device.\n- Being informed about who to reach out to if you have any questions or concerns.\nLiving with a VAD (Ventricular Assist Device) comes with certain limitations and responsibilities:\n- Power source: You’ll need a power source, either batteries or a wall socket, to keep the device running.\n- Medication: Taking blood thinners is necessary, and you’ll require regular lab tests to monitor their effectiveness.\n- Water avoidance: Avoid activities that may get the device wet, like swimming or taking baths. You can learn how to safely shower with it.\n- No contact sports: Steer clear of contact activities to prevent excessive bleeding in case of injury.\n- Medication management: Besides blood thinners, you may need other cardiovascular medications for blood pressure control and rhythm management.\n- Healthy lifestyle: Quit smoking, maintain a nutritious diet, and engage in regular physical activity for overall well-being.\n- Rehabilitation: Participate in cardiac rehabilitation to regain strength and confidence after VAD implantation.\n- Device maintenance: Properly care for your VAD, and your medical team will provide guidance and support.\nAfter discharge, routine follow-up visits are essential. Initially frequent, these visits become less frequent as you progress. During these visits:\n- Blood pressure monitoring: Regularly check and manage your blood pressure, as it can be a concern.\n- Medication review: Ensure your medications, especially anticoagulants, are correctly dosed.\n- Device data check: Assess VAD data for alarms or issues with device function.\n- Device adjustments: If needed, make adjustments to VAD settings.\n- Lifestyle progress: Discuss your progress with lifestyle changes and receive helpful tips.\nContact your healthcare provider if you experience:\n- Coughing up blood\n- Fever or chills\n- Chest pain\n- Dark-colored urine\n- Joint pain\n- Redness, tenderness, or unusual warmth near incisions or the driveline exit.\nRegular communication with your healthcare team ensures your VAD’s optimal function and your well-being."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:39d8eecd-b151-4e51-b256-985d79babfa5>"],"error":null}
{"question":"How do rising interest rates affect both cash reserve ratios and economic collapse?","answer":"Rising interest rates play significant roles in both banking regulation and economic collapse scenarios. During periods of economic collapse, interest rates peak at abnormally high levels, limiting available money for investment and making it costly for corporations and governments to service existing debt or take new loans. This hinders economic growth. In the banking system, these interest rate changes interact with the Cash Reserve Ratio (CRR), which is the amount of funds banks must keep with the central bank. When the central bank adjusts the CRR, it affects the available money with banks. The CRR must be at least three percent of Net Demand and Time Liabilities and can be increased up to twenty percent, allowing the central bank to control money supply during periods of economic stress.","context":["1. CASH RESERVE RATIO\nCash reserve Ratio (CRR) is the amount of funds that the banks have to keep with RBI. If RBI decides to increase the percent of this, the available amount with the banks comes down. RBI is using this method (increase of CRR rate), to drain out the excessive money from the banks. The amount of which shall not be less than three per cent of the total of the Net Demand and Time Liabilities (NDTL) in India, on a fortnightly basis and RBI is empowered to increase the said rate of CRR to such higher rate not exceeding twenty percent of the Net Demand and Time Liabilities (NDTL) under the RBI Act, 1934.\n2. STATUTORY LIQUIDITY RATIO\nIn terms of Section 24 (2-A) of the B.R. Act, 1949 all Scheduled Commercial Banks, in addition to the average daily balance which they are required to maintain in the form of….\n- In cash,or\n- In gold valued at a price not exceeding the current market price, or\n- In unencumbered approved securities valued at a price as specified by the RBI from time to time.\n3. REPO RATE\nRepo rate, also known as the official bank rate, is the discounted rate at which a central bank repurchases government securities. The central bank makes this transaction with commercial banks to reduce some of the short-term liquidity in the system. The repo rate is dependent on the level of money supply that the bank chooses to fix in the monetary scheme of things. Repo rate is short for repurchase rate. The entity borrowing the security is often referred to as the buyer, while the lender of the securities is referred to as the seller. The central bank has the power to lower the repo rates while expanding the money supply in the country. This enables the banks to exchange their government security holdings for cash. In contrast, when the central bank decides to reduce the money supply, it implements a rise in the repo rates. At times, the central bank of the nation makes a decision regarding the money supply level and the repo rate is determined by the market.\nThe securities that are being evaluated and sold are transacted at the current market price plus any interest that has accrued. When the sale is concluded, the securities are subsequently resold at a predetermined price. This price is comprised of the original market price and interest, and the pre-agreed interest rate, which is the repo rate.\n4. BANK RATE\nBank rate is referred to the rate of interest charged by premier banks on the loans and advances. Bank rate varies based on some defined conditions as laid down the governing authority of the banks. Bank rates are levied to control the money supply to and from the bank. From the consumer’s point of view, bank rate ordinarily denotes to the current rate of interest acquired from savings certificate of Deposit. It is most frequently used by the consumers who are concerned in mortgage\nSome commonest types of bank interest rates are as follows:\n- Bank rate on CD, i.e., on certificate of deposit\n- Bank rate on the credit of a credit card or other kind of loan\n- Bank rate on real estate loan\n5. INTERBANK RATE\nThe rate of interest charged on short-term loans made between banks. Banks borrow and lend money in the interbank market in order to manage liquidity and meet the requirements placed on them. The interest rate charged depends on the availability of money in the market, on prevailing rates and on the specific terms of the contract, such as term length.\nBanks are required to hold an adequate amount of liquid assets, such as cash, to manage any potential withdrawals from clients. If a bank can’t meet these liquidity requirements, it will need to borrow money in the interbank market to cover the shortfall. Some banks, on the other hand, have excess liquid assets above and beyond the liquidity requirements. These banks will lend money in the interbank market, receiving interest on the assets. There is a wide range of published interbank rates, including the LIBOR & MIBOR, which is set daily based on the average rates on loans made within the London interbank market & Mumbai Interbank Market.\n6. GROSS DOMESTIC PRODUCT\nThe monetary value of all the finished goods and services produced within a country’s borders in a specific time period, though GDP is usually calculated on an annual basis. It includes all of private and public consumption, government outlays, investments and exports less imports that occur within a defined territory.\nGDP = C + G + I + NX\n- “C” is equal to all private consumption, or consumer spending, in a nation’s economy.\n- “G” is the sum of government spending.\n- “I” is the sum of all the country’s businesses spending on capital.\n- “NX” is the nation’s total net exports, calculated as total exports minus total imports. (NX = Exports – Imports)\nGDP is commonly used as an indicator of the economic health of a country, as well as to gauge a country’s standard of living.\nInflation can be defined as a rise in the general price level and therefore a fall in the value of money. Inflation occurs when the amount of buying power is higher than the output of goods and services. Inflation also occurs when the amount of money exceeds the amount of goods and services available. As to whether the fall in the value of money will affect the functions of money depends on the degree of the fall. Basically, refers to an increase in the supply of currency or credit relative to the availability of goods and services, resulting in higher prices. Therefore, inflation can be measured in terms of percentages. The percentage increase in the price index, as a rate per cent per unit of time, which is usually in years. The two basic price indexes are used when measuring inflation, the producer price index (PPI) and the consumer price index (CPI) which is also known as the cost of living index number.\nIt is a condition of falling prices accompanied by a decreasing level of employment, output and income. Deflation is just the opposite of inflation. Deflation occurs when the total expenditure of the community is not equal to the existing prices. Consequently, the supply of money decreases and as a result prices fall. Deflation can also be brought about by direct contractions in spending, either in the form of a reduction in government spending, personal spending or investment spending. Deflation has often had the side effect of increasing unemployment in an economy, since the process often leads to a lower level of demand in the economy.\nWhen prices are falling due to anti-inflationary measures adopted by the authorities, with no corresponding decline in the existing level of employment, output and income, the result of this is disinflation. When acute inflation burdens an economy, disinflation is implemented as a cure. Disinflation is said to take place when deliberate attempts are made to curtail expenditure of all sorts to lower prices and money incomes for the benefit of the community.\nReflation is a situation of rising prices, which is deliberately undertaken to relieve a depression. Reflation is a means of motivating the economy to produce. This is achieved by increasing the supply of money or in some instances reducing taxes, which is the opposite of disinflation. Governments can use economic policies such as reducing taxes, changing the supply of money or adjusting the interest rates; which in turn motivates the country to increase their output. The situation is described as semi-inflation or reflation.\nStagflation is a stagnant economy that is combined with inflation. Basically, when prices are increasing the economy is deceasing. Some economists believe that there are two main reasons for stagflation. Firstly, stagflation can occur when an economy is slowed by an unfavourable supply, such as an increase in the price of oil in an oil importing country, which tends to raise prices at the same time that it slows the economy by making production less profitable. In the 1970’s inflation and recession occurred in different economies at the same time. Basically, what happened was that there was plenty of liquidity in the system and people were spending money as quickly as they got it because prices were going up quickly. This gave rise to the second reason for stagflation.\n12. FOREIGN INSTITUTIONAL INVESTMENTS\nForeign Institutional Investors (FIIs), Non-Resident Indians (NRIs), and Persons of Indian Origin (PIOs) are allowed to invest in the primary and secondary capital markets in India through the portfolio investment scheme (PIS). Under this scheme, FIIs/NRIs can acquire shares/debentures of Indian companies through the stock exchanges in India.\nThe ceiling for overall investment for FIIs is 24 per cent of the paid up capital of the Indian company and 10 per cent for NRIs/PIOs. The limit is 20 per cent of the paid up capital in the case of public sector banks, including the State Bank of India.\n13. FOREIGN EXCHANGE RESERVES\nForeign exchange reserves (also called Forex reserves) in a strict sense are only the foreign currency deposits held by central banks and monetary authorities. However, the term in popular usage commonly includes foreign exchange and gold, SDRs and IMF reserve positions. This broader figure is more readily available, but it is more accurately termed official reserves or international reserves. These are assets of the central bank held in different reserve currencies, such as the dollar, euroyen, and used to back its liabilities, e.g. the local currency issued, and the various bank reserves deposited with the central bank, by the government or financial institutions. and\nLarge reserves of foreign currency allow a government to manipulate exchange rates – usually to stabilize the foreign exchange rates to provide a more favorable economic environment.","What is Economic Collapse?\nEconomic collapse refers to a period of national or regional economic breakdown where the economy is in distress for a long period, which can range from a few years to several decades. During periods of economic distress, a country is characterized by social chaos, social unrest, bankruptcies, reduced trade volumes, currency volatility, and breakdown of law and order.\nDue to the magnitude of the economic distress, government interventions for economic recovery can be slow to bring the economy back on track, and the delay causes even greater disorganization of the economy.\nCauses of Economic Collapse\nThe following are some of the causes of economic collapse:\nHyperinflation occurs when the government allows inflationary pressure to build up in the economy by printing excessive money, which leads to a gradual rise in the prices of commodities and services. Governments resort to creating excess money and credit with the goal of managing an economic slowdown. Hyperinflation occurs when the government loses control of the price increases and raises the interest rates as a way of managing the accelerating inflation.\nStagflation refers to a situation in which the economy is growing at a slow rate while simultaneously experiencing high rates of inflation. Such an economic situation causes a dilemma among policymakers since the measures implemented to reduce the rise in inflation may increase unemployment levels to abnormally high levels. Stagflations and its effects on the economy may last for several years or decades.\nFor example, the United States experienced stagflation from the 1960s to the 1970s. During said period, economic growth was stagnant, and the inflation peaked at 13% per annum while the inflation rate in the United Kingdom was at 20% per annum. Once stagflation occurs, it is usually difficult to manage, and governments must incur huge costs to bring balance to the economy.\n3. Stock market crash\nA stock market crash occurs when there is a loss of investor confidence in the market, and there is a dramatic decline in stock prices across different stocks trading in the stock market. When a stock market crash occurs, it creates a bear market (when prices drop 20% or more from their highs to hit new lows), and it drains capital out of businesses.\nCrashes occur when there is a prolonged period of rising stock prices, price earning ratios exceed long-term averages, and there is excessive use of margin debt by market participants.\nScenarios that Define an Economic Collapse\nThe following are some of the things that characterize an economic collapse:\n1. Rising interest rates\nDuring periods of economic collapse, interest rates peak at abnormally high levels, and it limits the amount of money that is available for investors to invest. High interest rates hinder economic growth since investors, corporations, and the government find it costly to service existing debt obligations and take out new loans due to the high cost of capital.\nWhen a major company declares its inability to finance its debt obligations and resorts to disposing of its assets to pay creditors, investors lose confidence in the company and will be hesitant to trade their money during periods of financial distress.\n2. Sovereign debt crisis\nSovereign debts are debts taken up by a government to finance capital-intensive infrastructural projects. However, when the government takes on too many debts and is unable to pay principal and interest obligations when they fall due, it increases the risk of defaulting on its existing debt obligations and becoming bankrupt.\nA sovereign debt crisis occurs during periods of slow economic growth, wars, political instability, drought, and when investors lose confidence in the government. Due to the large size of sovereign debts, a default by the government is likely to affect the global economy and cause spill-over effects on other jurisdictions.\n3. Local currency crisis\nA local currency crisis occurs when the currency depreciates in value due to a loss of investor confidence. This occurs when foreign investors who have invested in a country and advanced credit to the government lose confidence in the government’s ability to meet debt obligations or generate the agreed-upon returns.\nIn such situations, the foreign investors withdraw their investments in the country. The move increases the selling of the borrowing country’s currency in the international market, resulting in currency devaluation. In return, the currency devaluation increases the country’s international debts, resulting in the loss of the country’s purchasing power.\n4. Global currency crisis\nA global currency crisis involves the loss of value of a major currency that is used in cross-border trade transactions between individuals, corporations, and governments. For example, the US dollar is used as the world reserve currency in the Bretton Woods institutions, which means that if the US dollar depreciates in value, it may trigger a global economic crisis.\nCFI offers the Financial Modeling & Valuation Analyst (FMVA)™ certification program for those looking to take their careers to the next level. To keep learning and advancing your career, the following CFI resources will be helpful:"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:caa331a7-2754-4359-b076-62524b9e36e5>","<urn:uuid:4e1c9e96-e3d0-4e83-bc54-de1084f5528b>"],"error":null}
{"question":"What are the key differences between hypotrophic scars and absorbable sutures in terms of their formation and duration?","answer":"Hypotrophic scars and absorbable sutures differ significantly in their nature and timeline. Hypotrophic (atrophic) scars occur due to deficient formation of connective tissue, where the skin doesn't form enough new connective tissue to fill the wound. These scars appear sunken below the skin level and are flat with small dimples, commonly seen in acne scars and stretch marks. In contrast, absorbable sutures are a temporary wound closure method that breaks down in the tissues and loses their strength within 60 days. These sutures do not need to be removed as they naturally dissolve, and are used to close skin and for internal uses where a permanent stitch isn't needed.","context":["A scar is a fibre-rich replacement tissue formed by connective tissue cells during wound healing. Scarring is a natural process that occurs when injured tissue heals. A scar occurs when not only the top layer of skin (epidermis) but also deeper layers of skin are injured. Scars are final but not immutable. Just as different as the types of scars are their healing and care processes.\nHow is a scar created?\nScar formation is the final step in wound healing, which is divided into three stages: Inflammation, repair and reconstruction phase. External scars usually develop after injuries to deeper layers of the skin. A small cut or superficial laceration often only injures the top layer of skin (epidermis). In this case, starting from the lowest, so-called basal cell layer of the epidermis, a new, intact skin layer forms, which closes the resulting wound.\nWhat types of scars are there?\nDepending on the type of injury, the condition of the wound edges, wound hygiene, but also on age and genetic factors, different visible scars can remain after an injury. A distinction is made between the following types of grain:\n- Hypertrophic scars\n- Keloid scars or proliferating scars\n- Hypotrophic (atrophic) / sunken scars\n- Sclerotic scars\n- Unstable scars\nHypotrophic scars (atrophic)\nHypotrophic scars are caused by the deficient formation of connective tissue. The term “atrophy” denotes a regression. In atrophic scars, the wound closes after healing, but the skin does not form enough new connective tissue to fill the wound. This creates sunken scars, the bottom of which is deeper than the surrounding skin. These scars are therefore usually found flat and in the form of small dimples. The wound tends to heal poorly because the formation of new connective tissue fibres is not sufficient. The sunken scar is therefore below the level of the skin, which is especially the case with acne scars. Stretch marks in bodybuilders or stretch marks in pregnancy are also atrophic scars.\nHypertrophic scars are raised scars which, in contrast to hypotrophic scars, are caused by an excessive formation of connective tissue. They are often the result of an injury that was stressed too early or an ongoing infection. Hypertrophic scars thus often develop when the wound has to close against strong tensile forces during healing and is thus exposed to constant movement or stress. The constant tension causes more blood vessels and connective tissue to form, which often makes hypertrophic scars appear very bulging and very red. They can also be itchy and sometimes painful. However, they remain limited to the original injury area, but protrude above the skin level. The scar growths develop directly within the first few weeks after the injury and not at a later time.\nKeloid scars or keloids are proliferating scars that form some time after wound closure due to an overproduction of connective tissue. Keloids are thick, curved, often very red, appear darker than the surrounding tissue and extend beyond the original area of injury. They are therefore often called “wild flesh” because they are not limited to the area of the wound, but often extend tumour-like to the neighbouring, healthy skin tissue. This also often causes itching. Especially young women and people with dark skin are affected by keloids. They only occur in pigmented skin and often develop on shoulders or earlobes. Burn wounds also often form keloids.\nSclerotic scars form after extensive injuries such as burns. They arise from the formation of new hard and dry tissue. During healing, the tissue contracts strongly, which is why the skin becomes hard and inelastic. Sclerotic scars also tend to shrink. If they lie over joints, they can cause contractures or immobilisation, which is why surgical correction is often advisable.\nUnstable scars are usually the result of poorly functioning wound healing. It often affects areas of the body where the skin is exposed to frequent tension. This includes, for example, the skin on the joints. Typical symptoms of unstable scars are, among other things, ulcers and tears that form again and again. It is imperative that this type of scar is treated professionally, because there is a risk that a malignant carcinoma will develop.\nWhat should absolutely be avoided in order for a scar to heal well?\nWound and scar healing depends on a wide variety of factors and can therefore vary from person to person. In order for a scar to heal well, contamination of the wound area, tension and irritation caused by tight-fitting clothing, and re-injury should be avoided. In addition, care should initially be taken to ensure that the scar is not exposed to strong temperature stimuli or extreme sunlight.\nWhen are the sauna, steam bath and swimming pool allowed again?\nAs a general rule, you should not go into the sauna, steam bath or swimming pool again until the wound has completely healed. In the case of scars after surgical procedures, this means that there are no more foreign bodies such as sutures or staples in the scar and that it has healed completely smoothly and free of crusts. If this is not the case, you run the risk of catching a serious infection.\nWhen can the scar be exposed to direct UV radiation or the solarium?\nSince the scar tissue has no pigment-forming cells, it cannot protect itself from UV rays by tanning. Scar tissue stands out from tanned skin in a radiant white and suffers from UV damage in the process. Therefore, direct sunlight as well as visits to sunbeds should be avoided for six months to a year after the wound has healed and a high sun protection with at least SPF 50 should also be applied to the scar afterwards.\nIn order to support the healing process of a scar, it is recommended to care for the scar properly, depending on its type and characteristics. This is because scar care can help the scar become paler, smoother and also flatter by stimulating blood circulation. Even older scars can still respond to scar care, provided the right product is used. Thus, proper scar care can prevent pathological scars from forming, such as hypertrophic scars or keloids. Unfortunately, scar care takes a lot of patience and is not done in just a few days. For special care products to work, they should normally be used for several months.\nWhen should scar treatment be started?\nIn order for scar care to have the best effect after an operation or injury, it should be started as early as possible. In the early stages, the scar is still in the process of remodelling and therefore more receptive to treatment. As soon as the wound is closed or the doctor has removed the stitches, you can start caring for the fresh scar. However, this should always be discussed with the attending physician beforehand.\nHow does scar care work?\nA wound that is not yet completely closed is best treated with healing ointments to support wound healing. Once the wound is completely closed, special products can be used for scar care. For example, silicone-containing gels and plasters are suitable for this. In addition, it is partly possible to support the process of scar formation and healing with massages. The scar should be stressed as little as possible. Regular massages and care with ointments and oils can prevent dehydration, redness and growths.\nHow long should the scar be tended for?\nBasically, the earlier you start to care for the scar, the better. Fresh scars should be cared for at least eight weeks, older scars even up to six months. The healing time of an injury and the duration of care for the resulting scar depend on the extent and severity of the injury. If only the epidermis is injured, the skin can renew itself on its own within 28 days. In most cases, no scar remains and the duration of care is correspondingly shorter.\nWhich products can be used?\nAs soon as the wound edges are completely closed and the stitches of surgical scars are removed, the scar should be applied with special creams and gels once or twice a day. Products with active ingredients such as heparin, dexpanthenol and allantoin, as well as silicone-based products, are particularly effective. They help reduce redness, soften scar tissue, relieve pain and itching, reduce inflammation and moisturise. In addition, when used correctly, you can reduce/prevent the growth of excess connective tissue of the scar. All products have moisture and fat as ingredients in common – this is the most important thing for the scar – regardless of manufacturer or marketing.\nOintments consist of a single-phase base, do not contain water and normally do not need to be preserved. Cream, on the other hand, is a multi-phase system that always contains emulsifiers and must be preserved. There are both lipophilic (fat-loving) and hydrophilic (water-loving) creams. Gels consist of a liquid phase that is thickened with a gelling agent. This creates a three-dimensional gel structure. Both should be applied to the affected skin area over several weeks, at least twice a day, and massaged in gently. Scar gels are suitable for both fresh and older scars. The attending physician can best explain which scar care is most suitable for the individual situation.\nWhat is a scar plaster?\nScar plasters are made of a special material that is breathable and permeable to water vapour. This creates a skin climate in the scar area that stimulates the metabolic processes and promotes cell formation.\nIn addition, scar plasters exert a slight pressure on the wound, which helps keep the tissue under the plaster supple. This makes the scar flatter, lighter and more elastic. Scar plasters thus protect against excessive scarring and are therefore popular for treating hypertrophic (bulging) scars. They can also lighten older scars and reduce excess scarring to a certain extent.\nThey should only be applied after wound healing is complete and are stuck directly over the scar. To achieve their full effect, the patches should remain on the scar for at least twelve hours. Treatment with scar plasters usually lasts two to three months. As they do not contain any active pharmaceutical ingredients, there are usually no side effects and they are therefore also very suitable for people with allergies, pregnant women as well as children and people with sensitive skin. However, caution is advised with allergies to silicone.\nCan home remedies for scar care also be used?\nHerbal skin oils are also suitable for scar care, as they have valuable ingredients that can have a positive effect on the healing process of the scar. Above all, vitamin E and vitamins of the A group are contained in almost all natural oils. They support the skin in regeneration and increase its elasticity. St. John’s wort, grape seed, marigold, almond or jojoba oil are particularly good for scar care. Such oils stimulate blood circulation and make the scar fade. In addition, they keep the skin elastic and supple and partly also have an anti-inflammatory effect. When using vegetable oils, however, it is also important to avoid direct sunlight, as otherwise unsightly discoloration can occur in the scar area.\nHow can scars be corrected?\nThere are many ways to correct scars. In many cases, the treating doctor uses several procedures to correct the scar. In addition to natural remedies and medical methods for scar care and correction, surgical procedures can also help to correct or even completely remove the scar. The doctor must always decide individually which method is best suited for the respective scar. Surgical correction is necessary for some more severe scars. Other scars can be corrected with laser treatment, chemical peeling or hyaluronic acid/platelet-rich plasma (PRP). For more information about the correction of scars, we recommend that you take a look at our page in the Gynaesthetic section.","Facts about and Definition of Removing Stitches (Sutures)\nAmong the many methods for closing wounds of the skin, stitching, or suturing, is the most common form of repairing a wound. Other methods include surgical staples, skin closure tapes, and adhesives.Removing stitches or other skin-closure devices is a procedure that many people dread. Understanding the various skin-closure procedures and knowing how they are put in and what to expect when they are removed can help overcome much of this anxiety.\nStitches (also called sutures) are used to close cuts and wounds in skin. They can be used in nearly every part of the body, internally and externally. Doctors literally “sew” the skin together with individual sutures and tie a secure knot. Stitches then allow the skin to heal naturally when it otherwise may not come together. Stitches are used to close a variety of wound types. Accidental cuts or lacerations are often closed with stitches. Also, surgeons use stitches during operations to tie ends of bleeding blood vessels and to close surgical incisions.\nSutures are divided into two general categories, namely, absorbable and nonabsorbable. Absorbable suturesrapidly break down in the tissues and lose their strength within 60 days. This type of suture does not have to be removed. These are used to close skin and for other internal uses where a permanent stitch is not needed. Nonabsorbable sutures, on the other hand, maintain their strength for longer than 60 days. These sutures are used to close skin, external wounds, or to repair blood vessels, for example. They may require removal depending on where they are used, such as once a skin wound has healed.\nThe general technique of placing stitches is simple. The “thread” or suture that is used is attached to a needle. The wound is usually cleaned with sterile water and peroxide. Betadine, an antiseptic solution, is used to cleanse the area around the wound. Next, the area is numbed with an anesthetic agent such as lidocaine (Xylocaine). Then the needle with the thread attached is used to “sew” the edges of the wound together, in an effort to recreate the original appearance. Several stitches may be needed to accomplish this. Once the wound is closed a topical antibiotic gel is often spread over the stitches and a bandage is initially applied to the wound. All sutured wounds that require stitches will have scar formation, but the scarring is usually minimal.\nSurgical staples are useful for closing many types of wounds. Staples have the advantage of being quicker and may cause fewer infections than stitches. Disadvantages of staples are permanent scars if used inappropriately and imperfect aligning of the wound edges, which can lead to improper healing. Staples are used on scalp lacerations and commonly used to close surgical wounds.\nSkin closure tapes, also known as adhesive strips, have recently gained popularity. The advantages of skin closure tapes are plenty. The rate of wound infection is less with adhesive strips than with stitches. Also, it takes less time to apply skin closure tape. For many people, there is no need for a painful injection of anesthetic when using skin closure tapes. Disadvantages of using skin closure tapes include less precision in bringing wound edges together than suturing. Not all areas of the body can be taped. For example, body areas with secretions such as the armpits, palms, or soles are difficult areas to place adhesive strips. Areas with hair also would not be suitable for taping.\nAdhesive agents can be used to close a wound. This material is applied to the edges of the wound somewhat like glue and should keep the edges of the wound together until healing occurs. Adhesive glue is the newest method of wound repair and is becoming a popular alternative to stitches, especially for children. The adhesive simply falls off or wears away after about 5-7 days.\nHow to Prepare for Removing Stitches (Sutures)\nIf a person has received stitches, they should be given instructions for taking care of the stitches and wound, and be given an approximate date to have the stitches removed. A sample of such instructions includes:\n- Keep wound clean and dry for the first 24 hours.\n- Showering is allowed after 48 hours, but do not soak the wound.\n- Bandages can safely be removed from the wound after 48 hours, unless the wound continues to bleed or has a discharge. If bandages are kept in place\n- An antibiotic ointment (brand names are Polysporin or Neosporin, for example) should be used after the wound is cleaned.\n- Notify the doctor if a suture loosens or breaks.\n- When scheduled to have the stitches removed, be sure to make an appointment with a person qualified to remove the stitches.\nget wet, the wet bandage should be replaced with a clean dry bandage.\nDifferent parts of the body require suture removal at varying times. Common periods of time for removal are as follows, but times vary according to the health care professionals that perform the procedure:\n- Face: 3-5 days\n- Scalp: 7-10 days\n- Trunk: 7-10 days\n- Arms and legs: 10-14 days\n- Joints: 14 days\nSutures may be taken out all at one visit, or sometimes, they may be taken out over a period of days if the wound requires it.\nRemoval of Stitches (Sutures)\nThe wound is cleaned with an antiseptic to remove encrusted blood and loosened scar tissue. Sterile forceps (tongs or pincers) are used to pick up the knot of each suture, and then surgical scissors or a small knife blade is used to cut the suture. Forceps are used to remove the loosened suture and pull the thread from the skin. These relatively painless steps are continued until the sutures have all been removed. You may feel a tug or slight pull as a stitch is removed. The wound is cleansed again. Adhesive strips are often placed over the wound to allow the wound to continue strengthening.\nStaple removal is a simple procedure and is similar to suture removal. Doctors use a special instrument called a staple remover. After cleansing the wound, the doctor will gently back out each staple with the remover. The doctor applies pressure to the handle, which bends the staple, causing it to straighten the ends of the staple so that it can easily be removed from the skin. The staple backs out of the skin the very same direction in which it was placed. People may feel a pinch or slight pull. The process is repeated until all staples are removed. The wound is cleansed a second time, and adhesive strips are applied. This is also a relatively painless procedure.\nSuture Removal and Healing Time for Wounds\nWound care after suture removal is just as important as it was prior to removal of the stitches. Take good care of the wound so it will heal and not scar.\n- Keep adhesive strips on the wound for about 5 days. Then soak them for removal. Do not peel them off.\n- Continue to keep the wound clean and dry.\n- Skin regains tensile strength slowly. At the time of suture removal, the wound has only regained about 5%-10% of its strength. Therefore, protect the wound from injury during the next month.\n- Injured tissue also requires additional protection from sun’s damaging ultraviolet rays for the next several months. The use of sunscreen during this period of healing is advised for those areas that are exposed.\n- The use of vitamin E topically has also been suggested to be helpful in the healing process of the damaged skin. This should only be considered once the skin edges are healed and are closed together.\nWhen to Call a Doctor after Suture Removal\nCall a doctor if you have any of these signs and symptoms after stitches (sutures) have been removed, redness, increasing pain, swelling, fever, red streaks progressing away from the sutured site, material (pus) coming from out of the wound, if the wound reopens, and bleeding.\nComplications of Removing Stitches\nWound infection: If signs of infection begin, such as redness, increasing pain, swelling, and fever, contact a doctor immediately.\nWound reopening: If sutures are removed too early, or if excessive force is applied to the wound area, the wound can reopen. The doctor may restitch the wound or allow the wound to close by itself naturally to lessen the chances of infection.\nExcessive scarring: All wounds will form a scar, and it will take months for a scar to completely contract and remodel to its permanent form. However, scarring may be excessive when sutures are not removed promptly or left in place for a prolonged period of time. This may result in a scar with the appearance of a “railroad track.”\nKeloid formation: A keloid is a large, firm mass of scarlike tissue. This scarring extends beyond the original wound and tends to be darker than the normal skin. Keloids are common in wounds over the ears, waist, arms, elbows, shoulders, and especially the chest. Keloids occur when the body overreacts when forming a scar. They are common in African Americans and in anyone with a history of producing keloids. People with a tendency to form keloids should be closely monitored by the doctor. Injection of anti-inflammatory agents may decrease keloid formation. Also, large keloids can be removed, and a graft can be used to close the wound.\nHypertrophic scars: Bulky scars can remain within the boundaries of the original wound. These occur mostly around joints. Hypertrophic scars tend to develop a peak size and then get smaller over months to years. Keloids, on the other hand, rarely go away. For people with hypertrophic scars, a firm pressure dressing may aid in preventing them from forming."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:6bd91de3-bebf-497c-81e2-17bd6c24cae3>","<urn:uuid:a0a53511-3534-42c8-88ad-b24ba7b48897>"],"error":null}
{"question":"How is deafness genetically inherited in dogs, and what role do pigmentation genes play?","answer":"Deafness in dogs can be inherited through various genetic mechanisms: autosomal dominant, recessive, sex-linked, or involving multiple genes. Two pigmentation genes are particularly associated with deafness: the merle gene and the piebald gene. The presence of white in the coat increases deafness likelihood. These genes affect pigment-producing cells (melanocytes) in blood vessels, which are critical for maintaining the cochlea's function. While neither Poodles nor Labrador Retrievers typically carry the merle gene, breeds that do have it (like Australian Shepherds) can pass it to mixed breeds like Labradoodles, potentially causing hearing problems.","context":["Dr. George M. Strain, Louisiana State University Comparative Biomedical Sciences, School of Veterinary Medicine, Baton Rouge, Louisiana 70803\n“Congenital deafness in dogs (or other animals) can be acquired [caused by intrauterine infections, ototoxic drugs like gentamicin, liver disorders, or other toxic exposures before or soon after birth] or inherited. Inherited deafness can be caused by a gene defect that is autosomal dominant, recessive, sex-linked, or may involve multiple genes. It is usually impossible to determine the cause of congenital deafness unless a clear problem has been observed in the breed, or carefully planned breedings are performed.” For more information, see the full article below.\nThe Brainstem Auditory Evoked Response (BAER) test is the only accepted method of diagnosis. Bone stimulation transducer may be used in addition when conduction deafness is suspected.\nOFA recommends this test be performed by board certified veterinary neurologists, but will accept test results from experienced veterinarians, neuroscience professionals, and audiologists. One test suffices for the lifetime of the animal.\nBilateral hearing passes the test. Unilateral or bilateral deafness fails.\nGenetics and Inheritance of Canine Deafness\nDr. George M. Strain, Louisiana State University Comparative Biomedical Sciences, School of Veterinary Medicine, Baton Rouge, Louisiana 70803, used by permission.\nCongenital deafness in dogs (or other animals) can be acquired [caused by intrauterine infections, ototoxic drugs like gentamicin, liver disorders, or other toxic exposures before or soon after birth] or inherited. Inherited deafness can be caused by a gene defect that is autosomal dominant, recessive, sex-linked, or may involve multiple genes (more on this later). It is usually impossible to determine the cause of congenital deafness unless a clear problem has been observed in the breed, or carefully planned breedings are performed. In this article I will discuss what is currently known about the genetics of deafness in dogs so that breeders can make the best informed decisions possible when attempting to reduce or eliminate deafness.\nCongenital deafness has been reported for approximately 80 breeds, with the list growing at a regular rate (see list of Dog Breeds with Reported Congenital Deafness); it can potentially appear in any breed but especially those with white pigmentation. Deafness may have been long-established in a breed but kept hidden from outsiders to protect reputations. The disorder is usually associated with pigmentation patterns, where the presence of white in the hair coat increases the likelihood of deafness. Two pigmentation genes in particular are often associated with deafness in dogs: the merle gene (seen in the Collie, Shetland Sheepdog, Dappled Dachshund, Harlequin Great Dane, American Foxhound, Old English Sheepdog, and Norwegian Dunker hound among others) and the piebald gene (Bull Terrier, Samoyed, Greyhound, Great Pyrenees, Sealyham Terrier, Beagle, Bulldog, Dalmatian, English Setter). However, not all breeds with these genes have been reported to be affected. The deafness, which usually develops in the first few weeks after birth while the ear canal is still closed, usually results from the degeneration of part of the blood supply to the cochlea (the stria vascularis). The nerve cells of the cochlea subsequently die and permanent deafness results. The cause of the vascular degeneration is not known, but appears to be associated with the absence of pigment producing cells (melanocytes) in the blood vessels. All of the function of these cells are not known, but one role is to maintain high potassium concentrations in the fluid surrounding the hair cells of the cochlea; these pigment cells are critical for survival of the stria. Deafness in the Doberman, which is also accompanied by vestibular (balance) disturbance, results from a different mechanism, where hair cell death is not the result of degeneration of the stria. Deafness may also occur later in life in dogs from other causes such as toxicities, infections, or injuries, or due to aging (presbycusis); these forms of deafness almost never have a genetic cause in animals and thus do not present a concern in breeding decisions.\nThe prevalence of congenital deafness in different breeds is seldom known because of the limited number of studies (see table on Breed-Specific Deafness Prevalence In Dogs). In the Dalmatian, where the incidence is highest, 8% of all dogs in the US are bilaterally deaf and 22% are unilaterally deaf. In the English Setter, English Cocker Spaniel, Australian Cattle Dog, and Bull Terrier, where fewer numbers of dogs have been hearing tested, the incidence appears to be about one third to one half that of Dalmatians. Unilateral or bilateral deafness is found in 75% of all white Norwegian Dunker hounds, but the incidence in normal-color dogs is unknown. Other breeds with a high incidence are the Catahoula and Australian Shepherd. The incidence of all types of deafness in the general dog population is low, reported to be 2.56 to 6.5 cases per 10,000 dogs seen at veterinary school teaching hospitals, but these data predate the availability of hearing testing devices and so are much lower that actual values. Recognition of affected cases is often difficult, because unilaterally deaf dogs appear to hear normally unless a special test (the brainstem auditory evoked response, BAER) is performed; facilities to perform the BAER are usually only available at veterinary schools (see list of BAER Testers). It should be noted that a unilaterally deaf dog can be as great a genetic risk for transmission of deafness to its offspring as is a bilaterally deaf dog.\nThe method of genetic transmission of deafness in dogs is usually not known. There are no recognized forms of sex-linked deafness in dogs, although this does occur in humans. The disorder has been reported to have an autosomal recessive mechanism in the Rottweiler, Bull Terrier, and Pointer, but this suggestion is not reliable because the reports were before the availability of BAER testing and the ability to detect unilaterally deaf dogs. References usually state that deafness transmission in most other breeds is autosomal dominant, but this is false, as will be discussed below. Pigment-associated inherited deafness is not restricted to dogs. Similar defects have been reported for mice, mink, pigs, horses, cattle, cats, and humans. Deafness in blue-eyed white cats is common and is known to be passed on as an autosomal dominant defect. Blue eyes, resulting from an absence of pigment in the iris, is common with pigment-associated deafness but is not, in and of itself, an indication of deafness or the presence of a deafness gene; however, in several breeds dogs (Dalmatian, English Setter, English Cocker Spaniel, Bull Terrier) with blue eyes are statistically more likely to be deaf. Waardenburg’s syndrome, a human condition, presents with deafness, a stripe of white in the hair and beard, blue or different colored eyes (even in blacks and asians), no pigment behind the retina, and minor structural deformities around the nose and eyes. This is an autosomal dominant disorder with incomplete penetrance, which means that individuals that inherit the disorder may not show all components of the syndrome – i.e.., they may not be deaf. Incomplete penetrance of a defect greatly complicates the determination of mode of inheritance. At present there is no documentation that incomplete penetrance is a factor in any canine deafness.\nIn simple Mendelian genetics, each dog carries two copies of each gene, one from each parent. The possible outcomes of breedings can be demonstrated with tables showing the genotype of both parents and the possible combinations in their offspring. If deafness is carried as a theoretical simple autosomal recessive gene (d), the breeding of two hearing carriers (Dd) (Table 1) will result, on average, in 25% affected dogs (dd), 50% hearing carriers (Dd), and 25% free of the defect (DD). The breeding of a carrier to a dog free of the defect (Table 2) will result in no affected dogs but 50% carriers and 50% free. The breeding of an affected dog to a carrier (Table 3) will result in 50% affected, 50% carriers, and no free. Finally, the breeding of an affected dog to a dog free of the defect (Table 4) will result in 100% carriers and no affected or free.\nIf instead the deafness is carried as a simple autosomal dominant gene (D), the breeding of an affected dog (Dd) to a free dog (dd) (Table 3) would result on average in 50% affected and 50% free. Dogs with the genotype DD would be unlikely to occur unless two deaf dogs had been bred. All of the above assumes that incomplete penetrance is not acting. If more than one gene (recessive and/or dominant) is involved in producing the deafness, the possible combinations become much more complicated. In humans more than 50 different autosomal recessive or dominant deafness genes or loci have been identified. The children of two deaf parents with two different recessive deafness can be unaffected but carry both genes. If deafness in dogs results from more than one recessive gene, the possible outcomes of breedings are more numerous and determination of the mechanisms of transmission will be difficult.\nAs stated above, deafness is often associated with the merle (dapple) gene, which produces a mingled or patchwork combination of dark and light areas. This gene (M) is dominant so that affected dogs (Mm) show the pattern, which is desirable in many breeds. However, when two dogs with merle are bred, 25% will end up with the MM genotype (i.e.., Table 1). These dogs usually have a solid white coat and blue irises, are often deaf and/or blind, and are sterile. Breeders of these dogs breeds know not to breed merle to merle. In this case the deafness is neither dominant nor recessive, but is linked to a dominant gene that disrupts pigmentation and secondarily produces deaf dogs.\nGenetic transmission of deafness in dogs with the piebald (sp) and extreme white piebald (sw) pigment genes, such as the Dalmatian, is less clear. These genes affect the amount and distribution of white areas on the body. Deafness in Dalmatians does not appear to be autosomal dominant, since deaf puppies result from hearing parents. It does not appear to be a simple recessive disorder: we have bred pairs of deaf Dalmatians and obtained bilaterally hearing and unilaterally hearing puppies, when all should have been deaf if the disorder was recessive. These findings might be explained by a multi-gene cause, the presence of two different autosomal recessive deafness genes, or a syndrome with incomplete penetrance. Further studies (in progress) will be required to determine the mechanisms. Several candidate genes known to cause pigment-related deafness in humans or mice have been eliminated as the possible cause of pigment-associated deafness in Dalmatians. Whole-genome screens will hopefully identify the cause in this and other breeds.\nRecent studies have shown that deafness in Dobermans, which do not carry the merle or piebald genes, results from direct loss of cochlear hair cells without any effects on the stria vascularis. Vestibular (balance) system signs, including head tilt and circling, are seen, and the deafness is transmitted by a simple autosomal recessive mechanism. A similar pathology has been described for the Shropshire Terrier.\nSo what should breeders do when deafness crops up? The most conservative approach would be to not breed the affected animal and not repeat the breeding that produced deafness. It is frequently recommended (i.e.. Dalmatian Club of America) that bilaterally deaf puppies should be euthanized, since they make poor pets, are difficult to train, are prone to startle biting, frequently die from misadventure (cars), and require excessive care. There is considerable controversy on this point, and there is no question that many people have successfully raised deaf dogs. For every story of a problem deaf dog there seems to be a story of one that was successfully raised. Unfortunately, there is no way to predict how a deaf puppy will turn out. Unilaterally deaf dogs can make good pets but should not be bred. When deafness is uncommon in a breed, affected dogs should not be bred, but this does not mean that all related dogs are a risk and must be retired from breeding. An understanding of simple autosomal recessive and dominant patterns, as explained above, can allow the breeder to make better informed decisions and likely avoid future deaf animals without sacrificing a breeding line that has been shaped over many years. However, extreme caution must be used when line breeding of dogs related to deaf dogs, whether the deafness is unilateral or bilateral. To make these decisions in an informed manner for breeds with known deafness, it is important that advantage be taken of hearing testing facilities at veterinary schools. Unilaterally deaf dogs cannot be detected by other means, and these dogs will pass on their deafness genes.","The blue merle Labradoodle is a color variation of the Labradoodle mix.\nBlue merle Labradoodles will have blue fur with spots or patches of black. The blue coloring can range from light blue to a deeper gray.\nThis coloring is very popular, and quite uncommon. But, dogs with merle coloring can be at higher risk for health issues, including vision and hearing problems.\nWhat is a Blue Merle Labradoodle?\nLike all Labradoodles, the blue merle Labradoodle is a cross between a purebred Labrador Retriever and a purebred Standard Poodle.\nWith first generation mixes like this, all traits are left up to chance, including coat type, size, and temperament. Your puppy could be any combination of its parents!\nThe exact positioning and size of their black markings will vary from one dog to the next, making each one truly unique.\nBlue merle is one of the most common color combinations of merle coats available. But, it’s still a striking and popular color variety.\nAnd, blue merle Labradoodles are the most likely Doodle type to have blue eyes.\nSo how does such an interesting coat color happen?\nBlue Merle Labradoodle Genetics\nThe blue merle Labradoodle is a mixed breed. So, he will inherit genes from both a Labrador and a Poodle that can impact his appearance, temperament, health, and more.\nHowever, neither Poodles nor Labrador Retrievers typically carry the merle gene.\nSo, in order for you to get a blue merle Labradoodle, your mixed pup must have another breed type in his DNA with the dominant merle gene. This breed will be able to pass the merle coat gene on to your Labradoodle puppy.\nThis is why your blue merle Labradoodle may also end up with blue eyes, even though neither the purebred Lab or the purebred Poodle will.\nIn simple words, a blue merle Labradoodle must have more than just Poodle and Lab in his DNA.\nPerhaps you have a Labradoodle Collie cross. Or a Labradoodle Australian Shepherd cross.\nThe Double Merle Gene\nIn order to get a blue merle Labradoodle, some breeders may decide to breed one merle Labradoodle with another merle Labradoodle.\nThis might seem like a great idea to guarantee a merle puppy. But, it can actually be detrimental to your dog’s health.\nThe double merle gene can lead to a high likelihood of issues, including vision and hearing loss.\nIn some cases, dogs with the double merle gene can be completely blind or deaf. Some may even be both.\nLuckily, responsible breeders will take care to reduce the chances of these issues by eliminating the double merle gene in litters.\nThis means, a merle Labradoodle should only ever be bred with a solid color Labradoodle.\nBlue Merle Labradoodle Temperament\nBecause blue merle Labradoodles must get their color variation from a different breed, their temperament can actually be quite unpredictable.\nAs we’ve already learned, mixed breeds can inherit any trait from either parent.\nSo, if you have a blue merle Doodle that is actually a Labradoodle Australian Shepherd mix, it could be anywhere between the temperaments of its two parents.\nGenerally, Labradoodles are friendly, affectionate, playful, and energetic. And, many of the dog breeds that have the merle gene are working dogs, known to be intelligent, energetic, and loyal.\nSo, a blue merle Labradoodle may have these traits.\nThe best way to predict what your dog’s temperament will be like is to interact with the parents, and socialize and train your dog from a young age.\nMake sure you find out from your breeder exactly how your blue merle Doodle is being bred, and if possible, try to meet the parent dogs in person.\nBlue Merle Labradoodle Training and Exercise\nTrainability and exercise needs will be influenced by your puppy’s genetics just as much as their appearance and temperament.\nGenerally, a blue merle Labradoodle is easily trained. But, again, the best way to predict your mix’s trainability is to interact with their parents.\nStart training and socializing from a young age.\nPositive methods will help you to build and reinforce a strong bond with your dog.\nAnd, it can be a great way to avoid any potential stubbornness.\nThese dogs will generally need at least 60 minutes of exercise a day as adults. This could be retrieving a ball, going for a hike with you, or just running around the dog park.\nWithout enough exercise, they may start to show unwanted and undesirable behaviors.\nBlue Merle Labradoodle Health\nAs we mentioned earlier, the double merle gene is related to some serious health issues surrounding your dog’s vision and hearing.\nHowever, aside from this, a blue merle Labradoodle will be prone to any other genetic health issue prevalent in its parent breeds.\nSo, this will include the Poodle and Labrador. But, it will also include the breed from which your Doodle got their coat color.\nCommon health issues that Labradoodle owners should be aware of include:\n- Hip dysplasia\n- Elbow dysplasia\n- Progressive Retinal Atrophy\n- von Willebrand’s disease\n- Ear infections\nBreeders should be able to show you clean certificates of health for your blue merle Labradoodle.\nHow to Groom a Blue Merle Labradoodle\nThe best products and techniques to groom a blue merle Labradoodle will depend on the coat type they inherit from their parents.\nLabradoodles can have one of three coat types – hair, fleece, or wool.\nAnd, your Labradoodle’s coat type may also be influenced by the breed that has given them the genetics for the blue merle coloring.\nSo, it will really depend on the traits your individual dog inherits.\nGenerally, Doodles with straighter hair coats will shed more heavily, but suffer fewer coat tangles.\nThose with fleece or wool coats may shed considerably less. But, they will need a lot more grooming in order to work tangles out of their curly fur.\nMany Labradoodle owners prefer to take their Doodle to a progressional groomer.\nAside from their Coat\nOn top of brushing and bathing, your blue merle Labradoodle will also need his ears cleaned and checked regularly to keep them free of wax that could build up and lead to ear infections.\nHis nails should also be trimmed on occasion, to keep them from cracking or splitting. If you aren’t confident doing this, most dog breeders will be able to do it for you.\nLike all dogs, the blue merle Labradoodle could also be prone to dental disease.\nTo keep his teeth as healthy as possible, brush his teeth daily with a dog-friendly toothpaste and toothbrush.\nFinding a Blue Merle Labradoodle Puppy\nA blue merle Labradoodle might not be easy to find, since it isn’t pure Labradoodle. Some other breed will be involved in the mix.\nAnd remember the risks of double merle-related health issues when searching for a puppy.\nDon’t be afraid to ask questions about your puppy’s parents to ensure one of them is a solid-colored Labradoodle.\nIt’s a good idea for you to try and meet the parent dogs used in this mix, since a non-Labradoodle breed will be involved in some way.\nThis can add an extra layer of unpredictability. But, meeting the parent dogs involved can help you see how your puppy may turn out.\nGenerally, Labradoodles cost over $1000, no matter what their color. The prices of blue merle Labradoodles can vary a lot, since this color is uncommon, but your Doodle won’t actually be a regular Poodle Labrador mix.\nIs the Blue Merle Labradoodle Right for You?\nWhilst the blue merle Doodle can look striking, it can be prone to certain health issues, especially if he is bred by an irresponsible breeder who doesn’t understand the impact of the double merle gene.\nThese dogs can make wonderful family dogs for the right home. But, you should find out which breed has given them the genetic code for a merle coat to further predict their temperament, health, and care needs.\nIs the blue merle Labradoodle your dream dog?\nReaders Also Liked\nReferences and Resources\n- Armstrong, J. ‘Color Genes in the Poodle’, University of Ottawa (1999)\n- ‘Merle’, UC DAVIS Veterinary Medicine, Veterinary Genetics Laboratory\n- Coren, S. ‘Your Dog’s Coat Color Predicts his Hearing Ability’, Psychology Today (2012)\n- Webb, A. & Cullen, C. ‘Coat Color and Coat Color Pattern-Related Neurologic and Neuro-ophthalmic Diseases‘, The Canine Veterinary Journal (2010)\n- Buzhardt, L. ‘Genetics Basics – Coat Color Genetics in Dogs‘, VCA Hospitals\n- Schmutz, S. & Berryere, T. ‘Genes Affecting Coat Color and Pattern in Domestic Dogs: A Review‘, Animal Genetics (2007)\n- Howell, T. (et al), ‘Puppy Parties and Beyond: The Role of Early Age Socialization Practices on Adult Dog Behavior‘, Veterinary Medicine: Research and Reports (2015)"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:14189908-d1e8-4a1d-bb55-48b732bcae30>","<urn:uuid:0a353f9a-6f4c-4dd5-a7ec-2288829b08f0>"],"error":null}
{"question":"How do the ages of Murujuga's rock art compare to the oldest known cave paintings in Europe?","answer":"Murujuga's rock art dates back at least 40,000-50,000 years, with some human face depictions being 30,000 years old, making them significantly older than the oldest known European cave paintings found in Chauvet, France, which have been dated to around 32,900±490 years ago.","context":["On a remote peninsula in Western Australia, a 16-hour drive from the nearest city, 30,000-year-old faces stare at the rare visitor to this wild location. Those human depictions are part of Murujuga, one of the world’s largest collections of ancient rock art. These artifacts are 10 times older than the pyramids of Egypt.\nDating back tens of thousands of years, this cluster of one million images on the Burrup Peninsula is like an artistic encyclopedia, depicting human and environmental evolution. Carved into rocks are images of changing landscapes, tribal customs, and now-extinct species such as the Tasmanian tiger and fat-tailed kangaroo. These petroglyphs also reveal the mythology of one of the world’s oldest civilizations, Aboriginal Australians.\nAlthough this extraordinary place is little known, even to most Australians, it is now gaining recognition for two contrasting reasons. There’s excitement around the tentative UNESCO World Heritage listing of Murujuga, which could drive a tourism boom. That is tempered, however, by grave warnings from rock art scientists that Murujuga could be destroyed within a century by pollution from the massive and growing industrial precinct that surrounds it.\nSuch a catastrophe is not unprecedented in Western Australia (WA), the economy of which relies on resource extraction. Two years ago, the world’s second biggest mining company, Rio Tinto, blasted a sacred 46,000-year-old Aboriginal rock art shelter, Juukan Gorge, as it expanded an iron ore project. That atrocity occurred about 140 miles south of Murujuga.\nRock art for the ages\nBoth of those rock art sites are located in the Pilbara. This rugged region in WA’s north features towering gorges, serrated mountains, vast plains of red earth, and many multibillion-dollar mines. WA is among the most sparsely populated territories on the planet. It has almost four times the land area of Texas, yet is home to only 2.6 million people, about 80 percent of whom live in the state capital Perth, and less than 4 percent of whom are Aboriginal.\nEven richer than the Pilbara’s resources of iron ore and liquid natural gas is its Aboriginal heritage. Since more than 50,000 years before the British brutally colonized Australia, this region has been inhabited by the Ngarda-Ngarli people. That is the collective term for the Aboriginal traditional owner groups of Murujuga—the Ngarluma, Yaburara, Mardudhunera, Yindjibarndi, and the Wong-Goo-Tt-Oo.\nIt was these people who named Murujuga, which covers the Dampier Archipelago and adjacent Burrup Peninsula. There, one of the world’s most important collections of petroglyphs was created over thousands of years, says Benjamin Smith, Professor of World Rock Art at University of Western Australia.\n(Thousands of Bronze Age petroglyphs provide clues about ancient society.)\nOf the other significant rock art sites in the world—from 7,000-year-old carvings in Norway to 25,000-year-old cave paintings in Brazil and 13,000-year-old paintings in Zimbabwe—none rival Murujuga for volume or continuity, he says. “What makes Murujuga special is the density and absolute amount of rock art,” Smith says. “The art also has a longer sequence than any of these other sites, extending from recent times back at least 40,000 years, probably 50,000 years.”\nRock art researchers so far have catalogued only 3 percent of Murujuga’s total area, an ongoing project that has recorded 50,000 images, Smith says. There could be up to 2 million petroglyphs at Murujuga.\nAs well as being majestic works of art, these carvings provide remarkable scientific insights. “Murujuga has some of the oldest known images of the human face and a series of extinct animals,” Smith says. “The changing fauna within the art shows massive climatic and environmental changes over time. The site was once more than 60 miles inland. Now it is a peninsula surrounded by sea.”\nAccording to the mythology of the Ngarda-Ngarli people, Murujuga’s rock art was shaped by the Marrga ancestral creator beings. These spirits helped to shape the natural world. They also inhabit the Dreamtime, a set of legends and beliefs that underpin Aboriginal culture, explaining creation and offering a guidebook to human life.\nCut into Murujuga’s rocks are Dreamtime stories thousands of years old. Yet this rock art remains greatly relevant to Aboriginals, says Marduthenera people custodian Raelene Cooper. To outsiders, Murujuga’s rocks may appear to be inanimate objects. But to her people they “hold DNA, a living, breathing, spiritual energy.”\n“The rock art tells the stories of evolution and are a biblical archive of our sacred ancient history,” Cooper says. “They carry and hold a deep connection to Mother Earth.”\n(The oldest North American rock art may be 14,800 years old.)\nMurujuga explains the past, present, and future to new generations, says Belinda Churnside, a Ngarluma custodian on the board of the Murujuga Aboriginal Corporation (MAC), which aims to represent the interests of the site’s traditional owners. “This rock art is from the beginning of time to the end of time,” Churnside says.\nThe fight to preserve Murujuga\nYet in a physical sense, Murujuga’s future is bleak, says Smith. Pollution generated by huge and expanding industrial complexes on the Burrup Peninsula threaten the ancient site. “If the pollution levels are allowed to continue at current levels, serious damage will be done to the rock surfaces of Murujuga and quickly,” he says.\nSome Aboriginal groups are opposing the Woodside company’s planned $12 billion Scarborough gas field development. Cooper claims toxic emissions from that project would damage Murujuga. “We can physically see the destructive consequences from chemical pollution and greenhouse emissions [from existing projects],” she says. A Woodside spokesperson has said the company supports the Murujuga Rock Art Monitoring Program run by MAC and the Western Australia government.\nYet amid this controversy, local communities remain enthusiastic about Murujuga’s pending UNESCO nomination and its potential as a tourist attraction. Cooper and Churnside both say the Ngarda-Ngarli people would be honored if their land became a UNESCO site. “To be given a global platform to share our ancient sacred history to the world is remarkable and elevates the struggles and trauma of the past,” Cooper says.\n(Explore stunning natural wonders across Australia.)\nWith plans to submit the final UNESCO application by early next year, local authorities are preparing for an expected influx of visitors to Murujuga. To accommodate them, a tourist zone is being created at Conzinc Bay. A new road would access that coastal location in the northwest of Murujuga, which currently can be reached only by all-wheel-drive vehicles. The planned hub would be the Murujuga Living Knowledge Centre. MAC chief executive Peter Jeffries says this facility would “tell stories from the stones and guide visitors through the ancient land that is Murujuga.”\nSome improvements to visitor infrastructure have already been completed, including the creation of the Ngajarli Art Viewing Trail. This 2,300-foot-long raised boardwalk features viewing platforms and signs explaining the rock art at Ngajarli Gorge. The ocher-colored rocks here are embellished by petroglyphs up to 47,000 years old, which depict goannas, turtles, kangaroos, and megafauna. “That [boardwalk] lets visitors view the petroglyphs up close while protecting them from degradation,” says Natasha Mahar, CEO at Australia’s Northwest Tourism.\nIf Murujuga’s UNESCO application succeeds, many more international tourists are likely to come eye to eye with the 30,000-year-old human faces telling the ancient story of an entire people and their cherished land—marvels etched by hand, imbued with wisdom, and designed to enthrall in perpetuity.\nRonan O’Connell is an Australian journalist and photographer that shuttles between Ireland, Thailand, and Western Australia.","The Upper Palaeolithic (Upper Paleolithic or Late Stone Age) is the third and last part of the Palaeolithic period. It lasted from about 40,000 to 10,000 years ago. Humans used tools for hunting and fishing. They also developed cave paintings. In this period, the Neanderthal man completely disappeared, leaving Homo sapiens as the only surviving species in the human genus.\nThe first modern humans found in Western Europe date back to about 36,000 years ago. Those fossils were found in the south-west of Romania. The founds were made in a stone cave called Peștera cu Oase.\nEvidence for belief in the afterlife in the Upper Palaeolithic: appearance of burial rituals and ancestor worship.\nCulture[change | change source]\nVenus figurines[change | change source]\nPossibly among the earliest traces of art are Venus figurines. These are figurines (very small statues) of women, mostly pregnant with visible breasts. The figurines were found in areas of Western Europe to Siberia. Most are between 20,000 and 30,000 years old. Two figurines have been found that are much older: the Venus of Tan-Tan, dated to 300,000 to 500,000 years ago was found in Morocco. The Venus of Berekhat Ram was found on the Golan Heights. It has been dated to 200,000 to 300,000 years ago. It may be the one of the earliest things that show the human form.\nToday it is not known what the figurines meant to the people who made them. There are two basic theories:\n- They may be representations of human fertility, or they may have been made to help it.\n- They may represent (fertility) goddesses.\nScientists have excluded that these figurines were linked to the fertility of fields, because agriculture had not been discovered at the time the figurines were made.\nThe two figurines that are older may have mostly formed by natural processes. The Venus of Tan-Tan was covered with a substance that could have been some kind of paint. The substance contained traces of iron and manganese. The figurine of Berekhat Ram shows traces that someone worked on it with a tool. A study done in 1997 states that these traces could not have been left by nature alone.\nCave paintings[change | change source]\nCave paintings are paintings that were made on the walls or roofs of caves. Many cave paintings belong to the Palaeolothic Age, and date from about 15,000 to 30,000 years ago. Among the most famous are those in the caves of Altamira in Spain and Lascaux in France.p545 There are about 350 caves in Europe where cave paintings have been found. Usually, animals have been painted, like aurochs, bisons or horses. Why these paintings were done is not known. They are not simply decorations of places where people lived. The caves they were found in usually do not show signs that someone lived in them.\nOne of the oldest caves is that of Chauvet in France. Paintings in the cave fall into two groups. One has been dated to around 30,000 to 33,000 years ago, the other to 26,000 or 27,000 years ago.p546 The oldest known cave paintings, based on radiocarbon dating of \"black from drawings, from torch marks and from the floors\". As of 1999, the dates of 31 samples from the cave have been reported. The oldest paintings have been dated from 32,900±490 years ago.\nSome archaeologists have questioned the dating. Züchner believe the two groups date from 23,000–24,000, and 10,000–18,000 years ago. Pettitt and Bahn believe the dating is inconsistent. They say the people at that periods of time painted things differently. They also do not know where the charcoal used to paint some things is from, and how big the painted area is.\nPeople from the Palaeolithic era drew well. They knew about perspective, and they knew of different ways to draw things. They also were able to observe the behaviour of animals they painted. Some of the paintings show how the painted animals behaved. The paintings may have been important for rituals.\nRelated pages[change | change source]\nReferences[change | change source]\n- \"Upper Paleolithic from Academic Press Dictionary of Science and Technology\". credoreference.com. 2011 [last update]. http://www.credoreference.com/entry/apdst/upper_paleolithic. Retrieved July 21, 2011.\n- \"Paleolithic Period (anthropology) -- Britannica Online Encyclopedia\". britannica.com. 2011 [last update]. http://www.britannica.com/EBchecked/topic/439507/Paleolithic-Period. Retrieved July 21, 2011.\n- \"The Stone Age\". history-world.org. 2011 [last update]. http://history-world.org/stone_age.htm. Retrieved July 21, 2011.\n- Trinkaus, Erik (2003). Early modern human cranial remains from the Pestera cu Oase, Romania. (45 ed.). Journal of Human Evolution. pp. 255–259.\n- \"'Oldest sculpture' found in Morocco\". BBC News online. 23 May 2003. http://news.bbc.co.uk/1/hi/sci/tech/3047383.stm.\n- Alexander Marshack (1997). \"The Berekhat Ram figurine: a late Acheulian carving from the Middle East\" (pdf). http://www.utexas.edu/courses/classicalarch/readings/Berekhat_Ram.pdf.\n- Klein, Richard G. 2009. The human career: human biological and cultural origins. 3rd ed, Chicago.\n- Quotes from Clottes 2003b p214.\n- Archaeologists sometimes use the phrase \"B.P.\" (before the present day) to mean \"years ago\"\n- Clottes 2003b p33. The oldest is sample Gifa 99776 from \"zone 10\". See also Chauvet (1996 p131, for a chronology of dates from various caves. Bahn's foreword and Clottes' epilogue to Chauvet 1996 discuss dating.\n- Züchner, Christian (September 1998). \"Grotte Chauvet Archaeologically Dated\". Communication at the International Rock Art Congress IRAC ´98. http://www.rupestre.net/tracce/12/chauv.html. Retrieved 2007-12-23.\nClottes (2003b), pp. 213-214, has a response by Clottes.\n- Pettitt, Paul; Paul Bahn (March 2003). \"Current problems in dating Palaeolithic cave art: Candamo and Chauvet\". Antiquity 77 (295): 134–141. http://www.antiquity.ac.uk/ant/077/Ant0770134.htm."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:af95b58c-a99f-4ba1-b551-11e851f16a1d>","<urn:uuid:c36dd9f4-9e71-4d92-b916-49935ced55ed>"],"error":null}
{"question":"As a dance student interested in male ballet pioneers, how do Douglas Becker and Mikhail Baryshnikov's career trajectories in ballet compare in terms of their roles with major ballet companies?","answer":"Both dancers had significant roles with prestigious ballet companies, but followed different paths. Douglas Becker joined the Joffrey Ballet, then the National Ballet of Canada under Alexander Grant, and later became a principal dancer with the Frankfurt Ballet under William Forsythe. Mikhail Baryshnikov had an illustrious career with the Russian Mariinsky Ballet in the 1960s, then became a principal dancer with the American Ballet Theater (1974-1978) and the New York City Ballet (1979), later serving as artistic director of ABT for 22 years from 1980.","context":["Douglas Becker - a Choreographer - alternative content\nDouglas Becker is a choreographer and teacher working in many idioms, including ballet, contemporary dance, and improvisation throughout Europe and the United States. A founding member and principal dancer of the Frankfurt Ballet under the direction of acclaimed choreographer William Forsythe, he played a pivotal role in the creation of many of the choreographer’s early signature pieces and is one of a number of individuals around the world with the authority to remount his repertory.\nIn the making of his own creations, Mr. Becker's choreography has been hailed as \"innovative and energetic, very graphic and with purity of movement\" (Swiss newspaper \"24 Heures\") and praised for its \"delicate, intricate partnering and fleet movements\" (The New York Times). Mr. Becker's collaborative process of choreographic development improvises upon and utilizes dancers' individual talents and characteristics. Spanning three decades of choreographic investigation, Mr. Becker has received commissions from, among others Belgium's Royal Flemish Theatre, Switzerland's Grand Théatre de Geneve, and France's CCN Ballet de Lorraine. His improvisation installation, The Third Eye was constructed for the Musée de Grenoble. As a Flemish Government Arts Commission grant recipient, Mr. Becker directed and performed in the stage work, Brutal Elves in the Woods, for Brussels' arts laboratory Nadine.\nDouglas Becker has a broad resume in education working as guest faculty at P.A.R.T.S. School Brussels, The National Conservatories of Paris and Lyon, New York University, and the University of California Irvine among others. Mr. Becker, between 2007 and 2011, founded and curated the Hollins University/American Dance Festival Masters of Fine Arts international extended studies program, under the direction of Donna Faye Burchfield; introducing dance artists to new ways of imagining their research, advancing their abilities to realize their work in a larger context, and supporting their participation in dialogues that move across geographies as well as disciplines. In 2010/11 Douglas Becker began as artist-in- residence/visiting master lecturer at The University of the Arts Philadelphia. In 2012/13 was Douglas Becker artist-in-residence at University North Carolina School of the Arts. Ballet training for Mr. Becker began in Texas under Nathalia Krassovska and Stanley Hall, and continued in New York City with David Howard, Maggie Black and Marjorie Mussman. In 1978, he joined the Joffrey Ballet and worked with choreographic masters including Agnes de Mille, Choo San Goh and Jerome Robbins, after which he joined the National Ballet of Canada under the direction of Alexander Grant. He toured extensively with the company, performing with guest artists Rudolf Nureyev and Erik Bruhn. Joining the Dallas Ballet under Danish choreographer Fleming Flindt, Mr. Becker was then invited by William Forsythe to join the Frankfurt Ballet, where he also worked with Amanda Miller, Stephen Petronio, Susan Marshall and Jan Fabre. A native of Dallas, Texas, he makes his home in Brussels Belgium\nNational Balet Academy Amsterdam/University of California Irvine http://www.the.ahk.nl/EN/02_studieaanbod/01_studierichtingen/07_nba/index.jsp Extreme Memory\n(workshop) CCN Ballet de Lorraine (dir. Didier Deschamps) http://www.ballet-de-lorraine.com/ The 3rd Eye\nGrand Théâtre de Genève (dir. Philippe Cohen) http://www.geneveopera.ch/ Each To His Own\nMy Ballet class has been developed for both classical and contemporary dancers. If we take as our point of departure the the idea that labor is accumulative, the class becomes a place where new ways of thinking about dance are considered, and that in the making and doing of dance one strives for technique rendered invisible by artistry.\nThe class begins slowly, putting an accent on attention to musical accuracy and detail. Line, counter point, and a fluid relationship between the legs and the arms are accentuated in relationship to the classical vocabulary. Each exercise is designed to develop correct posture, placement, alignment, and precise technical execution. The dance combinations that are set, and arranged in the beginning of the week are then (de/re) constructed throughout the week. Demi-plie, supple and intelligently used, is emphasized. Articulation (a working through) of the feet is looked-for. Rotation/Turn out is a chance to understand more about Ballets relationship to hip flexor and adductor . Refined port de bras is desired.\nThe use of breath, full body focus, length of spine, relaxed neck, and effortless epaulement are considered important elements for building and maintaining a healthy, economically sound and efficient technique. Self esteem is enhanced.\nCombinations are not complicated, but this can change depending on the level/desire of the group. Subtile changes in direction, speed, music and tempo are to be found.\n\"What makes something relevant is what vibrates within\"\nDonna Faye Burchfield\nAn intrinicl part of a process that culminated in the creation of the CD ROM 'Improvisation Technologies, A Tool for the Analytical Dance Eye', I have transformed over many years a methodology and collaborative process of choreographic imagination that was conceived with William Forsythe and first generation dancers of the Frankfurt Ballet into my own highly personalized educational tool for dance artists, choreographers, teachers, and actors.\nFirst and foremost to create an enviroment where the participants take ownership, make decisions, explore, analyze, and develop the imaginative, investigative, and creative elements that reside in the mind and the body of each individual, whle learning how to generate, modify, inscribe, trace, deconstruct, and reconstruct movement and text. Built into this working process is the ability for the artists to learn from each other, encouraging both the freedom of experimentation and the discipline to stay within specific tasks over a determined period of time.\nTo speak from an ethical place of knowledge, and an ethical place of access, to facilitate the transmission of what I know the best… dancing, our point of departure will be to go in the direction of, not as, not replicate, or imitate, but recover, re-visit and re-imagine. To offer access to knowledge that renders knowledge in the moment, and to think around the work, to appropriate, and to reflect.\nThis approach to learning will offer the artist, or group of any age new ways to think about dance.\nDuring my career as a performer/Principal Dancer in the Frankfurt Ballet, I either danced, acted, improvised, spoke, or sang in the following works created by William Forsythe:\nLDC, Artifact(I, II, III, IV), France/Dance, Baby Sam, Steptext, Behind The China Dogs, Big White Baby Dog, Die Befragung des Robert Scott, Enemy in the Figure, Herman Schmerman, Impressing the Czar, In the Middle Somewhat Elevated, Isabelle's Dance, Limbs Theorem, Love Songs, New Sleep, Same Old Story, Say Bye Bye, Skinny, Slingerland, Steptext, The Loss of Small Detail, The Second Detail, The Vile Parody of Address.\nOf these and other works to date I have reconstructed:\nSteptext, Artifact II, New Sleep (full length) and (pas de deux), The Vile Parody of Address, Die Befragung des Robert Scott, and Hypothetical Stream","Continue Reading Below\nThe ABC panel playfully laughed along at photos of the 6-year-old dancing.\n\"Oh, he looks so happy about the ballet class,\" she said as images of Prince George showed on the screen. \"Prince William says George absolutely loves ballet. I have news for you Prince William — We'll see how long that lasts.\"\nAfter being lambasted online, Spencer issued an apology on Instagram.\nCritics clapped back at her comments, including a popular Prince George Instagram comedy account run by Gary Jannetti, which leaned into the drama.\nSpencer told audiences during a live broadcast that she met with male members of the dance community to increase her awareness.\n\"I've spoken with several members of the dance community over the past few days, I have listened, I have learned about the bravery it takes for a young boy to pursue a career in dance and last night I sat down with three influential dancers who have lived it firsthand,\" Spencer said, introducing a profile story about famous dancers Robbie Fairchild, Travis Wall and Fabrice Calmels.\n\"There’s so much toxic masculinity in society and negative images associated with that,\" Leonides Arpon, assistant director of Earl Mosely's Hearts of Men, told FOX Business. \"We want to nurture male dancers and provide a place to become confident men regardless of if they will become professional dancers.\" Mosley's organization is aimed at nurturing young men and providing different perspectives through the medium of dance.\n\"[With Lara's] tone and facial expression, we recognize it immediately because we received it,\" Arpon said \"She did it on a national platform against a 6-year-old. It’s about educating her and she has such a platform to transform the stigmas, but instead, it was a put-down, unfortunately.\"\nDespite the controversy, ballet has led to a lucrative career for several world-renowned male dancers.\nHere are three of the highest-paid male ballet dancers of all time.\n3. Benjamin Millepied\nThe son of a former ballerina, Benjamin Millepied is the third highest-paid male dancer on the list with a net worth of $900,000. He began studies at age 8 under the revered Michel Rahn at the Conservatoire National as a teen. He joined the New York City Ballet in 1995 and began choreographing for ballets and operas around New York and France, eventually starting his own companies. His L.A. Dance Project continues today and caters to avant-garde artists and composers.\nBordeaux-born Millepied took center stage for his choreography on iconic ballet film “Black Swan” in 2009 and subsequently married lead actress, Natalie Portman.\n2. Rudolf Nureyev\nRudolf Nureyev eclipsed the fame of all dancers with his fastidious classical technique – and his estate is valued at $7.9 million. A contemporary of Mikhail Baryshnikov, the two venerable ballet dancers began their reign with Russian Mariinsky Ballet in the 1950s. Siberian-born Nureyev was a principal dancer in The Royal Ballet in Paris and performed in a never-ending repertoire of esteemed ballet productions including “Swan Lake,” “Sleeping Beauty,” “Romeo and Juliet” and “Don Quixote.”\nHis expertise allowed him entry into iconic social circles with Freddie Mercury, Jackie Kennedy Onassis and Andy Warhol. He was also the subject of Ralph Fiennes’ film “The White Crow” and starred in several documentaries about his life. The premier dancer died at age 54 due to complications from AIDS.\n1. Mikhail Baryshnikov\nArguably the most well-known male ballet dancer of all time, Mikhail Baryshnikov is the highest paid as well, having a net worth of $45 million.\nBaryshnikov toured with the Russian Mariinsky Ballet in the early 1960s. He later danced with the Royal Winnipeg Ballet and became a principal dancer with the American Ballet Theater (ABT) from 1974 to 1978, and then a principal dancer with the New York City Ballet the following year. After dancing, he returned to the ABT in 1980 for a 22-year run as the artistic director.\nThe Soviet-born dancer transcended the stage to the silver screen in films like “The Turning Point” and “White Nights,” even winning an Academy Award. He charmed a new generation in perhaps his most well-known role as “the Russian” opposite Sarah Jessica Parker’s character, Carrie Bradshaw, in “Sex in the City.”"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:3a006f68-cbbc-4eff-9c75-c09b6f4b6fc5>","<urn:uuid:039d7cbf-86a6-4838-bf34-9779ed7ad4c0>"],"error":null}
{"question":"What will be the key features of an effective business process model?","answer":"An effective business process model includes several key features: it is typically a diagram representing a sequence of activities from end to end, shows events, actions and connection points, focuses on both IT and people processes, is cross-functional combining work from multiple departments, includes resource processing, shows people's roles and responsibilities, may include external organizations' processes, and can vary in detail depending on organization size. The model is also defined by various computerized tools or software used in applying its methods.","context":["With many definitions going around and multiple ways to go about it, business process modeling can be a bit confusing, especially for a beginner. But when you go deeper you’ll realize that there isn’t much of a difference in most approaches. This business process modeling tutorial will help you learn more about the various definitions, features, history behind BPM. And we’ll briefly touch on the various business process modeling techniques as well.\nDefinition of Business Process Modeling\nAmong many definitions available online for Business Process Modeling, following are few that captured our attention;\n- BPM is a mechanism for describing and communicating the current or intended future state of a business process.\n- BPM is a means of representing the steps, participants and decision logic in business processes.\n- BPM is a method for improving organisational efficiency and quality. Its beginnings were in capital/profit-led business, but the methodology is applicable to any organised activity.\n- BPM aims to improve business performance by optimising the efficiency of connecting activities in the provision of a product or service.\n- BPM is a set of activities for representing business processes in a formal way enabling analysis and further improvement of these processes.\n- Business Process Modeling is a combination of various process related steps such as Process Mapping, Process Discovery, Process Simulation, Process Analysis and Process Improvement.\nWith all above being true, it can be summarized as how work gets done in an Enterprise or an organization.\nHow BPM Evolved\nBPM has emerged rapidly throughout the last two to three decades, and has replaced previous organizational efficiency practices such as the Time and Motion Study (TMS) or Total Quality Management (TQM). Such demand for BPM is a result of,\n- Increasing transparency and accountability of all organizations including public services and government\n- Increase in usage of information and communication systems\n- Modern complexity of business\nBPM can be considered as a quality management tool due to its I) technical nature, 2) the process emphasis and 3) analytical approaches & responsibilities arising in the improvement of quality, in the market. Business process modeling is highly useful in change management of organizations.\nNotable Business Process Modeling Features\nA summarized list of BPM features are as follows;\n- BPM is commonly a diagram representing a sequence of activities. It typically shows events, actions and links or connection points, in the sequence from end to end.\n- It mainly focuses on processes, actions and activities, etc.\n- A Business Process Model includes both IT processes and people processes.\n- Business Process Modelling is cross-functional, usually combining the work and documentation of more than one department in the organisation.\n- Resources feature within BPM in terms of how they are processed.\n- People (teams, departments, etc) feature in BPM in terms of what they do, to what, and usually when and for what reasons, especially when different possibilities or options exist, as in a flow diagram.\n- Business Process Modelling may also include activities of external organisations’ processes and systems that feed into the primary process.\n- In large organisation’s operations Business Process Models tend to be analysed and represented in more detail than in small organisations, due to scale and complexity.\n- Business Process Modelling is to an extent also defined by the various computerized tools or software which is used in applying its methods. These tools evolve with the change of time and therefore it is advised to keep an open mind on how BPM can be used.\nBusiness Process Model Hierarchy\nFollowing hierarchy is mainly used in process modeling for large enterprises. It categorizes all the processes of an organization in to five levels so that it is easier to streamline the outcome.\nHistory of Business Process Modeling\nThe origins of BPM runs centuries back . So lets take a quick run through a summarized history of it.\nIn ancient times, production in cottage industry happened by one person making one item from the beginning to the end. When factories became a standard, many employees making on item at a time proved time consuming and inefficient.\nIn 1776 “Division of Labour”– Adam Smith argued that breaking up the production process and creating peculiar tasks would simply and sped up the process. He showed that if the different stages of the manufacture were completed by different people in a chain of activities, the result would be very much more efficient. Hence business process was born.\nEarly 1900s “Time and Motion” – Thinking forward, Frederik Winslow Taylor merged his “time study” with “motion study” of Frank & L. Gilbreth, which resulted in new scientific management methods (1911) and the infamous ‘time and motion’ studies. These studies documented and analyzed work processes with the aim of reducing the time taken and the number of actions involved in each process, improving both productivity and workers’ efficiency. This was enthusiastically embraced by employers and viewed with cynicism and animosity by workers.\nEarly to mid 1900s “the one best way” – Frank Gilbreth developed the first method for documenting process flow. He presented his paper ‘Process charts – First Steps to Finding the One Best Way’ to the American Society for Mechanical Engineers (ASME) in 1921. By 1947, the ASME Standard for Process Charts was universally adopted, using Gilbreth’s original notation.\nIn 1930s disenchantment with the assembly line – In the first decade of the 20th century, ‘time and motion’ was a familiar concept, in tune with the modern ‘scientific’ age. However, by 1936, disenchantment had set in, reflected in Charlie Chaplin’s film Modern Times. The film satirized mass production and the assembly line, echoing cultural disillusionment with the dreary treadmill of industry during the great depression. It is perhaps no coincidence that theories for optimizing productivity, and those who profit most from them, are more strongly questioned or criticized when the economic cycle moves into recession.\nMid 1970s workflow – Research and development of office automation flourished between 1975 and 1985. Specialist workflow technologies and the term ‘workflow’ were established. While BPM has its historical origins in workflow, there are two key differences:\n- Document-based processes performed by people are the focus of workflow systems, while BPM focuses on both people and system processes.\n- Workflow is concerned with processes within a department while BPM addresses processes spanning the whole organization.\nIn 1980s, the quality era – Quality or Total Quality Management (TQM) was the fashionable management and business process theory, championed by Deming and Juran. Used initially in engineering and manufacturing, it is based on the Japanese philosophy of Kaizen or continuous improvement. The aim was to achieve incremental improvements to processes of cost, quality, service and speed.\nKey aspects of Total Quality Management have now become mainstream and successfully adapted to suit the businesses of the 2000s. Six Sigma and Lean manufacturing are the best-known of these methodologies.\nIn 1990s, Business Process Re-engineering (BPR)\nIn the early 1990s, Business Process Re-engineering made its appearance and started to gain momentum in the business community. While TQM (at this point facing a decline in popularity) aimed to improve business processes incrementally, BPR demanded radical change to business processes and performance.\nIn 1993, Michael Hammer and James Champy developed the concept in their book ‘Re-engineering the Corporation: A Manifesto for Business Revolution’. They stated that the process was revolutionary, fast-track and drastic rather than evolutionary and incremental. It was a huge success and organizations and consultants embraced it with fervor. The re-engineering industry grew and triumphed before it began to wane.\nBy the end of the 1990s, BPR as a whole-organization approach had fallen dramatically out of favor. It proved to be too long-winded for most organizations, was therefore poorly executed and has consequently been sidelined as a whole-organization approach.\nCritics of this completely ‘new broom’ methodology would say that it is impossible to start from a clean slate in an already established organization. Other criticisms were that it was dehumanizing and mechanistic, focusing on actions rather than people (Taylorism).\nCrucially, it is associated with the terms ‘delayering’, ‘restructuring’ and ‘downsizing’ of organizations, all lumped together as euphemisms for layoffs. Not what Hammer and Champy had envisaged.\nIn 2000s, Business Process Modelling (BPM)\nThe best principles of Business Process Re-engineering still survive in BPM, on a less drastic, less brutal and more manageable scale. With the lessons been learnt, Business Process Modelling can and does work, but it must be treated with caution. The key is in the implementation. When it is conducted and implemented sensitively and inclusively, it can be good for both the company and its staff.\nFor a workforce drowning in administration, much of it repeated or re-entered into multiple databases, BPM can be a great thing. It can free up time to focus on the ‘value added’ tasks that are empowering and rewarding: talking and listening to customers, making decisions or doing what they are good at rather than dealing with dull and meaningless duties.\nBPM is effective like any other methodology can be. In the wrong hands it can suffocate and obstruct an organization and its people. The tool does not produce the results – what matters is, how you use it.\nVarious Business Process Modeling Techniques\nIn Implementing Business Process Modeling, there are many techniques that have been tried and tested throughout the years. Some may have few drawbacks and some proven successful.\n- Flow Chart Technique\n- Data flow diagrams—Yourdon’s technique\n- Role-Activity Diagrams (RAD)\n- Role-Interaction Diagrams (RID)\n- Gantt Chart\n- Integrated Definition for Function Modelling (IDEF)\n- Colored Petri-nets (CPN)\n- Object Oriented Methods (OO)\n- Workflow Technique\n- Business Process Modeling Notation (BPMN)\n- UML Activity Diagram\nA detailed view of the above BPM techniques will be covered in our next article. Creately supports most of the techniques mentioned in above list.\nYour Thoughts on this Business Process Modeling Tutorial\nYour feedback is what keeps us going and it help us to cater our content to better serve your needs. If you have any questions about this article leave a comment and we’ll get back to you as soon as possible.\nAnd stay tuned for our next article where we’ll cover various BPM techniques in detail.\nMore Diagram Tutorials\n- Sequence Diagram Tutorial: Complete Guide with Examples\n- Ultimate Flowchart Guide ( Complete Flowchart Tutorial with Examples )\n- Use Case Diagram Tutorial ( Guide with Examples )"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:4a225096-0a75-42c3-adc5-b96515553ebd>"],"error":null}
{"question":"What makes the African savanna unique as a biome?","answer":"The African savanna is unique because it's an in-between biome, neither forest nor desert. Located between latitude 15° North and 30° South and longitude 15° West and 40° West, it's characterized by having only two seasons (wet and dry) instead of four. It consists mostly of grasses and scattered trees, notably the Acacia tree with its distinctive umbrella shape that giraffes feed on. The savanna is home to many species, including the world's largest land mammal, the African elephant.","context":["We are so excited for this week’s wild ride through the African savanna! Our lessons will also introduce your child to different areas within Africa, focusing a lot on Kenya (including several overlaps with our Level 3: Kenya Unit for homeschool families in both levels). Ready to get up close and personal with the flora, fauna, and people of this gorgeous area of the world? Start here by downloading your skills tracker—and then get ready for an adventure!\nNote: Occasionally we include project upgrades (for kids ready for more) and modifications (which can be useful for including younger siblings). We’ll mark those with the plus (+) or minus (-) symbols.\nWhat you need:\nBooks (find at your local library or order below on Amazon):\n- The Tree of Life: The World of the African Baobab by Barbara Bash (or read it online at OpenLibrary)\n- Planting the Trees of Kenya: The Story of Wangari Maathai by Claire A. Nivola (or read it here on OpenLibrary) OR Seeds of Change by Jen Cullerton Johnson\n- The Boy Who Harnessed the Wind by William Kwamkwamba and Bryan Mealer (or read it here on OpenLibrary)\n- Mama Panya’s Pancakes: A Village Tale From Kenya by Mary and Rich Chamberlain (or listen to this read aloud)\n- For You are a Kenyan Child by Kelly Cunnane (or read it here on OpenLibrary or listen to this read aloud)\nOptional additional reading:\n- The Boy Who Harnessed the Wind by William Kwamkwamba and Bryan Mealer – This is the chapter book version of the picture book we’ll read for our lessons. If your child is interested in learning more about William Kwamkwamba’s story, this would make a great read aloud book for the next few weeks.\nSupplies (use what you have, but here are links to shop if you need anything)\n- atlas, globe, or world map\n- African animal figurines\n- binoculars (optional)\n- Barrel of Monkeys set (or you can use any manipulative, but these fit nicely into our theme!)\n- paper + access to a printer (don’t have one? we like this model)\n- laminator + laminator sheets (optional, but recommended for repeating lessons)\n- velcro dots\n- popsicle sticks\n- paper cutter (not required, but it does make prep work go more quickly)]\n- purple party blowout\n- white cardstock\n- lined paper for copywork\n- 2 small paper cups\n- bendable straw\n- 4 pennies\n- block of wood (a sturdy piece of cardboard would also work)\n- hot glue gun + glue\n- wire cutters\n- paper plate\n- single hole punch\n- 1 1/4 cups flour\n- 2 cups of cold water\n- 1/3 cup vegetable oil\n- 1/2 tsp salt\n- 1/2 tsp cardamom\n- 1/2 tsp crushed red chili pepper flakes\nWhat to do:\nWe recommend doing the below lessons in this order to build on each skill your child will develop, but don’t feel that you *need* to do them in this order. Do what works for you and your child. If they love an activity, feel free to repeat! Not a winner? Skip and try the next thing. Have fun!\nNew to our phonics guide? Start here. The Phonics Guide this week will highlight the phonic rule for words that end in the letter A, as in “savanna.”\nLet’s start with some map work. Let’s find the African Savanna on Google Earth.\nThe African Savanna biome is a tropical grassland in Africa between latitude 15° North and 30° South and longitude 15° West and 40° West. (Use a globe or world map to review how to use longitude and latitude to find a location with your child.) It covers Guinea, Sierra Leone, Liberia, Cote D’ivore, Ghana, Togo, Benin, Nigeria, Cameroon, Central African Republic, Chad, Sudan, Ethiopia, Somalia, and the Democratic Republic of the Congo, Angola, Uganda, Rwanda, Burundi, Kenya, Tanzania, Malawi, Zambia, Zimbabwe, Mozambique, Botswana, and South Africa. (source)\nYou can also point out Kenya since we will be learning more about these places this week. You may also want to find these places in your atlas, on a map, or on your globe.\nActivity 1: What is a savanna? Begin by watching this video. These details are shared in the video. Review them with your child:\n- Savannas are an in-between biome, not really a forest and not really a desert. This habitat is home to many different species of plants and animals around the world, and in Africa it is home to the largest land mammal in the world, the African elephant.\n- Savannas are found in tropical hot climates and are defined by the seasons. They don’t have the traditional summer and winter seasons that we are familiar with in the United States. In the savanna, seasons are defined by how much rain falls. Because of this, they only have two seasons instead of four: wet and dry. (source)\n- Savannas are comprised mostly of grasses and a few scattered trees. The Acacia tree is an interesting plant in the savanna. It has an umbrella shape, with branches and leaves high off the ground that giraffes like to eat. (source)\nActivity 2: Does your child know what a biome is? A biome is a large area characterized by its vegetation, soil, climate, and wildlife. There are five major types of biome in the world (aquatic, grassland, forest, desert, and tundra), though some of these biomes can be further divided into more specific categories, such as freshwater, marine, savanna, tropical rainforest, temperate rainforest, and taiga. (source) You can show your child many of the major biomes in this world map.\n(+) If your child likes color-by-number pictures, you can have them create their own world biome map with this printable. Note: This map only highlights 6 of the terrestrial biomes—it doesn’t break up the types of grasslands.\nNext, let’s play a game inspired by Go Fish to review biomes. Click here to download and print the cards on cardstock. (You may also want to laminate them for durability.) Our card game reviews the 8 major terrestrial biomes. Print two sets for each person playing, and then deal each person four cards. Play Go Fish until your child ends up with a pair of each, reviewing each biome when you find a pair.\nActivity 3: Africa is an incredible continent, and one reason is because of its diversity of life. This diversity is possible in part because Africa has such a variety of biomes within its landmass. Let’s discover them with this color-by-number printable. (Note: This printable breaks down the biomes even further than our card game. Use it as an opportunity to do some online research into these more specific biome types.)\nActivity 4: We’ve done a lot of focus work today! Let’s end the day by getting up and getting outside (if possible). First, draw a map of your backyard (or living room). Next, use your African animal figurines to create your own safari for your child! Hide the figurines around the yard or room, and mark an X on your map where each animal can be found. Next, let them start finding animals! (You may also want to give them a pair of binoculars, but that’s purely optional!) Once they have found the animal, have them identify it. Is it an herbivore or a carnivore? How does it impact its ecosystem? Next, examine the landscape. What kind of biome do you live in?\nToday, we’ll learn more about some of the amazing life in the savanna. Start by reading the story The Tree of Life: The World of the African Baobab to learn about this amazing plant and all the animals that depend on it to thrive in the savanna.\nActivity 1: Our book today introduces a variety of animal that rely on the baobab tree to survive in the African savanna. These types of relationships are called symbiosis. Let’s watch this video to learn more about it. There are actually different types of symbiosis in the animal world—today, let’s learn about the three main types: mutualism, commensalism, and parasitism. Review the definitions of each below, and then complete our activity to bring it life in a hands-on way.\n- Mutualism: A symbiotic relationship where both sides benefit.\n- Commensalism: A symbiotic relationship where one side benefits and the other is unaffected.\n- Parasitism: A symbiotic relationship where one side benefits and the other side is harmed in some way.\nLet’s bring it to life! Print this symbiosis activity and laminate, if possible. Cut out the relationship cards and creature circles. Using velcro dots, put one dot on the back of each of the creature circles (use the rough side of the velcro dot) and put a soft velcro dot on the end of a popsicle stick. As you review each relationship cards, have your child attach the creature circle to the popsicle stick so it can “interact” with the plant or animal on the relationship card. Read them the details of the relationship and ask them to determine which type of symbiotic relationship this is. They can then sort the relationship card into the printed chart.\nActivity 2: The acacia tree, the acacia ant, and the giraffe are an awesome example of symbiosis on the savanna. Let’s learn more about the acacia in this video, then watch this video about the acacia and the ant’s mutual relationship. The giraffe relies on the acacia as a food source, but the acacia tree uses it’s super long thorns, a special chemical called tannins (that tastes bad to giraffes), and the stinging ant to protect itself. (source) But the giraffe has a clever trick to bypass some of those defenses—its super long tongue!\nLet’s learn more about giraffes in this video. Next, let’s make a craft to demonstrate the reach of that amazing purple tongue. Start by printing this mask template out onto cardstock or other thick white paper. Have your child color in the giraffe mask. Next, cut out the eyes and mouth holes. Finally, stick a purple party blower through the mouth to create a tongue, like this:\nActivity 3: One animal we saw in our book was the baboon! Let’s learn more about these interesting, inventive creatures in this video.\nNext, let’s get some monkey math practice with this printable and a Barrel of Monkeys set (or some other similar manipulative). Start by printing and laminating the baobab tree page. Next, set up a story about monkeys in the tree—but they keep falling out! Start with a certain number of monkeys in the tree, 1-20. Have your child write the number in the first box. Next, tell them that some silly monkeys fell out and give them a number to subtract. Have your child write a minus symbol (-) in the middle box the number to subtract in the second box. Finally, have them find the difference. You could even sing a version of the “5 little monkeys jumping on the bed” song, changing the words to:\n“10 little monkeys in the baobab tree.\n4 fell out…how many do we see?“\nYou could also use this activity to practice addition. Simply write the addition symbol (+) in the middle box and pretend more monkeys are climbing into the tree.\nActivity 4: Let’s practice a little safari literacy! Start by printing and laminating these pages. Next, cut out the safari car slider and the word cards. (We have also included blank cards to write in whatever words you like—just use a dry erase marker if you laminate.) Take your first word card and lay the safari card slider on top. Then, slide the safari card to the write, gradually revealing each letter to help your child focus on blending each letter sound. Work through the cards until your child reads all the words or for as long as they want to work!\nToday, we are going to learn about two famous people from Kenya who had a big impact on the world. The first is Wangari Maathai, the first African woman to receive the Nobel Peace Prize. (She was also the first woman in East and Central Africa to earn a doctorate degree!) Let’s learn more about her story by reading either Planting the Trees of Kenya: The Story of Wangari Maathai (or read it here on OpenLibrary) OR Seeds of Change. Want some background on the Nobel Peace Prize? You can also watch this video to learn more about how it was founded and how it works.\nActivity 1: Wangari Maathai is certainly an inspirational person. Let’s use one of her quotes for our copywork today! Choose the quote below your child likes best, and then write it clearly on lined paper. Have your child copy the quote directly below what you have written. (For more tips on our approach to copywork, check out this blog post.)\n“When we plant trees, we plant the seeds of peace and seeds of hope. We also secure the future for our children.”\n“As I told the foresters, and the women, you don’t need a diploma to plant a tree.”\n“No matter how dark the cloud, there is always a thin, silver lining, and that is what we must look for.”\n“The generation that destroys the environment is not the generation that pays the price.”\nActivity 2: Another person from Africa who made great strides in conservation was William Kwamkwamba. Let’s read more about his discoveries in The Boy Who Harnessed the Wind (or read it here on OpenLibrary).\nHow do wind turbines work? Learn more about it in this brief video. Next, let’s make our own windmill with this tutorial.\n(+) Does your child love LEGO? Try to make this LEGO windmill instead!\nActivity 3: African animals crossword.\nOne of the countries in Africa that is popular for safaris is Kenya. Located in Eastern Africa, Kenya has more than 40 national parks and game reserves have been set aside for the conservation of wildlife and natural habitat. The country’s official name is the Republic of Kenya, and its capital is Nairobi. (source) Help your child to locate Kenya in an atlas, on a globe, or on this world map. Let’s learn more about life in Kenya with today’s lessons, starting with the book For Your are a Kenyan Child. You can also watch this fun video about some of the history of Kenya.\nActivity 1: You can see a picture of the Kenyan flag here. The Kenyan flag was adopted on 12th December, 1963 as the country’s flag. The color black represents the people of the Republic of Kenya, red for the blood shed during the fight for independence, green for the country’s landscape, and the white was added later to symbolize peace and honesty. (source) Let’s color this picture of the Kenyan flag. As your child colors, you can show them this video tour through Nairobi.\nActivity 2: In Kenya, more than 60 languages are spoken and there are more than 40 ethnic groups. Almost everyone there speaks more than one African language, but Swahili and English are the two official languages. (source) Let’s learn some Swahili in this video!\nActivity 3: The traditional game shisima (which means “body of water”) from Kenya is a great, play-based way to teach math. let’s learn how to play it today in this post.\nFor our last day of our unit, let’s learn about a folk tale (or a traditional story that was originally passed on through word of mouth), music, art and cuisine from Kenya. Start by reading Mama Panya’s Pancakes.\nActivity 1: In our book, we see some of the young people playing a mbira, or a thumb piano. This instrument was invented by the Shona people of Zimbabwe, but it has become popular in other parts of eastern Africa, including Kenya. The instrument typically consists of 22 to 28 metal keys mounted on a gwariva (hardwood soundboard) made from wood of the mubvamaropa tree. Although the metal keys were originally smelted directly from rock containing iron ore, now they are made of steel from bed springs, bicycle spokes, car seat springs, and other recycled or new steel materials. (source)\nClick here to hear what a mbira sounds like. Next, let’s make our own with this simple tutorial.\nActivity 2 Another society found in parts of Kenya are the Maasai people. The Maasai land can be found in southern Kenya and Northern Tanzania. This website shares more information about the Maasai and their ways of life. One thing the Maasai are famous for is the intricate beadwork of their jewelry and traditional clothing. You can learn more about it and see more photos of this beautiful work here. Next, let’s make a necklace inspired by the Maasai’s work with this tutorial.\nActivity 3: We’ll end our week with a taste of Kenya using the pancake recipe from our book. Here’s what you’ll need:\n- 1 1/4 cups flour\n- 2 cups of cold water\n- 1/3 cup vegetable oil\n- 1/2 tsp salt\n- 1/2 tsp cardamom\n- 1/2 tsp crushed red chili pepper flakes\n- In a bowl, mix all the ingredients with a fork.\n- Preheat a nonstick pan at a medium to low setting.\n- Ladle 1/4 cup of batter into the center of the pan. Tilt the pan to spread the batter to about the size of a grapefruit.\n- Cook until you see tiny bubbles in the pancake, then gently flip it over.\n- When the second side begins to pop up from the heat, the pancake is ready.\nServing suggestions: You can fill your pancake with jam, tuna or chicken salad, seasoned hamburger or roasted nuts. Anything at all will do. Place your filling on one half and then roll it up and eat it.\n***Post contains affiliate links. If you make a purchase through a link, we may receive a small commission at no cost to you. Thank you for supporting our small business!***"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:907bf541-dc62-425e-bfba-675c53034c0a>"],"error":null}
{"question":"How do recent developments in photonic crystal nanocavities impact both optical data processing and nanoparticle manipulation in integrated chip applications?","answer":"Recent developments in photonic crystal nanocavities have enabled significant advances in both optical data processing and nanoparticle manipulation for integrated chips. For optical processing, these cavities achieve unprecedented low-energy switching in the attojoule range, operating at 40 Gbit/s with power consumption of only a few milliwatts for 1000 devices on a single chip. The devices can be integrated with a small footprint of less than 0.01 mm2. For nanoparticle manipulation, the technology enables precise trapping with ultra-large potential depth, achieving trapping forces of 8.28 nN/mW for 10nm particles and lateral trapping stiffness of 167.17 pN·nm−1·mW−1, which is over two orders of magnitude higher than previous on-chip traps. The technology's dual capabilities in both optical processing and particle manipulation make it particularly promising for developing versatile lab-on-a-chip devices.","context":["Ultralow-energy All-optical Switches Based on Photonic Crystal Nanocavities\nPhotonic crystal cavities having a wavelength-sized volume exhibit a strong light-matter interaction, which makes possible all-optical switches operating with ultralow energy consumption. Using the combination of an extremely small photonic crystal nanocavity and strong carrier-induced nonlinearity in InGaAsP, we have successfully demonstrated low-energy switching in the attojoule range for the first time. This switching energy is more than two orders of magnitude lower than that of previously reported optical switches. The ultrasmall cavity also contributes to fast carrier diffusion and hence a fast switching response of 20 ps at minimum. These all-optical switches with their small size and low-energy, fast-response operation may open up the possibility of a dense photonic network on a chip.\nPhotonics has played a key role in the progress of long-haul network links. On the other hand, although data processing at nodes or routers is still performed with integrated electronic circuits, their increasing power consumption and heat generation during high-bitrate operation is now becoming an obstacle to further improvements in network speed and traffic capacity. All-optical data processing with integrated photonic circuits is therefore expected to reduce the amount of power consumed by their electronic counterparts while keeping the high-speed properties of optical signals . However, current photonic processing devices generally require a very high driving energy and are too big for integration, though future interconnect technology will demand micrometer-scale optical components in a chip consuming less than a femtojoule per bit . These problems arise from the difficulty of confining light in a small volume and the weakness of light-matter interactions.\nPhotonic crystal (PhC) cavities, which exhibit a high cavity quality factor (Q) and an ultrasmall modal volume (V), are promising candidates as platforms on which to construct devices with dimensions on the order of a few wavelengths of light in matter. Since optical nonlinearities can be greatly enhanced in high-Q, small-V cavities (the optical-field intensity is enhanced in proportion to Q/V), very low operating energy/power can be expected if we apply them to various functional devices. The all-optical switch, which enables gating of an optical signal, should be one of the fundamental elements for constructing photonic circuits –, and it is a straightforward example of light-matter interaction enhancement by nanocavities –.\nA PhC nanocavity is schematically illustrated in Fig. 1(a). A two-dimensional array of airholes is patterned into a semiconductor thin plate having a thickness of ~200 nm. A line defect and a point defect in the array can act as an input/output waveguide and a nanocavity, respectively. The PhC nanocavity can strongly confine light, thereby achieving a strong light-matter interaction and optical nonlinear functionalities , . The operating principle of all-optical switching is shown in Fig. 1(b). The pump and signal light pulses are injected into the waveguide simultaneously. The pump light generates carriers in the nanocavity and induces a wavelength shift in the resonant transmission spectrum, which makes it possible to control the signal light output.\nThis article clarifies the design principle for a PhC-nanocavity-based switch, namely, what type of cavity, nonlinearity, and material we should use. Devices based on our designs exhibit all-optical switching with operating energy in the attojoule range and a time window of a few tens of picoseconds. Our results clearly show that PhC nanocavities enable unprecedented all-optical switches that may lead to high-speed, low-power information processing on a chip.\n2.1 Smallest PhC cavity\nAlthough we said that a high Q/V ratio is preferable for a lower switching energy, that is a bit too simple. In practice, we need to choose an appropriate Q according to the target operating speed because the photon lifetime in a cavity is proportional to Q. In contrast, cavity volume V should always be as small as possible. In this study, we used a lattice-shifted cavity (hereinafter referred to as an H0 cavity) . The cavity mode consists of only two primary antinodes (Fig. 2(a)). Importantly, the H0 cavity has the smallest V among dielectric-core PhC cavities. In our simulation, V was calculated to be only 0.025 µm3.\nAnother limiting factor for operating speed is the carrier relaxation time (τc). Generally, τc is as long as the nanosecond order, but PhC nanocavities offer an efficient way to reduce it. When the cavity becomes ultrasmall, the photogenerated carriers rapidly diffuse out from the cavity, and τc becomes small . We numerically solved the carrier diffusion dynamics in a PhC nanocavity. First, we investigated the carrier dynamics without a cavity but assuming an initial Gaussian carrier distribution centered at a certain point in a defect-free PhC lattice. In Fig. 2(a), four black lines show the decay for different initial distribution sizes. They clearly show that a small excitation leads to surprisingly fast diffusion. We investigated more realistic cases with an H0 cavity, as shown by the red line. The fitted tc values are as short as 3.5 ps. The time evolution of the carrier distribution for the H0 cavity is shown in Fig. 2(b). The carriers are initially localized in the cavity mode and then start to spread out. This result implies that we can expect a switching bandwidth of nearly 100 GHz. Consequently, it shows that an H0 cavity switch is promising in terms of high-speed response.\n2.2 Material optimization\nTo obtain lower switching energy, we want the pump light to be efficiently absorbed in the cavity and the subsequent refractive-index change to be large. InGaAsP is one compound semiconductor that exhibits these features effectively compared with other materials such as Si and GaAs at a wavelength of 1.55 µm.\nFigure 3(a) shows the calculated index change and the linear absorption lifetime as a function of the incident wavelength detuning from the bandgap wavelength λin – λg. In our switch, all-optical switching relies on band-filling dispersion (BFD) and free-carrier dispersion in InGaAsP to obtain a refractive index change . Since BFD depends strongly on the position of the electronic band-edge wavelength, we need to find an appropriate InGaAsP composition to suit the cavity°«s resonant wavelength. On the other hand, optical absorption relies on linear absorption (LA) and nonlinear two-photon absorption (TPA). LA also becomes stronger in the vicinity of the band-edge wavelength and allows efficient absorption. The important point is that excess absorption induces a degradation of cavity Q and increase in switching power, so appropriate adjustment of the composition is needed in order to minimize the switching energy.\nThe calculated switching energy Usw for the H0 nanocavity is shown in Fig. 3(b). In the vicinity of the band edge, LA boosts the absorption efficiency and BFD enhances the nonlinear resonance shift, thereby effectively reducing Usw. This results in a minimum value of less than 1 fJ at around λin – λg = 0.05 to 0.1 µm. We adjusted the InGaAsP composition to set the photoluminescence peak to 1.47 µm for an operating wavelength of around 1.55 µm.\n3. Switching demonstration\n3.1 Fabricated H0 nanocavity\nWe fabricated H0-PhC cavities in an InGaAsP slab using standard top-down processes, including electron-beam lithography and Cl2-based dry etching. A top-view image of the device is shown in Fig. 4(a). The air-hole diameter, lattice period, and slab thickness are 230, 460, and 200 nm, respectively. The H0 cavity, which was formed by shifting two neighboring air holes by 85 nm in opposite directions, is coupled with input and output PhC waveguides. The transmission spectrum acquired by scanning a wavelength of continuous-wave light is shown in Fig. 4(b). The periodic peaks in the spectrum are not a nanocavity mode, but appear as a result of interference with the Fabry-Pérot resonance caused by the facet end of the waveguide. The fitting curve (black) clarifies the nanocavity mode, indicating that the resonant wavelength λcav and linewidth are 1567.8 nm and 0.24 nm, respectively. The cavity Q factor is 6500 and the corresponding photon lifetime is τph = 5.4 ps, which is slightly longer than the calculated carrier relaxation time of 3.5 ps and is thus unlikely to restrict the switching recovery time.\n3.2 Pump-probe measurement\nTo measure the switching dynamics, we used a degenerate pump-probe technique with an optical pulse width of 14 ps , . The center wavelength of the pump pulse was always set to the resonance wavelength, while the wavelength of the probe pulse λprobe was set with detuning Δλdet = λprobe – λcav. Switching dynamics for different values of Δλdet are shown in Fig. 4(c). For Δλdet = 0.0 nm, the transmission of the probe pulse was abruptly switched off when the pump pulse temporally overlapped the probe pulse. On the other hand, the probe transmission was switched on for Δλdet = –0.3 nm and–0.6 nm because pump-induced carrier nonlinearity induced a resonant blueshift. Switching energies of 420 and 660 aJ were achieved for contrasts of 3 and 10 dB, respectively. These energies are over two magnitudes lower than those reported for Si- and GaAs-based PhC cavities. It should be noted that the switching time window is only 20–35 ps. This value is much shorter than the carrier recombination lifetime (several hundred picoseconds), which is attributable to the rapid carrier diffusion. The improvement in energy and speed is attributed to the ultrasmall size of the cavity and the strong nonlinearity of InGaAsP.\n3.3 Gate switching for 40-Gbit/s signal\nWe performed an experiment in which we extracted a pulse from a repetitive signal train to demonstrate the practicality of our all-optical switches. As shown in Fig. 5, we generated a signal train with four pulses with a 25-ps period (40-Gbit/s repetition). We also injected a pump pulse so that it temporally overlapped the second signal pulse to switch it selectively. Output signal pulses show the result of the pulse extraction experiment; they indicate that the second pulse (indicated by an arrow) was selectively switched off or on. These results are promising in terms of the suitability of PhC nanocavity switches for 40-Gbit/s operation.\n4. Comparison of all-optical switches\nVarious all-optical switches are compared in Fig. 6(a) in terms of their switching energy per bit and switching time. It is clear that our switch can operate with energy approximately two or more orders of magnitude less than for previously reported ones and has entered the attojoule energy range for the first time. In addition, the previous all-optical switches suffer from a trade-off between switching time and energy; that is, the energy-time product is limited to around 10–24–10–22. On the other hand, our device clearly overcomes this limitation, exhibiting an energy-time product of 10–26. This is attributed to the ultrasmall size of our cavity. On-chip all-optical switches are compared in Fig. 6(b) in terms of switching energy and device size. Our device exhibits both the smallest size and lowest energy. Although all-optical switches involving third-order nonlinearity  and inter-subband transition ,  can operate at a much higher bitrate, their high energy consumption and large size might be unacceptable for an on-chip integrated circuit. With our device, assuming 1000 devices on a single chip all operating at a bitrate of 10 Gbit/s, the power consumption is only a few milliwatts. In addition, the ultrasmall size of our device lets us integrate 1000 devices with a small footprint of less than 0.01 mm2 (assuming a footprint for a single device of less than 10 µm2). Consequently, the low-power, ultrasmall, and fast PhC-nanocavity switch studied here is unique.\nWe demonstrated all-optical switching with extremely low power consumption using a PhC nanocavity. The achievement of such a densely integrable PhC-nanocavity switch is very important because it will enable low power consumption in the milliwatt range, even in an integrated chip including thousands of devices with a sub-mm2-order footprint, and it operates at 40 Gbit/s. A wide variety of low-power optical devices, such as optical bistable memory and logic elements , , should be achievable in a similar way. These PhC-nanocavity-based functionalities are promising for use in optical processing in chip-scale photonic integration.\nWe thank Dr Toshiaki Tamamura of Nano F Consultant (Japan) for help with sample fabrications. This work was partly supported by Core Research for Evolutional Science and Technology (CREST) of the Japan Science and Technology Agency (JST).","Single nanoparticle trapping based on on-chip nanoslotted nanobeam cavities\nOptical trapping techniques are of great interest since they have the advantage of enabling the direct handling of nanoparticles. Among various optical trapping systems, photonic crystal nanobeam cavities have attracted great attention for integrated on-chip trapping and manipulation. However, optical trapping with high efficiency and low input power is still a big challenge in nanobeam cavities because most of the light energy is confined within the solid dielectric region. To this end, by incorporating a nanoslotted structure into an ultracompact one-dimensional photonic crystal nanobeam cavity structure, we design a promising on-chip device with ultralarge trapping potential depth to enhance the optical trapping characteristic of the cavity. In this work, we first provide a systematic analysis of the optical trapping force for an airborne polystyrene (PS) nanoparticle trapped in a cavity model. Then, to validate the theoretical analysis, the numerical simulation proof is demonstrated in detail by using the three-dimensional finite element method. For trapping a PS nanoparticle of 10 nm radius within the air-slot, a maximum trapping force as high as 8.28 nN/mW and a depth of trapping potential as large as 1.15×105 kBT?mW?1 are obtained, where kB is the Boltzmann constant and T is the system temperature. We estimate a lateral trapping stiffness of 167.17 pN·nm?1·?mW?1 for a 10 nm radius PS nanoparticle along the cavity x-axis, more than two orders of magnitude higher than previously demonstrated on-chip, near field traps. Moreover, the threshold power for stable trapping as low as 0.087 μW is achieved. In addition, trapping of a single 25 nm radius PS nanoparticle causes a 0.6 nm redshift in peak wavelength. Thus, the proposed cavity device can be used to detect single nanoparticle trapping by monitoring the resonant peak wavelength shift. We believe that the architecture with features of an ultracompact footprint, high integrability with optical waveguides/circuits, and efficient trapping demonstrated here will provide a promising candidate for developing a lab-on-a-chip device with versatile functionalities.\n基金项目：National Natural Science Foundation of China (NSFC)10.13039/501100001809 (61501053, 61611540346, 11474011, 11654003, 61435001, 61471050, 61622103); National Key R&D Program of China (2016YFA0301302); Fund of the State Key Laboratory of Information Photonics and Optical Communications (IPOC2017ZT05), Beijing University of Posts and Telecommunications10.13039/501100002766, China.\nFei Gao：State Key Laboratory for Mesoscopic Physics, School of Physics, Peking University, Collaborative Innovation Center of Quantum Matter, Beijing 100871, China\nQi-Tao Cao：State Key Laboratory for Mesoscopic Physics, School of Physics, Peking University, Collaborative Innovation Center of Quantum Matter, Beijing 100871, China\nChuan Wang：State Key Laboratory of Information Photonics and Optical Communications, Beijing University of Posts and Telecommunications, Beijing 100876, China\nYuefeng Ji：School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing 100876, ChinaState Key Laboratory of Information Photonics and Optical Communications, Beijing University of Posts and Telecommunications, Beijing 100876, China\nYun-Feng Xiao：State Key Laboratory for Mesoscopic Physics, School of Physics, Peking University, Collaborative Innovation Center of Quantum Matter, Beijing 100871, ChinaCollaborative Innovation Center of Extreme Optics, Shanxi University, Taiyuan, Shanxi 030006, China\n【1】D. Erickson, X. Serey, Y. Chen, and S. Mandal, “Critical review: nanomanipulation using near field photonics,” Lab Chip 11 , 995–1009 (2011).\n【2】D. G. Grier, “A revolution in optical manipulation,” Nature 424 , 810–816 (2003).\n【3】A. Ashkin, J. M. Dziedzic, J. E. Bjorkholm, and S. Chu, “Observation of a single-beam gradient force optical trap for dielectric particles,” Opt. Lett. 11 , 288–290 (1986).\n【4】A. N. Grigorenko, N. W. Roberts, M. R. Dickinson, and Y. Zhang, “Nanometric optical tweezers based on nanostructured substrates,” Nat. Photonics 2 , 365–370 (2008).\n【5】A. Ashkin, “History of optical trapping and manipulation of small-neutral particle, atoms, and molecules,” IEEE J. Sel. Top. Quantum Electron. 6 , 841–856 (2000).\n【6】A. Ashkin, “Acceleration and trapping of particles by radiation pressure,” Phys. Rev. Lett. 24 , 156–159 (1970).\n【7】K. C. Neuman, and S. M. Block, “Optical trapping,” Rev. Sci. Instrum. 75 , 2787–2809 (2004).\n【8】A. Ashkin, J. M. Dziedzic, and T. Yamane, “Optical trapping and manipulation of single cells using infrared laser beams,” Nature 330 , 769–771 (1987).\n【9】P. W. H. Pinkse, T. Fischer, P. Maunz, and G. Rempe, “Trapping an atom with single photons,” Nature 404 , 365–368 (2000).\n【10】L. Jauffred, S. M. R. Taheri, R. Schmitt, H. Linke, and L. B. Oddershede, “Optical trapping of gold nanoparticles in air,” Nano Lett. 15 , 4713–4719 (2015).\n【11】E. Vetsch, S. T. Dawkins, R. Mitsch, D. Reitz, P. Schneeweiss, and A. Rauschenbeutel, “Nanofiber-based optical trapping of cold neutral atoms,” IEEE J. Sel. Top. Quantum Electron. 18 , 1763–1770 (2012).\n【12】M. J. Morrissey, K. Deasy, M. Frawley, R. Kumar, E. Prel, L. Russell, V. G. Truong, and S. N. Chormaic, “Spectroscopy, manipulation and trapping of neutral atoms, molecules, and other particles using optical nanofibers: a review,” Sensors 13 , 10449–10481 (2013).\n【13】J. Huang, X. Liu, Y. Zhang, and B. Li, “Optical trapping and orientation of Escherichia coli cells using two tapered fiber probes,” Photon. Res. 3 , 308–312 (2015).\n【14】F. Lindenfelser, B. Keitch, D. Kienzler, D. Bykov, P. Uebel, M. A. Schmidt, P. St. J. Russell, and J. P. Home, “An ion trap built with photonic crystal fibre technology,” Rev. Sci. Instrum. 86 , 033107 (2015).\n【15】D. Grass, J. Fesel, S. G. Hofer, N. Kiesel, and M. Aspelmeyer, “Optical trapping and control of nanoparticles inside evacuated hollow core photonic crystal fibers,” Appl. Phys. Lett. 108 , 221103 (2016).\n【16】M. D. Baaske, and F. Vollmer, “Optical observation of single atomic ions interacting with plasmonic nanorods in aqueous solution,” Nat. Photonics 10 , 733–739 (2016).\n【17】Y. Zhi, X. Yu, Q. Gong, L. Yang, and Y. Xiao, “Single nanoparticle detection using optical microcavities,” Adv. Mater. 29 , 1604920 (2017).\n【18】B. Li, W. R. Clements, X. Yu, K. Shi, Q. Gong, and Y. Xiao, “Single nanoparticle detection using split-mode microcavity Raman lasers,” Proc. Natl. Acad. Sci. USA 111 , 14657–14662 (2014).\n【19】L. Shao, X. Jiang, X. Yu, B. Li, W. R. Clements, F. Vollmer, W. Wang, Y. Xiao, and Q. Gong, “Detection of single nanoparticles and lentiviruses using microcavity resonance broadening,” Adv. Mater. 25 , 5616–5620 (2013).\n【20】J. Zhu, Y. Zhong, and H. Liu, “Impact of nanoparticle-induced scattering of an azimuthally propagating mode on the resonance of whispering gallery microcavities,” Photon. Res. 5 , 396–405 (2017).\n【21】M. L. Juan, M. Righini, and R. Quidant, “Plasmon nano-optical tweezers,” Nat. Photonics 5 , 349–356 (2011).\n【22】K. Wang, E. Schonbrun, P. Steinvurzel, and K. B. Crozier, “Trapping and rotating nanoparticles using a plasmonic nano-tweezer with an integrated heat sink,” Nat. Commun. 2 , 469–475 (2011).\n【23】J. C. Ndukaife, A. V. Kildishev, A. Nnanna, V. M. Shalaev, S. T. Wreley, and A. Boltasseva, “Long-range and rapid transport of individual nano-objects by a hybrid electrothermoplasmonic nanotweezer,” Nat. Nanotechnol. 11 , 53–59 (2016).\n【24】Z. Chen, F. Zhang, Q. Zhang, J. Ren, H. Hao, X. Duan, P. Zhang, T. Zhang, Y. Gu, and Q. Gong, “Blue-detuned optical atom trapping in a compact plasmonic structure,” Photon. Res. 5 , 436–440 (2017).\n【25】B. S. Schmidt, A. H. J. Yang, D. Erickson, and M. Lipson, “Optofluidic trapping and transport on solid core waveguides within a microfluidic device,” Opt. Express 15 , 14322–14334 (2007).\n【26】T. H. Stievater, D. A. Kozak, M. W. Pruessner, R. Mahon, D. Park, W. S. Rabinovich, and F. K. Fatemi, “Modal characterization of nanophotonic waveguides for atom trapping,” Opt. Mater. Express 6 , 3826–3837 (2016).\n【27】A. H. J. Yang, S. D. Moore, B. S. Schmidt, M. Klug, M. Lipson, and D. Erickson, “Optical manipulation of nanoparticles and biomolecules in sub-wavelength slot waveguides,” Nature 457 , 71–75 (2009).\n【28】A. H. J. Yang, T. Lerdsuchatawanich, and D. Erickson, “Forces and transport velocities for a particle in a slot waveguide,” Nano Lett. 9 , 1182–1188 (2009).\n【29】S. Y. Lin, E. Schonbrun, and K. Crozier, “Optical manipulation with planar silicon microring resonators,” Nano Lett. 10 , 2408–2411 (2010).\n【30】P. T. Lin, and P. T. Lee, “All-optical controllable trapping and transport of subwavelength particles on a tapered photonic crystal waveguide,” Opt. Lett. 36 , 424–426 (2011).\n【31】M. G. Scullion, Y. Arita, T. F. Krauss, and K. Dholakia, “Enhancement of optical forces using slow light in a photonic crystal waveguide,” Optica 2 , 816–821 (2015).\n【32】N. D. Gupta, and V. Janyani, “Design and analysis of light trapping in thin film GaAs solar cells using 2-D photonic crystal structures at front surface,” IEEE J. Sel. Top. Quantum Electron. 53 , 4800109 (2017).\n【33】M. Barth, and O. Benson, “Manipulation of dielectric particles using photonic crystal cavities,” Appl. Phys. Lett. 89 , 253114 (2006).\n【34】A. Rahmani, and P. C. Chaumet, “Optical trapping near a photonic crystal,” Opt. Express 14 , 6353–6358 (2006).\n【35】T. Tanabe, M. Notomi, E. Kuramochi, A. Shinya, and H. Taniyama, “Trapping and delaying photons for one nanosecond in an ultrasmall high-Q photonic-crystal nanocavity,” Nat. Photonics 1 , 49–52 (2007).\n【36】C. A. Mejia, N. Huang, and M. L. Povinelli, “Optical trapping of metal-dielectric nanoparticle clusters near photonic crystal microcavities,” Opt. Lett. 37 , 3690–3692 (2012).\n【37】N. Descharmes, U. P. Dharanipathy, Z. Diao, M. Tonin, and R. Houdre, “Observation of backaction and self-induced trapping in a planar hollow photonic crystal cavity,” Phys. Rev. Lett. 110 , 123601 (2013).\n【38】A. Nirmal, A. K. K. Kyaw, J. Wang, K. Dev, X. Sun, and H. V. Demir, “Light trapping in inverted organic photovoltaics with nanoimprinted ZnO photonic crystals,” IEEE J. Photovoltaics 7 , 545–549 (2017).\n【39】M. Tonin, F. M. Mor, L. Forro, S. Jeney, and R. Houdre, “Thermal fluctuation analysis of singly optically trapped spheres in hollow photonic crystal cavities,” Appl. Phys. Lett. 109 , 241107 (2016).\n【40】P. T. Lin, T. W. Lu, and P. T. Lee, “Photonic crystal waveguide cavity with waist design for efficient trapping and detection of nanoparticles,” Opt. Express 22 , 6791–6800 (2014).\n【41】X. Serey, S. Mandal, and D. Erickson, “Comparison of silicon photonic crystal resonator designs for optical trapping of nanomaterials,” Nanotechnology 21 , 305202 (2010).\n【42】S. Mandal, X. Serey, and D. Erickson, “Nanomanipulation using silicon photonic crystal resonators,” Nano Lett. 10 , 99–104 (2010).\n【43】D. Yang, B. Wang, X. Chen, C. Wang, and Y. Ji, “Ultracompact on-chip multiplexed sensor array based on dense integration of flexible 1-D photonic crystal nanobeam cavity with large free spectral range and high Q-factor,” IEEE Photon. J. 9 , 4900412 (2017).\n【44】Y. Chen, X. Serey, R. Sarkar, P. Chen, and D. Erickson, “Controlled photonic manipulation of proteins and other nanomaterials,” Nano Lett. 12 , 1633–1637 (2012).\n【45】S. Lin, W. Zhu, Y. Jin, and K. B. Crozier, “Surface-enhanced Raman scattering with Ag nanoparticles optically trapped by a photonic crystal cavity,” Nano Lett. 13 , 559–563 (2013).\n【46】C. Renaut, B. Cluzel, J. Dellinger, L. Lalouat, E. Picard, D. Peyrade, E. Hadji, and F. Fornel, “On chip shapeable optical tweezers,” Sci. Rep. 3 , 2290 (2013).\n【47】C. Ciminelli, D. Conteduca, F. DellOlio, and M. N. Armenise, “Design of an optical trapping device based on an ultra-high Q/V resonant structure,” IEEE Photon. J. 6 , 0600916 (2014).\n【48】H. Du, X. Zhang, J. Deng, Y. Zhao, F. S. Chau, and G. Y. Zhou, “Lateral shearing optical gradient force in coupled nanobeam photonic crystal cavities,” Appl. Phys. Lett. 108 , 171102 (2016).\n【49】S. Han, and Y. Shi, “Systematic analysis of optical gradient force in photonic crystal nanobeam cavities,” Opt. Express 24 , 452–458 (2016).\n【50】F. Liang, and Q. Quan, “Detecting single gold nanoparticles (1.8?nm) with ultrahigh-q air mode photonic crystal nanobeam cavities,” ACS Photon. 2 , 1692–1697 (2015).\n【51】J. T. Robinson, C. Manolatou, L. Chen, and M. Lipson, “Ultrasmall mode volumes in dielectric optical microcavities,” Phys. Rev. Lett. 95 , 143901 (2005).\n【52】COMSOL Inc., https://www.comsol.com/.\n【53】J. Ma, L. J. Martinez, and M. L. Povinelli, “Optical trapping via guided resonance modes in a Slot-Suzuki-phase photonic crystal lattice,” Opt. Express 20 , 6816–6824 (2012).\n【54】J. D. Joannopoulos, S. G. Johnson, J. N. Winn, and R. D. Meade, Photonic Crystals: Molding the Flow of Light , 2nd ed. (Princeton University, 2008).\n【55】D. Yang, S. Kita, F. Liang, C. Wang, H. Tian, Y. Ji, M. Lonar, and Q. Quan, “High sensitivity and high Q-factor nanoslotted parallel quadrabeam photonic crystal cavity for real-time and label-free sensing,” Appl. Phys. Lett. 105 , 063118 (2014).\n【56】D. Yang, H. Tian, and Y. Ji, “High-Q and high-sensitivity width-modulated photonic crystal single nanobeam air-mode cavity for refractive index sensing,” Appl. Opt. 54 , 1–5 (2015).\n【57】C. W. Gardiner, Handbook of Stochastic Methods: For Physics, Chemistry, and the Natural Sciences , 3rd ed. (Springer, 2004).\nDaquan Yang, Fei Gao, Qi-Tao Cao, Chuan Wang, Yuefeng Ji, and Yun-Feng Xiao, \"Single nanoparticle trapping based on on-chip nanoslotted nanobeam cavities,\" Photonics Research 6(2), 99-108 (2018)"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:97775bab-61c2-497a-900f-6206d1b768d5>","<urn:uuid:65826396-3b66-4ff2-9bac-f9f14ebde1b0>"],"error":null}
{"question":"Which causes more direct physical damage to its host plant: potato leafhoppers in hop yards or dengue mosquitoes in human settlements?","answer":"Potato leafhoppers cause more direct physical damage to their host plants. They feed by sucking juices from leaf veins and inject a toxin through their saliva that blocks the veins, resulting in 'hopper burn' - a condition where leaf margins turn yellow then brown, followed by leaf curling and eventually dead leaves. This directly reduces yields and overall health of hop yards. In contrast, dengue mosquitoes don't cause direct physical damage to human settlements - they are disease vectors that transmit the dengue virus to humans, causing flu-like illness or dengue hemorrhagic fever, but don't physically damage structures or plants in urban areas.","context":["PAST CLIMATE IMPACTS ON HEALTH IN NICARAGUA\nAs part of the CRM TASP, Altamirano and Guzmán (2012) studied the impacts of past climate patterns on diarrhoea, dengue and\nleptospirosis. These are some of the most important diseases in Nicaragua and are expected to depend significantly on climate\nconditions. The statistical analysis focused on the SILAIS, or health regions, of Managua, León and Chinandega, because of the\nhigh prevalence of the three diseases in these areas. The authors looked at correlations of temperature and precipitation with the\nnumber of cases of the three diseases, both on a monthly basis to detect seasonal patterns, and with annual data, to look at the\nimpact of specific events as well as possible longer-term trends.\nThe World Health Organization (WHO, 2012a) defines diarrhoeal diseases “as the passage of three or more loose or liquid stools per\nday (or more frequent passage than is normal for the individual),” which leads to dehydration and loss of nutrition. Diarrhoea kills\n1.5 million children worldwide every year and is the second leading cause of death in children under five years of age. Diarrhoeal\ndisease is caused by bacterial, viral and parasitic organisms that infect the body through contaminated food or water, or from person\nto person. According to MINSA (2011), 264,848 cases of acute diarrhoea were reported in 2010 in Nicaragua. Of these, 99,469 cases\nwere reported in the SILAISs of Managua, León and Chinandega. This amounts to an increase in cases for these three regions of\naround 50 percent compared with the 2000 to 2009 average (Altamirano and Guzmán, 2012).\nClimate variables can have an important influence on the prevalence of diarrhoea. For example, at higher temperature levels, the\ninsects that carry bacteria reproduce more rapidly. Extreme rainfall can lead to contamination of water, which increases the breeding\nground for bacteria, viruses and parasites. Droughts can also reduce the amount of available clean water and thereby increase\nhuman contact with contaminated water.\nDue to a global resurgence in recent years, dengue is now the most common mosquito-borne viral disease in humans, with an\nestimated 50 million infections a year worldwide. It is mainly transmitted by the predominantly urban mosquito Aedes aegypti and\nleads to flu-like illness or even dengue haemorrhagic fever, which can be lethal.\nClimate conditions can influence the spread of dengue in several ways. Temperature levels affect the survival and speed of reproduction\nof both the virus and the mosquitoes that carry it. At different stages of their life cycle, mosquitoes require temperatures of around\n25° C to 32° C. Transmission can occur within a temperature range of around 14° C to 40° C. Higher temperatures can shorten the\ntime between the infection of the vector and the vector’s ability to infect someone. Water temperatures are particularly important,\nas the mosquito larvae grow in water. In warmer water they tend to develop more rapidly but will be smaller as adults, and as a\nresult, they need to bite more often in order to feed themselves with blood. Rainfall, on the other hand, can have an effect on the\navailable breeding area. Both heavy rainfall and droughts can create pools of water where mosquitoes can reproduce and grow.\nWater deposits in and around settlements are of particular importance for dengue, a predominantly urban disease (Altamirano and\nThe number of suspected monthly dengue cases closely follows seasonal rainfall patterns. Based on monthly averages for the period\nfrom 1993 to 2010 and applying a one-month lag between rainfall data and the number of cases, Altamirano and Guzmán (2012)\nfound a 78 percent, 89 percent and 93 percent correlation for Managua, León and Chinandega, respectively. While the analysis\ncannot prove a causal relationship, this result strongly supports the hypothesis that periods of heavy rainfall increase breeding\npools for disease-carrying mosquitoes. The second-highest\npeak on record occurred in 1998, when El Niño conditions initially brought warm and dry weather before Hurricane Mitch brought\ndevastating rains in October. In the month of October, 1,985 mm of rain fell in Chinandega. The year 2010, the last and highest peak\nin dengue cases, was a very wet year, with one tropical depression and two storms and annual precipitation exceeding averages.\nLeptospirosis is a bacterial disease that can cause high fever, severe headaches, muscle pain, chills, redness in the eyes, abdominal\npain, jaundice, haemorrhages, vomiting, diarrhoea and a rash. Humans contract it through water contaminated by urine from\ninfected rodents or other animals. It enters the human body through skin abrasions and the mucosa of the nose, mouth and eyes.\nAlthough not well-documented, leptospirosis occurs worldwide and peaks in the rainy season in endemic regions (WHO, 2012a).\nBacteria need water or high humidity and temperatures between 20° C and 37° C. They survive best in standing water, ponds, swamps and lagoons, but die quickly under\ndry conditions (Altamirano and Guzmán, 2012). Because transmission requires contaminated water, cases typically peak in the rainy\nseason, and the disease can reach epidemic dimensions during floods (WHO, 2012a).\nA comparison of climate and health data shows that observations in Nicaragua are in line with expected patterns. Of the 8,952 cases\nthat were reported in the three SILAISs of Managua, León and Chinandega between 1996 and 2010, over 80 percent occurred in the\nmonths of October and November, right after the peak of the rainy season. What is interesting about this disease compared with others is the fact that peaks are very\nhigh, so that there appears to be an epidemic threshold that, if passed, leads to a rapid spread of the disease.\nNicaragua’s ‘First National Communication to the UNFCCC’ (República de Nicaragua, 2001) looked at the impacts of climate change\non malaria. Higher temperatures are expected to increase the prevalence of malaria, because they increase the speed of reproduction\nand growth of the mosquitoes that transmit the disease. Also, below a certain temperature, the mosquitoes don’t survive. Rainfall\ncan affect the prevalence of malaria in similar ways as for dengue, as the mosquitoes need standing waters for reproduction.\nAccording to community consultations conducted by López et al. (2011), dry conditions often lead to an increased prevalence of\nrespiratory diseases, mainly from increased contamination of the air with dust. It is also worth mentioning the results of a study by\nDelgado Cortez (2009), which looked at the impacts of heat stress on sugarcane farm workers, who are exposed to temperatures\nof up to 34.5° C. The output of workers who didn’t drink at least six litres of water was almost one-third lower than that of workers\nwho did drink sufficiently. Apart from the occupational health risk for the involved workers, this highlights the economic damages\nof climate-induced health stress.\nHealth impacts of Hurricane Mitch\nECLAC (1999) conducted a thorough impact assessment shortly after Hurricane Mitch struck Nicaragua in October 1998. They\nlooked at, among other things, social and economic impacts in the health sector. This detailed analysis offers insights into some of\nthe more complex direct and indirect consequences of extreme events for health:\n- The main reported causes of morbidity were acute respiratory infections, acute diarrhoea-related disease, skin diseases, impetigo, conjunctivitis, and the resurgence of vector-transmitted diseases such as cholera, dengue and malaria.\n- The destruction of the health infrastructure reduced the system’s response capacity at a time when demand was soaring.\n- The high number of victims and displaced people and the destruction of water and sanitation infrastructure led to massive hygiene problems, which in turn facilitated the spread of respiratory and diarrhoea-related diseases as well as meningitis and dengue.\n- Providing people with enough water and food in the emergency was difficult because of high demand and destroyed infrastructure.\n- After the disaster, flooding, damming and landslides favoured the creation of new sites prone to the generation of vectors, which has caused increases in cases of leptospirosis, rabies, Chagas’ disease and leishmaniaiasis, along with acute respiratory\n- infections and diarrhoea-related diseases.\n- Total economic losses caused by the hurricane were estimated at US$53 million, including direct damage to structures, equipment and installations and indirect costs stemming mainly from campaigns to combat hurricane-related diseases.\nhealth_and_climate_nicaragua.pdf (2.23 MB)","Fact Sheet FS1272\nA variety of pests can be found in hop yards and may lessen yield, reduce quality, or affect plant health. Common insect pests in New Jersey hop yards include leafhoppers, aphids, and Japanese beetles. European corn borer and hop merchant have also been noted as causing some problems. Also, mites (mainly two-spotted spider mites) often cause problems.\nProper identification, timing of control measures, and proper application techniques are key elements to preventing damage to your hop crop. Regular scouting of hop yards in spring and summer is important to know when the pest is present in order to begin using cultural, biological, or chemical controls. Applying insecticidal sprays when insect pests are in low numbers, or when newly hatched, is important to achieve successful control. Following is a detailed description of the common pests found in hops production.\nThe potato leafhopper, Empoasca fabae (Harris), is the most common species in our area that feeds on hops. Leafhopper adults are small, 1/8-inch long, wedge-shaped insects. The potato leafhopper can be distinguished from other leafhoppers by the white spots on the head and thorax. Nymphs (immature stage) are wingless, and are light yellow to light yellowish-green in color. Potato leafhoppers prefer warm, dry conditions and are common in southern states where they overwinter. Due to New Jersey's climate, leafhoppers do not overwinter in our state (Ghidiu 2005). Potato leafhoppers migrate north from southern states and generally arrive in New Jersey during late April or early May.\nFemales lay 2–4 eggs per day in the leaf stems or veins of hop, with eggs hatching 7 to 10 days later. Afterwards, nymphs undergo five instars and reach full maturity in about two weeks. Newly emerged nymphs are nearly colorless, with red spots that fade as they grow. Nymphs then become yellow, finally changing to pale green in the third and later instars. There are 3–4 generations of potato leafhopper each summer in our region, making them a season-long pest in hops.\nLeafhoppers are abundant insects and can multiply rapidly. Since they are strong flyers, they move quickly from plant to plant and are known to serve as vectors for numerous plant diseases.\nPopulations in nearby host crops will affect populations in hop yards. Therefore, scouting is important to control the pest when first present in nearby fields and in the hop yard to avoid significant damage. Since there are many other host plants for potato leafhopper, populations in nearby crops will affect movement of leafhopper into hop yards. If this insect is not scouted for, damage may be seen before the actual pest is noticed.\nThe most obvious symptom of potato leafhopper feeding is hopper burn. Hopper burn is the yellowing of the leaf margin that later turns brown. This damage is followed by leaf curling and dry, dead leaves. Hopper burn occurs because potato leafhoppers feed by sucking the juices out of leaf veins and blocking the veins with a toxin found in their saliva. Damage from leafhoppers can reduce yields and overall health of hop yards.\nThere are many different aphids in the region that feed on many different plants. Many aphids may overwinter in New Jersey and thus can be a problem season-long during plant growth. Aphids are found overwintering in forested areas, shrubs and on numerous varied hosts. Aphids in general can be described as small, soft-bodied, slow-moving insects. Colors of aphids range from green, to black, brown, grey, and yellow. Winged and wingless forms can be produced within the same species.\nWhen looking for aphids they are most often found on the underside of leaves or on shoot tips. Aphid adults and nymphs pierce plants with their needlelike mouthparts and extract plant juices from foliage, usually from the undersides of leaves. Plants become weakened and lose vigor, leaves wilt and curl downward, and become distorted and discolored. Heavy damage will cause the leaves to dry up and drop off the plant.\nAphids also secrete honeydew (excrement) that sticks to leaves, shoots, and sometimes cones. The honeydew is a medium on which black sooty mold develops. Sooty mold on cones can make them unmarketable. Another indication of aphids in the yard is when ants are moving up stems into the canopy. Ants also feed on honeydew produced by the aphids.\nLastly, another good reason to control aphids is the fact that they can transmit viruses. Hop mosaic virus and American hop latent virus can be spread via saliva of aphids when moving from plant to plant.\nA common aphid that will feed on hops is the Damson hop aphid (Phorodon humuli (Schrank)). The Damson hop aphid is small, 0.05–0.1 inches, pear-shaped, and soft-bodied. It can be either winged or wingless. Those without wings are pale white to green and are typically found on the underside of leaves. The winged version are dark green or brown with black markings on the head and abdomen. Aphids have two cornicles or \"tailpipes\" at the end of the abdomen that helps with identification of this pest.\nThe most common beetle found feeding on hops in New Jersey is the Japanese beetle (Popillia Japonica). The adult Japanese beetle is shiny metallic green with bronze-colored wing covers. Along each side of the abdomen are six tufts of white hair. The adult is about 1/2-inch long. The larva is a large, 1/2- to over 1-inch long grub.\nBeetle larvae will overwinter underground, will move upwards in spring when soil temperatures warm, then begin feeding on roots, only to later emerge as flying adults that will feed on leaves and shoots. Adult beetles begin feeding in late May and can continue feeding into summer.\nThe beetles can cause significant damage to hop foliage and bines in a very short period of time if left uncontrolled. Just two beetles can strip a large hop leaf in a single day. When their numbers multiply and are left uncontrolled, beetles can strip a hop yard in just a few days (Mahafee, et.al. 2009). Beetles have two generations during the growing season, with feeding occurring throughout most of the production season.\nControlling beetles in the hop yard can pose a challenge. Adult beetles will cause damage by feeding on foliage and large numbers can fly in from different areas. Hop yards that are near wooded areas or near other favored host crops may be an attractive area for beetles to move into once other feeding sources are dried up or depleted. Regular insecticide applications are warranted once feeding begins.\nThe European corn borer, Ostrinia nubilalis (Hübner), is a serious pest to over 200 different host plants, especially vegetable and field crops. However, they have been known to deposit eggs on hops, and larvae can feed on shoots and tunnel into bines, causing significant damage.\nThe adult moths are not easy to find since they fly mainly at night. Female moths have a 1-inch wingspread and are a pale yellowish-brown with irregular dark wavy lines across their wings. Males are slightly smaller and are darker in color with olive-brown marks on their wings. Adults can be monitored using black light traps that can predict when populations are high, indicating larvae may be found in nearby host crops.\nThe Rutgers New Jersey Agricultural Experiment Station (NJAES) maintains a network of black light traps throughout the state to assist growers with monitoring; see plant-pest-advisory.rutgers.edu/?s=corn+borer. Females lay flattened, light-gray eggs in overlapping, scale-like masses of 20–30 eggs, forming a mass about 1/4 inch across and covered with a shiny waxy substance. Once hatched, eggs produce young larvae that are whitish with a black head and several rows of small round black or brown spots along the body. Mature larvae are about 1 inch long and have pinkish to flesh-colored bodies.\nIn New Jersey, mature larvae can overwinter by tunneling into stubble, stalks, or stems of the host plants, including many weeds. Then in April and May, larvae will pupate and moths appear in mid-May to June. If warm temperatures occur and humidity is high, females deposit eggs that hatch in 3 to 10 days. Young larvae begin feeding, once hatched on cones, leaf axils and will bore into vines. In early July the next pupation occurs, and in a couple of weeks the second generation of moths can be found in New Jersey. In the northern areas of the state, this second generation of moths lay eggs and larvae that will find hosts to use for overwintering. In southern New Jersey, a few second generation larvae will transform into pupae and adults, giving rise to a partial third generation that will not survive the winter (Ghidiu 2006).\nEuropean corn borer adults are difficult to control due to nighttime fight patterns. Therefore, controlling larvae is the best option. When eggs are first hatched, larvae are most susceptible to insecticides. When scouting hops look at cones, leaf axils, and undersides of leaves for larvae and feeding. Holes and stems with frass (excrement) deposits near the entrance are also indicators of larvae that have tunneled into vines. Once inside the plant, control will be difficult.\nHop merchant is a relatively small, inconspicuous butterfly that is rather closely associated with moist woods, but it sometimes strays into other areas. The hop merchant is also referred to as the eastern comma (Polygonia comma). Hop merchant is generally perceived as a pest with no economic importance. However, this might be because the Pacific Northwest, where most of the hops industry is located, is outside of the hop merchant's habitat range. There have been reports throughout the east coast of hop merchant larvae feeding on hops.\nFull-grown larvae are approximately 1.2 inches in length. The head has short spines and a pair of branching spines on top. Body color is highly variable, from white to greenish-brown to black, and the branching spines (scoli) on the body are also variable from black to white with black tips (Hall 2015).\nMites are not insects, but are eight-legged and are considered arachnids, like ticks and spiders. Mites can cause damage to hop plants by piercing leaf tissue, feeding on plant sap. Leaf feeding will cause silvering or bronzing discoloration on foliage. Damage on cones can cause significant crop loss and will show up as dry, brittle, and discolored (red) cones that may shatter from the bine. The two-spotted spider mite (Tetranychus urticae (Koch)) is the most common mite on hop. Females are larger than males, at 0.04–0.06 cm long, with males being about three-fourths the size of females (Mahafee, et. al. 2009). They are yellowish in color with a large dark spot on each side of the abdomen for both sexes.\nFemales can lay up to 16 eggs in a single day with up to 240 eggs over their lifetime, making this pest extremely prolific. When environmental conditions are favorable, mite populations can increase rapidly. Optimum temperatures for mite egg hatching and larval development are around 82°F.\nMites thrive under dry environmental conditions and may be washed off from leaves during heavy rainfall events. Mites also favor stressed and dust-covered plants. Therefore, preventing drought stress and properly fertilizing hop yards is important. Additionally, planting a low-growing turf or other non-competing cover crop will reduce dust and also create an environment for beneficial insects. When selecting a pesticide, be sure to choose a miticide; many insecticides will not control mites.\nInsect management in hop yards can be difficult since bines grow very high up, onto training strings. When shoots are first emerged and when at eye level, scouting for insect pests is easier. Checking new growth tips, undersides of leaves for live insects and generally looking for feeding signs are all important when scouting for insect pests. Applying insecticide controls should be administered after the pest has been properly identified, when numbers warrant application, before pest populations become too high, and are generally most effective when pests are newly hatched. Be sure to read the pesticide label and follow all instructions before making an application. All pests mentioned above are predominantly flying insects. Flying insects travel to the host plants from other areas and once established in a region, will most likely be a problem throughout the season.\nSanitation of the hop yard is one cultural control method that is extremely important for the future health of the crop. Any leftover refuse from harvest and uncut bines should be removed from the area after the first hard freeze since they may still contain live insects. Waiting until after the first hard freeze may reduce levels of pests on the debris and when moving debris out of the field. However, movement of infected material may spread pests to unaffected areas of the yard.\nThe waste collected can be burned to eliminate infested debris. Be sure to obtain a NJDEP Forest Fire Service open burning permit. Burning permits for infested plant life are required and need to be signed by your county agricultural agent certifying the reason for burning is due to disease infection. They are then sent to the Forest Fire Service in your region for approval. The forms are available from your local agricultural agent at your Rutgers NJAES Cooperative Extension county office. See njaes.rutgers.edu/county for the location in your county.\nNew Jersey does not currently have pesticide recommendations for insect control in hop yards. When using recommendations from other states, first confirm if the pesticide is approved for use in New Jersey with the Department of Environmental Protection. Please read the pesticide label for all instructions, regulations, and uses. Information available for pest control products can be found via the following sources:\nCopyright © 2017 Rutgers, The State University of New Jersey. All rights reserved.\nFor more information: http://njaes.rutgers.edu.\nCooperating Agencies: Rutgers, The State University of New Jersey, U.S. Department of Agriculture, and County Boards of Chosen Freeholders. Rutgers Cooperative Extension, a unit of the Rutgers New Jersey Agricultural Experiment Station, is an equal opportunity program provider and employer.\nSearch This Site:"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:cff2c9a2-0d57-400c-9ef0-732ee3008b8c>","<urn:uuid:28fc3e3a-92bf-49ea-abc0-402150bc7cac>"],"error":null}
{"question":"How do balance requirements differ between fishing and surfing? What are the key differences in maintaining stability in both activities?","answer":"Fishing and surfing have distinct balance requirements. In fishing, balance is primarily needed when casting near shores or handling caught fish, with the main focus being on staying steady on the boat or shore. In surfing, balance is much more critical and dynamic - it involves multiple positions including paddling (using alternating arm movements), sitting (distributing weight across the surfboard), and standing (requiring careful weight distribution in the center of the board while in motion). Surfing requires constant adjustment to the unpredictable waves, where even slight losses of balance can cause wipeouts, while fishing demands more static stability focused on patient waiting and careful movements when handling equipment.","context":["Lots of people like boating in their free time and like to fish as well. This is a great time to relax, but even frequent fishers can learn more. Read on to learn some new ways to improve your technique and have your next fishing outing be the most successful yet.\nPatience is key for going on a fishing trip. Sometimes you will have to wait for a very long time to get a single bite. If you allow yourself to become frustrated, you will lose out on all the relaxing benefits of the hobby.\nThe most important piece of equipment in your tackle box is a sharp hook. The fishing hook ensures that whatever you catch will stay on while you attempt to reel it in. Check your hooks’ sharpness regularly. Sharpen or replace them prior to a fishing trip before they become dull.\nWhen you go fishing it can help you to bring sunscreen, even if it isn’t all that bright out. The sunscreen will help you avoid getting sunburned while you are in the sun waiting for the fish to bite.\nYou may enjoy better results while fishing in lakes or rivers if you cast close to the shore. Fish like shallow waters, as they’re easier to find food in, so you’re more likely to get a bite there as well. Just be sure to avoid getting tangled in weeds!\nIf you have little fishing experience, try bass fishing. Bass can be easier to lure than other types of fish. Bass fishing also puts up a good fish making your experience more exciting.\nWhen planning a fishing trip, be certain to always include a knife that’s sharp inside your tackle box. This equipment is very important and you’ll have lots of problems if you do not have it. Remember to have a fishing knife that is of high quality, sharp and rust resistant.\nFind the perfect place to fish, and then cast slightly upstream of that spot. This will drag your hook and bait past the fish. The bait will look more natural as the current does the work for you. This technique is especially useful if fish are collected under a rock or other obstruction.\nWhen deep sea fishing, make sure that you are watching for signs of nearby fish. Perhaps you see floating debris or wood. If you see these signs, you are often seeing signs that you might be able to catch some larger fish in the area. Another great clue is gulls feeding on smaller fish, which tends to indicate large fish also nearby.\nIf you’re new at fishing, or if you have been a fisherman all your life, you should always fish with an attitude that is positive. It can be quite frustrating to fish, so don’t let negative thoughts creep in. Stay happy and remember that it’s the whole experience which makes fishing worthwhile.\nGive fly fishing a go! Although fly fishing is one of the best ways to fish, it’s a different style that you have to get used to. If you’re interested in fly fishing, you’ll need very different equipment such as clothing, lures, and rods.\nIf it is absolutely necessary to net a fish, do so with the fish’s head first. Because a fish can not swim backwards, netting the head first will make it less likely to move and thus, easier to net. You should only use a net when necessary.\nWalleye and smallmouth bass can be more particular about the live bait you use. Take along around two dozen leeches for those time when you need a variety of bait. Keep the leeches alive in a ventilated cup or container that has a few inches of water inside.\nBefore you leave to go fishing, always remember to check the weather report to make sure you are going to be safe. It is also good to bring along a radio to stay up-to-date in case there are any changes in the weather.\nBe sure to check the weather prior to going on a fishing trip. Certain weather conditions are better than others for fishing. The ideal time to fish is when the sky is overcast but not stormy. Fishing can be done in any weather conditions, but there is a good chance you will be successful with overcast skies.\nBring your camera with you so you can show people the fish you caught before putting them back in the water. This guarantees that you get your keepsake and the fish lives on.\nIf you catch a large fish which turns around during the reel in time frame, let it go. If this happens, it means your line isn’t long enough to use the pump and reel method. Next time this happens, give it a little more time before you reel in the fish.\nNo matter the size of the fish, always give your children praise for their effort and catch. Children do much better with a boost of confidence. Congratulating them for catching a fish, even if it’s a small fish, will really boost their egos and make them excited about fishing.\nWhen planning a fishing trip, be certain to always include a knife that’s sharp inside your tackle box. You can find yourself in serious difficulty if you forget to pack this vital piece of fishing gear. Ensure that your knife is sharp, rust-resistant and of a high quality.\nIt is wise for fly fishermen to practice. It takes a lot of practice to cast well, so invest the time and patience it requires. As time goes by, you will find your form improving, and you will be able to place flies much better.\nIf there’s one thing you can never have enough of when fishing, it is patience. If you are the type who can’t sit still, fishing may not be your sport. To develop the necessary patience, start with baby-step trips and work your way up to the bigger fishes.\nSpinnerbaits are great entry level lures that anyone can use. These types of lures work really well when fishing in shaded areas or in murky water around a dock. Bass tend to be attracted to spinnerbait, but so do crappies.\nIf you are going to go bass fishing, you need to have the correct type of lure. Many factors come into play when choosing a lure. You should use different lures based on the time of day, fish type and water type. Each lure has its purpose, and each of them are meant for a certain location and type of fish. In order to find the right lure, it usually takes trial and error.\nBarbless single hooks are the hook of choice for catch and release fishing, which returns the fish to the water after the catch. Doing this will keep the fish from being injured, and will allow you to return it to the water in perfect condition.\nDo not forget to bring extra food and water when you go out fishing, especially in the summer. The sun and heat can exhaust you, so it is essential that you have enough food and water to give you the energy that you will need. Snacks or easy to fix meals are a great choice if you are planning on a long day of fishing.\nIn situations where a net is required to land a fish, you want to get the fish into the net head first if at all possible. This reduces the amount of stress to the fish and prevents it from suffering too much damage from the net. Use both hand to help support the net. This will help ensure that the fish stays in the net.\nFishing is a good way to relax, enjoy the outdoors and have a great time. No matter how long you have been fishing, some new advice can always benefit you. Make sure to implement the tips you’ve just learned on your next fishing trip.\nAlways check out the weather forecast before going out on your fishing expedition. There will be days where the weather just doesn’t permit for good fishing. Many people say that when the sky is cloudy and overcast, it is a great time to fish. Although you could catch fish even in other weather conditions, you’re a lot more likely to reel in fish when there’s an overcast sky.","Learning about surfing doesn’t have to be hard for the newbie! All you need to do to get started is do some research. The best way to research is by learning the basic rules of surfing. Approaching surfing in a reckless way can be dangerous. Poor surfing posture, an ill-fitting surfboard, weak balance and safety can make surfing a life threatening sport. The number one thing a surfing newbie should remember is to stay safe. Surfing is essentially a game of balancing and manipulating your body and board against the unpredictable waves of the ocean. Just the slightest slip off your feet or center of balance can cause you to wipe out.\nCreate a Good Sense of Balance\nCreating a good sense of balance is the first important tip you’ll want to remember. Different types of surfboards float on water in distinctive ways. A surfboard won’t change its position on water surfaces when resting in relation to the water underneath. Lay on your surfboard in the water to test your balance. Lying on your board will test its buoyancy against the water. It should dip enough to know you’re being supported by the board’s strength. Find a spot to lie against on your board and memorize it. This keeps your board at the same level when resetting your surfing position on flat water.\nPosture isn’t just for formal occasions. It’s an important keyword in surfing too. Poor posture equals instant first aid or a trip to the emergency room. Three even more important posture related keywords to remember are paddling, sitting and standing. Paddling propels you and your surfboard through the water. Sitting lets you distribute your weight across your surfboard while moving it. Standing is the most important—it tests your center of balance as you slowly force your body to distribute its weight on your moving board. Knowing all three is necessary and all three work in relation to each other: paddling gets you out there, sitting keeps you on your board, standing naturally happens after sitting once the waves start approaching.\nPaddling, Sitting and Standing\nYou start paddling by lying down on your board and moving your arms one at a time like a crawl stroke. Moving both arms at the same time causes your board to become unstable against the water. Shift from a laying position into a sitting one. You’re going to want to keep calm and balance your weight the best you can. Don’t get frustrated because it takes good practice to sit right in the beginning. Standing comes after sitting. Beginning surfers should not rush into it! Shift back into a laying position with your head looking straight to the front. Take on a push-up position and bring your body upward while you shift your feet underneath. Center your weight at the middle of the board. When you come up, stay in a crouched position. Grip the board with your feet and keep your arms and head held up.\nBeginner surfers need to practice paddling, sitting and standing on their surfboards before they can surf like the pros. It’s best to have someone with you while practicing. Always have someone demonstrate for you if you don’t understand. Surfing can be fun if you take the time to learn. Rushing into advanced surfing tricks won’t be as fun. And quite frankly, you’ll risk injury if you rush too soon.\nThere are many small details to using a surfboard out on the waves. But, the good news is that with patience and practice you will be out there feeling the awesome power of the ocean beneath you and the warmth of the sun on your head."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:6834573c-43dd-45b4-865f-009e2b783f08>","<urn:uuid:bedfe3b8-e4ee-477b-a607-807aecefb9ce>"],"error":null}
{"question":"As a writing instructor concerned about academic integrity, what are the traditional characteristics of prose writing in literature, and how is AI technology challenging these established practices?","answer":"Prose writing in literature is characterized by straightforward communication using grammatical structure that mimics natural speech. It's the predominant form in novels, short stories, journalism, and academic writing. However, AI technology like ChatGPT is now challenging traditional prose writing practices by generating well-written, coherent essays in seconds that can pass plagiarism checks. This creates significant concerns for academic integrity since students can use AI to produce essays without demonstrating real understanding or critical thinking skills. The situation is particularly problematic because AI can access online information, incorporate key terms and definitions, and create original-seeming content that teachers cannot easily distinguish from genuine student work.","context":["Definition of Prose\nProse is a communicative that sounds natural and uses grammatical structure. Prose is the opposite of , or poetry, which employs a rhythmic structure that does not mimic ordinary speech. There is, however, some poetry called “prose poetry” that uses elements of prose while adding in poetic techniques such as heightened emotional content, high frequency of metaphors, and of contrasting images. Most forms of writing and speaking are done in prose, including short stories and novels, journalism, academic writing, and regular conversations.\nThe word “prose” comes from the Latin expression prosa oratio, which means straightforward or direct speech. Due to the definition of prose referring to straightforward communication, “prosaic” has come to mean dull and commonplace . When used as a literary term, however, prose does not carry this .\nCommon Examples of Prose\nEverything that is not poetry is prose. Therefore, every utterance or written word that is not in the form of verse is an example of prose. Here are some different formats that prose comes in:\n- Casual : “Hi, how are you?” “I’m fine, how are you?” “Fine, thanks.”\n- Oration: I have a dream that my four little children will one day live in a nation where they will not be judged by the color of their skin but by the content of their character. –Martin Luther King, Jr.\n- Dictionary definition: Prose (n)—the ordinary form of spoken or written language, without metrical structure, as distinguished from poetry or verse.\n- Philosophical texts: Whoever fights monsters should see to it that in the process he does not become a monster. And if you gaze long enough into an abyss, the abyss will gaze back into you. –Friedrich Nietzsche\n- Journalism: State and local officials were heavily criticized for their response to the January 2014 storm that created a traffic nightmare and left some motorists stranded for 18 hours or more.\nSignificance of Prose in Literature\nMuch of the world’s literature is written in a prose style. However, this was not always the case. Ancient Greek dramas, religious texts, and old epic poetry were all usually written in verse. Verse is much more highly stylized than prose. In literature, prose became popular as a way to express more realistic dialogues and present narration in a more straightforward style. With very few exceptions, all novels and short stories are written in prose.\nExamples of Prose in Literature\nI shall never be fool enough to turn knight-errant. For I see quite well that it’s not the fashion now to do as they did in the olden days when they say those famous knights roamed the world.\n(Don Quixote by Miguel de Cervantes)\nDon Quixote is often considered the forerunner of the modern novel, and here we can see Cervantes’s prose style as being very direct with some .\nThe ledge, where I placed my candle, had a few mildewed books piled up in one corner; and it was covered with writing scratched on the paint. This writing, however, was nothing but a name repeated in all kinds of characters, large and small—Catherine Earnshaw, here and there varied to Catherine Heathcliff, and then again to Catherine Linton. In vapid listlessness I leant my head against the window, and continued spelling over Catherine Earnshaw—Heathcliff—Linton, till my eyes closed; but they had not rested five minutes when a glare of white letters started from the dark, as vivid as spectres—the air swarmed with Catherines; and rousing myself to dispel the obtrusive name, I discovered my candle wick reclining on one of the antique volumes, and perfuming the place with an odour of roasted calf-skin.\n(Wuthering Heights by Emily Brontë)\nIn this prose example from Emily Brontë we hear from the narrator, who is focused on the character of Catherine and her fate. The prose style mimics his obsession in its long, winding sentences.\n“I never know you was so brave, Jim,” she went on comfortingly. “You is just like big mans; you wait for him lift his head and then you go for him. Ain’t you feel scared a bit? Now we take that snake home and show everybody. Nobody ain’t seen in this kawn-tree so big snake like you kill.”\n(My Antonia by Willa Cather)\nIn this excerpt from My Antonia, Willa Cather uses her prose to suggest the sound of Antonia’s English. She is a recent immigrant and as the book progresses her English improves, yet never loses the flavor of being a non-native speaker.\nRobert Cohn was once middleweight boxing champion of Princeton. Do not think I am very much impressed by that as a boxing title, but it meant a lot to Cohn. He cared nothing for boxing, in fact he disliked it, but he learned it painfully and thoroughly to counteract the feeling of inferiority and shyness he had felt on being treated as a Jew at Princeton.\n(The Sun also Rises by Ernest Hemingway)\nErnest Hemingway wrote his prose in a very direct and straightforward manner. This excerpt from The Sun Also Rises demonstrates the directness in which he wrote–there is no subtlety to the narrator’s remark “Do not think I am very much impressed by that as a boxing title.”\nThe Lighthouse was then a silvery, misty-looking tower with a yellow eye, that opened suddenly, and softly in the evening. Now—\nJames looked at the Lighthouse. He could see the white-washed rocks; the tower, stark and straight; he could see that it was barred with black and white; he could see windows in it; he could even see washing spread on the rocks to dry. So that was the Lighthouse, was it?\nNo, the other was also the Lighthouse. For nothing was simply one thing. The other Lighthouse was true too.\n(To the Lighthouse by Virginia Woolf)\nVirginia Woolf was noted for her stream-of-consciousness prose style. This excerpt from To the Lighthouse demonstrates her style of writing in the same way that thoughts occur to a normal person.\nAnd if sometimes, on the steps of a palace or the green grass of a ditch, in the mournful solitude of your room, you wake again, drunkenness already diminishing or gone, ask the wind, the wave, the star, the bird, the clock, everything that is flying, everything that is groaning, everything that is rolling, everything that is singing, everything that is speaking. . .ask what time it is and wind, wave, star, bird, clock will answer you: “It is time to be drunk! So as not to be the martyred slaves of time, be drunk, be continually drunk! On wine, on poetry or on virtue as you wish.”\n(“Be Drunk” by Charles Baudelaire)\nUnlike the previous examples, this is an example of a prose poem. Note that it is written in a fluid way that uses regular grammar and , yet has an inarguably poetic sense to it.","As publicly accessible artificial intelligence (AI) technology grows in popularity, the development of AI writing tools is raising concerns among teachers.\nOver the last few weeks, many educators who have experimented with the latest version of this technology, called ChatGPT, and have quickly realised that it has worrying implications for traditional student essays.\nRight now, students can sign up to AI programs like ChatGPT and, in just a few seconds, they can produce a full essay on any given topic.\nWhat is more, these essays are so well-written that teachers are often unable to tell the difference between real student work and AI-made essays. In fact, some students have already been fooling their teachers and submitting such work undetected.\nTherefore, there is a growing need for teachers to be aware of the current trends in AI programs and what they mean for the classroom.\nThankfully, there are things that educators can do right now to minimise their impacts on student work.\nWhat is ChatGPT?\nChatGPT is the latest in a growing number of natural language processing (NLP) tools developed by technology companies like OpenAI. ChatGPT is a variant of the GPT-3 language model, which is a machine learning system that can generate human-like text.\nAI like ChatGPT are programs that are designed to mimic human conversation. Online programs like this are able to understand and respond to a wide range of topics and can generate coherently written text.\nHow AI programs work\nMany people are surprised to learn that AI programs like this have been around for a while. For the last few years, people have been able to pay a monthly subscription to websites like Jasper AI which has allowed them to create several thousand-word essays on any topic in a matter of seconds.\nThe ability for a computer program to generate an extended piece of writing that is both coherent and detailed on any topic is genuinely impressive. In fact, many companies have begun utilising them to create advertising copy and online articles.\nAI can achieve this impressive feat because it draws upon the internet itself as its knowledge base. These programs access a range of websites, extract what it believes are the key terms, definitions, and relevant data, and incorporates all of these into automatically created sentences.\nAs a result, an extended written piece by such programs will generally be accurate and well-written. At this point, both teachers and students realise what this could mean for school assignment work.\nStudents using AI to write essays\nThe obvious potential for AI programs to be used to generate essays or other written assignments on behalf of students is concerning. Any learner with internet access and the willingness to pay the required fee can make use of this technology to cheat the educational system.\nFor example, a student could ask the program to “Write a history essay on the causes and consequences of the Black Death”. Then, in less than 10 seconds, the AI will produce a well-written, 800-word essay on the topic.\nWhat is most alarming to teachers is that, for all intents and purposes, the program has written a completely original piece of work. When put through plagiarism checkers, like Turnitin, it shows as 0% plagiarised.\nAs teachers, we want to know that our students have understood the content we've covered in class. For many years, written essays were thought to be the best way for students to demonstrate their learning.\nAlready, social media is abuzz with teachers panicking about the implications AI has on their classes in the coming year. Let's be honest: there is nothing stopping entire classes of students using these online tools to pass essay tasks with no real effort.\nSome teachers have already decided that assigning their classes in-class exams instead of take-home assignments is the best ways to avoid the problems that AI programs create.\nHowever, I think the rise of AI tools like this has finally highlighted an underlying problem with traditional history essays that has existed for generations.\nIn fact, I believe this will act as a catalyst to revolutionising how history essays are done in the classroom: a change that has been needed for a long time.\nThe weakness of traditional history essays\nOne of the major weaknesses of traditional student History essays is that they simply required students to regurgitate basic historical information.\nFor over a century now, teachers have asked students to recount a specific event or period in history. As long as students knew the important names, dates, places, and key concepts, they could incorporate them into their writing. While it took time and effort, such tasks were not intellectually difficult.\nIn fact, for at least the last two decades, teachers' main concern is that students have been able to complete these assignments by simply rewording information found on Wikipedia. Again, all that the students needed to demonstrate was the key information, rewritten into their own language.\nIn recent years, students have been able to use some pretty basic AI, such as Quillbot, to automatically rewrite Wikipedia sentences to avoid plagiarism.\nThe current generation of AI is just simplifying what students have already been doing already: it draws upon basic information provided online and rewords it into comprehensible sentences.\nIn reality, students have already been relying on the surface-level understanding of topics provided by Wikipedia rather than deeper analysis and critical thinking. AI just makes this process quicker for students.\nEducators have known for decades that simply asking students to restate in their own words basic historical information only promotes lower-order thinking.\nThere have been growing calls for more higher-order thinking in History essay tasks for quite a few years. However, schools and teachers have been reluctant to change outdated tasks like this. In the midst of the increasing workload placed upon educators, it has just been much easier to keep things the same rather than change them.\nSo, the challenges faced by teachers from the new AI programs might finally be the nudge we need to improve the assessment tasks we give to our students.\nA way forward for History essays\nThankfully, there is an essay assessment type that has been available to History teachers which can circumvent the issue of AI generated content: source-based history essays.\nThere is a key weakness of all AI programs at the moment. Namely, that they do not know how to incorporate historical sources into their writing. Not only can the AI not identify the correct historical sources for any given topic, it also cannot successfully extract key quotes nor incorporate them into a coherent historical argument.\nFurthermore, AI does not have the ability to analyse and evaluate such sources, nor synthesise this criticism coherently into its sentences. Simply put, the ability to engage critically with relevant sources is still far too complex for these programs.\nWhile AI algorithms are able to process large amounts of data and identify patterns and trends, they are unable to incorporate information from sources.\nFor example, an AI algorithm may be able to identify the most commonly mentioned words in a primary source document, but it may not be able to fully understand the meaning or implications of these words in the broader context of the document or the time period in which it was written.\nAs a result, source-based history essays have been the best higher-order thinking tasks available for students and teachers. The fact that there are multiple layers of complexity required by these tasks means that it will be a long time before AI can mimic them.\nUltimately, the analysis and evaluation of historical sources requires a deep understanding of historical context and the ability to think critically about the information presented. While AI algorithms can be helpful tools in this process, they cannot replace the skills and expertise of trained historians.\nFor those teachers who want to both avoid the problems created by AI, and genuinely engage with higher order thinking with their students, source-based history essays are the best solution.\nThe fear of student use of emerging AI technology to submit work that is not their own is should not keep teachers awake at night, worrying. Instead, it should encourage us to focus on tasks which is a much more genuine demonstration of their learning in the classroom.\nInstead of using out-dated, knowledge-based essays, we should be promoting critical thinking through source-based essays.\nOverall, the use of AI in student history essays highlights the obvious weaknesses of traditional class essays and provides an alternative approach that encourages critical thinking and analysis.\nBy focusing on the use of primary and secondary sources as the core element of historical essays, students can develop a deeper understanding of the past and the skills they need to succeed in the modern world.\nAs a result, History teachers should not be concerned about the impact of AI generated essays, as long as we shift the focus of our teaching to the critical use of historical sources in our classrooms."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:4d6514df-e988-489c-9779-51a4bd44ee6b>","<urn:uuid:3e725fdc-47d9-4737-ae0a-ed0eb6444e87>"],"error":null}
{"question":"¿Cuáles son las principales diferencias between XPS and Auger spectroscopy for surface analysis?","answer":"XPS and Auger have several key differences: XPS has a larger spatial zone of analysis (0.05 to 1.0 mm²) making it better for sampling large, diffuse surface layers, while Auger has a small, focused high-resolution beam suited for identifying composition of particles ~5 micrometers in size. XPS can detect both elements and their bonding states/chemical bonds, making it ideal for identifying compounds. In contrast, Auger is more sensitive to lower elemental concentration levels but generally cannot identify compounds or bonding parameters. Both techniques are sensitive to 1-2 nanometers depth and can generate composition depth profiles through sputtering.","context":["An adhesive must be able to effectively wet the substrate acted upon, harden, and withstand stress. Various internal and external forces such as heat, stress or chemical solvents can weaken an adhesive. When the strength of the adhesion weakens, the bonded surfaces may separate, causing failure.\nBonding failures can be classified as:\n- Cohesive – the failure originates in the bulk bonding material and may be caused by degradation of the adhesive.\n- Adhesive – the failure occurs at the interface of the two materials being bonded and may be caused by improper surface preparation. Even very thin contaminating films, only a few molecular layers thick, can prevent proper adhesion and cause failure.\n- Mixed – a combination of cohesive and adhesive failures.\nTechniques Used to Determine Adhesion Failure\nSEM Analysis with EDS\nScanning electron microscopy (SEM) with energy dispersive analysis (EDS) identifies the elemental microstructure of a sample and generates high resolution images. This is often the first step in determining particle characterization in any general failure analysis. However, the analysis depth of SEM/EDS below the surface is relatively deep. Thin surface layers or films can easily be obstructed by the signal from below the surface (typically 1-2 micrometers or 1000-2000 nanometers). When sampling the chemistry of the very top surface layer of 10 to 30 nanometers is necessary, instrumentation specifically designed for this purpose is required since even a very thin film can result in adhesion failure.\nXPS and Auger\nX-Ray Photoelectron Spectroscopy (XPS) and Auger Electron Spectroscopy (AES) are excellent surface analysis techniques perfectly suited for adhesion failures, surface staining, surface contamination, etc. Both instruments detect chemical species on the surface of the sample and are complementary to each other. The resultant spectra are “fingerprints” of the analyzed material. If the fingerprint cannot readily be determined by one instrument, the other sometimes may show a clearer fingerprint. XPS is used to analyze inorganic and organic compounds and various composite materials, such as metal alloys, polymers, and ceramics. It is an excellent technique to determine how the surface of a material was prepared for bonding or to determine the point of failure at the surfaces’ interface.\nAES is an efficient and straightforward characterization technique for examining the chemical and compositional elements of the top surface of a solid. This technique is known for its sensitivity and quantitative detail.\nComparison of XPS and AES\n- XPS has a larger spatial zone (or diameter) of analysis compared to Auger. It is typically 0.05 to 1.0 mm2, making it more suitable for sampling a relatively large, diffuse surface layer.\n- In contrast, Auger has a small, focused high-resolution beam which can be used to identify the composition of small particles ~5 micrometers in size.\n- In addition to detecting the elements themselves, XPS can detect the bonding states of elements as well as their chemical bonds, making it the technique of choice for identifying compounds on a surface.\n- While Auger is more sensitive to lower elemental concentration levels, it is generally not sensitive to bonding parameters or identifying compounds.\n- Both instruments isolate surface chemistry from the bulk, being sensitive to ~1 to 2 nanometers depth (3 to 10 atomic layers).\n- Both instruments can sputter away the surface layer, allowing a composition depth profile to be generated.\nFTIR and Raman\nBoth Fourier Transform Infrared spectroscopy (FTIR) and Raman spectroscopy are used to characterize and identify organic materials and adhesive, and/or the surface preparation and cleaning materials used in bonding. They require different sample sizes for analysis and have varying sampling zone (diameter) or spatial resolution of the spectra generated. As with the XPS and Auger techniques, these analytical methods complement each other by filling in the gaps to offer a complete picture of why materials, such as polymers and rubber, lubricants and liquids, fail to bond.\nFTIR can identify organic material and contaminants, including polymers, powders, and films, but provides somewhat limited inorganic data. Raman is used to characterize and identify both organic and inorganic materials and can determine the chemical structure of a material to identify its compounds.\nRaman and FTIR can be used for probing the bulk chemical composition of an adhesive for properties such as additive concentration, cross link density, oxidation, and other factors that would affect the cohesive strength of an adhesive.\nComparison of FTIR and Raman:\n- FTIR requires a sample of at least 15µm in diameter, but Raman has a minimum analysis area of approximately 0.5µm providing better spatial resolution.\n- FTIR is used typically for investigations consisting of organic materials but cannot be used for aqueous analyses; Raman is used to identify organic, inorganic and aqueous samples.\n- Raman also identifies crystalline structure which makes it easier to determine stress failures.\n- Raman imaging and confocal depth profiling can be used to produce maps that identify how components are dispersed in mixtures in three dimensions.\n- FTIR and Raman are both qualitative and quantitative however quantitative Raman analyses can be more difficult.\n- Both have access to spectral libraries to assist in identification.\n- Both techniques are non-destructive and can be used under ambient conditions.\nFor more information on failure analysis for adhesives and binding, please contact us at 1.800.860.1775."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:a15221a9-36de-4457-a1b8-0ed8a553c446>"],"error":null}
{"question":"First time project manager here - RAD vs traditional SDLC methodology?","answer":"RAD (Rapid Application Development) uses minimal planning in favor of rapid prototyping, with planning interleaved with writing the software itself. This enables faster development and easier requirement changes, but may compromise functionality and performance. The traditional SDLC follows a more structured approach with distinct phases: requirements gathering, system design, coding and implementation, testing, and deployment. Each phase has specific activities and deliverables, like detailed requirements documentation and comprehensive testing plans. RAD was developed as a response to problems with traditional methodologies, where applications took so long to build that requirements changed before completion, resulting in inadequate systems.","context":["Introduction to Software Engineering/Process/Rapid Application Development\nRapid application development (RAD) refers to a type of software development methodology that uses minimal planning in favor of rapid prototyping. The \"planning\" of software developed using RAD is interleaved with writing the software itself. The lack of extensive pre-planning generally allows software to be written much faster, and makes it easier to change requirements.\nRapid application development is a software development methodology that involves methods like iterative development and software prototyping. According to Whitten (2004), it is a merger of various structured techniques, especially data-driven Information Engineering, with prototyping techniques to accelerate software systems development.\nIn rapid application development, structured techniques and prototyping are especially used to define users' requirements and to design the final system. The development process starts with the development of preliminary data models and business process models using structured techniques. In the next stage, requirements are verified using prototyping, eventually to refine the data and process models. These stages are repeated iteratively; further development results in \"a combined business requirements and technical design statement to be used for constructing new systems\".\nRAD approaches may entail compromises in functionality and performance in exchange for enabling faster development and facilitating application maintenance.\nRapid application development is a term originally used to describe a software development process introduced by James Martin in 1981. Martin's methodology involves iterative development and the construction of prototypes. More recently, the term and its acronym have come to be used in a broader, general sense that encompasses a variety of methods aimed at speeding application development, such as the use of software frameworks of varied types, such as web application frameworks.\nRapid application development was a response to non-agile processes developed in the 1970s and 1980s, such as the Structured Systems Analysis and Design Method and other Waterfall models. One problem with previous methodologies was that applications took so long to build that requirements had changed before the system was complete, resulting in inadequate or even unusable systems. Another problem was the assumption that a methodical requirements analysis phase alone would identify all the critical requirements. Ample evidence attests to the fact that this is seldom the case, even for projects with highly experienced professionals at all levels.\nStarting with the ideas of Brian Gallagher, Alex Balchin, Barry Boehm and Scott Shultz, James Martin developed the rapid application development approach during the 1980s at IBM and finally formalized it by publishing a book in 1991, Rapid Application Development.\nThe shift from traditional session-based client/server development to open sessionless and collaborative development like Web 2.0 has increased the need for faster iterations through the phases of the SDLC. This, coupled with the growing use of open source frameworks and products in core commercial development, has, for many developers, rekindled interest in finding a silver bullet RAD methodology.\nAlthough most RAD methodologies foster software re-use, small team structure and distributed system development, most RAD practitioners recognize that, ultimately, no one “rapid” methodology can provide an order of magnitude improvement over any other development methodology.\nAll types of RAD have the potential for providing a good framework for faster product development with improved software quality, but successful implementation and benefits often hinge on project type, schedule, software release cycle and corporate culture. It may also be of interest that some of the largest software vendors such as Microsoft and IBM do not extensively use RAD in the development of their flagship products and for the most part, they still primarily rely on traditional waterfall methodologies with some degree of spiraling.\nThis table contains a high-level summary of some of the major types of RAD and their relative strengths and weaknesses.\n|Agile software development (Agile)|\n|Pros||Minimizes feature creep by developing in short intervals resulting in miniature software projects and releasing the product in mini-increments.|\n|Cons||Short iteration may add too little functionality, leading to significant delays in final iterations. Since Agile emphasizes real-time communication (preferably face-to-face), using it is problematic for large multi-team distributed system development. Agile methods produce very little written documentation and require a significant amount of post-project documentation.|\n|Extreme Programming (XP)|\n|Pros||Lowers the cost of changes through quick spirals of new requirements. Most design activity occurs incrementally and on the fly.|\n|Cons||Programmers must work in pairs, which is difficult for some people. No up-front “detailed design” occurs, which can result in more redesign effort in the long term. The business champion attached to the project full time can potentially become a single point of failure for the project and a major source of stress for a team.|\n|Joint application design (JAD)|\n|Pros||Captures the voice of the customer by involving them in the design and development of the application through a series of collaborative workshops called JAD sessions.|\n|Cons||The client may create an unrealistic product vision and request extensive gold-plating, leading a team to over- or under-develop functionality.|\n|Lean software development (LD)|\nCreates minimalist solutions (i.e., needs determine technology) and delivers less functionality earlier; per the policy that 80% today is better than 100% tomorrow.\n|Cons||Product may lose its competitive edge because of insufficient core functionality and may exhibit poor overall quality.|\n|Rapid application development (RAD)|\n|Pros||Promotes strong collaborative atmosphere and dynamic gathering of requirements. Business owner actively participates in prototyping, writing test cases and performing unit testing.|\n|Cons||Dependence on strong cohesive teams and individual commitment to the project. Decision making relies on the feature functionality team and a communal decision-making process with lesser degree of centralized PM and engineering authority.|\n|Pros||Improved productivity in teams previously paralyzed by heavy “process”, ability to prioritize work, use of backlog for completing items in a series of short iterations or sprints, daily measured progress and communications.|\n|Cons||Reliance on facilitation by a master who may lack the political skills to remove impediments and deliver the sprint goal. Due to relying on self-organizing teams and rejecting traditional centralized \"process control\", internal power struggles can paralyze a team.|\nSince rapid application development is an iterative and incremental process, it can lead to a succession of prototypes that never culminate in a satisfactory production application. Such failures may be avoided if the application development tools are robust, flexible, and put to proper use. This is addressed in methods such as the 2080 Development method or other post-agile variants.\nWhen organizations adopt rapid development methodologies, care must be taken to avoid role and responsibility confusion and communication breakdown within a development team, and between team and client. In addition, especially in cases where the client is absent or not able to participate with authority in the development process, the system analyst should be endowed with this authority on behalf of the client to ensure appropriate prioritisation of non-functional requirements. Furthermore, no increment of the system should be developed without a thorough and formally documented design phase.\n- Whitten, Jeffrey L.; Lonnie D. Bentley, Kevin C. Dittman. (2004). Systems Analysis and Design Methods. 6th edition. ISBN 025619906X.\n- Maurer and S. Martel. (2002). \"Extreme Programming: Rapid Development for Web-Based Applications\". IEEE Internet Computing, 6(1) pp 86-91 January/February 2002.\n- Andrew Begel, Nachiappan Nagappan. \"Usage and Perceptions of Agile Software Development in an Industrial Context: An Exploratory Study, Microsoft Research\". http://research.microsoft.com/pubs/56015/AgileDevatMS-ESEM07.pdf. Retrieved 2008-11-15.\n- E. M. Maximilien and L. Williams. (2003). \"Assessing Test-driven Development at IBM\". Proceedings of International Conference of Software Engineering, Portland, OR, pp. 564-569, 2003.\n- M. Stephens, Rosenberg, D. (2003). \"Extreme Programming Refactored: The Case Against XP\". Apress, 2003.\n- Gerber, Aurona; Van der Merwe, Alta; Alberts, Ronell; (2007), Implications of Rapid Development Methodologies, CSITEd 2007, Mauritius, November 2007 \n- Steve McConnell (1996). Rapid Development: Taming Wild Software Schedules, Microsoft Press Books, ISBN 978-1556159008\n- Kerr, James M.; Hunter, Richard (1993). Inside RAD: How to Build a Fully-Functional System in 90 Days or Less. McGraw-Hill. ISBN 0070342237.\n- Ellen Gottesdiener (1995). \"RAD Realities: Beyond the Hype to How RAD Really Works\" Application Development Trends\n- Ken Schwaber (1996). Agile Project Management with Scrum, Microsoft Press Books, ISBN 978-0735619937\n- Steve McConnell (2003). Professional Software Development: Shorter Schedules, Higher Quality Products, More Successful Projects, Enhanced Careers, Microsoft Prese s Books, ISBN 978-0321193674\n- Dean Leffingwell (2007). Scaling Software Agility: Best Practices for Large Enterprises, Addison-Wesley Professional, ISBN 978-0321458193\n- Download Free Staff Scheduling App For Trial Version EworksManager","What are the Key Steps in the Software Development Lifecycle?\nSoftware development is a complex process that involves a series of well-defined steps to create high-quality software applications. The Software Development Lifecycle (SDLC) encompasses these steps and provides a structured approach to ensure the success of software development projects. In this article, we will explore the key steps in the SDLC, the role of coding languages, the importance of choosing the right software development company, and more.\nThe field of software development has seen tremendous growth in recent years. As technology continues to evolve, businesses rely on software applications to streamline their operations, enhance productivity, and deliver value to their customers. However, developing software is not a straightforward task. It requires careful planning, collaboration, and adherence to established methodologies. This is where the Software Development Lifecycle (SDLC) comes into play.\nUnderstanding the Software Development Lifecycle (SDLC)\nDefinition of SDLC\nThe Software Development Lifecycle (SDLC) is a structured approach to software development that defines the processes and activities involved in creating, deploying, and maintaining software applications. It provides a framework for development teams to work systematically, ensuring that the end product meets the desired requirements and quality standards.\nImportance of SDLC\nThe SDLC is crucial for successful software development projects. It helps in reducing risks, improving efficiency, and enhancing the overall quality of the software. By following a well-defined process, development teams can identify and address issues at each stage, leading to more reliable and robust software applications.\nPhases of the Software Development Lifecycle\nThe SDLC consists of several distinct phases, each serving a specific purpose in the software development process. While the exact number and naming of these phases may vary depending on the methodology used, the core phases remain consistent across different approaches. Let's explore each phase in detail:\nThe first phase of the SDLC is requirements gathering. During this phase, the development team interacts with stakeholders to understand their needs and expectations from the software. The primary goal is to gather detailed and comprehensive requirements that will serve as the foundation for the development process.\nGathering user requirements\nThe development team engages with users, clients, and other stakeholders to identify their needs, challenges, and desired functionalities. This involves conducting interviews, workshops, and surveys to gather relevant information.\nAnalyzing and documenting requirements\nOnce the requirements are gathered, they are analyzed to ensure clarity, completeness, and feasibility. The team then documents these requirements in a detailed specification document, which serves as a reference for the subsequent phases.\nThe system design phase involves transforming the gathered requirements into a detailed architectural blueprint. This phase defines how the software will be structured, how different components will interact with each other, and how data will be stored and accessed.\nThe development team designs the overall architecture of the software, including the choice of technologies, frameworks, and platforms. This ensures that the software is scalable, maintainable, and meets the desired performance requirements.\nDuring this stage, the team designs the database structure that will store and organize the application's data. They determine the relationships between different data entities and create an efficient and secure database schema.\nUser interface design\nThe user interface design focuses on creating an intuitive and user-friendly interface for the software. The team considers factors such as usability, accessibility, and visual aesthetics to design a compelling user experience.\nCoding and Implementation\nThe coding and implementation phase involves translating the system design into actual code. This is where the development team brings the software to life by writing program code using selected coding languages.\nSelecting coding languages\nThe development team writes the code according to the requirements and design specifications. They follow coding standards and best practices to ensure code quality, readability, and maintainability. Additionally, they leverage software development tools and frameworks to streamline the coding process.\nConducting code reviews\nCode reviews play a crucial role in ensuring code quality and identifying potential issues early on. The team conducts thorough code reviews, where other team members review and provide feedback on the code. This helps in catching bugs, improving code efficiency, and maintaining coding standards.\nTesting and Quality Assurance\nThe testing and quality assurance phase is vital to validate the software's functionality, performance, and reliability. This phase involves various testing techniques and processes to identify and resolve any defects or issues before the software is deployed.\nTypes of testing\nDifferent types of testing are performed during this phase, including unit testing, integration testing, system testing, and user acceptance testing. Each type focuses on specific aspects of the software and helps ensure that it performs as expected.\nTest plan creation\nA comprehensive test plan is created to outline the testing activities, test cases, and test scenarios. This plan serves as a guide for the testing team and ensures that all the necessary aspects of the software are thoroughly tested.\nBug tracking and fixing\nDuring testing, defects or bugs may be identified. These issues are logged, tracked, and assigned to the development team for resolution. The team fixes the bugs and performs regression testing to ensure that the fixes do not introduce new issues.\nThe deployment phase involves preparing the software for release and making it available to users or clients. This includes installation, configuration, and training to ensure a smooth transition from development to production.\nInstallation and configuration\nThe software is installed on the target systems, and any necessary configurations are performed. This may involve setting up databases, configuring servers, and integrating with other software or systems.\nIf required, the software development company team provides training sessions to users or clients to familiarize them with the software's features and functionalities. This helps ensure a successful adoption of the software and maximizes its benefits."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:58820d87-315e-4a6a-ad57-be7345eb240e>","<urn:uuid:750f3581-a24b-4579-80ed-26f420b45361>"],"error":null}
{"question":"What environmental conditions make crops most vulnerable to both Nysius seed bugs and winter grain mites?","answer":"Crops become particularly vulnerable to both pests under specific environmental conditions. For Nysius seed bugs, problems are most serious in years with wet and cool springs, and they thrive in grassy or weedy fields. Winter grain mites are most damaging in fields with loose, sandy or loamy soils, rather than hard clay soils, and their activity increases with adequate moisture from rainfall. Both pests are more problematic in continuous cropping situations - Nysius bugs persist in areas with reservoir host weeds, while winter grain mite problems are worse in continuous wheat fields compared to rotated crops.","context":["California Pest Rating for\nNysius spp. : (Seed Bugs)\nPest Rating: NR\nPEST RATING PROFILE\nNysius spp. (Seed Bugs) are frequently intercepted by CDFA’s high risk programs and at border stations. These have a temporary rating of “Q”. A pest rating proposal is required to evaluate their pest risk.\nHistory & Status:\nBackground: The insects of the Nysius spp. commonly known as seed bugs or ground bugs and these are found on every continent except Antarctica2. The Lygaeidae is a very successful family of true bugs found worldwide. Several species of this family are well-known as major economic pests of a variety of crops. Some members of genus Nysius spp. are very useful for insect studies especially, insect physiology and evolutionary ecology.2\nThe insects of Nysius spp. are small insect commonly found within grassy or weedy fields, pastures, and foothills. Each spring, once the plants in these areas dry up, these insects migrates to find new places to feed. This becomes a nuisance for homeowners when these bugs migrate into their landscapes and homes and can cause problems for gardeners and farmers. The problems are most serious in the year with wet and cool springs.1\nNysius species are polyphagous insects that feed on a large number of crops, fruits & weeds5. The members of this genus have been associated with both endemic and introduced plant species from sea level to over 13,000 feet3. Crops attacked by these insects include: cabbage, rape, turnip, clover, lucerne, cucumber, carrots, potato, beets, cotton, sorghum, tomatoes and all types of squash, barley, wheat and many more crops. Many fruits plants were attacked by the insects of this genus, especially soft skin fruit like strawberries, kiwifruit and apple are seriously injured. Several weeds are reservoir hosts of these bugs, particularly those belonging to the Amaranthaceae, Chenopodiaceae, Asteraceae, Euphorbiaceae, and Portulacaceae families.4, 5\nWorldwide Distribution: The insects of Nysius spp. are considered among the most successful insects on earth; they are found on every continent except Antarctica.2\nOfficial Control: Nysius spp. are listed as harmful organisms by New Zealand, Japan, the Republic of Korea, Canada and Taiwan.8\nCalifornia Distribution: Nysius spp. are distributed all over California, but there are no official surveys done for these insects to confirm their presence. There are 106 described species in the genus Nysius and many of these have never been found in the environment of California.\nCalifornia Interceptions: Nysius spp. have been intercepted multiple times through border station inspections, dog teams and high risk pest exclusion activities. Between January 2000 and December 2016, they have been intercepted 990 times.6 Many of these specimen were submitted by homeowners from all over the state.\nThe risk Nysius spp. (Seed bugs) would pose to California is evaluated below.\nConsequences of Introduction:\n1) Climate/Host Interaction: Hosts plants of Nysius spp. are commonly grown in California and these species are expected to be established wherever the hosts are grown. It receives a High (3) in this category.\nEvaluate if the pest would have suitable hosts and climate to establish in California:\n– Low (1) Not likely to establish in California; or likely to establish in very limited areas.\n– Medium (2) may be able to establish in a larger but limited part of California.\n– High (3) likely to establish a widespread distribution in California.\n2) Known Pest Host Range: The insects of Nysius are highly polyphagous that can feed on variety of field crop and wild plants. It receives a High (3) in this category.\nEvaluate the host range of the pest.\n– Low (1) has a very limited host range.\n– Medium (2) has a moderate host range.\n– High (3) has a wide host range.\n3) Pest Dispersal Potential: The female of Nysius spp. generally lay eggs in clutches, which can range in size from 10 to over 100 eggs and may lay many clutches in their lifetime.2 The adults travel short distance in search of food and overwintering sites. They may move longer distances as result of hitchhiking on infested planting material or field equipment. It receives a High (3) in this category.\nEvaluate the natural and artificial dispersal potential of the pest.\n– Low (1) does not have high reproductive or dispersal potential.\n– Medium (2) has either high reproductive or dispersal potential.\n– High (3) has both high reproduction and dispersal potential.\n4) Economic Impact: Some species of Nysius considered pests of seedlings and in severe infestations they can damage young almond, pistachio, pomegranate, and citrus trees. Nysius huttoni feed on wheat grain in the milk-ripe stage with sucking mouthparts, which pierce through the glumes into the developing grain. It inject saliva that contains an enzyme, which bring changes in the flour protein makes it runny dough unsuitable for baking.4, 5 Most of the species are viewed as agricultural pests. It might reduce the crop yield and increase crop production costs for farmers. It is not expected to change cultural practice vector other organisms, injure animals, or disrupt water supplies. Depending on the species they could receive a Low (-1) to Medium (2) in this category.\nEvaluate the economic impact of the pest to California using the criteria below.\nEconomic Impact: A, B\nA. The pest could lower crop yield.\nB. The pest could lower crop value (includes increasing crop production costs).\nC. The pest could trigger the loss of markets (includes quarantines).\nD. The pest could negatively change normal cultural practices.\nE. The pest can vector, or is vectored, by another pestiferous organism.\nF. The organism is injurious or poisonous to agriculturally important animals.\nG. The organism can interfere with the delivery or supply of water for agricultural uses.\nEconomic Impact Score: 1-2\n– Low (1) causes 0 or 1 of these impacts.\n– Medium (2) causes 2 of these impacts.\n– High (3) causes 3 or more of these impacts.\n5) Environmental Impact: The insects of Nysius spp. are not expected to lower biodiversity, disrupt natural communities, or change ecosystem processes. It may effect sensitive species of Brassicaceae such as Caperfruit tropidocarpum (Tropidocarpum capparideum), Santa Cruz Wallflower (Erysimum teretifolium), Tiburon jewel flower (Streptanthus niger) and Metcalf canyon jewel flower (Streptanthus albidus ssp. Albidus). However, no significant documented impact occurs from native species on sensitive species. It would not be expected to disrupt critical habitats. If pest species were established then would it very likely trigger new treatment programs by farmers and residents who find infested plants unsightly. Depending on species it would a receive Low (-1) to High (3) in this category.\nEvaluate the environmental impact of the pest on California using the criteria below.\nEnvironmental Impact: B, D\nA. The pest could have a significant environmental impact such as lowering biodiversity, disrupting natural communities, or changing ecosystem processes.\nB. The pest could directly affect threatened or endangered species.\nC. The pest could impact threatened or endangered species by disrupting critical habitats.\nD. The pest could trigger additional official or private treatment programs.\nE. The pest significantly impacts cultural practices, home/urban gardening or ornamental plantings.\nEnvironmental Impact: Score: 1-3\n– Low (1) causes none of the above to occur.\n– Medium (2) causes one of the above to occur.\n– High (3) causes two or more of the above to occur.\nConsequences of Introduction to California for Nysius spp. (Seed bugs): Low -High (11-14)\n–Low = 5-8 points\n–Medium = 9-12 points\n–High = 13-15 points\n6) Post Entry Distribution and Survey Information: There are 106 described species of Nysius and many of them are established in California. They receive a High (-3) in this category. Nysius not established in California receive a Not established (0) in this category.\nThis genus receives a Not Established (0) to High Established (-3) in this category.\nEvaluate the known distribution in California. Only official records identified by a taxonomic expert and supported by voucher specimens deposited in natural history collections should be considered. Pest incursions that have been eradicated, are under eradication, or have been delimited with no further detections should not be included.\n–Not established (0) Pest never detected in California, or known only from incursions.\n–Low (-1) Pest has a localized distribution in California, or is established in one suitable climate/host area (region).\n–Medium (-2) Pest is widespread in California but not fully established in the endangered area, or pest established in two contiguous suitable climate/host areas.\n–High (-3) Pest has fully established in the endangered area, or pest is reported in more than two contiguous or non-contiguous suitable climate/host areas.\nThe final score is the consequences of introduction score minus the post entry distribution and survey information score: Low (8) to High (14).\nUncertainty is high as the species are hard to identify and they vary substantially in their current status and risk to CA.\nConclusion and Rating Justification:\nThere are many species of Nysius that have not been found in the environment of California. New species could have significant economic and environmental impacts. Examples of species of Nysius that are not found in California and would be likely to have significant impacts here include Nysius nemorivagus from Hawaii3 and Nysius vinitor from Australia.7 While on the other hand there are lot more species which are commonly found in California and are not expected to have significant economic and environmental impacts to California. Example of these species are Nysius raphanus Howard & Nysius tenellus Barber.\nIt is not possible combine pest rating all 106 species in one proposal. Pest ratings can be lawfully proposed for each individual species versus the whole genus. Non-native Nysius species can have significant impact on California agriculture whereas native species are already present in the state and are being monitored for spread and growth. Considering these facts, a “NR” rating is justified this genus.\n- R. Haviland, W. J. Bentley, 2016 UC IPM. Accessed on 2-08-17. http://ipm.ucanr.edu/PMG/PESTNOTES/pn74153.html\n- R. Burdfield – Steel, David M Shuker. 2014. The evolutionary ecology of the Lygaeidae. On line NCBI. Accessed on 2-08-17. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4201440/\n- Jayma L. M. Kessing, Ronal F.L. Mau. 1993. Crop knowledge master Hawaii. Accessed on 2-08-17. http://www.extento.hawaii.edu/kbase/crop/type/nysius.htm\n- Brambila. 2007. USDA- APHIS – PPQ Invasive Arthropod workshop. Accessed on Feb, 6 2017. https://www.freshfromflorida.com/content/download/9865/135458/pdf_brambila_heteroptera_spdn2007-small.pdf\n- Baker, R. Cannon. 2006. CSL pest risk analysis for Nysius huttoni Accessed on 2-08-17. https://secure.fera.defra.gov.uk/phiw/riskRegister/downloadExternalPra.cfm?id=3865\n- Pest and Damage Record Database, California Department of Food and Agriculture, Plant Health and Pest Prevention Services. http://phpps.cdfa.ca.gov/user/frmLogon2.asp\n- Pest Information wiki. Online Accessed on 1-31-17. http://wiki.pestinfo.org/wiki/Nysius_vinitor\n- USDA Phytosanitary Certificate Issuance & Tracking System (PCIT) Phytosanitary Export Database (PExD). Accessed on 1-31-17. https://pcit.aphis.usda.gov/pcit/\nJavaid Iqbal, California Department of Food and Agriculture; 1220 N Street, Sacramento, CA 95814; Tel. (916) 403-6695; plant.health[@]cdfa.ca.gov.\nComment Period: CLOSED\n45-day comment period: Mar 14, 2017 – April 28, 2017\n♦ Comments should refer to the appropriate California Pest Rating Proposal Form subsection(s) being commented on, as shown below.\nConsequences of Introduction: 1. Climate/Host Interaction: [Your comment that relates to “Climate/Host Interaction” here.]\n♦ Posted comments will not be able to be viewed immediately.\n♦ Comments may not be posted if they:\nContain inappropriate language which is not germane to the pest rating proposal;\nContains defamatory, false, inaccurate, abusive, obscene, pornographic, sexually oriented, threatening, racially offensive, discriminatory or illegal material;\nViolates agency regulations prohibiting sexual harassment or other forms of discrimination;\nViolates agency regulations prohibiting workplace violence, including threats.\n♦ Comments may be edited prior to posting to ensure they are entirely germane.\n♦ Posted comments shall be those which have been approved in content and posted to the website to be viewed, not just submitted.\nPest Rating: NR\nPosted by ls","Winter Grain Mite, Penthaleus major.\nWinter Grain Mite - © Photo by the Kansas Department of Agriculture\nCloseup showing anal pore.\nWheat plants stunted by winter grain mite feeding.\nField of wheat with severe winter grain mite damage.\nThe winter grain mite is known to be a pest of small grains and grasses throughout the temperate regions of the world. Heavily infested fields take on a grayish or silvery cast as a result of the puncturing of plant cells as the mites feed. Many times the infested plants do not die, but become stunted and produce little forage or grain. Damage on young plants is more severe than on large, healthy plants.\nWinter grain mites have a dark brown to almost black body with conspicuous reddish-orange legs. Their front legs are longer than the others, but not as pronounced as on the brown wheat mite. They also have two tarsal claws on the end of each leg. They are rather unusual in that they are the only mites of economic importance in this area with the anal pore on the upper surface of the abdomen. Most other mites have the anal opening on the underside or at the tip of the abdomen. This characteristic is best seen using a microscope, but can be observed with a hand lens. The anal pore appears as a tan to orange spot on top of the body, but disappears quickly in alcohol.\nThese mites generally have two generations per year. The first begins in September or October as weather conditions become favorable for the over-summering eggs to hatch. Populations reach a peak in December or January. The second generation develops from eggs laid by the first generation and reaches its maximum density in March or April. Populations then decrease as temperatures exceed the mite's range of tolerance. Females of the second generation lay aestivating or over-summering eggs. Larvae become active soon after hatching and begin to feed on the leaf sheaths and tender shoots near the ground. The nymphs, as well as the adults, feed higher up on the plants at night and on cloudy or cool days.\nAs the sun rises, the mites descend the plants and seek protection on the moist soil surface under the foliage during the hot part of the day. Mites damage plants by puncturing individual cells, which cause the leaves to take on a silvery-gray appearance. Heavily damaged leaves will have brown leaf tips. If the soil is dry and there is little foliage cover, mites will crawl into the soil in search of moisture and cooler temperatures. Temperature and moisture are the most important factors influencing mite development and abundance. Cool, rather than warm temperatures favor their development. Activity of these mites is the greatest between 40º and 70ºF. Aestivating eggs do not hatch in the fall until rains provide adequate moisture. On hot, dry days it may be necessary to dig into the soil to a depth of 4 or 5 inches to find the mites. The mites do not seem to be harmed by high humidity, rainfall, short periods of sleet or ice cover, or by ground frozen to a depth of several inches, in the fall. However, heavy spring rains may cause mite populations to disappear.\nFields with loose, sandy or loamy soils are more at risk than those with hard, clay soils. Significant infestations are more common in central Kansas. Because fall populations develop from eggs laid the previous spring, problems are worse in continuous wheat. Crop rotation is helpful in reducing problems, although field borders may be affected when mites migrate from wild grasses. Control may be necessary if large portions of a field show symptoms and mites appear abundant in relation to the amount of plant growth. Persistent dry conditions can lead to cumulative damage and plant recovery is often dependent on available moisture.\nJ.P. Michaud and P.E. Sloderbeck, July 2007.\nPlease refer to the most recent version of the Wheat Insect Management Guide for control options.\nPage last updated 10/31/2013 by J.P. Michaud."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:9a421f2f-8f11-4da7-b37a-855c97005b1b>","<urn:uuid:69940426-7afd-45b3-af6c-bbd507501074>"],"error":null}
{"question":"How do government policies address disability inclusion in society, and what treatment approaches are recommended for managing depression in disabled individuals?","answer":"Government policies aim to create an inclusive society through measures like the Disability Discrimination Act 1995, which makes discrimination against disabled people unlawful and requires service providers to remove physical barriers to access. The goal is to enable people with disabilities to achieve the highest possible living standard and full social, economic, and physical inclusion. Regarding depression treatment, multiple approaches are recommended, including antidepressant medications that alter brain neurochemicals, and various psychotherapies. These include interpersonal therapy to improve personal relationships, psychodynamic therapy to address past experiences, and cognitive-behavioral therapy to assess and change negative thoughts. The most effective treatment typically combines both medication and psychotherapy, with services tailored to individual patient needs.","context":["DCAN 11 (Draft): Access for all - Designing for an Accessible Environment\n1.1 Our built environment has not been designed with the needs of people with disabilities in mind, yet it is estimated that about 20% of the population has some form of permanent or temporary disability (Equality Commission for Northern Ireland ). This represents some 340,000 people.\nDisability covers more than the obvious conditions, such as blindness or use of a wheelchair. Breathlessness, pain, the need to walk with a stick, difficulty in gripping because of paralysis or arthritis, lack of physical co-ordination, partial sight, deafness and even pregnancy can all affect a person’s mobility in the environment.\n1.2 The physical barriers often presented by the built environment can severely restrict the mobility of many people. They have also made it difficult, if not impossible for people with disabilities to gain access to jobs, services, housing and entertainment.\n1.3 The Programme for Government wants to enable people with disabilities in Northern Ireland “to achieve the highest possible standard of living and to be fully integrated within our society.”\n1.4 This complements a central objective of Government policy, namely the creation of an inclusive society, and this means social, economic and physical inclusion. The ultimate goal is a modern, cohesive society in which all members of the community are included in everything that they do – at work, at home and enjoying themselves.\n1.5 The needs of disabled people are explicitly recognised in the Disability Discrimination Act 1995 (DDA). This legislation has introduced measures to make it unlawful to discriminate against disabled people and from October 2004 service providers such as shops, restaurants, banks, local authorities and government departments, will be obliged to take reasonable steps to remove or overcome physical barriers to access to buildings where services are provided.\n1.6 The new rights introduced by the DDA and the changing state of our built environment - whether by new development, redevelopment of outdated buildings or the upgrading and refurbishment of historic buildings - provide an opportunity to secure a more accessible environment for everyone. It is part of good design and is beneficial to business.\n1.7 This Advice Note has been drawn up following discussions with Disability Action, the Equality Commission and the Chief Building Control Officers Group. It seeks to increase awareness among building owners and managers, architects, surveyors and developers about their statutory and legal obligations regarding the needs of people with disabilities. Information about undertaking an access audit is included together with advice regarding improved access to historic buildings and sites.\n1.8 The Advice Note also provides supplementary design guidance for developers and their agents aimed at creating a more accessible environment for all in support of the Department’s planning policies.\n1.9 The guidance is clearly focused on people with disabilities, but creating a more accessible and hospitable environment will also benefit others, such as the elderly, carers, parents with small children, people with temporary mobility problems (e.g. a leg in plaster) or simply those carrying luggage or heavy shopping.\n1.10 The process of achieving a more accessible environment is relevant to all parties involved in the design, procurement, construction and occupation or investment in buildings, as well as for those granting or refusing statutory consents. The Department would therefore encourage all those involved in designing for access to consult widely with disabled users, access consultants and disability access groups who can provide useful advice on how services and access arrangements can be best provided for all employees and visitors.\n1.11 Put simply the objective of this Advice Note is to help promote a more inclusive environment with access for all.","Depressive Disorders among the Disabled\nDepressive Disorders among the Disabled\nMany disabled individuals experience some level of depression that need addressing to improve their living standards. Disability in the context can refer to a mental or physical condition that limits a person’s senses, movements and abilities to carry out activities. Depression among this group of people originates from several factors relating to their conditions and problems people face in societies. Individuals can develop different types of depressive disorders that are evident in the duration of types of symptoms. Depressive disorders affect the lives of disabled individuals and can cause adverse impacts on health. Prevention of adverse outcomes requires the treatment of depressive disorders using different approaches depending on the unique needs of each patient.\nIntervention to improve the mental health of disabled patients should begin with evaluation into the causes of depression. Çağan and Ünsal (2014) showed that social isolation and loneliness are some of the leading causes of depression among the population. Disabled individuals face the problems of the inability to move to some areas or carry out some activities that leave them out of the team. Besides, disability makes some of the affected individuals develop the feeling of sadness that leads to depression. Their inability to change their status and how other people view them contribute to the development of depressive disorders. In addition to these factors, other aspects such as loss of loved ones, economic problems, and abuse lead to the development of depressive disorders.\nThe presence of more than one of these factors increases the risk of depression among patients with disabilities. In most cases, disabled individuals face more than an issue that contributes to the development of depressive disorders. Affected individuals develop different types of depressive disorders that need addressing using evidence-based approaches. Analyses highlight different types of depressions affecting the mental health of disabled individuals. The duration of symptoms presented by the patient provides one of the ways that professionals use in classifying depressions.\nTypes of Depressive Disorders\nDisabled individuals are at increased risk of suffering from major depressive disorders due to their condition. Major depressive disorder refers to conditions where the individual present the symptoms for two weeks or extended periods. Information used in health facilities shows that patients should show symptoms such as fatigue, hopelessness, suicidal thoughts and loss of pleasure. Patients may even show remissions of major depressive disorder that significantly affect the health of disabled people (Culpepper, Muskin & Stahl, 2015). Thus, evaluation of the mental health of disabled individuals should consider the possibility of major depressive disorder.\nDisabled individuals may also develop bipolar disorder. Individuals having the condition show extreme fluctuations of moods, behavior, sleep and energy. The depression due to bipolar disorder may lead to suicidal thoughts that can further involve additional changes. Disabled individuals having manic depression show mood swings that can take place more frequently, such as every week or in sporadic nature, such as twice annually. Health professionals should evaluate the incidence of changes in moods of affected people to determine the presence of manic depressions.\nThe impact on the mental health of disabled individuals may also involve the development of dysthymia. The condition refers to a persistent depressive disorder that can last for many years. People who develop dysthymia can show symptoms that affect their relationships, daily activities and work. A constant feeling of sadness among people having dysthymia finds it hard to be happy on any occasion. Affected individuals may show signs of complaining most of the time that affect their relationships with other people in society.\nAssessment aiming to improve the mental health of patients should analyze the type of depressive disorder. The information gained from the client help in the provision of best care using a patient-centered approach. However, there is a need to understand the health impact of depression to facilitate interventions.\nImpact of Depression on Health\nOne of the most noticeable health concerns due to depression is cases of suicides. Disabled individuals who have depression have suicidal thoughts as one of the symptoms. Suicidal thoughts originate from an overbearing sense of sadness with affected individuals seeing no way out. Therefore, such situations present death as one of the options to escape from existing problems. Disabled individuals face many challenges that can prompt the feeling of hopelessness that make some individuals think suicide is the only way out of problems.\nDepression among disabled individuals shows significant impacts on their appetite. Change in appetite is one of the common signs of depression. Some individuals who have depression experience loss of appetite while others increase the amount of food they eat. In any case, changes in appetite result in health problems. Individuals may adopt the habit of taking less food that limits their ability to take nutrients that meet physiological requirements. Cases of nutritional deficiencies can lead to significant health problems depending on the deficient nutrient. Malnutrition also affects the immune system that can lead to infections affecting the health of the patient. Therefore, depressed individuals are at increased risk of malnutrition health problems.\nDepression can cause overeating that results in other sets of health problems. Overeating due to depression increases calories provided to the body. Constant accumulation of calories contributes to weight gain resulting in health problems such as diabetes, obesity and hypertension. The development of these health problems affects disabled and depressed individuals that can lead to fatalities. The gain of weight due to overeating can further lead to low self-esteem that further affects the mental health of the patient.\nThe presence of depression among disabled individuals also increases the risk of heart conditions. Depression causes low energy among disabled people that is evident in their unwillingness to carry out any physical activity. The person develops a sedentary lifestyle that increases the risk of heart problems. Studies also indicate the association between heart disease and depression among affected individuals. Studies show additional factors that increase the risk of cardiovascular diseases due to depression other than sedentary lifestyles. A study by Dhar and Barton published in 2016, showing an association between major depressive disorder (MDD) and coronary heart disease (CHD), highlighted that neurochemicals released due to MDD increase the risk of CHD. Thus, despite the lack of changes in the lifestyles of individuals, depression can still lead to heart problems.\nThe presence of these health problems shows the need for interventions aiming to improve the mental and physical health of patients having depression. In cases of physical health, health professionals should consider the need of every patient. At the same time, the assessment of the patient should involve evaluation of the cause of depression to provide patient-centered services. Therefore, support for disabled people should focus on specific aspects affecting each patient to improve the outcome. Health professionals should use various intervention approaches to improve the health of disabled people having depression.\nInterventions to Manage Depression among Disabled\nTreatment of depression among depressed individuals can involve the use of many approaches depending on the needs of clients. The prescription of medications to manage the mental condition of patients is one of the methods. Professionals can also opt for therapies that focus on the cognition and behavior of the patient to improve mental health. A combination of more than one method is also useful in some cases to further improve outcomes.\nTreatment of depression among affected individuals using medications primarily relies on antidepressant medications. The use of antidepressants aims to manage the symptoms presented by patients. Such drugs achieve the objective by changing neurochemicals in the brain to manage the feelings of individuals. Impacts of the medications restore chemical balance in the brain to relieve depression symptoms. Patients have a range of antidepressants to choose from while aiming to manage their health conditions. The choice of medication depends on symptoms, suspected side effects, possible drug interactions and costs for some patients. Thus, health professionals must consider symptoms presented and patients’ medical history of prescribing appropriate medication. However, the drugs do not provide immediate relief to patients at the beginning of using medication. Patients taking medications require about two to four weeks to start realizing the effects.\nDespite the effectiveness of medication in the ability to relieve symptoms, it is not the preferable approach to treat depression. Impacts of medications only last while patients use medications. Symptoms of depression may be present after patients stop using antidepressants. Medications are ineffective due to their inabilities to address the underlying problems leading to depression. Therefore, professionals prescribe medications together with other treatment methods to improve the outcomes.\nPsychotherapies show high efficacy in treating patients with depressive disorders. They involve evaluating causes of depression as part of addressing the problem. In some cases, professionals may use more than one method to enhance the outcomes. Therapists that use psychotherapies should develop strong relationships with patients to improve outcomes.\nInterpersonal therapy can be crucial to disabled individuals by improving personal relationships with other people (Van Hees, Rotter, Ellermann & Evers, 2013). Disabled people face isolation as one of the factors causing depression. Improving relationships helps in keep people around through interactions to eliminate loneliness and social isolation.\nPsychodynamic therapy focuses on the experience of disabled people to understand the source of depression. Most cases of depression arise from experiences of individuals that affect interactions and lives. Some of the disabled individuals may even show the impacts of experiences unconsciously leading to depression. The use of the approach focuses on such incidents to improve the recovery of patients.\nCognitive-behavioral therapy (CBT) is an effective method to treat depression (Vara et al., 2018). Psychotherapists applying the approach aim to assess negative thoughts associated with depression and make appropriate changes. Patients gain from the method by developing coping mechanisms to limit remissions.\nDespite the presence of different therapies for depression, effective treatments should involve a combination of different approaches. Professionals can prescribe medications for fast improvement of depression symptoms. However, interventions that ensure there are no remissions of depressive disorders require psychotherapies that focus on the cause of depression. Besides, professionals providing the services should evaluate the need of every client to provide patient-centered services during therapies.\nÇağan, Ö., & Ünsal, A. (2014). Depression and loneliness in disabled adults. Procedia–Social and Behavioral Sciences, 114, 754-760.\nCulpepper, L., Muskin, P. R., & Stahl, S. M. (2015). Major depressive disorder: understanding the significance of residual symptoms and balancing efficacy with tolerability. The American journal of medicine, 128(9), S1-S15.\nDhar, A. K., & Barton, D. A. (2016). Depression and the link with cardiovascular disease. Frontiers in psychiatry, 7, 33.\nVan Hees, M. L., Rotter, T., Ellermann, T., & Evers, S. M. (2013). The effectiveness of individual interpersonal psychotherapy as a treatment for major depressive disorder in adult outpatients: a systematic review. BMC psychiatry, 13(1), 22.\nVara, M. D., Herrero, R., Etchemendy, E., Espinoza, M., Baños, R. M., García-Palacios, A., ... & Franco-Martín, M. (2018). Efficacy and cost-effectiveness of a blended cognitive behavioral therapy for depression in Spanish primary health care: study protocol for a randomised non-inferiority trial. BMC psychiatry, 18(1), 74."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:0b9a72e4-a88c-4b12-a372-bc5b15bc259f>","<urn:uuid:1f09b8aa-934c-4808-9883-684ab1637673>"],"error":null}
{"question":"While researching cultural documentation methods, how do the Monroe Doctrine's policies on European intervention compare with Jenness's approach to studying indigenous peoples?","answer":"The Monroe Doctrine took a restrictive approach, prohibiting European powers from intervening in or colonizing the Western Hemisphere, while maintaining that the U.S. would not interfere in European affairs. In contrast, Jenness took an immersive, collaborative approach to studying indigenous peoples - he became fluent in Inuktitut, recorded hundreds of drum dance songs, poems, legends and stories, and carefully documented daily life of the Copper Inuit through direct observation and interaction. While Monroe's doctrine aimed to keep European influences out of the Americas, Jenness worked to preserve and understand indigenous cultures through direct engagement and detailed documentation.","context":["Monroe Doctrine, (December 2, 1823), cornerstone of U.S. foreign policy enunciated by Pres. James Monroe in his annual message to Congress. Declaring that the Old World and New World had different systems and must remain distinct spheres, Monroe made four basic points: (1) the United States would not interfere in the internal affairs of or the wars between European powers; (2) the United States recognized and would not interfere with existing colonies and dependencies in the Western Hemisphere; (3) the Western Hemisphere was closed to future colonization; and (4) any attempt by a European power to oppress or control any nation in the Western Hemisphere would be viewed as a hostile act against the United States.\nThe doctrine was an outgrowth of concern in both Britain and the United States that the continental powers would attempt to restore Spain’s former colonies, in Latin America, many of which had become newly independent nations. The United States was also concerned about Russia’s territorial ambitions in the northwest coast of North America. As a consequence, George Canning, the British foreign minister, suggested a joint U.S.-British declaration forbidding future colonization in Latin America. Monroe was initially favourable to the idea, and former presidents Thomas Jefferson and James Madison concurred. But Secretary of State John Quincy Adams argued that the United States should issue a statement of American policy exclusively, and his view ultimately prevailed.\nThe first draft of the message included a reproof of the French for their invasion of Spain, an acknowledgement of Greek independence in the revolt against Turkey, and some further indications of American concern in European affairs. Adams argued for the better part of two days against such expressions, which were finally eliminated from the message.\nAdams noted in his diary,\nThe ground that I wish to take is that of earnest remonstrance against the interference of the European powers by force in South America, but to disclaim all interference on our part with Europe; to make an American cause, and adhere inflexibly to that.\nThe Monroe Doctrine, in asserting unilateral U.S. protection over the entire Western Hemisphere, was a foreign policy that could not have been sustained militarily in 1823. Monroe and Adams were well aware of the need for the British fleet to deter potential aggressors in Latin America. Because the United States was not a major power at the time and because the continental powers apparently had no serious intentions of recolonizing Latin America, Monroe’s policy statement (it was not known as the “Monroe Doctrine” for nearly 30 years) was largely ignored outside the United States.\nThe United States did not invoke it nor oppose British occupation of the Falkland Islands in 1833 or subsequent British encroachments in Latin America. In 1845 and again in 1848, however, Pres. James K. Polk reiterated Monroe’s principles in warning Britain and Spain not to establish footholds in Oregon, California, or Mexico’s Yucatán Peninsula. At the conclusion of the American Civil War, the United States massed troops on the Rio Grande in support of a demand that France withdraw its puppet kingdom from Mexico. In 1867—partly because of U.S. pressure—France withdrew.\nAfter 1870 interpretation of the Monroe Doctrine became increasingly broad. As the United States emerged as a world power, the Monroe Doctrine came to define a recognized sphere of influence. Pres. Theodore Roosevelt added the Roosevelt Corollary to the Monroe Doctrine in 1904, which stated that, in cases of flagrant and chronic wrongdoing by a Latin American country, the United States could intervene in that country’s internal affairs. Roosevelt’s assertion of hemispheric police power was designed to preclude violation of the Monroe Doctrine by European countries seeking redress of grievances against unruly or mismanaged Latin American states.\nTest Your Knowledge\nLearning the Alphabet\nFrom the presidency of Theodore Roosevelt to that of Franklin Roosevelt, the United States frequently intervened in Latin America, especially in the Caribbean. Since the 1930s the United States has attempted to formulate its Latin American foreign policy in consultation with the individual nations of the hemisphere and with the Organization of American States. Yet the United States continues to exercise a proprietary role at times of apparent threat to its national security, and the Western Hemisphere remains a predominantly U.S. sphere of influence.\nCharles Evan Hughes’s article on the Monroe Doctrine appeared in the 14th edition of the Encyclopædia Britannica (see the Britannica Classic: Monroe Doctrine).","Information Archived on the Web\nInformation identified as archived on the Web is for reference, research or recordkeeping purposes. It has not been altered or updated after the date of archiving. Web pages that are archived on the Web are not subject to the Government of Canada Web Standards. As per the Communications Policy of the Government of Canada, you can request alternate formats. Please \"contact us\" to request a format other than those available.\nEthnologist, Anthropologist, Arctic Scholar\nGeological Survey of Canada (GSC), 1912–19\nDiamond Jenness is recognized as Canada's pre-eminent pioneer anthropologist. He documented Aboriginal life in Canada's North, the Inuit of the western Arctic, and First Nations communities across Canada. His contributions to the knowledge and understanding of Canada's native peoples earned him national and international distinction.\nDiverse education and experience\nDiamond was born in Wellington, New Zealand, on February 10, 1886. After graduating from Victoria College in Wellington in 1908, he studied at Oxford University, receiving a master's degree in anthropology in 1916.\nAs part of his studies at Oxford, he led an expedition to New Guinea to study the d'Entrecasteaux Islanders in 1911 and 1912. But one of his greatest experiences would begin on his arrival in Canada in 1913.\nSurviving the Canadian Arctic Expedition\nThe 1913–16 Canadian Arctic Expedition, directed by the GSC, was the first major scientific exploration of Canada's Arctic. Led by renowned Arctic explorer Vilhjalmur Stefansson, the expedition would explore the northern and western parts of the Arctic. Diamond was one of two ethnologists with the expedition.\nThe cruel Arctic weather intervened in the exploration's first year, and the main expedition ship, Karluk, became frozen in the ice. While Stefansson, Diamond and four others were ashore hunting caribou, ocean currents carried away the ship. Surviving expedition members were forced to abandon the Karluk when it was subsequently crushed by ice. Henri Beuchat, the other ethnologist, was one of the 13 men who perished in the tragedy.\nThe People of the Twilight\nWith Beuchat's death, Diamond performed the work of two men, spending the next several years studying the Copper Inuit of Coronation Gulf. He became fluent in Inuktitut and recorded hundreds of drum dance songs, poems, legends and stories on wax phonographic cylinders. He also carefully documented the Copper Inuit's daily life. Diamond's book, The People of the Twilight (1928), is still regarded as one of the best sources of information on the life of a nomadic, indigenous people.\nFinally hearing news of World War I, the GSC expedition survivors returned south in July 1916. Diamond served with the Canadian field artillery as a gunner, from 1917 to 1919. After the war, he returned to Ottawa to complete his Arctic reports and begin his career with the National Museum of Natural Sciences (now known as the Canadian Museum of Nature).\nFrom 1926 to his retirement in 1948, Diamond was Chief Anthropologist of the museum. He uncovered evidence of prehistoric Inuit cultures at excavations on Baffin Island and in Alaska, which he named the \"Dorset\" and \"Old Bering Sea\" cultures — fundamental discoveries in explaining migration patterns.\nDiamond spent his life studying and writing about Canada's Aboriginal peoples — producing more than 100 titles, including The Copper Eskimo (1923), The Indians of Canada (1932), The Corn Goddess and Other Tales from Indian Canada (1956) and Dawn in Arctic Alaska (1957). All are still recognized as influential works on Canada's native peoples.\nDuring his illustrious career, Diamond received six honorary doctoral degrees and numerous prestigious awards, including the Companion of the Order of Canada. He died near Ottawa on November 29, 1969.\nDiamond Jenness Peninsula on Victoria Island in the Northwest Territories is named for him.\n- 1911 — Graduated from Oxford University.\n- 1911–12 — Led anthropological expedition to New Guinea.\n- 1913–16 — Ethnologist on Canadian Arctic Expedition\n- 1926 — Chief Anthropologist at the National Museum of Natural Sciences\n- 1937 — President of the Society for American Archaeology\n- 1939 — President of the American Anthropological Association\n- 1940 — Deputy Director of Intelligence for Royal Canadian Air Force\n- 1962 — Awarded the Massey Medal from the Royal Canadian Geographical Society.\n- 1962–68 — Arctic Institute of North America issues his five volumes on Eskimo administration in Alaska, Canada and Greenland.\n- 1969 — Appointed Companion of the Order of Canada.\n- Date Modified:"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:f53243c0-a1be-445e-9f97-c390e842224d>","<urn:uuid:1435596d-6b13-45c8-ad9d-82f6fd9a3947>"],"error":null}
{"question":"What are the key considerations before choosing reinforcement learning for a project?","answer":"When considering reinforcement learning, several important factors need evaluation. First, reinforcement learning is not sample efficient and typically requires extensive training time - even simple applications can take hours or days. Second, neural network policies, while powerful, act as 'black boxes' that make it difficult to understand or guarantee specific performance outcomes. Additionally, setting up the problem requires many design decisions that may need multiple iterations to get right, including selecting neural network architecture, tuning hyperparameters, and shaping reward signals. For time-critical or safety-critical projects that could be solved with traditional methods, reinforcement learning might not be the best first choice. It's important to assess whether you have sufficient time and resources for the project before committing to this approach.","context":["Emmanouil Tzorakoleftherakis is the product manager for Reinforcement Learning and…\nIf you are following technology news, you have likely already read about how AI programs trained with reinforcement learning beat human players in board games like Go and chess, as well as video games. As an engineer, scientist, or researcher, you may want to take advantage of this new and growing technology, but where do you start? The best place to begin is to understand what the concept is, how to implement it, and whether it’s the right approach for a given problem.\nIf we simplify the concept, at its foundation, reinforcement learning is a type of machine learning that has the potential to solve tough decision-making problems. But to truly understand how it impacts us, we need to answer three key questions:\n- What is reinforcement learning and why should I consider it when solving my problem?\n- When is reinforcement learning the right approach?\n- What is the workflow I should follow to solve my reinforcement learning problem?\nWhat Is Reinforcement Learning?\nReinforcement learning is a type of machine learning in which a computer learns to perform a task through repeated trial-and-error interactions with a dynamic environment. This learning approach enables the computer to make a series of decisions that maximize a reward metric for the task without human intervention and without being explicitly programmed to achieve the task.\nTo better understand reinforcement learning, let’s look at a real-world equivalent situation. Figure 1 shows a general representation of training a dog using reinforcement learning.\nThe goal of reinforcement learning in this case is to train the dog (agent) to complete a task within an environment, which includes the surroundings of the dog as well as the trainer. First, the trainer issues a command or cue, which the dog observes (observation). The dog then responds by taking an action. If the action is close to the desired behavior, the trainer will likely provide a reward, such as a food treat or a toy; otherwise, no reward or a negative reward will be provided. At the beginning of training, the dog will likely take more random actions like rolling over when the command given is “sit,” as it is trying to associate specific observations with actions and rewards. This association, or mapping, between observations and actions is called policy. From the dog’s perspective, the ideal case would be one in which it would respond correctly to every cue, so that it gets as many treats as possible. So, the whole meaning of reinforcement learning training is to “tune” the dog’s policy so that it learns the desired behaviors that will maximize some reward. After training is complete, the dog should be able to observe the owner and take the appropriate action, for example, sitting when commanded to “sit” by using the internal policy it has developed. By this point, treats are welcome but shouldn’t be necessary (theoretically speaking!).\nBased on the dog training example, consider the task of parking a vehicle using an automated driving system (Figure 2). The goal of this task is for the vehicle computer (agent) to park the vehicle in the correct parking spot with the right orientation. Like the dog training case, the environment here is everything outside the agent and could include the dynamics of the vehicle, other vehicles that may be nearby, weather conditions, and so on. During training, the agent uses readings from sensors such as cameras, GPS, and lidar (observations) to generate steering, braking, and acceleration commands (actions). To learn how to generate the correct actions from the observations (policy tuning), the agent repeatedly tries to park the vehicle using a trial-and-error process. A reward signal can be provided to evaluate the goodness of a trial and to guide the learning process.\nIn the dog training example, training is happening inside the dog’s brain. In the autonomous parking example, training is supervised by a training algorithm. The training algorithm is responsible for tuning the agent’s policy based on the collected sensor readings, actions, and rewards. After training is complete, the vehicle’s computer should be able to park using only the tuned policy and sensor readings.\nWhen Is Reinforcement Learning the Right Approach?\nMany reinforcement learning training algorithms have been developed to date; this article does not cover training algorithms, but it is worth mentioning that some of the most popular ones rely on deep neural network policies. The biggest advantage of neural networks is that they can encode really complex behaviors, which opens up the use of reinforcement learning in applications that are otherwise intractable or very challenging to tackle with alternative methods, including traditional algorithms. For example, in autonomous driving, a neural network can replace the driver and decide how to turn the steering wheel by simultaneously looking at input from multiple sensors, such as camera frames and lidar measurements (end-to-end solution). Without neural networks, the problem would normally be broken down into smaller pieces: a module that analyzes the camera input to identify useful features, another module that filters the lidar measurements, possibly one component that would aim to paint the full picture of the vehicle’s surroundings by fusing the sensor outputs, a “driver” module, and more! However, the benefit of end-to-end solutions comes with a few drawbacks.\nA trained deep neural network policy is often treated as a “black box,” meaning that the internal structure of the neural network is so complex, often consisting of millions of parameters, that it is almost impossible to understand, explain, and evaluate the decisions taken by the network (left side of Figure 3). This makes it hard to establish formal performance guarantees with neural network policies. Think of it this way: Even if you train your pet, there will still be occasions when your commands will go unnoticed.\nAnother thing to keep in mind is that reinforcement learning is not sample efficient. This means that, in general, a lot of training is required in order to reach acceptable performance. As an example, AlphaGo, the first computer program to defeat a world champion at the game of Go, was trained nonstop for a period of a few days by playing millions of games, accumulating thousands of years of human knowledge. Even for relatively simple applications, training time can take anywhere from minutes to hours or days. Finally, setting up the problem correctly can be tricky; many design decisions need to be made, which may require a few iterations to get it right (right side of Figure 3). These decisions include, for example, selecting the appropriate architecture for the neural networks, tuning hyperparameters, and shaping the reward signal.\nIn summary, if you are working on a time- or safety-critical project that you could potentially approach with alternative, traditional ways, reinforcement learning may not be the best thing to try first. Otherwise, give it a go!\nReinforcement Learning Workflow\nThe general workflow for training an agent using reinforcement learning includes the following steps (Figure 4).\n1. Create the Environment\nFirst you need to define the environment within which the agent operates, including the interface between agent and environment. The environment can be either a simulation model or a real physical system. Simulated environments are usually a good first step since they are safer (real hardware is expensive!) and allow experimentation.\n2. Define the Reward\nNext, specify the reward signal that the agent uses to measure its performance against the task goals and how this signal is calculated from the environment. Reward shaping can be tricky and may require a few iterations to get right.\n3. Create the Agent\nIn this step, you create the agent. The agent consists of the policy and the training algorithm (refer back to Figure 2), so you need to:\na. Choose a way to represent the policy (e.g., using neural networks or look-up tables).\nb. Select the appropriate training algorithm. Different representations are often tied to specific categories of training algorithms, but in general, most modern algorithms rely on neural networks because they are good candidates for large state/action spaces and complex problems.\n4. Train and Validate the Agent\nSet up training options (e.g., stopping criteria) and train the agent to tune the policy. Make sure to validate the trained policy after training ends. Training can take anywhere from minutes to days depending on the application. For complex applications, parallelizing training on multiple CPUs, GPUs, and computer clusters will speed things up.\n5. Deploy the Policy\nDeploy the trained policy representation using, for example, generated C/C++ or CUDA code. No need to worry about agents and training algorithms at this point—the policy is a standalone decision-making system!\nTraining an agent using reinforcement learning is an iterative process. Decisions and results in later stages can require you to return to an earlier stage in the learning workflow. For example, if the training process does not converge on an optimal policy within a reasonable amount of time, you may have to update any of the following before retraining the agent:\n- Training settings\n- Learning algorithm configuration\n- Policy representation\n- Reward signal definition\n- Action and observation signals\n- Environment dynamics\nToday, tools like Reinforcement Learning Toolbox (Figure 5) can help you quickly learn and implement controllers and decision-making algorithms for complex systems such as robots and autonomous systems.\nRegardless of the choice of tool, before deciding to adopt reinforcement learning, do not forget to ask yourself: “Given the time and resources I have for this project, is reinforcement learning the right approach for me?”\nTo learn more about reinforcement learning, see the links below.\n- Reinforcement Learning (webpage): Learn about reinforcement learning and how MATLAB and Simulink can support the complete workflow for designing and deploying a reinforcement learning based controller.\n- Train DQN Agent to Balance Cart-Pole System (example): Find out how to train a deep Q-learning network (DQN) agent to balance a cart-pole system modeled in MATLAB.\n- Train Biped Robot to Walk Using DDPG Agent (example): Find out how to train a biped robot, modeled in Simscape Multibody, to walk using a deep deterministic policy gradient (DDPG) agent.\n- Reinforcement Learning (video series): Watch an overview of reinforcement learning, a type of machine learning that has the potential to solve some control system problems that are too difficult to solve with traditional techniques.\nReinforcement Learning with MATLAB (ebook): Find out how to get started with reinforcement learning in MATLAB and Simulink by explaining the terminology and providing access to examples, tutorials, and trial software.\nSubscribe to our NewsletterGet the latest updates and relevant offers by sharing your email.\nYou can write for us and be one of the 500+ experts who have contributed stories at AIM. Share your nominations here.\nEmmanouil Tzorakoleftherakis is the product manager for Reinforcement Learning and Controls at MathWorks. Previously, he has been a technical marketing specialist and application support engineer with the company. Prior to MathWorks, Emmanouil has been a research assistant and intern at Northwestern University, Siemens Corporate Technology, the Rehabilitation Institute of Chicago and the University of Patras, Greece. Emmanouil has a BS/MS in Electrical and Computer Engineering from the University of Patras as well as a Masters and Ph.D. in Mechanical Engineering from Northwestern University."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:49ce87d1-cd9e-4c9d-a3d1-5faf71d3c5cd>"],"error":null}
{"question":"What are the key responsibilities and skills needed for an instructional designer?","answer":"A successful instructional designer must have the ability to take information and present it in a way that effectively changes or reinforces behavior or thinking. They need to always have a goal as this is where the design process starts. The role involves creating course plans, developing storyboards, employing instructional strategies, and understanding design principles. They work with various tools and techniques to bring concepts and ideas laid out in course plans to life.","context":["Instructional Design: The Process – Part 2\nNOVEMBER 25, 2016\nIn the last article , we discussed ID, ISD and the various models and frameworks associated with both. The article ended with an introduction to ADDIE framework of learning design. The five phases of ADDIE framework encompass the entire content development process, from discovery to delivery. ADDIE Framework. Proposed instructional strategies. Design Phase.\nWebinar - The Art of Storyboarding\nJUNE 9, 2015\nStoryboard is like the blueprint of an eLearning course. Instructional designers and course creators mostly intend to create a good storyboard for their courses but something or the other hinders them. At times, some of them fear that the storyboard might make their course too linear. eLearning Instructional Design storyboard storyboarding webinar\nGaps in the ADDIE Instructional Design Model\nNOVEMBER 5, 2013\nI have often written in the past about the strengths of using an elearning model, such as ADDIE , for course design, development, and delivery. still happen to believe that ADDIE (or derivatives of this framework) tend to capture the most under the instructional design umbrella, but that’s not to say there aren’t any flaws. Some Weaknesses.\n‘Art of Storyboarding’ Webinar – Highlights\nJULY 9, 2015\nTeam Raptivity recently hosted a successful webinar on ‘ The Art of Storyboarding ’ by Desiree Pinder - Executive Director/Founder of Artisan E-Learning. Desiree answered some fundamental questions on storyboarding, through this webinar, such as: what is a storyboard, why create one, and how to build and show content in a storyboard. Happy storyboarding!\nBack To Basics: Instructional Design Terminology\nSEPTEMBER 1, 2015\nIn our first Back to Basics blog “What is Instructional Design?” we talked about the concept and definition of instructional design. This week, we wanted to cover some common terms that you are bound to hear in almost instructional design setting. For the experienced Instructional Designers out there, what terminology did you wish you had known first?\nTop Writing Mistakes in Storyboards\nAPRIL 15, 2014\nRecently we asked our instructional designers and storyboard editors what mistakes they were finding (and constantly fixing) during storyboard edits. In our storyboards, we don’t ever use these abbreviations in narration (because it is more of a writing convention than a speaking convention). This term is used just before you give an example of something (e.g.,\nTop Instructional Design Tips\nOCTOBER 20, 2015\nAt a recent workshop I was asked to put together a post with some of my top instructional design tips. Of course, over the past 8 years most of the posts are littered with various tips related to instructional design. Instructional Design Tips. Here’s a list of blog posts related to instructional design. Instructional Techniques. Tell,\n3 Signs Your Training Department is Instructional Design Deficient\nMARCH 29, 2016\nYou immediately started developing an eLearning course–writing storyboards, creating graphics, testing navigation, programming buttons, configuring quizzes, the whole nine yards. Well, what happened? I’ll tell you what happened: your training department was ID-deficient or instructional design-deficient. eLearning Featured Instructional Design Resources Resources\nStoryboarding, prototypes and e-learning design\nSpark Your Interest\nOCTOBER 11, 2011\nThis week they talked a bit about storyboarding. Kevin Thorn (aka learnnugget) was the guest and he said he is not a fan of showing any kind of graphic to client in the initial stages, as the client can become fixated with that element and derail the higher level instructional framework. He starts with the storyboard and maps it out. Have you heard of The Toolbar ? And beer.\nStoryboarding for E-Learning: Challenges and Tips\nJULY 20, 2012\nThe storyboard is a representation of content, visual design, interactivity, navigation, narration etc in soft copy form. For examples of storyboards, check out Connie Malamed’s blog. Storyboards are used by the instructional designer (ID) to: Gain agreement with the subject matter expert (SME) on the design approach of the e-learning course Provide instruction to the e-learning developer on [.]. Design Development\n4 Excellent Examples of E-learning Courses\nAUGUST 17, 2016\nAs an instructional designer, this could be your worst nightmare. Fully immersive and robust visual design. As a learning and design professional, how can you beat the short attention span of online learners? Great storyboarding and strong script. Eye-catching visuals, stylish design, and sophisticated animations. Instructionally sound.\nStoryboarding, prototypes and e-learning design\nSpark Your Interest\nOCTOBER 11, 2011\nThis week they talked a bit about storyboarding. Kevin Thorn (aka learnnugget) was the guest and he said he is not a fan of showing any kind of graphic to client in the initial stages, as the client can become fixated with that element and derail the higher level instructional framework. He starts with the storyboard and maps it out. For example, I recently reviewed some options with a client and he loved one element, but did not want to use it for the whole course, but for certain activities. learning Graphic design Instructional design Storyboard\nFree Storyboard Templates for e-Learning\nDECEMBER 12, 2010\nNews from the e-learning frontier Pages Home About Community Free e-Learning Resources Contribute to the e-Learning Community 12/12/2010 Free Storyboard Templates for e-Learning What I really like about the e-Learning community is that the most e-Learning professionals have a great passion of sharing their knowledge. The least that I can do is to share with you a list of Free Storyboard Templates for e-Learning ! Feel free to use these storyboard templates for your projects. Your storyboard template format could be either Word or PowerPoint. Thanks! Excellent post!\n10 “Super” Instructional Design Skills\nJANUARY 12, 2014\nIf you ever wondered what it takes to be a successful instructional designer, then the inforgraphic below (originally detailed by SHIFT elearning ) is a good place to start. Generally speaking, a good instructional designer is someone who can take information and present in a way that effectively changes (or reinforces) a particular behavior or way of thinking.\nBecome an Expert in Instructional Design\nAUGUST 18, 2014\nBecoming an instructional designer doesn’t happen overnight. What are the stakes to be an instructional designer? An instructional designer must and always have a goal for this is where it will start. Let us look at the basic 4Ds of Instructional design. ADDIE stands for Analyze, Design, Develop, Implement and Evaluate.\nGetting to know ADDIE. Part 3 - Development\nSEPTEMBER 9, 2015\nHaving scoped out the target audience, settled on what knowledge the course aims to impart, and composed a plan during the Design stage, we are prepared to move on to Development - a key stage of the ADDIE process, though not the last one. The instructional designers will be using content authoring tools to bring the concepts and ideas laid out in the course plan to life.\nInstructional Design for Responsive Projects in Captivate 8\nMAY 28, 2014\nResponsive projects in Captivate 8 offer the ability to design and publish responsive mobile projects in a way we haven’t seen in the past. In order to effectively design courses for this new capability, we may consider keeping a few things in mind further up-stream in the design process, starting with a mobile-first approach to course design and storyboarding.\nCaptivate 8 Custom Motion Effect Example\nFEBRUARY 10, 2015\nThe original concept for this activity came from another instructional designer on a recent project where she wrote the storyboard and I did the Captivate development. I created a Captivate activity with a custom effect using motion and scaling to animate two cars driving away to the horizon. Each car moves depending on how you answer a series of questions.\nADDIE vs AGILE: How to set up a fast and effective eLearning production process\nAPRIL 21, 2016\nThe ADDIE model for eLearning. ADDIE has been around since the 1950s. ADDIE is an acronym made up of five words: Analysis, Design, Development, Implementation, and Evaluation. In its purest form, each phase of ADDIE should be completed in turn with the outcomes fed into the next phase. Design. Pros of ADDIE. Cons of ADDIE. Analysis. Mobile?\nSo, you want to be an Instructional Designer?\nSEPTEMBER 4, 2015\nOur Senior Instructional Designer, Rhys Williams, gives us an insight into how someone becomes an instructional designer, and explains what the varied role entails… There’s a scene in the television sitcom Friends where Rachel and Monica are desperately trying to remember what Chandler – one of their best pals – does for a living. Well, I’ll tell you.\nTop 10 Instructional Design Tips for Effective eLearning\nDECEMBER 11, 2016\nInstructional Design (ID) plays a crucial role in designing an engaging and effective e-learning course. The instructional designer employs ID strategies to create good e-learning courses that facilitate optimum knowledge transfer. This blog shares a few tips on instructional design to help improve your efforts of designing good e-learning courses. 1.\nMeet Instructional Designer Brandon Winston [Audio Interview]\nThe Learning Dispatch\nAUGUST 4, 2016\nInstructional designer Brandon Winston joined the Microassist curriculum development team in May 2016. In roughly three minutes, Brandon gives insight on what an instructional designer does and why he’s so passionate about the process of developing custom training. BRANDON: I have been hired as an instructional designer here with Microassist. VIVIAN: And what does an instructional designer do? BRANDON: Absolutely, and I think that’s actually one of the best parts of being an instructional designer. Interview transcript.\neLearning Glossary Part 2: More Commonly Used Terms\nJUNE 9, 2016\nADDIE- The ADDIE model is a process used by instructional designers and training developers offering guidelines for creating effective training. This model is broken up into 5 phases: Analysis, Design, Development, Implementation, and Evaluation. AGILE- AGILE is the counter method to sequential processes like ADDIE. But you weren’t full.\neLearning Storyboards: How to Create A Clear Road Map Everyone Can Follow\nDECEMBER 13, 2013\nIf you are designing the course, it’s your job to set up a clear road map that will help your reviewers stay with you for the entire trip. You need to create documents such as storyboards to guide your journey. The storyboard document is the best way to lay out the details of an eLearning course. It’s the time of year when many of us make travel plans with family or friends.\nLearn How To Be An Instructional Designer\nNOVEMBER 26, 2012\nCheck out these awesome resources to learn more about becoming an Instructional Designer: Breaking into Instructional Design. Our friend Connie, the eLearning Coach, has a great eCourse for people who want to learn more about Instructional Design and Technology. In her eCourse she answers questions like: What type of work do Instructional Designers do? Do I need a degree to be an Instructional Designer? Making a great course starts with great instructional design. Storyboard That. Elearning Blueprint.\nCammy Beans Learning Visions: The Value of Instructional Designers\nJANUARY 7, 2008\nCammy Beans Learning Visions Musings on eLearning, instructional design and other training stuff. Monday, January 07, 2008 The Value of Instructional Designers Ive been having on ongoing conversation with other instructional designers as to whether or not we need to have the technical skill sets to actually build the courses we design. Lots of steps?\nWhy We Integrate Our Instructional Design and Product Design Teams\nMAY 23, 2016\nIn most cases (and especially for in-house instructional design situations) the instructional design and product design and development teams operate side-by-side. At eLearning Mind, we’ve rethought the way that our ID and multimedia design departments work together, and we realized that these teams are interdependent and collaborating over a wall isn’t the most effective way to get the most out of each. Reason #1: Learner-Centered Design. Each is stored neatly in its box, and any collaboration happens in an “over the wall” type of interaction.\nAn eLearning Design Checklist with Development in Mind\nOCTOBER 22, 2014\nTime to get proactive with eLearning Design! Here is a comprehensive checklist of things you should be thinking through and documenting for each page of your course design. If you need a storyboard template, download our Free Instructional Design Storyboard Template ! Visual Design Elements. If so, how will you account for that in the design?\n5 Killer Examples: How To Use Microlearning-Based Training Effectively\nAPRIL 26, 2016\nHere are 5 great examples of using microlearning-based training effectively. In this article, I will share 5 examples that will highlight how you can use microlearning-based training. These examples feature innovative learning strategies (videos, scenarios, white-board animation, and kinetic text). Killer Examples Of Using Microlearning-Based Training. Background.\nDefine and Develop\nAPRIL 13, 2016\nAt CourseArc we have created hundreds of engaging online modules for our clients and in this process we have found that many K-12 organizations, universities, and corporate training departments have few to no resources with formal training in instructional design. Learn more about instructional design and eLearning with our fourth module, titled Define and Develop.\nThe Instructional Design Review: Questions to Ask\nSEPTEMBER 13, 2013\nThis week let’s look at the Instructional Design Review. This is why we always check the instructional design. If you’re a one-person team, this means you’ll want to do a self-check. Ideally, though, you would have a separate team member with instructional design knowledge review the course. For example, are difficult concepts broken down properly? Could an example be used to clarify a point? The post The Instructional Design Review: Questions to Ask appeared first on E-Learning Uncovered The correct post is below.\nPractical Storyboarding – Not Just for Developers\nJANUARY 4, 2011\nWe designers work in the world of business needs as well as learner instructional needs. Pulling these two diverse needs together into one excellent eLearning product is a challenge we face with every design. For example: Suppose the specifications state that you must incorporate the company’s visual branding standards into a lesson? Ensuring that content points and specifications are cared for within the lesson’s design can be challenging, but it need not be daunting. Here are some other design uses for storyboarding: --1-- Outline your page flow.\nStoryboard Templates and Resources\nJUNE 29, 2009\nFor the last week I have been in storyboard mode for a large course I am creating. Since grad school I have been using the same format for storyboards, when I do use them. Some projects benefit from use of storyboards, some do not, but that is another post. The storyboard I use is a very simple, but flexible format that we used in my school's ISD program.\nUsing a screen type index to create balanced storyboards\nGood To Great\nFEBRUARY 16, 2011\nThe way I do this is to create what I call a ‘screen type index’ I put this at the start of every storyboard I write to give me that at-a-glance big picture information. Of course, like any patchwork, there’s no perfect design. So, when I’ve completed my storyboard, I complete the screen type index, assess the balance and make the appropriate changes."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:9b9ea8db-6737-4513-98a8-cb40c3ae8f88>"],"error":null}
{"question":"How do current healthcare transparency initiatives work in practice, and what are the specific patient information rights regarding treatment costs and provider quality?","answer":"Current healthcare transparency initiatives have shown limited success - patients rarely use price and quality transparency tools, find decisions complex, and fear disrupting physician relationships. Even when patients have access to information, both high-value and low-value care tend to decrease under high-deductible plans. However, patients do have specific rights regarding information access: they can request estimates of overall treatment costs, information about provider quality metrics (including procedure volumes and complication rates), details about provider authorization status, and professional liability coverage. They also have the right to receive this information in clear, understandable language and can request written documentation.","context":["This is the last part of the Fundamentals of U.S. Health Policy series! And it’s a super interesting one. Michael Chernew, Ph.D., wrote about the role of market forces in U.S. health care. Since this is squarely in my area of focus, I have a lot of thoughts. Thus, this week I’ll stick to summarizing Dr. Chernew’s article, and then next week I’ll provide some commentary.\nForewarning, I’m following the paper’s logic flow, which, to my brain, is a little meandering, so it’s easy to lose one’s place, but I’ll clarify as much as I can now and then attempt to provide additional insight next week.\nRemember how Total Healthcare Spending = Price x Quantity? (Well, actually, it’s the sum of the price x quantity of all the different services being provided in our healthcare system.) Dr. Chernew is basically using that equation when he starts out by saying that our challenge is to reduce the quantity of low-value services provided and to lower prices.\nAnd then the big question . . .\nWhat role should markets play in doing that?\nHe finally gets to the answer at the end, which is that markets and government should both be used to complement each other. Markets can be leveraged inasmuch as they will help, and this should be paired with the government regulations needed to help them work as well as they can.\nI won’t list his specific recommendations quite yet about how we could do that because first I need to review what he says in the rest of the article about markets and how they work.\nFirst, he says that markets are the “foundation of our economy,” and they promote efficient production and cost-reducing innovation. He doesn’t exactly give the step-by-step explanation of how they do that, but you can gather it from his next several paragraphs. Markets create competition, which is when consumers (in this case, patients) have “the ability and incentives . . . to seek low-price, high-quality providers. . . .” And because of that competitive pressure to win consumers, the players in a market are forced to innovate in ways that make production more efficient.\nGreat, so a good healthcare market will help patients choose low-price, high-quality providers. Unfortunately, healthcare markets are more imperfect than other markets. Want a big piece of evidence for this? Look at the extent of unwarranted price variations that exist in healthcare. It’s way more than in other markets.\nBut why is the healthcare market so bad?\n“Competition in health care fails for several fundamental reasons. First, patients often lack the information needed to assess both their care needs and the quality of their care. Second, illness and health care needs are inherently difficult to predict, exposing people to financial risks that they must insure against. This risk gives rise to an insurance system that shields patients from the price of care, dampening their incentive to use care judiciously and to seek care from providers offering high-quality care at affordable prices. The information problem, amplified by insurance, reduces the ability and incentives for patients to seek low-price, high-quality providers and impedes well-functioning markets. This problem has been magnified lately by consolidation of health care providers.”\nSo, basically, it’s difficult for patients to really know what care they need, they have a hard time assessing quality of care, they’re shielded from prices because of insurance, and consolidation has limited their options. The result of all that is they have neither the ability nor the incentives to choose low-price, high-quality providers.\nThis, by the way, sounds almost exactly like what I’ve written (or linked to) a thousand times before, which is that patients need to start making value-sensitive decisions, and to do that they need (1) multiple options, (2) the ability to identify the value of each option, and (3) the incentive to choose the highest-value option.\nRegarding consolidation, he gives some interesting data, which show that only 51% of markets have 3 or more hospital systems.\nBased on all of that, many would conclude that we should abandon markets altogether in healthcare. But he says, “The weaknesses associated with market-based health care systems are severe, but that does not mean the market should be abandoned.”\nAnd then he proceeds to give a few examples of beneficial things that have come from markets already, such as new payment models, telemedicine, a shift from inpatient to outpatient care, and narrow networks (which allows for lower prices).\nThose, however, end up being overshadowed by the list of ways we’ve tried and failed to bolster market function by providing patients with better information about quality and prices and by changing insurance benefit designs.\nThe summary of this section of the paper is that giving patients better information about quality and prices have had very little success because . . .\n- Patients rarely use price- and quality-transparency tools\n- These sorts of decisions are complex\n- Patients fear disrupting their relationships with their physicians\nChanging benefit designs to get patients to directly pay for more of their care (e.g., implementing high deductibles) has had a larger effect on utilization, but it hasn’t significantly impacted the market because . . .\n- What tends to happen is higher-value and lower-value care both decrease\n- Not enough patients end up getting steered toward higher-value providers to actually impact market prices.\nHe provides his explanation for all these failures: “The core problem is that for markets to work, patients must face the economic consequences of their choices, but labor-market concerns dampen employers’ enthusiasm for adopting plans that impose such consequences.”\nIn the realm of getting patients to choose higher-value insurance plans, there’s been a little bit of headway with insurance exchanges, although there are many drawbacks to those, too . . .\n- Beneficiaries make poor plan choices\n- Insurance exchanges induce more price sensitivity, which leads people to choose lower-premium plans that impose greater financial risk on them, which they often cannot bear\nAnd, to make things worse, many of the downsides of insurance exchanges can worsen inequity.\nDr. Chernew is not exactly giving a glowing review of market-based reform attempts, is he? His comments are all accurate though.\nNext, though, he says that “in evaluating their merits, we need to compare them with other systems, such as government-run models.” And government-run models have their own set of limitations.\nLuckily, we are not facing an either-or decision. The important question is how government and markets can complement one another. “We do not need to abandon markets–we can make them better.”\nFinally, getting to his recommendations about how to use markets and government to complement each other, he says we could work to increase the effectiveness of transparency initiatives, limit provider consolidation, and impose gentle regulations to prevent the most severe market failures (like limits on surprise billing and instituting price caps on the most excessive prices).\nDr. Chernew’s conclusion is that, “If we fail to improve market functioning, stronger government involvement will most likely be needed.” Agreed.\nNext week, I’ll give my thoughts on all this!","The Law of 24 July 2014 consolidates the rights and obligations of patients in a single law, thereby allowing for greater consistency and transparency in the relations between patients and healthcare providers. This law focuses on:\n- the importance of the information due to patients by health providers;\n- improving the extent to which patients' wishes are taken into account in the delivery of care, including the possibility of appointing a trusted person or support person;\n- the generalisation of and access to the patient's medical file;\n- the creation of a National Health Information and Mediation Service (Service national d’information et de médiation) on 1 January 2015.\nMutual respect, dignity and loyalty\nPatients have the right to the protection of their privacy, confidentiality, and dignity, and to respect for their religious and philosophical convictions.\nBy providing, in accordance with their abilities, information that is relevant for their care, and consenting to and cooperating in the latter, patients play a part in ensuring that they receive the best possible healthcare.\nAccess to quality healthcare\nDepending on their state of health, patients have equal access to healthcare. This care is provided efficiently and in line with scientific data and legally prescribed quality and safety standards.\nIn addition, healthcare should be organised in a way that guarantees continuity of care in all circumstances.\nFreedom to choose healthcare providers\nSubject to the organisational imperatives inherent in the delivery of healthcare, each patient has the right to freely choose the healthcare provider they wish to receive care from. They may change their mind at any time.\nFor all medical procedures delivered in a hospital setting, their choice is limited to the healthcare providers accredited by the establishment.\nRefusal to provide healthcare to patients\nA healthcare provider may refuse to provide care to a patient for personal or professional reasons. They may refuse to provide any care when they believe they are not in a position to do so appropriately.\nDifferences in treatment may be legitimate when founded on objectively justifiable reasons. Consequently, for example, when a patient's state of health does not justify the use of certain treatments, the healthcare provider cannot be accused of discrimination if their action is based on objective medical reasons.\nAt the patient's request, the healthcare provider will help the patient find a another provider that is capable of providing the required care. Providers are duty-bound to provide:\n- urgent first-aid care to the extent of their capabilities;\n- continuity of care in all circumstances.\nThe refusal to provide healthcare may not, in any case, be based on discriminatory considerations. When a patient can present factual grounds for supposing that they have been discriminated against, it is up to the healthcare provider to justify their refusal by providing objective non-discriminatory reasons for their decision. In practice, it is in healthcare providers' best interest to record the objective non-discriminatory reasons for their refusal to provide care in the patient's medical file.\nRight to assistance\nPatients have the right to be assisted in their healthcare-related formalities and decisions by a third party of their choosing, who may or may not be a healthcare professional. Any such person chosen by the patient to support and assist them are called support persons.\nTo the extent desired by the patient, the support person is integrated in the healthcare process and, if requested by an adult patient, the obligation to patient confidentiality may be waived in their regard.\nThe identity of a support person is noted in the patient file. The healthcare professional may however at any time freely decide to speak with the patient without the support person being present.\nPatients' right to information regarding their state of health\nHealthcare professionals must notify their patients in advance of the services that they are responsible for providing and inform them of their state of health and the probable assessment thereof. Such information must be provided to the patient in clear and understandable language; it is generally communicated verbally and, where applicable, can be communicated in writing.\nDecisions concerning a patient's health are taken jointly by the patient and their healthcare professionals. Care for patients having the necessary capacity can only be delivered with their prior informed and enlightened consent, after they have been adequately informed. In addition, patients may refuse or withdraw their consent at any time.\nAdvance notification refers to the essential information characterising the proposed healthcare, including adequate information on the objectives and foreseeable consequences of the care, its benefits and possibly urgency, generally known risks or frequent and/or serious events associated with the care, assessed in light of the patient's specific profile, as well as any possible therapeutic alternatives or options that may be envisioned, and the foreseeable consequences in the case of a refusal.\nAdvance notification also entails, at the patient's request, providing an estimate of the overall cost for the proposed treatment and conditions for coverage. Also, at the patient's request, such notification may include information on the availability, quality and safety of the proposed healthcare, including the number of procedures carried out by the provider, the rate of complications, the foreseeable duration of a hospital stay, if any, the authorisation or registration status of the healthcare provider, and the provider's professional liability insurance coverage.\nThe right 'not to know' and therapeutic privilege\nPatients may sometimes wish not to be informed of a diagnosis, prognosis or any other information in connection with their state of health or its probable evolution. If this is the case, their wishes must be respected, unless the non-communication of such information to the patient could cause serious harm to the patient's health or to that of a third-party. The patient's wish not to be informed should be recorded in their the patient file.\nExceptionally, doctors may decide not to communicate information that could manifestly cause serious harm to the their patients' health. In that case, the patient's doctor would first discuss the matter with a colleague and, in so far as possible, with the patient's person of trust. They would then record an explicit reason in the patient file.\nTo ensure the best possible healthcare, the patient should provide, to the best of their ability, any information that is relevant for their care, and generally cooperate with their healthcare providers in respect of the latter. During their treatment, patients are expected to respect the rights of healthcare providers and other patients.\nFreedom to choose a person of trust\nAll adult patients who have the capacity to give their consent, but who may no longer be able to express their wishes and receive information necessary for taking a decision related to their health, may appoint a person of trust. The appointment must be in writing, and dated and signed by the patient.\nPatients unable to express their wishes\nIf the patient is temporarily or permanently unable to express their wishes, the healthcare provider will attempt to determine the patient's supposed wishes. To do so, the healthcare provider can refer to the patient's appointed person of trust, or to any other person who is likely to be aware of the patient's wishes.\nIf, in a medical emergency, a patient is unable to take decisions concerning their health, and their wishes could not be established, the health care provider may, in the interest of the patient, immediately take any emergency medical measures dictated by the situation.\nThe rights of patients who are unemancipated minors may be exercised by their parents or by any other person who is their legal representative.\nIf an unemancipated minor patient possesses sufficient judgement to reasonably assess their interests, their physician, or any other healthcare provider responsible for their care, may allow them to exercise their rights in connection with their health in an independent manner. Legislators felt that it was preferable not to set a precise legal level, but to let doctors decide whether the patient is sufficiently mature.\nIf the minor patient's life or health is in serious and immediate danger, their physician may take any medical measures required by the situation. Where necessary, such emergency measures may be taken even if the minor's parents, or the person(s) with parental authority over the minor patient, refuse to consent to the measures in question. In that case, the doctor must send a report to the State prosecutor, within 3 working days, stating their reasons for the medical measures they decided to take.\nAll patients are entitled to a patient file that is carefully kept up-to-date. This file should provide a faithful and chronological record of the patient's state of health and its evolution over the course of their healthcare.\nThe holder of the patient file should provide safekeeping for the file for at least 10 years, starting from the end date of the healthcare.\nAccess to patient files and data on patients' health\nPatients have the right to access their file, and all their health-related information, held by their doctor or any other medical body. They have the right to have the contents explained to them by the healthcare provider, and may also, for that purpose, be assisted by a support person.\nIf the patient file is consulted, or their health-related data is accessed, in the patient's absence, by a third-party individual who is not a healthcare professional acting in their capacity as such, that individual must be able to produce a written approval to do so, dated and signed by the patient. Consulting a patient's file is the patient's absence must be exceptional.\nFurthermore, patients may obtain a full copy of their patient file, and demand that it be forwarded to the healthcare provider of their choice.\nExcept when the patient's state of health requires more urgent access, the holder of the patient file is expected to respond to such demands within no more than 15 working days of receiving the demand.\nConfidentiality and professional secrecy\nTwo or more healthcare professionals may exchange information related to the same patient receiving healthcare in order to ensure the continuity of their care or determine the best possible treatment, unless the duly informed patient is opposed to their doing so. The duly informed patient may refuse, at any time, to allow their information to be communicated to one or more healthcare professionals. However, the healthcare professional who initiated the healthcare service(s) remains entitled to access the contents of the file related to the service(s) that they provided.\nWith the patient's consent, healthcare professionals may also provide persons close to the patient with any information that is deemed indispensable for them to intervene in the patient's best interest. The patient's consent is not required if, in the case of a serious diagnosis or prognosis, the patient is unable to express their wishes and had not previously expressed their objection to the lifting of the obligation of medical confidentiality.\nPersons of trust, and support persons assisting patients in their relations with healthcare professionals, who fail to preserve the confidentiality of the patient's information face the risk of sanctions."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:c49e0bc8-39fc-4713-be9c-670e4394c2ad>","<urn:uuid:95da66fe-e786-4625-bf94-046be6988646>"],"error":null}
{"question":"How do the sensitivities of visceral pleura and parietal pleura differ, and how does this affect treatment approaches in stage I mesothelioma?","answer":"The visceral pleura and parietal pleura differ in their sensitivity to pain - the visceral pleura is not sensitive to pain, while the parietal pleura is pain-sensitive. In stage I mesothelioma, when tumors are confined to these pleural layers and haven't spread to lymph nodes, curative surgery is considered the first line of treatment. At this early stage, doctors usually recommend aggressive treatment including chemotherapy and radiotherapy, with extrapleural pneumonectomy being a potentially curative surgical option. The median life expectancy at stage I is 21 months, which is significantly better than later stages where patients may live 12 months or less.","context":["Visceral pleura Definition\nIt is a thin serous membrane tissue layer that sticks to the lung surface. It is the innermost of the two pleural membrane layers investing the lungs.\nIt is also known by the name Pulmonary pleura. In Latin, this structure is referred to as Pleura visceralis and Pleura pulmonalis.\nVisceral pleura Anatomy\nThis tissue layer comprises a smooth, continuous layer of mesothelial cells. It merges together with the parietal pleura at the root of the lungs to form a single layer. A fluid manufactured by the pleural layers is found to surround the lungs and act as a covering for the Visceral pleura.\nVisceral pleura Appearance\nIt is moist and shiny in appearance.\nVisceral pleura Location\nIt runs continuously with the parietal pleura, which is located at the lung base and covers the diaphragm as well as lines the inner chest walls. It firmly adheres to the lungs and cannot be separated from the organ.\nVisceral pleura Functions\nThe main functions of this structure are:\n- The Visceral pleura follow the underlying lung surface very closely. It covers the lungs closely and adheres to all the surfaces of the organ, except at the hilum and along the spot where it attaches to the pulmonary ligament.\n- It moves down into the fissures and covers the lobes.\n- It gives a smooth, slippery surface to the lungs and allows it to move freely in the parietal pleura.\nVisceral pleura Disorders\nThis serous membrane layer is prone to conditions like:\nIt refers to a swelling of the pleural layers. The visceral pleura, which is the inmost of the two layers, becomes inflamed due to cold weather and certain infections. This can give rise to an acute burning pain which can be experienced while breathing deeply. Swelling and discomfort due to the condition may be relieved by steroid medications.\nIt occurs due to an excessive production of pleural fluid in the pleural space, the area between the two pleural membranes. It is supposed to arise due to serious conditions like Pneumonia, Pulmonary embolism, Congestive heart failure or Cancer. It impairs the ability to breathe comfortably. Pleural effusion is a serious medical disorder that requires emergency treatment.\nIt can also suffer from a cancerous disorder, known as Mesothelioma which owes its name to the cell layer called Mesothelium which contains the pleural layers located around the lungs. This type of cancer is commonly found to arise in people who work with asbestos.\nExposure to the molecules of asbestos silicate is believed to give rise to Mesothelioma as well as another disease known as Visceral Pleural Fibrosis. Visceral Pleural Fibrosis leads to the thickening and reduced flexibility of the Visceral pleura. Sufferers may find it difficult to breathe in as the disease worsens. Generally, the damage is found to be irreversible.\nIn people suffering from lung cancer, Visceral Pleural Invasion (VPI) of the cancerous lung cells indicates a poor prognosis. Patients diagnosed with this problem may require further specialized treatment for cancer. According to some oncologists, Chemotherapy might be an effective treatment option for VPI.\nVisceral pleura and Parietal pleura\nThe inner side of the lungs is lined by two layers of membrane. The outer layer of the lungs is the Parietal pleura which adhere to the chest wall. Visceral pleura, which is the inner one of the two layers, varies from Parietal pleura in a few respects. It is not sensitive to pain, unlike the parietal pleura. It is also directly attached to the lungs, unlike the parietal pleura which adhere to the opposing thoracic cavity.\nThe space between the Visceral and the Parietal pleura is known as the pleual cavity or intrapleural space. This is filled by a serous fluid manufactured by the pleura that is known as Pleural fluid.\nVisceral pleura Pictures\nTake a look at some important images of Visceral pleura to know how it looks like.\nPicture 1 – Visceral pleura\nPicture 2 – Visceral pleura Image","There are four primary stages of mesothelioma that doctors use to identify the seriousness of a mesothelioma diagnosis, labeled stage I, II, III or IV. The staging systems used to identify what stage of mesothelioma a patient has are the Brigham, TNM or Butchart systems. The lower stage number you’re given, the more treatment options you have, and the better your chances are for long-term survival.\nStaging is an accepted way for cancer doctors to describe to patients – and to one another – how advanced the disease is, how prevalent tumors are and how far the cancer has spread. Given the aggressive nature of the cancer, mesothelioma staging is a key part of proper diagnosis and treatment.\nWhy is mesothelioma staging important? Because knowing where a cancer is in its development sets the table for treatment, which treatment options are available and which are not. For example, doctors use cancer stage as a guideline when deciding whether a patient is likely to benefit from surgery.\nIf a patient is healthy enough for aggressive treatment and doctors believe that all visible signs of cancer can be completely removed, they say the cancer is resectable and will likely recommend a treatment plan involving surgery. If the spread of cancer is too advanced, however, and can’t be removed completely, the cancer is unresectable. In this situation, doctors will recommend other treatment options, including chemotherapy and experimental treatments.\nStage I pleural mesotheliomas are generally resectable, as well as some diagnosed at stage II and III. But by stage IV, the cancer can't be removed completely with surgery. The cancer's subtype can also affect treatment options because most doctors agree surgery doesn't help with sarcomatoid mesotheliomas.\nWhen peritoneal mesotheliomas have spread beyond the abdomen, surgery likely is not helpful. Doctors consider peritoneal mesothelioma unresectable when it has invaded certain areas within the abdomen, including:\nIf the cancer has spread to these locations, doctors typically deliver chemotherapy to the entire body or directly to the abdomen.\nFree information, books, wristbands and more for patients and caregivers.Get Your Free Guide Get Your Free Mesothelioma Guide\nThe stage of cancer is vital information for oncologists (cancer doctors) who treat mesothelioma, but all experts agree universally on one principle: It's preferable to catch cancer, specifically mesothelioma, in the earliest stage possible.\nMany specialists who treat these cancers — even some of the most respected ones in the field — don't agree on one staging system. Although many doctors agree that staging is a strong predictor of lifespan, not all do.Learn More About Staging Methods\nIf you are diagnosed with mesothelioma, you should seek a second opinion. Oncologists treat a variety of cancers, but they may not have experience with staging asbestos-related cancers. Always find a doctor who does.Get Help Finding A Specialist\nUsing one of three systems, doctors will assign your cancer a stage of I through IV. The stage varies depending on several factors, including the size and location of your tumor and whether it has spread to lymph nodes or distant organs.\nDoctors use three primary systems for mesothelioma staging: Brigham, TNM, and Butchart. These systems are used primarily for pleural mesothelioma staging because there are currently no formal staging systems for the other types. TNM and Brigham are the most-used by mesothelioma specialists, although some doctors believe that none are accurate enough.\nStage I typically offers the best chance for survival because of the potentially curative surgical treatments available for patients. Most stage I patients qualify for extrapleural pneumonectomy, an aggressive surgery that attempts to remove as much of the tumor growth as possible. Surgeons take out an entire diseased lung and other tissues, including the diaphragm, nearby lymph nodes and the linings of the chest and heart.\nAt this stage of progression, most doctors only recommend palliative treatments because the cancer has spread to many parts of the body. Trying to remove all the tumors is too difficult, and many patients are too weak to withstand aggressive treatments. Palliative treatments, which focus on symptoms rather than curing the disease, can ease pain and suffering and improve quality of life.\nBecause of the nature of mesothelioma and the way the cancer grows and spreads, most people are not diagnosed until stage III or IV. Doctors usually treat asbestos-related cancers with a combination of chemotherapy, radiotherapy and surgery. Depending on the progression and location of cancer cells, they may recommend different options.\nWe'll help you or a loved one find the most qualified mesothelioma specialists and treatment facilities in your area.\nAt this point of early development, tumors are close to the original tumor, having only grown in one layer of the lining of the lungs (pleura). The cancer may also have grown into the covering of the heart and the diaphragm on the same side as it first formed.\nThe life expectancy for someone with stage I mesothelioma is significantly better that those with later stages, and these patients may live three years or longer. However, it is difficult to catch the cancer this early because people with this stage do not usually have symptoms.\nCurative surgery to remove the tumor is considered the first line of treatment, and doctors usually recommend aggressive treatment that includes chemotherapy and radiotherapy.\nMedian life expectancy at stage I is 21 months.\nStage II symptoms are vague and mild. Patients — and even doctors — sometimes mistake them for signs of other illnesses like the flu. Patients with peritoneal mesothelioma may lose weight and yet feel bloated.\nSeveral treatment options are available, and doctors can usually offer curative surgery to remove tumors. In studies, some patients diagnosed at this point survive for years.\nMedian life expectancy at stage II is 19 months.\nOnce the cancer progresses to stage III, it may have spread to several locations on the same side of the body as it formed. These areas may include the lymph nodes, esophagus, muscles, ribs, heart and the chest wall.\nPleural mesothelioma patients may suffer from more difficulty breathing and intense chest pain even when resting. Peritoneal mesothelioma usually involves some type of bowel obstruction and pain. Because the tumors have now spread to multiple areas of the body, discomfort may be felt in other parts of the body as well.\nTumors typically do not respond to potentially curative treatment, and some stage III cancer patients will find themselves ineligible for surgery. At that point, doctors offer palliative options.\nMedian life expectancy at stage III is 16 months.\nAbout 30 percent of mesothelioma patients are not diagnosed until this stage. By stage IV, tumors have metastasized (spread) throughout the body via the bloodstream and could be present on the liver, in the brain, bones or elsewhere.\nPatients experience extreme difficulty breathing and suffer from severe chest pain. Tumors can spread to the esophagus or stomach, causing digestive problems and difficulty swallowing or eating.\nDoctor's don't recommend aggressive surgeries. Instead, they fall back on palliative treatments designed to ease pain and control symptoms.\nMedian life expectancy at stage IV is 12 months or less.\n|Parietal pleura:||The lining of the chest cavity|\n|Visceral pleura:||The lining of the lungs|\n|Lung parenchyma:||Any form of lung tissue, including the bronchioles, bronchi, alveoli, interstitium and blood vessels|\n|Mediastinal:||Affecting the mediastinum, the area between the lungs containing the heart, aorta, trachea esophagus and thymus|\n|Diaphragmatic pleura:||The lining of the diaphragm, a primary breathing muscle|\n|Peritoneum:||The lining of the abdominal cavity|\n|Endothoracic fascia:||A layer of connective tissue that separates the ribs from the pleura|\n|Pericardium:||The lining of the heart|\n|Myocardium:||The muscle tissue of the heart|\n|Diffuse:||Spread across a wide area|\n|Multifocal:||Occurring in more than one location|\n|Unresectable:||Not capable of being removed with surgery|\n|Nontransmural:||Occurring not completely, but partially across the wall of an organ|\n|Ipsilateral:||On the same side of the chest as where the cancer formed|\n|Contralateral:||On the opposite side of the chest as where the cancer formed|\nThe Brigham staging system was created by Dr. David Sugarbaker at the Brigham and Women’s Hospital in Boston. With this system, doctors examine organs, tissues and other structures to learn how far the cancer has advanced.\nTumors are confined to the lining of the lungs and cancer has not spread to the lymph nodes.\nTumors are confined to the lining of the lungs. Either the intraparenchymal or mediastinal lymph nodes are cancerous.\nAggressive and unresectable tumors in the lining of the lungs have spread into the mediastinum or invaded the chest wall, diaphragm or contralateral lymph nodes.\nCancer has spread to other parts of the body and is unresectable.\nFind out if you or a loved one qualifies for free mesothelioma medical care.\nDoctors use the TNM system to stage many different types of cancer. In 1995, the International Mesothelioma Interest Group (IMIG) modified this system specifically for asbestos-related cancers because of the lack of a universally accepted staging system.\nAlso called the IMIG Staging System, this is the most widely used system for someone with pleural cancer. Doctors sometimes use it for other types as well. Earlier staging systems like the Butchart system and Brigham system were based largely on studies of patients who underwent extrapleural pneumonectomy.\nThe abbreviation TNM signifies three different parts of a diagnosis:\n(T) describes tumor size and location.\n(N) describes whether lymph nodes are affected.\n(M) describes whether the tumors metastasized, or spread, to other parts of the body.\nDoctors add a number after each letter to describe how far the cancer has advanced. As tumor size increases and the cancer invades more structures, the number increases.\nT describes the size and location of the tumor:\nThe primary tumor cannot be assessed.\nThere is no evidence of a primary tumor.\nThe tumor is limited to the ipsilateral parietal pleura as well as the mediastinal and diaphragmatic pleura. There is no involvement of the visceral pleura.\nThe tumor has spread to the ipsilateral parietal and visceral pleura, along with the mediastinal and diaphragmatic pleura.\nThe ipsilateral pleural surfaces, which include the parietal, mediastinal, diaphragmatic and visceral pleura, have been invaded by tumors. At least one of the following features is also included in this stage:\nThe tumor has expanded locally, but can potentially be removed with surgery. It has invaded all of the ipsilateral pleural surfaces, which include the parietal, mediastinal, diaphragmatic and visceral pleura. One or more of the following features will be displayed:\nThe tumor is locally advanced and is unresectable. It has spread to all of the ipsilateral pleural surfaces including the parietal, mediastinal, diaphragmatic and visceral pleura. These tumors will also display at least one of the following features:\nN describes whether the cancer has spread to glands of the immune system called lymph nodes:\n|NX||The regional lymph nodes cannot be assessed.|\n|N0||Cancer has not spread to the regional lymph nodes.|\n|N1||Cancer has spread to the ipsilateral bronchopulmonary or hilar lymph nodes.|\n|N2||Cancer has spread to the subcarinal or the ipsilateral mediastinal lymph nodes, which include the ipsilateral internal mammary nodes.|\n|N3||Cancer has spread to the contralateral mediastinal, contralateral internal mammary, ipsilateral or contralateral supraclavicular lymph nodes.|\nM indicates whether cancer has spread to other parts of the body. If the cancer has metastasized, that means it has spread, causing secondary tumors to grow in distant parts of the body away from where the first tumor formed.\n|MX||The presence of metastasis cannot be assessed.|\n|M0||The tumor has not metastasized to other parts of the body.|\n|M1||Metastasis to other parts of the body has occurred.|\nTNM stage grouping combines the tumor, node and metastasis values to reveal the cancer's stage.\n|Stage||Primary Tumor (T)||Regional Lymph Nodes (N)||Metastasis (M)|\n|Any T||Any N||M1|\nHear how survivors learned about their diagnosis and what steps they took to cope. Learn More\nA former shipyard worker visits his doctor and complains of relatively mild symptoms including difficulty breathing and a feeling of heaviness in the chest. Because the patient presents with a history of exposure to asbestos on the job, the doctor suspects mesothelioma.\nAfter conducting a series of tests, the doctor finds a tumor that involves the supporting tissues of the lung and the diaphragm (T2), but finds no evidence that the tumor has spread to the lymph nodes (N0) or other areas of the body (M0). The doctor would likely diagnose the patient with stage II mesothelioma.\nThe Butchart system is the oldest method of staging pleural mesothelioma. Eric Butchart proposed this system in 1976 in an article for Thorax medical journal.\nThe Butchart system classifies tumors as stage I through IV using the same basic parameters as the TNM and Brigham systems.\nButchart intended his system to help identify which patients are good candidates for radical treatment. Again, only patients with stage I or II cancer are candidates for curative treatment, and palliative treatment is offered to patients in the two late stages.\nThis system also may help doctors determine prognosis. In four multivariate studies, early stage is listed as a sign of good prognosis. Two of the studies cited stage I disease as a highly favorable factor for survival.\nThe cancer affects the pleura on only one side of the chest, and may have spread to the pericardium and diaphragm. Curative surgery is considered a first-line treatment.\nCancer has spread to the chest wall and may affect both sides of the pleura. It may have spread to the esophagus, heart or lymph nodes of the chest. Butchart primarily recommends high-dose radiation therapy to most patients with stage II mesothelioma.\nCancer has spread to the diaphragm or peritoneum. Mesothelioma cells have also traveled through the lymphatic system to lymph nodes outside the chest. These tumors are generally unresponsive to curative therapies, so patients are introduced to their palliative options. Butchart suggests tube drainage for pleural effusions and delivery of chemotherapy drugs directly to the chest.\nThe cancer has spread through the bloodstream to distant parts of the body. Tumors may now be present on the liver, brain or bones, among other organs. Patients with stage IV mesothelioma are considered terminal and Butchart recommends solely palliative treatments.\nThrough a process known as tumor grading, doctors classify cancerous cells based on how they appear under a microscope. Researchers confirmed that cellular features of tumors can offer signs to help doctors better predict survival among mesothelioma patients.\nOne of several approaches to tumor grading is nuclear grading. The process classifies the size and shape of the nuclei in tumor cells while examining other factors, including nucleolus size, chromatin patterns and the rate of cell division. These variables play central roles in cancer cell genetics, and ongoing studies are exploring their clinical value as indicators of prognosis.\nThe nuclear grading system developed at Cleveland Clinic inspired other studies that investigated the connection between tumor grade and mesothelioma survival. In what turned out to be the strongest evidence supporting this relationship to date, researchers at Memorial Sloan-Kettering Cancer Center analyzed 232 cases of epithelioid diffuse malignant pleural mesothelioma for the following features:\nThe analysis proved that nuclear atypia (variations in cell nucleus appearance) and mitotic count (the number of cells actively dividing) were directly related to a patient's prognosis.\nSevere nuclear atypia was found to drastically reduce overall survival, and a low mitotic count, which means that few cells are dividing and spreading, indicated the highest overall survival. Using this information, the researchers developed a three-tier nuclear grade score that divides patients into the following prognostic groups:\n|Grade||Median Overall Survival|\n|Grade 1||28 months|\n|Grade 2||14 months|\n|Grade 3||5 months|\nThe results of this study also showed tumor cell's chromatin, which is a combination of nuclear DNA and protein, was also linked to survival.\nNot only was nuclear grade found to be a simple, cost-effective prognostic tool for determining overall survival, but it also helped predict the time to mesothelioma recurrence among patients treated with surgery. Patients with a low mitotic count averaged 67 months before their cancer returned, and those with a high mitotic count averaged 14 months.\nView our resources for patients and familiesGet Help Today"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:588eaebc-4036-42e9-b61f-87d06fdfe001>","<urn:uuid:809a81ab-16f7-42ec-a82f-56e3e97ecfb7>"],"error":null}
{"question":"What are the barriers to girls' education in poor regions, and how does education benefit girls' long-term prospects?","answer":"In poor regions, girls face multiple barriers to education including harmful gender norms, early marriage, household responsibilities, and lack of nearby schools. For instance, in Iran's poorest province, girls must do household chores, face early marriages as young as 12, and often can't attend distant schools due to cultural restrictions. There's also a shortage of female teachers and inadequate facilities. However, education significantly benefits girls' long-term prospects. It delays early marriage, leads to smaller families, and increases their earning potential - a single year of primary education can increase a girl's wages by up to 20%, while secondary education can increase wages by 25%. Educated women are also more likely to seek healthcare, resulting in better health outcomes for themselves and their children.","context":["Opening up education to girls in Iran’s poorest province\nPeople here live off the land and, without rain, many of their goats have died and their small crops of wheat and other grain have withered. After years of struggling to cope with an inadequate water supply, many farmers were forced to sell their herds of goats, their main source of income, and look for work as manual labourers. Employment is scarce, conditions are unpredictable and the pay is low. In fact, Sistan and Baluchistan has the worst indicators for life expectancy, adult literacy, primary school enrolment, access to improved water and sanitation, infant and child mortality in Iran.\nAll family members are expected to do what they can to bring home income, and this means children are often taken out of school. Girls must do the household chores and look after younger siblings while boys run errands and do odd jobs to earn money.\nAs a result of isolation and poverty, many communities view education as a luxury and cultural attitudes towards women mean that more girls than boys are denied an education.\n“My mother didn’t let me continue my education because she told me I had to work at home,” says 15-year-old Asma Aboos, as she sits cross-legged in her one-room, mud-brick home. “I went to primary school but was not allowed to continue into secondary school. I wash dishes, clean the vegetables, cook, sew and collect water. I wish I could go back to school and become a teacher.”\n“If the secondary school was in the village, I wouldn’t have minded so much,” says Asma’s mother, Bari Khatoum. “But because there is no school nearby, Asma would have had to catch a bus and that is not good.”\nGirls here are not just disadvantaged by a lack of education. Old traditions mean that many of them face the prospect of early marriage (marriage for 12-year-old girls is common and they are powerless to refuse). Once married, their chances of an education decrease even more as their husbands are usually unwilling to let them leave the house unescorted and want them to concentrate on running their new households.\nEven in the few cases in which a girl is able to continue her education there are other problems to overcome. Sistan and Baluchistan is Iran’s poorest province and in this harsh environment with limited resources there is a desperate lack of experienced teachers, especially female ones. The shortage is so acute that in many villages young men are assigned to work as emergency teachers during their military service.\nTo help combat such geographical and gender disparities, UNICEF partnered with Iran’s Ministry of Education to devise a strategy aimed not only at keeping girls in school, but also at trying to develop a more participatory approach to educational activities.\nDozens of female assistant teachers were recruited from the community and trained to teach subjects such as hygiene, basic mathematics and science, literacy, life-skills education, school preparatory activities and storytelling. Training was also given to teachers on how to facilitate peer education, multi-grade classes and activity-based teaching. In order to enhance community participation, weekly after-school classes were held in villages with groups of young girls already enrolled in the school system supervising the activities. The results have been astounding: girls’ enrolment in primary school increased nearly threefold in one year.\n“Now that there are more female teachers, the situation for girls has improved a lot,” says Mehri Maleki Meshkini, a young teacher, who dresses in the traditional black chador. “In our classes we try to discuss serious issues like early marriage, so that the girls become more aware of the situation. But it is difficult because the men in the family decide everything.”\nTraditional attitudes are slowly beginning to change, however, as fathers see how their daughters are engaging in positive activities. Religious leaders have been asked to spread the word at Friday prayers and help transform old traditions into a new source of hope for girls in Iran.","Read our 2022 annual report\nEducation is a fundamental human right. But, for millions of children around the world, this right often goes unacknowledged.\nThis is especially true for girls: According to UNESCO, 132 million girls worldwide were out of school at the beginning of 2020. This was before COVID-19 lockdowns further compromised gender equality in education. Schooling was disrupted for 1.5 billion students, and millions of girls might not return to school. That would be a major setback for multiple reasons.\nGirls’ education: Facts and figures\n- Before the COVID-19 pandemic, UNESCO estimated 132 million girls around the world were out of school\n- The pandemic interrupted education for 1.5 billion students worldwide\n- Adolescent girls (12-17) are the highest risk for dropping out of school in low-income countries\n- About 40% of low-income countries have not taken any measures to support students who are most at-risk for exclusion\n- Three-quarters of all primary-age children who may never set foot in school are girls\n- Women still account for almost two-thirds of all adults unable to read\nWhy is gender equality important in education?\nWe need gender equality in education to ensure that both girls and boys can access and complete programs of study, as well as become empowered equally in and through education.\nLarge gender gaps exist in access to education, learning achievement, and continuing education, usually at the expense of girls. Despite global progress, more girls than boys remain out of school: 16 million girls will never attend school. What’s more, women account for two-thirds of the 750 million adults who are illiterate.\nGlobally, only 66% of countries have achieved gender parity in primary education. At the secondary level, the gap widens significantly, with only 45% of countries achieving parity in lower-secondary education and 25% achieving parity in upper-secondary education.\nWhat are some of the problems encountered by girls in education?\nThere are many barriers to education for both girls and boys, including poverty, geographic isolation, and minority status beyond gender. However, girls face some unique challenges to accessing education.\nHarmful gender norms\nHarmful gender norms result in many inequalities between girls and boys. While gender norms affect all children, they disproportionately affect girls. More than 575 million girls live in countries where inequitable gender norms contribute to violations of their rights, including to education.\nSchool-related gender-based violence and unsafe learning environments\nSchool-related gender based violence (SRGBV) is also a significant issue of gender inequality in education. Girls are often forced to walk long distances to school, increasing their risk of gender based violence (GBV) including sexual harassment, exploitation, and abuse. In the classroom, corporal punishment remains a reality in many schools, impacting overall well-being and also leading to lower attendance rates and higher dropout rates. One of the key challenges with SRGBV is that teachers may not be trained to identify or respond to GBV issues in school. They may also be the perpetrators of such violence. With poor accountability mechanisms, an unsafe learning environment will continue without any recourse.\nInadequate sanitation facilities\nAdolescent girls face additional challenges. Limited access to sanitary products, coupled with a lack of private space to wash, change, or dispose of them makes it more difficult for girls to attend school when they are menstruating. Additionally, shame and stigma around this is one of the main examples of gender discrimination in education, often discouraging girls from attending school when they have their periods.\nTeenage pregnancy and child marriage\nGirls who become pregnant often face significant stigma and discrimination from their communities. In many countries, government policies exclude pregnant girls from attending school. This can lead girls to drop out of school early and not return. Child marriage and education reach similar conclusions, with girls forced into early marriages and then expected to drop out of school to tend the home and start a family.\nWhy is girls’ education important?\nIn the long term, more years of education can break the cycle of poverty by improving gender equality. This can be good for entire countries, but also have significant benefits for individual girls.\nGirls’ education delays early marriage and leads to smaller families\nResearchers from the World Bank and the International Center for Research on Women examined 15 countries in sub-Saharan Africa (one of the regions of the world that is least conducive to education for girls). They found a strong relationship between education and child marriage. Each additional year of secondary education reduced the chances of child marriage.\nThe study also showed that educated women tend to have fewer children and have them later in life. This generally leads to better outcomes for both the mother and her kids, with safer pregnancies and healthier newborns.\nGirls’ education is linked to better health\nUSAID found that girls with a basic education are generally more aware of safe sex. With a comprehensive sexual health curriculum, they are three times less likely to contract HIV/AIDS. Moreover, if all mothers completed primary education, maternal death rates would be reduced by two-thirds.\nEducation also helps students to develop skills like critical thinking, decision-making, and responsibility. Educated girls are also more likely to seek healthcare for themselves and their families. This is why the children of educated mothers are twice as likely to survive past the age of five.\nGirls’ education increases their earning potential…\nEducated women are more likely to work and even own their own businesses and generally earn higher incomes throughout their lives. According to UNESCO, a single year of primary education can increase a girl’s wages later in life by up to 20%. An extra year of secondary school can increase their wages by up to 25%. What’s more, women invest up to 90% of their income back into their family, compared to the average 30–40% that men invest back into their households.\n…as well as that of their country’s\nIt’s not just a girl or woman’s immediate family that benefits from her quality education. Studies have shown that, when 10% more girls in a given country attend school, that country’s gross domestic product increases by an average of 3% — which can make a huge difference in a developing economy.\nHow does education promote gender equality?\nGender inequality is both a cause and consequence of inadequate or low-quality education.\nGender-equitable education systems empower both girls and boys to develop life skills that help young people succeed, including self-management, communication, negotiation, and critical thinking. And both boys and girls benefit from this approach.\nEducation also empowers girls and women to know and fight for their rights, and protects them from harmful gender practices, including child marriage and female genital mutilation (FGM). For men, education — especially gender-equitable education — helps them to be more equitable with the women in their lives. Educated men are also more likely to seek mental health support and less likely to abuse drugs and alcohol or engage in other high-risk behaviour.\nGirls’ education: Concern’s work\nConcern’s work is grounded in the belief that all children — regardless of gender — have a right to a quality education. We integrate our education programmes into both our development and emergency work to give children living in extreme poverty more opportunities in life and supporting their overall well-being. Concern has brought quality education to villages that are off the grid, engaged local community leaders to find solutions to keep girls in school, and provided mentorship and training for teachers.\nOur education programmes are designed to be gender-transformative, in keeping with the core values of all Concern programming. In addressing the unique barriers that each community faces to education, we also look at the barriers faced by girls within those communities, including lack of sanitary facilities, SRGBV, and other harmful gender norms that keep girls at home."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:18428e24-86fe-403b-b1b7-c7a622e8cacf>","<urn:uuid:c159354c-ffad-4e22-adbc-dfc86fbbc3fe>"],"error":null}
{"question":"I'm planning a trip to Crete and interested in exploring historical sites. Can you explain what makes the Palace of Knossos significant in Minoan civilization, and what are the unique features of the Toplou monastery that reflect Crete's turbulent history?","answer":"The Palace of Knossos was the center of Minoan civilization and served as King Minos's palace. This massive complex covered 20,000 square meters and contained 1,400 rooms, with specific sections dedicated to ceremonies, administration, and public storage. As for Toplou monastery, it's a 15th-century fortress-like structure that played a significant role in Cretan history. The monastery is fortified with 10-meter high square walls and covers 800 square meters across three floors, including cells, guest houses, kitchens, and warehouses. It has endured multiple attacks and devastations throughout its history, including plundering by Knights of Malta in 1530, abandonment during Turkish rule, and the execution of its abbot and monks during German occupation in 1941-44. Today, it houses an important collection of Byzantine icons, ancient engravings, and historical artifacts.","context":["The most magnificent collection of Minoan art and culture in the world, unique in beauty and completeness is housed in this museum. The exhibiton of the museum is organized in chronological order, ranging from the Neolithic period to the Roman era (4th century A.D.) and geographically, according to the provenance of the finds.\nThe famous Palace of king Minos and the centre of the Minoan civilisation 5km south of Iraklion. The Great Palace covered an area of 20.000 sq. meters and had 1.400 rooms. Every section of the Palace had a specific use. In the west side of the Palace were the chambers of the ceremonies, of the administration and of the public storehouse...\nThe archaeological site, the palace, the findings - The Festos Disc. According to mythology, Phaistos (or Festos) was the seat of king Radamanthis, brother of king Minos. It was also the city that gave birth to the great wise man and soothsayer Epimenidis, one of the seven wise men of the ancient world.Excavations by archaeologists have unearthed ruins of the Neolithic times (3.000 B.C.).\nLocated in the valley of Messara, Gortys or Gortyn (GR: Γόρτυς or Γόρτυνα) is a must visit for all visitors to Crete. It was inhabited during Bronze Age times, but its rise to glory came almost a millennium after the downfall of the 'Minoans'. Gortyn was a prosperous city from around the middle of the 5th century BC through to the early 9th century AD, when it was finally destroyed by the Saracens (824AD), never to be rebuilt.\nThe Palace of Malia, which covered an area of 7,500 sq.m. , was the third- largest of the Minoan Palaces and is considered the most \"provincial\" from the architectural point of view. The first Palace was built in 1900 BC and destroyed in 1700 BC when a new Palace was built. Following the fate of the other palaces in Crete it was also destroyed in 1450 BC. and the present ruins are mainly those of the new palace.\nThe cave of Psychro is one of the most important cult places of Minoan Crete. The excavators and several scholars identify the cave as the famous \"Diktaian Cave\", where Zeus was born and brought up with the aid of Amaltheia and the Kouretes, and which is connected with myths as this of the seer Epimenides who \"slept\" here, or the coupling of Zeus with Europa.\nThe \"Royal Villa\" at Ayia Triada which is situated very close to Phaistos, was built in about 1550 BC. i.e. just before the new palace at Phaistos, and was destroyed by fire in l450 BC, like all other important Minoan centres. It succeeded the first palace at Phaistos as the economic and administrative centre of the regions depriving the new palace there of this role, and appears to have had connections with Knossos. The two wings, with an open-air space between them, consisted of groups of interconnecting rooms (polythyra), storerooms and stairways. On the site of the ruins, a Mycenaean megaron, the so-called \"Agora\" and an open - air shrine were subsequently built. In the villa's disaster layer from the fire in 1450 BC, excavation revealed a valuable group of exceptional works of art, precious materials, records in Minoan script and seals. The famous black serpentine vessels, the \"Harvesters' Vase\", the \"Boxers' Vase\" and the \"Chieftain ‘ s Cup\", the wall paintings depicting the natural landscape, the sarcophagus, the bronze and clay figurines of worshipers and the copper ingots from the Treasury are among the most noteworthy findings.\nThe history and culture of Crete, from the first centuries of the Christian era to our present time. An exceptional museum featuring a collection of extremely precious objects, a must see for every visitor to Crete. The museum is housed in a two storey neoclassical building, which was constructed in 1903 on the site of an earlier mansion.\nThe Monastery of Arkádi (GR:Αρκάδι) built during the last Venetian period, it consists of a large set of fortress-like buildings. The main building included the cells, the warehouses where the agricultural products were treated and stored, the stables. In a word, it was a well-equipped little fortress where people could find refuge in times of trouble. There is an impressive church, with two naves dedicated to Saint Constantine and Saint Helen, and to Our Lord. Due to the holocaust it suffered in 1866, Arkadi has become the island's most famous monastery.\nThe museum is housed in the katholikon of the Venetian monastery of St. Francis. During the period of the Turkish occupation it was the Muslim mosque of Yussuf Pasha, while in modern times it was used as a cinema or a storehouse for military equipment. Since 1963 it has been functioning as the Archaeological Museum of the city. Apart from the permanent exhibition, the museum houses temporary exhibitions in the frame of certain local events 25 Chalidon Str., tel. +30821 90334 It contains impressive finds from the excavations of the ancient city of Kydonia, from Idramia, Aptera, Polyrinia, Kissamos, Elyros, Irtakina, Syia, Lissos, Chania, Axos, and Lappa.","Ad blocker interference detected!\nWikia is a free-to-use site that makes money from advertising. We have a modified experience for viewers using ad blockers\nWikia is not accessible if you’ve made further modifications. Remove the custom ad blocker rule(s) and the page will load as expected.\nMoni Toplou (Greek: Μονή Τοπλού) is a 15th century monastery located in a dry and barren area in the Prefecture of Lasithi, on the eastern part of the island of Crete in Greece. It is about 6 km north of the village of Palekastro and 85 km east of Agios Nikolaos. The monastery was originally called Panagia Akrotiriani (Virgin Mary of the Cape), after the nearby Sidero cape. Its current name literally means \"with the cannonball\", thus called by the Turks for the cannon and cannonballs (Turkish: top) it had in its possession for defensive purposes.\nToplou monastery is one of the most significant monasteries in Crete, dedicated to Panagia (Virgin Mary) and St. John the Theologian. It was founded around the mid 15th century, probably on the ruins of an earlier convent. The monastery was plundered by the knights of Malta in 1530 and shattered in 1612 by a strong earthquake. Due to its strategic position, the senate of Venice, then ruler of Crete, decided to financially aid in rebuilding it. The monastery flourished until the surrender of eastern Crete to the Turks in 1646, after which it was abandoned for a long time. In 1704, it acquired special protection privileges from the Patriarch (i.e., stauropegic) and was re-inhabited. After its monks were slaughtered by Turks in 1821 during the Greek Revolution of Independence, Toplou was again deserted until 1828. In 1866, during the massive Cretan revolt against the Turks, it was once again devastated. During the German occupation of 1941-44, Toplou was providing shelter to resistance fighters and housed their wireless radio. When this was discovered by the Germans, the abbot and two monks were tortured and executed.\nHaving to defend itself from pirates and invaders, Toplou monastery is heavily fortified, being laid out around a courtyard paved with sea pebbles and surrounded by strong, 10 meters high square walls. In its present form it extends to about 800 square meters in three floors, divided into cells, guest houses, kitchens and warehouses. The main church (katholikon) is built as a two-nave basilica and the belltower dates back to 1558.\nDespite its turbulent history, Toplou has many works of art to its possession. Today, it hosts an interesting exhibition of Byzantine icons, books and documents, a display of ancient engravings and a collection of artefacts which reflect its role in the historical events that influenced Crete during the last centuries. Several of its walls are also adorned with remarkable fresco paintings.\nThe Cavo Sidero dispute\nUK-based Minoan Group (formerly Loyalward Group Plc) plans a €1.2bn construction project on the 25.9 km2 Cavo Sidero peninsula that is located in the northeastern part of Crete. This land is owned by Toplou monastery and is leased for 80 years. Backed by strong political support, the so-called Cavo Sidero project is advertised as one of the largest tourist investments in Greece. It includes the construction of six tourist villages with 7,000 beds, three golf courses, a conference center a marina plus sport facilities. However, the Cavo Sidero peninsula is a Natura 2000 designated area of particular biodiversity and archaeological importance and home to the Vai natural palm forest, the largest of its kind in Europe. On top of that, it is one of the driest areas on Crete and the large amounts of water that would be required by the developments when in operation will have a tremendous negative impact on the environment,. Thus, despite the investors assurances that the project has been designed to operate according to the principles of sustainable development, there has been strong opposition against it by the local population and several environmental groups, including WWF. Serious doubts about the intentions and the financial strength of the investors have also been raised.\nIn April 2009, the Supreme Administrative Court accepted the request of about 300 Sitia residents who sought to annul the ministerial decree of 2007, which adopted the environmental impact study for the project. According to the Court, the environmental licensing of the project was not legitimate since the land use planning foresees only a mild tourist development for the area.\n- ↑ Harsh course of nature, The Guardian, 5 March 2008\n- ↑ Trying to Be Green, With Very Little Water, The NY Times, 19 August 2007\n- ↑ Save the Cretan Landscape (online petition)\n- ↑ Επενδύσεις στην άμμο (Investments on sand), Ο Ιός της Ελευθεροτυπίας, 15 Ιουλίου 2007\n- ↑ «Mπλόκο» από ΣτE στα έργα της Μονής Τοπλού, Το Βήμα OnLine, 10 Απριλίου 2009\n|Wikimedia Commons has media related to: Moni Toplou|"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:12b366c5-f2c5-42be-b1da-c049f1a2f1b0>","<urn:uuid:1c74d62d-3959-4078-84e3-0c314ec12a0b>"],"error":null}
{"question":"What are the key differences between snorkeling at the Great Barrier Reef and Hanauma Bay?","answer":"The Great Barrier Reef offers warm, clear waters with coral gardens that can be enjoyed year-round, while Hanauma Bay is a protected lagoon in an ancient volcano crater. The Great Barrier Reef has rich coral gardens that should not be touched or damaged, as even small breakages can take decades to regrow. In contrast, Hanauma Bay has rocky seabeds with little coral, especially inside the lagoon. Both locations offer organized tours and safety supervision, but Hanauma Bay charges entrance fees and has limited parking, whereas the Great Barrier Reef focuses more on guided tours and educational briefings in multiple languages.","context":["The easiest way to enjoy the splendour of the Great Barrier Reef is simply to snorkel in its warm, clear waters. Lying on the surface, watching the reef society go about its daily business below is both intriguing and relaxing. Everyone can enjoy snorkelling these beautiful coral gardens year-round.\nHere are a few tips on how to enjoy snorkelling safely whatever your age or swimming ability.\nNew to Snorkelling?\nAt Barrier Reef Australia, we have a saying; “If you can breathe through your mouth, you can snorkel, it is just a matter of not breathing through your nose. If you are a snorer then you are a fully qualified snorkeller for sure!”\nIf you are new to snorkelling, however, it’s best to go on an organised tour. All tours give either an entertaining “How to Snorkel” briefing or present an instructional video, on the cruise out to the snorkelling sites. They show you how to use the mask, snorkel and fins and what floatation devices, such as foam noodles and life jackets can be used to help you float effortlessly above the reef. This is the best time for you to put your hand up and ask any questions that you may have.\nAll tour boats have staff of many different nationalities, so there is usually someone who can explain the briefing in your own language if there’s something you don’t understand. Their staff can also help you with any particular concerns you might have regarding any past or current medical or health issues, or your swimming ability and fitness level.\nIf you’re not planning to join a tour, simply ask for advice wherever you get your gear. They can advise on how to check your mask fits, how to use the snorkelling gear and where best to snorkel.\nSnorkelling is easy\nYou’ll enjoy snorkelling more when you have mastered the basic principle: lying flat on the surface, legs straight and finning gently to keep moving slowly along the reef. Look down and slightly ahead and simply watch the underwater world unfold before you.\nIf you want to learn more about the marine life that inhabits the reef – and learn a little about the ‘reef society’ – go on an organised tour and join the guided ‘snorkel tour’ given by the tour staff.\nAlternately you can buy a fish and coral identification slate and take it into the water with you. See how many species you can identify.\nAdditional Safety Tips\n- Always stay within agreed or designated snorkelling areas. Snorkelling can be mesmerising – so remember to look around on the surface periodically and reorientate yourself to your entry point.\n- If you are not snorkelling on an organised tour, always snorkel with a buddy – you can keep an eye on each other and it’s more fun to share your underwater discoveries.\n- Whilst snorkelling is not considered a strenuous activity, it’s best to discuss any medical concerns you may have with the resort water sports staff or onboard crew so that they can advise you and ensure you enjoy your day in total safety and comfort.\n- They can also help you with any particular concerns you have regarding your past or current medical or health issues, your swimming ability or fitness level, or what you might encounter in the water.\n- Do not touch any of the marine life. Most marine species are coated in a protective layer of mucous and rubbing this off leaves them exposed to infection. More importantly, many creatures including the corals of the reef itself, have a touch-sensitive stinging defence mechanism.\n- If the water above a section of reef is too shallow don’t risk getting stranded – it’s impossible to fin backwards. Keep the corals at least at arm’s length – that’s the perfect distance to observe the marine life without frightening it away.\n- If you are not so competent about your swimming ability – take a floatation device with you. That way you can stay horizontal and on the surface. Remember to keep your fins close to the surface so you can avoid damaging any of the fragile corals – even accidentally.\n- Note that it is illegal to damage or remove anything from the Great Barrier Reef, and even the smallest breakages can take decades to regrow.","With its calm and protected turquoise waters, only a few minutes from the road for Honolulu, Hanauma Bay is the main snorkeling destination on the island of Oahu. Despite some drawbacks –among which are entrance fees and large visitor numbers– the lagoon, nestling in an ancient volcano crater, is ideal for beginners, who can observe diverse underwater life in good safety conditions.\nHanauma Bay is about 12 miles (20km) east of Waikiki. Take the Freeway H1 east, which later becomes the Kalanianaole Highway. The site is very well signposted from the main road. Try to arrive early, as parking places are limited ($1 per vehicle) and are very soon taken. It is also easy to get there by public transport. Rather than taking the shuttle service run by the Park, which is quite expensive (from $22 per person for a round-trip ticket), you can take the TheBus line 22 bus from Waikiki, which costs $2.75 for a one-way ticket and will drop you off in front of the Park in around 40 minutes.\nYou can enter the water anywhere along the beach, but you should avoid the area across from the channel if you are a beginner, as the current can be quite strong. The information center abounds in tips for exploring the site (circuits, the different areas, descriptions of the species of fish and shellfish, etc.), so make the most if it.\nThe spot offers a number of areas to explore. While beginners will want to stay in the natural pools between the beach and the reef (↕3-7ft/1-2m), experienced snorkelers will prefer (if the sea conditions permit) to swim to the channel and explore the areas beyond the reef, where the waters are deeper (↕+20ft/+6m) and the seabed is of a better quality. This is where you have the best chance of seeing green sea turtles, but they are less easy to observe here than in a number of other spots in the archipelago.\nIn winter and when the sea is rough, the currents and waves can be strong in the channel (well indicated by buoys). Visibility, in fact, is not always ideal. Lifeguards supervise the spot during the site’s opening hours, and they will quickly issue a reminder over their megaphones if you fail to respect the safety rules.\nUnder the water, you are sure to spot parrotfish, surgeonfish and triggerfish, and a very large number of equally colorful fish, some of which can only be found in Hawaii. The seabeds are rocky and have little coral, especially on the part inside the lagoon.\nThe Park has a wide range of restaurants, but prices are high. You can take your own picnic.\nSea turtles are a very familiar sight on many snorkeling spots in Hawaii, including Hanauma Bay. In order to be a responsible snorkeler, be sure to respect the following rules when observing them:\nThese snorkeling spots are accessible to beginners and kids. You will enter the water gradually from a beach, or in a less than 3ft. deep area. The sea is generally calm, shallow, with almost no waves or currents. These spots are usually located in marked and/or monitored swimming areas. It is not necessary to swim long distances to discover the sea life.\nThis level only apply when the spot experiences optimal sea and/or weather conditions. It is not applicable if the sea and/or weather conditions deteriorate, in particular in the presence of rough sea, rain, strong wind, unusual current, large tides, waves and/or swell. You can find more details about the definition of our snorkeling levels on our snorkeling safety page.\nYou must be logged in to post a comment.\nSnorkeling spots are part of a wild environment and their aspect can be significantly altered by weather, seasons, sea conditions, human impact and climate events (storms, hurricanes, seawater-warming episodes…). The consequences can be an alteration of the seabed (coral bleaching, coral destruction, and invasive seagrass), a poor underwater visibility, or a decrease of the sea life present in the area. Snorkeling Report makes every effort to ensure that all the information displayed on this website is accurate and up-to-date, but no guarantee is given that the underwater visibility and seabed aspect will be exactly as described on this page the day you will snorkel the spot. If you recently snorkeled this area and noticed some changes compared to the information contained on this page, please contact us.\nThe data contained in this website is for general information purposes only, and is not legal advice. It is intended to provide snorkelers with the information that will enable them to engage in safe and enjoyable snorkeling, and it is not meant as a substitute for swim level, physical condition, experience, or local knowledge. Remember that all marine activities, including snorkeling, are potentially dangerous, and that you enter the water at your own risk. You must take an individual weather, sea conditions and hazards assessment before entering the water. If snorkeling conditions are degraded, postpone your snorkeling or select an alternate site. Know and obey local laws and regulations, including regulated areas, protected species, wildlife interaction and dive flag laws."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:5f3842ed-33d1-48fc-b27d-9cd04b547a3b>","<urn:uuid:91df5daa-731c-4f5c-9803-caa813d8f1e6>"],"error":null}
{"question":"Can you compare the notable batting achievements of Joe Sewell and Al Kaline during their respective careers?","answer":"Joe Sewell and Al Kaline had contrasting but impressive batting achievements. Sewell was known for his extraordinary plate discipline, striking out only 114 times in his entire 14-year career, with just 48 strikeouts in his last nine seasons over 5,540 plate appearances (a remarkable 0.87% strikeout rate). Meanwhile, Kaline demonstrated early batting excellence by hitting over .400 well into May of 1955 and winning the league batting title that year with a .340 average at just age 20, marking his first full season as the Tigers' right fielder.","context":["60 years ago today, one of the best AL players of his generation debuted—Al Kaline. One of the few people to play for 20 seasons and never change franchises, for a generation of fans, Al Kaline became synonymous with the Tigers.\nHis debut on June 25, 1953 came under somewhat coerced reasons. You see, he was a very green 18-year-old who had just graduated from high school earlier the month and signed with the Tigers for the then-princely sum of $35,000 less than a week earlier.\nBy rules at the time, that $35,000 signing made Kaline a “bonus baby,” and baseball had its Bonus Rule back in those days. The rule stated that all amateurs signed for more than $4,000, the team would have to keep the player on the 25-man roster for two full seasons.\nThe purpose was simple—keep signing costs down by punishing teams who engaged in big bonuses. They’d have to put someone who wasn’t ready for the majors on their roster, essentially turning a 25-man roster into a 24.5-man roster, at best.\nEventually, the big leagues got rid of the rule and instead adopted the amateur draft. The problem with the bonus rule was that it hurt the development of the bonus babies, and since those were the biggest and most talented prospects in the game, a rule that hinders their progression is clearly a rule with some flaws in it.\nAnyhow, just a few weeks after taking classes like any other high school student, young Al Kaline made his big league debut. He didn’t start for Detroit, but took the field as a mid-game replacement in the sixth with the Tigers trailing the A’s, 5-0.\nStationed in center, Kaline wasn’t called on to make any defensive plays at all. He came to the plate in the ninth, and flew out to end his night. It wasn’t too exciting, but that’s how things typically went for bonus babies, who wouldn’t be on the roster if it wasn’t for the rule.\nKaline would appear in just 30 games on the season, and start just four—all of which came in September. The next year he became the starting right fielder, a slot he’d fill for the next two decades. Kaline wasn’t that good in as a 19-year-old in 1954, but again—he was only on the roster because of the Bonus Rule’s two year stipulation.\nThings changed completely in 1955. He had a hot start, hitting over .400 well into May. By the time the two-year limit had been reached and he could be sent to the minors, there was no reason to do so. Thus Kaline became one of the only players in modern times to never play in the minors. He ended 1955 with a league best .340 batting average for manager Bucky Harris and the Tigers. And the rest is history.\nBut the first step in Kaline’s Hall of Fame career was his debut—and that debut came 60 years ago today.\nAside from that, many other baseball events today celebrate their anniversary or “day-versary” (which is something that happened X-thousand days ago) today. Here they are, with the better ones in bold if you’d rather just skim.\n1,000 days since PBS first airs Ken Burns’ “Baseball: Bottom of the 10th Inning.”\n2,000 days since Oakland trades Nick Swisher to the White Sox for three players.\n3,000 days since Eddie Miksis dies.\n4,000 days since Ichiro Suzuki has his first multi-home run game in North America.\n4,000 days since Barry Bonds belts his 500th double.\n5,000 days since the Mets win Game Five of the 1999 NLCS over the Braves in 15 innings. It’s one of the greatest games in history, culminating in a bottom of the 15th inning comeback and Robin Ventura’s walk-off grand slam single.\n6,000 days since the Royals sign past their prime veterans Terry Pendleton and Lee Smith. Pendleton will spend his last season with the Royals. Smith will never pitch for them—or for anyone else ever again.\n8,000 days since Hall of Fame reliever Rich Gossage suffers his 100th career loss. His record is 117-100.\n8,000 days since Jack Clark hits three home runs in one game for Boston.\n1881 George Gore steals seven bases in one game.\n1885 George Strief sets a record that still stands by belting four triples in one game.\n1888 Jumbo Davis makes five errors in a game for the Kansas City Cowboys. He’ll end the year with 100 errors in 628 chances.\n1896 The Cleveland Spiders make a complete farce of a game. They so badly intimidate rookie umpire Stump Weidman that authorities will later arrest eight Spider players.\n1898 Red Ehret, decent 1890s pitcher, appears in his last game.\n1903 Ed Delahanty plays in his last game. He’ll be dead in about a week.\n1903 Wiley Piatt becomes the only pitcher since 1900 to lose two complete games in one day. Pitching for the Braves, he loses 1-0 and 5-3 to the Pirates, despite allowing just 14 hits in 18 innings.\n1906 First baseman Joe Kuhel is born.\n1908 Larry Gardner, star infielder, makes his big league debut.\n1912 The Boston Braves purchase minor league shortstop Rabbit Maranville from New Bedford of the New England League for $1,000. It’s the beginning of his Hall of Fame major league career.\n1915 Babe Ruth, just a young pitcher, becomes the second person to ever homer into Fenway Park’s right field stands.\n1918 Babe Ruth becomes the second person in AL history to homer in four straight games. (Bill Bradley did it back in 1902).\n1918 Hall of Fame first baseman Jake Beckley dies.\n1921 Yuck. The Yankees make six errors in the first inning versus Washington.\n1925 Lou Gehrig’s second career homer is the first of 10 inside the park shots.\n1928 Hall of Fame southpaw Eppa Rixey suffers his 200th loss. His record is 222-200 so far, but 136-100 since his 100th loss.\n1928 Hall of Fame third baseman Fred Lindstrom ties a big league record with nine hits in one doubleheader.\n1931 Oops. With runners on the corners, Dodger pitcher Dazzy Vance spends so much time throwing to first base to try to pick off the runner, that he completely forgets about lead runner George Watkins, who scores during Vance’s preoccupation. That proves to be the only run in a 1-0 Cardinals win.\n1932 Hall of Fame pitcher Lefty Gomez wins his 11th straight decision, his longest stretch. He’s 14-1 on the year so far.\n1934 Normally it’s nice when a pitcher it responsible for five straight strikeouts, but that isn’t the case today for Yankee hurler Johnny Broaca. His Ks come at the plate when he tries to hit. It’s OK, though. In the same game teammate Lou Gehrig hits for the cycle.\n1934 The Cardinals select aging former star pitcher Dazzy Vance off of waivers from the Reds.\n1937 Switch hitter Augie Galan becomes the first NL batter and second person overall to homer from both sides of the plate in one game.\n1938 Red Ruffing wins, improving his career record to 181-180. From now on, it’ll always be over .500.\n1939 They call him “Swish.” The Cubs purchase Bill Nicholson from the Senators for $35,000.\n1945 Brooklyn signs free agent Joe Medwick, whose career is winding down.\n1949 Former star slugger Buck Freeman dies.\n1949 Gil Hodges hits for the cycle.\n1950 Ralph Kiner hits for the cycle, going 5-for-6 with two home runs, and a personal best eight RBIs, helping the Pirates top the Dodgers, 16-11.\n1953 White Sox manager Paul Richards is at his most hyper-managing, as he uses five first baseman in today’s contest against the Yankees. It works, as Chicago wins, 4-2. Ace pitcher Billy Pierce even plays there for two batters.\n1954 Willie Mays legs out the first of six career inside the park home runs.\n1954 After 10 straight game with more than one hit, St. Louis’ Rip Repulski has to settle for just one.\n1959 Alejandro Pena, once led the NL in ERA, is born.\n1960 Tommy Corcoran, good-glove no-hit shortstop from the turn of the century, dies.\n1961 The Orioles and Angels combine to use a then-record 16 pitchers in today’s game. The O’s win 9-8 in 15 innings with each team bringing eight men to the mound.\n1966 Young Astros second baseman Joe Morgan suffers a broken kneecap on a batting practice line drive. He’ll miss 40 games and Houston will lose 28 of their next 31 contests.\n1967 Young pitcher Joe Niekro steals a base in just his 14th major league game. He’ll never steal another one.\n1969 As recounted in Ball Four Pilots pitcher Jim Bouton tries to sell the team some Gatorade. They aren’t interested.\n1970 Aaron Sele, pitcher, is born.\n1970 Bobby Murcer enters today after homering in his last four at bats yesterday. Cleveland manager Alvin Dark is determined to make sure he doesn’t set the record with five straight, and so decides to call all the pitches in Murcer’s first at bat. Murcer pops up on a 3-1 count.\n1971 Mets batter Cleon Jones draws six walks in a doubleheader against the Expos.\n1971 Future Hollywood star Kurt Russell makes his professional baseball debut in the Northwestern League. He singles, doubles, and steals two bases, but his diamond dreams will be dashed by an injury in 1973.\n1972 Slugger Carlos Delgado is born.\n1974 Tigers workhorse Mickey Lolich completes his 11th straight start. He’s 9-2 with a 1.98 ERA in 100 IP.\n1975 Mike Schmidt receives a walk-off walk from Pirates reliever Dave Guisti for a 7-6 Phillies win in 13 innings.\n1976 Toby Harrah has the most memorable day of his career. He hits a walk-off grand slam for an 8-5 Rangers win over Chicago in the first game of today’s doubleheader, and that’s not even the part people remember. Instead, his claim to fame becomes the only shortstop to work both ends of a doubleheader and handle zero defensive chances. Plenty have hit walk-off grand slams, but only Harrah has done that.\n1976 Javier Vazquez, pitcher, is born.\n1976 Mike Phillips hits for the cycle.\n1978 Rick Wise makes baseball history trivia history by becoming the first pitcher to defeat all 26 franchises then in existence. His final victim is Toronto.\n1980 Can’t anyone here pitch? Five Indians pitchers combine to walk 14 Detroit batters, five coming with the bases loaded. Shockingly, Cleveland loses, 13-3.\n1980 Tommy John pitches 10 innings in a complete game loss—yet doesn’t fan a single batter.\n1982 Paul Maholm, pitcher, is born.\n1983 Sabermetric darling Bobby Grich enjoys the last of his 13 multi-home run games.\n1983 Seattle fires manager Rene Lachemann.\n1984 One time Orioles star Ken Singleton gets his 2,000th hit.\n1984 Bill Krueger has one of the worst starts by any pitcher ever. He allows eight runs while recording zero outs.\n1985 The Yankees institute a new rule that bat boys must wear protective helmets during games. This rule is adopted the hard way, after a Butch Wynegar liner hit a batboy in the head.\n1986 D’OH! Oakland’s Ricky Peters makes a dumb mistake. When teammate Jose Canseco walks with Peters on third and another A’s player on second, Peters begins trotting home, thinking the bases are loaded. Nope. He’s easily out, and the A’s lose by one.\n1986 Mark Langston sets a Seattle record (since broken) with 15 Ks.\n1988 Cal Ripken plays in his 1,000th game in a row.\n1988 Major league baseball suspends former wunderkid pitcher Floyd Youmans for failing to comply with his treatment program for his alcoholism.\n1989 The Mets have zero defensive assists in their 5-1 win over the Phillies. Philadelphia fans 13 times, hits 12 fly outs, and the other two are unassisted groundouts to first. Sid Fernandez is the winning pitcher for the Mets, a fact which shouldn’t surprise anyone who remembers the high-King flyballer.\n1990 Soft tossing Scott Erickson makes his major league debut.\n1991 Detroit trades former franchise stalwart pitcher Dan Petry to the Braves.\n1991 Houston releases young Mark McLemore, whose career will recover rather nicely from this.\n1991 Ozzie Smith has his best game ever according to WPA. He’s 3-for-5 with a stolen base and four RBIs in St. Louis’ 10-9 win over the Phillies for a 0.572 WPA.\n1992 Former Seattle star first baseman Alvin Davis plays in his last game.\n1995 Andres Galarraga hits three home runs for Colorado—and in three straight innings, too: the sixth through eighth.\n1995 Dave Weathers has to leave despite throwing five hitless innings after getting hit by a pitch on his pitching hand by opposing hurler Tim Pugh.\n1996 Mark McGwire nails his 300th home run.\n1996 Jason Giambi enjoys his first multi-home run game.\n1997 Chipper Jones hits his first grand slam. His second will come a mere 10 days later. And the third only five days after that.\n1999 St. Louis pitcher Jose Jimenez, of all people, throws a no-hitter for a 1-0 win over Arizona. Randy Johnson fans 14 but is the hard luck loser, the first of four straight starts for him where Arizona will score no runs. The game’s only run is a bottom of the ninth one coming on a broken bat single.\n2000 For the second time in his career, Chipper Jones receives a walk-off walk.\n2001 Baltimore claims Tony Batista off of waivers from Toronto.\n2002 For the first time ever, a major league game pits two Dominican Republic born managers against each other, as Luis Pujols of Detroit and Tony Pena of KC match wits. The president of the Dominican Republic is on hand for the game, which is broadcast all over Latin America.\n2002 Odalis Perez throws his second one-hitter of the year. Not bad.\n2002 Toronto clobbers the Rays, 20-11, for the franchise’s first 20-run outburst since 1978.\n2004 Larry Walker smashes three home runs in a game for the third time.\n2004 For the only time in his career, Robin Ventura takes to the mound. He throws one scoreless inning, allowing just one hit, a Darin Erstad single.\n2005 Houston retires No. 24 for Toy Cannon Jimmy Wynn.\n2006 Tampa Bay uses 32-year-old Mark Hendrickson as their starting pitcher. They didn’t use another starting pitcher that old until 2013, when 32-year-old Roberto Hernandez started for them. Between them, their oldest pitcher was 30 (James Shields throughout 2012 and one start by Jae Weong Seo on his 30th birthday on May 24, 2007).\n2008 Houston suspends pitcher Shawn Chacon after he grabs GM Ed Wade by the neck and throws him to the ground prior to today’s game.\n2008 41-year-old Tim Wakefield and 45-year-old Randy Johnson square off for the oldest duel since 1965, when 59-year-old Satchel Paige faced a normally aged pitcher (Bill Monbouquette, if it matters).\n2010 The Cubs indefinitely suspend volatile starting pitcher Carlos Zambrano after a first inning dugout tantrum in a 6-0 loss versus the White Sox. Zambrano was upset with first baseman Derrek Lee over a grounder.\n2010 Dustin Pedroia hits three home runs in one game.\n2010 Arizona pitcher Edwin Jackson hurls one of the most poorly thrown no-hitters of all-time. He walks eight and throws 149 pitches.\n2010 Roy Halladay first faces his old team, the Toronto Blue Jays. The game is supposed to be in Toronto, but due to security concerns over the G20 summit, this Blue Jays home game is in Philadelphia. Toronto still bats last, though.\n2011 The Giants beat the Indians 1-0 with the only run coming on a seventh inning balk.\n2012 Tampa pitcher Alex Cobb throws a complete game despite allowing eight runs versus the Royals. It is the most earned runs in a complete game since Randy Johnson did it in 1988. Also, it’s a novel promotional night in KC: Water Awareness Night to promote efficient use of water.\n2012 Toronto signs free agent pitcher Jamie Moyer. It doesn’t take.\n2010 Baltimore releases former star infielder Miguel Tejada.\n2012 St. Louis wins an impressive comeback against the Marlins, 8-7 in 10 innings. The Cardinals plate four runs in the top of the ninth and another pair in the 10th before allowing one in the bottom of the 10th.\n2012 A minor league game between Missoula and Helena has maybe the least likely ending of all—a walk-off appeal play! A would-be game-tying homer with two outs in the bottom of the 10th is nullifying because the batter missed a bag when circling the bases. Missoula triumphs, 2-1.","Sunday, January 22, 2012\nJanuary 16-22, 2012 Players with under 5 HR and over 100 RBI in the same season\nQ. Prior to Eddie Mathews, who was considered the greatest third baseman in baseball history?\nHint: He broke up Herb Pennock’s 1927 World Series perfect game bid with an eighth inning single.\nTwint: As a child he took a job checking railroad cars loaded with explosives.\nA. Pie Traynor (Perfect breakup 07-Oct-1927)\nFCR - Gene Rudzinski, Buffalo [Don Harrison, Fairfield, CT the second go-around]\nQ. Who was the first player to have two hits in one inning in a World Series game?\nHint: John McGraw called him “my greatest outfielder.”\nTwint: His customary avoidance of water during workouts in the Texas heat may have contributed to the kidney troubles which eventually took his life.\nA. Ross Youngs (2 H [2B & 3B] in 7th inning 07-Oct-1921)\nFCR - J. R. Richardson, Clarksville, MD\nQ. Which player, known for his keen batting eye, struck out only 114 times in his 14-year career?\nHint: He only struck out 48 times in 5,540 plate appearances (0.87%) over his last nine major league seasons.\nTwint: He won the Southeastern Conference title as head coach at the University of Alabama in 1968.\nA. Joe Sewell\nFCR - Peter Beagle, Oakland\nQ. Who was the first first baseman to turn an unassisted triple play?\nHint: He won the American League MVP Award in his 13th Major League season, while setting the single-season doubles record.\nHint: He led the league in errors (most, not least) among first basemen four times during his career.\nTwint: He gave one of his bats to Joe Sewell on the day Sewell replaced Ray Chapman, and Sewell used the bat (which he nicknamed ‘Black Betsy’) throughout his 14-year career.\nFCR - Steve Bonfield, Calgary\nQ. Which Hall of Famer holds the record for most putouts in a season by a National League second baseman?\nHint: After singling in his first major league at-bat, he knocked himself out by fouling a ball off his head in his second time up.\nHint: Footsie Blair replaced him and hit a home run.\nHint: He got the only hit in the first of Mort Cooper’s back-to-back one-hitters in 1943.\nTwint: His granddaughter is the first lady of Indiana.\nTwint: He was named after a thrice-defeated Democratic Presidential candidate.\nA. Billy Herman (466 PO in 1933; Knockout-to-HR 29-Aug-1931; H [2B] vs. Cooper 31-May-1943 [g1]; granddaughter Cheri Herman Daniels, wife of Indiana Governor Mitch Daniels; given name William Jennings Bryan)\nFCR - Bill Lewers, McLean, VA\nQ. Who hit the first home run of the 1912 World Series, in the second inning of Game Seven?\nHint: His tenth inning sacrifice fly (NOT double) in Game Eight clinched the Series.\nHint: He was the first American League player to attend the University of Vermont.\nHint: He forewent his final year of eligibility to sign with the Red Sox after leading the school to the New England Championship.\nTwint: He was inducted into the Red Sox Hall of Fame in 2000.\nA. Larry Gardner (Bounce HR in the ’12 WS; 1908 New England Champions)\nFCR - Tom Kennedy, Houston\nQ. Whose throw in the 1934 World Series knocked pinch runner Dizzy Dean unconscious?\nHint: According to the headlines that followed, “X-Rays of Dean’s Head Revealed Nothing”.\nHint: In perhaps a bit of payback, he was released by the Cubs when Dizzy Dean was recalled from Tulsa in August 1940.\nHint: He and his infield-mates combined for 462 RBI in 1934.\nTwint: He set an American League record (later tied by Jose Canseco) when he received seven consecutive BB in 1938.\nTwint: His arm went dead after a late-1938 handball injury.\nA. Billy Rogell (Dean knockout 1934 WS Game 4, 4th inning; released by Cubs 28-Aug-1940; 1934 Tigers IF: Greenberg 139 RBI, Gehringer 127 RBI, Rogell 100 RBI, Owen 96 RBI; 7th consecutive BB 18-Aug & 19-Aug-1938 G1)\nFCR - Dave Wise, Hyde Park, NY\nQ. Who was the last player to get a hit in three games in one day?\nHint: His son played briefly for the Pirates during WWII.\nTwint: He moved to the outfield to make way for Pie Traynor.\nA. Clyde Barnhart (triple header 02-Oct-1920: game 1, game 2 and game 3; father of Vic Barnhart; moved to OF in 1922)\nFCR - Dave Serota, Kalamazoo\nQ. Who was the first modern player to collect seven extra-base hits in two consecutive games?\nHint: He led American League first basemen in fielding in 1926, and led National League first basemen in fielding in 1929.\nHint: He was an inaugural inductee into the Pacific Coast League Hall of Fame.\nHint: His son, Bud, played just over 100 career games for the White Sox.\nTwint: He had 3,659 combined major league and minor league hits.\nA. Earl Sheely (3 2B 20-May-1926, 3 2B & HR 21-May-1926 [Ed Delahanty, 1896, is the only prior player w- 7 EBH in 2 games]; 1943 PCL HOF inductee; Bud Sheely 101 games for 1951-53 White Sox; 1,340 ML hits, 2,319 ml hits)\nFCR - John Michael Pierobon, Ft. Lauderdale\nWEEKLY THEME – Player since the end of the Dead Ball Era (1920) with 100 RBI seasons who also hit fewer than five home runs that year.\nPlayer HR RBI Year\nBarnhart 4 114 1925\nBurns 4 114 1926\nGardner 3 118 1920\nGardner 3 120 1921\nHerman 2 100 1943\nRogell 3 100 1934\nSewell 3 109 1923\nSewell 4 106 1924\nSheely 3 103 1924\nTraynor 2 103 1931\nTraynor 3 124 1928\nTraynor 4 108 1929\nYoungs 3 102 1921\nFirst Correct Respondent to Identify Theme – Joe Ullian, Santa Barbara"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:93119e75-7f70-423c-a075-8067453ef213>","<urn:uuid:acb2ebaa-4cfb-4756-9ed1-b3a87260ccb4>"],"error":null}
{"question":"What are the key differences between single neuron experiments and functional imaging techniques in studying brain activity?","answer":"Single neuron experiments and functional imaging differ in several ways. Single neuron experiments use invasive electrodes to measure individual action potentials with very high temporal resolution, allowing observation of individual spikes from single cells. In contrast, functional imaging techniques like fMRI and EEG are typically non-invasive and provide a wider field of view, showing simultaneous activity across the whole brain. However, functional imaging only provides indirect measures of neural activity. For example, fMRI measures blood oxygenation changes related to neural activity, while EEG records electrical voltages from the scalp.","context":["|This article needs additional citations for verification. (May 2008)|\nSensory neuroscience is a subfield of neuroscience which explores the anatomy and physiology of neurons that are part of sensory systems such as vision, hearing, and olfaction. Neurons in sensory regions of the brain respond to stimuli by firing one or more nerve impulses (action potentials) following stimulus presentation. How is information about the outside world encoded by the rate, timing, and pattern of action potentials? This so-called neural code is currently poorly understood and sensory neuroscience plays an important role in the attempt to decipher it. Looking at early sensory processing is advantageous since brain regions that are \"higher up\" (e.g. those involved in memory or emotion) contain neurons which encode more abstract representations. However, the hope is that there are unifying principles which govern how the brain encodes and processes information. Studying sensory systems is an important stepping stone in our understanding of brain function in general.\nA typical experiment in sensory neuroscience involves the presentation of a series of relevant stimuli to an experimental subject while the subject's brain is being monitored. This monitoring can be accomplished by noninvasive means such as functional magnetic resonance imaging (fMRI) or electroencephalography (EEG), or by more invasive means such as electrophysiology, the use of electrodes to record the electrical activity of single neurons or groups of neurons. fMRI measures changes in blood flow which related to the level of neural activity and provides low spatial and temporal resolution, but does provide data from the whole brain. In contrast, Electrophysiology provides very high temporal resolution (the shapes of single spikes can be resolved) and data can be obtained from single cells. This is important since computations are performed within the dendrites of individual neurons.\nSingle neuron experiments\nIn most of the central nervous system, neurons communicate exclusively by sending each other action potentials, colloquially known as \"spikes\". It is therefore thought that all of the information a sensory neuron encodes about the outside world can be inferred by the pattern of its spikes. Current experimental techniques cannot measure individual spikes noninvasively, so electrodes must be used to reveal a neuron's spikes.\nA typical single neuron experiment will consist of isolating a neuron (that is, navigating the neuron until the experimentor finds a neuron which spikes in response to the type of stimulus to be presented, and (optionally) determining that all of the spikes observed indeed come from a single neuron), then presenting a stimulus protocol. Because neural responses are inherently variable (that is, their spiking pattern may depend on more than just the stimulus which is presented, although not all of this variability may be true noise, since factors other than the presented stimulus may affect the sensory neuron under study), often the same stimulus protocol is repeated many times to get a feel for the variability a neuron may have. One common analysis technique is to study the neuron's average time-varying firing rate, called its post stimulus time histogram or PSTH.\nReceptive field estimation\nOne major goal of sensory neuroscience is to try to estimate the neuron's receptive field; that is, to try to determine which stimuli cause the neuron to fire in what ways. One common way to find the receptive field is to use linear regression to find which stimulus characteristics typically caused neurons to become excited or depressed. Since the receptive field of a sensory neuron can vary in time (i.e. latency between the stimulus and the effect it has on the neuron) and in some spatial dimension (literally space for vision and somatosensory cells, but other \"spatial\" dimensions such as the frequency of a sound for auditory neurons), the term spatio temporal receptive field or STRF is often used to describe these receptive fields.\nOne recent trend in sensory neuroscience has been the adoption of natural stimuli for the characterization of sensory neurons. There is good reason to believe that there has been evolutionary pressure on sensory systems to be able to represent natural stimuli well, so sensory systems may exhibit the most relevant behaviour in response to natural stimuli. The adoption of natural stimuli in sensory neuroscience has been slowed by the fact that the mathematical descriptions of natural stimuli tend to be more complex than of simplified artificial stimuli such as simple tones or clicks in audition or line patterns in vision. Free software is now available to help neuroscientists interested in estimating receptive fields cope with the difficulty of using natural stimuli.\nSensory neuroscience is also used as a bottom-up approach to studying consciousness. For example, visual sense and representation has been studied by Crick and Koch (1998), and experiments have been suggested in order to test various hypotheses in this research stream.\n- Budai, D. (Mar 1994). \"A computer-controlled system for post-stimulus time histogram and wind-up studies.\". J Neurosci Methods 51 (2): 205–11. doi:10.1016/0165-0270(94)90012-4. PMID 8051951.\n- Theunissen, FE.; David, SV.; Singh, NC.; Hsu, A.; Vinje, WE.; Gallant, JL. (Aug 2001). \"Estimating spatio-temporal receptive fields of auditory and visual neurons from their responses to natural stimuli.\". Network 12 (3): 289–316. doi:10.1080/net.12.3.289.316. PMID 11563531.\n- Crick, F.; Koch, C. (Mar 1998). \"Consciousness and neuroscience.\". Cereb Cortex 8 (2): 97–107. doi:10.1093/cercor/8.2.97. PMID 9542889.","Functional imaging is the study of human brain function based on analysis of data acquired using brain imaging modalities such as Electroencephalography (EEG), Magnetoencephalography (MEG), functional Magnetic Resonance Imaging (fMRI), Positron Emission Tomography (PET) or Optical Imaging. The aim is to understand how the brain works, in terms of its physiology, functional architecture and dynamics. The framework for the conduct of these studies includes classical techniques of neuroanatomy, neurophysiology, and experimental psychology and the cognitive neurosciences, as well as more theoretical approaches, based on perspectives from computational neuroscience and statistics.\nModern functional imaging has two main advantages over the multi/single-unit recordings used to study the electrophysiology of neurons. The first is that it is generally non-invasive, and is therefore applicable routinely in humans. This allows for the study of unique human attributes such as language. The second is that it can provide a wide field of view. Rather than recording information about a single or small number of neuronal cells, an image may be gathered summarizing simultaneous activity across the whole brain. This provides a different yet complementary perspective on neural coding (see e.g., functional integration, below). A disadvantage, however, is that functional imaging provides only an indirect measure of the quantities of primary interest to neuroscientists e.g., firing rates and membrane potentials. Current research is aimed at bridging this gap using a combination of experimental and mathematical modelling approaches.\nCurrent imaging modalities include the Electroencephalogram (EEG) which records electrical voltages from electrodes placed on the scalp and the Magnetoencephalogram (MEG) which records the magnetic field from SQUID sensors placed above the head. Both MEG and EEG have a high temporal resolution (milliseconds), capable of detecting e.g., the 40Hz Gamma response implicated in object representation (Tallon-Baudry and Bertrand, 1999). Their spatial resolution is, however, usually of the order of centimeters rather than millimeters. This varies a great deal, depending on the nature of the neuronal activity one is trying to localize. It depends in particular on the number of sources that is activated at the time data is recorded. In practice this implies that e.g. for isolating subtle cognitive components, a lower resolution is to be expected, whilst the stronger early components of an auditory response can be localized to within millimeters in the brainstem.\nIn contrast, functional Magnetic Resonance Imaging (fMRI) has low temporal (hundreds of milliseconds or seconds) but relatively high spatial (millimeters) resolution. Increases in neural activity cause variations in blood oxygenation, which in turn cause magnetization changes that can be detected in an MRI scanner. This Blood Oxygenation Level Dependent (BOLD) signal peaks up to 6s after neuronal activity. Moreover, the hemodynamics act like a low pass filter (Logothetis, 2001), smearing out changes in local electrical activity.\nSimultaneous recordings of EEG and fMRI (Ritter and Villringer, 2006) have the potential to localise neuronal activity with both high temporal and spatial resolution. Other important imaging modalities are PET and Optical Imaging. PET's spatial resolution typically falls somewhere between that of fMRI and MEG/EEG. In addition, PET has very low temporal resolution (tens of seconds to minutes) and requires injection of a trace amount of radioactivity. This limits the number of measurements that can be made on any one individual. But a great advantage of PET is that it is particularly useful in the study of brain neurophysiology and neurochemistry e.g., one can image glucose uptake and the activity at serotonin and dopamine receptors, in systems of importance to those studying anxiety, depression and addiction. Optical Imaging  or Near Infrared Spectroscopy (NIRs) can also detect BOLD signals from changes in the amount of reflected light. This is an economical alternative to fMRI but is limited to imaging the cortex.\nFunctional imaging is also closely related to structural imaging, in which MRI is used to provide high resolution images with high contrast between e.g., white matter and gray matter. These detailed anatomical images have recently been complemented with data from Diffusion Tensor Imaging (DTI) which can show the direction of white matter fibres.\nThere are two key themes in the analysis of functional imaging data. They reflect the long-standing debate in neuroscience about functional specialization versus functional integration in the brain (Cohen and Tong, 2001). The first is brain `mapping’ where three-dimensional images of neuronal activation are produced showing which parts of the brain respond to a given cognitive or sensory challenge. This is also known as the study of functional specialization and generally proceeds using some form of Statistical Parametric Mapping (SPM). A classic example here is the identification of human V4 and V5, the areas specialized for the processing of color and motion.\nSPM is a voxel-based approach, employing classical statistics and topological inference, to make comments about regionally specific responses to experimental factors. PET or fMRI data are first spatially processed so that they conform to a known anatomical space, in which responses are characterized statistically typically using the General Linear Model (GLM). For fMRI data the GLM embodies a convolution model of the hemodynamic response. This accounts for the fact that BOLD signals are a delayed and dispersed version of the neuronal response. GLMs are fitted at each voxel and inferences are made about which parts of the brain are active, in a statistical sense. To accommodate the spatial nature of the imaging data (and account for the multiple statistical comparisons made) SPM techniques make use of Random Field Theory (RFT) (see Fig 1) and/or other statistical procedures, e.g., False Discovery Rate.\nThe SPM approach can also be used with structural data to find brain regions containing a higher gray matter density. This is known as Voxel-Based Morphometry (VBM) (Ashburner and Friston, 2000) and has been used, for example, to show that the posterior hippocampus, useful for spatial navigation, is enlarged in taxi drivers.\nFor MEG or EEG, data can be analyzed in sensor space, furnishing a crude spatial mapping of brain function. Functions can, however, be more accurately localized using source reconstruction methods (Baillet et al. 2001). These work by specifying a forward model describing how a current source in the brain propagates to become an MEG or EEG measurement, using Maxwell's equations (http://www.scholarpedia.org/article/Volume_Conduction). These models are then inverted using statistical inference. Data from sensory systems is often analyzed using an averaging procedure. The data immediately following a sensory event, e.g., hearing an auditory tone, is averaged over multiple events to produce an Event Related Potential (ERP). Components of the ERP can then be localized to different parts of the brain. Other cognitive components, however, are not easily isolated using this ERP approach. For these, a time-frequency characterization may be more appropriate (Tallon-Baudry and Bertrand, 1999). See also Makeig et al. 2002 for a recent critique of the averaging procedure.\nThe second theme is ‘functional integration’, where models are used to describe how different brain areas interact. A classic example is the use of models to find increased connectivity between dorsal and ventral visual streams after subjects learn object-place associations. A wide range of statistical techniques are being used to measure inter-regional connectivity. Both unsupervised (e.g., Independent Component Analysis , ICA) and supervised techniques (e.g., support vector machine, SVM) are used. Other models seek to directly measure \"causal\" connectivity based on static, statistical constraints (e.g., Structural Equation Modelling, SEM) or dynamic, more bio-physically motivated assumptions (e.g., Dynamic Causal Modelling, DCM). A challenge for functional integration models is to bridge the gap between the large-scale, statistical models of the whole brain, and the small number of highly constrained spatial regions needed to be able to apply SEM and/or DCM.\nDCM for fMRI uses a forward model in which neural activity generates BOLD signal changes via a `Balloon' model of vascular dynamics. The model is then inverted to provide estimates of changes in connectivity between brain regions. In DCM for ERPs, neural activity is described using neural-mass models, which then give rise to observed EEG or MEG data using Maxwell's equations (see above). Inversion of the model then allows one to make inferences about changes in long-range excitatory connections among different brain areas.\nThe above analysis approaches are implemented in various software packages such as SPM  (SPM is the name of a software package as well as a methodology), FSL , EEGLAB , BrainVoyager , or AFNI . They are also described in a recent textbook (Friston et al. 2006).\nThe applications of functional imaging are diverse and multitudinous. PubMed, for example, returns over 32,000 articles. The functional imaging journal `NeuroImage' classifies research articles under 'Anatomy and Physiology', 'Methods and Modelling', 'Systems Neuroscience' or 'Cognitive Neuroscience'. Additionally a number of applications in clinical and experimental medicine are emerging.\nFunctional imaging has been applied to all systems of the brain; whether visual, auditory, sensorimotor, emotional, memory, language, attention or control. Overviews of research findings are available in recent textbooks (Frackowiak et al. 2003, Gazzaniga 2004). Recent high-profile (and arbitrarily selected) applications of fMRI in these areas include a study of the effect of sleep on human memory performance (Yoo et al. 2007) and a study of the neuronal and cognitive components of altruism (Tankersley et al. 2007).\nImaging is also used for the study of basic brain anatomy and physiology. For example, DTI has recently been used to identify three regions of human parietal cortex based on their connectivity patterns with other brain areas (Rushworth et al. 2006). Imaging is also used clinically: The best established application is the use of fMRI for pre-surgical mapping to localize cerebral functions in tissue within or near regions intended for neurosurgical resection (Matthews et al. 2006).\nAuthors web page: http://www.fil.ion.ucl.ac.uk/~wpenny/\nR.S.J. Frackowiak, K.J. Friston, C. Frith, R. Dolan, C.J. Price, S. Zeki, J. Ashburner, and W.D. Penny (2003) Human Brain Function. Academic Press, UK, 2nd edition.\nC. Frith (2007) Making up the mind: How the brain creates our mental world. Blackwell Publishing.\nK. Friston, J. Ashburner, S. Kiebel, T. Nichols and W. Penny (2006) Statistical Parametric Mapping: The Analysis of Functional Brain Images. Elsevier, London.\nGazzaniga, M.S., Ivry, R., & Mangun, G.R. Cognitive Neuroscience: The Biology of the Mind. W.W. Norton, 2002. 2nd Edition\nM.S. Gazzaniga (2004). The Cognitive Neurosciences III. MIT Press, New York.\n- Jan A. Sanders (2006) Averaging. Scholarpedia, 1(11):1760.\n- Valentino Braitenberg (2007) Brain. Scholarpedia, 2(11):2918.\n- James Meiss (2007) Dynamical systems. Scholarpedia, 2(2):1629.\n- Paul L. Nunez and Ramesh Srinivasan (2007) Electroencephalogram. Scholarpedia, 2(2):1348.\n- Seiji Ogawa and Yul-Wan Sung (2007) Functional magnetic resonance imaging. Scholarpedia, 2(10):3105.\n- Mark Aronoff (2007) Language. Scholarpedia, 2(5):3175.\n- Anthony T. Barker and Ian Freeston (2007) Transcranial magnetic stimulation. Scholarpedia, 2(10):2936.\nJ. Ashburner and K.J. Friston. Voxel-Based Morphometry - The Methods. NeuroImage, 11:805-821, 2000\nS. Baillet, J.C. Mosher and R.M. Leahy (2001) Electromagnetic brain mapping. IEEE Signal Processing Magazine, pages 14-30.\nJ.D. Cohen and F. Tong (2001) The face of controversy. Science, 293, 2405-2407.\nN. Logothetis, J. Pauls, M. Augath, T. Trinath and A. Oeltermann (2001) Neurophysiological investigation of the basis of the fMRI signal. Nature 412, 150-157.\nS. Makeig, M. Westerfield, T Jung, S. Enghoff, J. Townsend, E Courchesne and T. Sejnowski (2002) Dynamic brain sources of visual evoked responses Science, 295, 690-694.\nP.M. Matthews, G.D. Honey and E.T. Bullmore (2006) Applications of fMRI in translational medicine and clinical practice. Nature Reviews Neuroscience, 7, 732-744.\nM.F. Rushworth, T.E. Behrens and H. Johansen-Berg (2006) Connection patterns distinguish 3 regions of human parietal cortex. Cerebral Cortex, 16(10):1418-30.\nC. Tallon-Baudry and O. Bertrand (1999) Oscillatory gamma activity and its role in object representation. Trends in Cognitive Sciences, 3(4), 151-162.\nP. Ritter and A. Villringer (2006) Simultaneous EEG-fMRI. Neuroscience and Biobehavioural Reviews. 30(6), 823-838.\nD. Tankersley, C J Stowe and S A Huettel (2007) Altruism is associated with an increased neural response to agency. Nature Neuroscience 10, 150 - 151.\nS.S. Yoo, P T Hu, N. Gujar, F A Jolesz and M P Walker (2007). A deficit in the ability to form new human memories without sleep. Nature Neuroscience 10, 385 - 392."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:0e6033cc-7d91-4ad7-9df8-52172c8d67ba>","<urn:uuid:9129a73e-9969-48ea-a807-09e74e27c94a>"],"error":null}
{"question":"What are the technical solutions for reducing display latency in gaming, excluding non-hardware fixes?","answer":"For gaming display latency reduction, there are several hardware-focused solutions. For monitor ghosting, upgrading to a gaming-specific monitor with faster refresh rates (90 Hz, 124 Hz, 240 Hz) and better response times (as low as 0.5 milliseconds) can help, as can replacing faulty display cables that might cause distorted and delayed images. For input lag reduction, using HDMI cables is recommended as they minimize acquisition time compared to analog connections like component or composite cables, which require additional time for analog-to-digital conversion. Additionally, gaming monitors with dual core processors can help reduce input lag when multiple processing options are enabled.","context":["Monitor ghosting can be annoying during gameplay. When you are shooting a player in one place, it turns out that they have already moved, and you were shooting at an image of them. This might not be of significance to a regular player but it can affect a professional’s performance and score.\nThere can be several reasons for an individual to experience monitor ghosting, and this article discusses how you can fix it for a better, more enhanced gaming experience.\nWhat is ghosting in Games\nGhosting can be used in multiple contexts. Here, to be specific, the term we are using is monitor ghosting. This refers to the “Ghosted” image one sees when the object on the screen moves quickly. It has a rippling effect, kind of like a blurred tail following a moving object. This occurs when a previous image (frame) continues to appear on the screen when it is not supposed to. You can understand the term “Monitor Ghosting” more clearly by having a look at the image below:\nMonitor ghosting is not only bad for the score but also in terms of the general gaming experience. In simpler words, one can refer to it as lag or delay between the information sent to the screen from the PC and the display quickness.\nWhat good is gaming if you do not have the optimized graphics experience? That is precisely why we have compiled a shortlist of steps for you to follow to fix monitor ghosting.\nHow to fix monitor ghosting\nAs mentioned earlier, there can be several causes of the ghosting effect. Continue reading the article to troubleshoot this issue to have a more refined gaming experience.\nUpdate PC to Windows 10 20H2 or later\nIt might seem odd initially, but Windows 10 20H2 comes with a built-in function to adjust the refresh rate. Updating your PC to this or a later version will ensure that your monitor receives the highest possible frame per second.\nIt would help if you maximized your monitor’s refresh rate so that maximum frames are displayed per second and monitor ghosting is avoided. This means that more images will be displayed in the same amount of time, giving the user a smoother experience.\nOnce you have updated your PC, you can find the refresh rate settings in the following location:\nStart Menu -> Settings -> System -> Display -> Advanced Display Setting\nUpgrade your monitor\nIt might seem like a costly fix, but it would be totally worth it. With the rapidly ever-growing technology, there are now monitors designed especially for gaming purposes. They offer faster refresh rates, such as 90 Hz, 124 Hz, 240 Hz, etc., and their response time is also better than the regular monitors.\nResponse time is how fast the monitor updates the image. In technical terms, it is time a pixel takes to change its color. The time is measured from when a pixel turns to white from black, and then again to black. There are currently monitors in the market that have a response time of as low as 0.5 milliseconds.\nTherefore, upgrading to a newer monitor might be the answer for you. However, if you already have an advanced gaming monitor and are still experiencing ghosting, you may need to continue down the article for more troubleshooting steps.\nCheck/replace your display cable\nOften than usual, faulty display cables are the cause of distorted and delayed images. You need to ensure that your display cable is not internally damaged or very old. Copper running through these wires might get damaged, or the connectors may have rust and carbon, ruining the end display results, and hence your gaming experience.\nCheck the display software\nMost monitors and GPUs come with their own software for support. Note that these are not drivers but dedicated software to manage how your hardware responds. You need to ensure that the software is up to date and that G Sync and FreeSync are enabled. The function of these is to reduce screen stuttering and tearing triggered by the monitor when it is not in sync with the frame rate.\nUpdate your hardware drivers\nDrivers are required to run the hardware. Without a compatible driver, your hardware is just another piece of metal. It is important to keep your GPU and monitor drivers up to date to provide the best performance. Outdated drivers can also be a reason for monitor ghosting.\nYou can find the associated drivers from the vendor’s website.\nAs a gamer, we would not want to experience ghosting even if we didn’t own the expensive gaming setup. Therefore, you don’t need to own a high-end GPU and monitor to get the most out of it.\nYou can often fix monitor ghosting by ensuring that both your current software and hardware are updated and intact, respectively.\n- How To Check If Your Computer Supports DirectX 12 Ultimate\n- What is VSync and Should I Turn it On or Off?\n- Download Latest Nvidia Drivers To Keep Your System Up To Date\n- How To Easily Calibrate Windows 10 Monitor Colors, Brightness And Saturation\n- 3 Ways To Change Screen Resolution In Windows 10","A television’s input lag is the amount of time that elapses between a picture being generated by a source and that image appearing onscreen. When gaming, you’ll experience this as the time between making an input and seeing the reaction appear onscreen. It’s only important for gamers, and even then, different gamers will have different sensitivity to lag.\nWe record the lowest input lag time of which a TV is capable, the amount of lag present when motion interpolation is enabled, and the amount of lag a TV has when using our calibrated settings.\nNote: Do not confuse the input lag time with the response time. The response time is the time it takes a pixel to shift from one color to another, which is significantly shorter than the input lag time. Response time is related to motion blur.\nFor older models, see our results for 2013 and 2014 TVs.\nWhen it matters\nInput lag only matters for playing video games, either on a console or on a PC.\nWith fast-paced games like shooters and fighting games, quick reflexes are key. Lower input lag can mean the difference between a well-timed reaction and a move that takes too long to register and ends up countered by the opponent before it can ever be performed. This lag doesn’t matter for watching movies, though, so unless you’re a gamer and are worried about PC peripheral lag, or Nintendo, Xbox One, or PS4 controller input lag (or other controllers too), you have nothing to worry about. Most people will not notice under 50 ms of input lag while more competitive gamers should look for a TV that can do below 40 ms. Almost everyone will also find anything over 100 ms terrible to play with.\nThis input lag test represents the lowest lag a TV is capable of achieving. This is the amount of lag that is best for gamers, and is pretty important for most fast-paced, competitive games.\nWe use Leo Bodnar’s input lag checker to perform this test, as it provides an accurate, continuous measurement of a TV’s input lag. To get the lowest amount of lag on most TVs, it’s necessary to enable game mode. On some, though, special steps are required, which we list in a section lower down.\nInput lag with interpolation\nThis input lag time represents the lowest amount of lag a TV can get with the motion interpolation feature (soap opera effect) turned on. If you want to increase the frame rate of videos by adjusting the TV’s settings, this is a test you should care about. Just keep in mind that this setting will usually have significantly higher input lag than the TV’s minimum, and so isn’t great for competitive games (it works well for most RPGs and turn-based games, though).\nWe use the same testing process as for the first test, only instead of enabling game mode, we enable motion interpolation at its highest setting.\nOn most TVs, this isn't playable for fast games. Some people chose to live with the higher input lag in order to get the smoother motion. Learn more about motion interpolation.\nInput lag outside game mode\nOur ‘outside game mode’ measurement represents the amount of input lag that is present when a TV uses our posted review settings – no game mode, no motion interpolation. Game mode disables many of the picture options a TV has, so this test is useful for people who want to play games with all of the TV’s settings available to them.\nWe use the same testing process as for the first test, only we don’t enable any settings other than what is used by our test calibration.\nHow input lag is measured\nInput lag is not an official television specification because it depends on two varying factors: the type of source and the settings of the television. The easiest way you can measure it is by connecting a computer to the TV and displaying the same timer on both screens. You can find a timer here. Then, if you take a picture of both screens, the time difference will be your input lag. This is, however, an approximation, because your computer does not necessarily output both signals at the same time. In this example image, an input lag of 40ms (1:06:260 – 1:06:220) is indicated.\nIn our tests, we measure input lag using a dedicated device made just for this purpose: the Leo Bodnar tool. This is a lot more accurate than the two screens method.\nWhy there is input lag on TVs\nThere are three main functions that delay the television: acquiring the source image, processing the image, and displaying it.\nAcquisition of the image\nThe more time it takes for the TV to receive the source image, the more input lag there will be. With modern digital TVs, using an HDMI cable will allow you to minimize the acquisition time, as that will transfer from the source a digital signal that is easily accepted by the TV.\nYou might find a bit more lag is present with analog connections, like component or composite cables. This is because the TV needs to convert the analog signal to digital before video can be displayed, and the conversion process takes time.\nOnce the image is in a format understandable by the video processor, it will apply at least some processing to alter the image in some way. A few examples:\n- Adding overlays (like menus)\n- Adjusting the colors and brightness\n- Interpolating the picture to match the television's refresh rate\n- Scaling it (like 720p to 1080p, or 1080p to UHD)\nThe time this step takes is affected by the speed of the video processor and the amount of processing needed. Though you cannot control the speed of the processor, you can exercise some control over how many operations it needs to do by enabling and disabling settings. The more settings you enable, the more work the processor needs to achieve.\nSome televisions have a dual core processor in them. This can help reducing the input lag if a lot of processing options are turned on.\nDisplaying the image\nOnce the television has processed the image, it is ready to be displayed on the screen. This is the step where the video processor sends the image to the screen. The screen cannot change its state instantly, and the amount of time it will take depends on the technology and components of the television. There’s unfortunately no way to improve or control the amount of time taken by this phase; it is a fixed amount of time for each television.\nHow to get the best results\nMost televisions can be adjusted so that they do not have high input lag. As a general rule, try the following (which is how we set up the TVs in our tests):\n- Set the TV to Game or PC Mode\n- Disable all of the television's settings\nAdditionally, you can try different combinations of settings/modes/inputs until you arrive at whatever balance of features and input lag that you like.\nHere are the steps necessary for getting minimal input lag on TVs from several brands:\n- Samsung: Go to Menu > System > General and set ‘Game mode’ to ‘On.’\n- Sony: Go to Menu > Picture adjustments and set ‘Picture mode’ to ‘Game.’\n- Vizio: Go to Menu > Picture > More picture and set ‘Game Low Latency’ to ‘On.’\n- LG: Go to Menu > Picture and set ‘Picture mode’ to ‘Game.’\n- Game mode will disable some of the television's most time-consuming processing. However, gaming mode is not necessarily the setting that guarantees the lowest input lag of the television; you will sometimes need to play with the other settings to get the optimal input lag time.\n- Inside game mode, it doesn't really matter what settings you turn on.\n- The input lag varies slightly depending on the input resolution and frame rate.\n- The input lag also varies in time. On some TVs, it even varies +/- 5ms.\nInput lag is the amount of time that elapses between performing an action with a source device and seeing the result onscreen. It’s important for gaming, and is particularly important for fast-paced, competitive games. Anything below 50 ms is unnoticeable by most people. We test to find the lowest amount of lag a TV can have, as well as how much lag is present when a TV has motion interpolation enabled, or when it has normal, non-gaming picture settings on.\nTo improve the amount of lag, the best thing to do is use ‘Game’ or ‘PC’ mode on your TV (depending on the brand). If you want to use other settings that aren’t available in game mode, you’ll unfortunately need to deal with a higher amount of lag.\nQuestions & Answers\nUpdate 9/14/2016: Now with the new HDR consoles, there is a benefit for gaming.\nAnd generally, apart from some of the settings being disabled, the settings within game mode aren't really different enough that it makes sense to give them their own section, so we likely won't be adding that. But thanks for the suggestion!\nAnd yes, we do plan to test an OLED TV sometime this year.\nIf this is indeed the case, to avoid any of this kind of skipping in the future, just disable the motion interpolation feature on whichever TV you get.\nBefore asking a question, make sure you use the search function of our website. The majority of the answers are already here."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:9d69ecc3-626a-47db-a967-44f669a0a342>","<urn:uuid:395ba685-3f34-4ec8-b72b-c28f135de64f>"],"error":null}
{"question":"I'm considering installing a water purification system at home. What are the maintenance requirements and costs associated with both Reverse Osmosis and Distillation systems?","answer":"Reverse Osmosis systems require regular filter changes every 4 months to 2 years, with pre-filters needing annual replacement and RO filters replacement every 2-3 years. The bladder tank needs cleaning with chlorine solution every 6 months to prevent bacterial colonization. Initial costs range from $200-500 for under-sink systems to $12,000-18,000 for whole house systems, plus installation. For distillation systems, countertop units cost $75-500, while larger automatic distillers range from $800-2,500+. They require regular cleaning with vinegar solution to remove scale buildup and periodic disinfection of the holding tank. Both systems are relatively inefficient, typically wasting 75-80% of feed water, which increases water bills. Additionally, both systems require electricity to operate and have ongoing maintenance costs for filter replacements and cleaning.","context":["Biosand filters (BSF) are a simple water treatment method which was developed from slow sand filters, and can remove pathogens and suspended solids from water.\nMechanism of Treatment\nWater is treated through a combination of biological and physical processes:\nMechanical trapping – suspended solids an pathogens are physically trapped in the spaces between the sand grains\nPredation – Micro-organisms in the biolayer (“schmetzdecke”) eat some pathogens\nAdsorption – Some pathogens get stuck to the sand grains\nNatural death – Some pathogens die because they lack food or oxygen, or because they finish their life cycle and die of old age.\n- Can be built locally using available materials and labor\n- Easy to operate\n- Relatively inexpensive\n- Can be used with most water sources (well, borehole, pond, river, tap-stand, rain)\n- Does not remove dissolved contaminants or chemicals (e.g. salt, arsenic, fluoride)\n- Can’t be used with chlorinated water, as chlorine will kill the biolayer\n- Up to 100% helminthes (worms)\n- Up to 100% protozoa\n- Up to 98.5% of bacteria\n- 70-90% of viruses\n- Up to 95% of turbidity\nChlorine Dioxide Water Treatment\nHydrogen Peroxide Water Treatment\nOzone Water Purification\nUV Water Disinfection\nReverse Osmosis (RO) is a water treatment method that uses filtration to remove particles from water based on size.\nMechanism of Treatment\nOsmosis is the scientific term to describe the natural movement of fluid across a membrane (filter) from a lower concentrated solution to a higher concentrated solution, which will then weaken (dilute) the higher concentrate.\nReverse osmosis is primarily a filtration/straining method, and is sometimes called hyper-filtration. Reverse osmosis occurs when pressure is applied to the higher concentrate side, forcing the fluid to move through the membrane (filter) in the opposite direction that it would naturally flow. The result is that fluid moves to the lower concentrate side, but the contaminants cannot pass through the filter, so they are left on the higher concentrate side.\nParticle filtration – removes particles of 3.9 x 10 -5 inches or larger\nMicrofiltration – removes parties of 50nm or larger\nUltrafiltration – removes particles of 3nm or larger\nNanofiltration – removes particles of 1nm or larger\nReverse osmosis / hyperfiltration – removes particles of .01 nm or larger\n- RO equipment is standardized(pumps, motors, valves, flow meters, pressure gauges etc.) so the learning curve of unskilled labor is short\n- Can remove all contaminants that are smaller in size than a water molecule (arsenic, sodium, copper, lead, some organic chemicals, fluoride, bacteria and pathogens)\n- Compact system, requires less space than some systems like distillation.\n- A good solution when having to deal with high TDS (Total dissolved solids) or high sodium (Salt) content.\n- Usually needs to be used in combination with other treatment methods, such as a pre-filter (to remove large contaminants), a carbon filter (used before RO to trap organic chemicals and remove chlorine which can damage the RO membrane filters, and can be used again after RO to capture chemicals not removed by RO), or ultraviolet (to sterilize microbes)\n- Can only remove contaminants that are smaller in size than a water molecule – does not remove VOC’s (volatile organic chemicals), chlorine and chloramines, pharmaceuticals, pesticides.\n- Healthy, naturally occurring minerals are also removed.\n- The membrane must be cleaned often, and can trap bacteria which can potentially pose other problems if membrane is compromised.\n- It requires diligent maintenance and has an ongoing cost of filter replacement (pre-filters every year, RO filters every 2-3 years)\n- Requires electricity to run the pumps (infrastructure)\n- Has a significant initial cost to purchase.\n- Loses at least 1 gallon of water per 1 gallon of clean water produced.\nUp to 99% of dissolved salts, particles, colloids, organics, bacteria, and pyrogens","Sign up for the latest water news and information.\nAn Overview of Reverse Osmosis and Distillation Systems\nWhat is Reverse Osmosis?\nReverse Osmosis is a water purification process that uses pressure to force water through a semipermeable membrane. It is commonly used during the production of bottled fresh water and the desalination of salt water.\nReverse Osmosis is also a common step in many household water purification systems. Such systems typically include:\n- A sediment filter\n- An activated carbon filter\n- A Reverse Osmosis membrane\n- An ultraviolet lamp for sterilizing microbes that escape filtration\nThe systems are typically installed under a sink, and pull in water from an incoming tap line.\nOsmosis is the, “Tendency of water to flow from a hypotonic solution (low concentration of dissolved substances) to hypertonic solution (higher concentration of dissolved substances) across a semipermeable membrane.”1\nThink of a tank of water with two chambers. One chamber has a low level of dissolved substances (salt, bacteria, or contaminants), while the other chamber has a high level of dissolved substances. Water would naturally flow through a semipermeable membrane toward the chamber with the higher level of dissolved substances in order to even out the concentrations between the two chambers.\nRO Uses Pressure to Control the Flow of Water\nReverse Osmosis pressurizes the solution with the higher level of dissolved substances in order to reverse the natural flow of osmosis. It forces water to move toward the chamber with the lower level of dissolved substances, resulting in filtered water accumulating in the second chamber.\nWhat can I expect from a Reverse Osmosis system at home?\nTypical Performance / What a Home System Removes\nAt-home Reverse Osmosis systems are capable of removing up to 99%+ of dissolved salts (ions), particles, minerals, organics, bacteria and cysts (such as cryptosporidium) from incoming tap water.2 They also reduce certain tastes, some pesticides, high chloride content, nitrate, heavy metals, and arsenic.3\nWhile models of Reverse Osmosis systems will vary, in general several categories of contaminants should be considered carefully when evaluating effectiveness: dissolved gases, trihalomethanes, VOCs and bacteria.\nAccording to Cornell Cooperative Extension, College of Human Ecology:\nReverse osmosis will not remove all contaminants from water. Dissolved gases such as oxygen and carbon dioxide pass through RO membranes into the treated water. Unfortunately, hydrogen sulfide gas, with its notorious odor of rotten eggs, also passes through the RO membrane. RO in general is not a very effective treatment for trihalomethanes (THMs), some pesticides, solvents, and other volatile organic chemicals (VOCs). However, RO systems can be certified by NSF for VOCs, THMs, and several pesticides and solvents if the contamination is not too high.4\nBacteria Reduction and Tank Colonization\nReverse Osmosis systems often add a UV light to the system in order to more effectively reduce bacteria.5 However, the UV light sometimes does not kill all the bacteria because any turbidity in the water can create shaded spots, preventing some bacteria from being exposed. In addition, the UV is installed before the bladder tank; however it is in the bladder tank that bacteria usually colonize. Therefore, if the bladder tank is not sterilized on a regular basis, it becomes a source for bacteriological contamination. Note that the carcasses of the dead bacteria may remain within the bladder tank (the filtered drinking water) with a Reverse Osmosis system.6\nMaintaining a Traditional Reverse Osmosis Systems\nReverse Osmosis systems can have up to four filter elements, with each needing to be changed at various intervals ranging from four months up to two years. This process may require that water pressure to the home be shut off, and part or all of the system be disassembled for maintenance. Additionally, the bladder tank should be washed with a chlorine solution at six-month intervals to kill any colonizing bacteria.7\nMaintaining a Tankless Reverse Osmosis Systems\nWhile some newer tankless Reverse Osmosis systems have overcome the issue of bacteria colonizing in the bladder tank, these systems have their own set of inherent issues, including:\n- TDS Creep: without a flush tank, the initial water coming out can taste horrid, due to increased TDS in the water. TDS creep is common to all systems using an RO membrane and results from the natural diffusion of TDS ions through the membrane from the feed side to the permeate side when the tank is full. This results in lowered or “dirty” water quality.\n- High Failure Rate: some tankless models have already been removed from the market, due to numerous issues, including high rates of failure. Noise: depending on the specific model and features, some units are extremely noisy to operate.\n- Excessive Water Waste: this is a problem with both standard and tankless Reverse Osmosis systems.\n- Cost: with multiple membranes being used, the overall cost per gallon, can be even more expensive than older-style Reverse Osmosis systems.\nIn an Emergency\nHere are several items to consider regarding relying on a Reverse Osmosis system during an emergency:\n- They require electrical power to function, which may not be available\n- They require water pressure to function, which also may not be available\n- They are not portable, and cannot travel with you during an evacuation\n- They are typically suggested for use only with “biologically safe” water sources\nWhat is distillation?\nDistillation is the process of purifying water through boiling the water and collecting the steam. When water is purified by distillation, it is boiled in a container and the steam is sent into cooling tubes.10 The steam is condensed and then collected as purified water in a second container. The impurities in the water are left behind in the first container and can be discarded. Distillation is used for many commercial purposes, including the production of gasoline, distilled water, alcohol, paraffin, kerosene and many other liquids.\nWhat can I expect from a distillation system at home?\nTypical Performance / What a Home System Removes\nDistillation removes sediment, high salt content, high total dissolved solids, pesticides (if properly equipped with a gas vent), fluoride, nitrate, lead, copper and other heavy metals, arsenic, and bacteria.11 Depending on the system, and if operated properly, it may also effectively inactivate microorganisms such as bacteria, cysts and viruses.12\nDistillation does not remove chlorine, chlorine byproducts, some VOCs, certain herbicides and other chemicals with boiling points lower than or near that of water.13 Contaminants that easily turn into gases, such as gasoline components or radon, may be retained in the water unless the system is specifically designed to remove them. In addition, bacteria may recolonize on the cooling coils during inactive periods. Distillation also removes beneficial minerals from the water.\nMaintaining Distillation Systems\nWater distillers range from relatively small countertop units to large floor models for commercial and industrial use. At-home distillation units typically consist of:\n- A boiling chamber, where the water enters, is heated and vaporized\n- Condensing coils or chamber, where the water is cooled and converted back to liquid water\n- A storage tank for purified water\nDistillation systems need to be cleaned with a vinegar solution regularly to remove scale buildup. The distilled water holding tank should also be disinfected periodically. For those with hard water, a water softener may reduce the frequency of required cleanings.14 Keep in mind this will add salts to the water. Unevaporated pollutants remaining in the boiling chamber need to be regularly flushed to the septic or sewer system.\nIn an Emergency\nHere are several items to consider regarding relying on a Distillation system during an emergency:\n- Smaller units are portable, with some being suitable for use with a fire or camping cooktop\n- However, portable units will not provide sufficient water for multiple individuals\n- A heating source may not be available\n- If a source of fuel is available, the system will require this precious resource\n- Larger units will require electrical power to function, which may not be available\nHow much do Reverse Osmosis and Distillation systems typically cost?\nReverse Osmosis systems are typically more expensive due to the cost of the system and the additional expense to have the system plumbed. Whole house systems often range from $12,000-$18,000, depending on the amount of water that needs to be generated daily. Under-sink Reverse Osmosis systems typically range from $200-500 plus installation. Overall, cost factors one may consider include:\n- The system\n- Membrane and pre-filter replacements\n- Cleaning and routine maintenance\n- Waste water from the process, which can drive up water bills\nNext in cost would be a Distillation unit. Heating water to form steam requires energy (typically electricity, but also potentially a stovetop or fire), which factors into the cost of operating a Distillation unit.\n/Small electric, countertop units tend to range from $75-$500. Outdoor, emergency distillers, which look similar to metal pans, are about $100. Cost factors for these types of units are limited:\n- Electricity or another fuel source (wood, propane stove, etc.)\n- Maintenance and cleaning\nLarger “automatic” distillers offer higher capacities than countertop units, ranging from about 7-12 gallons per day. They are freestanding units that run from approximately $800-2,500+, depending on the model. Cost factors for larger units include:\n- Installation, if connecting to a tap line\n- Maintenance and cleaning off scale\n- Filter replacements (many include a carbon filter step)\nNote that both Reverse Osmosis and Distillation systems can also be wasteful and inefficient. They use large amounts of water to create a very small amount of purified water, as typically 75-80% of the feed water may end up discarded.15\nWhat are the health concerns associated with Reverse Osmosis or distillation?\nMany Reverse Osmosis and distillation systems remove the “good” with the “bad.” Reverse Osmosis or Distillation strips out beneficial minerals from the water, making the water an acidic “hypotonic” solution.16\nA chemist will tell you that when a hypotonic (de-mineralized) solution comes into contact with a “hypertonic” (mineralized) solution, the minerals within the hypertonic solution will transfer out of hypertonic solution and into the hypotonic solution until equilibrium is achieved.17 What this simply means is that when one drinks hypotonic water, the minerals in the blood and lymphatic system, which are hypertonic, transfer into the hypotonic Reverse Osmosis or Distilled water that is consumed and the minerals are flushed out of the body upon urination. In an effort to re-mineralize, the blood and lymphatic systems then begin to scavenge for minerals from other parts of the body, such as bones and other organs, and this process repeats itself every time de-mineralized hypotonic water is re-consumed.\nSeveral studies suggest that people who drink demineralized water (hypotonic) over a long period of time tend to be more prone to degenerative diseases such as osteoporosis.18\nHow do Berkey® Systems compare to Reverse Osmosis and Distillation?\nBlack Berkey® Purification Elements are Unique\nBerkey® Water Purification Systems address a broad universe of potential contaminants, including viruses, bacteria, pesticides, pharmaceuticals, heavy metals and even radiologicals while leaving in the healthy minerals your body needs.\n- Addresses pathogenic bacteria without a UV light\n- Removes dead bacteria bodies during the purification process\n- No need to turn off water pressure to the home\n- There are no coils to maintain\n- Addresses chlorine, chlorine byproducts and VOCs (among many other chemicals)\n- Not susceptible to contaminants that may pass through Distillation systems in a gaseous state\nBlack Berkey® Purification Elements Leave in Beneficial Minerals\nBerkey® purification systems leave in the beneficial minerals your body needs. Enjoy powerful purification without sacrificing positive components of your water.\nLearn More About Black Berkey® Purification Elements\nExplore other articles for additional, in-depth performance information:\n- Black Berkey® Purification Elements Test Results\n- Hurricane Preparedness: A Plan for Potable Water\n- After the Hurricane: Contaminated Flood Water Precautions\n- E. Coli Bacteria Explained\n- Black Berkey® Purification Elements: The Final Barrier against PFOA and Other PFCs in Drinking Water\n- A Look at Radiological Water Contaminants\n- Understanding Aging Water Infrastructure in the US\n- Newest Berkey® Systems Lead Water Test Now Available\nConvenient to Maintain\nBerkey® systems can be easily cleaned by removing all filter elements and then washing the top and bottom chambers of the system in warm water and dish soap. For users in areas of hard water, calcium scale may build up on the spigot and chambers after prolonged use. To remove, simply soak affected part(s) in vinegar or a 50-50 mix of vinegar and water for about 15 minutes.\nA Berkey® system also typically costs less than either Reverse Osmosis or Distillation at about 1.8 cents per gallon. Also, Berkey® systems use all of the water that flows through the purification elements, leaving only the contaminants behind.\n- Enjoy a 6,000 gallon lifespan per pair of Black Berkey® Purification Elements\n- Uses water more efficiently, reducing “waste” water and expense\n- Powered by gravity- no electricity or other fuel cost\n- No professional installation cost\n- Limited maintenance required\nFits Virtually Any Need\nBerkey® systems are available in seven different sizes, from a 1 quart system to a 6 gallon system capable of producing up to 26 gallons / hour in an emergency. In addition, larger systems may be expanded to accommodate additional purification elements. This speeds up the process, and reduces wait time. Visit our \"Find your Berkey® System\" page to determine what might work best for your family or group.\nEssential During an Emergency\nBerkey® systems are portable, require no electricity or water pressure to operate, and are made for the extreme. Berkey® systems can easily purify ordinary tap water and well water, yet are powerful enough to efficiently purify raw, untreated water from sources such as remote lakes and streams. Berkey® systems used several different independent labs, took multiple samples and performed Extreme Testing for Lead and PFC’s in order to review the effectiveness of our Black Berkey® Purification Elements. Test results for Black Berkey® Purification Elements are readily available to interested customers.\nBerkey® Sytem Benefits\nMake your choice with confidence, based on detailed, high-quality information.\nBerkey® Systems are the top choice for everyone who is considering a gravity-fed system. We hope that demonstrating how to evaluate major performance factors assists you in making a great decision.\nBerkey® System Benefits\nPowerful- Berkey® purifiers remove a broad universe of contaminants, from toxic chemicals and minerals to pathogenic bacteria and virus, while leaving in the healthy minerals your body needs.\nEfficient- Berkey® purifiers are the fastest-producing gravity fed water purification systems on the market, purifying water up to eight times faster than other available systems.\nConvenient- The purification elements may be cleaned, eliminating frequent replacements.\nEffortless- The design replaces slow and exhausting manual pumps with the natural force of gravity. At the same time the system remains simple to use, making it a breeze; whether in the kitchen or deep in the Rocky Mountains.\nPortable- Berkey® purifiers travel easily and function without electricity or water pressure.\nDurable- Constructed of highly-polished surgical grade AISI 304 stainless steel, the housing is built to last.\nEconomical- Two Black Berkey® Purification Elements, which come standard with most systems, average 2 cents per gallon of purified water, and last up to 6,000 gallons, or approximately 5-8 years with typical use.\nProven- Used by individuals, missionaries, and relief organizations worldwide, Berkey® systems have truly stood the test of time.\nShare this Important Information\nUnderstanding is key. Share this with others so they may make informed choices about their water.\n(1) Biology Online Dictionary- Osmosis\n(2) Puretec- What is Reverse Osmosis?\n(3) A Guide to Drinking Water Treatment Technologies for Household Use\n(4) Cornell Cooperative Extension, College of Human Ecology- Reverse Osmosis treatment of Drinking Water\n(5) A Guide to Drinking Water Treatment Technologies for Household Use\n(6) Puretec- What is Reverse Osmosis?\n(7) CHLORINE & DISINFECTION\n(8) What is Reverse Osmosis?\n(9) Cornell Cooperative Extension, College of Human Ecology- Reverse Osmosis treatment of Drinking Water\n(10) Wikipedia.com- Distilled water\n(11) Drinking Water Treatment: An Overview\n(12) Contaminants Removed from Water by Distillation\n(13) Drinking Water Treatment: An Overview\n(14) Water Distillers\n(15) Environmental concerns of desalinating seawater using reverse osmosis.\n(16) Dr. Jacqueline Gerhart: There's good and bad to using reverse osmosis water systems\n(17) Wikipedia.com- Tonicity\n(18) What Are the Dangers of Drinking Distilled Water?"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:9dff9c56-7b9c-4992-873f-ea1198b2b29f>","<urn:uuid:63054593-5433-4db2-ac63-8447f798c00b>"],"error":null}
{"question":"Speaking as a jazz band director, how did these two trumpet legends - Art Farmer and Miles Davis - approach band formation and collaboration in their careers?","answer":"Both trumpeters led significant bands but with different approaches. Art Farmer played in various ensembles including the Kenny Clarke/Francy Boland Big Band and Peter Herbolzheimer's Rhythm Combination & Brass in Europe, also leading his own quintet in Vienna. Miles Davis, on the other hand, was known for forming groundbreaking groups, including his nine-piece Nonet in 1948 (which helped launch the cool jazz movement) and his influential quintet in 1955 featuring John Coltrane, Red Garland, Paul Chambers, and Philly Joe Jones, with whom he recorded several important albums for Columbia and Prestige.","context":["Art Farmer (1928–99)\nFarmer was an American jazz trumpeter and flugelhorn player. He also played flumpet, a trumpet–flugelhorn combination specially designed for him. He started to play the piano in elementary school in Phoenix, Arizona, before settling on trumpet at the age of 13, and began playing professionally while in high school.\nFarmer moved to Los Angeles in 1945 with his double-bassist twin brother Addison, attending a music-oriented school, where he received music instruction and met other developing musicians. He subsequently moved to New York, where he performed and recorded with musicians such as Horace Silver, Sonny Rollins and Gigi Gryce, and became known principally as a bebop player.\nRetuning to Los Angeles, Farmer was engaging in studio recording and the recording of his composition “Farmer’s Market” in 1952 brought him greater attention. During the 1950s, Farmer was featured in recordings by leading arrangers in a wide range of styles and was much in demand because of his reputation for being able to play anything.\nIn the 1960s, Farmer made several tours to Europe and in 1968 settled in Vienna. He began appearing with the Kenny Clarke/Francy Boland Big Band with many well-known American and European musicians. He also played alongside well-known expatriates such as Dexter Gordon and Ben Webster. From the early 1990s, Farmer had a second house in New York and divided his time between Vienna and the US. He continued performing on both sides of the Atlantic and recorded extensively as a leader throughout his later career, including classical music with US and European orchestras.\nBiography by Mike Rose\nAn appreciation by Ron Simmonds\nLes Tomkins interviewed Art Farmer twice, in 1965 and 1988. Farmer talks about the many bands and musicians he played with and notes the Lionel Hampton band where he was part of a trumpet section with Clifford Brown and Quincy Jones. In the third interview, trumpeter Ron Simmonds writes an appreciation of fellow trumpeter, Art Farmer.\nArt Farmer: Interview 3\n|1st January 2000\n|Image source credit\n|Image source URL\nSometimes it seems so easy—it seems like the music is just flowing through you. It’s coming not from you but right through you; you’re just on instrument—it’s just there. And you can’t make these times happen; so you have to enjoy them when they do happen, but you can’t wait for them to happen, either, you know. You have to go ahead and play your horn. You have to be in a position for these minutes to happen—and when they don‘t, there’s no need to cry about it, because you never know anyway. It might happen the next day.\nAt the time Art Farmer spoke these words he was living in Vienna. He led a quintet there for several years, travelling frequently to the USA for recordings and the occasional jazz festival. In Vienna he built a practice room in his house. The immediate proximity of his neighbours had made it impossible to practise in New York: now he could do so for up to seven hours a day.\nSince coming to Europe he had played regularly in Germany with Peter Herbolzheimer’s Rhythm Combination & Brass. Of the RCB band he had this to say. “Of course, that was an outstanding ensemble concept, with rhythm and brass—that was what I would call a hot band! Around that same time I was also playing with the Kenny Clarke–Francy Boland band. In fact, one night I played with both bands! It was on a festival—when one went on, I went there; then the next one went on, and I just went back up again.”\nI played in the Herbolzheimer band for over ten years (says Ron Simmonds) and Art was standing right beside me throughout. Can you imagine that? To play with him and be able to listen to him for all that time? The other two trumpets were Ack van Rooyen and Palle Mikkelborg, sometimes Alan Botschinsky or Dusko Goykovich came instead of Palle. It was a dream section.\nSome of the conditions we played in were less than ideal. In Onkel Po’s Jazz Club in Hamburg, for instance, we had to stand on orange boxes in lieu of a proper podium. I used to think: Here is Art Farmer, one of the greatest trumpet players in the world, and he has to stand on an orange box to play! It was so dark in there that one of the trombones had a candle on his stand in order to see the music.\nWith a section full of such outstanding players the solos had to be shared out. When Art took a solo he sometimes amazed me by playing only a few quiet notes, with quite long silences between some of them. I used to think: What is he doing? When I heard the playbacks they were always perfect. Everything he did was perfect.\nIn the big band he was superb, but the best I ever heard him was together with Gerry Mulligan again, playing Festive Minor on one of our jazz galas.\nSoft spoken at all times, only once did I ever hear Art raise his voice in anger. Travelling to a festival in Ljubljana, Art boarded the wrong plane in Vienna and landed in Budapest at midnight. Without the necessary visa he was not allowed to leave the arrival hall and became locked in when the building closed. He spent the night in pitch darkness trying to sleep on a bench in the completely deserted airport. Next day he flew to Ljubljana and let off steam in a big way. The sight of the rest of the band around him convulsed with laughter did little to cool him down.\nArt was always immaculate in every way, in appearance and his playing. He was a beautiful man and a perfect musician, his playing full of soul and feeling. It was always sheer heaven just listening to him. Ron Simmonds\nCopyright © 2000 Ron Simmonds. All Rights Reserved","Miles Davis Biography\nMiles Davis was one of the 20th century's most innovative musicians. Throughout a long and illustrious career that spanned the latter half of the 20th century, he was the epitome of the consummate professional. A master innovator, he was a primary force in the development of jazz from bebop through fusion. His concise, lyrical phrasing, introspective style, and boundless invention continue to influence jazz musicians throughout the world.\n“Nothing is out of the question for me. I’m always thinking about creating. My future starts when I wake up in the morning and see the light… Then, I’m grateful.” ~ Miles Davis\nBorn on May 26, 1926 in Alton, Illinois, to dental surgeon Dr. Miles Dewey Davis, Jr., and music teacher Cleota Mae Davis, Miles grew up in the black middle class community of East St. Louis, Illinois. His interest in music developed early on and by the age of 12 he had begun taking trumpet lessons. He began playing bars while still in high school and at 16 he was playing out-of-town gigs. He was 18 years old and just out of high school when he got the chance to sit in with Dizzie Gillespie and Charlie Parker both of whom were playing in Billy Eckstine's band. Understandably, he fell under the spell of these founders of bebop.\nHis mother had wanted him to attend college, so as a compromise, he entered Julliard in New York City in September 1944. He immediately began playing in clubs with Parker. By 1945 he had dropped out of school in favor of a full-time career as a jazz musician. He played with Benny Carter, Billy Eckstine, as well as Parker. In the summer of 1948 he formed a nine-piece band, The Miles Davis Nonet. It was distinguished by a unique horn section. In addition to his trumpet, it featured an alto sax, a baritone sax, a trombone, a French horn, and a tuba. The Nonet recordings, later released as Birth of the Cool, had a significant influence on several of the band's musicians, including saxophonists Gerry Mulligan and Lee Konitz, and pianist John Lewis, and are considered the beginning of the West Coast cool jazz movement.\n“If you got up on the bandstand at Minton’s and couldn’t play, you were not only going to be embarrassed by the people ignoring you or booing you, you might get your ass kicked.” ~ Miles Davis\nHis progress as a musician was marred by heroin addiction in the early fifties, but, by the middle of the decade, he had kicked his habit. In July 1955, he appeared at the Newport Jazz Festival and created a sensation playing \"'Round Midnight.\" The performance led to a contract with Columbia Records and allowed him to put together a permanent band. He went on to organize a quintet featuring saxophonist John Coltrane, pianist Red Garland, bassist Paul Chambers, and drummer Philly Joe Jones. They began recording his Columbia debut, Round About Midnight, in October. At the same time, he was still obligated for five albums on an earlier contract with Prestige. Over the next year, in order to satisfy this commitment, he alternated his Columbia sessions with sessions for Prestige. The products were Prestige albums The New Miles Davis Quintet, Cookin', Workin', Relaxin', and Steamin'. Davis' first quintet was one of his better documented groups.\n“We’re not going to play the blues anymore. Let the white folks play the blues. They got ‘em, so they can keep ‘em.” ~ Miles Davis\nFurther milestones lay ahead for Davis -- his groundbreaking orchestral work with his musical soul mate Gil Evans, the recording of the most popular jazz album ever (Kind of Blue), further endeavors with another pivotal quintet in the '60s and finally, \"the fathering of the Free Improvisation and Funk-tinged riffs and grooves of the Fusion age with Bitches Brew.\" Through it all, Davis remained the consummate professional and master innovator, never resting on his laurels, always focusing on the better riffs to come.\nIn early September 1991, Davis checked into St. John's Hospital near his home in Santa Monica, California, for routine tests. After repeated bouts of bronchial pneumonia, doctors suggested he have a tracheal tube implanted to relieve his breathing. The suggestion provoked such an outburst from Davis that it led to an intra-cerebral hemorrhage followed by a coma. After several days on life support, his machine was turned off and he died on September 28, 1991. He was 65 years old. A funeral service was held on October 5, 1991, at St. Peter's Lutheran Church on Lexington Avenue in New York City. It was attended by around 500 friends, family members, and musical acquaintances, with many fans standing outside in the rain. He was buried in Woodlawn Cemetery in The Bronx, New York City, with one of his trumpets, near the site of Duke Ellington's grave."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:43c721dd-ef74-43ef-8105-26278b70daac>","<urn:uuid:8ad3270f-bb88-4acd-b7fe-0a933d57a7c9>"],"error":null}
{"question":"How is finite element analysis used in designing sail ship masts?","answer":"Finite element analysis is used to conduct comprehensive analyses of sail ship masts under different loading cases. Some ship classification societies, like the Germanischer Lloyd (DNV GL) in Germany, require finite element analysis in their design guidelines for classifying large sail ship masts and rigging. This analysis includes performing preliminary quasi-static collapse analysis to identify post-buckling response and analyzing the mast's response to dynamic loads from sources like wind gusts and sudden course variations.","context":["Dynamic Stability Analysis of Sail Ship Masts\nA reliable mast is critical to the safety and performance of a sail ship. The collapse of a mast not only impairs the mobility of a ship, but can cause injuries to the passengers and crew, and also serious damage to the ship.\nEngineers designing a mast need to conduct comprehensive analyses of the mast under different loading cases. Finite element analysis has proved to be efficient for this purpose. In fact, some ship classification societies, such as the Germanischer Lloyd (DNV GL) in Germany, prescribe a finite element analysis in their design guidelines for the classification of large sail ship masts and rigging. In this Brief, we feature the use of ADINA for designing safe and reliable masts for large modern sail ships, see also ref.\nThe modern sail ship mast is typically a hollow column made of aluminum alloy. Figure 1 shows the cross section of an actual aluminum alloy mast used in the yacht industry.\nFigure 1 Mast cross section, about 900mm by 400mm\nThe total length of the mast of a large sail ship can be 60 or 70m, about the same order of magnitude as the length of the ship, but typically the unsupported span length of a bottom mast panel (the lower span of the mast) is approximately 10m. Figure 2 shows the ADINA finite element model. The bottom of the mast is fully clamped. A compression loading of 5000kgf is applied on the top. The loading is used to represent the weight of the upper rigs and the pre-tension introduced by the mast tuning.\nFigure 2 The finite element model of the mast, about 10m long\nA preliminary quasi-static collapse analysis is first performed to identify the post-buckling response of the mast. The computation consists of a linearized buckling mode step and a static step. In the linearized buckling mode step ADINA extracts the buckling shapes of the masts (the eigenmodes). In the static step an assumed initial imperfection is applied based on the buckling shapes. The load-displacement curve then calculated indicates the progressive collapse behavior, as shown in Figure 3. The progressive collapse is clearly a global buckling phenomenon, as can be seen in the post-buckling shape of the buckled mast (Figure 4).\nFigure 3 Progressive collapse analysis: compressive force vs. longitudinal displacements in the z-direction, showing pre- and post-buckling stages up to collapse\nFigure 4 Mast collapse due to axial static compressive loading, showing band plot of von Mises effective stress\nA more complex instability issue is the collapse under impulse dynamic loads. The dynamic load comes from multiple sources, including wind gusts, sudden course variations, and abrupt accelerations/decelerations. Such dynamic loads can well exceed the load limit established in the quasi-static analysis, but usually act on the mast for only a short period of time.\nFor simplicity, the dynamic load is defined as a half-sinusoidal impulse, see Figure 5. The load factor is the ratio of the dynamic load to the quasi-static stable load limit. Different wavelengths and factor amplitudes are employed in a series of dynamics analyses to establish the mast response (see Figure 6).\nFigure 5 Time loading functions adopted in dynamic analysis\nFigure 6 Dynamic response chart of the mast\nAs seen in Figure 6, the stability of the mast depends not only on the load magnitude, but also on the period of the load. Another observation is that although under quasi-static loading the mast always experiences a global instability (collapse in the first eigenmode), it can experience local instability when it is subjected to a dynamic load. The above movie shows the collapse of the mast under dynamic loading.\nIt is seen that the comprehensive range of analysis capabilities in ADINA is an asset in the design and engineering of structures pertaining to ships and boats.\n- M. Gaiotti and C. M. Rizzo, \"Dynamic buckling of masts of large sail ships\", Ships and Offshore Structures, 2014.\nMast, sail ship, ship structure, buckling, frequency, eigenmode, quasi-static, dynamic analysis\nCourtesy of M. Gaiotti and C. M. Rizzo, University of Genova, Italy"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:82ec60fe-646c-4474-8bb3-b18e6f8798b7>"],"error":null}
{"question":"I'm studying radio propagation and notice different signal behaviors between day and night. What causes these variations in ionospheric conditions?","answer":"The ionosphere changes significantly between day and night. During daytime, the D and E layers are continuous, and the F layer splits into F1 and F2 layers. At night, the F1 and F2 layers merge into a single F layer positioned between their daylight positions, the D layer disappears completely, and the E layer becomes patchy. These changes affect signal propagation, which is why propagation often improves during nighttime when the D layer's thickness decreases, allowing more RF to travel through to the E and F layers. This is particularly evident in phenomena like chordal hop propagation, which involves the daylight F2 layer and nighttime F layer.","context":["Past months propagation improved with nice HF propagation. Those who have DX ears must have noticed that 10m long distance propagation acted in a more or less remarkable way. Fellow DX’ ers on forums reported signals with multiple echoes that are certainly not from a CB echo mike. Some say it must be a signal that travels all around the globe that causes the echo. Well, it’s all got to do with short path, long path and chordal hop propagation. I will try to explain.\nSignificant signal loss on normal multihop paths\nLet’s pretend we have two stations making QSO on 10m, PA9X near Rotterdam and imaginary station VK5ZQZ down in Adelaide. The solar flux is high enough (140) and K-index low enough (1) to allow propagation between the two stations. We are making a nice QSO, I have got my antenna directed at 70° bearing and he has got his antenna directed at 340°. The signal follows the shortest path with a distance of about 16,500km from The Netherlands across north east Europe, Asiatic Russia, China, south east Asia to Adelaide. It hops between the F2 layer, which floats between roughly 250-400km altitude and the Earth’s surface. Using F2-layer propagation one single hop can do a distance between 1800km and 3500km on 10m. So for the complete path to Australia, the signal makes about 5 to 6 hops. With each hop, bouncing off the Earth’s surface, there is significant signal loss, especially bouncing of land. But there is another path, the long path.\nLong path propagation signals can be stronger than short path\nThe long path runs the other way around the globe. Starting from The Netherlands, along the Azores, Atlantic Ocean northern Brazil, Peru, Pacific Ocean, across New Zealand’s Southern Island into Sydney, a distance of roughly 23,500km. The long path to Australia would be 7 to 8 hops in theory. But but why are long path signals sometimes remarkably stronger than short path signals when they make more hops? Could be that because the long path runs mainly over salt water, but this only counts for this specific situation. But there is more going on, a propagation mode that is believed to responsible for long path propagation with strong signals is called chordal hop propagation.\nSignal that bounces along the ionosphere\nChordal hop propagation is a propagation mode involving the daylight F2 layer and night time F layer. At daytime there are two upper layers in the ionosphere, the F1-layer at ± 150-200km and the F2 layer at 250-400km. Shortly after sunset these two layers merge into the F layer and split up again into F1 and F2 layer at sunruse. During night time the F layer loses it’s ionization density, and it’s ability to reflect signals back to Earth. But sometimes the F-layer is just dense enough to reflect the signal back, but with a less steep angle, causing the signal to be directed to another part of the ionosphere thousands of km’s ahead, not touching the ground. Here is a picture I have drawn to visualize.\nThe purple line represents the ionosphere, the orange line the short path and the green line the long path. A part of the green line is chordal hop propagation, there where the signal does not touch Earth’s surface but reflects of the F-layer.\nLess attenuation with chordal hop\nWith chordal hop propagation you have much less attenuation due to the fact the signals does not reflect against Earth’s surface. In this occasion the signal that uses long path propagation arrives at the other station with much less attenuation, thus with a stronger signal than the short path. one Funny thing is that a station at night time, like imaginary station PY4FTL has signals travelling above him along the ionosphere, but he cannot receive them.\nThere are also scientist that believe that chordal hop could be the result of signals that travel between the F2 or F layer and a sporadic-E layer. One more theory is that a signals travels within the F2 or F layer in an kind of duct for thousand of km’s.\nSignal with many different components\nThe multiple echoes and hollow modulation you hear, are the result of multiple signals of different phases that arrive at the receiver end and mix together. One single signal can consist of many different components, for example:\n- Long path propagation signal\n- Short path propagation signal\n- Short path propagation signals that also travelled along the long path (went all around the globe)\n- Long path propagation signals that also travelled along the short path (went all around the globe)\n- Signals that went multiple times around the globe in both directions\n- Backscatter that travelled the short path while beaming short path\nThese many “components” all add up into a sometimes spectacular combination of echoes. In one rare occasions I received an S9 signal. It had so many echoes that it was barely readable, an R2/S9 report! In another occasion I received a station from Moscow via the long path (37,800km) while the short path signal (2,200km) did not appear to be present at all!\nLong path and chordal hop propagation are beautiful phenomenon that not too many operators on 10m band know how to take advantage of. Hope this post will help.\n73 de PA9X Jean-Paul Suijs","Main article: Wikipedia:Radio propagation\nGround waves are \"line of sight\" communication. Atmospheric conditions come into play when transmitting between ham radio stations that are farther away and so, because of the curvature of the earth, cannot be reached through a straight line. This is called a sky wave. Since those atmospheric conditions are ever-changing, there are resources to allow operators to predict those conditions and the idea propagation techniques to use.\nVarious bands have different properties regarding those atmospheric conditions and can bound off the ionosphere or other particles to propagate further away.\nMore powerful stations are going to be able to reach further than weak stations, although some ham operators take it as a challenge to reach other stations with weaker power ratings (~5W instead of 100W to sometimes 1000W). This is called QRP.\nSky wave propagation\nThe E and F layers contain a high density of ionized particles. DX (long distance) communications are possible when radio waves are reflected off the E and F layers in the ionosphere. This way radio waves can propagate all over the earth, bouncing up and down between the ground and the ionosphere. This is called a sky wave. Note that during the day, energy from the sun splits the F layer into two distinct parts known as F1 and F2.\nThe F-layer (between 120km and 400km above earth) contains the highest densities of ionised particles, is the most reflective of RF energy, and hence is responsible for most DX propagation of HF communications.\nThe E-layer can be found between 90km and 120km above the earth. Reflection of RF waves by this layer is responsible for most of the long distance propagation valued by hams. The E layer both thins out at night, and rises, again resulting in increased propagation distances. E layer propagation is most useful for frequencies below 10MHz.\nSporadic E-Skip is caused by \"clouds\" of especially intense ionisation. These are highly reflective and allow DX communication from 25MHz to 225MHZ. The clouds are usually relatively short lived, and occur mostly during summer months.\nThe D layer can be found between 50km to 90km above earth. Propagation by reflection from this layer is quite rare as it is highly absorbent of RF energy. The thickness of the D layer decreases during the night, allowing more RF to travel through to the E and F layers, hence propagation often improves during the night.\nDay and night ionospheric conditions\nIonisation of the atmosphere varies considerably between day and night. During the day:\n- the D and E layers are continuous\n- the F layer splits into F1 and F1 (F1 is lower than F2)\nDuring the night:\n- The F1 and F2 layers coalesce to form a single F layer that lies between the position of the daylight F1 and F2\n- the D layer disappears altogether\n- the E Layer is patchy\nThese ionosperic variations are responsible for the variations in propagation noticed across a 24 hour period.\nHow are ionospheric conditions measured?\nIonospheric conditions are measured using an ionosonde. This has two variants:\n- a single high frequency radar that sends short pulses vertically upwards and records the time for the signals to be reflected back to the same place. Ionosondes are used to measure the height of the various ionised layers.\n- pairs of transmitter and receiver that are a measured distance apart. These allow propagation between points to be measured\n- Tropospheric ducting\n- Sunspot Cycle\n- Meteor scatter\n- Trans-Equatorial Propagation\n- Lightning scatter\n- Moonbounce (EME)\n- Electromagnetic Waves\n- What causes QRN?\n- Skip Zone\n- Propagation Resource Center\n- Miles per watt/ km per watt calculator You will need to enter the gridsquares of two stations and the tx power of the transmitting station.\n- Australian Space Weather Agency (IPS) - Australian prediction center\n- Radiowave Propagation Center\n- D-Region Absorption Prediction Center\n- Online VOACAP propagation tool\n|Propagation and radio wave theory|\n|Propagation||Aurora * E-Skip * IPS * Lightning scatter * Meteor scatter * Satellites * Trans-Equatorial Propagation * Tropospheric ducting|\n|Interference||QRM * QRN|\n|Theory||Electromagnetic Waves * Frequency Wavelength and Period|"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:02a594e1-e9fd-43d1-be32-0fb0fa680a93>","<urn:uuid:8f6b58c8-a5a1-4421-8870-5e6a88c65a4c>"],"error":null}
{"question":"What are the physical capabilities of scissor lifts and what safety measures must be followed when operating them at height?","answer":"Scissor lifts can reach heights up to 60 feet and hold weights between 500-2,500 pounds on their platforms that measure 69-90 inches wide. For safe operation at height, workers must use appropriate fall arrest equipment attached to suitable anchor points inside the platform. Full guardrails and toe boards should be installed, and a safety harness is required if full guarding isn't possible or when reaching over. Additionally, operators must ensure platforms are kept clear of slip and trip hazards, properly store all tools and materials, wear appropriate PPE including safety glasses, construction hard hats and gloves, and follow proper inspection protocols. The equipment should never exceed manufacturer's safe working load limits or be operated in high winds.","context":["What is a Scissor Lift?\nA scissor lift is a type of motorized aerial forklift (aka construction lift) that helps workers, such as construction workers, reach elevated heights more safely and easily. Scissor lifts consist of a raised platform that holds workers and their tools and crisscrossed metal support bars, which raise and lower the platform as needed.\nScissor lifts are very sturdy, versatile equipment. Because they can hold weights of up to 2,500 pounds and reach hard-to-reach, elevated heights of up to 60 feet, they are one one of the most commonly used forklifts in the construction industry today.\nWhere and how are scissor lifts used?\nScissor lifts, like boom lifts, are most commonly used in the construction industry to reach elevated heights. However, they are used in different ways than boom lifts. Scissor lifts can reach a maximum height of around 60 feet while some boom lifts can reach up to 180 feet. Additionally, scissor lifts only move vertically (up and down) while boom lifts’ arms can move horizontally and vertically (up and over). With that being said, if you need to reach heights of up to 60 feet and only need to move upwards, a scissor lift is your best bet in terms of forklift equipment.\nScissor lifts can be operated using either gas or electricity. Take a look at some common industries and applications where scissor aerial lifts are used:\n- Outdoor construction sites\n- Warehouse (ie. high ceilings and storage)\n- Distribution and industrial (ie. deliveries)\n- Maintenance (ie. city building maintenance and construction)\n- Infrastructure (ie. hanging signs)\n- Home renovation and remodeling\n- Transportation (ie. trucking, railroad facilities)\n- Landscaping (ie. tree trimming)\nTypes of Scissor Lifts\nWhen exploring different types of scissor lifts, there are two different factors to keep in mind – electric vs. gas-powered lifts and slab vs. rough terrain scissor lifts. Electric vs. gas refers to the scissor lift’s power source – in other words, what fuels the aerial lift and allows it to run. On the other hand, slab vs. rough terrain refers to the work environment where the lift will be used.\nElectric vs. Gas Powered\nAlthough most scissor lifts will look very similar, there are two different “engines” they might have – gas or electric. Electric scissor lifts are emission-free and run using a battery. They produce very little noise and are best used for indoor applications or small, narrow spaces\nThere are also gas-powered scissor lifts, which use an engine that requires diesel gas fuel. As a result, gas-powered scissor lifts do usually emit fumes. However, they typically have heavier duty tires and are better equipped for rugged, wider outdoor areas.\nSlab vs. Rough Terrain\nYou will typically see two different options when it comes to scissor lifts – slab vs. rough terrain. These two options refer to the different terrains or work environments where the forklift will be used.\nSlab scissor lifts: Best for indoor jobs or narrow work areas\nSlab scissor lifts are designed to function well on slab, flat surfaces, such as concrete or indoor flooring. They are typically electric-powered forklifts and run by using a low emissions battery. While they usually reach lower heights compared to rough terrain lifts (around 45 feet), they tend to be more nimble and versatile. Slab scissor lifts are great for maneuverability and narrow, tight workspaces. You might find a slab scissor lift used in a parking garage, narrow urban area, or retail setting.\nRough terrain scissor lifts: Best for outdoor construction and maintenance\nThese forklifts are designed to function efficiently on rugged or uneven outdoor terrains, such as gravel or rock. Most rough terrain lifts are diesel gas-powered and equipped with heavy duty tires, four-wheel drive and traction control. They also are better suited to larger or wider work spaces compared to slab lifts, and usually have work platforms with higher weight capacities (1,000-2,500 lbs).\nDimensions and Capacities\nWhen looking for a scissor lift suited to your project’s needs, it’s important to pay close attention to the dimensions and capacities. This includes the foirklift’s maximum reachable height, platform weight capacity, and workspace width.\nScissor lifts will vary depending on the manufacturer and type of scissor lift. Remember that gas-powered and rough terrain scissor lifts usually reach higher elevations and can hold heavier weight on their work platforms. Electric and slab scissor lifts may have relatively lower weight and height capacities.\nTake a look at the ranges for height, weight, and platform width for scissor lifts:\n|Max height capacity||Weight capacity||Platform Width|\n|19-60 ft.||500 - 2,500 lbs.||69-90” wide|\nCommon Rent vs. Purchase Pricing\nOf course, pricing for scissor lifts will vary depending on whether you are looking to rent vs. buy. Rentals are typically charged per day, week, or month, depending on the duration in which you need the forklift. Purchase prices vary depending on whether you are buying brand new vs. used equipment.\n|Per day: $100-$300|\nPer week: $350-$500\nPer month: $800-$1500+\n|$5,000 - $40,000|\nDepending on type and new vs. used\n|Delivery and pickup: $150 per service|\nInfluencing factors on pricing\nKeep in mind that several additional factors will impact rental and purchase pricing. For instance:\n- Electric vs. Gas-powered\n- Slab vs. Rough Terrain\n- Height and weight capacities\n- Supplier delivery cost\nTypically gas-powered and rough terrain scissor lifts go for higher costs compared to electric and slab scissor lifts. Additionally, for the most part, the higher elevation the scissor lift can reach, the more it will cost to rent and buy.\nIf you are buying, remember that used scissor lifts will cost less than brand new ones. When buying, also make sure to pay attention to things such as the manufacturer, maximum lift height, and weight capacity.\nIf you are renting, you also want to keep in mind factors such as your geographical location and your supplier’s delivery charges. For instance, if you live in a big city such as New York or Los Angeles, you will likely pay more to rent a forklift compared to more rural areas. Pay attention to how far your supplier is from your location, as more mileage traveled during delivery may hike up your delivery charge.\nWe always recommend comparing your total rental cost to your potential purchase cost to see which is the more cost-effective option. You can also use our free service to match with aerial lift providers in your area offering the most competitive rental and purchase prices!","From Jason of riskatmedia.com\nThere are particular hazards associated with working at height, some of these hazards accompany almost all types of work activity, such as manual handling and slips and trips. Others, such as falls from height and contact with moving or falling objects are of particular concern to personnel who work at height. Falls from height are the leading cause of fatal injuries at work. While contact with moving or falling objects is the third greatest cause of workplace fatality and the second greatest cause of major accidents at work.\nManual Handling Manual handling related activities are a major cause of occupational injury. Low back pain, joint injuries and repetitive strain injuries affect over a million people each year, and many of these injuries are the result of manual handling. Prior to carrying out any unavoidable manual handling activity, you should help to protect yourself from injury by following good manual handling practices. Consider the task to be carried out and the nature of the load, and be aware of your own capabilities and the environment in which you are working. In particular, tools, equipment and materials can be extremely heavy and you should give consideration to safe manual handling practices before undertaking any job that involves lifting these items while working at height, where manual lifts may be made more difficult by space restrictions and the potential to fall.\nSlip, Trips and Falls Slips and trips represent a significant cause of work related injury. Slips and trips can result from contamination, obstacles, inappropriate footwear, reduced visibility, the environment and people’s attitudes. It is extremely important that elevated work platforms and access ladders are kept clear of slip and trip hazards that could result in a possible fall from height. By removing waste materials to waste skips you can contribute significantly to good housekeeping. You can also reduce the risk of slips and trips by properly routing any cables that you use, by only taking the tools, equipment and materials necessary to the job aloft, by appropriately storing all such items and keeping walkways clear, by ensuring that you always wear appropriate footwear, and by taking responsibility for your own and your colleagues safety and containing any spills that you might discover. Scaffolders should ensure that no loose scaffolding materials are ever left on a finished scaffold.\nContact with Moving or Falling Objects Contact with moving or falling objects is a significant hazard to personnel involved in work at height, as well as to personnel who may be working in the areas beneath operations conducted at height. Access to the area in which you are working from a ladder should be restricted by barriers when moving vehicles, trailers and hand bogies pose a collision risk. Suitable barriers and signs should be used to keep the access roads used by Mobile Elevated Work Platforms clear. This will reduce the risk of collision with structures, people and other mobile plant and vehicles. Care should still be taken to ensure that the platform’s boom or knuckles do not impede the access and working areas used by other plant and vehicles.\nSafe working practices should always be followed to prevent the fall of materials from a height. All tools, equipment and materials essential to work aloft should be appropriately stowed to ensure that they do not fall. Any excess materials and all debris should be removed from elevated work platforms, all loose items should be placed in storage boxes, bagged or secured where possible, edge protection should be used to prevent items falling from sloping roofs and all items you need to carry while using a ladder should be held in a belt pouch. Where there is still a possibility that materials may fall from a height, access to the work area should be restricted by barriers and appropriate warning signs or by posting a banks-man.\nElevated Working Platforms Where work at height cannot be avoided, safe working practices should always be followed to reduce the risk of falling any distance which could cause injury. You should always use an approved and safe means of gaining access to elevated working platforms. Wherever possible, work at height should be carried out from an appropriate working platform with edge protection, making use of a safety harness and any other available work equipment and fall arrest systems to prevent falls from height. Where this is not possible, work of a light nature and short duration can be undertaken from a ladder, while other work can be undertaken by suitably trained, experienced and medically fit individuals using a body harness and other fall arrest equipment identified during detailed planning. You should never use temporary makeshift working platforms.\nWork platforms on scaffolding should have full guardrails and toe boards. A safety harness is required if full guarding is not possible, or if the work involves reaching over. Mesh should be installed if scaffolding is designed specifically to store bulk materials.\nAppropriate fall arrest equipment attached to a suitable anchor point inside the platform should be used whenever you work from a mobile elevated work platform.\nEdges of roofs from which a person could fall should be provided with a barrier that is properly constructed, anchored and fitted with toe boards as appropriate. Roof openings should be protected by barriers or covers. Fragile roof areas should be identified and protected by a barrier, or fitted with crawling boards, crawling ladders or duck boards that are properly supported.\nWhere ladders must be used, they should be securely lashed at the top, tied at the bottom, or held by a second person to ensure that they do not move or slip. Ladders should be long enough to allow 3 or 4 rungs above the working step, 3 points of contact should be maintained at all times and a harness clipped to a secure anchor point should always be used.\nStructural Integrity Elevated working platforms should be structurally sound to prevent accidents which could lead to falls from height. All scaffolding should be erected by trained and competent personnel, they should verify that all materials are fit for purpose and that the load bearing ground and surfaces are fit for purpose. Never be tempted to make unauthorised alterations to scaffolding. Prior to using any scaffolding on site, you must check that the scaff tag is in date and valid. Scaffolding should be inspected weekly and following adverse weather conditions by a competent person. If you are at all unsure that the appropriate inspections have been carried out, do not use the scaffolding and report to your manager.\nAll ladders should also conform to the relevant Standard, be free from patent defect, and be inspected once a week by a competent person. If you have any reason to doubt that a ladder has been appropriately inspected, do not use it and report to your manager.\nLanyards should be inspected before each use and should be subjected to a detailed inspection by a competent person at least once every six months. If you have reason to think that any fall arrest equipment has not been properly inspected, please do not use it and discuss the matter with your manager.\nPlanning To reduce the risk of personnel falling all work at height should be suitably planned. Where the use of a body harness cannot be avoided for work at height, a competent manager should provide a plan which incorporates safe access and egress, as well as the type of harness and double lanyard that should be used.\nA suitable rescue plan should also be in place where a harness or restraints are used. The rescue plan should ensure that effective communication links have been established so that assistance can be summoned.\nTraining All personnel need appropriate training and experience to safely carry out operations which involve working at height. Only suitably trained and authorised personnel are allowed to use mobile elevated work platforms and body harnesses. Cradles should only be used by personnel who are suitably trained and experienced.\nHuman Factors Human factors play a significant role in safe working at height. For this reason, personnel should not work at height if you are tired or otherwise unable to give your full attention to the task at hand. If you feel that your medical history, age, health or fitness might affect your ability to carry work at height, please discuss this with your manager. Remember that you should never come to work in possession of, or under the influence of, alcohol or drugs. These substances expose everyone on site to risks that are unnecessary and easily avoided.\nEnvironments The risks associated with work at height can be increased by the environment in which you are required to work, Hot and cold environments can result in physical stresses and affect concentration, as they have the potential to raise or lower you body core temperature beyond safe limits. Confined spaces limit your ability to move around, can be poorly lit and may increase the length of time taken to provide medical assistance in the event of an emergency. It is therefore extremely important that you follow safe working practices when working at height in these environments.\nHot Works Particular attention should be given to establishing a safe place of work and to following safe working practices when you carry out hazardous tasks, such as hot works, at height. The risks associated with fumes, explosions and electric shock can result in falls from height. Ensure that you use appropriate Personal Protective Equipment, or PPE, that you use and store gas bottles safely and that all items of electrical equipment carry a valid appliance testing label.\nPersonal Protective Equipment PPE PPE is an important means of reducing the risks associated with work at height. At least standard PPE with safety glasses, construction hard hats and gloves should be worn, with a harness attached to suitable fall arrest equipment while accessing or working on elevated platforms. Additional PPE, such as dust masks, breathing apparatus and hearing protection may be required for particular tasks, as outlined in the relevant risk assessment.\nYou should also wear close fitting garments, not wear a tie or jewellery and tie back long hair and long beards to reduce the risk of entanglement. Only PPE in good working condition will reduce the hazards associated with your work, so it is important to remove, clean and store your PPE correctly.\nElectricity and Overhead Power Lines Electric shock can result in death and severe burns. In addition, serious injury can occur if an individual falls from height as a result of an electric shock. All items of electrical equipment should carry a valid test certificate or label to reduce the risk of electric shock. Testing should be carried out by suitably trained individuals. If any item of electrical equipment does not carry a valid test label, it should not be used and you should inform your manager. You should also carry out a visual pre-use inspection of all equipment to ensure that it remains undamaged.\nBecause of the risk of electrocution, aluminium ladders should never be used where there is a risk of contact with electricity.\nElectric shock can also occur as a result of contact with overhead power lines. Where mobile elevated work platforms are to be used near overhead power lines, an appointed person should plan and supervise operations. Power lines should be made dead where at all possible. Where this is not possible, work under power lines should be of short duration and all appropriate restraints and barriers should be in place to ensure that the platform cannot reach or accidentally come into contact with live power lines.\nMobile Elevated Work Platforms MEWPs Other safe working practices are necessary to prevent mobile elevated work platforms from overturning during work at height. The manufacturer’s safe working load should never be exceeded and the platform should never be operated in high winds. You should also avoid handling sheet type materials in windy conditions.\nWhere the use of a platform on uneven ground cannot be avoided, they should be used at reduced speed, they should not be turned on slopes and the use of road plates should be considered.\nEmergency Procedures In the event of an emergency while working at height, isolate any equipment that you have been using, make your work area safe and use a safe means of egress to evacuate the area according to the site emergency procedures. If you are involved in an accident, seek immediate assistance.\nRiskatmedia – Safety Training Videos"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:b8296e63-7689-494c-b3de-2fb80a640cef>","<urn:uuid:f3bc2f0a-bcef-4c4f-91c5-5420f2352d3b>"],"error":null}
{"question":"How are healthcare facility infections reported and monitored, and what are the protocols for vaccine delivery systems?","answer":"Healthcare facilities must have written policies for identifying, reporting, and investigating infections in patients and hospital personnel. The infection control professional must report increased nosocomial infections to the Department of Health and immediately report communicable diseases to city, county, or district health officers. The program must be integrated with quality assurance programs and include corrective action plans. Regarding vaccine delivery, the HSE National Cold Chain Service manages temperature-controlled distribution through United Drug distributors. Approved doctors can order vaccines monthly online at www.ordervaccines.ie, and vaccines are delivered directly to GP surgeries and Local HSE Offices, with emergency supplies available when needed.","context":["Title: Section 405.11 - Infection control\n405.11 Infection control. The hospital shall provide a sanitary environment to avoid sources and transmission of nosocomial infections and of communicable diseases which may lead to morbidity or mortality in patients and hospital personnel. The hospital shall establish an effective infection control program for the prevention, control, investigation and reporting of all communicable disease and increased incidence of infections, including nosocomial infections, consistent with current acceptable standards of professional practice. The hospital-wide infection control program shall be reviewed as frequently as necessary but not less than once per year, and updated as necessary to promote optimal effectiveness.\n(a) Organization. The hospital shall designate an infection control professional who is responsible for the development and implementation of a hospital-wide infection control program. This individual shall be qualified by training in infection surveillance, prevention and control and also have knowledge or job experience in epidemiological principles, infectious diseases and infection control procedures.\n(b) Nosocomial surveillance, prevention and control. The hospital-wide infection control program shall include processes designed to reduce the risk of endemic and epidemic nosocomial infections and communicable diseases in patients and hospital personnel. Such processes shall include methods to:\n(i) collect and analyze surveillance data including case findings and identification of epidemiologically important nosocomial infections and communicable disease;\n(ii) prevent or reduce the risk of nosocomial infections; and\n(iii) control the spread of infection and communicable diseases and epidemiologically important organisms.\n(c) Reporting of infections and communicable diseases. There shall be written policies and procedures for identifying, reporting and investigating infections, and communicable disease of patients and hospital personnel, both community acquired and nosocomial. The professional responsible for the hospital-wide infection control program shall report to the Department of Health, in a manner specified by the Commissioner of Health, any increased incidence of nosocomial infections, as designated in section 2.2 of this Title and defined by the department, or nosocomially acquired communicable disease designated in section 2.1 of this Title. This individual shall also report, immediately, the presence of any communicable disease as defined in section 2.1 of this Title, to the city, county, or district health officer.\n(d) Integration with the quality assurance program. The professional responsible for the hospital-wide infection control program shall ensure that all hospital infection control activities are integrated with the quality assurance program required by section 405.6 of this Part, including identification, assessment and correction of problems related to infection and communicable disease control.\n(e) Infection control education. The hospital shall require compliance with written requirements for orientation and ongoing education programs that are relevant to the hospital's infection control program for all personnel whose activities are such that they are at risk of directly or indirectly contributing to the transmission of infection or communicable disease from or to patients, other health care personnel or themselves.\n(f) Corrective action plans. The hospital shall be responsible for the implementation of acceptable corrective action plans related to infection control and resulting from problems identified through quality assurance or regulatory oversight activities and the professional responsible for the hospital-wide infection control program shall report to the chief executive officer progress in correcting identified problems.\n(g) (1) The hospital shall possess and maintain a supply of all necessary items of personal protective equipment (PPE) sufficient to protect health care personnel, consistent with federal Centers for Disease Control and Prevention guidance, for at least 60 days, by August 31, 2021.\n(2) The 60-day stockpile requirement set forth in paragraph (1) of this subdivision shall be determined by the Department as follows for each type of required PPE:\n(i) for single gloves, fifteen percent, multiplied by the number of the hospital’s staffed beds as determined by the Department, multiplied by 550;\n(ii) for gowns, fifteen percent, multiplied by the number of the hospital’s staffed beds as determined by the Department, multiplied by 41;\n(iii) for surgical masks, fifteen percent, multiplied by the number of the hospital’s staffed beds as determined by the Department, multiplied by 21; and\n(iv) for N95 respirator masks, fifteen percent, multiplied by the number of the hospital’s staffed beds as determined by the Department, multiplied by 9.6.\n(3) The Commissioner shall have discretion to increase the stockpile requirement set forth in paragraph (1) of this subdivision from 60 days to 90 days where there is a State or local public health emergency declared pursuant to Section 24 or 28 of the Executive Law. Hospitals shall possess and maintain the necessary 90-day stockpile of PPE by the deadline set forth by the Commissioner.\n(4) In order to maximize the shelf life of stockpiled inventory, providers should follow the appropriate storage conditions as outlined by manufacturers and inventory should be rotated through regular usage and replace what has been used in order to ensure a consistent readiness level, and expired products should be disposed of when their expiration date has passed. Expired products shall not be used to comply with the stockpile requirement set forth in paragraph (1) of this subdivision.\n(5) Failure to possess and maintain the required supply of PPE may result in the revocation, limitation, or suspension of the hospital’s license; provided, however, that no such revocation, limitation, or suspension shall be ordered unless the Department has provided the hospital with a fourteen day grace period, solely for a hospital’s first violation of this section, to achieve compliance with the requirement set forth herein.\nVOLUME C (Title 10)","Vaccine Ordering and Storage\nAll vaccines are stored and delivered under temperature controlled conditions by the HSE National Cold Chain Service which is managed by United Drug distributors. Approved doctors may order vaccines on a monthly basis online using the following website www.ordervaccines.ie\nIf you have any questions about your order please contact the HSE National Cold Chain Service by Phone 01 463 7770 or Fax 01 463 7788 or Email firstname.lastname@example.org\nVaccines are then delivered directly to GP surgeries and Local HSE Offices. Additional supplies can be delivered in an emergency.\nFor more details about the HSE National Cold Chain Service see:\nSPCs and PILs for the following vaccines are available on www.hpra.ie (as per the \"Vaccines available from HSE National Cold Chain Service\" download above)\n- 4 in 1 - Infanrix-IPV\n- 6 in 1 - Infanrix Hexa\n- Hepatitis A - Havrix Monodose, Havrix Junior Monodose\n- Hepatitis B - Engerix (adult), Engerix (paediatric), Fendrix (renal insufficiency), HBVAXPRO 5mcg, HBVAXPRO 10mcg, HBVAXPRO 40mcg\n- Hepatitis A+B - Twinrix (adult), Twinrix (paediatric) Influenza - Influvac\n- Hib - Hiberix\n- HPV - Gardasil9\n- Influenza - Influvac\n- MenACWY - Nimenrix\n- Men C - Menjugate\n- MMR - Priorix and MMR Vax Pro\n- PCV - Prevenar 13\n- Pneumococcal - Pneumovax 23\n- Td - DiTe Booster\n- Tdap - Boostrix\n- Td/IPV - Revaxis\n- Tdap/IPV - IPV-Boostrix\n- Tuberculin - 2TU\nDetails of vaccines not on www.hpra.ie\nAdverse Reaction Report Form. Please visit the Health Products Regulatory Authority website to download the form and report an adverse reaction. You can also report adverse reactions online from this website.\n- Vaccine refrigerators are recommended for storage of vaccines. Manufacturer's recommendations on storage should be observed. Vaccines must be kept at temperatures between 2-8oC.\n- Vaccines should be stored in the pharmaceutical refrigerator which should not be overfilled to allow air to circulate around the packages. They should not be stored on the shelves or storage compartments of the door of non pharmaceutical refrigerators.\n- The vaccine packs should not touch the sides or back of the refrigerator.\n- Door opening should be kept to a minimum\n- A maximum/minimum thermometer should be used in refrigerators where vaccines are stored, irrespective of whether the refrigerator incorporates a temperature indicator dial. The maximum and minimum temperatures reached should be monitored and recorded daily. Temperature record logs are best kept close to the refrigerator for ease of reference. If temperatures are recorded outside the permitted range or if there is a breakdown in supply or equipment, the local Senior Medical Officer should be contacted for further advice.\n- The vaccine refrigerator should be cleaned every two months with a 1:10 solution of sodium hypochlorite.\n- Care should be taken to ensure that the electricity supply to the vaccine storage refrigerator cannot be accidentally interrupted. This can be achieved by using a switchless socket or by placing cautionary notices on plugs and sockets.\n- Food and drink must not be stored in refrigerators used for vaccines.\nDisposal of Vaccines\nReconstituted vaccine must be used within the recommended period, varying from one to four hours, according to the manufacturer's instructions. Single dose containers are preferable as once opened, multidose vials must not be kept after the end of the session. Unused vaccine, spent or partly spent vials should be disposed of safely by incineration. Contaminated waste and spillage should be dealt with by heat sterilisation, incineration or chemical disinfection as appropriate.\nExpired vaccines should be returned to the HSE National Cold Chain Service at the next delivery.\nNeedles and Syringes\nNeedles and syringes must be securely stored and delivery and distribution recorded. Needles and syringes should be disposed of in sharps bins. Sharps bins must not be left unattended in schools. Sharps bins should be collected regularly and be disposed of safely.\nThis page was updated on 13 January 2020"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:2d4d03c0-3265-4591-956b-4e6f0c9e7bed>","<urn:uuid:a05f0d3e-a336-40f3-bc12-cdc5b4402873>"],"error":null}
{"question":"How does the handling of dough differ between high-hydration sourdough at 74.5% versus standard 60% hydration bread?","answer":"A 74.5% hydration dough, being wetter and stickier, requires specific handling techniques including light, brief touches to prevent sticking, and needs strong gluten development through mixing and fermentation to maintain shape. It must be shaped tightly but handled gently to avoid degassing, and requires lateral support during proofing to prevent spreading. In contrast, a 60% hydration dough is considered more beginner-friendly as it's drier and easier to handle. The lower hydration creates a denser loaf structure, while the higher hydration leads to a more open crumb structure. Both hydration levels need proper shaping, but the higher hydration demands more careful attention to prevent sticking and maintain structure.","context":["Scoring Bread made with high-hydration dough\nScoring hearth loaves made with high hydration doughs is a challenge. Expressions of frustration with this in TFL postings are not rare. Much good advice regarding how to accomplish nice scoring of wet, sticky dough has been offered, but it is scattered. So, I thought I would share my own advice on this subject in one place.\nThese two bâtards are San Joaquin Sourdoughs. (For the formula and procedures, please see San Joaquin Sourdough: Update. Today's bake was different only in that I used just 100 g of 100% hydration starter.) The effective hydration of this dough is 74.5%. It is a sticky dough and a good test of one's shaping and scoring abilities. Yet, as you can see, it is possible to get nicely shaped loaves from this dough with cuts that bloom nicely and form impressive ears.\nThe key points in achieving this are the following:\nA Key Point\nGluten must be well-developed by mixing and fermentation. Good dough “strength” is important for crumb structure, but also for successful shaping. It is even more critical in wet doughs, because these tend to spread out and form flat loaves if their shape is not supported by a good, strong sheath of gluten.\nPre-shaping and shaping can add to dough strength through additional stretching of the dough in the process of forming the loaves. A wet dough like this needs to be tightly shaped. This is a challenge, because it also has to be handled gently. Rough handling will result in excessive de-gassing and a dense loaf. It will also tend to make the dough stick to your hands more. When it sticks, it tears and makes weak spots in the loaf surface which are likely to burst during oven spring. The goal is to form the tight gluten sheath by stretching the dough and sealing the seams while avoiding downward pressure on the dough pieces being shaped. “An iron hand in a velvet glove.” Dough sticking to your hands can be decreased by lightly flouring your hands, wetting them or oiling them. However, the most helpful trick is to touch the dough lightly and as briefly as possible each time.\nThe loaves need to have lateral support during proofing. This is to prevent them from spreading out. Support can be provided by a banneton (proofing basket) or on baker's linen or parchment, where folds in the couche material, sometimes reinforced with rolled up towels or the like under the material, provide the support. (I suppose the “ultimate support” is provided by a loaf pan.)\nThe ideal material to support proofing loaves is absorbent. Baker's linen, cloth-lined bannetons and floured, coiled cane brotformen all absorb some moisture from the surface of the loaves in contact with them. This makes that surface a bit less sticky and easier to score without the cut edges sticking to the blade excessively. (I do not want the loaf surface so dry it forms a “skin.”) I like to proof loaves with the surface I am going to score on the absorbent material. This means baguettes and bâtards are proofed smooth side down (seam side up). Note that baking parchment is not absorbent, so, while advantageous for other reasons, it is not ideal for this purpose.\nLoaves should not be over-proofed. A greatly over-proofed loaf may actually collapse and deflate when scored. Short of that, it will still have less oven spring and bloom. This is a relatively greater problem with high-hydration doughs which are more delicate to start with. I find the “poke test” as reliable as any other criterion for when a loaf is ready to bake. However, it is not quite as reliable with very wet doughs. Neither is the degree of dough expansion. You just have to learn through experience with each formula when it is perfectly proofed.\nLoaves should be scored immediately after transferring to a peel and immediately before loading in the oven. Letting high-hydration doughs sit too long on the peel is asking them to spread out, especially if they have been scored ,which disrupts the supportive gluten sheath.\nThe wetter the dough, the shallower the cuts. This is not as critical for boules, but, for long loaves like baguettes and bâtards, if you want good bloom, and especially if you want good ear formation, The cuts need to be very shallow (about 1/4 inch deep) and at an acute angle (30-45 degrees). A deeper cut creates a heavy flap that will collapse of its own weight and seal over, rather than lifting up to form an ear as the cut blooms open. The cuts made on the loaves pictured here were barely perceptible on the unbaked loaf surface. Resist the temptation to re-cut!\nMinimize dough sticking to the blade and getting dragged, forming a ragged cut. The cuts need to be made swiftly and smoothly, without hesitation. A thin, extremely sharp blade is best. Some find serrated blades work well for them. I find a razor blade on a bendable metal handle works best for me. The cuts are made with the forward end of the blade only, not the whole length. Some find oiling or wetting the blade lessens sticking. I have not found this necessary.\nHumidify the oven with steam during the first part of the bake. This delays firming up of the crust which would restrict the loaf from expanding (oven spring) and the cuts from opening (bloom).\nMost of these points apply to scoring in general. I have indicated where there are differences or special considerations applying to high-hydration doughs.\nFinally, a mini-glossary:\nScoring refers to the cuts made on the surface of the loaf prior to baking. The primary purpose of scoring is to create an artificial weak spot and direct expansion of the loaf to it so the loaf doesn't burst at some random point. Secondarily, the scoring pattern influences the final shape of the loaf. And lastly, the pattern of cuts can be decorative and, if unique, can serve as a “signature” for the baker.\nOven spring is the expansion of the loaf when exposed to oven heat.\nBloom refers to the opening up of the scoring cuts during oven spring. The French term for this is grigne.\nEar, when pertaining to bread, is a flap of crust that separates from the surface during oven spring and bloom.\nFor additional information regarding scoring and a more basic introduction to this topic, please see The Scoring Tutorial Also, excellent examples of shaping and scoring can be found in videos on youtube.com, particularly those made by Ciril Hitz, and on the King Arthur Flour web site. I have not found any that address the peculiar challenges presented by higher-hydration doughs, however.","Over the last couple of weeks we’ve had a look at two of the four main ingredients in bread – flour and yeast – and today I’d like to have a think about a third, water.\nNow you may not think that there is much to say about water and you might be right. I mentioned the other day that I don’t think using bottled water is worth the cost and although we’ve recently started using a water filter jug I haven’t noticed any difference in the bread using filtered water. I thought instead I’d look at it from two different angles – how much to use (hydration), and what you could substitute it with.\nHow much water you should put in your dough is one of those questions that doesn’t ever have a straight answer. Different flours absorb different amounts (e.g. Wholewheat flour needs more water because the larger bits of wheat are able to absorb more than finer white flour). Equally some breads require different hydration levels because their particular characteristics demand it (e.g. Ciabatta gets its soft, open structure in part from its high water content).\nA general rule is to use as much water as you can handle – remember that wetter is better! If you haven’t baked much before you’ll probably want to start with a slightly drier dough, maybe around 60% hydration (we looked at baker’s percentages a few weeks ago so feel free to refer back if that doesn’t make sense). As you get more used to handling dough (with help from your trusty dough scraper!) you can increase the hydration. I think the highest I have gone is about 80% but I have heard of sourdough loaves going as high as 100%. I tend to work somewhere around 70%.\nYou may be wondering why the water content is so important? Well what happens to make bread go stale? It dries out as the the starch bonds crystallise so it stands to reason that the more water there is to begin with the longer it will take to dry out.\nIncidentally this is the same reason you shouldn’t cut it as soon as it comes out of the oven – all that lovely steam that comes out is really just the water escaping and means that the bread will go dry quicker.\nThe other reason is simply that more water makes for a softer, more open bread whereas less water makes for a drier, more dense bread.\nOne of the fun things about baking is that one you’ve got the hang of the basic recipe you can play around with it as much as you like. There are lots of alternative liquids you can use in place of the water in different types of bread and I’ll just share a few that I have used here. The important thing to remember is that the hydration level is the combined amount of liquids so don’t put 300g of water and 300g of milk into 500g flour!\nI frequently add a bit of oil to my dough, this gives it a slightly silkier feel and slows down the staling process. If you use really good oil it can add an extra dimension to the flavour as well.\nThis is what I use most often. Swapping up to half the water for milk gives a softer, richer dough. I generally do this for pizza and sometimes put a bit in normal bread as well.\nSubstituting the water for a bottle of ale (minus a few sips obviously!) gives a great, rich bread. The darker the ale the better in my opinion (both for baking and drinking). Because the alcohol can inhibit the yeast it may need a slightly longer rise but it is well worth it.\nJust like ale, cider is a great alternative to water. I once made a beautiful loaf of bread which used cider and had chunks of apple and cheese in it. Lovely stuff!\nI make hot cross buns for our good Friday service at church each year and I use orange juice as the main liquid which complements the fruit and spices beautifully."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:c3a9deae-0662-469d-a8ca-6bb91994637a>","<urn:uuid:9196b5fc-d038-4aa8-85b9-9d73b2a5091a>"],"error":null}
{"question":"Hi! I am planning to take art lessons. What are basic art techniques taught to beginners and how are safety precautions incorporated in art classes?","answer":"Basic art techniques taught to beginners include drawing with various materials (pencil, charcoal, chalk), mark making, continuous line drawing, tonal work, color theory, painting techniques, and printmaking. Students learn fundamental skills like creating tonal ladders, hatching and crosshatching, brush work and blending, and various printing techniques including poly-block printing. Regarding safety, art classes typically begin with a health and safety lesson covering proper use of materials and equipment. Students learn how to organize their workspace, properly handle art materials, and follow safety protocols. In printing workshops specifically, group sizes are kept small (around 6 people) to ensure safe access to equipment like printing presses, and proper instruction is given on handling tools and materials.","context":["Lino Printing Evening Class\nA two-week introduction to the reduction lino printing technique. We begin with an introduction to the reduction printing process and then you will begin to design and carve your two or three colour design. By the end of the course you will have a good understanding of how to set up your studio space, which inks to use and how to register and print a multicolour design. You will be able to use the relief press, but also you will be shown multiple techniques for lino printing at home.\nYou will leave with … five two-colour prints of your design on paper and the skills and knowledge to continue printing at home. If you select ‘with lino printing kit’ you will also take home a small kit, including a five-blade wooden handle cutting set, a roller, block of lino and tube of ink. This is everything you’ll need to continue printing at home.\nWho is this workshop for? Beginners with no experience in printmaking and those with a little printmaking experience who would like to refresh their knowledge.\nMaterials and equipment: All printmaking tools, equipment and materials will be provided. Please bring along your sketchbook and any special reference material, as it helps if you have some design ideas in mind to work from.\nLino kits and blades will be available to purchase in advance or on the day. Please select ‘with lino printing kit’ below.\nImportant details: Group size is limited to 6 people to give you plenty of space to work in a friendly and relaxed atmosphere and to allow you good access to the press. There will be plenty of opportunities to ask questions throughout the course. The workshop runs over two Tuesday evenings, beginning each day at 18:00. Please arrive five minutes before the first session for a cup of tea or coffee and introductions. The workshop will finish at approximately 20:30, depending on the complexity of your design and the speed at which you work.\nTuesday 26 March and Tuesday 2 April 2019\nMy interest in art has taken me all around the world, working in the Australian outback, London’s Mayfair and now back home in Essex. I studied printmaking at the OCA, Gainsborough's House and Colchester Institute and really enjoy helping people discover how easy it is to create their own designs using simple printing processes. I opened Slamseys Art so that everyone can have a go at learning new creative skills.\nFind out more about my printmaking and view student work on Instagram.\n\"We really had a fantastic time learning how to screen print and creating our own designs and would love to do more with Slamseys.... Highly recommend both in terms of the fab venue and teaching!!\"\nI go on courses when other people go on holiday and as it happened 2 booked courses had been cancelled. It was at this stage I discovered Slamsey’s printing courses and had a rush of blood to the head, booking 4 courses in the Autumn 2017 programme. I started to panic: What if I didn’t the first one and had paid the fees for 3 more? Would the content overlap too much that I wouldn’t learn much and get bored? What is I didn’t like the tutor?\nIt has been a total success! These courses are equal in standard to the mosaic courses run the artist Anne Schwegmann-Fielding, another professional artist, and previously the best I have attended.\nRuth Wheaton is a very experience tutor class and gently moves students on so that the all the activities are completed and yet there is flexibility. The equipment and block printing ink are freely available when you need them and there are lots of helpful tips along the way so that you can continue at home. She has a real love and understanding of the print-making process.\nThe description on the website is full and very accurate and if you explore the super website you will see the standard of work which is possible achievable to.\nEach course covered a different area. To me learning how to build up a design and use the colour wheel to make all the colours needed from red, blue and yellow ink has been really useful. I believe I have a acquired very thorough grounding and with practice at home, and likely achieve very satisfying results.\nThe details list the kind of people who will enjoy the courses. I would say that with Ruth’s very gentle teaching style, providing you consider yourself ‘crafty’ you will get a great deal out of them. You need no more knowledge than having done potato printing and having enjoyed the process. You will meet a whole variety of people. I call myself a mosaic artist, but not having done any printing in a degree module for nearly 20 years because of a depressive illness resulting in complete loss of confidence.\nEverything associated with Slamsey’s is of the highest quality and matches the branding of the website. The barn is a very large airy space and very conducive to encouraging art activities. There being no more than 6 in a class there is no need for the diffident to feeling overwhelmed. …. and there is plenty of parking!\nWhat more can I say? Only I am already planning to join more course in the future. My money was well spent!\nThe 5 Week Printmaking Short Course was a perfect introduction to a variety of ways of printing art, that could easily be replicated at home. Ruth is a very talented artist who is able to generate an enthusiasm for what she does during the course. She is very patient and allows each member of the class to 'do their own thing'. The surroundings in which the course takes place are ideal and the equipment provided is very good. I will certainly continue to print art at home and have already begun collecting a few things with a project in mind. I thoroughly enjoyed each morning.\nThe Fabric Printing One-to-One course was fantastic. We were so impressed with the course from booking to finish. Ruth was flexible and accommodating when we needed to change our booking at the last minute. We would just love to do more printing now! The follow up information about where to purchase supplies etc. is extremely helpful. This will ensure we carry on!\nI found the Fabric Printing Short Course really interesting and informative, especially as this is something I've never tried before, it was very good value for money. I especially liked the small class and the laid back approach Ruth has, it really does makes people feel at ease and able to express themselves individually. Also being able to experiment with different techniques and mediums. The cakes were a real treat too!","|First lesson; Health & Safety lesson, introduction to the art department, expectations and rules.|\nLesson two, Poster on health and safety in the art room. Use pencil.\nWhat is Art, Craft & Design?\n-Definitions of Art, Craft and Design\n-What is Historic and Contemporary art?\nWhat is Art? PP discussion, researching your favourite piece. Looking at and discussing art using the formal elements. Definition of 2D, 3D and relief.\nResearching artists and discussing their work.\nInvestigating Conceptual art.\n- Using the internet to collaboratively explore the Art galleries in the North East and what they offer us....local artists.\n- Investigating the periods of art throughout time.\nCreating our own art timeline\nMaking quizzes and interactive games to explain the periods of art.\n|‘What is a drawing’ |\n- An introduction to new approaches and techniques in drawing pp\nDiscussion of mark making and ‘taste’ group discussion and sharing ideas. (Encouraging pupils to build upon their confidence and articulate their thoughts and opinions on Art)\n- Experimenting with new drawing materials and techniques and evaluating them.\n- Drawing from observation using line.\n- Continuous line drawing from observation.\n- Tonal ladders. Tonal shapes using pencil, charcoal and chalk.\n- Textural studies, hatching and crosshatching, pattern.\n- Investigating our senses, Kandinsky and responding to different genres of music.\n- Scaling/gridding up a face and using tone.\n- Researching an artist.\n|Painting and Colour\n- Organising your space, materials and looking after the art equipment.\nIntroduction to colour theory, new vocabulary (primary, secondary, complementary and tertiary colours., colour wheel, colour mixing, tints, shades complementary colours, hues)\n- Thinking skills colour, moods and associations, chakras, artist research.\n- Brush work and blending skills.\n- Research into artist (Klaus Haapaniemi/Hundertwasser)\n- Start to plan out monster /imaginary landscape/redesigned school building image.\n- Proportions of the face, hair texture and skin tones in pencil/charcoal and chalk.\n- Mixing and painting skin tones in paint.\n- Investigating portrait artists research (Chuck Close, Lucien Freud, Freda Kahlo, Robert Arneson)\n- Independent learning; “Finding portraiture artists I like which may influence my work” power point presentation.\n- Expressions and emotions.\n- Looking at different compositions.\n- Creating a self-portrait in paint.\n- Mounting and displaying my work.\n- Investigating the history of the gargoyle. Independent research on the p.c what is relief art? 6 facts on gargoyles, find 3 images and draw one.\n- Research page on gargoyles from info on pp.\n- Reference to prior learning; proportions/expressions of face/exaggerating features.\nDesigning your gargoyle considering the relief.\n- Note taking during clay/papier mache demo noting health & safety. Step by step instructions to self.\n- Make relief gargoyle, paint, add texture effects.\n|Poly-block printing & collage.\n- Investigating printing techniques, mono printing, poly-block ( extension; lino)\n- Research portraits and printing Dain, Andy Warhol and Papaya Arts.\n- Independent learning; find your own portraits which will inspire your own ideas.\n- Collage techniques; creating backgrounds.\n- Planning and composition variations.\n- Poly-block printing to create detail of face.\n(Pair work creating a giant bag of crisps using recycled materials)\n- Investigating packets of crisps through observational drawings and photographs.\n- Looking at proportions and scale.\n- Researching graphics and packaging.\n- Investigating the form and structure of crisps.\n- Research into Pop Art and Claes Oldenburg’s food sculptures and drawings.\n- Step by step explanations how to make the sculpture, modelling and assemblage techniques, reference to health & safety considerations.\n- Make the sculpture.\n|Surrealism/steam punk drawing.|\nResearch into Dan Hillier/Surrealism.\n- Observational drawings of found objects (skulls, foliage, organic objects etc)\n- Collage using Victorian wood pints and advertising/Mexican day of the dead.\n- 3 variations with a final idea.\n- Add font if desired.\n- Scan final idea and print onto cushions, t shirts and bags.\n- Display of work in school.\n|Surrealist Photoshop project.\nUsing the work from the previous project in term 2, collaged backgrounds will be created then photo shop techniques will be applied to layer scanned drawings into a series of prints and greetings cards.\nPupils will have a chance to sell these in school with the possibility of some work being used in a school calendar which may be sold to parents and staff.\n- Pupil voice online questionnaire to assess the ks3 sow.\n- (An art/photography trip for those pupils opting to study GCSE in year 10. Pupils will produce research pages and document their recent experience of art/photography work; Yorkshire sculpture park, Side Gallery, local Art galleries/events)"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:8bdc76c3-ee53-41d6-8ca7-35ed4d1f15ea>","<urn:uuid:557c411c-b5ed-4267-92b9-c10cde844717>"],"error":null}
{"question":"How has workplace gender equality progressed in women's economic empowerment, and what educational discrimination challenges still persist in achieving workplace equality?","answer":"While progress has been made in women's economic empowerment, with women holding 34% of senior management positions, significant challenges remain. Men still outnumber women in top executive positions by four to one, despite women attributing educational credentials and hard work as keys to promotion. The pace of change has been slow, even in advanced economies, with wide gender gaps in labor force participation, wages, pensions, and leadership positions. Additionally, educational discrimination persists when employers set unnecessary academic requirements or use educational demographics to exclude certain groups, though civil rights laws prohibit discrimination based on race, gender, national origin, disability or age.","context":["Speech: “We must transform the world of work”—Lakshmi PuriRemarks by UN Women Deputy Executive Director and UN Assistant Secretary-General Lakshmi Puri at the Latin America and the Caribbean Regional Consultations for CSW61 in Panama on 6 February.\n[Check against delivery]\nExcellencies, distinguished delegates, dear colleagues and friends,\nBuenas tardes Latinoamerica… tierra de inspiracion!!\nI would like to start by thanking the Panamanian Government for its generous hospitality and leadership in organizing these regional consultations, together with the National Women’s Institute of Uruguay in its current capacity as Chair of the Regional Conference on Women in LAC.\nToday, the government officials and ministers of your countries have gathered to exchange views and build consensus for the common good and progress of your peoples. I could not think of a more inspiring and fitting venue. It is here that UN Women has worked with the LAC parliament on the Framework Law on Parity Democracy and on the Economy of care both critical for Women’s economic empowerment.\nAllow me to share with you how thrilled I am to come back to Latin America for these regional consultations. I had the honor of attending the first of these consultations hosted by El Salvador in 2013, as we were preparing for CSW57, which as many of you will recall addressed as its priority theme the critical issue of preventing and ending violence against women and girls.\nExpectations were very high at the time for a strong outcome and, in the end, we achieved the most forward looking and progressive language and commitments to end gender based violence and adopt a Global plan for implementation.\nAnd Latin America and the Caribbean played a pivotal role in making this happen, through its active and constructive coordination and by spearheading efforts to position fundamental issues, most notably femicide, front and center in the global agenda. We counted on Latin America and the Caribbean then and, at this time, we continue to do so even more. We would count on you to advocate for the adoption of a Global plan for Implementation on Women’s Economic Empowerment in a Changing world of work.\nWith the 61st session of CSW coming up, I would like for all of us to recognize that we build on and contribute to the implementation of the historic and unprecedented gender equality compact of 2015-2016. The motherboard of gender equality and women’s empowerment norms like the Beijing Platform for Action been recommitted to for full, effective and accelerated implementation\nGender equality and women’s empowerment has moreover been prioritized and integrated into every epic normative intergovernmental undertaking. Gender equality and women’s empowerment is at the heart of the 2030 Agenda for Sustainable Development, the New Urban Agenda, the Climate Change Agreement and even the New York Declaration on Migration.\nThis reflects the tremendous work that we all have accomplished together with LAC countries. CSW60 set out a road map for the gender responsive implementation of 2030 Agenda and you had a signal role to play.\nThe theme of CSW61, “Women’s Economic Empowerment in The Changing World of Work”, highlights that women’s economic empowerment is indispensable for the full, effective, accelerated and gender responsive implementation of the Sustainable Development Goals. In particular, the achievement of the six targets of SDG 5 including universal access to sexual and reproductive health and rights are enabled by and dependent on women’s economic empowerment and vice versa. In this regard, CSW61 provides us an opportunity to make decisive strides forward.\nIn achieving women’s economic empowerment, we must transform the world of work, which is still sadly very patriarchal and treats the equal voice, participation and leadership of women as an anomaly, tokenism, compartment or add on.\nProgress has been achieved, but there is still much ground to cover and the progress itself has been slow, even in advanced economies. There has to be an urgent acceleration of the pace of change otherwise it could take another century to close the gender gap.\nWe must address and close the wide gender gaps that exist in terms of women’s labour force participation, wage, income pension, vulnerable and informal employment, social protection, unpaid care work and domestic work, entrepreneurship and leadership, sticky floors, glass walls and ceilings. Employers and governments must also resolutely work to end Violence and sexual harassment at the workplace that women face.\nTo accelerate progress in the changing world of work, we must provide solutions from both the demand and supply sides. From the demand side of jobs for women, we need to build a value chain of confidence and aspiration, education, skills, training and capacity building which lead to decent work and productive employment.\nOn the supply side of the job market we must address the special measures and affirmative action we need for substantive equality to be taken in public and private sectors for the recruitment, retention and promotion of women in all work areas, for improving their conditions of work and ensuring women's human rights at work.\nWe must create an enabling environment for girls, young women and women throughout their life cycle to embark and continue on a journey to economic independence as well as make all women and girls believe they can do anything in the world of work they dream of.\nThe United Nations Secretary-General in his report on the priority theme of CSW61 identifies are four concrete action areas in achieving women’s economic empowerment in the changing world of work:\n- Strengthening normative and legal frameworks for full employment and decent work for all women at all levels\n- Implementing economic and social policies for women’s economic empowerment\n- Addressing the growing informality of work and mobility of women workers and technology driven changes\n- Strengthening private sector role in women’s economic empowerment\nWe must muster together our collective efforts in executing these actions.\nWomen’s economic empowerment requires political will and partnerships of all stakeholders to develop and implement policies that integrate gender equality perspectives in labour and economic institutions and programmes at local, national and global levels and for Gender institutions to coordinate and mainstream this agenda in an all of government approach.\nWomen’s economic empowerment means providing women workers with social protection and income security, as well as recognizing, reducing and redistributing and provisioning unpaid care and domestic work by public and private sector including creating a jobs rich quality paid care economy as advocated for in our Flagship Report on Transforming Economies, Realizing Rights.\nThe adoption of a Comprehensive National Care System by Uruguay sets a pioneering and ambitious example in this regard and is precisely the type of action that is required to make this possible.\nSignificantly increased transformative investment and financing from all sources is required for the full and accelerated implementation of new and existing commitments to empower women economically and provide full and productive quality employment for women.\nWomen's participation and leadership in all professions, including political, and new and emerging sunrise areas like technology and ICT enabled ones and in all contexts of migration, refugees, humanitarian and post conflict peace building at all levels including local levels is a crucial game changer\nThe engagement and solidarity of all stakeholders including men and boys, women's and youth organizations, faith based organizations and the media for movement building to transform gender stereotypes and social norms is vital.\nWe are aware that this is a challenging time for Latin America and the Caribbean. Economic growth has proved elusive for the past two years and projections for 2017 are for a sluggish recovery. It is widely acknowledged that empowering women can unleash the full economic and productive potential of our societies and economies. Feminization of poverty is an impediment to eradicate it from the continent.\nSo let's make this necessity and urgency of rapid economic growth and poverty eradication into a mother of invention by creating an equal opportunity world of work for women. Let us ensure that all your sustainable development, economic growth, poverty eradication and sectoral strategies, plans, programs and investments must give priority to women’s economic empowerment.\nTo redeem the pledge of Agenda 2030 of leaving no one behind and reaching the farthest, first, we must ensure that women in those communities most left behind and marginalized groups including disabled women must be supported through special and targeted efforts and investments.\nYou have much to build on, as LAC has posted truly remarkable achievements that are an inspiration for the rest of the world. I would like to applaud last year’s historic peace agreement in Colombia and its robust integration of a gender perspective across the board. Now, as Colombia prepares to implement these accords, the gender provisions must become a lived reality for Colombian women and girls for peace to be sustainable\nMeanwhile, Bolivia has led the way to reaching parity democracy in all levels of public administration by turning women’s equal representation into law, following the achievement of parity in the National Congress.\nAnd Latin American and Caribbean countries have moved swiftly to ratify ILO Convention 189, enforcing States to extend basic labor rights to domestic workers, including overtime, annual paid leave, minimum wage and safe work conditions.\nFurthermore, as CSW prepares to consider the empowerment of indigenous women as its emerging theme, we have high expectations for this region’s vibrant indigenous organizations and networks to push the boundaries and aim for bolder commitments from Member States and for Latin American and Caribbean countries to showcase their advances in ensuring indigenous peoples and indigenous women their rightful place in the economy, world of work and decision-making at all levels.\nThese groundbreaking advances are what we need to push forward the implementation of the commitments made by Member States to their women and girls. This bold and decisive action is what will allow us to achieve Planet 50-50 by 2030 crucially in the world of work.\nWe count on you, once again, to be a leading force at CSW for the advancement of women and girls in Latin America, the Caribbean and beyond. We need south-south cooperation as also development assistance and ODA to move the needle.\nTo finalize, I would like to pay tribute to a pioneering feminist, Clara Gonzalez de Behringer, who after becoming the first woman lawyer in Panama in 1922, was forced to wait two full years to actually practice her profession, until a ban that forbade women to engage in legal practice was lifted. She went on to found a feminist movement aimed at ensuring women a larger share in public life and their right to vote. Let us draw inspiration from women like Clara who persisted in times of uncertainty and setbacks and, in the end, achieved historic victories for all women.\nLet us adopt here in Panama a groundbreaking Regional Plan of implementation to achieve Women’s economic empowerment in a changing world of work that would inspire and shape a Global plan of implementation to be adopted at the CSW 61.\nMuchas gracias Panamá\nMuchas gracias América Latina","Many employers require you to have a specific type and level of education to qualify for certain jobs. Workplace educational discrimination happens when an employer requires that you have a level of education that isn’t necessary for the job. While there are laws to prohibit educational discrimination, some biases can still emerge in the workplace.\nCalling It Even\nEducation discrimination involves academic-related requirements set by employers that may violate the law. For example, discrimination might be the case if an employer is advertising for “college graduates” to apply because such advertising can deter otherwise qualified non-graduates from applying. Any tests of your skills or knowledge that an employer requires you to take must be necessary and job-relevant as well.\nCivil rights laws in the U.S. prohibit educational institutions from discriminating against you on the basis of race, gender, national origin, disability or age. You can’t be denied the opportunity to get your GED because you’re 50-years old any more than the opportunity to complete a Ph.D. because you’re a woman. Still, discrimination can happen when an employer understands the educational demographics of people in the job market, or educational backgrounds of particular individuals, and sets requirements for hiring or promotion that are meant to exclude certain individuals or groups.\nLaws and Learning\nMany federal laws focus on preventing education discrimination. For example, the Age Discrimination in Employment Act requires that employers let you enter training or apprenticeship programs despite your age. Title VII of the Civil Rights Act of 1964 prevents employers from requiring you to be fluent in English before hiring you for a job unless the employer can prove that speaking English is essential to performing that job. Employers also can’t hold a learning disability against you. They can only decide whether to hire you or not based on your qualifications and capability to do the job.\nMore women than men attribute educational credentials, along with hard work and long hours, as keys to getting promotions at work, according to a 2010 study by the Center for Work-Life Policy. Still, even though the study found that women held 34 percent of senior management positions, men outnumbered women in top executive positions by four to one. Bias against promoting educated and hard-working women all the way to the top is still prevalent in workplaces throughout the U.S. despite widespread improvements to workplace equality in general.\n- FindLaw: How to File a Discrimination Complaint With the Government\n- U.S. Equal Employment Opportunity Commission: Federal Laws Prohibiting Job Discrimination Questions and Answers\n- Center for Work-Life Policy: Harvard Business Review Research Report: The Sponsor Effect: Breaking Through the Last Glass Ceiling\n- U.S. Equal Employment Opportunity Commission: Prohibited Employment Policies/Practices\n- Jupiterimages/Stockbyte/Getty Images\n- What Does EOE Mean in Reference to a Job?\n- The Objectives for a Teaching Resume\n- Define Workplace Discrimination\n- Strategy to Stop Discrimination in the Workplace\n- Promotion Discrimination in the Workplace\n- Age Requirements for the U.S. Army Rangers\n- Can My Boss Force Me to Go to Employee Counseling?\n- Security Guard Training Requirements"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:fb68b89d-2d58-473c-9b58-b705cd83dae7>","<urn:uuid:0255e196-c5f6-420f-bc4c-da342e27ac03>"],"error":null}
{"question":"How do the transportation and access options differ between visiting historical sites in Saint-Imier versus exploring Fribourg's urban areas?","answer":"Saint-Imier's tourism experience is centered around walking tours, initially offered through a paper brochure called 'Saint-Imier à pied' and later digitized into an interactive mobile app that guides visitors through the watch industry history and architecture. In Fribourg, visitors have multiple transportation options: they can use the public transport network Frimobil, cycle through well-established cycling routes supported by organizations like Pro-Velo Switzerland, or explore on foot despite the steep terrain. Fribourg also offers parking solutions through the Agglo Fribourg Park and Ride system for those arriving by car.","context":["Learn how we applied Pindex Tours in Saint-Imier to give the city a new boost of authenticity through a unique tour for its residents.\nIn 2013, the municipality of Saint-Imier publishes « Saint-Imier à pied » (« Saint-Imier on foot » in French), in partnership with Jura bernois Tourisme. This 30-pages paper brochure guides visitors through the watch industry History and Architecture of the city through 2 thematic walks. A map combining the paths and points of interest of both tours shows the route to visitors while texts and images from the official center of documentation and research of the Jura bernois, Mémoires d’Ici, quenches their thirst for knowledge.\nA few years later, our team offers the municipality to refresh the tours from « Saint-Imier à pied » by creating a digital version of them in a mobile app. An interactive map using geolocation will help visitors find their way more easily and naturally. At the same time, the content will be updatable in real-time, without the need to print and publish new brochures. Last but not least, the app will be free for everyone to use and will enable visitors to visit Saint-Imier more spontaneously by keeping them from getting their paper copy of the map at the local tourism office. The new project is accepted, and the new digitized tours are released just before the Summer break in 2020, slightly later than expected.\nHowever, at Pindex, we decide not to communicate about this event. Indeed, even though we fulfilled our mandate, we deem our product only partly appropriate. As a matter of fact, texts are too long and bloat our visitors’ small smartphone screens, and pictures are not enough to compensate for this unfriendly content.\nMid-2020, we propose to the city to digitize both tours one step further to better leverage fantastic smartphone capabilities at our disposal: videos, audio files, augmented reality (AR), quizzes, mini-games, and Internet-of-Things (IoT). Again, this new idea seduces the Economics and Tourism Committee in charge of the project. Therefore, we happily and enthusiastically embark on this new adventure.\nHowever, our core business and expertise really focus on technology, not touristic content creation. Consequently, we partner with Fluorescence, an association of cultural mediation experts, and an independent graphic designer, Sébastien Gerber. Together, they shape up and articulate new original content from Mémoires d’Ici into a complete and coherent story, embellished with testimonials from city inhabitants created especially for this project. On our side, at Pindex, we provide our technical expertise to the artists and augment our app with new media types designed to make both tours more interactive and pleasant to the visitors in Saint-Imier.\nIn this context, we develop the quiz, letting visitors test their knowledge about sports at the old « Magasin de Sport Rochat.» We also design a buildings detection mechanism allowing them to see the evolution of different building façades at the « Place du Marché,» through original anecdotes by a former city resident, Henri Diener. Among other scenario achievements, we mention original recordings of inhabitants telling their memories and a unique audio recording of a musical piece that a resident wrote more than 100 years ago, from which no single work had ever been transcripted digitally.\nIn June 2021, we release the new tour « Saint-Imier vue par ses habitants » (« Saint-Imier as seen by its residents » in French) during an official launch event receiving significant coverage from the local press. As a result, the municipality can now offer its visitors a new high-quality, original experience that will stay in the spirit of the times for a long time, thanks to its digital nature.","Public transport in Fribourg\nFrimobil (DE/FR) is Fribourg's cantonal transport network that provides information on all aspects of public transport within the Canton of Fribourg and parts of Vaud. Purchase tickets at any SBB/CFF counter or ticket machine. Public transport in Fribourg is run by TPF (DE/FR). Check their website for maps, prices and discounts on bus and train travel in Fribourg.\nNote: You must carry exact change for the ticket machines in Fribourg. They do not accept cards, nor do they give change. Buying a ticket through Zone 10 is a good option for most people and will allow you to travel most places in Fribourg.\nTravel passes and basic tips\nThere are several common travel passes that are useful for families and individuals. A few include the Half-Fare pass, Junior pass, GA pass and day passes. Check out this article on the Swiss public transport system to learn about standard regulations and ticket options. Also, keep an eye out for frequent seasonal and holiday offers from SBB/CFF.\nHere are a few helpful tips to get you started on Swiss public transport:\n- You must purchase your ticket before boarding (exceptions made for rural routes with no machine at the stop).\n- Children under six years of age travel for free.\n- Travelling with a dog: Dogs over 30 cm tall (about 12 inches) need to pay second-class half fare (there are also day cards and GA passes for dogs). Small dogs can travel for free in a carrier or basket.\n- Travelling with a bicycle: You are required to buy a supplementary bike ticket. You can bring your bicycle or unloaded bike trailers onto most SBB/CFF trains, private railways and PostBuses. Folded bikes can be stored as hand luggage for free. Note: Capacities for bicycle transport may be restricted during peak traffic periods.\n- Utilise the excellent SBB/CFF apps for iPhone and Android.\nCycling in Fribourg\nLike the rest of Switzerland, Fribourg is a great place for cycling. Pro-Velo Switzerland is the national cycling association of Switzerland which provides comprehensive cycling information for all levels. Visit Veloland, Bikemap, and Routeyou to find national, regional, and local cycling routes in English, as well as maps and other useful information. Fribourg Tourism also offers great ideas for cycling routes in the area.\nFribourg on foot\nAlthough steep in parts, it is easy to explore Fribourg's significant heritage on foot. Fribourg Tourism offers several walking activities and themed guided tours (try their urban golfing!) The WWF walking trail (DE/FR) is a great way to discover Fribourg's urban biodiversity.\nFind more Fribourg activity suggestions in Enjoying Fribourg.\nParking in Fribourg\nStreet parking can be difficult to find in most of Switzerland. For most street parking you need to buy a ticket from a meter to place in your window. Blue zones allow free parking up to 1.5 hours with a parking disc marking your arrival time in the window (buy this at the post office). You can also find parking at most main SBB/CFF stations.\nFribourg is small, but expect some traffic before and after work. Mobility car sharing is a convenient and eco-friendly alternative to owning a car. Fribourg also has the Agglo Fribourg Park and Ride system (DE/FR). These are parking lots on the outskirts of the city with monthly parking options available for those with no parking at their home.\nYou may also be interested in the article Driving and owning a car."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:3052afc1-33fc-4217-a9ab-cf084b6a4b95>","<urn:uuid:7a091a47-312d-4713-8c9a-5b9f313fe621>"],"error":null}