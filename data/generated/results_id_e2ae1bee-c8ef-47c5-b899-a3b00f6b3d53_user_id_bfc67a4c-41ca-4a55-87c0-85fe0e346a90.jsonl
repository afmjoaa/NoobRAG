{"question":"As a marketing manager planning a new campaign, what factors should I consider when choosing advertising media, and how can market mix modeling help optimize media decisions?","answer":"When choosing advertising media, key factors include: advertising objectives (e.g., introducing new products or building image), market nature (local vs national), product characteristics, message type, buyer characteristics (education level, language), media cost and availability, competitor strategies, and distribution approach. For image building, prestigious magazines work well, while urgent messages suit newspapers or radio. Market mix modeling can optimize these decisions by quantifying each marketing vehicle's ROI and effectiveness. It helps identify optimal execution levels while accounting for media saturation, evaluates different channels (TV, digital, etc.), determines ideal campaign durations, and enables sales forecasting. The model considers various factors including base factors, trade merchandising, promotions, media advertising, and seasonality to create an optimal marketing mix plan for achieving target revenue growth.","context":["Media decision is an important decision upon which the effectiveness of the advertising campaign depends. There are wide varieties of media available for advertisers. However, each media has its own features, advantages, and disadvantages.\nTherefore, all media are not suitable for all kind of advertising.\nHow to Choose Right Advertising Media Decision\nThe advertiser has to consider the following factors while choosing the right media.\n1. Objective of Advertising\nThe objective of advertising is an important factor affecting media decision.\nThere may be the number of objectives of advertising such as to introduce a new product, to expand the market, to remind, to counter competitors advertising campaign, to remove misunderstanding.\nIf the objective of advertising is to build an image, it can be advertised in a Prestigious magazine. In case, a message is to be given to the customers immediately, it would be better to use newspaper or radio.\n2. Nature of the Market\nThe nature of the market also affects the media of advertisement.\nIf the advertising is confined to local markets, advertisements may be given in local newspapers, cinema slides or by outdoor advertising.\nOn the other hand, if advertising is extended to the entire country, newspapers, and magazines of national circulation, televisions will be selected.\n3. Nature of the Product\nThe nature of products determines the choice of media.\nConsumer goods can be advertised in newspapers, magazines, radio, and television and through outdoor displays, Industrial Products could be advertised through technical and professional journals. 14 Importance of Study Consumer Behaviour (Explained).\n4. Nature of Message\nIt is essential to consider the nature of the message to be given in the advertisement while selecting advertising media.\nMessages can be of different types like the long message, short message, urgent message, the message with pictures and sketches. Accordingly, media may also differ.\n5. Characteristics of Buyers\nIf the message is to be conveyed to the illiterate or less literate people, radio, television, and cinema will serve the purpose in a better way. Magazines, newspapers or direct mail may be used to covey the message to more educated people.\nSince different languages are spoken in different regions, advertisements in different languages may be given to popularize the product.\nSimilarly, age, sex, social status, the standard of living, the economic condition of the prospective buyer may also affect the media decision.\n6. Media Cost\nIt is also essential to consider the cost of media.\nThe media cost would be depending upon its coverage and circulation.\nThe cost to be incurred will be affected by the paying capacity of the advertiser.\nEconomically viable media should be selected. The availability of funds with the advertisers may dictate the choice of media.\n7. Image of the Media\nThe Prestige of media will be depending upon the number of viewers/readers/audience.\nAdvertisers may also consider the media creditability, which also depends upon the type of products being advertised as well as the support of prestigious advertisers to that media.\n8. Media Used by the Competitors\nThe choice of media also depends upon the advertising strategies of competitors.\nThe advertiser cannot afford to dismiss outrightly the media selected by competitors.\nMedia chosen by the competitors are prepared to counter their advertising strategy.\n9. Media Availability\nThe question of media availability is quite relevant while considering media because all media not available all the time to the advertisers.\nDesired space may not be available in the prestigious newspaper or magazine on a particular time/date. Similarly, on TV, Primetime is always in high demand.\n10. Size and Nature of Business Enterprise\nThe size and nature of business enterprise also play an important role in making a choice of advertising media.\nDifferent media will suit departmental stores, small shops, manufactures and produces, etc.\nA big business enterprise may advertise through television, radio or newspaper having a national coverage whereas a small business unit may prefer local newspapers, Cinemas slides, etc.\n11. Distribution Strategy\nThe advertising media should be compatible with the distribution strategy of the company.\nFor example, the company is selling through wholesalers and retailers then Outdoor Media duly supported by television, radio, and newspaper may be helpful, in case, if the company wants to sell direct, mail advertising will be more useful.\n12. Market Requirements\nWhile selecting advertising media, the company’s market requirements should be considered.\nFor example, specialized High Fashion colored magazines would be the most suitable media for consumers with high-income groups and sophisticated tastes.\nSimilarly, Outdoor Media would be appropriate when consumer action is to be induced at the point of purchase.\n13. Life of the Advertisement\nLife of the advertisement varies from media to the Media.\nOutdoor displays, magazines, and direct mail have a longer life but the life of radio, television, and newspaper advertisement is very short unless the advertisements are repeated regularly.\nTherefore, the advertiser should take into consideration the duration for which he wants to create an impression in the minds of the prospective customers.\n14. Media Circulation\nThe company should take into account the circulation of advertising media.\nIf the media has a greater circulation, the message of the advertiser will reach to a larger number of people.\nIt may be mentioned that newspapers have the widest circulation but other media have limited and services circulation.\n15. Comparative Study of Different Media\nThe characteristics, advantages, and limitations of each media should be studied before taking a final decision on a particular media of advertising.\nThat media should be selected which could reach the maximum number of prospects, could gain the attention of the maximum number of readers and have wide coverage at low cost.\nThus, now you know the How to Choose Right Advertising Media Decision and how factors influencing advertising media decision.","Retail CMOs and marketing managers invest 8-10%1 of their sales revenue into marketing. Their biggest concern is how to best allocate their limited budget into a wide range of marketing activities. In a market where competitors often upset a strategic plan, companies have to consider several key factors such as product launches, trade merchandising, media campaign quality, base erosion, products out-ofstock, consumer movement across channels, consumer response, seasonality, holidays, market growth, macro-economic factors and other elements before planning their spend allocation across the various marketing channels.\nTherefore, there is a need for a comprehensive solution which will address these concerns and equip managers to create an effective investment plan.\nMarket mix modeling: a potential solution\nA market mix model is a decision making tool that marketing professionals can leverage to figure out an optimal spend allocation. It is the process of quantifying the impact of each marketing vehicle in terms of ROI and effectiveness. Doing so can help create a roadmap for their future strategy by defining how to plan or adjust their marketing budget and channels, as well as other tasks such as creating pricing and distribution strategies and countering the competition.\nThis technique segments sales into base and incremental components. Base component indicates long term consumer preferences. Incremental components indicate short-term variations due to promotions, temporary price changes, and above-the-line (ATL2 ) and below-the-line (BTL3 ) media activities. It also provides the optimum execution levels of media activities while accounting for media saturation. This helps identify the best marketing channels to invest in, thereby creating effective media campaigns and setting better execution levels. Doing so also assists in running effective discount campaigns while avoiding base erosion, planning trade merchandising in modern retail, creating channel-level strategies, maintaining the right distribution levels, evaluating price elasticity from the competition, as well as evaluating and countering the impact of competitors’ promotions.\nWhat can market mix modeling offer?\n- Identify the marketing channels such as TV, digital media or others that will result in maximum returns + Compare traditional marketing channels with emerging, cost effective ones to address the future needs and requirements\n- Explore which products, brands or markets are more sensitive to advertising and promotions. For example, TV campaigns designed for core brand-form might result in higher returns than a newly launched spin-off\n- Propose the right execution levels for highly effective marketing channels to avoid saturation\n- Investigate successful campaigns, their types or quality scores. For instance, this could track whether campaigns focusing on women in the18-30 years age group would result in higher awareness and returns\n- Compare campaign durations (10sec/20sec/30sec), cable networks (HBO/ABC/etc.), continuous versus pulsing advertising, in-season versus offseason campaigns, etc.\nSimulation & optimization\n- Forecast sales trend as per current marketing mix plan\n- Predict sales impact subject to a planned increase/drop in advertising/ price\n- Create an optimum marketing mix plan for achieving target revenue growth\nKey inputs affecting brand sales\nIn a competitive market, there are many factors which affect the sales revenue. Market mix modeling techniques examine the major factors which are responsible for explaining the sales variation in a given time period, typically a week or a month. These are categorized as:\nBase factors: Sales metrics such as price and distribution are considered as base factors. Competitors’ activities that are not in control of the manufacturers are often considered as base as well.\nTrade merchandising: The marketing activities driven by retailers in the form of display, feature, temporary price reduction, etc. Manufacturer also invests in maintaining a significant shelf share and SKU share in retail outlets.\nPromotions: There are some promotions driven by the manufacturers in the form of promo packs, coupons, new launches, and other methods to promote their product in the market.\nMedia: Media advertising, in the form of various channels such as TV, digital media, radio or other mediums, is one of the marketing activities used by the manufacturers to drive their product into the market.\nSales seasonality and macro-economic: These are other factors which are also responsible for affecting sales revenue for overall market.\nThe table below lists the key factors within each category\nTechniques for variable transformations\nSplitting average price into base price & discounts\nFactors that can affect a product’s average price include the impact of a manufacturer’s discount, the retailer’s temporary price reductions and the stock keeping units (SKUs) level price difference. To evaluate the impact of promotional discounts and base price elasticity, it is important to divide the average price into (i) a base price component, that is how a brand is listed on price relative to other brands, and (ii) a promotional/ temporary discount component. So, a market mix model should identify and distinguish both of these components to correctly specify and fully capture the effects of price.\nTotal distribution points\nJust like price, the distribution of the product during the period under consideration will also affect the sales.\nTherefore, it is important to identify and account for the impact of distribution in the model. Total distribution point (TDP) is the indicator of the depth and width of distribution. Numerical distribution4 and %ACV5 (all commodity volume) are the popular distribution metrics capturing the width of distribution, however, it is critical to consider the impact of depth of distribution as well, which is usually represented by number of SKUs or share of SKUs metrics. TDP is the metric which captures the impact of both the metrics and can be represented as TDP = Average of %ACV X Average of SKUs. TDP share is also the other metric that captures the impact of competition.\nSeasonality is a periodic repetition or cyclic variation in time series data. Due to seasonality, sales follow the same periodic variations. There are various factors which may lead to this phenomenon, such as holidays, moving holidays like Ramadan, payday, weather conditions, the start and end of school terms or the number of weekends in a month. Therefore, before quantifying the impact of marketing promotions, it is crucial to separate the impact of seasonality. Not doing so may lead to misguided recommendations.\nMarketing promotions synergy effect\nA synergy effect occurs when the total effect of marketing promotions is greater than the sum of their individual parts. When display and feature promotions run simultaneously in modern retail, it difficult to guess how their overlapping sales impact is even higher than the summation of their individual impacts. The answer lies in their synergy effect.\nDecay effect on media promotions\nThe effect of media promotions gradually fades from the minds of audience. Therefore, in any given period of observation, the effect of a media channel is sum of the effect of ongoing promotion and the un-faded/un-decayed part of all the promotions conducted in past. The decay effect can be mathematically modeled and is usually expressed in terms of the half-life of the advertising.\nWith the same concept, the media gross rating points (GRPs)6 can further be used to derive adstock, which is defined to describe the prolonged or lagged effect of advertising on consumer purchase behavior. It is also known as ‘advertising carry-over’. Mathematically, it is shown as:\nAdstockt = GRPt + α*Adstock(t-1) ; where α is the decay coefficient varying from 0-1 depending on the product, media, campaign etc.\nIn the below example, an advertising campaign has been running for four periods with equal GRPs. Carryover effect results in a higher adstock in the second week, which will be a summation of second week GRPs and the carried over part of the previous week’s GRPs. Similarly a decayed adstock can be found even after not running the advertising GRPs after 4 weeks.\nSaturation effect on media promotions\nThe relationship between media adstock and sales impact is not linear. Increased increments in media support impacts sales with diminishing returns. A logistic function can map the media impact on sales. Media campaigns won’t result in incremental sales despite high media GRPs, as it reaches saturation. A typical saturation curve for media effect can be expressed as:\nY = A(1 – exp(-BXC)) ; where X is the Adstock and B and C are parameters that define the saturation curve and A is the model coefficient.\nThe media saturation curve helps the business in identifying the optimal execution levels for maximum returns.\nConcept of diminishing returns\nAdvertising has diminishing returns to scale. In other words, the relationship between advertising and sales is nonlinear. The effect of 100 GRPs of advertising on sales may be lesser than twice that achieved with 50 GRPs of advertising. Typically, each incremental amount of advertising causes a progressively lesser effect on demand increase. This is a result of advertising saturation.\nOther key factors\nThe halo effect is defined as a customers’ bias towards certain products due to their favorable experience with the other products made by the same manufacturer. The halo effect is driven by brand equity.\nA classic example of the halo effect is the increase in MacBook sales during the launch of iPod. IPod focused marketing campaigns resulted in improving the brand equity of Apple, which was significantly reflected in increased sales of MacBooks.\nThere are some possible reasons which can explain this phenomenon, such as consumers relating the brand name to other products during the promotion rather than the sub-brand. Alternatively, the promoted product could be priced higher than the product which gains sales, or the promoted product has poor distribution.\nCannibalization is defined as the reduction in sales of a manufacturer’s existing product due to the new launch or promotion of another product. The new or promoted product eats into the sales of existing product of the same category. However, the manufacturer could gain in overall sales. Like other factors, it is important to identify and distinguish the effect of cannibalization in order to correctly estimate the impact of promotions.\nSales decomposition, contributions and Due-To’s\nIn a competitive market, where a manufacturer is doing a number of marketing activities, it is important to identify the sales contribution of each marketing vehicle and identify the growth drivers which will enable better decision making for marketing managers. Here, the total sales can be decomposed into base and incremental sales, where incremental sales can further be decomposed into sales of each contributing marketing vehicle.\nA typical weekly volume decomposition chart will look like below where all the sales peaks are contributed by marketing promotions.\nThe below chart shows the full year contributions of all marketing vehicles.\nA Due-To analysis explains the key growth drivers which are responsible for overall sales growth and by what amount. A DueTo growth rate analysis for a marketing channel is defined as the year-on-year growth in the sales contribution of that channel. The below figure shows a DueTo analysis of key growth drivers that are contributing to the overall sales growth.\nModel outputs provide contribution of each marketing vehicle, which along with marketing spends, provide marketing ROIs. It also captures time decay and diminishing returns on different media vehicles, as well as the effects of other non-marketing factors discussed above and other interactions like the halo effect and cannibalization. The model output provides all the necessary components and parameters required to arrive at the best media mix under any condition.\nWith the help of the model output, simulations can be run to optimize plans for various scenarios.\nCommon challenges of implementing market mix models\nMarketing managers and CMOs are quite aware of the market mix technique and its business related outcomes. However, they still face certain implementation challenges which sometimes restrict them from leveraging the process to meet their needs. The common challenges are:\n- The process requires leveraging diverse set of data sources, which itself is a huge challenge. Related challenges include data quality and timeliness.\n- For external data such as media data provided by some an outside agency, there are no standards of measurements followed. This sometimes creates doubts about the data’s credibility and its utility when making decisions.\n- Leveraging model outcomes to design a marketing strategy for a newly launched product or for an unstable or noncompetitive market is also common challenge.\n- Capturing the long term effects of advertising campaigns is often difficult, results in creating brand equity and is now a component of base sales.\n- Making market mix recommendations actionable is often an operational challenge. Sometimes, achieving optimal execution for the proposed recommendations becomes a headache for the marketing planners.\nHowever, these challenges can be overcome with proper planning and help of industry experts.\nMarketing managers and CMOs looking to optimize their budget planning should ask themselves a few questions:\n- ØIs the current marketing spend producing the right ROI?\n- Are the right channels being used?\n- Among these channels, is the allocation ideal?\nIf the answer to any of these questions is a no, it may be a good time to revisit the current market mix strategy. Optimizing this plan often requires a substantial analytics capacity and high-quality data sources. If these resources are available in-house, they can provide a head start on accomplishing this goal. However, retailers who attempt to build these capabilities up from the ground floor themselves often find it to be a lengthy and expensive process. Many turn to third-party experts for help when formulating the best ways of enhancing their marketing spend.\n- (n.d.). Retrieved fromhttps://www.emarketer.com/Article/CPG-Advertisers-Spend-Nearly-6-Billion-on-Digital-Advertising-This-Year/1014012\n- (n.d.). Retrieved from https://en.wikipedia.org/wiki/Numeric_distribution\n- (n.d.). Retrieved from https://www.google.com\n- Above The Line (ATL) advertising is where mass media is used to promote brands and reach out to the target consumers. These include conventional media such as television and radio advertising, print and internet. This is communication that is targeted to a wider spread of audience, and is not specific to individual consumers. ATL advertising tries to reach out to the mass as consumer audience.\n- Below the line (BTL) advertising is more one to one, and involves the distribution of pamphlets, handbills, stickers, promotions, brochures placed at point of sale, on the roads through banners and placards. It could also involve product demos and samplings at busy places like malls and market places or residential complexes.\n- Numeric distribution is based on the number of outlets that carry a product (that is, outlets that list at least one of the product’s stock-keeping units, or SKUs). It is defined as the percentage of stores that stock a given brand or SKU, within the universe of stores in the relevant market.\n- % ACV Distribution is often just abbreviated “% ACV,” especially when talking about it verbally. It can generally be thought of as “% of stores selling,” but with stores weighted based on their size.\n- TV awareness is generally quantified in terms of gross rating point (GRP), which is defined in terms of reach and frequency GRP = Reach X Frequency; Reach is % of customer watches the TV advertisement and Frequency is average number of times they actually watch the same advertisement"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:a478ce8b-481e-4547-9249-d7fccad31d2f>","<urn:uuid:0950d3e9-9e37-46b2-9ac3-568992ff0e5c>"],"error":null}
{"question":"Compare cleaning baby teeth vs adult teeth - which needs more careful routine?","answer":"Both baby and adult teeth require careful cleaning routines. For adults, regular brushing and flossing isn't enough as plaque and calculus buildup can only be removed through professional cleaning. For babies, cleaning should start even before teeth appear using a washcloth and water on the gums. Once teeth erupt, a soft-bristled toothbrush with a rice-sized amount of fluoridated toothpaste should be used. Additionally, special attention must be paid to prevent early-childhood caries in babies by avoiding sugary liquids in bottles and frequent night feeding.","context":["Prevention is Key – Routine dental exams & Hygiene visits twice per year\nAn ounce of prevention…The best way to maintain a healthy mouth and to save money on dental care is to have routine comprehensive dental exams. The American Dental Association (ADA) recommends that adults and children have a dental exam approximately every six months.\nWhat is done in a routine dental exam?\nAt a routine exam (dental checkup) we examine the outside of your mouth (including the TMJ or jaw joint), and the inside of your mouth. We check to make sure that the tissues in your mouth are healthy, and we examine your gums to check for gum disease and/or periodontal disease. And of course, we check your teeth to make sure that all of your existing crowns and fillings are intact, and to make sure that you don’t have any new cavities present. And as part of our intraoral exam, we perform an oral cancer screening.\nA Cleaning Every Six Months is Critical\nThe ADA recommends that you have a dental cleaning and exam every six months.\nBrushing alone isn’t enough\nMany people believe that they don’t need to have dental cleanings because they brush and floss their teeth regularly. And others think that they can skip brushing and flossing and that having their dental cleaning will “fix everything.” In fact, both assumptions are wrong.\nAs much as you brush and floss at home, you will get a buildup of plaque and hardened plaque (calculus) that you simply cannot remove with a toothbrush. Some people have light amounts of this type of buildup, and other people have extensive buildup. Regardless of the cause, only a professional cleaning can remove this type of debris on your teeth.\nAnd if you don’t brush and wait until your cleaning to care for your teeth, the chronic inflammation you will likely have can lead to serious problems\nWhat if I don’t have regular dental cleanings?\nIf you don’t have regular dental cleanings or if you don’t take care of your teeth in between cleanings, you may wind up losing teeth. You won’t lose teeth right away, but not caring for your teeth over a long period of time can lead to chronic inflammation and weakening of the gum tissues that hold the teeth in place. This condition is called periodontal disease (gum disease), and in many cases, it’s preventable.\nDo I need x-rays at every visit?\nGenerally speaking, no. We do take a full set of x-rays the first time you see us as a patient, and after that we recommend a full set every 3-5 years. And although the levels of radiation used in dental x-rays are incredibly low (we use digital x-rays for the lowest exposure possible), we do need to take routine check-up x-rays when indicated.\nPatients who have a history of extensive decay, or with conditions that can lead to decay such as dry mouth (xerostomia), often need x-rays more frequently than patients who have never had a cavity, and who have normal flow of saliva.\nContact us today to schedule your exam & cleaning – it’s never too late to get a fresh start with your oral home care.","When should my child first visit the dentist?\nThe American Academy of Pediatric Dentistry, the American Academy of Pediatrics, and the American Dental Association recommend that your child’s first pediatric dental visit should be scheduled once the first tooth erupts or by age one. At your child’s first dental visit, your pediatric dentist will perform a comprehensive exam to ensure your little one’s teeth are erupting properly. Your dentist will also review important oral health and diet practices that will minimize decay. After your initial visit, your little one should continue to see the dentist every six months.\nWhy are baby teeth so important?\nOne of the most common questions we get is, “Why are baby teeth important? They are going to fall out anyway.” Primary teeth are vital for your child’s overall health and play an important role in facial growth and development. They also act as natural space maintainers and guide the eruption of the adult teeth. Decay in primary teeth can cause pain, swelling, infection and early loss of primary teeth.\nWhen will my child’s teeth begin to grow in?\nTeeth development in general begins before birth for baby teeth and around birth for permanent teeth. Typically, the lower front teeth will begin to erupt at approximately 6 months on average.\nThe permanent teeth typically begin to erupt at age 6 with the lower front teeth and the adult molars and will end around age 12.\nWhat is teething?\nTeething occurs when baby teeth begin to grow in. While this is relatively problem-free for most children, others may experience some discomfort. We recommend frozen washcloths, frozen teething rings and if needed, some Motrin or Advil to provide comfort through this process. You may notice a drop in appetite, but it’s important to ensure that your little one is well hydrated.\nIt is always wise to contact your dentist or health professional if you have additional questions or concerns about teething.\nHow can I clean my little one’s teeth?\nFor infants, you can use a washcloth and water to clean the gums, but once the teeth begin to erupt, a soft bristled toothbrush can be introduced. The current ADA and AAPD guidelines recommend that a rice size amount of fluoridated toothpaste can be used for this age group.\nBottle use or nursing at bedtime\nPutting a baby to sleep with a bottle of milk or juice, or nursing a baby to sleep can be harmful to the teeth. The normal mechanisms that help keep teeth clean during the day, like salivary flow, are much lower at night. Meanwhile the teeth are bathed in sugar from the drinking or nursing, which provides a perfect environment for decay.\nChildren who frequently bottle- or breastfeed while napping, sleeping, or between meals, run the risk of developing extensive tooth decay. Fruit / citrus juices, and sweetened beverages cause the most damage. You can brush your baby’s teeth after the last feed at night with a soft-bristled toothbrush and use a washcloth to wipe the teeth if your infant has multiple feeds throughout the night.\nPreventing early–childhood caries\nEarly-childhood caries is formerly known as baby-bottle tooth decay or nursing caries. The condition occurs when a child is overexposed to sugary liquids. Severe early childhood caries usually occurs when the child is routinely put to bed with a bottle containing juice, soda, or milk. It can also occur as a result of “grazing” with a sippy cup.\nEarly-childhood caries can destroy teeth, require major dental treatment, and even potentially damage your child’s adult teeth if left untreated. To prevent this condition:\n- Help your child start learning to drink from a regular cup by their first birthday.\n- If your child “grazes,” only allow water in any bottles or sippy cups used.\n- Clean your baby’s gums with a fresh gauze pad after each feeding.\n- Begin brushing as soon as you see the first tooth.\n- Never give your child a pacifier coated in sugar or dipped in honey.\n- The bacteria that causes caries is transmissible, avoid sharing drinks or kissing your baby close to the mouth, especially if you have not seen your dentist for regular appointments.\nThumb sucking or pacifier use\nSucking is a normal reflex for infants and is present even before birth. It helps infants to self soothe. Most children stop sucking fingers, thumbs, or pacifiers on their own; but some children may continue beyond toddlerhood. Every effort should be made to help your child move on from thumbs or paci’s by age 4 or 5. Prolonged thumb sucking or pacifier use can result in orthodontic changes such as a narrowed arch. As the adult front teeth begin to come in, if the habit persists, the teeth can be flared.\nDental emergencies are common as toddlers become more mobile and active. Precautionary measures such as childproofing your home are a great first step. It is, however, impossible to watch your toddler every moment of the day, and emergencies can happen even under the most watchful eyes!\nEstablishing a good dental home for your little one where they can be evaluated as quickly as possible is important. And of course if a fall or trauma goes beyond dental, call 911 or take your toddler to the ER to be evaluated for more serious head or neck injuries."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:460e6445-c217-4f9c-8a3e-2237b5c2fb02>","<urn:uuid:66fa5d3b-b7bb-4b3f-ae9d-d66610bfaed7>"],"error":null}
{"question":"I'm a nursing student and want to learn more about pressure ulcers! Can you tell me what are the stages of pressure ulcers development and how much does treatment cost for each stage? 🤔","answer":"Pressure ulcers progress through several stages. A suspected deep-tissue injury appears as purple or maroon discolored intact skin or blood-filled blister. Stage I shows nonblanchable redness over a bony prominence. Stage II involves partial thickness loss of dermis with a shallow open ulcer. Stage III shows full thickness tissue loss without exposed bone or muscle. Stage IV has full thickness loss with exposed bone, tendon, or muscle. Some ulcers are considered Unstageable when covered with slough or eschar. Regarding treatment costs, as of August 2011 in the UK, costs increased with severity: Stage I cost 1,214 Euros, while Stage IV cost up to 14,108 Euros. More severe cases require longer healing time and more intensive care, leading to higher treatment costs.","context":["approximated wound edges\nedges of a wound that are lightly pulled together, epithelialization of wound margins, edges touch, wound is closed\ndiscoloration of an area resulting from infiltration of blood into the subcutaneous tissue\nstage of wound healing in which epithelial cells move across the surface of a wound margin; tissue color ranges from the color of a ground glass to pink\nfluid that accumulates in a wound, may contain serum, cellular debris, bacteria, and white blood cells\nnew tissue that is deep pink/red and composed of fibroblasts and small blood vessels that fill an open wound when it starts to heal. characterized by irregular surface like raspberries\ncondition characterized by yellowness of the skin, whites of eyes, mucous membranes, and body fluids as a result of deposition of bile pigment resulting from excess bilirubin in the blood\nsoftening of tissue due to excessive moisture overhydration related to urinary and fecal incontinence, related to moisture, ph changes, and overgrowth of bacteria and infection of the skin and erosion of skin from friction\nstrategies used in patient care to reduce overall number of microorganisms or to prevent or reduce overall number of microorganisms or to prevent or reduce the risk of transmission of microorganisms from one person to another or from one pace to another. Involves meticulous hand washing, maintaining a clean environment by preparing a clean filed, using clean gloves and sterile instruments and preventing direct contamination of materials and supplies\npersonal protective equipment\nequipment and supplies necessary to minimize or prevent exposure to infectious material, including gloves, gowns, masks, and protective eye gear.\nlesion caused by unrelieved pressure that results in damage to underlying tissue, can be acute or chronic\noccur in older adults due to aging skin, chronic illnesses, immobility, malnutrition, fecal and urinary incontinence, and altered level of consciousness\nother at risk groups - spinal cord injuries, traumatic brain injuries, or neuoromuscular disorders\n2 mechanisms contribute - external pressure and friction and shearing forces\nstrategies used in patinet care to reduce exposure to microorganisms and maintain objects and areas as free from microorganism as possible. Involves meticulous hand washing, use of a sterile field, use of sterile gloves for application of a sterile dressing and use of sterile instruments\npassageway or opening that may be visible at skin level but with most of the tunnel under the surface of the skin\nareas of tissue destruction underneath intact skin along the margins of a wound; associated with Stage 3 or 4 pressure ulcers\nfigure eight bandage\noverlap evenly and by 1/2 to 2/3 the width of the bandage. oblique overlapping turns that ascend and descend alternately. used around the knee, elbow, ankle, and wrist\nabnormal passage from an internal organ to the outside of the body or from one internal organ to another\nnegative-pressure wound therapy, promotes wound healing and wound closure through the application of uniform negative pressure on the wound bed, reduction in bacteria, and removal of excess wound fluid while providing a moist environment. used to treat acute or chronic, heavy drainage. failing to heal or healing slowly\nwhite blood cells, liquefied dead tissue debris and both dead and live bacteria. thick with musty or foul odor and varies in color such as dark yellow or green depending on organism.\nlarge numbers of red blood cells and looks like blood, bright-red sanguineous drainage is indicative of fresh bleeding darker indicates older bleeding.\nmade up of skin, subcutaneous layer under the skin and appendages of the skin including glands, blood vessels nerves and sensory organs of the skin. 2 layers epidermis and dermis\nfunctions as - protections, temperature regulation, psychosocial, sensation, vit d production, immunological, absorption, elimination\nage -related skin changes\nsubcutaneous and dermal tissue become thin, sebaceous and sweat gland decreases, cell renewal is shorter delaying healing, melanocytes decline in number, collagen fiber is less organized causing loss of elasticity.\n4 stages repair injury\nhemostasis - occurs immediately, vessel constriction and blood clotting, exudate occurs with swelling and pain., heat and redness\ninflammation - lasts 4-6 days, leukocytes and macrophages move in. the macrophages ingest debris and release growth factors to attract fibroblasts, patient generalized body response with elevated temp, leukocytosis and generalized malaise\nproliferation- fibroblastic, regenerative or connective tissue. synthesize and secret collagen and induce blood vessel formation as increasing endothelial cells , granulation tissue forms the foundation for scar tissue\nmaturation 3 weeks after injury remodeling\nwell approximated wound edges, intentional wounds with minimal tissue loss heal this way\nnot well approximated large, open wounds -burns or major trauma and often contaminated, Infected primary intention wounds heal by secondary takes longer and forms more scar tissue\nleft open for several days to allow edema or infection to resolve or exudate to drain and then closed.\nfactors affecting wound healing\nlocal - pressure, desiccation, maceration, trauma, edema, infection, necrosis,\nsystemic factors - age, circulation and oxygenation, nutritional status, wound condition, medications and health status, immunosuppression,\nrisks for pressure ulcer dev\nimmobility, nutrition and hydration, moisture, dehydration, incontinence, skin hygiene, diabetes, diminished pain, fractures, corticosteroid therapy, immunosuppression, poor circulation, significant obesity or thinness\nstages of pressure ulcer\nsuspected deep-tissue - purple or maroon localized area of dicolored intact kin or blood-filled blister due to damage of underlying soft tissue - painful, mushy, firm, boggy, warmer, or cooler area\nStage I - defined area of intact skin with nonblanchable redness of localized area over a bony prominence\nStage II partial thickness loss of dermis, shallow open ulcer with a red pink wound bed without slough, shiny or dry without bruising., or intact or open rupture serum-filled blister.\nStage III full thickness tissue loss but no bone, tendon, or muscle is exposed. SLough, undermining and tunneling may be present.\nStage IV full thickness loss with exposed bone, tendon, or muscle, slough or eschar present.\nUnstageable when covered with slough, and/or eschar.\npressure ulcer risk scale\n19-23 no risk, 15-18 mild risk, 13-14 moderate risk, 10-12 high risk, 9 or lower very high risk\nopen drainage system\nempties into absorptive dressing material, passive drainage, no sutures. steril large safety pin attached to outer portion to prevent drain from slipping back into incised area. Penrose drain.\nclosed drainage system\ndrainage tube that may be connected to an electrical suction device or have portable built-in reservoir to maintain constant low suction. Jackson-Pratt tubes and Hemovacs sutured to the skin. prevents microorganisms from entering the wound. accurate measurement of drainage. must be emptied and suction reestablished .\nadditional wound healing techniques\nfibrin sealants, NPWT, growth factors, oxygen therapy, heat and cold therapy, or perhaps surgery.\ndry sterile dressing\nadminister pain meds if appropriate\nclean gloves to loosen tape on old dressing, remove and assess of drainage on dressing\nremove gloves, inspect wound, use sterile technique, open cleaning solution, apply sterile gloves, clean the wound, dry the wound, apply dressing gauze, second layer, surgical or ABD pad, remove gloves, apply clean gloves, tape dressing in place\nclean gloves, remove dressing, assess dressing, assess the wound, remove gloves and go tot sterile technique, prepare solution for dressing, apply sterile gloves, clean the wound, dry the area, apply skin protectant, fluff and squeeze gauze, pack wound loosely, apply dry sterile gauze pads. apply ABD pad, remove gloves, reglove with clean and tape.","This article identifies pressure ulcers as problems in people’s lives. They become health burdens. Once they occur they become difficult to treat. The article seeks to describe how prevention can be achieved. Proper risk assessment approach is suggested as one of the best measures to prevent pressure ulcer development. In addition, the article proposes the use of preventive interventions to prevent occurrence of pressure ulcers. For instance, the article proposes that the use of advanced static support surfaces were better than use of standard mattresses in preventing pressure ulcers in patients who are at risk. However, the article accepts that there was no enough evidence to support nutritional supplementation, skin care as well as repositioning interventions as more effective compared to the standard care.\nPrevention of pressure ulcers in nursing home residents\nThis article identifies a problem of heel ulceration. It pays special attention to this form of ulceration in the nursing home setting. Heel becomes an important anatomical part of the body that is prone to ulceration. That is because it has a thin subcutaneous tissue that is between the skin and the bone. The article describes the effect of education as well as the level of knowledge on the prevention of development of heel ulcers. The article proposes educating patients in the nursing home residents as a solution to this problem.\nThe cost of pressure ulcers in the United Kingdom\nThis article identifies the cost of treating pressure ulcers as a problem. In August 2011, the cost of treating pressure ulcers ranged between 1,214 Euros for stage one to 14,108 Euros for stage four. It was found out that the cost of treating pressure ulcers increased as the case became severe. Severe cases took longer to heal, therefore, required more attention than less severe cases. The article proposes that prevention of development of pressure ulcers as the only solution to prevent high costs of pressure ulcer treatment.\nStrategies to improve the prevention of pressure ulcers\nThis article looks at the lack of proper preventive measures of development of pressure ulcers as the main problem in the fight against pressure ulcers. The article describes factors that can improve prevention of pressure ulcers. It was found that early risk assessment on heel pressure ulcers reduced the development of heel pressure ulcers. Therefore, the solution that the article proposes is to initiate an early risk assessment. When risk is found to be high, appropriate measures are used to prevent pressure ulcers from developing.\nRegistered nurses’ thinking strategies on malnutrition and pressure ulcer ulcers in nursing homes: a scenario-based think-aloud study\nThis article identifies the problem of the thinking of the nurses and their attitudes on the prevalence of pressure ulcers. It is based on the fact that clinical reasoning is an important aspect of nursing practice. Pressure ulcers are the biggest problems that encounter patients especially in the nursing homes. According to the article, registered nurses think about making choices, forming relationships as well as making conclusions. They missed one important aspect in reasoning. They did not consider risk assessment. This article proposes that registered nurses in the nursing homes should be trained on the need of risk assessment as part of the clinical reasoning.\nAssessment and management of pressure ulcers in the elderly: current strategies\nAccording to this article, the problem of pressure ulcers is a big menace in the health system. The article identifies the bedridden elderly as the most vulnerable populations. If not handled in time, pressure ulcers progress to chronic stages and can have lifetime implications on the individual. the article also identifies factors that predispose the elderly to this risk. They are impaired immobility, nutritional deficiencies as well as other debilitating chronic diseases. In addition, an aged skin has less ability to protect tissues from interfacial pressure. According to the article, when managing pressure ulcers in the elderly, focus should involve more than just the wound. For instance, it is important to consider nutritional state, severity of the condition, functional state, co-morbidities and social support of the patient.\nA pressure ulcer prevention program specially designed for nursing homes: does it work?\nThis article tires to answer the problem of whether a pressure ulcer prevention program specially designed for nursing homes was effective. It was important to determine whether it was feasible to implement it and what its impact on the care staff was. In addition, the article seeks to describe the impact of the program on the pressure ulcer outcomes. It was found that the program was very effective and was feasible. It produced positive outcomes and it was acceptable. It was found to promote prevention of pressure ulcers better than the normal care program. The article suggests that the program should be introduced and implemented in all centers that take care of the patients at risk of developing pressure ulcers.\nAcute care patient mobility patterns and documented pressure injury prevention – an observational study and survey\nThe article identifies the issue of repositioning as a major way of preventing pressure injuries. The article is based on the observations made on various positioning patterns and their impact on the development of pressure injuries. It also identifies a problem in the documentation of the strategies used in order to assist in the continuity of care. Different positions adopted were recorded. It was discovered that most acute patients were mostly in positions that increased their risk of pressure injuries. The article suggests that proper pressure injury prevention measures should be adopted such that they account for the patient movement patterns in the acute care. Such strategies should also address the deficiencies identified in the documentations of care. The article recommends that repositioning is the best strategy that can be employed to prevent pressure injuries. However, according to the article, there is suboptimal application of the pressure injury guidelines. Another solution that the article suggests is that patients who can move independently can be taught on the importance of repositioning themselves. That is because by doing so they redistribute pressure on the body tissues.\nInnovations in the reduction of pressure ulceration and pain in critical care\nThe article explains new techniques that can be adopted in the prevention of the pressure ulcers. In addition, it describes different approaches that can be employed to reduce pain in critical care. It was established that emotional support for the patients in the critical care reduced pain significantly. In addition, frequent repositioning was found more effective in prevention of pressure ulcers compared to the traditional repositioning strategy. The article proposes that it is important to incorporate different methods as provided in the nursing guidelines for the prevention of pressure ulcers.\nKnowledge on pressure ulcer prevention among nursing professionals\nThe article tries to describe the level of knowledge of nurses as far as prevention of the pressure ulcers is concerned. Nursing professionals studied were those that deal with adults and elderly. The article identifies a problem of whether nurses really have adequate knowledge needed to assist in the prevention of the pressure ulcers effectively. According to the article, it was realized that nurses demonstrated a significant deficiency in the knowledge required in the prevention of the pressure ulcers. Effective prevention of pressure ulcers in the hospital environment is important and indicates the quality of care being received by the patients. However, such effective prevention is possible if the nurses involved have necessary professional knowledge and skills needed. The article suggests that proper strategies should be adopted in an attempt to increase the level of knowledge of the nurses. The article proposes that there is need for a strategic planning to be used to disseminate prevention measures that nurses need to know.\nPatient repositioning and pressure ulcer risk – monitoring interface pressures of at-risk patients\nThe article sheds light on the impact of the patient repositioning on the pressure ulcer development. It tries to find out whether patient repositioning helps in the prevention of the pressure ulcers. It also tries to establish whether repositioning prevents interface pressure. According to the article, it was important to find out whether it was true that repositioning did not relieve all areas that were at risk as advanced by the previous works. Patients in routine repositioning program in preventing the risk of perisacral pressure ulcers were assessed. According to the article, it was discovered that some areas of the body had a high potential for development of the pressure ulcers despite all repositioning strategies. It was also discovered that nurses did not know the effectiveness of their repositioning interventions on the pressure ulcer prevention on different body tissues. The article proposes that repositioning practice needs to be improved for it to be successful in the prevention of the pressure ulcer development.\n‘Beyond the cushion’: a study of occupational therapists’ perceptions of their role and clinical decisions in pressure care\nAccording to this article, the problem being addressed is the perception of the occupational therapists in their role as carers in the pressure ulcer management. In addition, there is the article describes the influence that these perceptions have on the clinical decisions that occupational therapists make. It was established that most of the occupational therapists do not understand their role in the pressure care. In addition, most of them expressed concern about how other health care professionals viewed them in that role. It was also established that the ability of occupational therapists to make clinical decisions was influenced by their knowledge and experience. In addition, availability of resources influenced decision making. The article proposes that the role of occupational therapists in pressure care should be made clear. Occupational therapists should promote their role to be recognized by other health care professionals.\nThe role of barrier protection in pressure ulcer prevention\nThe question being answered in this article is whether skin has any protection on the development of the pressure ulcers. The skin is a natural protection of the body tissues. Being a good barrier, it should prevent development of pressure ulcer development on the tissues. It was found that skin has no demonstrable ability to prevent pressure ulcer development because of the impact of the pressure on the superficial skin itself. The article proposes that the skin should be protected from high impact actions.\nRegistered nurses’ attention to and perceptions of pressure ulcer prevention in hospital settings\nThe problems being answered in this article are with regard to how nurses perform, document and reflect on pressure ulcer prevention. It is recognized that nurses should provide qualified and safe pressure ulcer preventive measures. It was found that nurses did their interventions guided by their experience. There was variability in the application of the evidence-based guidelines. There was lack of nurses’ attention on prevention because the registered nurses trusted on the ability of the assistant nurses. The article proposes that application of evidence-based practice was important in the prevention of the pressure ulcers.\nNurses’ attitudes and perceived barriers to pressure ulcer prevention in Jordan\nThe article addresses the problem of the attitude of nurses in Jordan regarding pressure ulcer prevention and the barriers that are encountered in the prevention of the pressure ulcers. It was found that positive attitude on the side of nurses was increased by their level of experience. Perceived barriers to the care were lack of adequate staff, time and patient condition. The article stipulates nurses’ positive attitude is not enough in prevention of pressure ulcers. Overcoming barriers is also very important.\nBangova A (2013) Prevention of Pressure Ulcers in Nursing Home Residents. Nursing Standard; 27, 24, 54-61.\nChou, R., Dana, T., Bougatsos, C., Blazina, I., Starmer, A., Reitel, K., & Buckley, D. (2013). Pressure Ulcer Risk Assessment and Prevention: a Systematic Comparative Effectiveness review. Annals of Internal Medicine; 159(1), 28-38.\nDealey, C. C., Posnett, J. J., & Walker, A. A. (2012). The Cost of Pressure Ulcers in the United Kingdom. Journal of Wound Care; 21(6), 261-264\nElliott, J. (2010). Strategies to Improve the Prevention of Pressure Ulcers. Nursing Older People, 22(9), 31-36.\nFossum, M., Alexander, G., Göransson, K., Ehnfors, M., & Ehrenberg, A. (2011). Registered Nurses’ Thinking Strategies on Malnutrition and Pressure Ulcers in Nursing Homes: a scenario-based think-aloud study. Journal of Clinical Nursing; 20(17/18), 2425-2435.\nJaul, E., (2010). Assessment and management of pressure ulcers in the elderly: Current strategies. Drugs & Aging; 5(2), 91-98.\nJohnstone, A., & McGown, K. (2013). Innovations in the Reduction of Pressure Ulceration and Pain in Critical Care. Wounds UK; 9(3), 80-84.\nKwong, E., Lau, A., Lee, R., & Kwan, R. (2011). A Pressure Ulcer Prevention Program specially Designed for Nursing Homes: does it work? Journal of Clinical Nursing; 20(19/20), 2777-2786.\nMcInnes, E., Chaboyer, W., Allen, T., Murray, E., & Webber, L. (2013). Acute Care Patient Mobility Patterns and Documented Pressure Injury Prevention — an observational study and survey. Wound Practice & Research; 21(3), 116-125.\nMiyazaki, M., Larcher, & Benedita. (2010). Knowledge on Pressure Ulcer Prevention Among Nursing Professionals. Revista Latino-Americana De Enfermagem (RLAE); 18(6), 1203-1211.\nPeterson, M. J., Gravenstein, N., Schwab, W. K., H., & Caruso, L. J. (2013). Patient Repositioning and Pressure Ulcer Risk-Monitoring Interface Pressures of at-risk Patients. Journal of Rehabilitation Research & Development; 50(4), 477-488.\nRose, A., & Mackenzie, L. (2010). ‘Beyond the cushion’: a Study of Occupational Therapists’ Perceptions of their Role and Clinical Decisions in Pressure Care. Disability & Rehabilitation; 32(13), 1099-1108.\nStephen-Haynes, J. (2013). The Role of Barrier Protection in Pressure Ulcer Prevention. British Journal of Nursing; S52-8.\nSving, E., Gunningberg, L., Hogman, M., & Mamhidir, A. (2012). Registered Nurses’ Attention to and perceptions of Pressure Ulcer Prevention in Hospital Settings. Journal of Clinical Nursing; 21(9/10), 1293-1303.\nTubaishat, A. A., Aljezawi, M. M., & Al Qadire, ,. M. (2013). Nurses’ Attitudes and Perceived Barriers to Pressure Ulcer Prevention in Jordan. Journal of Wound Care; 22(9), 490-497."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:b0297a71-c64d-48f0-9551-3c1b724e32c5>","<urn:uuid:1b5d39a9-6937-4ca1-bb50-ba9d67952c7f>"],"error":null}
{"question":"Hi! Could you please provide a comprehensive explanation of what exactly space weather is and how different types of solar phenomena contribute to it?","answer":"Space weather describes the changes in solar activity as observed on our planet, affecting technology and the magnetic organization of the solar system. It involves several solar phenomena: 1) Coronal Mass Ejections (CMEs) - massive clouds of charged particles traveling through space, 2) Solar flares - bright X-ray flashes during energetic explosions that can make the sun 1000 times brighter in X-rays than usual, and 3) High-speed streams (HSS) - powerful gusts in the solar wind with velocities of 300-1000 km/s, originating from dark regions in the solar corona called coronal holes. These HSS events can produce interplanetary shocks and often recur every 27 days as their source coronal hole rotates into position.","context":["I was asked to explain the various sources for Space Weather. So here we go:\nSolar activity describes events that take place on the sun, while space weather describes the change in solar activity as seen on our planet, by technology, and in the magnetic organization of the solar system.\nA Coronal Mass Ejection (CME) occurs when a prominence suspended above the surface of the Sun erupts and sends millions of tons of material into space. This cloud of charged particles is generally confined within a magnetic field (like a magnetic bubble), expanding and traveling out through the solar system at speeds from about 200 km/s up to a staggering 2000 km/s. When directed toward Earth, a CME typically arrives 2-3 days after eruption but in exceptional cases can arrive in less than 24 hours.\nA solar flare is a bright flash of X-rays seen during an energetic explosion in an active region of the Sun. It’s usually seen as a large burst of X-rays, but may also have a coincident bright flash of white light. A flare lasts a matter of minutes but releases an immense amount of energy. During solar flares the sun can be 1000 times brighter in X-rays than usual.\nA high-speed stream (HSS) is like a powerful gust in the solar wind. Near the edges of a HSS are regions of high particle density and strong magnetic fields, while inside the stream the density and field are low and the temperature and velocity are high. The flow velocity inside a HSS can reach 300 to 1000 km/s. In some cases they will produce interplanetary shocks. A HSS comes from a coronal hole, a dark region in the solar corona, and is most common during the declining phase of the solar cycle. A HSS can last a long time: we often see a HSS every 27 days as its home coronal hole rotates into the right position.\nEarth’s magnetosphere is a bubble created around us by our magnetic field, which protects us from most of the particles the Sun throws at us. When a CME or high- speed stream arrives at Earth, it buffets the magnetosphere. If the arriving solar magnetic field is directed southward, it interacts strongly with Earth’s oppositely oriented magnetic field. As a result, Earth’s magnetic field is peeled open like an onion, allowing energetic solar wind particles to stream down the field lines to hit the atmosphere over the poles. At Earth’s surface, a magnetic storm is seen as a rapid drop in Earth’s magnetic field strength (typically a drop of 30 to 500 nT in 1-2 hours). This decrease lasts about 6 to 12 hours, after which the magnetic field gradually recovers over a period of several days.\nA constant rain of galactic cosmic rays (GCRs) passes through all of space. These charged particles are accelerated to extremely high energies by galactic events such as supernovae (the catastrophic collapse of a star), although some may come from outside the Milky Way. Their high energy allows GCRs to penetrate through Earth’s magnetosphere and deep into the atmosphere. As they enter the atmosphere, they collide with molecules of oxygen and nitrogen and produce millions of tiny fragments. The GCR ionization rate (roughly the radiation dose) reaches a maximum at an altitude of about 12 km (40,000 ft) and then decreases toward Earth’s surface. The intensity of GCRs at Earth is highest during solar minimum. This happens because the intense regions of magnetic field extending out from the sun into the solar system during solar maximum divert the charged cosmic rays away from the solar system. As a result, the GCR intensity at Earth during solar maximum is half the GCR intensity during solar minimum.\nMain Photo Credit: NASA, Artist’s depiction of an active Sun that has released a Coronal Mass Ejection. CMEs are magnetically generated solar phenomenon that can send billions of tons of solar particles, or plasma, into space that can reach Earth one to three days later and affect electronic systems in satellites and on the ground."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:2a570bfa-efbb-458e-a03a-3f1efe78eee5>"],"error":null}
{"question":"Can anyone explain how treatment approaches differ entre apraxia et dyslexia? Looking for a detailed comparison of the therapeutic strategies used for both conditions...","answer":"The treatment approaches for apraxia and dyslexia are quite different. Children with apraxia of speech benefit most from intensive one-on-one treatment programs using specific tools, including oral-motor exercises, tactile 'touch' cues, visual cues (watching in a mirror), and auditory feedback. Typical articulation and language therapy approaches are not very effective for these children. For dyslexia, treatment focuses on different strategies including early oral reading, writing, drawing, and practice to develop print knowledge and linguistic awareness. Treatment also involves multi-sensory structured language instruction using sight, sound and touch, along with classroom modifications like extra time for assignments, help with note-taking, and oral testing. Additionally, dyslexia treatment may incorporate books on tape and assistive technology such as screen readers and voice recognition software.","context":["With help, kids can overcome apraxia of speech\nNancy E. Labaki, M.A., CCC-SLP\nMedCentral Pediatric Therapy\nAs parents of most healthy kids know, speaking is easy for them after they've had a chance to practice syllables and words. But it isn't easy for children with apraxia of speech. This motor speech disorder prevents kids from consistently moving their face, tongue, lips and jaw into the correct position to make speech sounds or produce syllables or words.\nSymptoms of apraxia of speech may include:\n- Difficulty imitating mouth movements\n- Use of vowels, grunting or single syllables for communication\n- Limited ability to produce speech sounds\n- Inconsistent speech sound errors\n- Groping for correct placement of the tongue, lips and jaw during speech\n- Abnormal voice quality\n- Omission of words in sentences\n- Unusual or lack of facial expression\n- History of feeding difficulties\n- Limited expressive language skills, in spite of good understanding of language.\nWhen a physician suspects a child might have apraxia of speech, he or she might ask that a certified speech-language pathologist (SLP) assess the child's oral-motor abilities, melody of speech and speech sounds development. The SLP will determine if speech is developing normally, at a slower rate than typical or if the speech is truly disordered. A child's speech and language skills are considered to be developmentally delayed when the child is following the normal path of development, just at a slower rate. In contrast, some children are able to understand language (good receptive language skills), but have unclear speech - or no expressive speech at all. This might simply indicate an expressive language disorder; however, the child should be evaluated for the presence of a specific speech disorder such as apraxia. In addition to the unclear or absent speech we observe with apraxia, many kids also need to develop language skills, such as using appropriate word endings, verb tenses and sentence structure.\nWith some exceptions, most children with apraxia of speech learn to speak clearly. Some might have minor lingering differences in their speech patterns, and a few might even need alternative methods of communication. One thing that is certain is that learning to speak clearly is often very challenging for children with apraxia, and treatment might be necessary for a long time. Children with apraxia benefit most from intensive one-on-one treatment programs. Typical articulation and language therapy approaches are not very effective for these children. Kids with apraxia of speech benefit more from a combination of apraxia specific tools used by a SLP, in addition to oral-motor exercises, tactile \"touch\" cues, visual cues (watching in a mirror) and auditory feedback. The SLP will design a therapy regimen that best suits your child, and will modify that approach as your child's speech improves.\nBecause many of these children will become frustrated in their efforts to communicate, a strong family support system is critical. Encouraging children in their communication efforts and providing unconditional support will help them feel like and become successful communicators.\nIf you think your child may have any symptoms of apraxia, please discuss it with your physician, consult with a speech therapist in your area or visit the American Speech-Language-Hearing Association online at www.asha.org.","WHAT IS DYSLEXIA??\nDyslexia is a reading processing disorder that can hinder reading, writing, spelling and sometimes even speaking. Dyslexia is not a sign of poor intelligence or laziness. It is also not the result of impaired vision. Children and adults with dyslexia simply have a neurological disorder that causes their brains to process and interpret information differently.\n• Dyslexia is often characterized by difficulties with accurate word recognition, decoding and spelling.\n• Dyslexia may cause problems with reading comprehension and slow down vocabulary growth.\n• Dyslexia may result in poor reading fluency and reading out loud.\n• Dyslexia is neurological and often genetic.\n• Dyslexia is not the result of poor instruction.\n• With the proper support, using alternate learning methods, almost all people with dyslexia can become good readers and writers.\nDyslexia occurs among people of all economic and ethnic backgrounds. Often more than one member of a family has dyslexiaMuch of what happens in a classroom is based on reading and writing. So it’s important to identify dyslexia as early as possible.\nWHAT ARE THE WARNING SIGNS OF DYSLEXIA?\nThe following are common signs of dyslexia in people of different ages. If you or someone you know displays these signs, it doesn’t necessarily mean you have a learning disability. But if troubles continue over time, consider testing for dyslexia.\nDYSLEXIA: WARNING SIGNS BY AGE\nHOW IS DYSLEXIA IDENTIFIED?\nTrained professionals can identify dyslexia using a formal evaluation. This looks at a person’s ability to understand and use spoken and written language. It looks at areas of strength and weakness in the skills that are needed for reading. It also takes into account many other factors. These include family history, intellect, educational background, and social environment.\nHOW IS DYSLEXIA TREATED?\nIt helps to identify dyslexia as early in life as possible. Adults with unidentified dyslexia often work in jobs below their intellectual capacity. But with help from a tutor, teacher, or other trained professional, almost all people with dyslexia can become good readers and writers. Use the following strategies to help to make progress with dyslexia.\n• Expose your child to early oral reading, writing, drawing, and practice to encourage development of print knowledge, basic letter formation, recognition skills and linguistic awareness (the relationship between sound and meaning).\n• Have your child practice reading different kinds of texts. This includes books, magazines, ads and comics.\n• Include multi-sensory, structured language instruction. Practice using sight, sound and touch when introducing new ideas.\n• Seek modifications in the classroom. This might include extra time to complete assignments, help with note taking, oral testing and other means of assessment.\n• Use books on tape and assistive technology. Examples are screen readers and voice recognition computer software.\n• Get help with the emotional issues that arise from struggling to overcome academic difficulties.\nReading and writing are key skills for daily living. However, it is important to also emphasize other aspects of learning and expression. Like all people, those with dyslexia enjoy activities that tap into their strengths and interests. For example, people with dyslexia may be attracted to fields that do not emphasize language skills. Examples are design, art, architecture, engineering and surgery.\nSOURCE : NATIONAL CENTRE FOR LEARNING DISABILITIES\nFor more information on Dyslexia in Nigeria, please contact\nC.A.D.E.T Academy, 10b Owena Close, Off Yedseram Street, Maitama Abuja, Nigeria\nPhone: 0705 763 0825"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:104d2be9-cf7f-4e09-96e7-64cf4d20d687>","<urn:uuid:6624009d-4aaa-40c0-a666-65156a3e7e50>"],"error":null}
{"question":"Can both radiologists and pathologists determine if a condition is spreading to other parts of the body? 😮","answer":"Yes, both radiologists and pathologists can determine if a condition is spreading, but through different methods. Radiologists use imaging tools to visually detect if diseases have spread, collaborating with physicians to interpret these results. Pathologists examine tissue samples microscopically to determine if tumors are invasive (can spread) or noninvasive (cannot spread), and check lymph nodes and blood vessels for signs of cancer spread. They specifically note in pathology reports whether cancer cells are present in lymph nodes or if tumors have grown into blood or lymph vessels.","context":["Following are the things that radiologists do.\n1. Diagnosis of Illness\nThe radiologists get patients with reference of doctors or when the doctor refers a patient to a radiologist. They have tools like DICOM viewer mac, X-rays and other machines to better find out the illness and its conditions. We know that some diseases cannot be diagnosed with bare human eyes or by the doctors. For such diseases, the equipment and machines can help and that is when a radiologist comes in handy.\n2. Collaborate with Physicians\nWhen the radiologists perform tests and get the image results using a DICOM viewer mac, they forward the reports and images to the doctors. The physician then calls the patients to update them about the diseases and conditions. If there is a need of any treatment, the doctor tells the patients for check-ups and follow up. In some cases, the physicians ask for further tests and image results that radiologists do perform.\n3. Patient Efficiency\nAccording to a Medscape report, the radiologists see more patients than other medical specialist. The reason is that a dental surgeon, a traditional doctor, surgeon of cancer or any other disease sends the patients for imaging and test to a radiologist. If you count all the patients, they are more than any doctor. For this, the radiologists must be efficient and show great work passion. They have to perform quick tasks and move on to the others.\n4. Administer Treatment\nThe primary duty and responsibility of a radiologist is diagnosis of the illness and disease. But in some cases, they administer the treatments of patients and cooperate with the team members. There are some cases when the physicians recommend therapies that radiologist perform with the help of relevant medical teams. In serious cases of cancer, aggressive chemotherapies and radiation treatments are suggested that radiologist administer.\nApart from what we have discussed, there are some more duties and responsibilities of the radiologists that are as follows:\n- They provide guidance and advice on qualities of radiology tools and instruments for maintaining better facilities.\n- They do participate in research work and studies related to radiology.\n- They develop treatment plans and options for the radiology patients.\n- Sometimes, they administer and supervise radiology works by the juniors and make sure the right techniques are used for diagnosis.\n- Interpret and prepare image results using tools like DICOM free online viewer.\n- They also conduct physical examinations to inform decisions.\n- Evaluate and assess radiology tools and machines to reduce patient risks and harms.\n- Ensuring patient safety while they do imaging and other tests recommended by the doctors.\n- They should know when it comes to operating radiology machines and equipment\n- They also monitor some patients carefully.\n- On the basis of image results and reports, doctors consult with the radiologists to recommend courses and treatment options that they have to do.\n- Administering nuclear medicine, oncology, or other radiation techniques to treat patients with illnesses or diseases.\n- They are sometimes also responsible for prescribing medication.","A pathology report is a medical document written by a pathologist. A pathologist is a doctor who diagnoses disease by:\nExplaining laboratory tests\nEvaluating cells, tissues, and organs\nThe report gives a diagnosis based on the pathologist’s examination of a sample of tissue taken from the patient’s tumor. This sample of tissue, called a specimen, is removed during a biopsy. Learn about the various types of biopsies.\nBy looking at and testing the tumor tissue, the pathologist is able to find out:\nIf the tissue is noncancerous or cancerous. A cancerous tumor is malignant, meaning it can grow and spread to other parts of the body. A noncancerous, or benign tumor, means the tumor can grow but will not spread.\nOther specific details about the tumor’s features. This information helps your doctor figure out the best treatment options.\nYour doctor will receive these test results as they become available. It may take a few days to a few weeks to receive the full report. The timing depends on the testing needed. You are allowed by law to receive a copy of your pathology report. But you should expect the report to contain highly technical medical terms. Ask your doctor to explain the results in the pathology report and what they mean.\nParts of a pathology report\nDifferent pathologists use different words to describe the same things. But most pathology reports include the sections discussed below.\nPatient, doctor, and specimen\nThis section lists the following items:\nPatient's name, birth date, and other personal information\nAn individual number assigned to the patient to help identify samples\nThe pathologist’s and oncologist’s contact information, as well as the laboratory where the sample was tested\nDetails about the specimen, including the type of biopsy or surgery and the type of tissue\nGross, or obvious, description\nThis section describes the tissue sample or tumor as seen with the naked eye. This includes the general color, weight, size, and consistency.\nThis is the most technical section of the report. It describes what the cancer cells look like when viewed under a microscope. There are several factors noted in this section that affect diagnosis and treatment.\nWhether the cancer is invasive. Tumors of many types may be noninvasive (in situ, which means “in place”) or invasive. Invasive tumors can spread to other parts of the body through a process called metastasis. Although noninvasive tumors do not spread, they may grow or develop into an invasive tumor in the future. For invasive tumors, it is important for the pathologist to note how much the tumor has grown into nearby healthy tissue.\nGrade. Grade describes how the cancer cells look compared with healthy cells. In general, the pathologist is looking for differences in the size, shape, and staining features of the cells. A tumor with cells that look more like healthy cells is called \"low grade\" or \"well differentiated.\" A tumor with cells that look less like healthy cells is called \"high grade,\" \"poorly differentiated,\" or \"undifferentiated.\" In general, the lower the tumor’s grade, the better the prognosis. There are different methods used to assign a cancer grade for different types of cancers. Learn more about grading for specific cancer types.\nHow quickly cells are dividing, mitotic rate. The pathologist usually notes how many cells are dividing. This is called the mitotic rate. Tumors with fewer dividing cells are usually low grade.\nTumor margin. Another important factor is whether there are cancer cells at the margins, or edges, of the biopsy sample. A “positive” or “involved” margin means there are cancer cells in the margin. This means that it is likely that cancerous cells are still in the body.\nLymph nodes. The pathologist will also note whether the cancer has spread to nearby lymph nodes or other organs. Lymph nodes are tiny, bean-shaped organs that help fight disease. A lymph node is called “positive” when it contains cancer and “negative” when it does not. A tumor that has grown into blood or lymph vessels is more likely to have spread elsewhere. If the pathologist sees this, he or she will include it in the report.\nStage. Usually, the pathologist assigns a stage using the TNM system from the American Joint Committee on Cancer (AJCC). This system uses 3 factors:\nThe size and location of the tumor (Tumor, T)\nWhether cancer cells have spread to the lymph nodes located near the tumor (Node, N)\nWhether the tumor has spread to other parts of the body (Metastasis, M).\nPathologic stage, along with the results of other diagnostic tests, helps determine the clinical stage of the cancer. This information guides a person’s treatment options. Learn more about the stages of cancer.\nResults of other tests. The pathologist may perform special tests to identify specific genes, proteins, and other factors unique to the tumor. The results of these tests may be listed in a separate section or in a separate report. These additional tests are especially important for diagnosis because choosing the best treatment option may depend on these results.\nThis section provides the \"bottom line.\" You may find this section at the beginning or the end of the report. If cancer has been diagnosed, the section may include the following:\nThe type of cancer, such as carcinoma or sarcoma\nLymph node status\nAny other test results, such as whether the tumor has hormone receptors or other tumor markers\nSynoptic report, or summary\nWhen the tumor was removed, the pathologist will include a summary. This lists the most important results in a table. These are the items considered most important in determining a person’s treatment options and chance of recovery.\nSometimes, a cancer may be difficult to diagnose or the development of the cancer is unclear. In these situations, the pathologist may use the comments section. Here, he or she can explain the issues and recommend other tests. This section may also include other information that can help the doctor plan treatment.\nSometimes, the pathology report for a biopsy may be different from a later report for the entire tumor. This happens because the features of a tumor can sometimes vary in different areas. Your doctor will consider all of the reports to develop a treatment plan specific to you.\nQuestions to ask your health care team\nTo better understand what your pathology report means, consider asking your health care team the following questions:\nWhat type of cancer do I have and where did it start?\nHow large is the tumor?\nIs the cancer invasive or noninvasive?\nHow fast are the cancer cells growing?\nWhat is the grade of the cancer? What does this mean?\nWas the entire cancer removed? Are there signs of cancer cells at the edges of the sample?\nAre there cancer cells in the lymph vessels or blood vessels?\nWhat is the stage of the cancer? What does this mean?\nDoes the pathology report specify the tumor characteristics clearly? Should we get another pathologist’s opinion?\nDo any tests need to be done again on another sample or in another laboratory?\nGetting a second opinion\nIt may be helpful to talk with more than one doctor about your diagnosis and treatment plan. This is called a second opinion. It is important to get a copy of the pathology report and any other medical records.\nIf you choose to get a second opinion, you will want to share these with the second doctor. Some doctors work closely with their own pathologists and may want their own pathologist's opinion too. Other tests can also be done on the biopsy sample if needed. The tissue sample is kept for a long time and is available upon request. Learn more about getting a second opinion."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:9f6fbc7c-a7fd-40b6-a516-30cd7169a7fb>","<urn:uuid:b0cb384e-2fb9-4a6c-af6b-e1210f5d7819>"],"error":null}
{"question":"How do the artistic approaches of Alain Resnais and Jules Bastien-Lepage differ in their portrayal of social realities?","answer":"Alain Resnais and Jules Bastien-Lepage had contrasting approaches to depicting social realities. Resnais focused on contemporary urban social events and relationships through experimental cinema, making politically charged films that reflected post-war French society's disquiet. His work often dealt with controversial subjects like the Hiroshima bombing and the Algerian war. In contrast, Bastien-Lepage concentrated on rural life and peasantry, painting with frank realism and authenticity. He focused on capturing the beauty of countryside and manual labor, as seen in works like Potato Gatherers and Haymakers, avoiding sentimentality while depicting the truth of rural existence.","context":["Born: 3 June, 1922, in Vannes, Brittany. Died: 1 March, 2014, in Paris, aged 91.\nIn the 1950s, Alain Resnais established himself as the leading French film director, pioneering cinema that was aggressive and socially aware.\nThe Nouvelle Vague (New Wave) group were like-minded innovators who refused to abide by the conventions of the status quo and wanted to make films about contemporary social events and relationships.\nResnais and his contemporaries were out to sweep aside the costume dramas that prevailed and introduce radical, experimental real-life dramas that reflected the post-war disquiet that pervaded French society.\nResnais was a member of the Rive Gauche section of the New Wave – the Rive Droite was more traditional and had stronger backing. Resnais was a more bohemian, experimental figure, and financially less secure.\nThey frequented the cafés of Montparnasse along with literary figures like Gide and Sartre. Resnais captured their socialist views in films that included Hiroshima Mon Amour and Last Year at Marienbad.\nHis last film, The Life of Riley, based on an Alan Ayckbourn play, premiered at the Berlin Film Festival last month, where it won the Silver Bear Alfred Bauer Prize for a feature film that opens new perspectives.\nHis career spanned sixty years and his genius as a film-maker and social commentator never faltered. Resnais remained inventive and gave all his movies a stamp that was personal and distinctive.\nHe spoke at the Cannes Film Festival in 2012, saying that he made films “for myself: like DIY. It’s like a laboratory experiment in which you mix things without knowing what the result will be.”\nResnais studied at a film school in Paris, but made his first film when he was 13, based on the storybook villain Fantomas.\nHe worked as a film editor, making several documentaries and short movies – notably Night and Fog (1955), which highlighted the Nazi concentration camps.\nHiroshima Mon Amour was his first full-length film and is now recognised as a classic of the New Wave movement.\nMade in 1959, Resnais based the film on a play by the distinguished playwright Marguerite Duras.\nWith subtle cinematography and gentle directorship, Resnais intensified Duras’ conversation between a French-Japanese couple about memory and forgetfulness.\nHe made clever use of flashbacks to tell the story of the Hiroshima bombing without actually depicting it, and the film won an Oscar nomination.\nBut it was controversial in that it dealt with the atom bomb and its use against Japan. Significantly, it was not considered for an award at the 1959 Cannes Film Festival in case it upset the US government. Many consider it the greatest of Nouvelle Vague films, in its originality and controversy. Critics have called it “the first modern film of sound cinema” and last year it was shown at the Cannes Festival, as part of the Cannes Classic section.\nIn 1961, Resnais directed a dream-like movie which has confused film experts for years. Last Year at Marienbad has a wonderfully enigmatic narrative, in which Resnais deliberately misleads the audience as to what is the truth and what is make-believe. Consequently some hail it as a great film, while others dismiss it as incomprehensible. Delphine Seyrig, dressed magnificently throughout by Chanel, delivers a mesmerising performance as A – Resnais having added to the confusion by not providing any of the characters with a distinctive name.\nMuriel followed in 1963 and it, again, explored how memory can influence the present. It made reference to the hugely controversial subject of the French attitude to the war in Algeria, which at the time divided the country.\nSeyrig again played the influential central role of a woman obsessed with a teenage lover. The film was a disappointment at the box office but within a few years had become a cult classic.\nThose three films made Resnais a director of real consequence, and while his films in the Sixties lacked the political backbone of earlier years (although many consider Stavisky, with music by Stephen Sondheim and starring Jean-Paul Belmondo, undervalued), he returned to cutting-edge drama with Providence in 1974.\nThe English-language movie had a script by David Mercer and starred Sir John Gielgud and Dirk Bogarde. It has a magisterial quality, and Resnais depicts Gielgud’s near-death and bitter character with a strong slice of caustic humour.\nResnais created for Gielgud something very far from the classical character that the public would have expected, with the actor swearing like a trooper, spitting out four-letter words with venom and obvious relish.\nThe film was a huge success and won many awards. For his part, Gielgud loved the experience and called Resnais “the most English of French directors”.\nResnais continued to direct films and his reputation along with such New Wave directors as Jean-Luc Godard, Jacques Rivette and François Truffaut grew over the years.\nHis work was recognised for its originality, imagination and perplexing honesty. Throughout his career Resnais maintained the ability to astound, beguile and confuse.\nHe received many cinema awards including a lifetime achievement award at the Cannes Film Festival in 2009.\nResnais married twice, and is survived by his second wife, Sabine Azema.","Biography of Jules Bastien-Lepage\nChildhood and Education\nJules Bastien-Lepage was raised in a close family unit that included both parents, his beloved grandfather, and a younger brother (Émile, who would also become a professional artist). His parents owned a modest farm in the idyllic village of Damvillers in Northern France, close to the France/Belgium border. This setting proved to be the key influence on his paintings of rural life.\nBastien-Lepage's father was a keen amateur artist, and nurtured his sons' aptitude for drawing. According to his biographer André Theuriet, his father \"required that Jules should draw with pencil on paper the various articles in use upon the table - the lamp, the jug, the inkstand, etc. It was to this first education of the eye and of the hand that Bastien-Lepage owed that love of sincerity, that patient seeking for exactness of detail, which were the ruling motives of his life as an artist\". That being said, his father was not comfortable with the idea of his son's interest developing beyond a hobby and was unhappy when he declared his desire to pursue art as a profession. To fulfil his father's wishes, and with a great strain being put on the family resources, Bastien-Lepage attended the College of Verdun from the age of eleven. It was hoped that the young Jules would harness his sublime drawing ability and pursue a career as a draftsman. However, Bastien-Lepage showed little aptitude for draftsmanship and preferred to fill his drawing pads with scenes of rural life.\nAged 19, his college education complete, Bastien-Lepage expressed his desire to travel to Paris to study art. His ambition was beyond the financial reach of the family and, on the advice of a family friend, he took up a job with the Central Postal Administration. Though he earned barely enough to live on, the post meant he was able to attend art classes at the Ecole des Beaux Arts where he registered under his given name, Jules Bastien (he soon added Lepage, his mother's maiden name, as a way of sounding more distinctive). For six months he balanced both roles. He worked at the post office from 3:00 to 7:00 am in the sorting office, before delivering mail till noon. He attended school in the afternoons and, with the added burden of homework, his double life became impossible to maintain. After six months Bastien-Lepage resigned his post at the Post Office and entered art school as a full-time student.\nArmed with a letter of recommendation from the academic painter William Bouguereau, Bastien-Lepage approached Alexandre Cabanel who took him under his wing and schooled him in the traditions of academy painting. Realizing his potential, Bastien-Lepage's supported their son as best they could. His mother sent him a small sum of money every month and this, with an allowance of 600 francs from the Council General of Meuse, covered his bed-and-board. Bastien-Lepage's threw himself into his studies, but his ambition was interrupted with the outbreak of the Franco-Prussian War in 1870 when he enlisted in a company of volunteers. According to Theuriet, \"One day in the trenches a shell burst near him and sent a clod of hardened earth straight at his chest. He was taken to the ambulance, where he remained during the last month of the siege, while another shell fell upon his studio, and there destroyed his first composition, a nude nymph with her arms clasped over her blonde head, and bathing her feet in the waters of a spring\". He would spend the majority of 1871 recuperating in Damvillers before returning to Paris in 1872.\nBastien-Lepage had first submitted work to the Paris Salon in 1870. In these early works he copied the landscapes of Jean-Antoine Watteau and inserted fashionable young women into the scenery. These pieces went largely unnoticed but in 1874 he painted his grandfather in the family garden. It proved to be a turning point in his career. His grandfather, with his proud beard and black velvet beret, was illuminated against a rich green backdrop of trees. The painting perfectly captured humble country life, and it delighted the Parisian public. It won him a third-place medal and the name Bastien-Lepage started to feature in Salon literature and reviews.\nThe following year, Bastien-Lepage entered The Communicant, and a portrait of the politician and industrialist Simon Hayem, to the Paris Salon. The Communicant in particular showed the artist's ability at revealing his subject with a frank realism that touched the public in a way that academic art could not. According to Theuriet, \"this young girl's simple awkward bearing, as she stands out from a creamy background, with all the stiffness of her starched white veil, naively opening her pure hazel eyes, and crossing her fingers, ill at ease in the white gloves, is a marvel of truthful painting [...] It is interesting, as being the first of those small, lifelike, characteristic portraits, in a style at once broad and conscientious, which may be reckoned among the most perfect of this painter's works\".\nLike other exceptional students at the time, Bastien-Lepage was entered into the prestigious Prix de Rome competition. Having been tipped to win, he took his first defeat in 1875 to heart, but having failed to win on a second occasion (in 1876), he realized that he had no real interest in travelling to Italy to study and had only entered the competition because that was the path elite academy students were expected to follow. Indeed, Bastien-Lepage began to question the true value of academic training and vowed to produce a more personal body of work. He said later, \"I learned my business in Paris, I shall not forget that; but my art I did not learn there. I should be sorry to undervalue the high qualities and the devotion of the masters who direct the school. But is it my fault if I have found in their studio the only doubts that have tormented me? When I came to Paris I knew nothing at all, but I had never dreamed of that heap of formulas they pervert one with. In the school I have drawn gods and goddesses, Greeks and Romans, that I knew nothing about, that I did not understand, and even laughed at. I used to say to myself that this might be high art; I wonder sometimes now if anything has resulted from this education\".\nBastien-Lepage's preference for the sombre tones of Naturalism clearly distinguished him from the light and colorful Impressionists who were gaining popularity in France at the time. Indeed, historian William S. Feldman stated that Bastien-Lepage saw himself \"as heir to the Realist tradition of Millet and Courbet\" and espoused the philosophy that \"'Nothing is good but truth [and] I come from a village in Lorraine [and] mean, first of all, to paint the peasants and landscapes of my home exactly as they are'\". While he kept an apartment and studio in Paris, Feldman observed that the artist remained \"True to his credo [and] conceived a continuing series of monumental plein air compositions focused in this direction, and executed in a highly personal, eclectic style\". Even when he took on bigger subjects, such as his painting of French martyr Joan of Arc, he placed her in the Damvillers countryside and used a local teenager as his model. Historical figures aside, he conceived of a series of large plein air paintings that evoked his own upbringing and saw him produce what were arguably his most important works, Haymakers (1877) and Potato Gatherers (1878).\nBoth paintings were exhibited at the Paris Salon in 1879 where they attracted attention for their candid realism. Identical in size and shape, and featuring the same model, Bastien-Lepage's cousin, Marie-Adéle Robert, it was Potato Gatherers that drew direct comparisons with Millet, and especially his 1857 painting, The Gleaners. Both Potato Gatherers and The Gleaners celebrated the beauty of the land, and both exalted the virtues of manual labor, but Bastien-Lepage's painting was considered much less sentimental in tone. Both Haymakers and Potato Gatherers were a success but it was the latter that became the key attraction, winning over both the public and the Salon jury.\nAs the painter and writer Cathy Locke observed, along with Haymakers and Potato Gatherers, Bastien-Lepage also exhibited a portrait of the rising French actress Sarah Bernhardt. \"Painted in high key values\", Locke noted that \"this small gem of a portrait\" was made when both Bernhardt and Bastien-Lepage were rising stars, and since both were \"making the rounds with the social elite of Paris, it would have been only natural for [Bernhardt] to sit for this portrait\". Locke adds that at the age of just thirty-one, Bastien-Lepage was awarded the prestigious Legion of Honor for the three Salon entries, and thereby confirmed the painter's \"official arrival on the French art scene\".\nBastien-Lepage's rising star brought him financial security. According to Theuriet, \"he took his mother to a large shop and had silks for dresses spread out before her. 'Show some more,' cried he. 'I want Mama to choose the best'\", and even though she was disinclined to wear such finery, \"She was obliged to give way\". He was also comfortable enough to purchase land adjacent to the family farm. As Theuriet explains, Bastien-Lepage, \"employed his gains in adding to the paternal domains. [including the purchase of] an orchard situated in the old moat of the town, which had belonged to an unfrocked priest\".\nWhile peasant and nature scenes brought him success and personal satisfaction, Bastien-Lepage had also made his name as a painter of beautiful, and highly realistic, portraits. His reputation spread beyond France to the English nobility and he received the important commission to paint the portrait of the Prince of Wales which he rendered in 1879. According to Crastre, \"to Bastien-Lepage portrait painting was only a side issue, a form of relaxation between two landscapes; his predilection, his one object in life, so to speak was to return constantly to his peasants, his scenes of toil, his fields of Lorraine\". That his work appealed to the young artists of the day can be seen in the friendship he developed with the Russian Maria Bashkirtseff in 1882. His influence is evident in the way she rendered both her portraits and her street scenes. While they shared a love of art, they were also both fated to tragically short lives with Bashkirtseff dying of tuberculosis at the age of twenty-five, less than two months before Bastien-Lepage's own death.\nAt the beginning of 1881, Bastien-Lepage made painting excursions to Switzerland and Italy. Sadly, serious illness would make these the Frenchman's last overseas tours. In 1883, Bastien-Lepage developed intense kidney pains and, as Crastre describes, \"he became melancholy, nervous, irritable; he shut himself up in his studio [...] and even his best friends could not gain admittance\". Eventually his doctor was able to convince him to travel to the coastal region of Brittany in the hope that it might aid his recovery. For a brief period the beautiful nature and sunshine helped him to forget his troubles and he was able to paint freely. Soon, however, his condition worsened and he was forced to return to Paris to receive treatment. Writing to a friend, he complained that his, \"digestive tube is always kicking up a row\".\nThe Paris doctors informed the artist of the gravity of his situation and this time encouraged him to travel to Algeria believing that the North African climate might help his condition. He was in such poor health that he could not make the journey alone so his mother accompanied him in the capacity of his nurse. At first he seemed to be on the road to recover and, inspired by the new sights and sounds, he began painting in earnest. Focused only on his art he stated, \"I am not afraid of death. Dying is nothing - the important thing is to survive oneself, and who can be sure of establishing a claim upon posterity? But there! I am talking nonsense! So long as our work is true, nothing else matters\". Sadly, his health continued to deteriorate and, as Crastre put it, \"before long the ravages of the disease began to make headway; the kidneys no longer performed their function, and he suffered atrocious agonies which stretched him for days at a time on his back. Even the burning heat of the African sun no longer had strength enough to animate his shattered physique; the brush, which the artist from time to time attempted to take up, fell from between his fingers\".\nA seriously ill Bastien-Lepage returned to Paris in May of 1884. He still longed to paint and, as Crastre describes, \"as long as he could hold a brush, Bastien-Lepage continued to work, in spite of the sufferings which racked him\". He eventually grew so weak that he could barely hold a brush. He stated, \"If I was told: They are going to cut off your two legs, but after that you will be able to paint again, I would willingly make the sacrifice\". Having put up a brave fight, Bastien-Lepage succumbed to his illness six months after returning to Paris. He was aged just thirty-six. The Musée d'Orsay records on its website that \"The day after Jules Bastien-Lepage died at his Paris studio on rue Legendre, on the 10th December 1884, the world press announced his premature death and burial in the family cemetery in Damvillers in the Meuse region of France. In just ten productive years, this son of a modest farming family had won an eminent place in the French and international art scene\".\nThe Legacy of Jules Bastien-Lepage\nAlthough his career lasted no more than a decade, Bastien-Lepage made a profound impact on the French art world. Referred to as \"the grandson of Millet and Courbet\" by French novelist Émile Zola, he captured the beauty of the French peasantry and countryside by rendering his subjects with an authentic realism. Speaking of the impact of his work, author George Clausen acknowledged the artist's, \"love of nature and resolute determination not to depart from the strict literal truth as he saw it\", and in so doing, \"he brought to us what was in some ways a new view of nature - one whose truth was at once admitted\".\nBastien-Lepage offered a challenge to the prevailing Impressionist style though this was not a conscious move on the part of the artist. As author Fr. Crastre explains, \"as painter of the open air, he became in a certain sense the founder of a school, without meaning to be; for his conception of the painter's art won over a whole group of young artists who united in hailing him as their master\". Author William S. Feldman added, \"for a younger generation of painters disenchanted with the excesses of both conservative and radical camps, the 'juste milieu' ['middle ground'] approach of Bastien-Lepage represented the viable alternative. Consequently, a school of Bastien-Lepage followers emerged virtually overnight, attracting practitioners from France, Great Britain, and throughout Europe\".\nHe had (albeit inadvertently) suggested a new way for future generations and directly influenced artists such as James Guthrie, John Lavery, and William York Macgregor who, as members of The Glasgow Boys, followed Bastien-Lepage's lead in painting rural Scotland. His style was also influential on a group of Australian Impressionist artists who adapted his techniques to capture the beautiful nature of their country and included artists Charles Conder, Fred McCubbin, Tom Roberts, and Arthur Streeton. His influence can still be felt today in the work of contemporary artists such as John Felsing, Steve Kestrel, and Ron Kingswood all of whom work in the style of Naturalism.\nContent compiled and written by Jessica DiPalma\nEdited and revised, with Summary and Accomplishments added by Anthony Todd\nContent compiled and written by Jessica DiPalma\nEdited and revised, with Summary and Accomplishments added by Anthony Todd\nFirst published on 20 Feb 2021. Updated and modified regularly"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:43a3e9a8-946a-43c7-83c9-b7eac5ab9c64>","<urn:uuid:21912373-4bd7-45f8-8596-6c4b0c4ade3d>"],"error":null}
{"question":"How has watch illumination technology evolved from radium to modern tritium tubes?","answer":"Watch illumination began with radium, a highly radioactive material that glowed on its own but was phased out due to health hazards. This was followed by tritium-loaded paint, which while safer for wearers, posed risks to paint operators. Modern technology uses sealed glass tubes (Trigalight) filled with tritium gas at specific pressures, internally coated with zinc sulphide phosphor. These tubes are laser-cut to size with sealed ends, providing safe and consistent illumination. Today, companies like ArmourLite Watch Company manufacture watches using this technology, with T25 and T100 variants containing different amounts of tritium gas, offering varying brightness levels and longevity of illumination. The T100 technology represents the latest advancement, providing much brighter illumination and approximately twice the lifespan of T25 watches.","context":["Light It Up!\nAs we dive into the annual festive season, we see all sorts of decorative lights hanging in the streets, in the trees and in our homes. But let’s not forget that in our watch boxes, there are also small little machines with their own decorative, and functional, lights. Just exactly how does our watches light up? We have a nerdy write up today to go into the science of illumination. The mechanics of the lume.\nPhysics of illumination\nBe it Christmas LED lights, television or the backlit screens on the classic F91W, the reason we see light is because there are packets of energy, called photons, entering our eyes. And when these photons vibrate at a particular wavelength, we see them as a particular colour. It is probably much easier if we looked at some familiar watches and explain from there.\nOne of the most efficient form of illumination is the light emitting diode, or LED as most of us know it. A current passes through a semiconductor which causes electrons to move, releasing photons.\nA more modern, but still classic example would be the Casio F-91W, which some Singaporeans would call the Army watch. It’s low cost, high versatility and ultra light weight makes it a great choice for intense outdoor training in the military. The LED backlight on this is green, suggesting that indium gallium nitride was used in the semiconductor.\nA newer, and higher end and fully mechanical watch that uses LED is the impressive HYT H4 Alinghi. I say impressive not so much because of the LED technology, but the engineering efforts to miniaturize a brushless dynamo into a corner of the watch, eliminating the need for a battery or electrical energy storage.\nLED bulbs tend to give an uneven illumination since it is a small bulb tucked away at the edge of the dial. Along comes electroluminescent (EL), or commonly known as Indiglo in Timex watches. Casio’s F-105W and uncountable other digital watches uses EL backlight in place LED bulbs for aesthetics reasons and better night time legibility. Underneath the dial, there is a very thin film consisting of essentially 4 layers: a conducting layer, a phosphor layer, an insulator and another conducting layer. When a current is applied across the thickness, the electrical current energizes the phosphor and causes photos to be emitted evenly across the film.\nPhosphorescence, or lume, is the go-to passive method of illumination on watches. We love our lume. There are even hashtags for lume like #frinightlume #lumeparty. What’s not to love about that gentle glow? To better describe how the glow is created, we’ll have to get into some very technical discussion that’s usually left for physicists. Phosphors are a group of materials that have the ability to absorb light energy (even if slightly outside the visible band, such as UV or IR) to bring electrons to an “excited” state. Once the energy source is removed, the material can continue a slow decay where the electrons will jump back to their “relaxed” stated. During which, the energy is released as photons of specific wavelength. There are a few types of phosphors, but most commonly used is zinc sulfide and strontium aluminate. This slow decay happens over a time frame between minutes to hours depending on the age and quality of the lume.\nA prominent lume that most readers have seen is the chromalight by Rolex, which has a signature blue tint instead of green. We now understand that the glow comes from a photon being released as the electrons jump from “excited” to “relax” states. Going one step deeper in the science, it is also possible to tune the colour we want, by adding in various activators to control the wavelength of the released photons. In the familiar green glow we know, the dopant is copper and releases photons around 535nm. In the more exotic Chromalight, europium is used and the releases photons at wavelength of about 490nm.\nMost lume only hold their charge for under an hour. Of course good watch brands such as Rolex are able to produce paints with superior phosphorescent properties and even then, it tends to last no more than a few hours. Pretty sure you have noticed that as the paint ages, it holds less charge and shows reduced brightness and glow time. To overcome the short glow time, some other form of activation has to be used. Introducing radioactive emission. As radioactive materials decay, they emit energy that can be absorbed by phosphor, and then the phosphor does it’s magic where electrons that surround it jumps from a higher state to a lower more stable state, and releases a photon in the process. As the radioactive material does not stop decaying, it is constantly bombarding the phosphor with electron beams which creates a persistent glow by the phosphor.\nAlmost a century ago, radium was the radioactive material of choice to be put in lume paint. On its own, it appears as white. Radium and all its isotopes are highly radioactive and glows on its own without further addition of phosphors. However, the detrimental health effects soon caught attention and it was completely phased out, albeit taking decades to be completely eliminated. In more recent history, radioactive material tritium (3H, a radioactive isotope of Hydrogen) became a more popular choice as shown in the Sinn EZM1. In those days, the tritium loaded paint (and radium) was applied as a paint. While harmless to the wearer, the operators who applied the paint manually were exposed to an amount that accumulated and became harmful. More so for those who had the habit of licking their paint brush tips to get it sharp for these precise applications. Eventually, tritium also became less popular and got replaced by light-activating lume.\nIn more recent years, engineers worked around the safety aspect of using radioactive materials and introduced the concept of using sealed tubes of tritium gas. Mb-microtec manufactures these tubes with the brand name Trigalight.\nIt’s a long glass tube internally coated with a phosphor (zinc sulphide with customizable dopants in this case) and then filled with tritium gas to an optimal pressure. The higher the pressure, the more radiation to activate the phosphor and thus increasing brightness. The long glass tube is then laser cut into the small required lengths, with the laser sealing the ends as it cuts. Although it will not need re-charging, the overall life of the lume depends on the half life of the tritium gas (about 12 years).\nProbably about half the watches available comes with some form of illumination; from the most economic Casio to haute horology such as A. Lange & Söhne Datograph Up/Down Lumen. You’re likely to own at least one that illuminates, so pick out your favourite and go join the festive night lights this holiday season!","❍ Servicing & Repairs\n❍ Returns & Exchanges\n❍ Military, Law Enforcement & First Responders Discount\n❍ Custom Tritium Illuminated Watches\nSwiss Tritium Illumination\nHow does Swiss tritium illumination technology work?\nArmourLite and Isobrite watches have tiny glass tubes that are filled with tritium gas and then hermetically sealed. The electrons of the gas continuously react with phosphorescent paint resulting in a radio luminescent light source that glows constantly. No external power source or charging-by-light is necessary to maintain the glow. The glow is completely self-powered.\nWhat do the T25 and T100 designations mean?\nThe T25 and T100 designations refer to the combined amount of tritium gas that is hermetically sealed in the glass tubes in the watch. T25 watches have up to 25 mCI (millicuries) of tritium gas in the watch whereas T100 means there is up to 100 mCi of tritium gas in the watch.\nWhat are the benefits of a T100 watch versus a T25 watch?\nT100 represents the latest technology in tritium watches and makes seeing the time in the dark much easier on the eyes. Since the Isobrite T100 watches hold more tritium gas than T25 watches do, the higher volume of gas results in more electrical energy being created. More electrical energy results in much brighter visible illumination. When transitioning from a bright environment to a dark environment it takes the eyes time to adjust. With the increased visible brightness of T100, the transition is much easier on the eyes. Plus, as a person ages and the eyes don’t see as well, having T100 means the time can still be easily read in the dark.\nWhy do T100 watches last so much longer than T25 watches?\nThe Isobrite T100 watches contain approximately four times the amount of tritium gas as the T25 watches do. The higher volume of tritium gas results in a much brighter visible illumination combined with a much longer life span of illumination. The Isobrite T100 watches will have a life span that is approximately twice as long as the T25 watches. Plus, at 25 years the T100 watch will have the same visible illumination that the T25 watch started out with. By that point the tritium in a T25 watch would have become fully decayed and barely visible. On the other hand, the T100 watch will continue to glow for approximately 50 years.\nWhy isn’t the glow visible in bright lighting?\nThe tritium markers in the watch are constantly glowing. However, the glow is only visible to the eye as the ambient light starts to fade. The glow cannot compete with ambient light. As the level of ambient light decreases, the glow will become increasingly more visible.\nWhy do the tritium markers glow under ultraviolet light?\nShining ultraviolet light on the face of the watch is commonly done during sales presentations to show the colors of the tritium markers and simulate how the glow will appear in the dark. The science behind this is that ultraviolet light activates the phosphorescent paint in the tritium markers so that the glow becomes visible anytime and anywhere. It is important to note that ultraviolet light cannot differentiate between T100 and T25, or show how bright the watch will actually be in the dark. The intensity of visible brightness can only be seen in actuality by taking the watch into a dark environment.\nWhat kind of warranty do the watches come with?\nYour new watch from ArmourLite Watch Company comes with a manufacturer’s warranty of two (2) years. The warranty covers material and manufacturing defects existing at the time of purchase.\nServicing & Repairs\nWhere can I get my watch serviced and/or repaired?\nArmourLite Watch Company operates a state-of-the-art service and repair center in Miami staffed by a team of watchmakers and technicians that are trained to service and repair all ArmourLite and Isobrite watches. By utilizing advanced tools and testing equipment, the center is able to ensure servicing and repairs are performed accurately, economically, and as quickly as possible. Our in-house center also maintains a comprehensive inventory of genuine spare parts to ensure the quality and integrity of the servicing.\n⚫ Click Here To Download the Servicing & Repair Form for ArmourLite T25 Watches\n⚫ Click Here To Download the Servicing & Repair Form for Isobrite T100 Watches\nDo you offer free shipping?\nYes, we offer free ground shipping, including insurance, for orders shipping anywhere in the United States and Canada. You don't have to spend a minimum amount to qualify for the free ground shipping.\nWhat are my complete shipping options for orders shipping within the United States?\n$32.95 FedEx Standard Overnight® (1 day)\n$21.95 FedEx 2Day® (2 days)\n$14.00 FedEx Express Saver® (3 days)\n$12.95 USPS Priority Mail (2-4 days)\nFree Ground Shipping (2-5 business days)\nDo you offer international shipping?\nYes, we can ship anywhere in the world. We ship internationally via FedEx and provide both tracking and insurance.\nReturns & Exchanges\nWhat is your return policy?\nIf for ANY reason within ten (10) days after delivery of the order you are unsatisfied with your purchase you may return the item(s) to us for a refund or exchange. Please review the conditions that apply for returns and exchanges here: Return Policy\nMilitary, Law Enforcement & First Responders Discount\nDo you offer a discount for Military, Law Enforcement and First Responders?\nArmourLite Watch Company is proud to offer a discount to Military, Law Enforcement, and First Responders. The Military discount is available to Active Duty, Retirees, Veterans, Military Spouses, and Military Family Members. The Law Enforcement and First Responders discount is available to both active and retired Police, Fire and EMT customers. To verify eligibility for the discount, please send an email to: firstname.lastname@example.org\nCustom Tritium Illuminated Watches\nDo you make custom tritium illuminated watches?\nArmourLite Watch Company is proud to be one of the only watch companies that can design and manufacture custom tritium illumination watches. We have manufactured custom watches for companies, agencies, military units, and more. Some of our clients include the National Rifle Association (NRA), the Federal Bureau of Investigation (FBI), the United States Secret Service, and the United States Army. Please email us at email@example.com to get started."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:cbde0d79-83e8-4f4b-90e6-be2de3a38fcc>","<urn:uuid:9bbc940c-51fa-45cb-9dc0-0ac7a4fe500a>"],"error":null}
{"question":"What essential steps should be taken to protect against buffer overflows and maintain network security?","answer":"To protect against buffer overflows and maintain network security, several essential steps must be taken: 1) Check the length of all user-supplied input variables and reject excessively long inputs in all system components, particularly when using C/C++ code, 2) Use strncpy and strncat functions instead of strcpy and strcat to limit character copying, 3) Implement firewall software to monitor traffic and prevent unauthorized network access, 4) Install and regularly update anti-virus software to identify and remove malicious programs, 5) Use strong passwords and two-factor authentication, 6) Regularly backup data and store it securely, and 7) Keep all installed software up to date. These measures help ensure both protection against buffer overflow vulnerabilities and overall network security.","context":["Buffer-overflow vulnerabilities are back in the news. Buffer overflows, as this tip from InformIT explains, usually...\nBy submitting your personal information, you agree that TechTarget and its partners may contact you regarding relevant content, products and special offers.\ntarget poorly constructed C/C++ code. Elfriede Dustin, Douglas McDiarmid and Jeff Rashka, the writers of this article, provide some tests to see if your system is susceptible to a buffer overflow.\nOne common security hole exploited on Web systems is the buffer overflow, used mostly against compiled executables, particularly operating system tools and utilities. A buffer overflow occurs when the length of a program or a function input exceeds the space allocated to store it. C/C++ programs are particularly vulnerable to this kind of attack, as developers often declare variables that reside on the program's stack. For example, consider the following C/C++ code:\nvoid createFullName(char* firstName, char* lastName)\nstrcat(fullName, \" \");\nThis C++ code simply takes the supplied first and last names and puts them together, separated by a space. Of particular importance is the fullName variable. The way it is declared causes it to reside on the stack. The problem is that this variable can easily exceed 1,024 characters whenever firstName or lastName or both values are too long.\nIn most cases, this situation will simply cause a program crash as the stack is corrupted by the strcpy or strcat function calls. However, if these arguments are carefully crafted, they can in fact be used to send malicious code to the program, embedded in the first- and last-name arguments. If the arguments manage to overflow the fullName stack variable, they can cause the execution of this code by manipulating the return address, which also resides on the stack.\nThe return address is a hidden piece of data that resides on the stack with the rest of the variables passed to a function. Depending on the specific language, compiled programs place this data on the stack prior to calling the function. That way, the program knows where to go when the function is finished.\nBy overflowing one of the variables on the stack, a malicious input can overwrite the return address, as it exists on the stack as well. In the example of createFullName, by overflowing the firstName or lastName inputs with precisely the correct number of characters, the return address can be overwritten and made to point back to a specific place in the data that was supplied in the firstName or lastName inputs. With a little creativity, this data can be executable program code, with malicious intent. Because the return address is simply a pointer to code, the return address is pulled off the stack when the function completes and is used as the place to start executing the next sequence of instructions. Unfortunately, the next sequence of instructions is code written by the attacker and will be blindly executed by the server.\nOnce the malicious program argument has been submitted and the input buffer has been successfully overflowed, the attacker effectively has his or her own code executing on the site's Web server machine. Depending on the buffer size, a lot of bad things can happen. The code could read the contents of the password file, e-mail the file, make changes to configuration files, start up a TELNET session, or even connect to another Web server and download a larger, more damaging program, such as a Trojan horse.\nPreventing buffer overflows consists mainly of checking the length of user-supplied input variables. In the previous example, limiting the size of the firstName and the lastName inputs to 511 bytes would protect against overflows (511*2 = 1,022, plus one for the space and one for the terminating \"null\" totals 1,024). In addition, using the strncpy and the strncat functions instead of strcpy and strcat is also advised, as the former two functions limit the number of characters copied into the buffer. Keep in mind that it is not realistic to restrict the input on the Web page or form, as a malicious user could simply alter the page on his or her local disk and remove the length restrictions on the form fields or simply access the URL of the form action directly, without using the form itself. The only sure way to prevent buffer overflows is to examine and to reject excessively long inputs in all Web system components.\nAgain, the most common source of buffer overflows is C/C++ code, particularly when string manipulation is involved. Scripting languages, such as Perl and Java code are less of a risk, as they use dynamic memory management to allocate space for variables. This does not mean that input lengths should be ignored when using scripting languages or Java. It is still possible that these variables could be passed to other programs that are susceptible to buffer overflows.\nSometimes, variables are simply \"passed through\" a program. For example, a Perl script could be created that takes a form input, such as a customer name, and simply hands it off to another program, possibly one written in C++ or one that came with the operating system that was probably also written in C or C++. The second program in the chain may be susceptible to buffer overflows, so the attacker could still cause damage. It just wouldn't affect the Perl script, as it is passing it through. Therefore, it's important to always check input lengths regardless of language, function and so on.\nTesting for buffer overflows\nMuch like testing component inputs for dangerous metacharacters, testing buffer overflows requires that the component actively check the lengths of all inputs. Component inputs are either HTML form fields or name/value pairs. A name/value pair is a field name and a value, separated by = on the URL line and is formatted as a name/value pair. A sample set of input might look something like name=Norma, orderID=12345.\nWhen an input string that exceeds the maximum length is detected, the component should return a predetermined error page. The successful display of the error page represents a successful test result. The security test procedure is concerned with verifying that an overflow condition cannot be created, by supplying long inputs to the component. The error message produced is simply a way to confirm that the component has properly detected the invalid input and then aborted the activity.\n- The list of Web system components should be augmented with the maximum length of each input that the component will accept. If the engineering techniques described in Chapter 2 are used, these lengths should be documented in the interface use case descriptions. If the input length exceeds this value, the component must return the predetermined error page. If the error page is not returned, a security flaw exists.\n- For each input to the component, supply an input string that exceeds the maximum size by one character. This should return the predetermined error page. Also try the test with an input string that is exactly equal to the maximum size of the field and watch the behavior of the component with this input. Boundary conditions often cause problems when dealing with buffers and string operations. Any printable character is acceptable as input, as long as it is within the valid set of characters that the component is willing to process. A long string of A characters will usually suffice.\nTypically this type of test must be performed either by saving the HTML page to disk and manually editing the value attribute of an input field or by using an automated testing tool to send in the data. As with metacharacter testing, make sure that each field is tested one at a time so any problematic fields can be isolated.\nRead more of this article at InformIT. Registration is required, but it's free.","A network system refers to the interconnection of two or more computers to share resources (such as printers and scanners) to easily exchange and transmit information through cables or wireless connection that follows certain rules or protocols.\nThere are three common types of network, the Local Area Network (LAN), Wireless Local Area Network WLAN), and Wide Area Network (WAN).\n- Local Area Network (LAN) generally covers a limited geographic area like an internet café, school laboratory, or office. Computers are connected to a server or workstations. The server is configured so it can do its function without direct human intervention. Its function includes software hosting, file storage and retrieval, complete access control for the network resources, and many more. On the other hand, workstations need human users to interact and utilize network services, unlike servers.\n- Wireless Local Area Network (WLAN) is the same as LAN, but the connection on the network is wireless.\n- Wide Area Network (WAN) connects a larger geographic area such as a whole state, country, or even the world. This global network is made possible by connecting through the satellite. WAN is a complex network because it uses a variety of connection materials such as multiplexers, bridges, and routers to connect locally, then metropolitan networks for global connection like the internet.\nConnecting to any network will create an opening to potential risk and attacks on the security of computer systems. Thus, an effective and strict computer security system must be implemented.\nWhat are the main types of computer security?\nComputer security protects the information that is stored in a computer from anyone who wants to harm or steal it and prevents unauthorized access. Below are the main types of computer security:\n- Physical Security – setting a unique password to a computer. The password will be known to authorize the person only to assure the safety and confidentiality of the data being stored in that computer.\n- Network Security – the network has many entry points. Thus, network security requires defense methods such as firewalls. A firewall is a software device that monitors the traffic and prevents access to the network depending on security rules.\n- Executable security – installing an anti-virus. An anti-virus is a computer program that will try to identify, remove and prevent viruses from harming your computer system. A virus may delete, duplicate or even damage your whole computer system. So it is a must to have an installed and updated anti-virus.\nThe main goal of computer security is to ensure the confidentiality of the information to authorized persons, to maintain the integrity of the information (which means there are no deviations from the original information being stored), and to make information available to concerned persons only.\nWhat are the 3 basic internal security of computer system?\nAttacks on the computer system and networks are rampant these days; thus, a computer’s internal security system must be strictly implemented and observed. There is three basic internal security of a computer system: management security, operational security, and physical security.\n- Management security is referred to as administrative controls, which provide the guidance, rules, and procedures for implementing the overall security of the system.\n- Operational security is referred to as technical controls, which assure the effectiveness of your controls, such as access controls, authentication, and security topologies applied to networks and installed applications.\n- Physical security refers to the protection of personnel, data, hardware and etc., from physical attacks and threats that can harm, damage, disrupt the operation or even destroy and affect the confidentiality, integrity, or availability of data or the system.\nWhat are computer security risks?\nNowadays, it is complicated and unpredictable to know how or when an attack on your computer system will occur. Harming your computer system through physical damage and non-physical (viruses) may happen at any time, so it’s always better to be careful and avoid it from happening as much as possible.\nComputer security risk is an act that could damage the hardware and software of a computer unit. Following are examples of computer security risks:\n- Humans – the biggest security risk. In an organization, if your employee is not well trained about safety and good practices, misuse of a computer system may happen. It is critical to make sure that they are knowledgeable in terms of security, especially if that employee holds or is assigned in a very important and confidential task.\n- Bad backups – if your backup is corrupted or lost, you will face big trouble, especially if you don’t have any alternative copies. Always make sure that you properly back up the data and keep it in a safe place.\n- Not maintaining hardware or software – will give a potential entry to an attacker. It is always recommended to have regular check-ups of your hardware and software systems.\n- Not planning for a natural disaster and more.\nConnecting to a network or to the internet and installing software, your computer system is already introduced to risk. Today, almost all people need the internet to communicate and need to install new software and applications, we can never assure 100% security, but building and creating strong computer security helps alleviate the risk.\nWhat is the importance of computer security?\nComputer security will keep all information protected. Tightening the security of your computer will prevent unnecessary and harmful incidents that may result in information leaks and loss, and even hardware damages. Being aware and knowing the do’s and don’ts in terms of security will help you to sidestep unwanted occurrences.\nStrengthening computer security, guaranteed safety processing, storage of sensitive information, and more. It also provides confidentiality, integrity, and availability for all components of the computer system.\nComputer security allows your computer system to avoid the following:\n- Viruses – are malicious programs that will be installed into your computer without your knowledge. This program can duplicate, hide and even delete your data. It can also replicate itself and infect other installed software in the system and files.\n- Computer worm – a software program that replicates itself in greater volume and excessive speed and infects from one computer to another computer without the users’ knowledge. This program will use and consume users’ computer hard disk space.\n- Man-in-the-Middle Attacks (MITM) – a cybercrime that occurs when a visitor user uses an unsecured public Wi-Fi network. Hackers interrupt the traffic between the user and the network, and during that time, they can steal data by using malware to install the software.\n- Phishing – a cybercrime that involves someone pretending to be a legitimate and trusted institution over the telephone, email, or even text messages to trick possible targets into disclosing personal and confidential information (such as email, passwords, bank details and etc.). This may result in financial loss and stealing and using your identity.\n- Botnet – a network of connected computers that are conceded by hackers without the users’ knowledge. The infected individual computers are now called a ‘zombie computer,’ which now is the bot. It will be used for doing malicious activities on bigger-scale attacks such as DDoS.\n- Rootkit – malicious software designed to enable unauthorized access to a computer system or restricted network while hiding its existence. It allows a hacker to modify system settings and configurations and execute commands remotely.\n- Keylogger – a software monitoring that can track and record all keyboard strokes and activities without the users’ knowledge\nAdditional precautions to secure harmful incidents:\n- Do not leave your computer unattended\n- Do not visit websites without SSL certificates\n- Do not click links that you are not familiar\n- Be mindful of the information you are entering\nHere are some important reasons why computer security is important:\n- Protects your personal information\n- Protects your companies property\n- Stops unauthorized access\n- Prevents data leak and theft\n- Prevents viruses and malware\nHow do we maintain computer security?\nMaintaining your computer security allows you to avoid hacking and stealing attempts on your information and even your computer hardware. Computer hardware can be secured and protected through lockers, doors, confidential information, and unauthorized access to your system requires complex security strategies and practices. Below are some strategies and practices to maintain your computer security:\n- Backup your data regularly and store it in a protected place\n- Use a unique and strong password and change it regularly\n- Use a two-factor authentication\n- Use a firewall. Always activate your firewall because it acts as a security guard between your local network and the internet.\n- Keep all your installed software up to date.\n- Install reliable and trustworthy anti-virus software and malware protection\n- Be cautious of suspicious emails. Do not easily disclose personal information when asked. Always verify the reliability of all emails you receive.\n- Do not click any link provided or open any attachments, especially if it’s not familiar to you.\n- Make sure that the Wi-Fi is secure. Browse the web safely. Do not visit sites that offer prizes or promos.\n- Avoid pirated materials and cracked software. Always use and install software from a trusted source.\n- Do not leave your computer unit unattended.\n- Avoid using USB and other external devices unless you own it\n- Educate yourself with basic aspects of computer security and the latest security, as well as the evolving threats and ways to attack.\n- Always perform a daily or regular full system scan.\n- And more..\nA network system is critical in terms of security mainly because security threats are constantly evolving, and so as the security programs to defend the system against any threats and attacks continuously.\nThere are numerous threats and attacks that only wait for the opportunity to enter your system. However, understanding them and knowing how to avoid these threats is a key to preventing or defending your network and system from potential malicious activities.\nStrengthening your computer security system is always the best solution to avoid any harmful effects on your software, hardware, and network.\nJust visit www.thepractical.co.th for more information or any queries and concerns."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:ff7a58c9-2ab7-4279-ae73-c225e59065d1>","<urn:uuid:8cd50449-54a2-4538-995d-3016a79a4b22>"],"error":null}
{"question":"What is the main process powering both our Sun and stars in general, and how does it generate energy?","answer":"Both our Sun and stars generate energy through stellar nucleosynthesis, which is the process of nuclear fusion where atomic nuclei combine to form heavier elements. In the Sun specifically, this occurs when hydrogen atoms combine to form helium in its core, releasing thermal energy in the form of heat. The energy comes from breaking and forming energy bonds between the subatomic particles (neutrons and protons) in atomic nuclei. This nuclear fusion reaction releases a large amount of energy that transforms matter to a plasma state.","context":["The atomic nucleus is the small central part of the atom, with positive electric charge and in which most of the mass of the atom is concentrated. It was discovered by Ernest Ruthenford in 1911. After the discovery of the neutron, in 1932, the atomic nucleus model was quickly developed by Dmitri Ivanenko and Werner Heisenberg.\nThe main subatomic particles of the nuclei of atoms are protons and neutrons or nucleons (except that of ordinary or own hydrogen, which contains only one proton). The same chemical element is characterized by the number of protons in the nucleus that…\nThe basis of everything related to nuclear energy lies in the atom, since nuclear technology is based on the use of the internal energy contained in atoms. For this reason, to understand how nuclear reactions occur (nuclear fission or nuclear fusion) it is useful to understand how an atom is structured.\nAn atom is the smallest constituent unit of ordinary matter that has the properties of a chemical element.\nThe atom is composed of a nucleus and one or more electrons linked to the nucleus. The nucleus is made of one or more protons and, typically, a similar number of…\nThe atom is a structure in which matter is organized in the physical world or in nature. The atoms form the molecules, while the atoms in turn are formed by subatomic constituents such as protons (with positive charge), neutrons (without charge) and electrons. (with negative charge).\nIn a graphical way, what is an atom? Let's imagine that we have a piece of iron. We split it. We still have two pieces of iron but smaller ones. We will start them again, again ... Each time we will have more smaller pieces until a moment will come, in which if we go back to…\nDefinition of proton\nA proton is a subatomic particle with positive electric charge that is inside the atomic nucleus of atoms. The number of protons in the atomic nucleus determines the atomic number of an element, as indicated in the periodic table of the elements.\nThe proton has charge +1 (or, alternatively, 1.602 x 10 -19 coulombs), exactly the opposite of the charge -1 that contains the electron. In mass, however, there is no competition - the mass of the proton is approximately 1,836 times greater than that of an electron.\nIn physics and chemistry, the atomic theory is a scientific theory of the nature of matter, which states that matter is composed of units called atoms. Atomic theory began as a philosophical concept in ancient Greece and entered the mainstream of the nineteenth century when discoveries in the field of chemistry showed that matter really behaves as if it were an atom.\nThe word atom originates in the atomic adjective of the ancient Greek, which means \"indivisible\". As explained in the history of nuclear energy. Nineteenth-century chemists began to use the term in relation to the growing…\nNuclear fission is the physical-chemical reaction through which the nucleus of an atom is split. In the main interest of the fission reactions is that by this operation a large amount of energy is obtained. Nuclear energy is the energy contained in the nucleus of an atom and the energy that is obtained is thermal energy, energy in the form of heat.\nThe other form of exploitation is through nuclear fusion reactions. In this case, the process is inverse, fusing two different nuclei forming a single atomic nucleus.\nAfter the fission of the atomic nucleus we obtain…\nAtomic energy comes from the atom, it is the energy that holds together neutrons and protons of the nuclei of atoms. It is also known how nuclear energy, which comes from the nucleus. The name nuclear energy is used because most of the energy of an atom resides in its nucleus.\nTwo parts of the atom, the nucleus and the crust are differentiated. In the cortex, an indeterminate number of electrons orbit around the nucleus. The nucleus is composed of an indeterminate number of neutrons and protons. The amount of protons in the nucleus will determine the element that the atom is treated,…\nWe define radioactivity as the spontaneous emission of particles (alpha particles, beta particles, neutrons) or radiations (range, K capture), or both at the same time, coming from the disintegration of certain nuclides that form them, due to an arrangement in its internal structure.\nRadioactive decay occurs in unstable atomic nuclei, that is, those that do not have enough binding energy to keep the nucleus together due to an excess of protons or neutrons.\nRadioactivity can be natural or artificial. In natural radioactivity, the substance already has it in the natural state. In…\nA neutron is a subatomic particle that is part of the atom (along with the proton and the electron). Neutrons and protons form the atomic nucleus. Neutrons have no net electric charge, unlike the proton that has a positive electric charge.\nThe difference in the number of neutrons in the nucleus of an atom does not imply the variation of the nature of the atom itself, but it does determine the isotope to which it is a part.\nIn nuclear energy the concept \"uranium enrichment\" refers to the alteration of the number of neutrons in the atomic nucleus in order to obtain another…\nWhat is nuclear energy? Nuclear energy is the internal energy in the atomic nucleus, that is, the central part of an atom. Atoms are the smallest particles in which a material can be divided. The nucleus of an atom is composed of two subparticles: neutrons and protons. These subparticles are held together due to energy links. At the moment in which these bonds are modified, a large amount of thermal energy is released in the form of heat.\nNuclear technology deals with the use of this internal energy for a wide variety of applications. The most well-known…\nNuclear fusion is a nuclear reaction through which two light nuclei of atoms, usually hydrogen and its isotopes (deuterium and tritium), are combined forming a heavier nucleus. This binding is usually accompanied by the emission of particles (in case of deuterium nuclei one neutron is emitted). This nuclear fusion reaction releases or absorbs a lot of energy in the form of gamma rays and kinetic energy of the emitted particles.This large amount of energy transforms matter to a plasma state.\nThe nuclear fusion reactions can emit or absorb…\nUranium is the most commonly used nuclear fuel in nuclear fission reactions. It is a natural element that can be found in nature. However, in order to be able to use uranium in a nuclear reactor it must undergo some treatment.\nTo know the peculiarities that make uranium so different from the other substances we must first consider some basic nuclear physics. Basic physical considerations of uranium\nAn atom of a nucleus and electrons surrounding this nucleus. In turn, a nucleus consists of protons and neutrons. A proton has a positive charge. A neutron has no electric charge…\nNuclear energy is the energy contained in the nucleus of atoms. Nuclear energy is used in multiple applications (see applications of nuclear energy), but the best known application is the generation of electricity.\nThe term nuclear energy is often used to refer to the electrical energy generated by nuclear power plants.\nOne of the ways of harnessing…\nTo explain the history of nuclear energy we could distinguish three major stages:\n- Physical and chemical scientific studies of the elements.\n- The development of the nuclear bomb during World War II.\n- Use of nuclear energy in the civil field.\nScientific studies cover this whole period since the first Greek philosophers began to define atoms, until the development of the first nuclear bomb. In this process, different scientists discover the presence of electrons, neutrons and protons and properties that make one atom more radioactive than another.…\nAlpha particles or alpha rays are a form of radiation with high ionizing, corpuscular energy and with a low penetration capacity due to the high cross section. Alpha particles consist of two protons and two neutrons joined by a strong force. From a chemical point of view, alpha particles can also be identified with the symbol 4 He ++. Along with the 3 He isotope, the alpha particles belong to the elion family. The beta decay is mediated by a weak force, while the alpha decay is mediated by a strong force. Definition of alpha particle\nAn alpha particle is a…\nAbility of a physical system to produce a job.\nThis definition, although it is the most widespread, can be misleading because of the vagueness of the term \"capacity\", and therefore, it is better to define the energy as what, when a work occurs, decreases in an amount equal to the work produced. Energy, then, is measured in the same units as labor. History of the study of energy\nThe notion of energy appears for the first time from the investigations of Joule and Carnot on the conversion of heat in mechanical work and thanks to the theory of Helmholtz in which it connects…","Stellar nucleosynthesis is the theory explaining the creation (synthesis) our sun produces 10% of its energy from the cno cycle. Whether ccsne or sne ia were the dominant sites of the nucleosynthesis of the p-nuclei in the sun is (2010) nucleosynthesis in thermonuclear supernovae. Nucleosynthesis: nucleosynthesis, a process that entails large-scale nuclear reactions including those in progress in the sun and other stars. Stellar nucleosynthesis refers to the synthesis of heavy element nuclei due to nuclear fusion this cycle dominates in stars more massive than the sun. Stellar nucleosynthesis creates heavier elements from hydrogen and helium the energy released during this process is what causes the sun.\nIf iron is the heaviest element made by nuclear fusion, then how do we have so many elements heavier than iron on earth ----- here's a rather cool. Nucleosynthesis, solar system nucleosynthesis, lecture notes - geochemistry such as c and n than the sun explosive nucleosynthesis the r-process. Stellar nucleosynthesis this would be the end of the story, except main sequence stars like our sun burn out and become cold white dwarves. Origin of elements •the big bang: h, d, 3,4he, li •all other nuclei were synthesized in stars •stellar nucleosynthesis ⇔ 3 key processes. Ep 107: nucleosynthesis: elements from stars the universe kind of resembled the inside of the sun that were going on during the big bang nucleosynthesis.\nStellar nucleosynthesis refers to the assembly of the natural abundances of the chemical elements by the prime energy producer in the sun is the fusion of. An introduction to the evidence for stellar nucleosynthesis george sivulka march 8, 2017 submitted as coursework for ph241 the spectral lines of the sun.\nStars with masses roughly ten times the mass of the sun die in violent explosions known as type ii supernovae element formation occurs in such massive stars both. If the star has more than about twice the sun's mass the heaviest elements of all are produced by explosive nucleosynthesis in supernova explosions. Cosmic nucleosynthesis but stellar fusion in main sequence stars like the sun haven't had enough time since the big bang to generate this much helium. Nucleosynthesis in the sun when we learned about the sun, we learned that in the core, four hydrogen atoms combine to form one helium atom.\nStars with masses roughly ten times the mass of the sun die in violent explosions known as type ii supernovae element formation occurs in such massive stars both during the pre-explosion. The origin of the elements by stuart surrey for the sun, beneath the stellar nucleosynthesis of the elements up to iron has involved exothermic nuclear fusion.\nStellar nucleosynthesis evolution and nucleosynthesis calculation of gamow peak assumes that the process is far interior of the sun. Nucleosynthesis - wikipedianucleosynthesis is the process that creates new atomic nuclei from pre-existing nucleons, of those with more than eight times the mass of. Educational interactive movies/games have you ever wondered what powers our sun and the other stars in the sky nucleosynthesis in the sun.\nStellar nucleosynthesis is the process by which the natural abundances of the chemical elements within stars vary due to nuclear fusion reactions in the. Stellar nucleosynthesis is the process by which the natural abundances of the chemical elements within stars change due to the sun itself has a core temperature. Nucleosynthesis in the sun nucleosynthesis in the sun stellar nucleosynthesis – wikipedia stellar nucleosynthesis is the process by which the natural. Have the elements evolved by stellar all theorists acknowledge that cno reactions in the sun lead to virtually no nucleosynthesis of atoms heavier than helium3. The proton-proton cycle operates in less massive and luminous stars like the sun nucleosynthesis the process by which elements are formed.\nElement synthesis and isotopes nucleosynthesis 2 hydrogen 10 20 30 40 50 60 70 80 90 the sun late sun large star as red giant. Nucleosynthesis definition nucleosynthesis is the process by which heavier the most prevalent reaction in smaller stars like our sun is the fusion of hydrogen. Stellar nucleosynthesis is the process by which the natural abundances of the chemical elements within stars change due to nuclear from the sun to nearly. The process is called nucleosynthesis are thought to be produced in stars that contain at least ten times as much matter as our sun our sun is currently. Start studying nucleosynthesis learn vocabulary, terms, and more with flashcards, games for a sun-sized star this increase in heat will cause the star to."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:289b032b-5d25-42f1-a4d0-6dfacfebdba9>","<urn:uuid:5c7baa7f-e02b-436a-bb0d-59d76cff1cdb>"],"error":null}
{"question":"What are the structural properties of peptide bonds and their role in bacterial resistance?","answer":"While peptide bonds are planar and symmetric, they typically have asymmetric alpha-carbon on either sides that interact to give CD signals sensitive to secondary structure. This structural characteristic is relevant to antimicrobial peptide development, as these molecules can be designed to target bacterial membranes while providing better potency and resistance to protease degradation compared to natural antimicrobial peptides.","context":["The visible CD spectrum of bacteriorhodpsin (bR) in purple membrane has a negative CD band at ∼600 nm and a positive band at ∼530 nm and has been variously interpreted as resulting from exciton coupling within the bR trimer, heterogeneity in protein conformation, or the presence of two distinct low-energy electronic transitions in bR. The FR70 monoclonal antibody reacts with mouse CD70, a 30-33 kDa type peptide chromophore cd transitions np II transmembrane glycoprotein and a member of the TNF superfamily. Accordingly, highly selective detection of phosphorylated tyrosine np in proteins is. In the next part, the conditions necessary for the peptide chromophore cd transitions np CD. Furthermore, it can be advantageous when reinvestigating well-annotated genomes. University Contact Info. Antimicrobial peptides are diverse group of biologically active molecules with multidimensional properties. Jonsson T, Waldburger CD, Sauer RT, (1996) Nonlinear free energy relationships in Arc repressor unfolding imply the existence of unstable, native-like folding intermediates.\nexamined how a much smaller, simpler construct might begin to achieve similar aims (see the Perspective by Thiele and peptide chromophore cd transitions np peptide chromophore cd transitions np Ulrich). . The membrane that surrounds a cell is made up of a mixture of lipid molecules and peptide chromophore cd transitions np proteins. peptide chromophore cd transitions np The protein tyrosine phosphatase CD45 is necessary for both the activation and suppression of T cells upon interaction with APCs. To perform these tasks, these proteins commonly need to bind to other molecules—collectively known as ligands—that cd are found either inside or. 2,7 The Breathing Not Properly Multinational Study 2 and the Pro-BNP Investigation of Dyspnea in the Emergency Department 7 showed that NP levels were more accurate for diagnosis or cd exclusion of ADHF than clinical judgment, particularly in the context of. Although the peptide bond is planar and hence symmetric, there is usually an asymmetric alpha-carbon on either side, hence the peptide bond transitions interact to give a CD signal which is very sensitive to secondary. Stupp; “The Contrasting Behavior of Chemically Ordered Versus Chemically Disordered Liquid-Crystal np Polymers.\nNatriuretic peptides as an peptide chromophore cd transitions np emerging target for. Histone deacetylases act via the formation of large multiprotein. &0183;&32;Cell surface proteins are major targets of biomedical research due to their utility as cellular markers and their extracellular accessibility for pharmacological intervention. Transition moments in chromophores that are regularly orientated produce positive and negative LD signals depending on their orientation. 35–39 In the manuscript, cd expressions peptide chromophore cd transitions np for dipolar and rotational strengths generalized for arbitrary wavelengths are derived and applied within the transition dipole coupling (TDC) model.\nThe intrinsic CD of the retPSB chromophore was calculated using time-dependent density functional theory (TDDFT) and, for comparison, the semiempirical ZINDO method. Importantly, CD-NP also has antiproliferative actions in cultured human cardiac fibroblasts and stimulates cGMP production in these same cells to a greater extent than equimolar concentrations of BNP. , ; Massanella and Richman, ) for all 25 participants (). Although natriuretic peptides were originally identified as modifiers of blood pressure, they also exert metabolic effects, and obese individuals have decreased circulating natriuretic cd peptide concentrations. All three peptides show peptide chromophore cd transitions np a large negative signal at peptide chromophore cd transitions np ~200 nm arising from the π-π* transition of the β-sheet peptide backbone. 05% of the total phosphorylation. CDs) peptide chromophore cd transitions np •Protection of chromophore depends on isomer conformation J.\nFar UV scan,nm, is dependent on peptide bond alignment/positioning. Responsible for the deacetylation of peptide chromophore cd transitions np lysine residues on the N-terminal part peptide chromophore cd transitions np of the peptide chromophore cd transitions np core histones (H2A, H2B, H3 and H4) (PubMed:. Specifically, transitions they designed an artificial peptide with a photosensitive group at one end and embedded it in a phospholipid.\nHowever, this phosphorylation is usually accompanied by more abundant phosphorylation of serine and threonine residues in the proteins and covers only 0. Watson BS, Hazlett TL, Eccleston JF, Davis C, Jameson DM, Johnson AE. within a hard corona, on the NP-induced structural transitions, the determinants of binding and the dynamic exchange processes at the nano-bio interface.\nAt these wavelengths the chromophores are the aromatic amino acids and disulfide bonds, and the CD signals they produce are sensitive to the overall tertiary structure of the protein. Proteins embedded in peptide chromophore cd transitions np cell membranes perform peptide chromophore cd transitions np a wide variety of signaling and transport functions through conformational shifts. Vor 1 Tag peptide chromophore cd transitions np &0183;&32;Structural Studies of Peptides Interacting with Bacterial Cells. &0183;&32;Hollow vesicles formed from block copolymers, so-called polymersomes, have been extensively studied in the last decade for their various applications in drug delivery, in diagnostics and as nanoreactors. In peptide chromophore cd transitions np the far UVnm) the CD of a protein is primarily that of the amide chromophores along the backbone which result from bonding between the component amino acids. .\nCD70 is a ligand for CD27 and their interaction promotes T and B cell cross-stimulation and co-stimulation of B cell proliferation and immunoglobulin production. In the near-UV region, in which the peptide chromophore is known not to absorb, both ψCS-NH 2 and ψCS-NH 9 display two well-separated Cotton effects at about 3 nm. Structural changes of peptides may be investigated by different techniques, such as Circular Dichroism (CD) that affords information on the secondary structure np of peptides, or NMR that gives. First-order perturbation theory was used to calculate the effects of coupling of the retPSB transitions with the ππ* transitions of the aromatic chromophores and the ππ* and nπ* transitions of the peptide groups in rhodopsin. sought to determine whether these metabolic effects were exerted by signaling in skeletal muscle or adipose tissue. SPring-8 is being np widely opened for domestic and foreign researchers to strengthen advanced research in peptide chromophore cd transitions np fundamental science and industrial applications and so forth.\nFrom these data, we calculated the mean of IUPM. 197333 ; The Bcl-2 Inhibitor VI, ABT-737, also referenced under CAS, controls the biological activity of Bcl-2. The HIV-1 reservoir in np resting CD4+ T cells was measured using quantitative viral outgrowth assays (QVOAs) (Lee et al. The far- (250–195 nm) and near-UV transitions (400–250 nm) CD spectra of the three peptides, recorded in 2,2,2-trifluoroethanol (TFE) solution, are illustrated in Figure 3. Deconvolution of CD Spectra It is well known that the CD spectrum of a peptide is a function only of the relative orientation of the chromophores responsible for the cd chirooptical properties; CD spectroscopy, therefore, has been widely used for the analysis of np the secondary structure of polypeptides and proteins in solution, due to its sensitivity in detecting the presence and incidence of a. The CD spectra from the Waltz peptides are more complicated. coli cells, some. Ampliﬁed Photoresponse of a p-Phenylazobenzene Derivative of an Elastin-like Polymer by alpha-Cyclodextrin: The Ampliﬁded delta Tt Mechanism.\nThe aggregate has been considered important in the disease pathology and has been studied extensively. Circular dichroism (CD) measures differences in the absorption of circularly polarized light peptide chromophore cd transitions np arising from structural asymmetry. We spectroscopically monitored a nucleation process in the cd self-assembly of p-conjugated molecules into helical supramolecular fibrillar structures. T cells are activated by contact with antigens on the surface of antigen-presenting cells (APCs). , ; Ho et al.\nIn comparison with the intense absorption at 675 cd nm of the monomeric PF solution in DMSO, the absorption of PFI exhibited no redshift (see ). Using a computational approach, Courtney et al. Alternate: uv CD - helix correlate peptide chromophore cd transitions np to negative intensity at 222 nm, CD spectra in far-UV dominated by helical contribution Problem - limited to one factor, -interference by chromophores. Ellipticity data in the far UV is de-convoluted to estimate secondary structures such as alpha-helix, beta-sheet, or random coil. This small molecule/inhibitor is primarily used cd for Activators/Inducers applications. it may help reveal peptide chromophore cd transitions np the transition from compensated hypertrophy to early decompensation in patients with.\nProtocols have been achieved that permit facile introduction of s-tetrazine into unprotected peptides and the protein, thioredoxin, between two cysteine sulfhydryl groups np (i. NDI chromophore is characterised by easily tuneable emission wavelengths 23, and therefore adaptable for the design of FRET pairs with other dyes along the peptide backbone. Cd, le Ni, le Pd et le Hg. Attachment peptide chromophore cd transitions np of UV chromophores allows efficient photoactivation of not only np the precursor ions but also any fragments that retain the chromophore functionality. &0183;&32;DeGrado, WF and Lear, JD (1985) Induction of peptide conformation cd at apolar/water interfaces: a study with model peptides of defined hydrophobic periodicity. ” Macromolecules 21,. Brain natriuretic peptide and NT-proBNP levels are increased in HF, and correlate well with ventricular wall stress and severity of peptide chromophore cd transitions np HF.\nSignals in the region fromnm are attributable to phenylalanine residues, signals fromnm are peptide chromophore cd transitions np attributable to tyrosine, and peptide chromophore cd transitions np those fromnm are attributable to tryptophan. The possibility peptide chromophore cd transitions np to perturb the aggregation behavior of S by use of NPs has attracted considerable interest 39,40. transitions peptide chromophore cd transitions np Mice with an peptide chromophore cd transitions np adipose tissue–specific deficiency in the. Here, we applied an advanced proteogenomics approach, peptide chromophore cd transitions np combining standard proteogenomics with peptide de novo. Longitudinal IUPM measurements were obtained for 10 participants (Supplementary file 1). Nonribosomal peptide synthetases (NRPSs) and polyketide synthetases (PKSs) play a pivotal role peptide chromophore cd transitions np in the production of bioactive natural products, such as antibiotics and cytotoxins. minor allele at NPPA rs5068 was reported to lower cardiometabolic risk, 64 thus supporting the protective role of higher NP levels.\nThe presence of unusual amino acids and structural motifs in AMPs confers unique structural properties to the peptide that. peptide chromophore cd transitions np Here, we report on an ABC transporter that forms a vital. , staple), followed by photochemical release (i. The S,S-tetrazine macrocycles in turn provide a.\nTransition T< T t Soluble Unimers T t< T Insoluble Aggregate. 1 H NMR spectrum (500 MHz, CD peptide chromophore cd transitions np 3 OD) of polymer NP synthesized by the copolymerization of 5 mol% AAc, 40 mol% TBAm, 53 mol% NIPAm, and 2 mol% BIS (values are in monomer feed ratio). Histone deacetylation gives a tag for epigenetic repression and plays an important role in transcriptional regulation, cell cycle progression and developmental events (PubMed:. In certain cases, these reticulated phylogenies have resulted in phenotypic and molecular overlap that challenges the construction of practical approaches for species identification in the clinical microbiology.\n-> Super glint transitions 244480 zippyshare\n-> Musica do comercial lentes transitions","Multidrug resistance of infectious bacterial pathogens has emerged as one of the greatest human health problems. As such, there is a great impetus to develop new generation antibiotics. Cationic amphipathic antimicrobial peptides (AMPs) are nature's ancient antibiotics. They possess a broad spectrum of antimicrobial activity. However, AMP based antibiotics have short lifetime due to protease degradation. Peptide mimics, the so-called peptidomimetics, were recently conceptualized and explored in hope of identifying antibiotic substitutes with superior antimicrobial properties. We are at the nexus of this effort. In particular, we have developed a new class of peptidomimetics referred to as ?-AApeptides, which are named after the N-acylated- N-aminoethyl amino acid unit. ?-AApeptides outperform natural AMPs by providing better potency, and excellent resistance to protease degradation. They are also not prone to eliciting bacterial resistance. Another advantage of ?-AApeptides is their easy accommodation of versatile functional groups. This enables us to design and synthesize various types of ?-AApeptides. So far we have developed a few subclasses of ?- AApeptides through cyclization and lipidation. It was found that ?-AApeptides primarily target bacterial membranes. To translate ?-AApeptides into clinically efficient and safe antibiotic substitutes, a detailed mechanistic understanding of ?-AApeptides interacting with bacterial membranes becomes a prerequisite. For this proposal our first specific aim is to determine interactions between ?-AApeptides and model membranes mimicking bacterial cytoplasmic and outer membranes. Our biophysical studies will elucidate (1) bacterial membrane properties (i.e., structure and nanomechanics) with different lipid compositions, (2) bacterial lipid effect in dictating ?-AApeptide activity, and (3) structural and compositional characteristics of ?-AApeptides that govern their antimicrobial activity. Our second specific aim is to study morphology and elasticity of live bacterial cells modulated by ?-AApeptides. The whole-cell studies will unveil (1) multiple stages employed by ?- AApeptides in killing bacteria, ranging from cell envelope alteration to complete lysis, and (2) dynamic impact of ?-AApeptides on mechanical integrity of live bacterial cells. Bacterial membrane interactions emerging from molecular and cellular level studies will be integrated into our third specific aim, which is to develop potent ?- AApeptides specifically targeting Gram-positive methicillin-resistant S. aureus (MRSA) and Gram-negative P. aeruginosa with minimum inhibitory concentrations meeting clinical standards. The innovation of the proposed project is to establish a platform maximizing the expertise of biophysicists and biochemists. We believe a detailed mechanistic understanding discerned from our biophysical studies will substantially benefit structure and function optimization in generating novel ?-AApeptides. The results from the proposed project will have a strong impact on rational design of novel peptidomimetics with larger antimicrobial efficacy and less propensity of eliciting bacterial resistance.\nMultidrug resistance of infectious bacterial pathogens has emerged as one of the greatest human health prob- lems. We propose to study molecular and cellular level interactions between a novel class of peptidomimetics referred to as ?-AApeptides and bacterial membranes. Our ultimate goal is to translate laboratory research findings into the development of new generation peptidomimetic based antibiotics with superb potency and di- minished bacterial resistance.\n|Ho, Chian Sing; Khadka, Nawal K; She, Fengyu et al. (2016) Polyglutamine aggregates impair lipid membrane integrity and enhance lipid membrane rigidity. Biochim Biophys Acta 1858:661-70|\n|Pan, Jianjun; Khadka, Nawal K (2016) Kinetic Defects Induced by Melittin in Model Lipid Membranes: A Solution Atomic Force Microscopy Study. J Phys Chem B 120:4625-34|\n|Ho, Chian Sing; Khadka, Nawal K; She, Fengyu et al. (2016) Influenza M2 Transmembrane Domain Senses Membrane Heterogeneity and Enhances Membrane Curvature. Langmuir 32:6730-8|"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:89b971d7-6041-438a-ab88-7867884ab0e9>","<urn:uuid:3190dc3f-fa9f-482e-8f4f-84490146669f>"],"error":null}
{"question":"What were the 12 labors of Herakles depicted in the Temple of Zeus metopes?","answer":"The 12 labors of Herakles were depicted in metopes on the pronaos and opisthodomos of the temple. These included: 1) Killing the Nemean Lion, 2) Battling the Lernean Hydra, 3) Bringing the Stymphalian Birds to Athena, 4) Capturing the Cretan Bull, 5) Taming the Keryneian Hind, 6) Killing Amazon Queen Hippolyte, 7) Capturing the Erymanthian Boar, 8) Handling the Mares of Diomedes, 9) Killing Geryon, 10) Retrieving the Golden Apples of the Hesperides, 11) Capturing Kerberos, and 12) Cleaning the Augean Stables.","context":["Temple of Zeus at Olympia\nSite plan of Olympia with the Temples of Zeus and Hera emphasized in black.\nGoogle Earth View of Olympia\nTemple of Hera II at Paestum, c. 450. The Temple of Zeus is now destroyed, but the Temple of Hera II at Paestum gives us a sense of the proportions of the Olympia temple. The Temple of Zeus was designed by Libon of Elis. Archaeologists have demonstrated that he employed a scheme of proportions. The height columns is 32' (Peloponnesian foot= 32.6 cm=12.75\"). The interaxial measurements of the columns is 16'. The distance between the centers of triglyphs is 8'. The width of the abacus of the Doric capital is also 8'. It is evident that Libon based the proportions on a geometric progression: 1, 2, 4, 8, 16, 32.\nPlan and Sculptural Program of the Temple of Zeus at Olympia: the temple was begun around 470. The construction was funded by the spoils of a local war between Elis and Pisa, whose defeat and destruction occurred about 470 B.C. The temple was apparently completed by 457 when a golden tripod was dedicated by the Spartans on the peak of the gable after their defeat of Athens in the battle of Tanagra in that year.\nEast Pediment Reconstruction: Oath of Pelops and Oinomaos before their Chariot Race. The artists represent the moment of krisis (relate to our word \"crisis\") which in Greek tragedy is that moment of suspense before the outcome has been resolved. Relate the tension here to what we saw last week in the vase paintings of Exekias.\nEast Pediment: Pelops on left, Zeus in center, and Oinomaos on right.\nMost recent reconstruction of the east pediment. Note the reversal of the Oinomaos and Pelops figures.\nEast Pediment: seated boy.\nEast Pediment: Seer\nEast Pediment: River God Cladeos.\nWest Pediment: Battle of Lapiths and Centaurs.\nDetail of Apollo in the center of the west pediment.\nDetail of Lapith and Centaur.\nDetail of Lapith and Centaur.\nMetopes: the Labors of Herakles\nReconstruction of the 12 metopes showing the labors of Herakles from the pronaos and the opisthodomos. Metopes from the West end: top row left to right: 1)Herakles Kills the Nemean Lion;2) Herakles Battles the Lernean Hydra; 3) Herakles brings the Stymphalian Birds to Athena; 4) Herakles Captures the Cretan Bull; 5) Herakles Tames the Keryneian Hind; 6) Herakles Kills the Amazon Queen Hippolyte; Metopes from the East end: Bottom row left to right: 7) Herakles with the Erymanthian Boar; 8) Herakles with one of the Mares of Diomedes; 9) Herakles Kills Geryon; 10) Herakles and the Golden Apples of the Hesperides; 11) Herakles and Kerberos; 12) Herakles Cleans the Augean Stables.\nHerakles Cleaning the Augean Stables\n(The Augean Stables in Elis housed huge herds of cattle, whose dung had not been cleared for thirty years. Herakles wagered Eurystheus, king of Tiryns, that he could clean the stables in a single day. He made two holes in the stable walls, through which he diverted the River Alpheios.\nHerakles and the Cretan Bull\nHerakles aided by Athena holding up the heavens while Atlas fetches the golden apples.\nReconstruction of the entablature with the metope of Herakles fetching the Golden Apples/\nThe Chryselephantine Statue of Zeus by Phidias\nThe sculptural programme of the Temple of Zeus was completed by the addition of the monumental statue of the enthroned Zeus holding a Nike figure made by Phidias. The statue like the apparently earlier Athena Parthenos in the Parthenon also made by Phidias was a chryselephantine statue made of gold and ivory.\nA coin from Ellis showing the statue of Zeus.\nCross section of the Temple of Zeus."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:7a94f5c6-d1e4-4546-a975-c0644742a6a3>"],"error":null}
{"question":"What are the key requirements for optimal face mask ventilation, and what special considerations should be taken for obese patients with obstructive sleep apnea?","answer":"The key requirements for optimal face mask ventilation are proper mask seal and an open airway, achieved through a properly sized mask covering from nose bridge to chin, two-handed technique to secure the mask, head elevated sniffing position, and airway adjuncts. For obese patients with obstructive sleep apnea, special attention must be paid to their airway due to increased risk of respiratory difficulties, particularly with increased neck circumference. These patients require careful monitoring of respiratory status and are at higher risk for respiratory depression, especially when heavily sedated.","context":["The goal of airway management: Ensure adequate oxygenation and ventilation of the patient.\nJust the Facts\n- Airway management can be addressed with a number of tools and techniques. Not all situations require advanced airways and management is not always a linear process. Choose the right tool for the right job and apply the necessary amount of aggressive intervention to address the problem at hand.\n- Optimal airway management is a team process. Use shared mental modeling and teamwork to ensure best practices and optimal outcomes.\n- Oxygen is a drug used to treat hypoxia. It is indicated when signs and symptoms of hypoxia are present or in undifferentiated conditions where hypoxia is likely. In general, oxygen should be administered to maintain oxygen saturation at 94-95% (≥90% in COPD patients). Follow local protocol.\n- Adequate ventilation assures the removal of sufficient amounts of carbon dioxide. Capnography provides an excellent assessment of this capability, but also remember altered mental status, diminished lung sounds and inability to speak can all point to inadequate ventilation.\nProper head position in airway management – The Head Elevated Sniffing Position\nOptimal positioning of a patient for airway management is achieved by flexing the cervical spine and extending the atlanto-occipital joint (C1 and C-2).\nProper position is evidenced by:\n- Ear aligned with suprasternal notch\n- Face parallel to the ceiling\nQuick Tips (Patient Positioning)\nThe head elevated sniffing position is not always effective (particularly in bariatric patients). If it is not effective, consider the “ramp position.”\n- Raise the patient’s torso to a 45-degree angle\n- Provide a plateau to the patient’s head\nSuccess here will is also measured by ensuring that the patient’s ear is aligned with the suprasternal notch and the patient’s face is parallel to the ceiling.\nAn electric suction device should be able to manufacture 300 mmHg of suction pressure when occluded and 30 Lpm of suction while flowing.\nFace Mask Ventilation\nIndications for Face Mask Ventilation\nShort term ventilator control of an apneic patient or enhancement of tidal volume in a breathing patient in respiratory failure.\nSide Effects of Face Mask Ventilation:\n- Gastric distention\n- Excessive tidal volume and increased intrathoracic pressure\nFace Mask Ventilation Performance Points\nThe key requirements for optimal face mask ventilation are mask seal and an open airway. Best practices to achieve both requirements include:\n- A properly sized mask (covering from the bridge of the nose to the cleft of the chin and wide enough to cover the entire mouth).\n- Two hands used to secure the mask to the face (consider the “thenar eminence” technique for two-handed bag mask ventilation.\n- The head elevated sniffing position\n- An airway adjunct (OPA or NPA or both)\nQuick Tips (Face Mask Ventilation)\n- If face mask ventilation fails, always focus troubleshooting steps to ensuring mask seal and an open airway. Common problems include large beards interfering with mask seal and large body habitus requiring different patient positioning to achieve an open airway (ramp position).\n- Consider elevating the head of the stretcher to at least 30 degrees for all patients being ventilated. This elevation reduces the decrease in functional residual capacity in the lungs caused by a pure supine position.\n- When using face mask ventilation to pre-oxygenate prior to intubation or when attempting to ventilate a severely hypoxic patient, consider applying a nasal cannula flowing at 15 Lpm to aid in oxygenation and denitrogenation.\nJust the Facts – Excessive ventilation is harmful.\n- Ventilate only to observable chest rise and if using a manometer, when possible, keep ventilation inspiratory pressure to no more than 25 mmHg.\n- Hyperventilation results in hypocapnia and cerebral vasoconstriction. When possible, use external feedback to assure appropriate rates of ventilation. Use capnography to aid ventilation (beware of this technology in poorly perfused patients)\n2017 American Heart Association recommended rescue breathing rates:\n- Adult – 1 breath every 5-6 seconds (10-12 breaths per minute)\n- Pediatric: 1 breath every 3-5 seconds (12-20 breaths per minute)\nThere are many different airway devices intended to open the airway but not go through the glottic opening (King, LMA, etc.). You should be familiar with the indications, specific insertion techniques, contraindications and limitations of the device your state, region or service utilizes.\nIndications for a Supraglottic Airway:\n- Temporary protection of an airway that does not require the absolute isolation of the trachea afforded by endotracheal intubation.\n- Failed intubation\nQuick Tips (Supraglottic Airways)\n- Often supraglottic airways can be inserted without the need to stop chest compressions, making them a valuable tool in cardiac arrest management. Follow local protocol.\n- In most cases, the presence of a gag reflex contraindicates the use of a supraglottic airway.\n- Use capnography to assure proper placement of any advanced airway, including supraglottic airways.\nAdvantages of Endotracheal Intubation:\n- Complete tracheal isolation\n- Impedes gastric distention during ventilation\n- Eliminates the need to maintain a mask seal\n- Offers direct route for suctioning\nDisadvantages of Endotracheal Intubation:\n- Requires specialized training and expertise\n- High-risk skill (deadly if performed incorrectly)\n- Specialized additional equipment (laryngoscope, etc.)\n- Direct visualization of vocal cords can be challenging\n- Bypasses PEEP by glottic opening\n- Bypasses warming and filtering of air by upper airway structures\nIndications for Endotracheal Intubation:\n- Long-term airway isolation and protection\n- Unsecured airway/risk of aspiration\n- Risk of airway collapse (anaphylaxis, burns, etc.)\n- Respiratory failure (clinical course would require mechanical ventilation)\nPerformance Goals for Intubation:\n- Prepare patient\n- Prepare equipment and team\n- Ensure preoxygenation/denitrogenation\n- Sedate and paralyze (if using rapid sequence intubation)\n- Visualize the trachea\n- Advance the tube\n- Confirm the tube\n- Secure the tube\nPre-Intubation Preparation – The Patient\n- Proper positioning (head elevated sniffing position, ramp position).\n- At least 30 degrees of elevation. No supine position unless absolutely necessary.\n- Administer high concentration oxygen to increase oxygen saturation to as close to 100% as possible. Consider nasal cannula at 15 Lpm, CPAP.\n- In an apneic patient, ensure at least 3 minutes of face mask ventilation paired with high concentration oxygen.\n- In a breathing patient, ensure at least eight vital capacity breaths with supplemental high concentration oxygen and/or CPAP.\n- Consider hemodynamic effects of intubation. Is there a need for fluid bolus or pressors prior to intubation?\nPre-intubation Preparation – The Team\n- Ensure proper personal protective equipment for all team members.\n- Pre-procedure huddle. Review steps of the procedure, team member roles, and failed airway procedure.\n- Check equipment (laryngoscope blade and bulb, endotracheal tube, ready bougie).\n- Ensure failed airway equipment is ready.\nEndotracheal Intubation Procedure\n- Properly prepare patient –ensure appropriate preoxygenation, denitrogenation. Ensure adequate sedation and paralysis (when appropriate).\n- Properly prepare team – everyone knows their roles and the procedure (including failed airway procedure).\n- Insert blade, identify landmarks (epiglottis, posterior notch).\n- Consider bimanual laryngoscopy to improve the view.\n- Advance bougie or endotracheal tube.\n- Pass tube to appropriate depth (visualize guide on the tube).\n- Remove laryngoscope, inflate the cuff, attempt to ventilate.\n- Confirm placement (waveform capnography, chest rise, lung sounds).\n- Secure tube using a commercial tube holding device.\n- Reassess the patient.\nQuick Tips (Endotracheal Intubation)\n- Proper preoxygenation and denitrogenation will extend safe apnea time. Do not rush. Take time to adjust and clearly identify landmarks.\n- Do not allow hypoxia through multiple failed attempts. If you are unable to identify landmarks after two attempts, consider alternative plans such as a different intubator or supraglottic airway.\n- Intubation is not about brute strength. Subtle movements of the laryngoscope result in optimal intubation conditions.\n- Waveform capnography is the gold standard for confirmation. If capnographic changes are not immediately obvious, your tube is misplaced and you must remove it.\nJust the Facts\n- There is no way to fully predict a difficult airway. Always be ready before you pick up a laryngoscope.\n- There are four ways to get air into a patient: face mask ventilation, supraglottic airway, endotracheal intubation and surgical airway. One of these methods will work.\n- One of the worst enemies of executing a well-planned failed airway plan is situational stress. Take a moment to calm yourself. Consider resilience breathing, positive self-talk, and teamwork.\nFailed Airway Procedure\nNote: Failed airway procedures vary greatly depending on the local protocol, equipment, and scopes of practice. The following is intended to be a general guide, not a comprehensive plan. Always practice your specific plan and follow local protocols.\n- Take a breath. Control your personal airway. Manage your stress.\n- Get help. Say the words failed airway. Ensure the entire team understands the situation.\n- Ask yourself, is this intubation essential? Can we manage the patient with basic maneuvers or a supraglottic airway? If so, do it.\n- If intubation is essential, ensure that reoxygenation is occurring with face mask ventilation (buy time).\n- Make your next pass your best pass.\n- Could another intubator be successful? If so try it.\n- Can you readjust the patient position or your position? Have you optimized the conditions around you?\n- Would bimanual laryngoscopy improve your view?\n- Would different equipment (such as a straight laryngoscope blade or a bougie) help?\n- Can you consider another technique such as video laryngoscopy?\n- Do not repeat unsuccessful attempts without making a significant change to the procedure.\n- Do not repeat unsuccessful attempts at intubation. Move on if it is not working.\n- If three attempts are unsuccessful and no obvious improvements toward success are forthcoming, move to a surgical airway.","Caring for the Hospitalized Obese Patient\nAs the first medical malpractice insurer to establish a patient safety department, The Doctors Company remains the leader in developing innovative tools that can help you reduce risk and keep your patients safe.\nPatients admitted to your facility deserve proper care and respect regardless of their complaint or any comorbidities. Increasingly, a common comorbidity is obesity. These patients pose exacerbated patient safety issues. The challenge is being prepared to accommodate obese patients and to care for them with dignity while keeping both patient and staff safe. Consider the following as you treat obese patients.\nIncrease Patient Satisfaction\nBeing in the hospital already puts a patient in a vulnerable position, and making comments or facial expressions that signify degrading or offensive feelings about a patient’s obesity can negatively influence care. For example, patients treated with antipathy might not be as forthcoming about medical concerns or complaints of pain.\nBe Aware of Obstructive Sleep Apnea\nKnown or undiagnosed obstructive sleep apnea (OSA), particularly with an increased neck circumference, places the obese patient at higher risk for respiratory difficulties. A claims review by The Doctors Company revealed a pattern of severe postoperative respiratory depression in obese patients with OSA. In addition, heavy sedation in these patients puts them at risk. When caring for an obese patient with OSA, pay close attention to the airway, and consider additional monitoring of his or her respiratory status.\nReduce the Risk of Pressure Ulcers\nObese patients are at an increased risk of developing pressure ulcers. Poor circulation of oxygen to fatty tissue is a factor that can make the patient more vulnerable to pressure ulcers. Because pressure ulcers are potentially a hospital-acquired condition, make sure you perform a thorough skin assessment upon admission, and document any ulcers with accurate descriptions, measurements, and pictures. Regular repositioning of an obese patient, along with a suitable bed with a pressure-reducing mattress, may help to reduce this risk.\nTake Fall Precautions\nDue to possible limited mobility, illness, medications, and/or other conditions, obese patients can be at an increased risk for falls. Conduct the appropriate fall assessment in use at your facility, making sure to note obesity as a risk factor. Clearly advise the patient to call for help when moving from a bed, chair, or bathroom or when making any other transfer. Institute other fall precautions as indicated.\nEnsure Properly Sized Equipment\nPatient care areas need equipment designed for obese patients. This includes, but is not limited to, blood pressure cuffs, bandages, gowns, extra-long needles, and adequate weight-bearing beds, toilets, chairs, and wheelchairs.\nFor example, most wall-mounted toilets are designed to support up to 350 pounds. Floor-mounted toilets typically support much more weight. There are also small supports specifically made to be placed under wall-mounted toilets that will help increase the support for heavier patients.\nIt is best to design and equip designated patient rooms to accommodate morbidly obese patients. Guidelines are available for bariatric patient rooms when upgrading rooms in your facility. Properly sized MRI, CT, and other diagnostic equipment must be available at the facility, or transfer agreements need to be in place. Safe care of the obese patient requires the appropriate equipment.\nFocus on Staff Safety\nThe obese patient presents further challenges to staff when assisting with ambulation. Always make certain that adequate staff is available to help move a patient or to assist a patient getting in or out of bed. In patient rooms, a ceiling-mounted or portable lift can help reduce the risk of injury to staff.\nSeveral states have enacted safe patient handling legislation to encourage the use of lifting devices and incorporate lift teams that result in better coordination of a patient transfer. Check hospital policies and procedures for additional requirements. Regularly review and practice proper lifting techniques to avoid pulled muscles or back injuries.\nBy Paul Nagle, ARM, CPHRM, Director of Physician Patient Safety.\nCaring for the Hospitalized Obese Patient (Video)\nOur video offers tips about caring for obese patients.\nThe guidelines suggested here are not rules, do not constitute legal advice, and do not ensure a successful outcome. The ultimate decision regarding the appropriateness of any treatment must be made by each healthcare provider in light of all circumstances prevailing in the individual situation and in accordance with the laws of the jurisdiction in which the care is rendered."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:b822b2cb-e7dd-4250-a48d-a186816ce9ab>","<urn:uuid:635925b5-3bee-417f-a133-29d249eafa9b>"],"error":null}
{"question":"What are the key steps involved in conducting a sound/light line search operation, and what are the main categories of risk analysis that should be considered in maritime environments?","answer":"A sound/light line search operation involves several key steps: Two person teams spread 50m apart along a track, with the leader at the rear. They shine torches in figure-eight patterns while moving forward. Upon the leader's two whistle blasts, they stop, count down while turning off lights, then blast whistles for 4-5 seconds and listen for responses. Additional teams follow 15-20 minutes behind. As for maritime risk analysis, there are two main categories: Ship traffic risk analysis, which estimates the likelihood of ship accidents and potential consequences like fatalities or spills, and Environmental risk assessment, which evaluates potential impacts to sensitive areas and wildlife. These analyses are typically conducted sequentially to first determine accident probability and then assess environmental impact.","context":["A sound/light line search is a quick and efficient method using whistle, voice and light to attract the attention of a responsive lost person at night and then listen or see a response from them.\nThis search is used early in a search along tracks and other lineal features during darkness or daylight (without torches) to attract a lost person.\n- Loud whistle\n- Bright Torch with spare batteries\n- Two person search team with a nominated leader\n- The team spreads along the track approx 50m apart with the leader at the rear\n- Both team members shine their hand held torches slowly in a figure of eight pattern from side to side as they move forward\n- Two short whistle blasts from the Leader for team to stop\n- Both members count down – 5, 4, 3, 2, 1 while turning off their lights\n- Both members block their ears and blast on their whistles for 4-5 seconds\n- When appropriate, both members can also then call the lost person’s name\n- Both searchers listen and look for sounds or light from the lost person\n- Leader estimates a 10-15 second controlled listening and looking period\n- One short blast from the Leader and both members resume shining their torches and move further along their route until the leader is 10-15m past where the forward person was previously and repeat the process\n- A second team follows behind the first team delayed by about 15-20 minutes.\n- If the first pass has not found the person it may have alerted the lost person by way of sound or light and the second team may then hear or see light from the lost person as they pass.\n- One short blast: From the leader and the team starts walking\n- Two short blasts: Team stops walking and prepares for whistle blast\n- Three short blasts: Help, I need assistance\n- Four short blasts: Come to me (recall)\n- One or two near continuous long blasts: Alert the lost person and then listen for a response\n- Covers a large area very quickly with a small number of searchers\n- Allows searching 24/7\n- Acts as attractor for the lost person to head towards either the light or the whistle blasts\n- The lost person can be mobile or immobile and still be heard or seen, if they have source of light; even a mobile phone can be seen from a considerable distance, provided it is pointed in the right direction\n- Can be used very early in a search when very little information is available and the likelihood of the lost person being responsive is high\n- Low technology and easy to implement with simple instructions and a small number of experienced searchers\n- The sound light line search is safer to use at night on lineal features, compared with negotiating terrain and vegetation off tracks.\n- Requires a responsive lost person\n- The wind and weather can limit the sound penetration\n- Searchers need to be experienced in searching at night, there is an increased risk of injury to searchers\n- Requires multiple teams to repeat the search over time intervals\n- Successive teams need to be separated in time so the whistle blasts/light from each team do not cause\n- confusion between them\n- Extra batteries are required for the high power torch\n- Team spacing and distance between blast cycles depends on terrain, weather conditions and background noise. It could be 50-200m. To check, do a test before you start the search to see how far you short blasts travel.\n- The larger distance between searchers the greatest coverage and the quicker the search. The shorter distance between searchers increased detectability of a possible lost person response.\n- Ensure you have overlap of the sound circles. Stop spots do not need to be precise.\n- Probability of detection in increased by good listening and watching, doing so in all directions.\n- Having no light or sound during the listening watching period focuses the senses of the searchers.\n- When calling the lost person’s name always make it positive and friendly.\n- Radios should be switched off or on a channel with no other traffic to ensure you are not disturbed during the listening period . Keep talking between searchers to a minimum.\n- Given that 25% of hiker lost persons are found within 50m of a track, 50% are found within 100m and 95% are found within 500m, it provides good initial coverage of linear features using this Track Offset probability of success very early in the search while the lost person is still likely to be responsive.\n- This technique could potentially be adapted for motorbikes, ski mobiles, horses, etc.","Home / Risk assessment / Stage 3\nThe risk analysis follows a sequential approach in estimating the risk:\nIn the following, the Guideline distinguishes between two categories of risk analysis. These are:\nA) Ship traffic risk\nB) Environmental risk assessment\nThe two categories of risk analysis are often conducted in sequence; first A to estimate the likelihood of marine accidents and loss potential (e.g. oil spill amount) and then B to assess the potential impact to the environment (e.g. sensitive and vulnerable areas, and wildlife).\nShip traffic risk analysis aims to estimate the likelihood of ship accidents in a given waterway or sea area, and the potential consequences in terms of fatality risk and/or spill risk. The analysis often uses the current ship traffic conditions, as well as likely future scenarios involving changes in traffic volume or fleet composition, changes in routes, or changes in the applied Aids to Navigation (AtoN), etc.\nIt is recommended that the risk analysis calculates likelihood of occurrence for the following types of marine accidents:\nAccident types of operational character, e.g. bunkering, fuel/cargo transfer, etc. is not part of this Guideline.\nOutput (frequency of accidents)\nIt is recommended that the likelihood of events is expressed in terms of annual frequencies, and separated according to relevant categories of ship types, ship sizes (tonnage and/or length), season, geographical areas, as found relevant.\nFurther, consequences of potential events must be modelled, e.g. taking into consideration of probability of oil spill, expected spill volumes for different categories of ships. Finally, likelihood of events and consequences should be combined in a risk metric.\nOutput (frequency of fatalities and/or oil spills)\nExample risk expressions are listed below:\nBest practice quantitative risk calculation methods incorporate the following elements:\nApplying the elements above will ensure risk assessments are more harmonized in terms of their inputs and outputs. Tools and methods which apply many of these elements are; (listed in alphabetic order);\nAISyRisk The AISyRisk model comprises novel high-resolution AIS-data-based approaches for calculating grounding and collision accident frequencies. Additionally, existing models for fire, explosions, and foundering accidents have been adapted for use with AIS data input. The model outputs in grid system; accident frequencies, spill frequencies, loss of life and oil spill frequencies. Earlier version of the methodology in AISyRisk has been applied in Barents Sea, North Sea and waters around Svalbard and Greenland. The model is developed by the Norwegian Coastal Administration and DNV GL.\nLinks:Barents Observer Article: Future Governance of Environmental Risk\nArea‐wide risk assessment model; focus on oil spills and hazardous liquid substances from shipping accidents or from other offshore activities. The methodological principles are taken from the methodology of the BRISK project (BRISK, 2012) and adjusted to the specific conditions of the North Sea area. The model has been applied in the Greater North Sea and its wider approaches, as part of the co-financed EU project BE‐AWARE.\nLinks:Be-Aware Summary Report\nIWRAP - IALA Waterway Risk Assessment Programme\nThe IWRAP MK II tool is based on a model for calculating the collision and grounding frequency on a specific route. This model involves a so-called causation probability, which is multiplied with a theoretically obtained number of grounding and collision candidates. IWRAP has been applied in many local risk assessments of specific fairways. IWRAP is limited to grounding, collision and contact accidents. IWRAP is developed by IALA and Project 44.\nLinks:IWRAP MK2 Introduction\nMarinRisk: The MarinRisk tool can be used to estimate the total expected number of accidents, oil spills and associate costs, for each ship for each area for which AIS data is available, by using a risk index. The model use a Nautical Risk Index to determine the contribution of the total risk of individual vessels. Also, when all risk numbers are aggregated for a certain area, the total risk value of that area (grid system) can be determined. The general risk modelling is based on the ideas of SAMSON (Safety Assessment Model for Shipping and Offshore on the North Sea). The model is under development by Maritime Research Institute Netherlands (MARIN).\nSAMSON - Safety Assessment Mode for Shipping and Offshore in the North Sea\nThe Model calculates the frequency, volume, location and oil type of a potential ship-source oil spill within each grid cell. The model has been applied in “Transport Canada Area Risk Assessment Methodology Development for Ship-Source Oil Spills in Canadian Waters”. The model is developed by Maritime Research Institute Netherlands (MARIN).\nLinks:Area Risk Assessment methodology for ship-source spills in Canadian waters.\nA comprehensive review of all available methods and tools are provided in OpenRisk Guideline for Regional Risk Management to Improve European Pollution Preparedness and Response at Sea.\nThe list below presents best practice models on including ARIFs. Elements and principles of these models and methods can be used in Marine Risk assessments.\nThe quantitative Risk model use a Safety and Operability Index (SOI), giving a rating to risk factors relevant to Arctic operation. Arctic elements in the SOI include: Open water season, ice coverage, ceiling, visibility, SAR, temperature, days below -20, daylight, wind speed, marine icing and wind chill index.\nThe safety and operability index reflect when work outdoors can be performed safely, and under which circumstances it must be avoided. The index is rooted in the NORSOK standard S-002 on Arctic working environment.\nArctic Shipping Risk model\nArctic Shipping Risk model is a quantitative route-based ship traffic risk model. It has been applied on two cases for Arctic shipping; cruise-ship activities in ice-infested waters and use of the NSR for transit shipping. It includes the following arctic elements:\nThis study was carried out on behalf of PAME. It identifies the risks associated with the use and carriage by ships of HFO in the Arctic. It includes the following arctic elements:\nIt includes the following arctic elements:\nPOLARIS provides a risk assessment framework to assess navigational safety in a given ice regime, using observed or historical ice conditions and concentration and a polar ship classification.IMO (2016): GUIDANCE ON METHODOLOGIES FOR ASSESSING OPERATIONAL CAPABILITIESIMO/IACS (2014): POLARIS – proposed system for determining operational limitations in ice\nThe environmental risk part of the marine risk assessment builds upon the ship risk assessment and quantified probabilities for spill of various hazardous substances. The approach is to assess the potential ecological and sometimes socio-economic consequences of such spills by considering the sensitivity of different types of environments to spill-related damage.\nThe approach includes mapping of sensitive areas and periods and a vulnerability assessment towards the different spill types. Sometimes, the approach includes spill modelling in order to account for the fate and trajectory of the spill.\nThe EPPR screening study shows that few of the environmental risk methods and approaches considered specific factors for arctic environments directly, the arctic elements are more often reflected indirectly in the data and datasets used to determine vulnerability and consequence potential.\nA short overview of the most relevant best practises for marine environmental risk assessment are given below:\nNOAA. RPS ASA, Env Research Cons., RPI, Louis Berger Group (2014) including a Spill Risk Calculator tool. Assessment intended to identify broad regions and seasons within Alaska having both high relative environmental vulnerability and high relative oil spill probabilities and spill volumes. The report also includes a Literature review of methods for vulnerability and risk assessments. The method has included ice as a separate habitat in the vulnerability assessment.\nDefence Command Denmark. DNV GL (2015). The risk assessment was intended to quantify the likelihood of marine accidents with and without pollution by spill of oil products, oil/chemical cargo products and fuels, as well as corresponding spill volumes and environmental risk. Risk calculations performed on resource groups seabirds, marine mammals, shoreline and fish.\nTransport Canada. WSP / SL Ross (2014). The methodology was developed to best represent and compare the relative risk of pollution from ship-source spills of oil products and select HNS transported in bulk in Canadian arctic waters (18 sub-sectors). The assessment includes ice as shoreline type and made some adjustment of the Human-use index to arctic conditions (incl. Coastal Population Index).\nTransport Canada. Dillon, MARIN, RPS ASA, Royal Haskonig DHV (2017). Objective of this assessment was to produce a consistent methodology be used to assess the risks posed by ship-source oil spills in Canadian Waters. In addition to determine the most vulnerable areas within Canadian Waters to a ship-source oil spill (south of 60th Parallel). The method includes stochastic oil spill modelling to calculate prob. of exposure for selected spill scenarios.\nBE-AWARE had the objective to identify the most effective risk reducing (RRM) and response measures (RM) for the five project sub-regions in the greater North-sea based upon: the risk and size of oil spills calculated in the BE-AWARE I project, modelling of the outflow of oil from those spills, an environmental and socioeconomic sensitivity analysis and a combined impact assessment. Includes a comprehensive vulnerability assessment (not Arctic specific) of sensitive features and socio-economic features.\nSome arctic considerations towards elements in the environmental risk assessment are given below:\nNone of the methods for marine risk assessment in arctic waters have applied oil spill modelling in ice, although such short-duration modelling is feasible with existing oil spill models.\nFor regional assessments that want to support more site-specific consequence maps (grid cell level), modelling of spill trajectory and fate can be performed for open-sea areas or areas with lower ice concentrations. A simplified approach can be followed by applying a look-up spill radius for each oil type and volume category (see Greenland assessment) or more detailed as Strategic modelling in homogenous areas.\nLinks:Marine Environmental Risk Assessment GreenlandBe-Aware Summary Report\nFor site-specific risk assessments, modelling of spill trajectory and fate were performed. This could be as a stochastic approach to cover variations in weather (winds/currents) within different seasons (see Canadian risk assessment south of 60th parallell) or for selected spill scenarios (most probable, worst case etc.) at risk hot-spots (see Area Risk Assessment in Canadian waters).\nLinks:Risk Assessment for Marine Spills in Canadian Waters Phase 1: Oil Spills South of 60th ParallelArea Risk Assessment methodology for ship-source spills in Canadian waters.\nThe environmental vulnerability of an area will scale the environmental consequence from a spill in terms of seriousness of impact towards different habitats, species or species groups, protected areas, socio-economic resources or human-use of an area. Most regional-scale risk assessments use some type of environmental vulnerability index, but different methods and criteria is in use with different data requirements to support the vulnerability assessment. Separate vulnerability scores are applied for species (birds, marine mammals, fish) and habitats (bottom, shoreline).\nUse of Arctic Risk Influencing Factors\nThe Alaska marine oil spill risk assessment have included sea ice as a separate habitat type in the vulnerability assessment.Assessment of Marine Oil Spill Risk and Environmental Vulnerability for the State of Alaska\nFor socio-economic features, Canada did some arctic adjustments to the Human-Use Resource index north of 60th, by including a coastal population index (see Risk Assessment in Canadian waters). In general, cultural features are more influenced by national prioritisations and seems harder to harmonise in an arctic context.Risk Assessment for Marine Spills in Canadian Waters. Phase 2, Part B: Spills of Oil and Select HNS Transported as Bulk North of the 60th Parallel North.\nAs the arctic ecosystem are influenced by very dynamic and seasonal processes, a high temporal resolution in data seems necessary to capture the various possible outcomes in consequence of a spill in the arctic marine environment. This is relevant in order to capture the dynamics in areas of high biological productivity like the marginal ice zone, polar front areas or polynyas, but also with regards to abundance of species and the variation in spill vulnerability related to behavioural aspects and ecological use of areas for different purposes (spawning, nursery, migration, wintering, staging, moulting, breeding, feeding etc.).\nThe spatial resolution of data needed depends on the objective of the risk assessment and may span from AMSA IIc data of areas of heightened ecological significance (AMAP, CAFF, SDWG, 2014) down to very specific datasets like the Ecological Atlas of the Bering, Chukchi, and Beaufort Seas (Audubon, 2017). The latter represents a comprehensive and well documented dataset covering both the biological environment (fish, birds, mammals, primary productivity, ice phytoplankton concentration, zooplankton, benthic biomass) and the physical environment (ocean currents, sea ice, climate) at a monthly resolution.\nA set of quality indicators of Arctic environmental datasets for the purpose of risk assessment are given below:\nThe endpoint in the environmental risk assessments combines the input from the spill risk with the environmental vulnerability and is typically expressed with relative consequence and risk categories.\nSome relevant risk expressions are outlined below:\nRisk = Oil modified Environmental Vulnerability x Incident rate x Spill VolumeAssessment of Marine Oil Spill Risk and Environmental Vulnerability for the State of Alaska\nEnvironmental Risk index (ERI) = Frequency x Quantity x Env. Sensitivity Index pr. sub-sectorRisk Assessment for Marine Spills in Canadian Waters. Phase 2, Part B: Spills of Oil and Select HNS Transported as Bulk North of the 60th Parallel North.\nRisk Score = Frequency of exposure (FOE) x Consequence of exposure (COE)Area Risk Assessment methodology for ship-source spills in Canadian waters.\nEnvironmental Risk = Probability for 6 environemntal consequence categoriesMarine Environmental Risk Assessment Greenland\nRisk of damage = Vulnerability x Average impact of oil (water column, sea surface)Be-Aware Summary Report\nIt seems beneficial to harmonize the consequence and risk categories to enable more comparable assessments.\nArctic Risk Influencing Factors\nIce may affect hull structure, stability characteristics, machinery systems, navigation, the outdoor working environment, maintenance and emergency, preparedness tasks and malfunction of safety equipment and systems.\nTopside icing, with potential reduction of stability and equipment functionality.\nLow temperature may affect the working environment and human performance, maintenance and emergency preparedness tasks, material properties and equipment efficiency, survival time and eperformance of safaety equipment and systems.\nExtended periods of darkness or daylight\nExtended periods of darkness or daylight as it may affect navigation and human performance.\nHigh latitude, as it affects navigation systems, communication systems and the quality of ice imagery information.\nRemoteness and possible lack of accurate and complete hydrographic data and information, reduced availability of navigational aids and seamarks with increased potential for groundings compounded by remoteness, limited , readily deployable SAR facilities, delays in emergency response and limited, communications capability, with the potential to affect incident response.\nLack of crew experience\nPotential lack of ship crew experience in polar operations, with potential for human error\nLack of emergency equipment\nPotential lack of suitable emergency response equipment, with the potential for limiting the effectiveness of mitigation measures.\nSevere weather conditions\nRapidly changing and severe weather conditions, with the potential for escalation of incidents.\nThe environment with respect to sensitivity to harmful substances and other environmental impacts and its need for longer restoration."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:57dcc6e9-2f48-4c5b-b355-2295e7e2b7c6>","<urn:uuid:872a7808-e169-4e0d-9ebc-fe1f40509417>"],"error":null}
{"question":"can u tell me how writers helped preserve culture - Dobrovsky for Czechs vs Kovacic for Slovenes?","answer":"Both writers played important roles in preserving their cultures, but through different approaches. Josef Dobrovsky, along with others like F.M. Pelcl and Frantisek Tomsa, worked as grammarians, lexicographers, and historians to systematically revive and document the Czech language and cultural identity in the late 18th century. In contrast, Kovacic preserved Slovenian cultural memory through autobiographical fiction, particularly in his Newcomers trilogy, documenting the traumatic period of 1938-1948 and the effects of historical upheaval on both individual and national identity. His work was so valued for its cultural significance that when he faced deportation, a literary critic who admired his writing intervened to help him remain in Yugoslavia.","context":["Hugh LeCaine Agnew. Origins of the Czech National Renascence. Pennsylvania and London: University of Pittsburgh Press, 1993. ix + 338 pp. $49.95 (cloth), ISBN 978-0-8229-3742-5.\nReviewed by Gary B. Cohen (University of Oklahoma)\nPublished on HABSBURG (July, 1995)\nStudents of Czech history and more broadly of Central and East Central Europe should welcome new scholarship on the beginnings of the Czech national revival in the late eighteenth and early nineteenth century. Many of the more general Czech-language studies from the Communist era combined schematic Marxist-Leninist explanations of the dynamics of the national renascence with elements of older Czech nationalist hagiography and demonology. There were surprisingly few studies published in Western languages between the late 1940s and the 1980s. The oft-cited English-language essays on Czech nationalism of Jan Havranek and Joseph Zacek, for instance, date back to the late 1960s and only offer cursory overviews of the early phase of modern Czech nationalism. Even older, Hans Kohn's Pan-Slavism: Its History and Ideology (Notre Dame, 1953; New York, 1960) picks up the development of Czech nationalism only at the end of the Napoleonic Wars and can only discuss a few major Czech thinkers. French and German-language scholarship on the early phase of the Czech national revival has also been scarce since the 1950s with the notable exception of Walter Schamschula's writings. Hugh Agnew's treatment of the cultural and intellectual origins of the revival represents, then, a significant and much needed contribution to the literature.\nIn a clearly written and logically organized presentation, Professor Agnew discusses the efforts of Josef Dobrovsky, F. M. Pelcl, Frantisek Tomsa, V. M. Kramerius, and many other Czech grammarians, lexicographers, historians, and poets of their generation to revive the Czech language and encourage its use in literature, theater, journalism, and general discourse. Agnew generally follows the three-phase scheme for understanding national revivals among the smaller European peoples that has been developed by Miroslav Hroch and others. Agnew has chosen, though, to examine the first phase when the intellectual and cultural foundations were laid and before the movement began to build important social networks or engage in significant political agitation. The useful introduction briefly but soundly surveys the major interpretive positions regarding the origins of the Czech revival and the continuities and discontinuities of Czech cultural history in both the Marxist and non-Marxist historiography. Agnew's overall treatment of the early period of the national revival makes clear, though, that he has little interest in arguments that ascribe the movement's emergence essentially to the struggles of the Czech peasants, urban petty bourgeoisie, and nascent bourgeoisie to emancipate themselves from feudal exploitation and the oppression of Habsburg absolutism as those old economic and political systems waned. Rather, the author emphasizes the importance of cultural change and new intellectual trends, particularly those associated with the Enlightenment, which readily crossed state borders in Europe and transcended class lines between nobility and commoners.\nFor Agnew the beginnings of the Czech national revival represented above all a process of cultural and intellectual change and construction. Putting more nails in the coffin of old Czech nationalist myths, Agnew points to the many positive contributions to the national movement made by noblemen, Catholic clergy, and the Habsburg authorities, particularly under Joseph II. Despite the continuation of censorship and then the intensification of controls over society during the French wars, the Austrian government assisted the activity of Czech intellectuals, both indirectly and directly, through educational reforms, academic appointments, the introduction of religious toleration, and granting approval for Czech newspapers, other publications, and theatrical productions.\nPerhaps the greatest strength of this book lies in its treatment of how early Czech nationalist thinkers grappled with the definition of Czech nationhood and Czechs' relations with the other Slavic peoples of Europe and their German-speaking neighbors. Professor Agnew cogently discusses what Dobrovsky and others of his generation had to say about the place of the Czechs among the peoples of Europe and points to the centrality of the Czech language in all the awakeners' thinking about Czech culture and group identity. They conceived of Czech as a branch of the larger family of Slavic tongues, and many described the Czechs as only part of a larger Slavic nation. Still, it was clear in the late eighteenth and early nineteenth century, at least before the Romantic generation reached maturity, that Czech intellectuals' interests in the other Slavic languages and peoples were generally apolitical and were motivated by primary concerns for the Czech language and its speakers.\nDespite the emphasis in this book on cultural and intellectual processes in the early phase of modern Czech nationalism, the resulting portrait of just how the emerging nationalist culture and discourse were created and propagated is not altogether satisfying. Some problems derive from the author's desire to mention within a brief compass the efforts of the wide range of scholars and writers who were at work in the late eighteenth and early nineteenth century. Including so many results at times does little more than catalog the work of many lesser figures. Agnew's focus on the cultural and intellectual content of early nationalist thought with only a cursory treatment of institutional and political relationships and only the briefest discussion of the thinkers' social and economic situations and their audiences leaves the reader to wonder just how the emerging nationalist value system and discourse were constructed socially and politically. One discerns from Agnew's narrative that by the end of the Napoleonic Wars a community of nationalist intellectuals who worked with each other had emerged, they had an institutional structure to support them, and they were beginning to find audiences and support through patronage, associations, press, and publications. Still, we need more systematic analysis of the processes of social and political construction of the emerging Czech nationalist culture according to the most advanced methods of intellectual and cultural history.\nProfessor Agnew first conceived this study as a dissertation in the late 1970s. It would be unfair, then, to fault him for not taking more advantage here of recent post-structuralist cultural studies that analyze the emerging Czech nationalist culture and discourse, but there is also the older school of the social and political history of ideas, whose methods he might have used profitably to explain the how and why of the cultural efforts of the early Czech nationalists. One can only hope that Professor Agnew and others will continue to work at explaining the complex cultural, social, and political processes by which early Czech nationalism emerged.\nCopyright (c) 1995 by H-Net, all rights reserved. This work may be copied for non-profit educational use if proper credit is given to the author and the list. For other permission, please contact firstname.lastname@example.org.\nIf there is additional discussion of this review, you may access it through the list discussion logs at: http://h-net.msu.edu/cgi-bin/logbrowse.pl.\nGary B. Cohen. Review of Agnew, Hugh LeCaine, Origins of the Czech National Renascence.\nHABSBURG, H-Net Reviews.\nCopyright © 1995 by H-Net, all rights reserved. H-Net permits the redistribution and reprinting of this work for nonprofit, educational purposes, with full and accurate attribution to the author, web location, date of publication, originating list, and H-Net: Humanities & Social Sciences Online. For any other proposed use, contact the Reviews editorial staff at email@example.com.","Published by Archipelago Books, 2016 | 356 pages\nHalfway through Newcomers: Book One – Lojze Kovačič’s novel-cum-memoir about his adolescence in Slovenia during the years leading up to the outbreak of the Second World War – Bubi (alias Alojz Samson), our protagonist, glimpses his reflection in a jewelry store window. As he steps up to the glass, his gaze zooms in to focus on signs of hopelessness and confusion in the shadows of his eyes and nose. “All the rest,” he laments, “was some unknown brat, whoever he was, who could very easily also have been my enemy, but under no circumstances my close friend … more likely an obstacle, the way other boys I tried to avoid were obstacles to me…” Bubi leaves off from his meditation in the mirror and gets on with life, but not before concluding: “I was one of those kids I had to run from because they were constantly blocking my path …” Set in the Slovenia of Bubi’s childhood, yet told through a narrative saturated with the spiritual exile of Kovačič’s adult years, Newcomers: Book One bears stark witness to both the immediate and lasting effects of immense trauma on an individual and a country.\nBorn in 1928 to a German mother, Lojze Kovačič lived in Basel, Switzerland, until the age of ten, when the Swiss authorities relocated the family to Yugoslavia because his Slovene father, a prosperous furrier, had not applied for citizenship. The family lived in the countryside first, then in Ljubljana, where Kovačič learned Slovene, speaking with the accent that would draw anti-German sentiment throughout his life. He’d written his first stories by 1942. Charged with the attempted sale of a sewing machine that had been declared state property upon his father’s death, and threatened with deportation at a time when his mother and sister languished in a refugee camp, Kovačič managed to remain in Yugoslavia through the intercession of a literary critic who admired his writing. In his adult years Kovačič achieved renown equally for his novels and his children’s books, and he continued to write until his death in 2004. Kovačič’s creative period thus commences during World War Two, and continues throughout the second half of the century, a period which endured the Cold War, witnessed the dissolution of the Warsaw Pact and the collapse of the Soviet Union, and began drawing to a close with the 1991 advent of Slovenian independence.\nThe Newcomers trilogy originally appeared in 1984-85, at a moment when Yugoslavia’s dictator Josip Broz Tito had already died, but when the constituent parts of the crumbling Eastern Bloc country had not yet coalesced into the sovereign nation-states of Bosnia-Herzegovina, Croatia, Kosovo, Macedonia, Montenegro, Serbia, and Slovenia. The events of Newcomers – told from the first-person perspective of “a nervous, volatile boy,” the son of a Swiss German mother and a Slovenian Yugoslav father – commence in the year 1938, when the Samson family arrives in Slovenia, and conclude in 1948, when Kovačič joined the Slovene army. The story, in large part Kovačič’s own, relates the forced repatriation of a struggling petit bourgeois family of furriers, from Basel to Ljubljana, in the months leading up to the 1939 Nazi invasion of Poland. Archipelago’s edition of Book One, the initial installment of Kovačič’s three-volume work of autobiographical fiction, translated from the Slovene by Michael Biggins, is the first substantial English translation of this author’s work, a significant event considering Kovačič’s’s status as one of the most highly regarded Slovene writers of the twentieth century.\nKovačič employs two main stylistic features in Newcomers to summon the spirit of his childhood self. The first of these is a fragmentary and reflexive stream-of-consciousness narration. The reader becomes a third-party witness to the author’s dialogue with himself, in his nostalgia for the bygone wholeness of childhood, amidst the darkness and glimmer of present-day life: “He knew Greek and Latin… Knowing several languages meant you could change the world, your surroundings even more … the woods by the train tracks … could become an ancient forest with pagan gods and bulls that knew how to speak …” The fragmentation and stream of consciousness narration of the text document the formation of the narrator’s sensibility.\nThe second salient stylistic feature of Newcomers is the Low German dialect – the tongue of Bubi’s mother – in which Kovačič has written the family conversations. Translations appear in footnotes in the English text, as in the original Slovene. In its return to the language of Kovačič’s childhood, Book One transports us back to a vanished time before the Wehrmacht invaded Yugoslavia, casting Bubi’s childhood into stark relief, an effect compounded in the light of the historical fact that some four decades later, in the years when the trilogy was being written, Slovenia stood on the verge of self-determination, soon to gain a new national identity. At one point Bubi’s classmates ignorantly conflate his native German tongue with pro-Nazi sentiment and taunt him mercilessly: “‘Zurick in die Schweiz! Heilhitler! Heilhitler’ they hollered …” In its bilingualism and in the complexity of its narration, Book One possesses a rough mystique. The untranslated Low German dialect of the quoted speech, accompanied by footnoted translations, compounds the effects of the stream of consciousness narrative, interposing a distance between the reader and the text which doubles that between the protagonist and his world. As readers, we witness first hand Kovačič’s struggles with physical and spiritual exile.\nLiterary works reflect the spirit of the age in which they are written, and the text of Newcomers, the story of a boy coming into consciousness during a propagandistic time, is no exception. Book One displays its artifice everywhere, but it does so in such a way as to omit any sense of illusion or spectacle. Too mannered to pass as realism, its style simultaneously lacks the effete and awkward self-consciousness that is so prevalent in the postmodern novels of the period in which it was written. As autobiography, Newcomers is extraordinary in its eschewal of summary and narrative explication; as fiction, it is extraordinary in its forgoing of figures of speech. Kovačič builds up a gritty naturalism, directing our attention to a single criterion of truth: the bearing of witness to the cataclysms of history, and the trauma of their aftermath. For Kovačič, the incommunicable and the incomplete are the writer’s domain.\nErik Noonan is from Los Angeles. He attended Hampshire College, Utrecht University, and New College of California. He is the author of the poetry collections Stances and Haiku d’Etat, as well as numerous articles on literature, film, and art. He lives in San Francisco with his family."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"sensitive"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:355e5a6c-8eaf-4b67-a099-1a07a60c8889>","<urn:uuid:d0abc0bf-6193-452b-a657-2a61e9be45b2>"],"error":null}
{"question":"How do probiotics affect gut health, and what are the potential risks for people with compromised immune systems?","answer":"Probiotics may affect gut health by helping maintain a healthy community of microorganisms in the digestive tract, aiding in food digestion, and potentially improving symptoms like irregular bowel movements and stomach pain. However, for people with compromised immune systems, probiotics pose significant risks. These individuals may experience infections, and the World Gastroenterology Organisation specifically recommends against probiotic use for people with serious illnesses or compromised immune systems. There's also a risk of harmful effects including the production of harmful substances by probiotic microorganisms and the transfer of antibiotic resistance genes to other microorganisms in the digestive tract.","context":["Medically reviewed by Roxana Ehsani, MS\nPeople take probiotics for a variety of health concerns, especially for digestive conditions such as diarrhea, constipation, or irritable bowel syndrome (IBS). Probiotics may have an effect on the gut microbiome, which is made up of all the microorganisms that live in the digestive tract.\nWhat’s not clear is which types of probiotics are best for which conditions, what time of day to take probiotics, or whether probiotics need to be taken with food. This article will include a review of what is known about the best times to take probiotics.\nHow to Take Probiotics\nProbiotics come in several forms. You may consume probiotic-rich fermented foods, such as yogurt, kefir, kimchi, sauerkraut, kombucha, and miso. Or, you may take oral probiotic supplements in capsule or liquid forms. In addition to these are topical probiotics that are applied to the skin or the vagina.\nOne of the main issues with oral probiotics is how to keep them alive long enough to go through the acidic stomach environment and the small intestine to reach the colon, where they are thought to have the most effect.\nStarting a Probiotic\nIt’s important to consult with a healthcare provider before starting a probiotic. Probiotics are available in many different forms and strains, and it may be difficult to determine which type to try. A healthcare provider can help by recommending a specific probiotic.\nSome research has been done on what time of day probiotics should be taken and what types of food they should be taken with for the greatest effect, but a general agreement by the medical community has not been reached. However, there are some rules of thumb that can help you make choices about probiotics.\nOne study sought to determine if what you eat or drink with probiotics affects how they work. The experiment was not done on humans but instead was an in vitro (not using a live organism) study using a model of the digestive tract.\nThe strain in the study was Lactobacillus fermentum K73. Researchers found this particular strain lived longer when it was mixed with milk vs. water. This could mean that taking a probiotic with milk might be more helpful than taking it with water. But the authors couldn’t say for sure this made a significant difference in the makeup of the human gut microbiome.\nA 2017 study from Italy looked at a combination of Bifidobacterium longum and Lactobacillus rhamnosus, two types of bacteria found in the digestive system. The participants took the probiotics either 30 minutes before or 30 minutes after breakfast. Timing didn’t factor into the results, as both groups were shown to have an increase in “good” and a decrease in “bad” bacteria in their gut.\nPrebiotics are also important when taking probiotics. Prebiotics are nondigestible parts of plants that become food for the probiotics (and the existing microorganisms in the gut microbiome). Some food sources of prebiotics are nuts, legumes (beans, chickpeas, lentils, soybeans), onions, garlic, wheat products, artichokes, and chicory root.\nThe takeaway is that there's no one agreed-upon way to take a probiotic. It should be based on each individual's needs, how they structure their day, and if the probiotics are causing any side effects.\nWhen to Take Probiotics: Morning or Night?\nMost clinical trials on probiotics don’t include the time of day the supplements were taken, such as first thing in the morning or before bedtime. This could mean that researchers don't expect time of day to have much of an impact.\nThe best time to take a probiotic might be the time when it’s most convenient to take it. A probiotic that isn't taken because it is forgotten won't have any opportunity to work.\nTalk with a healthcare provider about how to take a probiotic. In addition, every probiotic will have its own instructions on how best to take it. Probiotics are made using various methods and with different ways of encapsulation, such as coatings on capsules to protect the contents from stomach acid or to allow timed release.\nThe product label should give details on when is the best time to take the probiotic, whether it should be taken with or without a meal or with milk or other foods, and if it's most effective when taken in the morning or at night.\nWhen Is the Best Time to Take Probiotics?\nThere’s not much evidence that shows the best time to take probiotics, so schedule it when it's most convenient for you to remember to take it.\nIf you experience symptoms after taking a probiotic, you can try changing the time of day you take the probiotic or taking the probiotic with a meal. If it’s recommended to take the probiotic with food, take it with the same meal each day (such as breakfast).\nCheck with a healthcare provider if it makes sense to take a probiotic at a different time of day from other medications. For example, they might recommend not taking it at the same time as an antibiotic, as that drug might kill the probiotic organisms.\nWhat might be most important is to take the probiotic consistently and to take it for the recommended amount of time.\nTips on Starting a Probiotic\nWhen starting a probiotic:\nAsk a healthcare provider for advice on which probiotic to take (such as a particular strain or brand).\nRead the label carefully to understand when and how to take it.\nTake the probiotic at the same time each day.\nAssess how you feel after you take it.\nConsider keeping a symptom diary to track progress.\nSigns Probiotics Are Working for You\nWhether a probiotic might help with symptoms will depend on a number of factors, such as the strains, formulation, dosage, and the condition being addressed. In general, one measure of whether a probiotic is working is that symptoms start to improve. However, you should keep in mind that the probiotic might not be the reason for the improvement.\nPeople take probiotics for several desired outcomes. Most of these have not been verified by research. Possible benefits you may notice include:\nMore regular bowel movements\nLess stomach pain/cramping\nClearer skin (fewer breakouts or improved eczema)\nImproved immunity (fewer colds or viral infections)\nPotential weight loss\nFewer vaginal infections\nSigns Probiotics Are Not Working for You\nThe evidence on probiotics is not clear-cut. It’s still unknown which strain (or strains) might be best for each person and for which health condition. Because probiotics usually are not covered by health insurance plans and can be expensive, consumers will want to know that what they're taking is worth the added expense.\nIf you are taking a probiotic supplement, check the label to see how long the manufacturer recommends taking it to experience benefits. See your healthcare provider if your symptoms continue or do not improve. If you have a health condition or concerning symptoms, do not take probiotics instead of seeing a healthcare provider.\nIf your healthcare provider has recommended a probiotic, ask your provider when you can expect to see results and what signs may indicate the probiotic is working. If there’s no noticeable change in that time period, ask whether you need to change the strain or discontinue taking probiotics.\nAny adverse effects that occur after starting a probiotic, such as a headache, bloating, or excess gas, are a reason to ask a healthcare provider about changing the probiotic or stopping it.\nProbiotics are largely thought to be safe. However, they are living organisms that can grow and colonize in ways that aren't intended. When using a probiotic, you may experience rare adverse events, including infections, especially if you are immunocompromised.\nFor that reason, it’s important for anyone with a health condition who wants to take probiotics to first ensure that it is safe to do so.\nWho Should Not Take Probiotics?\nProbiotics usually don’t cause any serious problems. Minor symptoms in some people include gas, as well as infections, though infections are uncommon.\nHowever, there are some people who might be at a greater risk of having a bad outcome. People who are very ill or who have a compromised immune system may not be good candidates for probiotics. The World Gastroenterology Organisation recommends against people with serious diseases or a compromised immune system taking probiotics.\nFor those who want to try probiotics, asking a healthcare provider will be the best way to get advice before exploring the option. The time of day probiotics are taken and if they should be taken with a meal will depend on the strain and on the manufacturer's recommendation.\nBenefits should be noticed within a few weeks. Probiotics are safe for most people, but those who have serious illnesses or are immunocompromised may be at a greater risk of complications.","What are they?\nProbiotics are live microorganisms that are intended to have health benefits when consumed or applied to the body. Probiotics may contain a variety of microorganisms. The most common are bacteria that belong to groups called Lactobacillus and Bifidobacterium. Other bacteria may also be used as probiotics, and so may yeasts such as Saccharomyces boulardii.\nThe U.S. Food and Drug Administration (FDA) regulate probiotics as a dietary supplement, a food ingredient, or a drug.\nWhy are they needed?\nPeople often think of bacteria and other microorganisms as harmful “germs,” however many are actually helpful. Probiotics might:\n- Help digest food, destroy disease-causing cells, or produce vitamins.\n- Help your body maintain a healthy community of microorganisms or help your body’s community of microorganisms return to a healthy condition after being disturbed\n- Produce substances that have desirable effects\n- Influence your body’s immune response.\nProbiotics have shown promise for a variety of health purposes, including prevention of antibiotic-associated diarrhea (including diarrhea caused by Clostridium difficile), prevention of necrotizing enterocolitis and sepsis in premature infants, treatment of infant colic, treatment of periodontal disease, and induction or maintenance of remission in ulcerative colitis.\nWhere do you find it naturally?\nProbiotics are added to some foods like yogurt and cultured cottage cheese. They are naturally occurring in fermented foods like buttermilk, kefir, and sauerkraut. Raw cheese made from unpasteurized milk is another source.\nProbiotics for kids\nChildren develop their microbiome in the womb and through early childhood. It’s thought that an unhealthy microbiome is responsible for many diseases.\nA study published in JAMA Pediatrics found that giving infants probiotics in the first three months of life may help prevent colic, constipation, and acid reflux.\nSome decent brands for probiotics:\n- Nature’s Way:\n- Ultimate Flora:\n- Check the label to make sure the yogurt you choose has “live and active cultures.”\nProbiotics have an extensive history of apparently safe use.\n- Since long-term probiotic effects on kids are unknown, children shouldn’t use probiotic supplements as a preventive remedy, unless recommended by a doctor.\n- The risk of harmful effects from probiotics is greater in people with severe illnesses or compromised immune systems. When probiotics are being considered for high-risk individuals, such as premature infants or seriously ill hospital patients, the potential risks of probiotics should be carefully weighed against their benefits.\n- Possible harmful effects of probiotics include infections, production of harmful substances by the probiotic microorganisms, and transfer of antibiotic resistance genes from probiotic microorganisms to other microorganisms in the digestive tract.\n- Some probiotic products have been reported to contain microorganisms other than those listed on the label. In some instances, these contaminants may pose serious health risks.\n- Kids with compromised immune systems may experience infection. Others may have gas and bloating. Probiotics can cause serious side effects in very sick infants. Check with your pediatrician before giving probiotic supplements to your child.\nSource: Internet, CDC, NIH, FDA, Healthline, Medical News & Others\nThe views expressed in this article should not be considered as a substitute for a physician’s advice. Always make sure to seek a doctor or a professional’s advice before proceeding with the home treatment plan."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:718f93cf-4839-45d9-be23-dac1b5e325ed>","<urn:uuid:18b099d3-380b-4d1e-9834-7ed4ed088b12>"],"error":null}
{"question":"What makes HDPE pipe particularly suitable for karst environments, such as those found in Cave City, and how does it address the unique challenges of water leak detection in these areas?","answer":"HDPE pipe is particularly suitable for karst environments because it creates a fully-restrained, joint-free pipe system. In karst areas, water leaks are difficult to detect since water typically disappears into sinkholes rather than surfacing. The HDPE pipe sections are heat fused together to create a monolithic pipeline that will not separate at joints during ground movement, which is common in karst topography. The fused joints significantly reduce water loss and prevent pipe separation, making it nearly impossible to pull apart - a feature that has been proven effective in numerous earthquakes worldwide.","context":["Water Loss Solved; Delicate Environment Protected\nCAVE CITY, Ky – The Caveland Environmental Authority (CEA) has found the solution to reduce its 40 percent unaccounted water loss and protect the area’s sensitive subterranean ecology by replacing aging and leaking water lines with high-density polyethylene (HDPE) pipe. Completed in May 2016, the project used more than 3.5 miles of PE 4710 pipe.\n“During the past five or six years they’ve been replacing old water lines,” explained Roger M. Boyers, P. E. of Water Management Services, LLC, the Nashville, Tennessee-based firm that designed the project. “This includes replacing PVC, some cast iron, asbestos concrete and even clay pipes. Everything they’ve been putting in now is HDPE. Our company first started working on this years ago with the Mammoth Cave National Park project, which was replacing all the water lines in the park with HDPE pipe. They were concerned with chlorinated water possibly getting into the cave structures and causing a problem down there.\nCave City operates and maintains the Mammoth Cave National Park water and sewer system.\n“When all of the water mains were replaced within the park, the National Park Service required CEA to install only HDPE pipe as this was the best choice for preventing water leaks and line breaks,” he continued. “Prior to this, the water loss was very high. Now, the park’s unaccounted water is less than 10 percent, and most of this is due to air release valve failures, not the pipe itself. HDPE pipe systems have a zero allowance for water loss per AWWA M55 Manual which is part of the design with these improvements.”\nFor a karst area such as Cave City, finding water leaks is difficult because the water usually does not come to the surface but goes down into one of the sinkholes. For this reason the Cave City system went to HDPE pipe to have a fully-restrained-pipe, and joint-free pipe system. Heat fusing the HDPE sections makes a monolithic pipeline, which minimizes water loss and means that the pipe will not separate at the joint if there is ground movement or a seismic shift.\n“With bell and spigot joined pipe systems this is not the case unless restraints are used,” Boyers explained. “Fused HDPE pipe is just about impossible to pull apart. This has been proven in numerous earthquakes around the world. Once again this is part of the design for this project. The fused HDPE pipe also significantly reduces any chance for water loss from the joints and prevents separation of the pipe due the karst nature of the topography.\n“We based our design on a working pressure of 160 psi which requires a DR 13.5 HDPE (PE 4710 with Pressure Class, PC 160) pipe. To meet the same design requirements, PVC C900 DR 14 pipe with a PC 305 would be required to handle working pressures, surge pressures and the 100 year fatigue life. While PVC pipe has a higher static hoop stress capability, our focus was on the pipe’s dynamic strength and not its static strength. In our design, we had to take into account the proposed water booster station. While we were trying to minimize the pumps turning on and off, there was the potential for water hammer and therefore, the overall long-term fatigue of the pipe had to be taken into consideration. This resulted in increasing the PVC pipe pressure class to account for the flow velocities and number of cycles from the booster station.”\nAccording to the Plastics Pipe Institute, Inc. (PPI), the major North American trade association representing all segments of the plastic pipe industry, PE 4710 is the highest performance classification of HDPE piping material for potable water applications.\n“PE 4710 compounds offer utilities and designers a greater level of performance,” stated Camille George Rubeiz, P.E., F. ASCE, senior director of engineering for PPI’s Municipal and Industrial Division. “This translates to the ability to utilize HDPE with increased flow capacities plus increased resistance to surge pressure, fatigue and slow crack growth when compared to previous PE 3608 compounds. The updated ANSI/AWWA C906-15 standard includes PE 4710 for sizes up to 65 inches and recognizes the increased durability and reliability of HDPE pressure pipe used in water systems.”\nAccording to Rubeiz many engineers select the pipe’s pressure class based on the system’s working pressure and on the pipe’s static hoop stress capability. “However, and as noted by Mr. Boyers, Water Management Services provided its client, Cave City, with solutions and accounted for the static and dynamic strength of each pipe material. To simplify these preliminary calculations, eTrenchless Group developed and maintains a web- based calculator called PPI PACE. The free program allows users to complete pressure pipe designs including water distribution lines, transmission main systems and force mains in accordance with existing AWWA and ASTM published standards.\nSince PPI PACE compares HDPE, PVC and Ductile Iron pipe, relevant standards used include AWWA C900, AWWA C901, AWWA C905, AWWA C906, C150/151 along with AWWA manuals such as M23, M55 and M41. PPI PACE also contains extensive information on the selection of design parameters and typical input valves for all three major materials used in water systems.\nRubeiz also discussed this essential and major difference in industry standards for pipe. “The AWWA defines pressure class differently for all three of the different materials. For example, the pressure class for PVC and Ductile Iron pipe is not equivalent or relevant for HDPE pipe. For this project and to meet the design requirements of 160 psi, the required flow velocities and fatigue life, PVC pipe with a DR 14 and pressure rating of 305 psi would be required, while PE 4710 pipe with a DR 13.5 and rated at 160 psi met the project design requirements. For a project like this it would be common for a PVC pipe with DR 25 (PC165) or DR 18 (PC235) to be specified to satisfy the 160 psi design pressure requirement. If this was done, the system would be under-designed by having a fatigue life that is less than 100 years. For Ductile Iron pipe, the fatigue life calculations are not available in AWWA C150/151 and M41.”\nThe Cave City project used 18,500 feet of eight and twelve inch PE 4710 pipe from PPI member company, WL Plastics, Inc. (Ft. Worth, Texas). Installation was done using open trench, cut and cover. Jack and bore was used to go under several state roads and railroad tracks. According to Boyers, any new lines will also use PE 4710 pipe\nFor additional information, please visit the PPI website: www.plasticpipe.org.\n# # #\nThe Plastics Pipe Institute Inc. (PPI) is the major North American trade association representing all segments of the plastic pipe industry and is dedicated to promoting plastics as the material of choice for pipe applications. PPI is the premier technical, engineering and industry knowledge resource publishing data for use in development and design of plastic pipe systems. Additionally, PPI collaborates with industry organizations that set standards for manufacturing practices and installation methods."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:30835dbe-a7f4-4d45-b9b0-3499352b3952>"],"error":null}
{"question":"what types of full stack devs work with security + memory mgmt?","answer":"System security developers specifically design security measures and tests for software security improvement. Regarding memory management, low-level developers are particularly involved as they work close to the hardware using languages like C and assembly, dealing with critical issues like buffer overflow, out-of-bounds write/read operations, and proper memory allocation. Additionally, embedded developers work with hardware and electronic interfaces, while SDETs create software to check the quality and security of other software systems.","context":["Full stack development is a practice, not a product\nThe key to understanding full stack development is that it’s a practice, not a product. Technically, any website or application is considered to be full stack if it’s built with all three layers—front-end, back-end and database. However, generally speaking, it’s considered full stack when there are separate teams dedicated to each layer. This allows companies to dedicate developers and engineers specifically for each part of the project. In other words: It gives developers more control over their entire project from start to finish.\nFull stack developers do more than code\nIt’s commonly misconceived that a full stack developer only codes for a living. However, it’s actually a term describing someone who is good at front-end and back-end development. It’s an umbrella term used to describe a developer who knows everything there is to know about building a functional website or web application – from database integration to user interface development. This includes not only programming but also designing and site architecture/planning/maintenance.\nWhat Is the Difference Between Backend and Frontend?\nLet’s start with a quick definition. The frontend refers to what you see when you visit a website, such as its layout, colors, graphics and text. The backend is made up of everything that powers it behind-the-scenes—the code behind it all, databases, servers and more.\nFront-end and back-end developers work together to construct the systems that allow an application or website to function properly. However, The concerns they have differ.\nFull Stack Developer Is in High Demand\nThe advantage of being a specialist in one’s field and mastering a particular aspect of technology is obvious, but as technology rapidly expands and evolves, companies are looking for developers who can work on both the front and back end to build functional products with minimum input and support.\nDifferent types of full stack developers\nFront end, back end, and full-stack development are the most well-known development categories. There are some that are less well-known, such as:\nSystem security developers design security measures and tests for software to improve system security.\nLow-level developers write code that’s close to the hardware using languages like C and assembly.\nHigh-level developers write code that’s far from the hardware in languages like Perl, PHP, Ruby, and PHP.\nEmbedded developers work with hardware, such as electronic interfaces, internet of things (IoT) devices, serial data transmission, and hardware drivers.\nSoftware development engineers in test (SDETs) create the software that checks the quality of other software systems. Their tools ensure systems are working.\nBig data developers write programs designed to store and access enormous amounts of data. These systems include extract transform load systems, data warehouses, and regional databases.\nCRM web developers focus on systems that collect user data to improve satisfaction and sales.\nDevOps developers work with technologies used in the development of systems for building and using back-end systems.\nGraphics website developers write software used to shade, cull, render, light, shadow, and manage scenes. Their work is necessary for video and gaming projects.\nGame developers write code for video games. They often have skills in other areas with a focus on creating video games.\nData scientists write code for programs that analyze data sets for things like statistical analysis and predictive modeling.\nDesktop developers work on software that runs on operating systems.\nMiddle-tier developers write non-UI code for browsers. They also can do some front-end and back-end work.\nWeb developers specialize in building websites. They can be front-end, back-end, middle-tier, or full-stack","Differences between the SANS Top 25 and OWASP Top 10\nWhile they both serve as a reference point for software security and are partly based on the same source data, the SANS/MITRE CWE Top 25 and the OWASP Top 10 differ in scope and purpose. The OWASP list groups the most prevalent web application security weaknesses into ten categories corresponding to broader cybersecurity concerns. With each subsequent edition, the categories have been moving away from specific vulnerabilities or even common vulnerability classes and towards a more strategic view – see our post on the 2021 OWASP Top 10 to learn what this means in practice.\nThe SANS/CWE Top 25 lists the most prevalent issues from the Common Weakness Enumeration (CWE). In a way, CWE takes the opposite approach to the OWASP list, focusing on specific weaknesses rather than more abstract classifications. This makes the list more directly useful for developers and security engineers, as each item relates to concrete implementation flaws that can be found and addressed. Interestingly, although the SANS/CWE Top 25 applies to all types of software while the OWASP list is limited to web applications, with each edition there is more and more common ground between web and non-web software security.\nWeaknesses vs. vulnerabilities: Both the SANS Top 25 and the OWASP Top 10 deal solely with CWEs, i.e. security weaknesses that commonly occur during software development. These are different from CVEs, which are confirmed security vulnerabilities in specific products. In simple terms, exploitable weaknesses reported in production become vulnerabilities.\nCommon themes in software security weaknesses in 2021\nThe SANS Top 25 list is based on the prevalence of specific weaknesses in real-life vulnerabilities taken from the NIST NVD. Each CWE that has led to a vulnerability gets a score that reflects its frequency and severity (see here for the actual formula), and this score determines its position on the list. A dry technical list doesn’t seem particularly useful or exciting, but if you read closely, the CWE codes, scores, and trends tell the story of modern software development and security – a tale of trust, deceit, and demons of the past, all set firmly in the cloud. Let’s look at the four common themes running through the Top 25.\nWeb application security is everywhere\nIf you came to the SANS TOP 25 CWEs from the OWASP Top 10, you’d be forgiven for having a sense of deja vu, as eight of the top 25 weaknesses are either web-specific or most commonly found in web applications. It’s no secret that as software development moves to the web, so does application security. Here are the four web-specific weaknesses on the list, along with their official names and overall positions:\n- #2: Cross-site scripting (XSS), officially Improper Neutralization of Input During Web Page Generation [CWE-79]\n- #9: Cross-site request forgery (CSRF) [CWE-352]\n- #23: XXE injection, officially Improper Restriction of XML External Entity Reference [CWE-611]\n- #24: Server-side request forgery (SSRF) [CWE-918]\nApart from these, several other weaknesses in the list usually occur in web security contexts, notably SQL injection, OS command injection, and path traversal (a.k.a. directory traversal). While these can apply to other types of software, they are easiest to exploit in web applications. Again, the position reflects the frequency and severity of vulnerabilities linked to a specific weakness, so having XSS way up at #2 means there is a lot of cross-site scripting going on.\nMemory management issues never go away\nOn the one hand, we see that all the cloudy headlines are true – software development is increasingly web development, and software security is increasingly web application security. However, the #1 weakness (along with five relatives) serves as a stark reminder that a lot of critical software relies on lower-level programming languages that need careful memory management. The top software security weakness of 2021 is essentially buffer overflow, though this specific term is considered too general for CWE. Here are the weaknesses related to low-level memory operations:\n- #1: Out-of-bounds write (code can write to memory that shouldn’t be accessible) [CWE-787]\n- #3: Out-of-bounds read (code can read memory that shouldn’t be accessible) [CWE-125]\n- #7: Use after free (code uses a variable that shouldn’t be used anymore) [CWE-416]\n- #12: Integer overflow or wraparound (mismanagement of large numeric values) [CWE-190]\n- #15: NULL pointer dereference (code attempts to access a non-existent value) [CWE-476]\n- #17: Improper restriction of operations within the bounds of a memory buffer (code can operate on memory that shouldn’t be accessible) [CWE-119]\nTrust no one with your inputs\nThe other overarching theme of this software security tale is trust. It is difficult enough to write software that works correctly with the expected data and users. When every user could be malicious and every input could be an attack attempt, writing even the simplest piece of code is like walking through a minefield. How can you do anything when everyone is suspicious? How can you check every piece of data? And yet this is the reality of application security, as shown by over a quarter of the top 25 being weaknesses related to blindly trusting your inputs:\n- #4: Improper input validation [CWE-20]\n- #5: OS command injection, officially Improper Neutralization of Special Elements used in an OS Command [CWE-78]\n- #6: SQL injection, officially Improper Neutralization of Special Elements used in an SQL Command [CWE-89]\n- #8: Path traversal/directory traversal, officially Improper Limitation of a Pathname to a Restricted Directory [CWE-22]\n- #10: Unrestricted upload of file with dangerous type [CWE-434]\n- #13: Loading saved data without checking, officially Deserialization of Untrusted Data [CWE-502]\n- #25: Code injection, officially Improper Neutralization of Special Elements used in a Command [CWE-77]\nIn all these cases, failure to sanitize user-controlled inputs can have devastating consequences, from software crashes to information exposure or code execution. And as mentioned earlier, many of these are typically found in web application security, where user-controlled inputs make up most of the data your application uses.\nTrust no one with access\nThe threat landscape is easily the biggest change across the history of software security. With threat actors now active at every stage of the application lifecycle, access control should be an integral part of software and data design – except that it’s not. All the remaining weaknesses from the Top 25 are related to implicit trust or failures to protect sensitive data at all times, showing that, all too often, security is still an afterthought during development:\n- #11: Missing authentication for critical function [CWE-306]\n- #14: Improper authentication [CWE-287]\n- #16: Use of hard-coded credentials [CWE-798]\n- #18: Missing authorization [CWE-862]\n- #19: Incorrect default permissions [CWE-276]\n- #20: Exposure of sensitive information to an unauthorized actor [CWE-200]\n- #21: Insufficiently protected credentials [CWE-522]\n- #22: Incorrect permission assignment for critical resource [CWE-732]\nThe importance of such trust-related issues is also reflected in the OWASP Top 10, where the top categories are now Broken Access Control and Cryptographic Failures. Ensuring application security means encrypting sensitive data (or all data, in many cases) at rest and in transit using secure algorithms while also thinking of authentication and authorization when designing user roles and function access.\nTo be effective, security must come first\nWith over half of the SANS Top 25 security weaknesses being related to trust and access control, it’s no coincidence that CISA is calling for organizations to implement zero trust principles in their systems. What’s more, the three fastest risers on the list since 2020 are all trust-related: Incorrect Default Permissions, Missing Authentication for Critical Function, and Deserialization of Untrusted Data. And remember that the list is based on prevalence in real-life vulnerabilities, so these weaknesses are out there and growing in frequency or severity (or both).\nThere are no shortcuts to avoiding software vulnerabilities, only hard work to build a security-first mindset and embed security into every stage of the software development lifecycle (SDLC). Vulnerability testing, mitigation, and remediation all need to be a routine part of the development workflow, built on a solid foundation of education and security awareness.\nOrganizations can no longer afford to compromise on security or accept security risks as the price of rapid development and growth. When anything can be a target and anyone can be an attacker, security must come first."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"sensitive"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:8333d2dd-59e0-4523-9d10-2e93fde8ca04>","<urn:uuid:8aeaf7b5-d75a-4b60-bbba-d0bb596a8aad>"],"error":null}
{"question":"What are the similarities and differences between survival training requirements for Antarctica's McMurdo Station and Eugene Mountain Rescue in terms of basic skills and certification needs?","answer":"Both programs require basic survival skills training, but have different specific requirements. For McMurdo Station, everyone who is field-bound must complete SnowCraft I (nicknamed 'Happy Camper'), which teaches survival skills like snow quarrying, cold weather cooking, staying warm, and building survival trenches. For Eugene Mountain Rescue, members must complete SAR 101, a 50-hour training that covers basic search and rescue techniques, radio usage, wilderness survival, and backcountry navigation. Additionally, EMR requires pre-existing skills before joining, including navigation with map and compass, proper clothing for all seasons, and the ability to tie specific climbing knots. Both programs emphasize self-awareness and safety in harsh conditions, but EMR requires ongoing training of 30 hours per year to remain eligible for missions, while McMurdo's training is a one-time requirement before going into the field.","context":["If you plan on going out of “town” around here at any point in time, it is essential that you know some very important survival skills. After all, Antarctica is a harsh continent. Luckily, for those of us that have no previous survival training, the USAP provides a course called SnowCraft I designed to fill that void. Every single field-bound person (and even those not field-bound but that want to get out and explore beyond the trails in and around the McMurdo station) that comes through McMurdo as part of the USAP program is required to go through this survival training, and there seem to be bipolar opinions regarding SnowCraft I: you either love it, or you hate it. Those of us that are on the “loved it!” side refer to this survival training as “Happy Camper,” and the affectionate name really does describe the feelings that engulf you while you’re out and about experiencing the real Antarctica – you are a VERY happy camper! In an attempt to keep this post short on the text (HA! Watch me try!) and longer on the pictures, I’m only going to brush over some of the basic survival skills that I learned… and that I still remember.\nThe main theme of Happy Camper is to mitigate the risks that you take every time you leave the safety of McMurdo station. As I said before, Antarctica is a harsh continent, and the weather can change on a dime, from calm and so beautiful that you forget you’re in a frozen world one minute, to deadly and unforgiving the next, the same kind of deadly and unforgiving storms that claimed the lives of many an intrepid and experienced polar explorer. So what does the term “risk mitigation” mean? It means finding the right balance between the consequences of a particular event and the probability of those consequences actually happening. In simple terms, it means using your available resources to reduce the likelihood of getting caught in a life-or-death situation. As an example, let’s say you’re out exploring the sea-ice when it turns to Condition 1 in Antarctica (refer to my previous post on weather conditions if you’ve forgotten what Condition 1 means): the consequence of getting caught out during Con1 is death, and the likelihood is very high(“certain”). How do we mitigate this situation? One simple way to do it is to bring extra protective gear with you; e.g. pack extra fleeces and jackets for warmth, dry clothes, a sleeping bag and blankets, a stove with extra food & snacks, a tent, tools for snow-crafting (like a snow saw, shovel), etc., on separate duffle bags and bring them with you when you go exploring. By having these items with you, you have already drastically decreased the probability of facing death (the consequence) if you were caught in Con1 weather away from station. You have mitigated the risk! These are actually all items that are included in a survival bag, which is issued to you by the Berg Field Center (or the BFC), McMurdo’s version of REI. Not familiar with REI? WHAT? Ok… click here: REI. Search for “Mountain Hardware.” Pick something cool. Put it in a box and send it addressed to me at McMurdo Station, APO AP 96599-1035… kidding, I know you are all somewhat intimately familiar with REI. You are required to take your survival bags with you anytime you leave station (except if you’re going on local hikes). Among the things that we learned at HappyCamper was how to use everything that comes in a survival bag (the items above, and some more). Other highlights of training:\n– Properly setting up a Scott Tent: which includes building snow and ice anchors for the wind ties, and building a snow bank around the tent to secure the seams so that air doesn’t get in (Scott tents don’t typically have floors, but you can add a simple tarp so you’re not sleeping directly on the snow). I learned a very clever way to anchor the wind ties down: dig a small trench parallel to the side of the tent (to ~2-3 ft. deep), then dig a smaller trench (wide enough for the string ties) toward the tent perpendicular to the trench and at an angle reaching from the bottom of the trench to the surface close to the tent, next tie a stick (or your tent steak) to the end of the wind tie, and lastly bury the stick horizontally at the bottom of the trench with the string coming up along the smaller trench. You’re essentially using the snow and ice around you as a secure anchor for your tent’s wind ties! And it works fantastically!\n– Snow quarrying: out on the Ross Ice Shelf, the ice is a few hundreds of meters thick (400-500 m), and the snow cover above that can be tens of meters thick, and because essentially all of that snow is hard-packed by the very strong Antarctic winds, it is wonderful for cutting into blocks. We flag an open space for our “snow quarry,” and using a saw we cut the snow into blocks which we can then break off by shoving a shovel underneath it. We primarily use these blocks for building snow walls to shelter our tents and cooking area from the wind, but the possibilities are endless!\n– Cold weather cooking basics: we learned how to use MSR Whisperlite stoves, running on white gas, to cook and melt snow for hot water. We also learned good ways to clean them and troubleshoot them if they stop working. Going along with cooking, we build a kitchen area by digging a trench in which we can stand so that we can keep the stoves low on the ground yet still at a comfortable working level (above your waist). A small snow wall is also a nice addition to the kitchen, and it protects us from the wind when the stoves are lit.\n– Staying Warm: little things that you can do to keep warm (both during the day and while you sleep), which include the use of internal heat sources (heat from your body, like exercising to increase circulation of your blood, specially to the extremities of your body) and external heat sources (using toe and hand warmers, and also using simple things like water bottles filled with hot water), and most importantly fueling up! Our body needs fuel to produce heat, so we ate a LOT of chocolate! 🙂\n– Survival Trenches: the most effective way to shelter yourself from bad weather in Antarctica is to dig a trench and jump in it. It is relatively easy to make a survival trench when you have a shovel and a saw, but it is quite exhausting and time-consuming. We had the option to sleep in a trench, rather than a tent, for the night, provided we were willing to build it ourselves. Well, guess what I decided to do? That’s right… and it took me well over 5 hours to do so! Here are the steps to building your own trench, in pictures:\n– Self-Awareness: Lastly, and perhaps most importantly, we learned a lot about the important of self-awareness. Though, it can be interpreted in many ways, in our situation, self awareness simply meant being attentive to how our body was doing in the Antarctic conditions, knowing the limits for what your personal body can handle, and learning to recognize symptoms associated with cold-induced injuries such as hypothermia and frost-bite (as well as frost-nip, the less severe version of frost-bite, which one of our happy campers began to develop).\nAs you can see, this was a futile attempt at keeping this post short on the text, but hey, here are some more pictures!","Published: Thursday, 06 March 2014 02:24\nEugene Mountain Rescue is a volunteer organization based in Eugene, Oregon. We formed in 1968 to provide Lane County with trained volunteer rescuers for emergencies in mountainous areas, steep terrain or backcountry areas requiring mountaineering proficiency to work safely. Our search and rescue missions involve lost and/or injured climbers, skiers, hikers, hunters or downed aircraft victims. EMR is a member of the Oregon Mountain Rescue Council and national Mountain Rescue Association.\nOur primary mission is to provide search and rescue personnel for situations requiring mountaineering skills in rock, glacier, snow and steep terrain in support of the Lane County Sheriff. Additionally we are a part of a larger network of volunteer groups in the county, the state of Oregon, and nationwide that provide search and rescue services. As part of this network, EMR volunteers may be called upon to assist other search and rescue groups in emergencies that may not require mountaineering or technical rescue skills. EMR is also called upon to assist mountain rescue organizations from other counties in Oregon, or in other states, if they need additional assistance.\nIn addition to search and rescue work, EMR volunteers work in the community to promote outdoor safety and safe mountaineering practices. We do this through community education projects and educational programs in schools.\nPublished: Monday, 05 April 2010 19:52\nCompetitive candidates will possess a combination of general backcountry knowledge and technical skills. While we incorporate basic skills such as navigating with a map and compass, wilderness survival, and tying climbing knots into our practices, we do not actively teach these skills. We assume that you possess these basic skills prior to applying for membership. Our training program is focused on technical skills related to rope rescue systems, backcountry medicine, and search and rescue operations.\nWe conduct training and missions in steep, rugged, snowy, and remote terrain regardless of the weather. You need to possess skills to take care of yourself in these environments as team member safety is our top priority.\nQualified candidates must have ALL of the following skills:\n- General backcountry knowledge: Able to navigate using map and compass, own proper clothing and equipment for travel, training, and survival in inclement weather in all seasons.\n- Good fitness levels: Capable of carrying heavy loads in backcountry situations, and adequate fitness level to participate in strenuous activity, sometimes for long periods of time. Team members are expected to perform at the \"arduous\" level consistent with the U.S. Forest Service Wildland Firefighter Pack Test. This is a timed test of a 3-mile hike on level terrain under 45 minutes. Packs must be carried during the test and will weigh 45 pounds or one-third of your body weight, whichever is less.\n- First Aid and CPR: You must have a current First Aid and CPR certification OR commit to attaining it within three months of your acceptance date. There are opportunities for classes if you are accepted as a candidate.\n- Climbing Knots:Capable of tying basic climbing knots- the Water Knot, Double Fisherman’s, Prusik Knot, Figure 8 Follow Through, Alpine Butterfly, Münter Hitch, and Clove Hitch.\nQualified candidates are also required to have basic skills in at least one technical area below:\n- Rock climbing: Capable of climbing at least low 5th class rock.\n- Snow/glacier: Able to self-arrest, use crampons and ice axe properly, and be familiar with roped glacier travel.\n- Ice climbing: Capable of climbing WI2 or AI2. Familiar with use of ice protection methods, ice tools and crampons.\n- Rope rescue technician training: Certified Technical Rope Rescue Operations Level 1 (NFPA 1670)\nOther information about joining the team:\n- EMR is part of the Lane County Sheriff's office. In order to join EMR, you must also complete a Lane County Sheriff's volunteer application. Links to two versions of the application can be found here:\n- All new team members are required to attend Search and Rescue (SAR) 101, a class offered through the Lane County Sheriff's Office. It covers basic search and rescue techniques, using radios, wilderness survival, backcountry navigation, and other essential skills. SAR 101 is required to become a certified Search and Rescue volunteer in the state of Oregon. The training is 50 hours, usually offered February through March. There are two classroom sessions a week, a few weekend days, and a final overnight exercise. The class costs $20 and includes First Aid and CPR certification.\n- Once you have completed SAR 101, all team members must complete 30 hours of training a year to be eligible for Search and Rescue missions. This ensures that all team members can safely and effectively assist during a mission.\nHow to Apply:\nIf you meet the requirements above, complete our application found at the link below. If you do not meet the requirements, simply attain the skills you need, and then apply. If you don't know how or where to attain these skills, you may contact us for some suggestions.\nDownload the Eugene Mountain Rescue application here:\nSubmission instructions are found on the last page of the application. Once received, the Board of Directors will review your application and decide if you are accepted as a training candidate. The most efficient method is to scan your completed application and send it to email@example.com. You may also bring your completed application to one of our meetings or you can mail it to us.\nIf you decide to bring your application to a meeting or choose to mail it, please contact us ahead of time so we know to expect you. Include a phone number so we may contact you if necessary.\nWe appreciate your interest in joining our team!\nPublished: Wednesday, 05 March 2014 20:00\nWhat is the training time commitment if I join?\nWe usually have one meeting and one training event per month from September through May. In addition, we expect new members to attend a training class in basic search and rescue skills (affectionately known as SAR 101) which involves 12 evening class sesions over a 2-month time period, plus some training events on weekends.\nOnce you complete SAR 101 in your first year, team members must attend 30 hours of training a year to remain eligible for missions.\nIf I join, do I have to go on every mission?\nIt is not necessary to participate in every mission. We recognize that people have other obligations in their lives and can not always participate. However, we do expect all members to put in at least 30 hours of participation per year. This time does not all have to be on search missions; training sessions meetings, and public service events also count toward the 30-hour requirement.\nCan I join EMR if I am not a climber?\nIf you have backcountry skills such as backpacking and backcountry skiing but no rock climbing or mountaineering experience, the best route to take to get involved in Search and Rescue is to join the Ground Search and Rescue team\nThis group does searches in moderate terrain, sometimes in remote settings, and does not do high-angle rock rescue or rescues requiring mountaineering skills. Meanwhile, you can begin to acquire climbing skills by taking a basic mountaineering class, such as the one offered jointly by Eugene Mountain Rescue and the Obsidians every spring. Once you have some basic skills you may join the mountain rescue group.\nHow many searches are there in a year?\nIn a typical year, there are about 1000 search and rescue missions in the state, and roughly 100 of them occur in Lane County.\nIs EMR a government organization?\nEugene Mountain Rescue is a 501c(3) nonprofit, volunteer organization. We are not part of the Sheriff's Office or any other governmental agency.\nWho pays for search and rescue?\nLane County Sheriff's Office has two full-time people who coordinate search and rescue missions in the county.\nWhen someone is reported missing, the first person to respond is generally from law enforcement (for example, a deputy from the sheriff's office or one of the Search and Rescue coordinators). Once it has been established that there is a need to conduct a search, the people who are sent into the field are generally volunteers.\nUnless the search becomes very extensive and lasts for multiple days, the county's costs for the search are minimal.\nWe are opposed to the idea of charging people for rescue. If someone is missing and his or her family hesitates to call because of concerns about what it might cost, it is more likely that the person will become more seriously lost or develop a life-threatening medical issue before being found."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:5643558a-2e74-439d-9822-32084a6bca83>","<urn:uuid:43c506f4-3c0e-4210-bade-6522c08b7132>"],"error":null}
{"question":"How do mobile crushers differ from traditional stationary crushing equipment in quarry operations?","answer":"Mobile crushers offer significant advantages in quarry operations. They can be transported on a single truck, provide ease of transportation, and can be quickly set up with support legs. They can be moved directly to the working face as primary crushing equipment and can crush rocks as large as 60 inches. This mobility helps reduce haulage costs compared to traditional stationary crushing setups.","context":["Crusher - Wikipedia\nA crusher is a machine designed to reduce large rocks into smaller rocks, gravel, or rock dust , by the downstream machinery Some crushers are mobile and can crush rocks as large as 60 inch , This is a crusher bucket, crushing rock in a quarry, for recycling the material previously extracted A crusher bucket is an.\nMobile Jaw Crushing Plant\nMobile jaw crusher is Shanghai Zenith many years of carefully crafted product, crushing , new series of rock crushing equipment, he has significantly expanded the concept of crushing operations , With support legs on truck, quick to set-up\nMobile Crushing Plant,Screening,\nMobile Crushing and Screening Plant,Crushing Plant,Stone Crusher Plant,Portable , 150 T/H Mobile Crushing Plant - Turbo Series Mobile Crushing Machine 100T/H , Crushing Plant Equipment Price Quarry Crushing and Screening Machine , It can be transported on single truck, and provides ease of transportation\nSelection of the optimum in-pit crusher location for an,\n, quarry trucks together with a rotary drilling machine , and the quarry trucks has always been , Using an in-pit mobile crusher can reduce the haulage costs.\nJG-1 Mobile Crusher - Komatsu\nquarry industry where jaw crusher has traditionally been used, KOMATSU designed a , trucksUnder such social circumstances, in 1992 Komatsu introduced mobile crusher , the working face as the primary crushing equipment, and the\nWHO WE ARE - Kidner Contracting\nOur services include Mobile Crushing & Screening, Quarry Products, Ready-mixed Concrete, Truck & Earthmoving Machinery Hire and all your favourite tyre.\nMobile Stone Crusher Price - Alibaba\nmobile stone crusher machine price in india Add to Compare Response Rate: , Mobile stone crusher type truck mounted stone crusher price Add to Compare\nWorld Highways - machines help improve,\nmachines help improve quarry output in Thailand , aggregate/month, with around 40 trucks transporting the product away every day , The decision to acquire a stationary or a mobile crushing plant was never seriously questioned\nQuarry & Aggregate For Sale | IronPlanet\nBuy used Conveyors, Crushers, Electrical Distribution Equipment, Off-Road Trucks, Screening Equipment from , ABB, Powerscreen, Cedarapids, , and more , Check out our quarry and aggregate equipment in our upcoming auctions , 2011 LT106 Tracked Mobile Jaw Crusher Plant\nUsed Mining and Quarry Equipment for Sale | Auto Trader,\n34 results , Search for used Mining and Quarry Equipment for sale on Auto Trader, UK's no 1 website to trade second hand , DOLLY TRAILER FOR CRUSHER Plant Picture , Trade, Jge Truck & Plant Ltd , Auto Trader on mobile\nConstruction provide mobile crushing and,\nFeb 9, 2016 , Construction provide mobile crushing and screening plant , various types of machines with different sizes and finally a crushing , from the quarry by heavy load trucks to the crushing plant site which is not far away\nLetter Ruling 08-14: Machinery and Equipment,\nDec 24, 2008 , Mobile equipment unloads blasted rock into these hoppers to be crushed Grizzly Feeder Feeds stone into crushers at a controlled rate removing smaller , With respect to the truck scales and loaders used in the \"quarry.\nHow Volvo CE is engineering a quarry run by , -,\nOct 18, 2016 , Volvo Construction Equipment, the Swedish Energy Agency, Skanska , it can be moved to the crusher, or a mobile crusher can be moved to the rock , rock is typically loaded by an excavator into rigid-frame dump trucks\n| Aggregates Equipment | Quarry Equipment |,\nDoing this phase effectively will improve conveying, crushing and screening , Hauling Equipment (Haul trucks perform heavy lifting in quarry operations)\nMobile Crushing and Screening Hire |,\nWe have reliable modern well maintained plant for both quarry and production hire , Hire mobile Crushing and Screening plants mounted on tracks , Our list of machinery is as Follows: , 966 -972) Rockhammer available; Excavator 25 tonne and 45 tonne; Rigid Trucks; Truck & Dogs; Water Truck; 25 tonne Dump Truck\nCrushers - Sigma Plantfinder\nRubble Master RM 60 Impact Crusher | Impact crusher with 1000 working hours Great condition , Crushers for sale See all Machines Of The Weeks for sale\nProducts | Masaba - Masaba Inc\nConveyors, Screens, Fixed Systems, Portable Plants, Crushing , Masaba is proud to offer a full line of products and equipment to increase the , Whether you are recycling, mining, or using it in the quarry, your Masaba Portable Jaw Crushing Plant will be , Material can be discharged onto trucks, hoppers, or conveyors\nrock crushing quarries - YouTube\nFeb 14, 2016 , Quarry Equipment; Rock Crusher; Ore Grinder; Mill Machine; , mobile crusher South Korea for sale Home Rock Crushing Plant cone , This noise comes from blasting, stone crushers, truck traffic tertiary crushing quarri\nPrice For Mobile Stone Crusher - Alibaba\nPrice For Mobile Stone Crusher, Wholesale Various High Quality Price For Mobile Stone Crusher Products from Global Price For Mobile Stone Crusher.\nCrusher - Used crushers for sale - Mascus,\nAfter a used crusher? At Mascus UK you can browse our range of second hand crushers for sale including top brands like Komatsu, and\nAggregate & Quarry Operations - Kirby-Smith,\nThe Authority on Aggregate and Quarry Equipment , We offer mobile crushers, wheel loaders, excavators, dozers, articulated trucks, motor graders, and more.\nInternational manufactures screening and crushing equipment , have introduced many of today's key mobile screening and crushing innovations\nsand, gravel, and crushed stone on-the-job training,\noperating and loading quarry trucks, customer finished-product trucks, and , and to transport unfinished material from the quarry to the crusher, or from the pit to the , Mobile equipment operators, who transport materials, must know haulage.\nCrusher Stock Images, Royalty-Free Images & Vectors,\nRelated: crush, stone crusher, rock crusher, conveyor, cement, paper crusher, mining equipment, grinder, primary crusher, cement crusher, quarry Mobile.\nCrusher Works: Your Truly Mobile Screening and,\nCrusher Works is an authorized dealer of world leading, quality equipment from many sources including , EDGE, BRETEC, ATLAS COPCO and.\nMinimizing Quarrying Costs by Correct Shotrock , -,\nShotrock Fragmentation and In-pit Crushing , Challenge in Quarry Development , inpit, fully mobile Optimum drilling and blasting Optimum crushing Cost , Scope of equipment changes due to more difficult and/or longer cycle times , Cost comparison between conveyor belt transport and dump truck haulage hauling\nUsed 2012 M&J 4000M for sale | 2012 QJ341 Jaw,\nWe can supply used tracked crushers, used mobile crushers, crushers on semitrailers, and static crushing plants To view more quarry machinery click here.\nMobile Crushing & Screening Plants | Damian Connelly,\nMar 29, 2016 , The history of mobile crushing plants started with roadside crushing and , to contract crushing in the iron ore industry the demand for larger equipment grew , Exposure to silica dust causes many problems to miners and quarry workers , Conveyors are much more economical than using trucks (at 80%."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:145cd401-92c3-4d43-af74-183cafeb2f3c>"],"error":null}
{"question":"How can I reduce the costs of professional services while maintaining quality?","answer":"You can reduce professional service costs through several methods: 1) Choose small-business focused providers instead of large firms with high overhead, 2) Use paralegals and bookkeepers for straightforward tasks, 3) Do your own organizational work and documentation, 4) Schedule regular meetings to reduce phone calls and communication costs, 5) Use attorneys as coaches for minor legal matters, 6) Demand and examine monthly invoices, 7) Negotiate prompt-payment discounts of 5-10%, 8) Consolidate your questions into single calls instead of making multiple impromptu calls, and 9) Consider outcome-based fee arrangements with attorneys where fees are adjusted based on results.","context":["Professional Service Providers\nDefinition: Individuals who provide your company with specialized service, including but not restricted to lawyers, accountants and management consultants .\nAfter you've owned your own business for a while, you know how to run it. You've probably done everything from answering the phones to hiring a general manager, and you can justly claim to know your business inside and out, in general and in detail. In case there's any operation you can't personally undertake, one of your employees probably can. There are, however, exceptions to this rule. Highly technical matters of law, accounting, management and marketing are usually best handled by outside experts. Attorneys, accountants and management and marketing consultants have specialized knowledge about niche areas that you couldn't--and shouldn't--hope to duplicate either personally or in the form of an in-house employee.\nHaving access to legal, accounting and other expertise is important to help your business grow as rapidly and efficiently as possible. Given enough time, you may be able to master the intricacies of law and finance. But why bother? Hand these duties off to professional service providers. They can do them faster and more effectively than you ever could. Besides, your skills are needed in helping your business expand.\nReferrals are the best way to get a new professional service provider. The best source of referrals is other entrepreneurs. Make a point of asking people in the same business sector (service, retail, restaurant, manufacturing, etc.) for referrals. You can also get good referrals from other professionals. That is, ask your accountant for an attorney's name and your attorney for an accountant's name. Other service providers, such as recruiters and bankers, are also good sources. Don't forget to ask suppliers and customers. Trade associations can also be good places to find names of professional service providers.\nOnce you are outfitted with a few referrals, contact several to gauge their interest in you and your interest in them. Then personally interview at least three prospects.\nAt your first interview with a professional service provider, be ready to describe your business and its legal, accounting or other needs. Take note of what the provider says and does, and look for the following qualities:\n- Experience. Although it's not essential to find an expert in your particular field, it makes sense to look for someone who specializes in small-business problems as opposed to, say, maritime law. Make sure the professional is willing to take on small problems; if you're trying to collect on a relatively small invoice, for example, will the lawyer think it's worth his or her time?\n- Understanding. Be sure the professional is willing to learn about your business's goals. You're looking for someone who will be a long-term partner in your business's growth. Does the professional understand where you want to be tomorrow and share your vision for the future?\n- Ability to communicate. If the lawyer speaks in legalese or the accountant uses lots of arcane financial terms without bothering to explain them, look for someone else.\n- Availability. Will the professional be available for conferences at your convenience, not his or hers? How quickly can you expect emergency phone calls to be returned?\n- Rapport. Is this someone you can get along with? You will be discussing matters close to your heart with this person, so make sure you feel comfortable doing so. Good chemistry will ensure a better relationship and positive results for your business.\n- Reasonable fees. Attorneys, accountants and other professionals charge anywhere from $90 to $300 (or more) per hour, depending on the location, size and prestige of the provider. Shop around and get quotes from several providers before making your decision. However, beware of comparing one provider with another on the basis of fees alone. The lowest hourly fees may not indicate the best value; an inexperienced professional may take twice as long to complete a project as an experienced one will.\n- References. Don't be afraid to ask for references. What types of businesses or cases has the attorney worked with in the past? Get a list of clients or other professionals you can contact to discuss competence, service and fees.\nSome jobs, such as auditing financials to satisfy the requirements of lenders or investors, simply must be done by a professional with specific credentials. A certified public accountant is a good example. If you are looking for legal advice, you certainly want an attorney with a juris doctor or equivalent degree who is a member of the bar.\nYou have more flexibility in looking for other credentials. The initials MBA after a person's name suggest that, as the holder of a master's of business administration degree, that person is well-trained. However, highly experienced people may be just as effective even if they lack the diploma and the initials. Evaluating the worth of credentials can be tricky. Check with associations such as the American Bar Association, the American Institute of Certified Public Accountants, or the government agency in your state charged with granting CPA credentials.\nThe professional services marketplace is a buyers' market these days. Here are 10 steps to keep your costs in check without hurting your chances of growing:\n1. Choose the right professionals. The key is to match your needs with the skills and resources of the provider. Most small-business owners simply do not need a large, major-city law firm or international accountant. The overhead expenses of such megafirms are passed on to their clients in the form of high hourly rates. Instead of a big name, look for small-business expertise.\n2. Examine your fee agreement. Once you find a professional with whom you feel comfortable, read the fee agreement letter carefully. Focus on hourly rates, expenses such as postage and photocopying, and travel time. Ask candidates for a sample of their standard fee agreement for your review. Be suspicious of any professional who balks at this request.\n3. Use paralegals and bookkeepers as part of your professional team. Certain legal tasks are straightforward enough that utilizing a paralegal instead of a business lawyer can result in significant savings. The same goes for using a bookkeeper instead of an accountant.\n4. Do your own footwork. Keeping organized records, indexing volumes of documents and writing out memorandums can reduce your professional fees significantly. Professionals will do all this for you--but at their hourly rates, and on your tab.\n5. Meet with your professionals regularly. At first, this may not seem to be a very effective way to keep fees down, but you'll be amazed at how much it actually reduces both the number of phone calls your provider has to make and the endless rounds of telephone tag.\n6. Use your attorney as a coach for minor legal matters. When you have a customer who owes you money and refuses to pay, do you turn the case over to your lawyer? Some entrepreneurs do, but some handle small legal matters on their own by using their attorneys as coaches. Lawyers can be very effective in coaching you to file lawsuits in small-claims court, draft employment manuals, and complete other uncomplicated legal tasks.\n7. Demand and examine monthly invoices. While most professionals are diligent about sending out monthly invoices, some wait until the bill is sufficiently large. If yours does not bill in a timely manner, ask for a breakdown of the time spent and costs incurred to date, and for similar monthly invoices to be sent thereafter. When the invoice comes, check the work description to be sure you weren't inadvertently billed for work performed for another client.\n8. Negotiate prompt-payment discounts. If you are paying a retainer fee, request that your bill be discounted by 10 percent. (A retainer fee is an amount of money that acts as a fee pre-payment; the remainder is refunded to the client.) Even if you did not pay a retainer, negotiate a prompt-payment discount if you pay your fees within 30 days of your invoice date. You may not get as much of a discount using this method, but even a 5 percent discount on your monthly legal fees can add thousands of dollars per year to your business's bottom line.\n9. Do not make impromptu calls to your professional. Most attorneys bill under a structure that includes minimum time increments for repetitive functions such as phone calls. This means when you call your lawyer for a quick question, you will be subject to a minimum time increment for billing purposes. For instance, if you place four impromptu calls a week to your professional at a minimum time increment of a quarter-hour per call, you'll get a bill for an hour of your lawyer's time--even though you only received five minutes' worth of advice! Keep a list of subjects you need to discuss, and make a single call to discuss them all.\n10. Negotiate outcome-based fee arrangements with attorneys. Although this is a relatively new concept in the legal market, more and more firms agree to such arrangements in this competitive marketplace. An outcome-based fee arrangement is a risk-sharing plan. Simply put, if your lawyer accomplishes a particular favorable outcome, the bill is adjusted to increase the fees by a preset formula. But if the outcome is not favorable, the final bill is adjusted downward (though not eliminated.)\nGet everything in writing when dealing with professional service providers. Your written agreement should cover the scope of the services to be rendered, the duration of the agreement and the fees. The fee schedule should state whether fees are to be based on an hourly, daily or project rate, and who is responsible for paying expenses. You should consider having fees based at least in part on performance to protect you from having to pay top rates for shoddy work.\nYour agreement should also specify who will be performing the work for your company. Some professional services firms have certain people whose primary job it is to solicit business, while others do the actual work. However, you may not want a lower-level attorney or junior accountant working on your project.\nFinally, the contract should explain how the agreement can be ended prematurely, typically with some kind of notice to the other party. This will allow you to get out of an unsatisfactory contract without having to pay the full amount.\nHaving access to top legal, accounting and other professional service expertise is essential to your business's long-term health. With these professionals on your side, you can deal effectively with legal, tax and financial issues that might require years of study to master. So instead of trying to do a professional's job, stick to doing what you do best--growing your business."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:ff5e531f-ec57-4526-827d-b3987b4eeafb>"],"error":null}
{"question":"Could you please help me understand how the travel distance and logistics compare between visiting Plaza de Armas in Lima versus reaching Machu Picchu from Lima?","answer":"Visiting Plaza de Armas in Lima is straightforward as it's located in Lima's historic center and is easily accessible. In contrast, reaching Machu Picchu from Lima requires significant travel, as it's located 312.5 miles from Lima. To get to Machu Picchu, travelers need to first reach Cusco, which can be done by taking the Peru Hop bus through recommended routes via Huacachina and Arequipa. This makes visiting Machu Picchu a much more complex journey compared to visiting Plaza de Armas, which can be accessed directly within Lima.","context":["Best Places to Visit in Lima\nAt first glance, Lima seems endlessly stretching from the sea to the hills, a sprawling metropolis nothing like the tourist image of brightly dressed Andean villagers posing with their camels in front of towering peaks. But a closer look reveals that this gigantic city, home to about a third of Peru’s population, has its own attractions that are as fascinating and colorful as the interior landscape you draw.\nSo take time to explore this vibrant city, visit its outstanding museums, and set the historical and cultural scene for what you’ll see elsewhere in Peru. Admire its colonial architecture, beautifully punctuated by intricately carved wooden balconies and Baroque-style décor.\nDine with the locals at some of South America’s best restaurants, wander through expansive green parks, and relax in Lima’s bustling seaside suburbs. Founded in 1535 under Francisco Pizarro, you’ll understand why the Spanish conquistadors called Lima the “king of the city”.\nFollow our list of the Best Places to Visit in Lima to find the fascinating places to visit in this exciting city.\n10 Best Places to Visit in Lima\nLet’s explore the top 10 Most Beautiful and Best Places to Visit in Lima:\n1. Plaza de Armas\nAlso known as the Plaza Mayor, this wide square is Lima’s historic center and the most logical starting point for sightseeing. Much of the original city was lost in the 1746 earthquake – the only original structure in Lima’s Plaza de Armas is the bronze fountain in the center, built in 1651.\nIts buildings were rebuilt after the earthquake and are today a UNESCO World Heritage Site. Besides the cathedral, the square is surrounded by the Archbishop’s Palace, the Casa del Oidor, and the Palacio del Gobierno, the presidential residence built by José San Martín on July 28, 1821, when Peru declared independence. You can see the Changing of the Guard here at noon on weekdays, and it has always been a popular attraction for tourists.\nFrom Plaza de Armas to Plaza San Martin, the pedestrian-only Jiron de la Union is a mix of old and new, with restaurants and shops. Here you will find La Merced Church, which was completed in the late 1700s and has an ornate Baroque Colonial façade.\nOne of the oldest and best preserved colonial mansions in South America, Casa de Aliaga dates back to the city’s early days. It has been run by the Aliaga family for 17 generations since 1535, making it the oldest single family owned and resident in South America. The colonial-style cottages are from the 16th, 17th and 18th centuries and are open for guided tours by prior reservation.\n2. Convento de San Francisco\nThe San Francisco church and monastery are famous for its catacombs where the bones of about 10,000 people were buried when it was the first cemetery in Lima. Below the church is a labyrinth of narrow aisles lined with bones on both sides.\nIn one area, a large circular hole is filled with bones and skulls arranged in a geometric pattern like a work of art. If the ritual were to be held high, the sound would echo eerily in the catacombs.\nVisiting them is not suitable for the claustrophobic as the ceilings are low and the doors between rooms are even lower and require people to take shelter when entering. But the catacombs are at the end of the church tour, so you can skip them.\nThere is so much more to see here. The upstairs library holds thousands of old books, and the cloister houses an impressive collection of religious art. He is best known for his Last Supper fresco, which shows the apostles eating guinea pigs and a demon standing next to Judas.\nConsecrated in 1673, the San Francisco Church and Convent is one of the best preserved colonial churches in the city, having withstood the earthquakes of 1687 and 1746, although it was badly damaged in the 1970 earthquake.\nAddress: Jiron Lampa y Ancash, Lima\nOfficial website: http://museocatacumbas.com/\n3. Magic Water Tour in Park of the Reserve\nThe Magic Water Tour opened in Conservation Park in 2007 and attracted 2 million visitors in one year. It holds the record for the largest fountain complex in the world with its 13 separate fountains.\nThe largest, Fuente Mágica, draws more than 80 meters of water, while the Fuente Túnel de las Sorpresas (Tunnel of Surprises) is a 35-metre length, walkable water tunnel. In Fuente de la Fantasia you can see lasers and painting shows, jets synchronized with music.\nAddress: Madre de Dios, Lima\n4. Parque del Amor (Park of Love)\nYou won’t find a more romantic place to watch the sunset than the Parque del Amor (Love Park) on the Miraflores Malecón. Mosaic designs, consisting of small tiles covering undulating walls, are often compared to the designs created by Antoni Gaudí for Park Guell in Barcelona, Spain.\nThe poems of Peruvian poets, including Abelardo Sanchez León and Augusto Tamayo Vargas, were mosaiced. The trail winds up from the top of the flower-covered cliff and leads to El Beso (The Kiss), a large sculpture of an embracing couple by Peruvian sculptor Victor Delfín, in the center of the park.\nParque del Amor is open every evening until 23:00.\nLocated on the sea cliffs south of central Lima, Miraflores is a neighborhood of modern glass and steel commercial buildings, some beautiful old colonial houses, and plenty of green space. Here you will find stylish shops and restaurants serving the “new Peruvian” cuisine that has caught the world’s attention in the culinary world.\nBeautiful parks and green spaces stretch along the cliff top overlooking the water, and sail blades can often be seen drifting off the cliff above surfers in the waves below. Expect slightly higher prices in this wealthier neighborhood.\nBesides browsing the shops and tasting new Peruvian dishes, you’ll find plenty to do here. Those interested in pre-Columbian culture should visit the Amano Museum to see a chronological collection of Peruvian ceramics and textiles.\nPre-Columbian cultures including the Chimu and Nazca are well represented, while the Amano Museum is known for its outstanding textile collection from the lesser-known Chancai culture of the north coast. Tours must be booked in advance.\nAddress: Retiro 160, Miraflores\n6. Museo de la Nacion (National Museum)\nThe largest museum in Lima, the National Museum is the best place to explore ancient Peruvian history and learn about Peruvian culture. The museum covers the entire archaeological history of Peru, from its first inhabitants to the Inca Empire.\nCeramics and textile exhibits and scale models of archaeological sites such as Machu Picchu and the Nazca Lines are arranged in chronological order, showing the progression from one culture to the next. Most impressive is the replica of Lord Sipan’s tomb, the first Moche mummy found at Vacara Jada in Sipan, Peru. Most screens are marked and explained in Spanish and English.\nAddress: Av. Javier Prado Este 2465, San Borja\n7. Larco Museum\nSouth of central Lima in Pueblo Libre is the Rafael Larco Herrera Museum, commonly known as the Larco Museum. This 18th-century governor’s mansion was built on the site of a 7th-century pre-Columbian pyramid.\nThe Larco Museum houses more than 40,000 Peruvian pottery, most of which come from the Moche and Chimu cultures. Not all of these are displayed at the same time. It’s also a collection of fine gold work, along with some textiles, stonework and metalwork, that make it one of Lima’s most popular attractions.\nAddress: Av. Bolivar 1515, Pueblo Libre, Lima\nOfficial website: www.museolarco.org\n8. Santo Domingo\nBuilt in 1540 on land given to Dominican Priest Vicente Valverde by Francisco Pizarro, Santo Domingo church and monastery is one of the oldest, most historic churches in Lima. Here you will find the remains of the Holy Rosary of Lima; San Juan Marcias; and San Martin de Pores, America’s first black saint.\nThe Saint Rose statue was gifted to Santo Domingo by Pope Clement X. The monastery is famous for its tiled mosaics depicting the life of San Domenico de Guzmán, who founded the Dominican Order. Inside the colorful monastery is a peaceful green garden. The church is a short walk northwest of Plaza de Armas in central Lima.\n9. Huaca Pucllana\nThe pyramid-shaped temple of Huaca Pucllana is located in the heart of Miraflores and is now surrounded by buildings. Constructed of adobe and clay bricks – a building material that could not last more than 1,000 years in any other climate – the pyramid consists of seven tiered platforms.\nThe Lima culture, which built the pyramids, flourished on Peru’s central coast between 200 AD and 700 AD. It is known from the artifacts found here that it is important both as a ceremonial and administrative center.\nThe area is divided into two parts, one with evidence showing it was used to feed fish, and the other as an administrative area. A tomb with human remains was found here, and artifacts from the late Wari culture flourished in the area between 500 and 900 AD.\nAddress: Calle General Borgoño cuadra 8, Lima\n10. National Museum of Archeology\nThe National Museum of Archeology, Anthropology and History is not as large as the National Museum and does an excellent job of exploring Peru’s history from prehistoric times to colonial times. The display is well organized, the amount of material won’t overwhelm you, and it’s easier to understand.\nThe variety is impressive, with priceless ceramics, stone carvings, obelisks, wrapped mummies, tombs, jewellery, tapestries, and gold and metalwork, many of which are displayed in scale models of the archaeological site.\nThe ceramics collection includes pieces dating from 2800 BC, and sculpted obelisks include the granite Tello Obelisk and the famous Estela Raimondi. Parents should note that some ceramic figures may not be suitable for children. The museum contains the adjacent houses where Jose de San Martin and Simon Bolivar once lived.\nAddress: Plaza Bolivar, Pueblo Libre, Lima\nHope you like our choice of the best places to visit in Lima. If you think there are some more beautiful places to visit in Lima, we should cover them. Write us below in the comment box.","- 1 What is the closest city to Machu Picchu?\n- 2 Why is Machu Picchu so special?\n- 3 Is Machu Picchu the lost city of Incas?\n- 4 How far is Lima to Machu Picchu?\n- 5 Do you have to be fit to climb Machu Picchu?\n- 6 How many days do you need for Machu Picchu?\n- 7 What does Machu Picchu mean in English?\n- 8 Who rediscovered Machu Picchu?\n- 9 How safe is Machu Picchu?\n- 10 Why did Incas leave Machu Picchu?\n- 11 Why was Machu Picchu so hard to find?\n- 12 Why is Machu Picchu a hidden city?\n- 13 Is Lima worth visiting?\n- 14 How much does it cost to visit Machu Picchu?\n- 15 Is Lima safe for tourists?\nWhat is the closest city to Machu Picchu?\nAguas Calientes is the closest town to the archaeological center of Machu Picchu. It is also known as Machu Picchu town.\nWhy is Machu Picchu so special?\nMore than 7,000 feet above sea level in the Andes Mountains, Machu Picchu is the most visited tourist destination in Peru. A symbol of the Incan Empire and built around 1450AD, Machu Picchu was designated a UNESCO World Heritage Site in 1983 and was named one of the New Seven Wonders of the World in 2007.\nIs Machu Picchu the lost city of Incas?\nIt’s not actually the Lost City of the Inca. When the explorer Hiram Bingham III encountered Machu Picchu in 1911, he was looking for a different city, known as Vilcabamba. This was a hidden capital to which the Inca had escaped after the Spanish conquistadors arrived in 1532.\nHow far is Lima to Machu Picchu?\nMachu Picchu is located 312.5 miles from the capital city of Lima. To get from Lima to Machu Picchu, you have the following options: Go from Lima to Cusco using the Peru Hop bus. The route via Huacachina and Arequipa is recommended.\nDo you have to be fit to climb Machu Picchu?\nIn order to tackle any Machu Picchu trek you need to be in good physical shape. You don’t need to be an olympic athlete but you should be able to walk 5-7 hours a day over relatively tough terrain for 3-4 days in a row. We recommend undertaking a basic training regime (see below) 3-6 months before departing.\nHow many days do you need for Machu Picchu?\nHis recommendation was to stay overnight and spend two days at Machu Picchu. We took this advice and would recommend it to everyone. You can see Machu Picchu in 3 to 4 hours, but you need at least two visits to fully appreciate it and experience the many things there are to do at the site.\nWhat does Machu Picchu mean in English?\nThe Citadel of Machu Picchu is considered the main tourist attraction in Peru and one of the most visited worldwide. Machu Picchu is a Quechua word that comes from “ Machu ” that means old or ancient, and “ Picchu ” meaning mountain. Therefore, Machu Picchu translates as “Old Mountain.”\nWho rediscovered Machu Picchu?\nWith the boy leading the way, Hiram Bingham stumbled upon one of the greatest archaeological finds of the 20th century—and what was named in 2007 as one of the new seven wonders of the world: Machu Picchu.\nHow safe is Machu Picchu?\nFor most visitors, travel to Machu Picchu is quite safe. You will need to be much more vigilant when traveling through large cities such as Cusco and Lima. Such issues within Machu Picchu and along the Inca Trail, however, are nonexistent.\nWhy did Incas leave Machu Picchu?\nGenerally, all historians agree when said that Machu Picchu was used as housing for the Inca aristocracy after the Spanish conquest of in 1532. After Tupac Amaru, the last rebel Inca, was captured, Machu Picchu was abandoned as there was no reason to stay there.\nWhy was Machu Picchu so hard to find?\nLikely abandoned in the sixteenth century and invaded by the Andean forest throughout the proceeding years, Machu Picchu remained hidden to the world for centuries. The Spanish conquistadors never found it and the Incas who knew the location never revealed its existence and was forgotten.\nWhen the explorer Hiram Bingham III encountered Machu Picchu in 1911, he was looking for a different city, known as Vilcabamba. This was a hidden capital to which the Inca had escaped after the Spanish conquistadors arrived in 1532. Over time it became famous as the legendary Lost City of the Inca.\nIs Lima worth visiting?\nLima is worth seeing It’s home to more than a quarter of Peru’s roughly 30 million people, has wonderful food, the beautiful Miraflores district (where you can drink while overlooking beaches lined with small rocks that form eye-catching patterns each time the tide rolls out) and excellent museums.\nHow much does it cost to visit Machu Picchu?\nAn official in-person ticket to the Machu Picchu site is $60 (USD) and a ticket that also gives you access to the Machu Picchu or Huayna Picchu mountain during any of the two time-slots is $80. Online prices are generally higher, but they also offer sales and specials at various times of the year.\nIs Lima safe for tourists?\nThere are safe neighborhoods in Lima when you travel to the suburbs. But, if you get into the city, there are dangerous situations at night and even during the day. Thieves roam the streets looking for tourists to target. It’s best to stay in the suburbs like Miraflores to spend your night there."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:585aa10c-ac8e-4e3b-817f-0cb48c0cf299>","<urn:uuid:c7b3fc4a-5d6b-4557-b187-25d828c014f0>"],"error":null}
{"question":"How do mitochondria and interdependent social systems show similar organizational principles?","answer":"Both mitochondria and interdependent social systems demonstrate complex organization with specialized, interconnected parts. Mitochondria operate as specialized cells within larger cells, maintaining their own DNA and ribosomes while depending on the host cell for survival. Similarly, human systems, whether in health, society, or personality, function as interdependent networks where different components are united and interdependent, yet capable of maintaining their own distinct characteristics while contributing to the larger whole.","context":["Interdependent sentence example\n- In fact, analogical, inductive and deductive inferences, though different processes of combining premises to cause different conclusions, are so similar and related, so united in principle and interdependent, so consolidated into a system of inference, that they cannot be completely investigated apart, but together constitute a single subject of science.\n- There are three interdependent methods of social media marketing.\n- The most basic concept of this ancient philosophy of yin and yang is that everything, all energies, are interdependent upon one another.\n- While yin energy and yang energy are interdependent upon one another to exist, the goal behind yin yang energy and concepts is balance.\n- In fact, one of the basic philosophies of yoga is that the breath and the mind are interdependent.Advertisement\n- Human personality, we learn, is the temporary manifestation of a complex organization consisting of \"seven principles,\" which are united and interdependent, yet divided into certain groups, each capable of maintaining temporarily a spurious kind GI personality of its own and sometimes capable of acting, so to speak, as a distinct vehicle of our conscious individual life Each \"principle\" is composed of its own form of matter, determined and conditioned by its own laws of time, space and motion, and is, as it were, the repository of our various memories and volitions.\n- Hence upright posture, sophisticated speech and deciduous dentition develop together as an interdependent system.\n- All aspects of health are interrelated and interdependent, and there needed to be a broader and more holistic approach to employe well-being.\n- Indeed, it is possible to identify two basic lines of our current secularized society that are clearly interdependent.\n- His speculative ideas, his moral teaching and his poetical power are indeed interdependent on one another, and this interdependence is what mainly constitutes their power and interest.Advertisement\n- The child's immune system is an intricate network of interdependent cell types, substances, and organs that collectively protect the body from bacterial, parasitic, fungal, viral infections, and tumor cells.\n- However, global warming makes people realize how interdependent the nations of the world are even if they are not aware of all the essential cycles and processes going on in the natural world.\n- This interdependent relationship is evident in the traditional yin yang symbol, in which the dark side (yin) has a spot of the light and the light side (yang) has a spot of dark.\n- These three conditions were interdependent; and Henry IV., with his persuasive manners, his frank and charming character, and his personal valour, seemed capable of keeping them all three.\n- Assuming as an axiom that the centre of gravity of any number of interdependent bodies cannot rise higher than the point from which it fell, he arrived, by anticipating in the particular case the general principle of the conservation of vis viva, at correct although not strictly demonstrated conclusions.Advertisement\n- The arrangement was too cumbersome, for there was nothing which would be called a central execntive; the three bodies (two of twenty-four members each, the third of fifteen) were interdependent, and none of them possessed efficient control over the others.","One of the interesting questions about the history of life is the question of where eukaryotic cells came from. Our fossil record shows us pretty clearly that for the first couple of billion years of the existence of life, all of Earth's life forms were prokaryotic. So where did eukaryotic cells come from?\nAn initial question might be why eukaryotic cells arose. Problems like this rarely have real answers just good ideas. One important thing that changed in the world about the time that the first eukaryotic fossils appear is that the atmosphere of the planet was beginning to fill up with free oxygen, almost certainly due to the increasing numbers of photosynthesizing organisms in the world. (Photosynthesis produces oxygen as a waste product). The introduction of significant oxygen into the environment would have initially been a disaster for most life forms of the times, since organisms that live without oxygen (anaerobic organisms) are poisoned by oxygen, which is frankly a very destructive substance. However, those organisms that survived the introduction of oxygen (because they were lucky enough to be able to perform some kind of chemical process that would use up oxygen, thus preventing it from messing around with the vital chemistry of the cell) got a bonus. Oxydation chemistry tends to release energy. So we're pretty sure that it was about this time than aerobic cellular respiration arose. The overwhelming difference between anaerobic cellular respiration and aerobic cellular respiration is the amount of energy produced by the two processes. Aerobic respiration produces many times more energy than anaerobic respiration does. Hypothetically, this increase in available energy was at least part of what promoted the development of the much larger and more complex eukaryotic cell.\nSo where did all of the new structures in eukaryotic cells come from? The probable answer is that there were at least two different kinds of events that added to the complexity of cell structure. Many of the structures in eukaryotic cells probably developed from the elaboration of the membranes of the cell. This is the likely explanation for the origin of the endoplasmic reticulum, golgi apparatus, and nuclear envelope. But the evidence strongly suggests a much more interesting origin for the two great energy processing organelles, the mitochondrion and the chloroplast. These two structures probably arose through a process known as endosymbiosis. [endo=inside; sym=together; bio=life]\nSymbiosis is a dependent relationship between two organisms. There are three basic kinds: parasitism, in which one partner (the parasite) benefits and the other (the host) is harmed; commensalism, in which one partner (the commensal) benefits and the other partner (the host) is indifferent, and mutualism, in which both partners benefit. These three states are evolutionarily related to each other: parasitic relationships tend to evolve into commensalistic relationships, and commensalistic relationships tend to evolve into mutualistic relationships. This makes perfect sense. Any accidental genetic change in the host which reduces the harm (or causes benefit) from the parasite would certainly be favored by selection; any accidental genetic change in the parasite which keeps its host, upon which it depends, healthy and alive longer would also be favored. So the selective pressure on both sides is toward less and less damage to the host.\nEndosymbiosis is a symbiotic relationship between two organisms in which one of the organisms lives inside the other. The relationship can be any of the three types of symbiosis. A frequent trend in endosymbiotic relationships is for the endosymbiont the one inside to become more and more specialized (and thus dependent upon the host). Endosymbiotic relationships are extremely common; there are endosymbionts living inside your body at this very moment.\nAlmost all biologists believe that this phenomenon explains where mitochondria and chloroplasts came from. Not all organisms developed the ability to perform photosynthesis, or to convert to aerobic cellular respiration. Many of those that didn't make these alterations themselves went into partnership with other organisms that did. If an anaerobic cell could engulf an aerobic one (without digesting it), it could get the benefit of the ATP overflow from its captive partner. Given a couple of billion years to get used to each other, the inside, aerobic partner became so specialized for aerobic cellular respiration that it lost almost all of the basic life skills, depending upon the external host cell to support it. Voila` mitochondrion. If you tell the same story, but substitute \"photosynthesis\" for \"aerobic cellular respiration,\" you have a recipe for the invention of chloroplasts.\nThis is an interesting story, but pretty outrageous unless there's some evidence that indicates that it might be true. Glad that you asked ;^)\nThere are some very interesting things about mitochondria and chloroplasts that have had biologists scratching their heads for quite a while. For one thing, both of these organelles have their own DNA molecules. Their DNAs are not duplicates of nuclear DNA they are exclusive to the mitochondrion or the chloroplast. Mitochondrial DNA carries genes necessary to produce some of the molecules vital for the aerobic respiration process, and chloroplast DNA carries the genes for substances necessary for photosynthesis. The nuclear genes can't duplicate these. Unlike the DNA in the nucleus, mitochondrial and chloroplast DNAs are naked and circular just like a prokaryotic cell's DNA. These two organelles also have their own ribosomes and they are different from the ribosomes out in the cytoplasm. The proteins coded for by the mitochondrial genes are produced by mitochondrial ribosomes, and those coded for by the chloroplast genes are produced by chloroplast ribosomes.\nAnother interesting aspect of mitochondrial function is that not all parts of cellular respiration occur in the mitochondria. Aerobic cellular respiration has three parts, but only the second and the third require the presence of oxygen. The first part of cellular respiration (called glycolysis) requires no oxygen, and takes place not in the mitochondria, but out in the cytoplasm of the cell. It is identical to the anaerobic cellular respiration which occurs in cells which cannot use oxygen.\nThere are two very interesting conclusions to draw from these pieces of information. First, these observations strongly suggest that a mitochondrion or chloroplast is very much like a highly specialized and simplified cell living inside a larger cell. Second, it looks very much like the process of aerobic cellular respiration arose as an \"add on\" process. The aerobic process performed by the mitochondrion were tacked onto the end of the older, anaerobic respiration process.\n© 2000 College of DuPage\nCenter for Independent Learning (630) 942-2185\nUpdated 25 September 2004"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:9ca5996d-51dd-4424-80da-e18e761f74b9>","<urn:uuid:1da622f4-9109-48bb-87af-6e0369691589>"],"error":null}
{"question":"How do corn earworm and European corn borer differ in their impact on mycotoxin contamination in corn?","answer":"The European corn borer and corn earworm have different impacts on mycotoxin contamination. Bt corn is effective at reducing feeding damage from European corn borer but not from corn earworm. As a result, reductions in mycotoxin contamination may occur if European corn borer is the main pest, but not if corn earworm predominates. Additionally, while European corn borers feed on all parts of the corn plant, earworms feed almost exclusively on the ear tips, leaving no visible damage on husks or leaves.","context":["Corn grain can be naturally contaminated by mycotoxins, natural toxins produced by fungi.\nWounding of the kernel (Figure 2) makes it easier for infection by the fungi that produce mycotoxins. European corn borer and other caterpillars commonly produce wounds that favor infection and mycotoxin contamination.\nThat being the case, does the use of genetically modified Bt corn—genetically engineered to provide control of certain insect pests—result in reduced mycotoxin contamination?\nFigure 1. Fusarium ear rot of corn, usually associated with fumonisin contamination.\nTo address this question, field studies have been conducted comparing Bt-hybrids to non-Bt hybrids that are otherwise genetically very similar. Thus far, the overall results are that meaningful mycotoxin reductions sometimes occur due to the Bt trait. A more detailed discussion follows.\nField studies in the USA and Europe have shown reduced Fusarium ear rot and reduced fumonisin contamination from Bt corn. These reductions have been associated with reduced insect feeding on kernels containing the Bt toxin.\nIn studies where reductions in fumonisin concentration occurred, these ranged from 20% to 90%, often bringing the grain below concentrations that pose risks to humans and the most sensitive animals.\nFigure 2. Aspergillus ear and kernel rot of corn, often associated with aflatoxin contamination. The fuzzy appearance inside the kernel is the sporulation of the invading fungus.\nIt is interesting to note that the application of synthetic insecticides to control kernel-feeding insects on non-Bt plants also sometimes reduces insect feeding and fumonisin contamination. This raises the question as to which is more sustainable: use of the Bt trait or application of insecticides. Either way, reductions in fumonisin contamination are highly desirable.\nAs with fumonisins, field studies have often documented reductions in aflatoxin contamination due to the Bt trait. Significant reductions were most common when aflatoxin levels were at moderate to high levels in the non-Bt corn.\nIn studies where reductions were documented, these commonly ranged from 50-90%. In some cases, aflatoxin reductions from the Bt trait were sufficient to bring the corn below 20 parts per billion, an important regulatory threshold for use of the grain in human foods. As with fumonisins, aflatoxin reductions from the Bt trait have generally been linked to reductions in kernel injury from insect feeding. In some cases, the Bt trait did not result in aflatoxin reductions. This may be due to several reasons:\nIn those fields, drought stress may have been more important in promoting aflatoxin accumulation than insect damage to kernels;\nAs noted below, the Bt trait isn’t effective against all kernel-feeding insects.\nThere are several caveats and complicating factors in this line of research, as follows:\nBt corn isn’t a “magic bullet”, eliminating all mycotoxin contamination. Indeed, sometimes reductions in mycotoxin contamination do not occur. This is true for both fumonisins and for aflatoxins. However, reductions of fumonisins and aflatoxins have been documented commonly in field studies, especially under conditions moderately to highly favorable for ear rot and mycotoxin contamination. Such reductions occur commonly enough—with no known “downsides” relating to consumption of the grain—that the appropriate use of Bt corn is considered to be beneficial both for food safety as well as for animal health.\nThe Bt toxin must be expressed in the corn kernel in order to reduce insect injury and, ultimately, mycotoxin levels. In some Bt corn hybrids, the Bt toxin is not expressed in the kernel. Such hybrids have no protection against wounds created by insect feeding, and therefore fumonisin contamination is not reduced in these hybrids.\nWhile the Bt trait is important in reducing mycotoxin contamination, the magnitude of mycotoxin reduction can depend on the genetic background of the corn hybrid. In other words, if you compare two hybrids that have the same Bt trait, one may have less fumonisin than the other, simply because of its genetic background.\nThe effectiveness of the Bt trait in reducing mycotoxin contamination depends on the insect pest present. For example, Bt corn is often effective at reducing feeding damage from the European corn borer, but not the corn earworm. Consequently, reductions in fumonisin contamination may occur if the European corn borer is the principal pest in a field, but not if the corn earworm is predominant.\nCorn hybrids that express the Bt trait in kernels sometimes exhibit reduced kernel contamination from fumonisins and aflatoxins. This is thought to be due to reduced insect feeding on the kernels.\nThe Bt trait is not a “silver bullet”, eliminating all mycotoxin risk. However, reductions occur commonly enough, with no known “downside”, that the Bt trait is thought to contribute to food safety and livestock health in both developed and developing countries. While it is well-documented that the Bt trait can reduce mycotoxin contamination, it is best used wisely, and only in fields with a moderate to high risk of damage from the target insect pests.","Table of Contents\nThe corn earworm (Helicoverpa zea, formerly known as Heliothis zea) is a major pest of late-season sweet corn in Ontario and is responsible for a large percentage of grade-out corn. Earworms feed almost exclusively on the tips of the ears, leaving no visible damage on the husks or leaves. Fall armyworms - another late-season pest - feed extensively on the leaves, and often enter the ears through the side. European corn borers, which are present throughout the season, feed on all parts of the corn plant. Most farmers consider an intensive spray program necessary to control these pests.\nThe earworm has a wide host range, feeding on many cultivated crops and weeds. Elsewhere in its range, it is also known as the cotton bollworm, tomato fruitworm or tobacco budworm. In Ontario, however, it is a major problem only on sweet corn. Most seed corn fields are harvested before the worst of the earworm damage, and the damage to field corn is not considered economic.\nThe corn earworm is one of several pests that does not overwinter in Ontario. The entire population is killed by low winter temperatures. Each spring, the moths must reestablish themselves in Ontario, from the overwintering populations in the southern United States and Mexico. They begin their northward migration around May and usually reach Ontario sometime in August. Since moths are not strong enough to fly that distance on their own, they are carried by high-level winds. The warm southerly winds that blow into Ontario in the summer carry the moths long distances. These winds often bring rain, which commonly brings the moths back to earth.\nSince their migration is so weather-dependent, the moths arrive in Ontario at different times each summer. Although they may arrive in late July some years, at other times they may not arrive until September.\nEarworm distribution in Ontario also differs each year. While generally found in higher numbers in the counties that border Lake Erie and Lake Ontario, they may not infest parts of the province some years. For example, in 1994 there were no reports of earworms from any sites east of Toronto. Even when earworms are present in large numbers in one area, other corn fields that are only a few kilometres away may not be infested.\nThere are four stages in the corn earworm's life cycle. Earworms arrive in Ontario as moths. The moths lay eggs, which then hatch into larvae (caterpillars). After feeding for two to four weeks, the larvae become pupae. In Ontario, larvae are typically killed by frost before pupation. In warmer climates, adult moths emerge from the pupae to repeat the cycle.\nThe corn earworm adult is a buff- or tan-colored moth with a wingspread of 3.5 to 4 cm (1.25 to 1.5 inches) (Figure 1). The forewing may have several darker markings and always has a central brown dot, clearly visible on the underside of the wing and faintly visible from the top. The hindwings are very pale in color, with a darker brown border. Eyes of living moths are bright green, which fade to dull olive green or brown a few days after death. There is no easy way to differentiate male from female moths.\nFigure 1. Corn earworm moth.\nAlthough earworms may lay their eggs anywhere on the corn plant, almost all will be laid on fresh silks if these are available. Although eggs are laid individually, one female can lay over 100 each night, and over 1000 during her lifetime. Each egg is nearly spherical, and about the same color and diameter as a corn silk (Figure 2 and Figure 3). Most eggs are laid on the outer third of the silks and hatch in 2 to 10 days, depending on the temperature.\nFigure 2. Corn earworm egg on fresh silk (actual size).\nFigure 3. Egg magnified 10 times.\nUpon hatching, young earworms (Figure 4) crawl down the silks toward the ear. After feeding on the silks inside the husk for a few days, they begin feeding on the kernels at the ear tip. The worms will grow up to 3.7 cm long (1.5 inches), with prominent stripes running the length of their bodies (Figure 5). The size and the presence of stripes differentiate earworms from European corn borers, while their head color (tan) differentiates them from fall armyworms. See Table 1 for more complete descriptions of these three pests.\nFigure 4. Corn earworm larva, recently hatched. Note the stripes running longitudinally.\nFigure 5. Fully grown larva, up to 3.7 cm (1.5 inches) long. Note the stripes and the tan-colored head.\nWhen corn earworms are present, all sweet corn with exposed fresh silk is susceptible to damage. Corn in the tassel stage does not need to be protected from earworms but may be attacked by other pests. Where sequential plantings are located close together, the field with the most fresh silk will likely receive the bulk of the egg-laying; other fields, however, are not immune.\nEarworms normally feed only on the kernels of sweet corn, beginning by feeding at the tip of the ear and moving down the ear as they grow. Feeding is almost always confined to the top third of the ear. Fecal matter is found as large moist pellets in the silk channel and at the ear tip. Earworms do not bore into the cob, as European corn borers and fall armyworms sometimes do.\nAlthough earworms damage only a small percentage of the kernels, their presence and droppings are very distasteful to most consumers. Where control has been less than perfect in the field, growers are forced to check ears at harvest and cull the damaged ones. Earworm-infested ears can sometimes still be marketed if the tips are cut off, although this practice significantly reduces the shelf life of the corn. Sweet corn destined for the processing plant may be able to sustain some earworm damage at the ear tips, as the tips are not used in the finished product.\n* A hand lens is the most reliable means of separating corn borers from the other two species.\nIn addition to direct damage, earworms can also predispose the crop to attack by other pests. Sap beetles will be attracted to the smell of fermenting sugars after earworms begin feeding on sweet corn kernels. In field corn, ear molds developing in the damaged kernels can cause toxicity problems for livestock.\nThe biology of the corn earworm makes it a very good candidate for an on-farm insect-monitoring program. The pest is present only at certain times of the year, its distribution is sporadic, and a cheap, effective monitoring method gives growers adequate time to implement a control program once the pest has been detected.\nOMAFRA monitors corn earworms and other sweet corn pests at several sites across the province. Information about pest activity in your local area can be obtained by calling a regional vegetable agriphone. While regional information is helpful, it is not as reliable as insect counts from traps located on your own farm.\nCorn earworms are monitored using a Heliothis trap, manufactured by Scentry Inc. (Figure 6) A pheromone lure (corn earworm Luretape manufactured by Hercon), which imitates the female earworm's sex attractant, is placed in the trap. Male earworm moths are attracted to the lure and are caught inside the trap. Since pheromone traps only catch males, they cannot be used to control an infestation. Their value lies in detecting the presence of the pest and in estimating the size of the infestation. (For information about ordering traps, call your local Pest Management Advisor.)\nFigure 6. A Heliothis trap, baited with a pheromone lure, is a simple way to monitor for corn earworms\nSweet corn growers should use at least two Heliothis traps to monitor for earworms. Place one trap per field in the two corn fields that are farthest apart. If two traps are used in a single field, place them at opposite ends. Move traps as often as necessary to ensure that each trap is always next to a field with fresh silk. Establish traps in early July, then check them twice a week. Replace the pheromone lure every two weeks. (Store extra lures in the refrigerator or freezer.)\nMost years, the first earworms will not be trapped until sometime in August. When moths are caught in the traps, compare them with Figure 1 to see if they are earworms and not a similar species. After the first earworms arrive, begin checking the traps three times a week. Continue monitoring until the last planting no longer has any fresh silk showing.\nBefore modern insecticides became available, corn growers applied mineral oil to the silk of each ear to prevent invasion by corn earworms. This method is still used occasionally by some organic farmers and home gardeners. It effectively controls the pest, but is time-consuming and the oil left at the ear tip may be distasteful to consumers.\nSeveral predators and parasites, including ladybird beetles, lacewings, predatory bugs, and parasitic flies and wasps, attack earworm eggs and larvae. These beneficial insects, present naturally in the field, exert an ongoing influence in keeping the pest population in check. They are not, however, adequate for economic control. To date, no commercially available biological control agents will effectively control the earworm.\nThose who want to avoid earworm damage without the use of insecticides must plant early. Sweet corn harvested before the middle of August is usually free of earworms. After that time, insecticides are necessary to protect the crop.\nCorn earworms can be controlled with insecticide sprays applied every three to seven days while fresh silks are present. For best results, sprays should be based on trap counts and temperature. (See Table 2 for spray intervals.) Since there is more risk of damage, spray intervals are shortened as the earworm population increases. Intervals are also shortened as the temperature increases, because unexposed silks are growing faster and because the insecticides break down more rapidly.\nThe choice of insecticide is very important in controlling earworms, which have developed resistance to some insecticides in the carbamate family (such as Sevin and Furadan) and are no longer controlled by these chemicals. They can be controlled by synthetic pyrethroids; currently available products are listed in OMAFRA Publication 363, Vegetable Production Recommendations.\nSince egg-laying occurs at dusk and since pyrethroids are more effective in cool temperatures, it is best to spray in the evening. Aim the nozzles at the ear zone and ensure that the silks are well covered.\nFor more information:\nToll Free: 1-877-424-1300"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:abde58d4-e219-4410-b5f6-5373d8c466bd>","<urn:uuid:14173ac3-a740-42cf-b1ab-72feaa5590ed>"],"error":null}
{"question":"How do the visual storytelling techniques in 'Numbered' and 'Beijing Spring' differ in their approach to documenting historical events?","answer":"'Numbered' employs a distinctive visual style combining color footage with black-and-white still portraits, close-up shots of survivors, and captures the actual process of photography, including photo flashes and shutter clicks. It deliberately avoids traditional narrative structure and historical explanations. 'Beijing Spring', on the other hand, relies heavily on recovered 16mm archival footage from 1978-1979, combined with testimonials from artists and democracy leaders who participated in the Democracy Wall movement. The film uses this preserved footage to reveal officially censored history through the perspectives of those who experienced it firsthand.","context":["Three years ago, an elderly patient came into an emergency room in northern Israel complaining about chest pain. When a young doctor, Dana Doron, examined her, the woman pulled up her sleeve, pointed to a tattooed number on her arm and asked, “Do you know what is it?” She then proceeded to talk for an hour. The next day, the woman’s daughter came in to apologize. “Each time that my mother wants to talk about her number, she goes to an emergency room and complains about chest pain,” she said.\nThis experience made Doron consider how Holocaust survivors with tattooed numbers think and talk (or don’t talk) about this physical mark of the past. Her thoughts led to a desire to commemorate the survivors, and then to capture them on camera — to create images that will become, as she says, “a time capsule.” Doron asked her friend Uriel Sinai, an Israeli photojournalist, to take pictures of survivors with their numbers exposed. He agreed, but it soon became clear to both of them that what they needed were not just images, but also the stories and the relationships that the survivors shared with them. Sinai got out his video camera and started filming. The result is a beautiful and moving film — “Numbered” — that premiered in July at the Jerusalem International Film Festival.\nThe film opens with a quotation from Primo Levi: “With time, my tattoo has become a part of my body. I do not display and do not hide it. I show it unwillingly to those who ask out of curiosity, readily and with anger to those who say they are incredulous. Often, young people ask me why I don’t have it erased. This surprises me: Why should I? There are not many of us in the world to bear this witness.” Through its remarkable visual poetry, “Numbered” lets other, lesser-known survivors’ voices tell us their stories.\nThe documentary provides no linear narrative and no historical explanations. Instead, the film intercuts stories of several survivors, told by them and their families. Handsome, cocky Daniel ran away from the Nazi bullets in the ghetto. Since then, he has considered himself “superior” — because he owes his life only to himself. Artistic, thoughtful Gita protected her children from her grief, singing to them enchanting songs of her native Macedonia, as she was crying silently inside. Zoka, a shopaholic, uses clothes to protect herself from the memories of cruelty and the enduring sorrow. These and other people kept their numbers. They incorporated them into their present as a reminder of a painful but also victorious past. For them, the mere fact of their survival, and then of finding love and raising families after Auschwitz, is a victory. Their love stories are the most moving aspect of the film.\nOthers chose to erase the numbers. In the early days of the State of Israel, those with tattoos on their arms faced a terrible stigma. Ruth was a journalist who felt that the moment people saw her number, they knew (or rather assumed they knew) more about her than she did about them, so she had her tattoo surgically removed. Another survivor kept his tattoo, but in a lifetime in Israel he never wore a shirt with short sleeves.\nSurvivors didn’t have a choice about their tattooed numbers; those were forced on them in the camps, often with pain and violence. But today, their children and grandchildren, the second- and third-generation survivors, are choosing to be marked with a tattoo, replicating the numbers on their bodies. A middle-aged woman who just lost her father recalls that his number was incorporated into the very fabric of their life: The family used it for lock codes and passwords; the number became part of everyday routine. When the woman’s father passed away, she had his number tattooed on her leg. A young grandson of Abramo, a survivor from Thessaloniki, had his beloved grandfather’s number tattooed on his arm, in the exact same place as Abramo. “Is this so that they don’t forget?” the old man asks him. “So that I’ll never forget,” his grandson answers.\nThe film is stunning for the physicality of its images. Often, an image of a survivor fills up the whole screen, with only darkness for a background. The camera does not shy away from close-ups of the old faces, old bodies, old arms; the sight of human flesh is overwhelming. Through Sinai’s remarkable cinematography, people on screen become almost palpably present. Black-and-white still portraits of survivors punctuate the colored moving footage. Not only do we see the stills, but we also witness the very act of photography, when survivors pose for the camera to tell their stories. And then — a burst of a photoflash, a click of a shutter, and a stark image freezes on screen.\nSinai explained why they use still photography instead of voiceover: “We didn’t want a plot, a traditional narrative, so the fact that I am photographing the survivors on screen was the organizing principle of the film.” The physicality and the embodiment of images were so important for Sinai that for this project, he set aside his digital equipment and took pictures on actual film, “so that something physical and concrete will remain as a result of our work.”\nBut the film is more than just a physical monument. Through its deliberate and emphatic depiction of the filmmakers in the act of creating tangible memories for the future, “Numbered” joins the new generation of Holocaust documentaries that explore how the event might survive through generations. Along with other recent Israeli films, such as “Don’t Touch My Holocaust,” “A Film Unfinished” and “A Documentary About the Holocaust,” “Numbered” does more than tell survivors’ stories — it also pushes us to rethink the very nature of memory and the possibilities of representing national trauma and personal pain.\nOlga Gershenson is associate professor of Judaic and Near Eastern studies at the University of Massachusetts Amherst. Her new book about the Holocaust in Soviet films is forthcoming from Rutgers University Press.","The International Festival of Films on Art (FIFA) is pleased to announce the opening film for its 39th edition, which will take place online March 16 to March 28. The film is Andy Cohen and Gaylen Ross’s Beijing Spring in its North American premiere.\nAfter the incredible discovery of unpublished archives that had escaped the Chinese government’s censors for more than nearly four decades, the two American directors take viewers into a slice of Chinese history: between November 1978 and December 1979, the Democracy Wall, a brick wall, stood in the centre of Beijing west of Tian'anmen Square. Onto the wall were glued handwritten posters (dazibao) made by artists to disseminate appeals, poems, and artworks, and to open political discussions. This collective action took place after the end of the Maoist period and the defeat of the bloodthirsty Chinese leaders known as the Gang of Four, who were arrested and stripped of their positions soon after the death of Mao Zedong. The movement that surged around this site was a burst of spontaneous freedom. It drew large crowds, who organized improvised meetings.\n“The choice of Beijing Spring to open the 39th edition of the Festival was obvious: it’s an extremely powerful, exceptional, and timely film,” stated Philippe U. del Drago, executive and artistic director of FIFA. “It offers a strong demonstration that artists are a pillar of society, representing guardrails against dictatorship. The Chinese artists in this film are admirable for their courage and their talent. This documentary is an ode to freedom and resistance at a time when Asia is undergoing many political convulsions and struggles for democracy, whether in Hong Kong, Taiwan, North Korea, or Myanmar, whose peoples aspire to feel the wind of liberty. Art is an act of expression but also a call to action.”\nDrilling down into the political context of the time, this feature-length film traces a fascinating art movement launched after a long period of censorship. Accompanied by the moving testimonials of appealing and eloquent artists, including Ma Desheng, Shao Fei, Wang Keping, Bao Kun, Yan Li, Huang Rui, Li Shuang, Li Xianting, and Zhou Xu, who carried the torch for freedom, this fascinating documentary immerses us in a pivotal period in Chinese cultural history. Preserved in recently discovered 16 mm images that were hidden from the authorities for 35 years, this officially censored history is now revealed by those who lived through it – democracy leaders, poets, photographers, and artists – some of whom were imprisoned for 20 years.\nNORTH AMERICAN PREMIEREThe presentation of Beijing Spring is a North American premiere, in collaboration with Amnistie internationale Canada francophone.\nOnline on lefifa.com, Tuesday, March 16, at 7 pm. Available in replay throughout the festival starting the day after the premiere.\nFilm presented in Chinese, French, and English, with English subtitles.\nDuration : 100 minutes.\nABOUT THE DIRECTORS\nANDREW “ANDY” COHEN is an American independent filmmaker and journalist who is devoted to social development and protection of human rights. He has produced, directed, and written a number of award-winning documentaries, including Dealers among Dealers (1996), Killing Kasztner (2008), Ai Weiwei: Never Sorry (2012), The World Before Her (2012), Hooligan Sparrow (2016), Human Flow (2017), Ximei (2019), and Beijing Spring (2020). His films have been presented at FIFDH Geneva, Movies That Matter, the Venice Film Festival, Telluride, Tribeca, Traverse City, the Toronto International Film Festival, the Berlin Film Festival, and the Sundance Film Festival, among others, and broadcast on PBS, BBC, UK Ch4, ARTE, Netflix, and Amazon. Cohen has also written and directed a series of nine short films on important Chinese contemporary artists in collaboration with the magazine ArtAsiaPacific, of which he is one of the contributing editors.\nGAYLEN ROSS has produced, directed and written award-winning documentary films for over 25 years, premiering in national and international film festivals. Among the films she directed, Killing Kasztner: The Jew Who Dealt With Nazis, was an official selection of the Toronto International Film Festival, broadcast on BBC Storyville in the UK, and shown in 11 countries and has enjoyed an unprecedented theatrical release in the US, Israel and France. It was named by the LA Times as one of the best documentary films of 2010. Her documentary Dealers Among Dealers which aired in the United States on PBS and in Canada on CBC, was the official selection of the Berlin Film Festivaland awarded a Gold Plaque from the Chicago Film Festival. She produced and wrote the Emmy award-winning Blood Money: Switzerland’s Nazi Gold. a feature-length documentary on the Swiss Banks and the Holocaust accounts, a selection of the Berlinale, and broadcast on A&E Television. Currently with Andy Cohen she is directing and producing the feature documentary film on the legendary music and theater cabaret of New York’s Greenwich Village, The Village Gate.\nFIFA Passport 39The FIFA 39 Passport grants access to the full program of the 39th edition of FIFA, from March 17 to March 28, 2021. Please note that the opening film Beijing Spring will be available on March 16, 2021 starting 7:00 pm, and that the closing film will be available only starting 7:00 pm on March 28, 2021.\nBox office: http://bit.ly/39thboxoffice"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:5022d170-bdff-4dfa-aa8e-b685ec82a031>","<urn:uuid:47a4c1bb-848c-4386-a2d3-c25e82196d3b>"],"error":null}
{"question":"As an environmental researcher, could you compare the water usage between the circular economy approach in the food industry and traditional agricultural irrigation?","answer":"The traditional agricultural irrigation system accounts for about 70% of global freshwater withdrawal, creating a significant strain on water resources. In contrast, the circular economy approach in the food industry focuses on using and managing resources effectively while minimizing waste. It emphasizes shorter supply chains, recycling processes (such as producing starch from wastewater), and more efficient water use systems. This model aims to transform the current imbalanced system where the food industry consumes more resources than it creates.","context":["Over the last 30 years, factors such as global population growth, shifting demographics, and continual change in consumer demands have fundamentally transformed the food industry. Sadly, with a degrading impact on natural resources and the climate. As the worlds largest industry, food is currently creating an imbalance by taking up more resources than it’s creating. In order to transform this system into a more sustainable and modern approach, the way how food is grown and produced needs to change. However, this system by itself is incredibly complex and also interlinked with other industries such as health, production, energy, and climate. The required resources to sustain the food industry are vast: 50% of the planet’s habitable land and 70% of freshwater demand is taken up by agriculture.\nThis is where the principle of circular economy comes into play, where all resources must be used and managed effectively and waste production is kept at a minimum.\nA circular economy is relying on industries being restorative and regenerative in structure while carrying out system-wide innovations. Essentially, for the food industry, it should aim to redefine products and services in a way that focuses on reducing waste and minimizes the negative impact on the environment. Radical innovation will play an important role in building entirely different designs, from process to materials, such as artificial protein and 3D food printing. These R&D projects are already taking place across the globe, indicating the food industry is shifting its design to efficiently use, re-use and recycle its resources.\nFood production is a cross-sector activity. Businesses need to collaborate and coordinate across all different sectors to reduce waste, greenhouse effects, and fossil fuel use. The current production model that is being used to produce and distribute most of our food is not using current resources effectively and consequently creates a number of problems like waste. The industrialization of food has increased worldwide food production, but this came at a price. The food industry should embrace circular characteristics such as establishing shorter supply chains between the farmers and retailers or even consumers. This to reduce waste associated with transport, and increase local urban resilience in food production.\nThe global food distribution network is huge, with established long value chains of stakeholders. Food distributors can offer products that could be simply re-used,-cycled, but with a shorter shelf-life. By separating supply chain distributions into two streams, agricultural and urban, the distribution circle can be shortened to maximize efficiency.\nBy making more conscious food choices, the consumer can help steer towards a circular economy. Through a change in consumer behavior, consumers can have a strong impact on the food industry. Consumers are already starting to impact and create a new supply-demand model that shifts to a more food-conscious one. This can also lead to the acceptance of new food production technologies that are more environmentally friendly.\nFood producers are responsible to create a model that enables for optimal re-usability of their products or excess products as a result of the production process. Processes such as packaging can be adjusted to allow for either a limitation is excess waste or material used, or alternatively, the re-usability of the packaging can be considered and optimized.\nRecycling is another important driver for the circular design economy. A well-established, cost-efficient recycling process turns all waste material into reusable goods. For example, starch can be produced from wastewater, orange peels can be used for brewers, stale bread for fermentation, and also glass, cardboard and PET recycling are just a few of the possibilities within the immense potential of recycling in the food industry and the impact on the environment.\nWaste prevention is valued more highly than using energy from waste. There are a lot of initiatives being done but in essence, none of them contribute to the fight against waste but these seldom just treat the symptoms without actionable results. There is a serious need for mapping the results of food waste and to create actionable plans on how to prevent these from happening, in the long term. Clearer insights in for example quality management and food tracking & tracking can effectively prevent food wastage and help environmentally friendly options.\nBy 2050, the Dutch government is planning to switch to a fully circular economy. In order to achieve this, several ministries will need to cooperate. With the food industry covering multiple supply chains, the awareness of this new economical method will impact stakeholders at all levels. The situation of the planet’s environment makes an incredibly strong case for change and to realize this vision at a full scale. It is very likely similar initiatives will be set in motion across different national governments, forcing the local food businesses to adapt as well.","of water use as demand for agricultural, industrial, and domestic uses continues to increase.6 11. Appropriate water pricing is important for improved water demand and conservation of water (UNESCO 2001d, Pimentel et al. Water sustains Oklahoma's agricultural and energy sectors, manufacturing, and its citizens. It was developed in response to rapid population growth, drought conditions from climate change, and the overall increased demand for water. Department of Agriculture and Consumer Services; agricultural water conservation and agricultural water supply planning. This chapter is intended to provide basic, conservative, water demand design criteria which may be used in the absence of any more appropriate information. Production of crops and livestock is waterâintensive, and We use a new, gridded, globalâscale water balance model to estimate interannual variability in global irrigation water demand arising from climate data sets and uncertainties arising from agricultural and climate data sets.  Agricultural water use accounts for around 70% of the total water that is withdrawn from surface water and groundwater. The climate in the interior region is quite diverse. Water use for agriculture will continue to be an important factor in water supply planning in Florida. The major constraints facing agricultural activity are the opportunity cost of land, scarcity of water resources, and high labour costs. By 2050, the global water demand of agriculture is estimated to increase by a further 19% due to irrigational needs. To incorporate the climatic diversity, climate layers were developed for the entire region on a 500 m x 500 m grid. The objective of the FSAID planning process is to identify potential future agricultural water demands to Agriculture water demand modeling has been addressed mainly by an economical perspective, namely, estimating the water's economical value [Howitt, 1995; Berger, 2001; Fisher et al., 2002; MedellínâAzuara et al., 2012]. The sufficient supply of clean water is essential to public health, ecosystem function, and economic growth. Countries like India, China and Australia, among others are battling the challenge of producing food for an ever-increasing population amid droughts and increasingly volatile climate conditions . Much of the water utilized by crops comes in the form of rain and imposes no net claim on water that could be put to other uses. That includes Californiaâs Westlands Water District, the nationâs largest agricultural water district. Agricultural operations can also negatively affect water quality. Rising demand for energy and for food will increase future demand for water supplies, presenting important challenges for the future. Various crop diseases also affect the demand for water because growth does not take place in diseased plants whereas they need regular supply of water. Box 31, Holetta, Ethiopia Birhanu Zemadim International Crops Research Institute for the Semi- Arid Tropics (ICRISAT), ICRISAT Bamako ABSTRACT The water demand and irrigation requirement of Holetta Catchment is not fully studied. 4. Agriculture Water Demand Model â Report for Cowichan Valley Regional District June 2013 11 Climate Information The agricultural water demand is calculated using climate, crop, irrigation system and soil information data. The relatively high cost of treating and delivering water has led many world governments to subsidize water for agricultural and household use. Since 1962, demand for Oklahoma's water resources has increased due to the expansion of irrigation agriculture, population growth, and diversification of its economy. Accounting for 70% of all water extracted for human purposes, agriculture is by far the biggest water user worldwide. The preferred criteria â¦ Three options are available for simulating water use for agriculture: (1) user-specified demands, (2) demands â¦ Thus, admixture of type of agriculture and sources of irrigation directly affect the demand for water. Prior to the statutory requirements for FDACS to prepare projections, Floridaâs five Water Management Districts prepared estimates independently. Holetta Agricultural Research Center, P.O. During periods of water shortage, domestic use, industry, and the environment generally take precedence. The main cause of this mismatch on a world-scale is due to agricultural irrigation, which accounts for about 70% of freshwater withdrawal. But demand for industrial users and domestic use is now increasing more rapidly. Wilson Walker reports. Water, agriculture and climate change. Agriculture Water Demand Model â Report for Squamish-Lillooet Regional District April 2018 11 Climate Information The agricultural water demand is calculated using climate, crop, irrigation system and soil information data. This report reviews the main linkages between climate change, water and agriculture as a means to identifying and discussing adaptation strategies for better use and conservation of water resources. increasing water demand â will only heighten concerns about the reliability of future supplies for irrigated agriculture. Agriculture is the largest consumer of the worldâs freshwater resources, and more than one-quarter of the energy used globally is expended on food production and supply. 97% of the water on the Earth is salt water and only three percent is fresh water; slightly over two thirds of this is frozen in glaciers and polar ice caps. The 2019 report is the sixth, annual update of FSAID water use estimates. The climate generally gets cooler and wetter Water resources are natural resources of water that are potentially useful. The first priority set out by the executive order is increasing dam storage and other water storage, long a demand of farmers and farm interests in the West in particular. Agriculture is the biggest consumer of freshwater in the world, amounting to up to 70% of the total use , which makes the case for smart water management in order to guarantee water and food security to the worldâs population. While most of the worldâs water will continue to be used in agriculture, industrial and household demand is expected to rise at a faster rate as the agriculture sector becomes more water efficient. (1/8/18) The agriculture water demand study has been conducted on behalf of the Regional District of Central Kootenay to help identify the current and potential agricultural demand for water within the Erickson water service area boundary. CHAPTER 5 WATER DEMAND REQUIREMENTS A fundamental consideration for the sizing of any water system, or its component parts, is an estimate of the amount of water expected to be used by the customers on the system. Agriculture Water Demand Model â Report for the City of Kamloops May 2016 11 Climate Information The agricultural water demand is calculated using climate, crop, irrigation system and soil information data. Globally, there is an imbalance between water availability and water demand. Water Supply and Demand Agriculture plays an important part in Californiaâs economy and irrigation water is an essential factor in agricultureâs success. Can solar powered desalination meet water demand for agriculture? The inextricable linkages between these critical domains require a suitably integrated approach to ensuring water and food security, and sustainable agriculture and energy production worldwide. At the same time, waste water treatment plants discharge large volumes of water, hence diminishing the quality of the limited fresh water resources. Net water requirement (NWR) and the risk of changes in water demand volume of crops (e.g., wheat, barley, alfalfa, soybean, feed corn, forage, potato, and walnut orchards) are computed by entering 200 monthly time series of downscaled temperature and precipitation in future periods. Agriculture Water Demand Model â Report for Regional District of Central Kootenay June 2017 6 Background The Agriculture Water Demand Model (AWDM) was developed in the Okanagan Watershed. 2004). Quality of irrigation water used in crops, agricultural practices, and soil preparation methods also affect demand for water. However, California faces serious water supply issues, in which agricultural uses must compete with environmental uses and the demands of a growing population. The development of agriculture in Malta is constrained by the natural and geographical characteristics of the islands. Demand for water increasing dramatically in all major use sectors The demand for water originates from four main sources, namely, agriculture, production of energy, industrial uses and human consumption. Agricultural; Industrial; Domestic; Total; Figure 1: Past and projected water demand in million cubic metres (MCM) for the years 1990, 2000 and 2025. The climate in the interior region is quite diverse. demand for fresh water for agricultural purposes as discussed below. In agriculture, water must be of suitable quality to irrigate crops or provide drinking water for livestock. Irrigation systems and field application methods for the cultivation of crops play an important role therein. Water and agriculture involve two key interrelated issues: Water quality is a measure of the suitability of water for a particular use based on selected physical, chemical, or biological characteristics. The AG Package uses pre-existing hydrologic simulation provided by MODFLOW and GSFLOW. Agriculture is by far the largest consumer of the Earthâs available freshwater: 70% of âblue waterâ withdrawals from watercourses and groundwater are for agricultural usage, three times more than 50 years ago. Inch by inch, a critical, gravity-fed canal system is collapsing in California's Central Valley. All living things require water to grow and reproduce. Uses of water include agricultural, industrial, household, recreational and environmental activities. Traditionally, the largest demand for water comes from agriculture, around 70%. Water demand is expected to increase over the next 30 years. April 20, 2019 - by Agriculture Post - Leave a Comment. Demand for biofuels and fiber drives the demand for agricultural products further and hence increases pressure on agricultural land & water. Agricultural Irrigation Demand project, or FSAID. Water for use in power generation is also an important source of demand. The climate generally gets cooler and wetter from south to north and as elevation increases. AGRICULTURAL WATER DEMAND Agricultureâs share of the GDP for Malta is about 2.5 percent. Modeling the demand for fresh water All crops require water to grow. The Agricultural Water Use (AG) Package was developed for simulating demand-driven and supply-constrained agricultural water use in MODFLOW and GSFLOW models. Climate change, water and agriculture: towards resilient systems / OECD Studies on Water, 2014. For example, the agricultural demand for water increased from 1,152 MCM in 1990 to 1,546 MCM 2011 and, consequently, supply from groundwater aquifers increased from 899 MCM in 1990 to 1,269 MCM in 2011. It is unlikely that natural supplies will be sufficient to meet that demand in some parts of the world. The bulk of non-domestic consumption relates to the water used for agriculture, occasionally delivered from integral water supply systems, and for industry and other commercial uses (shops, offices, schools, hospitals, etc.)."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"sensitive"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:dc92db5a-6825-43fa-bc7c-a6676d17ce6a>","<urn:uuid:424ef85d-edf2-49d6-908f-af26868544d0>"],"error":null}
{"question":"How do stockholders and bondholders differ in their relationship to a company's debt-to-equity ratio?","answer":"Stockholders and bondholders have different relationships to a company's debt-to-equity ratio. Stockholders are owners of the company whose equity is measured against the company's debt in the ratio, while bondholders are lenders to the company whose loans constitute part of the debt. This ratio is particularly important to bondholders and bankers who use it alongside other measures like profitability and cash flow to assess lending risk and make lending decisions.","context":["A debt-to-equity ratio is an assessment of the financial leverage for a business it compares the company's total assets to its owners' equity company shareholders and potential creditors are among the stakeholders interested in your ratio. The equity ratio highlights two important financial concepts of a solvent and sustainable business the first component shows how much of the total company assets are owned outright by the investors in other words, after all of the liabilities are paid off, the investors will end up with the remaining assets. The debt-to-equity ratio (d/e) is a financial ratio indicating the relative proportion of shareholders' equity and debt used to finance a company's assets. Take a look at the important debt-to-equity ratio, a key metric of financial leverage, and learn what the average debt/equity ratio is for banks. Raising capital: equity vs debt jill hamburg coplan there's also a shift in thinking about a company's optimal debt-to-equity ratio, or its level of debt compared to shareholder equity .\nIt is important to note that there are many ways to calculate the debt-to-equity ratio, and therefore it is important to be clear about what types of debt and equity are being used when comparing debt-to-equity ratios. Important lending ratios to pre-qualify commercial credit this is an important ratio debt to asset ratio and debt to equity ratio are the old favorites. The debt-to-equity ratio can help business managers understand the status of their debt to equity, so that they can make knowledgeable decisions about important financial strategies for their company.\nWhy does industry view the debt-to-equity ratio as important investors may find it useful to compare the debt to equity ratio of a company compared to that of the industry and comparative companies this helps to provide a relative gauge of how the company is perceived from a risk perspective. Is it interest-coverage ratio is it debt-service-coverage ratio the most important commercial loan underwriting ratio that there is the leverage ratio is . Learn about the maximum acceptable debt to equity ratio, what it means about a company's capital structure and why the optimal ratio can vary so widely take a look at the important debt-to . Financial statement ratio analysis bankers before granting loans total equity the debt/equity ratio is an important ratio in that it provides a measure of the. Debt-to-income ratio, which is often abbreviated as dti and refers to how much debt you have in comparison to your income this is an important number for lenders because it can help determine your ability to pay back your debts .\nLiquidity is important to a company because when times are the debt-to-equity ratio measures the relationship between the amount of capital that has been . Learn why debt-to-income ratio is important and how to lower it with this better money habits article home equity mortgage basics mortgage is a ratio that . The long term debt to equity ratio is the same concept as the normal debt to equity ratio, but it uses a company’s long term debt instead like the other version of this ratio, it helps express the riskiness of a company and its leverage. Before granting a loan, bankers closely evaluate an organization’s debt-to-equity ratio explain why this ratio is so important to bankers bankers will look at the debt-to-equity ratio closely because it will show them the value and of how the organization is growing. Why is the capital structure of a company, like the debt-to-equity ratio, important to investors if a company's debt to equity ratio is greater than 1:1, does this mean that shareholder's equity is negative.\nThe debt-to-equity ratio, is a quantification of a firm’s financial leverage estimated by dividing the total liabilities by stockholders’ equity this ratio indicates the proportion of equity and debt used by the company to finance its assets. Debt-to-equity ratio, often referred to as gearing ratio, is the proportion of debt financing in an organization relative to its equity it is therefore important . This isn’t always straightforward but checking recent debt ratio trends can be a good indicator telus corp’s debt to equity ratio it’s important to . “bankers, in particular, love the debt-to-equity ratio and use it in conjunction with other measures, like profitability and cash flow, to decide whether to lend you money,” explains knight.\nLearn how to calculate the debt to equity ratio and why it is important 09 once you know how to calculate the return on equity financial ratio mentioned above . Is it important to know about debt equity ratio here’s why | apnacourse by n raja about debt equity ratio by bankers are 3:1: vinu: ohdebt of 3/4=75% . What is debt equity ratio & its significance by mirae asset knowledge academy what is 'debt equityratio' the debt-to-equity ratio (d/e) indicates the proportion of the company’s assets that are being financed.\nWhat are the merits of the following financial ratios: profitability ratio liquidity ratio debt ratio asset activity ratio market values ratio what do these financial ratios tell us about a firm why is it important to. Are ratios that bankers and bondholders want similar if so why yes ratios like debt-to-equity ratio = long-term debt/total equity is similar for bankers and bond holders because they both are lenders.\nWhy is the debt equity ratio important to bankers debt/equity ratio what does debt/equity ratio mean a measure of a company's financial leverage calculated by dividing its total liabilities by its stockholders' equity it indicates what proportion of equity and debt the company is using to finance its assets. The debt to equity ratio when you apply for a loan or line of credit, this will be one of the first ratios that your banker will look at in essence, it tells the bank how much money you have invested into your business, for every dollar of debt that lenders have put in. In this lesson we learn how vigilant leaders manage debt with the debt to equity ratio and the current ratio current ratio is another very important key ratio to .","Difference Between Stocks vs Bonds\nIt is often seen that both the terms ‘stocks vs bonds’ are used simultaneously and interchangeably making us think that both are the same in the investment world. But both are significantly different but complement each other. Both are very much good options to be kept in the investment portfolio as they offer different benefits in different types of market environments.\n- Stocks help in having ownership in a business enterprise. When a company sells its stocks then it is selling a part of the company in exchange for cash. In simple words, stocks can be also understood as shares of individual companies. When a company thinks of expanding but is unable to do so with the income it is earning through its operations, it takes the help of financial markets for additional financing.\n- The company can split it up into shares and then sell these shares in the open market. So basically a person who buys a stock is having an actual share of the company. For this reason, the stock is also known as Equity. Stockholders are paid dividends only if the company declares a dividend. The biggest corporations trade their stocks on stock exchanges. Stocks are either publicly or privately issued. If it is publicly issued then it is traded on stock exchanges like NASDAQ. When stocks are privately issued then it is held by a small group of individuals having a substantial percentage of ownership.\nBonds mean long-term debt. When the government, corporation needs to have cash then it borrows money from the public market and then pays interest on the money raised to the investors. The issuing corporation of bonds makes a promise of paying the principal amount at a specific date. Bonds issued pay interest to the bondholders. As per a fixed contract, a fixed interest payment has to be made after specific intervals generally every six months. Bigger corporations may trade their bonds in the bond market. A corporation issues bonds to invest in plant and equipment or acquisition of another business. The government issues bonds generally to raise financing for capital improvement projects or other obligations.\nTypes of Stocks and Bonds\nBelow are the different types of stocks and bonds that are as follows:\n- Common Stock – This kind of stock gives general ownership in the company. The common stockholders can elect and vote but in case of liquidation, they come much after bondholders and preferred shareholders.\n- Preferred Stock – Shareholders under this category don’t have voting rights but are eligible to get dividends before common stockholders. They get fixed dividend payments.\n- Growth Stock- This stock invests its profits in helping to grow the company. This stock may not pay a dividend or may offer a very small dividend.\n- Dividend Stock – These types of stockholders are given much of the company’s profits as dividends. It may offer some capital appreciation but the main focus is the dividend yield.\n- Value Stock- These are the stocks that are taken out of favor by the general investing public.\n- Convertible bonds – These are corporate bonds but there is a provision to convert them into company stocks.\n- High-yield bonds – These bonds also known as junk bonds pay higher interest rates and are issued by issuers with low credit ratings.\n- Foreign Bonds – These are issued by foreign governments and corporations. Investors invest in these since they pay higher interest rates than domestic bonds.\n- Municipal Bonds – These types of bonds are issued by states, countries, and municipalities. The interest paid is tax-free.\n- S. Government bonds – These are the debt obligations of the US government and are known as treasuries. These are generally issued for terms of 20 and 30 years.\nHead to Head Comparisons Between Stocks vs Bonds ( Infographics)\nBelow is the top 8 Difference between Stocks vs Bonds\nKey Differences between Stocks vs Bonds\nlet us discuss some of the major Differences Between Stocks vs Bonds:\n- Stocks are financial assets issued by a company and have ownership rights. Bonds are long-term debt instruments issued to raise capital with a promise of payback of the principal along with interest.\n- Stocks are equity instruments and bonds are debt instruments.\n- The stocks give returns known as dividends while bonds give interest. Return on stocks is not guaranteed but there is a guarantee on return from bonds.\n- Stocks are riskier than bonds.\n- The stock market has a centralized trading system whereas bond is traded over the counter.\n- Stockholders are owners of the company while bondholders are lenders to the company.\nStocks vs Bonds Comparison table\nBelow is the Comparison table between Stocks vs Bonds\n|The Basis of Comparison||\n|Definition||These are a financial instrument which gives ownership interest and is issued by the company in exchange for cash.||The debt instrument issued by companies or governments to raise capital along with the promise of payback after fixed time with interest.|\n|Issuance||Issued by companies.||Issued by the government, financial institutions or companies.|\n|Instrument||This is an Equity instrument.||This is a Debt instrument.|\n|Type of Return||Gives a dividend.||Gives interest.|\n|Guarantee of Return||No guarantee.||Guaranteed.|\n|Risk Levels||High risk.||Low risk.|\n|Add on benefits||Stockholders have voting rights.||Bondholders get preference at the time of repayment.|\n|Ownership||Stockholders. Stockholders are owners of the company.||Bondholders. Bondholders are lenders to the company.|\nBoth stocks vs bonds are good ways of raising capital from the market and are very useful financial instruments. A well-balanced portfolio has both bonds and stocks and proper allocation can help in maximizing growth and minimizing risk.\nThis has been a guide to the top difference between stocks vs bonds. Here we also discuss the stocks vs bonds key differences with infographics, and a comparison table. You may also have a look at the following articles –"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:cfeb77d9-a6fb-4f35-8178-cbb6987a2107>","<urn:uuid:a2a2bce1-534d-4dae-b41c-5aaaa7e1acf8>"],"error":null}
{"question":"How do current reconstructive orthopedic surgeries attach muscles and tendons, and what is the main problem with this method?","answer":"Current reconstructive orthopedic surgeries use sutures to attach muscles and tendons. This leads to poor surgical outcomes because the sutures have a limited ability to transmit the muscle's forces and movement to the tendons.","context":["Mentor: Dr. Julie A. Adams\nThis project seeks to develop new methods for adapting interaction between a human and a robot in a one-to-one partnership relationship based on the human's cognitive workload. The human dons various wearable sensors that provide a data stream that is analyzed to detect high and low workload conditions. Once a high or low workload condition is detected, the robot can adapt its interaction method with the human. REU students will work to develop new algorithms for detecting changes in workload and new algorithms for adapting the robot's interactive capabilities based on the workload detection. The project will require algorithm design, implementation and software testing. Further, REU students will evaluate the algorithms with actual human-robot teams.\nHumans have no trouble picking up and manipulating objects, yet we've struggled to impart that ability to robotic manipulators. This is in part because robotic manipulators lack much of the sensory feedback human hands have, but it's also because we, as humans, are not very good about reasoning about what we do instinctively. The goal of this project is to develop tools and user studies that will let us capture that knowledge and apply it to robotic manipulation tasks.\nDesign of Implants for Attaching Muscle and Tendons to Improve Human Hand Function\nMentor: Dr. Ravi Balasubramanian\nCurrent reconstructive orthopedic surgeries use sutures to attach muscles and tendons. However, this leads to poor surgical outcomes because of the suture’s limited ability to transmit the muscle’s forces and movement to the tendons. It is expected that using passive implants, such as pulleys and rods, to surgically construct mechanisms in situ using the existing biological tendons will significantly improve post-surgery function (when compared to using sutures) and lead to the development of new surgical procedures.\nExample student projects:\nMentor: Dr. Ross Hatton\nSpiders use vibrations in their webs to sense the presence of other insects, similarly to how whales, bats, and submarines use sonar to hunt their prey. In collaboration with a biology team, we're building and instrumenting a giant spider web to examine how this works.\nGeometry of Locomotion\nMentor: Dr. Ross Hatton\nThe effectiveness and efficiency with which robots move through the world is strongly affected by their geometry, including both the physical geometry of their bodies and the abstract geometry of the patterns in which they move their joints. This project is about exploring these geometric effects, and generating fundamental understanding about how robots (especially snakes and other crawling robots) exploit them. Depending on your interests, this project can range from hands-on experiments that generate data for geometric modeling to delving deeper into the mathematics behind locomotion.\nExample project: Snake scales have more friction for sideways motion than forward motion, and we have good geometric models for how they exploit this difference in friction. They also have more friction for backwards motion than sideways or forward, but we don't have a geometric model for when this difference matters or how they exploit it. This project would include building a simple snake robot with different forward-backward-sideways friction coefficients, testing its motion with different shape-change cycles, and identifying patterns in the resulting motion.\nLearning from Humans for Robotic Deburring\nBurrs are undesirable projections of material at the edges of a finished part’s surface. They pose a fundamental problem for manufacturing operations since they affect part handling and assembly operations and lead to part failure. Deburring, the process of removing burrs, is currently performed by human operators. But deburring is labor-intensive and causes injury to operators due to the forces and movements involved. This project seeks to learn the deburring process from a human (including forces and movements involved) and develop a robot to automate the deburring process.\nExample student projects:\nAutonomy for Underwater and Surface Vehicles Exploring Ocean Environments\nThere are many ocean environments that are unsuitable for manned research vessels, either because they are too dangerous (e.g., near a calving glacier or in the deep ocean), or require too many resources to be effective. We seek to design a new generation of autonomous underwater and surface vehicles that can be programmed to autonomously measure ocean dynamics in a wide range of environments. This research project involves the design and programing of robust surface and underwater systems that will be used to explore ocean dynamics along the pacific coast, Greenland, Alaska, and in remote ocean basins. The REU student will join an interdisciplinary team of researchers from mechanical engineering, computer science, and oceanography to assist in designing autonomy algorithms and programming marine vehicles at Oregon State University.\nPassive Dynamics and Applied Control for Legged Locomotion\nStudents on this project will participate in research with our human-scale walking and running robot Cassie. We are primarily focused at this time on controls and perception for Cassie; interns will work closely with graduate students to test new control ideas with the robot both in the lab and outside, read data from our LIDAR, cameras, and IR camera, implement a VR environment for telepresence control, or other interesting and fun projects. This is not a great introduction to these topics, it will be a better fit for an intern seeking a challenge.\nExample Student Projects:\nControl of a self-driving wheelchair\nMentor: Dr. Bill Smart\nWe are working on a self-driving powered wheelchair, for use by people with severe motor disabilities, such as Amyotrophic Lateral Sclerosis (ALS, or Lou Gehrig's disease) and quadriplegia. The goal is to develop a low-cost package that can be added to a traditional powered wheelchair, and turn it into a self-driving system. Self-driving capabilities would afford the wheelchair user more independence, and allow them to move about their world more easily and efficiently.\nThe project leverages the Robot Operating System (ROS) software for navigation and localization of the system, with special-purpose code for wheelchair-specific features. Our goal is to release both the hardware design and software for the system under an open-source license.Our goal is two-fold: understand how people think about privacy and how that is the same (or different) when it is a robot and not a person, and develop algorithms that lets us \"try out\" various methods for preserving privacy.\nRobots Against Ebola\nMentor: Dr. Bill Smart\nCan we use robotics and automation in the fight against highly-infectious diseases, such as Ebola Virus Disease? We are working with Doctors Without Borders, a non-governmental organization that organized much of the response to the recent outbreak of Ebola in West Africa, to see how we can improve the quality of care that they can deliver by using robotics and automation. Ebola thrives in environments that are hot, humid, and remote; often there is little in the way of reliable infrastructure. This project is focused on practical solutions that will let us improve the quality of care that can be delivered to people with the disease and, ultimately, to reduce the mortality rate in future outbreaks. We are building mathematical models of the tasks that health care workers do, using optimization techniques to reorder these tasks and subtasks to make them more efficient and effective, and looking at how we can use robotics and automation in support of the human workers.\nMentor: Dr. Kagan Tumer\nMany interesting real world problems require multiple robots to work together. For example, search and rescue missions require coordinating dozens of autonomous robots, as well as ensuring that the robots and humans work together. But providing hard-coded coordination instructions is too limiting. This project explores the science of coordination, and focuses on how to provide incentives to individual robots so that they work collectively.\nBat Sonar for Robots\nBat sonar is an incredibly rich sense - bats can spot and catch insects in the air, fly through dense forest canopy - all by emitting chirps and listening for echos. They accomplish this feat through deformable ear and nose geometries that let them \"shape\" the sound scape. In this project we aim to build a soft, deformable bat ear and nose in order to mount such a sensor on the robot."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:0d0b4cd3-4814-4c56-a5a0-0f7ef6b19eca>"],"error":null}
{"question":"¿Cuál es el lifecycle del Hoplia beetle y el lesser cornstalk borer? Can you compare them?","answer":"Hoplia beetles have one generation per year. Adults emerge from soil in mid-March to May, mate on host flowers, females lay eggs in soil, larvae feed on plant roots, overwinter as larvae, and pupate in spring. Lesser cornstalk borers have 2 generations per year. They overwinter as larvae or pupae, adults emerge in spring, females lay about 200 eggs underground, larvae mature in 20 days through 5-9 instars, pupation takes 7-13 days, and adults live about 10 days.","context":["Hoplia beetles are close relatives of the dreaded Japanese and Chinese Beetles. They are day flyers and prefer the flowers of white to whitish flowers. The adults are small about 1/4 inch to 3/8 inch long and somewhat squarish and dorsally flattened in shape. Their ventral surface are covered with silvery and coppery colored scales while the elytra and dorsal surface is brownish to reddish brown in color. Silvery scales are also found on the dorsal surface giving the dorsal surface a mottle appearance on various shades of brown. There is only one generation per year with the adults emerging from mid March through May in areas from Palm Springs and Cathedral City in southern California to Redding in northern California.\nThere are several species of Hoplia beetles throughout the United States. The western species, Hoplia callipyge LeConte, is also known as the grapevine hoplia and it appears to be the only species of this group that attacks roses. According to Essig (1926:445) this beetle also feeds on the young leaves, flowers, and fruit of grapes, peaches, and almonds and on the flowers of ceanothus, greasewood, calla lilly, California poppy, magnolia, lupine, olive, orange, and willow, and on the leaves of beans, horse beans, peas, and vetches. On roses the beetles feed on the petals of opening white and light colored roses by chewing small holes and leaving the petals with a lacey appearance. In my garden, they prefer the white flowers of Sally Holmes, Popcorn, Irresistible, Grace Seward, Class Act, and the Apricot Nectar, a peach colored rose.\nLIFE CYCLE: Adult beetles emerge from the soil in as early as mid March in southern California and they can be common from late April through mid May in norhtern California. They are present for only about a month in any one area of California depending on the weather. Mating orgies take place on host flowers. The gravid females oviposit in the soil near host plants and upon hatching the larvae burrow in the soil and feed on the host plant roots. They overwinter as a larvae and continues development in the spring. The full grown larva or grub is white. Pupation occurs in the spring. The hoplia beetle has only one generation per year.\nCONTROL: Scouting or monitoring for the presence of hoplia beetles is very important for adequate control. Adults are readily attracted to many light colored roses and other flowers. Light colored roses should be inspected from mid March through May for skeletonized petalsthrough July, the foliage should be inspected for skeletonized rose petals and for the presence of adult beetles tucked inside the flowers between the rose petals.\nHoplia beetles can be handpicked and destroyed if the infestations are light. They can also be handpicked and drowned by dropping them into a bottle filled with soapy water. Satisfactory control can be achieved with the application of insecticides such as carbaryl (Sevin), acephate (Orthene), diazinon, and chlorpyrifos (Dursban), Merit, etc. Please read the insecticide label throroughly and as some insecticides such as diazinon can cause phytotoxic reactions to new foliage if applied when the weather temperatures are above 80 degrees F.\nEssig, E. O. 1926. Insects of Western North America. The MacMillan Company, NY, p. 445.\nJohnson, Warren T. and Howard H. Lyon. 1988. Insects that Feed on Trees and Shrubs, 2nd edition, Cornell University Press, Ithaca, NY., pp. 236-37.\nDreistadt, S. H. 2001. Integrated Pest management for Floriculture and Nurseries. UC Statewide IPM Project. Division of Agriculture and Natural Resources, Publication 3402, 422pp.\nIf you have any questions or constructive comments, I would love to hear from you, please send e-mail to Baldo Villegas\n[ Home ]\n[ Rose Pests Master List]\n[ Rose Diseases ]\n[ Insect, Mites, and other\nAnimal Pests ] [ Abiotic\nRose Problems ]\n[ Weed Pests ] [ Hawaii Pests ] [ The Good Guys ] [ Help! Rose IPM ] [ Horticulture Links ]\nCopyright© 1995-2002 by Baldo\nLast updated: June 3, 2002","About the Lesser Cornstalk Borer\nReproduction and Life Cycle\nThe lesser cornstalk borer has 2 generations per year. This pest overwinters as larvae or pupae in the ground. When they emerge as adults in the spring, they will mate and the female will lay her eggs underground beside host plants. Sometimes; however, they are placed on leaves, stems, or on the soils surface. A female can lay around 200 eggs over her lifespan, sometimes more. The eggs will hatch within 3 days. Larvae live in the ground, creating silken tunnels that they remain in, except for when they feed off of the plants stalk and stems. The larvae will mature within 20 days. On average they go through 6 instars, but this can be modified to 5 or up to 9, contingent on climate conditions. The mature larvae will then enter the pupal stage of development, which takes on average 10 days to complete, but can range anywhere from 7 to 13 days. When the adult emerges, their lifespan is approximately 10 days.\nLesser Cornstalk Borer Identification and Habitat\nThis pest as an adult will develop a wingspan of about 17 to 22mm in length. You can easily distinguish the males from the females. The male’s forewing is generally yellow-brown with a band that is darker than the body on the inner margin and has purple scales. The females are black with purple or red scales at the forewing’s base. The thorax is also darker on the females. At rest, wings are held close to the body on their back. The adults tend to be the most active during the evening as long as temperatures are above 80 degrees Fahrenheit, and especially when it is humid, as these conditions are ideal for oviposition and mating. The eggs are only about 0.6mm long and 0.4mm wide. They are oval and start off as a green colour, but will later change to pink and then red. Once they have matured, the larvae are typically 16mm in length. During this stage, they are extremely active and will wiggle aggressively when disrupted. When they first hatch, larvae are yellow-green with red pigmentation on their back. Later in their development they will have white stripes running lengthwise down their body (mainly noticed during the fifth instar). Fully mature larvae will be a blue-green or red-brown with yellow-white stripes on its back. Their heads are black. When they enter their pupal cells, they will be about 8mm long. They construct their pupae out of silk and sand, creating the cocoon which is 16mm long. They start as a yellowish colour and will then turn brown and black right before the adult emerges.\nThe lesser cornstalk borer is common in the western hemisphere, mainly in the southern areas of the United States. They tend to prefer plants that grow in poor soil types, especially those that are sandy. They are attracted to grasses, and a variety of crops, mainly corn. They thrive in hot, dry weather. These warmer temperatures can lead to higher cornstalk borer populations since eggs are laid at a quicker rate in that type of condition. If there are more than 10 days in the summer that have temperatures of at least 95 degrees Fahrenheit with precipitation less than 2.5mm, an outbreak is very possible. If there are 5 to 9 days with the above mentioned conditions, there is still a possibility of an outbreak and lesser cornstalk borers should be scouted for in your fields. The larvae will tunnel into the plants stem and feed from within, whether above or below ground. The plant will often be stunted, will wilt, and buds sometimes wither. This can eventually lead to the plants death. Another way to tell that this pest has infested your crops is silken webbing that forms a tube by the stalks base in the soil.\nLesser Cornstalk Borer Management and Control Methods\nFor preventative measures, ensure you are practicing clean cultivation. Keep weeds along the fields edges to a minimum. Plowing in the fall can help kill any borers who are overwintering. Early planting can also keep their population in check. Always make sure weeds are removed from the field before planting. However, it is important to note that conservation tillage can also lessen plant injury because when crop residue remains at the soils surface, the larvae will sometimes feed on this instead of the newly seeded plants. Keeping the soil moist through irrigation can also be a useful deterrent for the females from laying their eggs. You can plant RIB (refuge in a bag) corn seed; this seed helps in the protection from corn pests while still having 5% seed that has not been treated with Bt in order to keep this method effective, as that 5% refuge seed does not allow the pest to grow immune to this treatment plan. Not a lot of natural enemies have been overly successful at controlling the lesser cornstalk borer. Note that having satisfactory control is often dependent on using an integrated management system that includes the use of cultural and chemical methods.\nGranular insecticide types are often used to control this pest. They are typically applied in the seed furrow. Liquid pesticides can work as well, but must be aimed at the plants roots. In the past, chlorpyrifos, chlorantraniliprote and bifentrhin have been helpful in suppressing the cornstalk borer. Be sure to carefully read the label for cautions and proper application. It is important to never spray on days that are windy. After applying insecticides, it is important to irrigate sprayed area to increase the insect control. That being said, a large rainfall or irrigation soon after the application can reduce the concentration insecticides.\nLatin / Alternative Lesser Cornstalk Borer Names\n- - Elasmopalpus lignosellus\n- - Jumping borer"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:8904de2e-6b19-4607-bd56-c440cb0f0a11>","<urn:uuid:3945e810-b920-4c64-a3e5-e2d23fc5b12a>"],"error":null}
{"question":"Compare the precision requirements for radial dead zones in gaming controllers versus MetaHuman Creator's facial feature customization - list the key differences.","answer":"Radial dead zones require specific numerical precision, typically ranging from 0.1 for Xbox 360 controllers to 0.25 for OUYA controllers, and focus on preventing unwanted input from loose thumbsticks. In contrast, MetaHuman Creator's facial feature customization offers much more granular precision through an extensive set of parameters. While dead zones deal with basic input values between -1 and +1, MetaHuman Creator provides detailed control over numerous facial features, allowing for minute adjustments to create photorealistic characters. The software uses advanced algorithms and machine learning to ensure precise control over every aspect of the character's appearance, from facial structure to detailed elements like blemishes and freckles.","context":["As OUYA Kickstarter backers begin receiving their dev units, I’ve seen several discussions pop up about thumbstick dead zones. Unfortunately most of the advice I’ve seen is pretty bad, so I thought I’d share some simple techniques I’ve learned over the last six years working on major PS3 titles Warhawk and Starhawk.\n(Note: The following code samples are in C# and based on Unity, but the basic principle should be clear enough to adapt to whatever language/API you’re working within.)\nWhat’s A Dead Zone?\nSkip this section if you already know. For the rest of you, here’s a quick primer!\nAnalog thumbsticks typically send input to your code in the form of two numbers: one for the X (horizontal) axis, and one for the Y (vertical) axis. Usually the number ranges from −1 (fully extended one direction) to +1 (fully extended the opposite direction), where 0 is dead-center. The assumption is that if you’re not touching the stick, it’ll return (0, 0).\nIn reality, though, thumbsticks vary in quality and wear out over time. You’ve probably used a gamepad at some point that had a loose or “wiggly” stick; in that case, the neutral position is just a little bit off from (0, 0), even though you’re not touching the stick. To your code, that’s indistinguishable from the player pushing the stick just a tiny, tiny bit.\nDead zones are simply a minimum input threshold, often somewhere between 0.1 to 0.2. If the input received from the stick is smaller than that, it’s ignored.\nHave you ever played a game where the camera moved or rotated very slowly of its own accord, even though you weren’t touching the stick at all? That’s a case of a missing (or too-small) dead zone. (Curiously, I see this issue in a lot of Xbox 360 first-person shooters.)\nSo to sum up: dead zones prevent unexpected input from loose thumbsticks, which makes players happy. :)\nThe Naïve Way — Axial Dead Zone\nOkay, let’s start with a look at the naïve implementation of a dead zone. This is the method everyone jumps to first, because it’s the most immediately intuitive:\nfloat deadzone = 0.25f; Vector2 stickInput = new Vector2(Input.GetAxis(“Horizontal”), Input.GetAxis(“Vertical”)); if(Mathf.Abs(stickInput.x) < deadzone) stickInput.x = 0.0f; if(Mathf.Abs(stickInput.y) < deadzone) stickInput.y = 0.0f;\nSimple enough: if our input magnitude in either direction is less than our dead zone, we simply zero out the input in that direction, and that’s all there is to it… right?\nWell, here’s a diagram of what this kind of dead zone looks like. The circle represents rotation space of the thumbstick (it’s the same as circular opening in your controller that the stick is seated in), and the red shaded area represents where the dead zone will kick in and cancel out your input:\nIn practice, this implementation feels very bad, and you’ll notice it whenever you try to rotate the stick in a sweeping motion (which is a really common gesture in first-person shooters). What happens is, as you rotate the stick across one of the cardinal directions — anywhere within the red shaded area — you’ll feel it “snap” to the cardinal. If you’re making a game that’s all about 2D four-directional movement (maybe a Bomberman clone or something) then that’s great, but for anything requiring analog precision (like a first-person or twin-stick shooter) this is nowhere near accurate enough.\nA Better Way — Radial Dead Zone\nFortunately it’s really easy to get rid of the cardinal-direction snap. We simply test the magnitude of the entire input vector, rather than testing each axis separately:\nfloat deadzone = 0.25f; Vector2 stickInput = new Vector2(Input.GetAxis(“Horizontal”), Input.GetAxis(“Vertical”)); if(stickInput.magnitude < deadzone) stickInput = Vector2.zero;\nThis is much better. For many games, you could probably ship with this; in fact, this method is the most common one I’ve seen people propose recently. Here’s what that dead zone looks like on the stick:\nWhen we think about dead zones, this is usually the kind of thing we’re envisioning: a very small area in the center of the stick within which input is ignored. The size of the area is simply our best guess at how far a loose, worn-out stick is likely to wiggle on its own, without physical input.\nThe High-Precision Problem\nIf you’re making a first- or third-person shooter, odds are you need all the input precision you can get. The previous method covers you for large movements, but you’ll find a flaw when you try to make very fine, low-magnitude adjustments (like aiming a sniper rifle). As you slowly push the stick away from neutral, you’ll feel the edge of the dead zone as your aim suddenly “kicks” into motion. This doesn’t feel smooth, and can make high-precision gameplay feel extremely tedious in a way that can be hard to define.\nThe problem with the previous method is that it’s clipping the input vector below the dead zone, which means all the precision that exists inside the dead zone is completely lost. In other words, you can’t smoothly ramp your input from 0 to +1 any more; instead you snap from 0 to +0.2 (or whatever your dead zone is), and then you ramp from +0.2 to +1.\nHere’s an illustration:\nThe gradient indicates the strength of the resulting input (after the dead zone is applied). Note that the edge of the dead zone is clearly visible: as you push the stick away from the center, the gradient value changes suddenly, not smoothly, at that edge.\nThe Right Way — Scaled Radial Dead Zone\nFortunately, the high-precision problem is also very easy to fix. We just need to rescale the clipped input vector into the non-dead zone space:\nfloat deadzone = 0.25f; Vector2 stickInput = new Vector2(Input.GetAxis(“Horizontal”), Input.GetAxis(“Vertical”)); if(stickInput.magnitude < deadzone) stickInput = Vector2.zero; else stickInput = stickInput.normalized * ((stickInput.magnitude - deadzone) / (1 - deadzone));\nHere’s what the adjusted dead zone looks like:\nNotice that there’s no longer a visible edge: as you push the stick away from the center, the gradient value changes smoothly while the dead zone is still preserved. This feels buttery-smooth, just as God intended. ;)\nDead Zones For Fun and Profit\nI called it the “right” way but that doesn’t mean you’ll never use any other method, ever. The most important thing is to use the method that makes sense for your particular project. Here are a few scenarios:\n- Tile-based (4-way) movement: The Axial Dead Zone actually works well here since it snaps analog input to the only four input vectors that are actually relevant.\n- Twin-stick shooter: In these games the magnitude of input rarely matters — all you care about is direction — so the simple Radial Dead Zone should be perfectly suitable here.\n- Super-polished FPS: Sometimes you need to sweep your aim through a line, and keep the crosshair on or close to the line. In this case you might want to blend a Scaled Radial with a modified Axial Dead Zone, such that the stronger your input in one axis, the larger the dead zone gets for the other axis. (At LightBox we called this the “bowtie” because the dead zone diagram looks like… a bowtie. I’ll leave the implementation of this one as an exercise for the reader!)\nNow go forth and implement your dead zones properly! It’s easy, and your players will appreciate it. :)\nP.S. For what it’s worth, I’ve noticed that the OUYA controller seems to require a larger dead zone than the Xbox 360 controller. I had to go up as high as 0.25 to get a new, unworn OUYA controller to sit reliably at neutral, while a new, unworn Xbox 360 controller was fine around 0.1.\n(Oh and by the way: I'm available for contract work doing the sort of things you just read about. Hit me up if you're interested!)Posted In:","How To Drop Things In Roblox? Roblox is a massively popular online platform where players can create and play games. One of the key features of Roblox …Read Article\nEpic’s free MetaHuman Creator software is pretty amazing\nEpic Games, the creators of the popular game engine Unreal Engine, have introduced an incredible new software called MetaHuman Creator. This groundbreaking tool allows users to easily create realistic human characters with astonishing detail and customization options.\nTable Of Contents\nWith MetaHuman Creator, game developers, filmmakers, and designers can bring their characters to life in a way never before possible. The software utilizes advanced algorithms and machine learning to generate lifelike virtual humans, complete with facial animations, hair, clothing, and even unique personality traits.\nThe level of realism achieved with MetaHuman Creator is truly impressive. Users can customize every aspect of their characters, from facial features and body proportions to clothing styles and hairstyles. The software even allows for the creation of diverse characters with various ethnicities, ages, and body types.\nWhat sets MetaHuman Creator apart from other character creation tools is its ease of use. Users can simply select the desired features and adjust them using intuitive sliders and controls. For those who wish to delve deeper, the software also offers more advanced options, allowing for fine-tuning and precise adjustments.\nWhether you’re a professional in the gaming, film, or animation industry, or simply an enthusiast looking to unleash your creativity, Epic’s MetaHuman Creator is a game-changer. It opens up new possibilities and unleashes the potential for creating truly immersive and engaging experiences. The future of character creation has arrived, and it’s more realistic and customizable than ever before.\nEpic’s Free MetaHuman Creator Software: An Amazing Innovation\nEpic’s MetaHuman Creator software is a groundbreaking innovation in the world of computer graphics and character creation. With this free software, users can easily create photorealistic human characters in a matter of minutes, eliminating the lengthy and complex process that was previously required.\nThis innovative software utilizes cutting-edge technology to generate highly realistic characters with unprecedented detail. Users have the ability to customize each aspect of their character’s appearance, including facial features, hair, clothing, and more. The software’s intuitive interface makes it accessible to both professionals and beginners alike, opening up a world of possibilities for character creation.\nOne of the key features of MetaHuman Creator is its extensive library of pre-built components. Users can choose from a wide variety of pre-designed heads, bodies, and facial expressions, allowing for quick and easy character creation. The software also includes a range of advanced tools for fine-tuning the character’s appearance, ensuring that each creation is unique and personalized.\nAnother impressive feature of MetaHuman Creator is its ability to generate characters with a high level of animation fidelity. The software incorporates advanced rigging and animation systems, enabling users to create characters that can be easily animated and brought to life. This opens up new possibilities for game developers, filmmakers, and other creatives who rely on realistic character animations in their projects.\nIn addition to its powerful character creation capabilities, MetaHuman Creator also offers seamless integration with other popular industry tools. Users can easily export their characters to other software such as Unreal Engine, Autodesk Maya, and more, making it a versatile solution for a wide range of creative workflows.\nEpic’s MetaHuman Creator software is a game-changer in the field of computer graphics. Its user-friendly interface, extensive customization options, and advanced animation capabilities make it an invaluable tool for artists and designers. Whether you’re creating characters for a video game, film, or any other form of media, this amazing innovation will undoubtedly streamline your workflow and bring your visions to life.\nRevolutionizing the World of Character Creation\nCharacter creation has always been a crucial part of various industries such as video games, films, and advertising. It is a painstaking process that requires skilled artists, extensive time, and a meticulous attention to detail. However, Epic’s Free MetaHuman Creator software is set to revolutionize the way characters are created.\nWith the MetaHuman Creator, artists and designers gain access to a powerful tool that simplifies the character creation process. It offers a wide range of realistic and customizable human models that can be easily modified to meet individual requirements. The software generates highly detailed characters with realistic facial expressions, hair, and clothing.\nOne of the key features of the MetaHuman Creator is its intuitive interface. Artists can easily navigate through the software and make adjustments in real-time. The drag-and-drop functionality allows users to modify various aspects of the character, such as facial features, body proportions, and even unique details like scars and tattoos.\nAnother advantage of the MetaHuman Creator is its extensive library of pre-designed assets. Users can choose from a wide range of hairstyles, clothing options, and accessories to further customize their characters. This not only saves time but also ensures consistency and enhances the overall design quality.\nFurthermore, the MetaHuman Creator incorporates advanced technologies such as Unreal Engine’s MetaHuman framework. This enables the software to create characters that possess realistic movements, facial expressions, and even emotions. The characters can be animated easily, making them suitable for use in video games, films, and other forms of media.\nIn addition to its features, the MetaHuman Creator offers a seamless integration with other software and tools commonly used in character creation pipelines. This allows artists to utilize the software as part of their existing workflows, ensuring a smooth transition and efficient production process.\nOverall, Epic’s Free MetaHuman Creator software is set to revolutionize the world of character creation. It provides artists and designers with a powerful and user-friendly tool that simplifies the creation process while delivering highly realistic and customizable characters. With its advanced technologies and extensive library of assets, it is sure to become a game-changer in the industry.\nUnparalleled Realism and Customization\nOne of the most remarkable features of Epic’s Free MetaHuman Creator software is its unparalleled realism and customization options. The software’s advanced technology and cutting-edge graphics deliver virtual characters with an astonishing level of realism. Whether it’s the facial features, hair, or clothing, each element is meticulously designed to appear highly lifelike.\nThe MetaHuman Creator offers an extensive library of realistic presets, which serve as a starting point for creating unique characters. Users can choose from a wide range of ethnicities, ages, and body types, allowing for representation and inclusivity in character creation. The software also provides detailed control over minute details like blemishes, scars, and freckles to add depth and character to the virtual individuals.\nCustomization options in the software are unparalleled, empowering users to create characters that truly embody their vision. Users can modify parameters such as height, weight, facial structure, and even more specific features such as eye shape and nose size. The software’s user-friendly interface allows for easy manipulation of sliders and buttons, enabling precise adjustments in real-time.\nFurthermore, the MetaHuman Creator allows for infinite possibilities in character customization through the use of accessories and clothing options. From trendy outfits to formal attire, users can dress their characters to fit any narrative or style. The software also supports the addition of accessories like glasses, hats, and jewelry, providing further opportunities for personalization.\nThe software’s rendering capabilities further enhance the realism by utilizing groundbreaking technologies like subsurface scattering and strand-based hair. The characters created with MetaHuman Creator display convincing skin textures, realistic hair physics, and dynamic facial expressions, bringing them to life on screen.\nAdditionally, the software’s built-in animation system enables users to give their characters a range of movements and expressions. From simple gestures and emotions to complex interactions, the characters can be animated to suit various scenarios and storytelling needs. This versatility adds another layer of realism and authenticity to the virtual individuals created with the MetaHuman Creator.\nIn conclusion, Epic’s Free MetaHuman Creator software provides an unprecedented level of realism and customization options. With its advanced technology and user-friendly interface, users can create virtual characters that resemble real people in an astonishingly accurate manner. The software’s extensive library of presets, customization parameters, and clothing options allow for infinite possibilities in character creation. Combined with its rendering capabilities and animation system, the MetaHuman Creator represents a major innovation in digital character creation.\nStreamlined Workflow for Professionals\nThe MetaHuman Creator software by Epic Games offers a streamlined workflow that is specifically designed for professionals in the entertainment industry. With its intuitive user interface and powerful features, it allows artists, designers, and developers to easily create high-quality digital human characters.\nThe software provides a comprehensive set of tools and options that enable professionals to efficiently model, customize, and animate realistic human characters. Artists can easily sculpt the facial features, adjust the body proportions, and fine-tune every detail to achieve the desired look and feel.\nOne of the standout features of MetaHuman Creator is its extensive library of pre-made assets. This includes a wide range of hairstyles, clothing options, and accessories that professionals can choose from. These assets can be easily customized to match the specific requirements of the project, saving a significant amount of time and effort.\nFurthermore, the software offers a variety of animation controls and rigging options, allowing professionals to create lifelike movements and expressions. They can create custom animations or use the built-in animation library to add realistic gestures, facial expressions, and body movements to their characters.\nThe streamlined workflow of MetaHuman Creator also extends to the collaboration process. The software allows multiple professionals to work on the same project simultaneously, facilitating efficient teamwork and ensuring a smooth production process. Artists can easily share their creations with others and receive feedback in real-time, making it easier to iterate and refine their characters.\nOverall, the streamlined workflow offered by MetaHuman Creator enables professionals to create stunning digital human characters in a fraction of the time it would take with traditional methods. Its intuitive interface, powerful features, and collaborative capabilities make it an invaluable tool for professionals in the entertainment industry.\nAccess to Cutting-Edge Technology for Everyone\nWith the introduction of Epic’s Free MetaHuman Creator software, access to cutting-edge technology is now available to everyone. This revolutionary software allows users to create highly realistic digital humans easily and efficiently, without the need for specialized skills or expensive equipment.\nWhether you are a game developer, filmmaker, or designer, the MetaHuman Creator provides an unprecedented level of realism and detail, offering endless possibilities for creative expression. The software’s advanced facial and body modeling capabilities, combined with a vast library of pre-built components, allow users to craft unique characters with astonishing precision.\nOne of the most impressive features of the MetaHuman Creator is its ability to generate a wide range of diverse human characters. With just a few clicks, users can adjust various parameters such as age, ethnicity, and gender to create lifelike individuals that accurately reflect the diversity of real-life populations.\nFurthermore, the software’s intuitive user interface makes it accessible to users of all skill levels. Whether you are a seasoned professional or just starting out, the MetaHuman Creator offers a streamlined and user-friendly experience, allowing you to focus on your creative vision rather than technical hurdles.\nIn addition to its ease of use, the MetaHuman Creator also provides a high level of customization. Users can fine-tune every aspect of their characters, from facial features and hairstyles to clothing and accessories. With this level of control, you can bring your imagination to life and create characters that stand out from the crowd.\nMoreover, Epic’s Free MetaHuman Creator software is not only accessible to professionals but also to students, hobbyists, and enthusiasts. This democratization of cutting-edge technology opens up new possibilities and encourages innovation within the creative community. By removing barriers to entry, Epic is empowering individuals from all backgrounds to unleash their creativity and bring their ideas to life.\nIn conclusion, Epic’s Free MetaHuman Creator software provides access to cutting-edge technology for everyone. Its intuitive interface, advanced features, and extensive customization options make it a game-changer in the world of digital character creation. Whether you are a professional or an amateur, this software empowers you to create realistic and unique characters, turning your imagination into reality.\nWhat is Epic’s MetaHuman Creator software?\nEpic’s MetaHuman Creator software is a revolutionary tool that allows users to easily create high-fidelity, realistic digital humans. It provides a library of pre-built characters that can be customized using a wide range of parameters and features.\nHow does MetaHuman Creator work?\nMetaHuman Creator uses a combination of advanced scanning technology and machine learning algorithms to generate detailed 3D models of human faces. These models can then be customized to create unique characters with various hairstyles, facial features, and other characteristics.\nWhat are the benefits of using MetaHuman Creator?\nMetaHuman Creator offers several benefits for developers and creators. It saves time and resources by providing pre-built character assets that can be easily modified. It also allows for a high level of customization, ensuring that each character is unique and fits the desired aesthetic. Additionally, MetaHuman Creator simplifies the process of creating realistic digital humans, which can be time-consuming and technically challenging.\nCan MetaHuman Creator be used in different industries?\nYes, MetaHuman Creator has applications in a variety of industries. It can be used in film and television production, video game development, virtual reality experiences, and even in architectural visualization. Essentially, any industry that requires the creation of realistic human characters can benefit from this software.\n- Shadow Legends Mobile: Learn How to Sell Items in the Game\n- How to Unlock the Saint George Armor in Assassin's Creed Valhalla\n- Top Brawler for Dealing Maximum Damage in Brawl Stars 2022\n- How To Check Fortnite Stats: A Step-by-Step Guide\n- How to Evolve Nosepass in Pokemon Go: Step-by-Step Guide\n- Get Your Own Leon's Jacket in Resident Evil 4 Remake: The Ultimate Fan Merchandise\n- Discover the Most Rare Pokemon Card and Its Value"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:02fd66f8-745c-40a5-87eb-005bde07bec7>","<urn:uuid:62186940-e8ee-4840-8c6b-7035ace4237a>"],"error":null}
{"question":"What's the connection between Narayana and Vajra Pani at Japanese temple entrances? What do they symbolize?","answer":"At Japanese temple entrances, Narayana (Vishnu) keeps his mouth open while Vajra Pani (Shiva) keeps his closed. This symbolizes that the deities swallow all evil and let only virtue pass into the temple. Another interpretation is that they symbolize all the secrets of the universe from beginning to end. Their placement represents AUM, where the open mouth (A) symbolizes the beginning of creation through Brahma, and the closed mouth (M) represents Shiva, the destroyer and end of existence.","context":["The 900 year old Sanjusangen-do Zen temple in Kyoto\nSanjusangen-do (Built in 1164 CE and reconstructed in 1266 after a fire broke out)\nis a Buddhist temple in Kyoto that's guarded by around 1000- Armed Human sized\nstatues of Kannon, the Japanese Goddess of compassion. 500 statues sit on either\nside of the main Deity of Sahasrabhuja-Arya-Avalokiteśvara.\nThe 1000 armed Kannons\nThe 1000 life size Kannons carved from wood, have 28 guardian deities in front,\nthat are larger in size. These Guardian deities are of Indian origin and primarily\nIn ancient Japanese Buddhism, Hindu Deities are revered and given their place of\nrespect in the Buddhist realm of deities. The Hindu deities are considered guardian\nDeities of the Buddhist concept of the ‘Clear Mind Buddha’. Which according\nto them is the Buddha that is of prime importance. The physical Buddha is seen as\nholy and revered, but he is also seen as an example of human bondage and suffering\nin samsara. Like a normal human Buddha had to overcome his delusions and finally\nhad to shed the body. The ‘Clear Mind Buddha’ who is pristine and pure\nin thought (Nirgun as the hindus say) however, is the primordial state without beginning\nor end and is the state which all humans must aspire to achieve to get salvation\nfrom rebirth. One has to not get obsessed with the physical Buddha, as he is external\nto you, while the ‘Clear Mind Buddha’ is the object of possible achievement\nfor all humans through meditation and practice of dharma. This realization which\narises from within is considered more important than bondage to external objects,\nhowever revered or holy they are.\nThis ‘Clear Mind Buddha’ as the silent observer, is similar to the\nconcept of the ‘Nirgun Bhraman or Paramatma’ in Hinduism, with all other\ngods/goddesses being considered its emanations with specific energies, which is\nvery similar to the Shinto concepts of Kami. The beautiful Harmony with which the\nancient Japanese monks have managed to fit all gods and deities from the parent\nstream of Hinduism into its brilliant offshoot Buddhism is remarkable. It is a brilliant\ntestimony to their abilities and vast intelligence to bring harmony, accommodation,\nrespect and deep understanding to the evolving spiritual streams, as well as merger\nof existing streams. After all there is only one truth and no one has the monopoly\non it. This is something the ancient Japanese masters understood so well.\nAn achievement, one has to bow down to in deep respect.\nShintoism the original indigenous religion of Japan, Shinto teaches that everything\nin nature consists of a spiritual essence, or a spirit called a Kami. The Kami resides\nin all things. But there are certain designated places where the Kami interfaces\nmore intensely with people. There are 8 million kami (not literally, but just a\nrepresentation of many Kami that exist). The primary Kami is Amaterasu or the Goddess\nof the sun. Hence Japan is called the land of the rising sun. This is extremely\nsimilar to Hinduism where we are considered Suryavanshi (descendents of the sun).\nIn Hinduism, the entire universe is considered an emanation of Paramatma; hence\neverything in it carries the spirit of the great creator. Lay people get confused\nthat Hinduism has a million deities, and villagers are busy creating one every day.\nWhat an outsider does not see is that Hindus consider everything in nature as having\na spirit that is derived from that one source to manifest. So having a million deities\nis the same as having one. Shintoism too is similar to that form and belief system\nin Hinduism. Ancestors are considered Kami too and just as we worship and follow\nthe system of gotras (descendents of Rishis), they worship their ancestors.\nIn the next few frames , I shall post pictures of some of the critical Hindu “Guardian\nDeities “ that have been exhibited in the Great 120 meter hall, which is 900\nyears old. These Guardian deities stand on either side of the Avalokiteshwara Deity\n(The Buddhist God of compassion) and in front of the 1000 Kannons. The Hindu deities\ndon’t resemble the Indian versions as there were conceptually transplanted\nabout 900 years ago and are carved based on Japanese interpretations of Indian and\nThe descriptions are brief and have been copied from the official booklet available\nat the Sanjusangen-do centre. It is highly recommended for any visitor to Japan\nto visit this center and buy this book.\nJapanese Name: Naraen Kengo, Sanskrit: Narayana\nThe original Sanskrit name of this deity is Narayna, also called Vishnu in India,\nthe Hindu god of preservation of all creation. This god/statue is used in many ancient\nBuddhist temples of Japan at the gates where Narayan keeps his mouth open in conjunction\nwith Vajra Pani (Shiva), who keeps his mouth closed. This according to them symbolizes\nthat the deities swallow all evil and let only virtue pass into the temple through\nthe gates. Another interpretation says they symbolize within lies all the secrets\nof the universe from beginning to end. The most amazing example of these Deities\nis at the entrance to the Todai-ji temple at Nara. Amazingly Nara has 7 prime temples\n(Much like the Tirupathy temple, where the Lord is considered the lord of the seven\nhills which represent the Adisesha or the serpent of Vishnu with 7 heads that is\nsupposed to hold up creation).\nThe placement of Narayna (Vishnu) and Vajra Pani (shiva) in the entrance to symbolize\nAUM is amazing. A (mouth opens to symbolize beginning of all creation from Brahma,\nwho is born of Vishnu), and M (mouth shuts to symbolize the destroyer and therefore\nthe end of existence) is Shiva, the God of death. The Japanese Buddhist integration\nwith Hinduism is breath taking.\nJapanese Name : Raijin the Thunder God, Sanskrit Name : Varuna\nThis deity has its origin in the god of water “Varuna” which later transformed\ninto Thunder God as water was always associated with thunder. The iconography of\nthis statue is based on ‘Senju Darani-kyo’ Buddhist Sutra. As per the\nRig Veda, Varuna is considered the counterpart of Mitra, Varuna rules the night\nand Mitra rules during the day.\nJapanese Name: Basu senin Sanskrit: Vasu\nVasu in Hindu tradition can be interpreted in many ways. Vasudewa was the father\nof Krishna. Related to this name is an early Hindu belief system, sometimes called\nBhagavatism that was largely formed by the 4th century BC.\nVasudeva was worshiped as the supreme Deity in a strongly monotheistic format, where\nVasudewa was considered the Supreme Being\nbecause he had the attributes of being perfect, eternal and full of grace.\nVasu could also mean God of all the elements in creation which is very similar to\nJapanese Shintoism wherein they recognize about 8 million gods of various elements\ncalled Kami. Hinduism on the other hand recognizes 8 primary elements and all else\nare its combinations. That is why Vasu is the god of the 8 gods or the various base\nJapanese Name: Nanda Ryu-o Sanskrit: Nanda Naga Raja\nIf the Sanskrit translation were to be applied directly, it would mean The King\nof the snakes from the Nanda Dewi Mountain (The abode of Lord Shiva). In this carving\nat some point of time the snakes became dragons through Chinese influence and came\nto be in Japan, as Buddhism reached Japan via China and Tibet.\nJapanese Name: Fujin Sanskrit; Vayu\nAs introduced in the ancient Indian sacred texts Rig Veda Vayu is the deity that\npulls carriages through the air, defeating armies, bring fame, fortune. The design\nis completely based on Japanese interpretations of texts from Indian and Chinese\nJapanese Name: Birubakusha Sanskrit: Virupaksha\nThe Japanese translation means the deity with many eyes and a wider vision. Notice\nthe third eye between the two eyes, and the weapon in the hand is the same as Shivas.\nIn Hindu Tradition Virupaksha is a form of Shiva’s and there is indeed a Virupaksha\ntemple in Hampi Karnataka with the same Sanskrit name as in Japan.\nJapanese Name: Karura Sanskrit: Garuda\nThe original Sanskrit name of this deity is Garuda, In ancient India it was believed\nto be a giant bird that ate cobras and carried the Hindu deity Vishnu on its back.\nLater on it was adopted in Buddhism as a deity, and was included in the 8 guardians\nof Buddhism. The statue represents a bird headed figure playing a flute while keeping\ntime with the foot.\nJapanese Name: Mawara-nyo. Sanskrit: Maha Bala\nMaha Bala in Hindu scriptures translates directly to Durga Dewi, based on whom,\nmany Bala mantras exist. Mawara-Nyo represents the indomitable spirit and the gentle\nfeminine subtle energies of the universe, gentle yet decisive.\nJapanese Name: Daibenkudoku-ten Sanskrit: Sridewi\nThe original name for this deity is Sridewi, also called Laxmi, written in India\nas Lakshmi. She is born from the sea and is Vishnus (Narayana’s) wife. In\nBuddhism she is a daughter of the dragon King and Kishimojin (Hariti). As in Hinduism,\nin Buddhism too, she presides over prosperity\nJapanese Name: Taishaku-ten Sanskrit: Indira\nAccording to the ancient Indian writings in the Rig Veda, He is the most significant\nheroic deity. In Buddhism this deity is supposed to live in Kiken castle As the\nlord of the realm of Tory ten. He is considered to have helped Buddha in his novitiate\nyears. In Hinduism Indra is the god of the lesser heavens where ordinary mortals\nreside for limited periods of time in their endless cycles of birth and death.\nThe very best for the last, This is where the Japanese place Hinduism and\nBuddhism in perfect equal footing. Literally as parallel similar universes with\ndifferent tag names.\nJapanese Name : Daibon-ten Sanskrit: Maha Brahman\nThe Highest Hindu god and the creator of the universe is Maha Brahman.\nSince Maha Brahman was adopted into Japanese Buddhism, it is\nbelieved he is a guardian of Buddhism together with the Buddhist Deity\nTaishaku-ten. They both are equally tasked as partners with running\nthe universe. According to the Japanese legend when Buddha reached self- realization,\nhe was overwhelmed but was hesitant to preach to people. Maha Brahman advised him\nto start preaching in order to redeem ignorant people and their souls. It is for\nthis reason the Japanese accord Maha Brahman an equal status to the Buddhist diety\nAbout Author - In search of the ultimate truth beyond concepts and notions, in that\nsilence, after 20 years in soulless corporate board rooms. https://twitter.com/#!/sanjayrao1010\nPictures of Virupaksha Temple Hampi\nGaruda is National symbol of Thailand. Pic no 3 has image.\nIndian Cultural Influence on Japan\n• The Great Buddha Statue in Bodh Gaya ie sponsored by the Japanese.\nBodh Gaya Temple pictures.\n• Rajgir – Shanti Stupa ie sponsored by the Japanese.\n• Japanese Monasteries at Bodh Gaya and many others."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:1b6c44fd-7462-46e2-85f2-560eeaf61fea>"],"error":null}
{"question":"What is the purpose of feedback in learning, and how should trainers implement it in TIBL courses?","answer":"Feedback serves as information about performance or understanding of a task, provided by agents like teachers or peers. It helps keep students moving forward with course content by offering corrective information, alternative strategies, and encouragement. In TIBL courses, trainers must implement feedback as part of their essential responsibilities, including giving positive motivating feedback in forums, reviewing assignments, moderating web-based group work, and evaluating online activities. The feedback must be prepared and available at the course start, covering both trainee and trainer feedback as part of the feedback culture. For effectiveness, feedback should address three key questions: 'Where am I going?' (feed up), 'How am I doing?' (feedback), and 'Where to next?' (feed forward). Additionally, feedback should be timely, focused on specific tasks or processes, and provide clear guidance for improvement rather than just praise or criticism.","context":["This is a short summary of the items relevant for the course implementation. Additionally, an interactive checklist is available to find out, if a course implementation fits to the TIBL concept.\nConsiderations for the Implementation\nThe TIBL Method is based on Blended Learning (as the delivery model of the course). This requires training based on onsite learning (group based) and a Distance Learning part (here called “Distance Training”, realized with the Moodle platform).\nPreparation work – Preconditions\nThe implementation of a course implicates a certain preparation. This must cover\n- Course planning\n- Course design\nIf you are not sure about these items, please read the section in the MOOC or the short explanations in the toolbox description.\nThe preparation work for a TIBL method course must include the setup of the onsite teaching (room(s), facilities, time table), the availability of the material (copies, electronically delivered material), organizational issues (planning of learning groups, use of labs, schedules), assignments and assessments.\nFor the Distance Training all the materials must be available (provided by the learning platform) in a ready-to-use form. This includes the correction of the material, needs the use of approved and pre-tested lessons (or learning units) and requires the delivery in the appropriate way (branding of the material, defined delivery processes, availability of the necessary bandwidth for the delivery, and related items).\nThe Trainer’s Role and Education\nBlended Learning changes the role of the trainer (in the TIBL method, the trainer shifts from the simple “responsible for knowledge transfer and assessment of training results) to a facilitator and coordinator of the training. In the same way the trainees should develop to self-determined learners.\nMake sure that all involved trainers are well-educated to run the course (in the sense of the issues mentioned above). In the TIBL method trainers are the facilitators of the training, they are responsible for a lot of essential tasks:\n- Guide trainees through the course\n- Motivate trainees\n- Supervise the onsite training (for example the moderation of group work, discussion rounds, practical hands-on training)\n- Give (positive motivating) feedback in various cases (for example in forums, answering and reviewing assignments, moderation of web-based group work)\n- Give support to trainees\n- Care for the weaker trainees through the personalised delivery of adapted material and a closer tracking of their performance to detect when and where they need extra help.\n- Evaluation of trainees’ activities (for example online activities in the training platform)\n- Initiate, perform and evaluate various forms of assessments (for example online assessment, or group base onsite assessment, formative as well as summative assessment)\nCourses should provide some way of individualized training. This can be implemented using different ways or approaches. Training paths can be complex and provide several parallel structures – this enables a big number of possibilities and combinations. Each combination of individualized training paths must be tested on consistency and appropriate working. An individualization also is possible using interactive videos (this works by selecting specific items and the video continues at the relevant scene defined with the timeline).\nThis short video gives a short introduction to individualized training.\nVideo: Short overview of simple individualisation (Peter Mazohl), Source: TIBL-Chanel (YouTube)\nControl and check the technical infrastructure that is used for the training. This affects the onsite training (where normally some infrastructure is needed to perform the training) as well as the technology behind the Distance Training.\nFor onsite training in most cases a robust network (today based on WLAN using WiFi) is the standard.\nFor the Distance Training the Moodle server must provide the needed capacity, shortest response times, and a well-working connection to the internet providing the necessary bandwidth for the expected number of trainees and the type and weight of the contents or data to be retrieved.\nUse of Material\nThe material provided for the course should be created following the guidelines as mentioned in the chapter about course creation. This should make it easy for trainees to handle the content, the individual assignments or the group work.\nFor training in a company (employees of this SME) the material used must comply with the corporate identity.\nTrainers should give attention to the use of multiple devices. All material, especially multimedia material, should be tested on all possible devices (differences in technical layout or in the used operating system). Content – delivered by the preferred platform (Moodle for the TIBL method) – normally can be displayed without problems on all devices. Nevertheless, a pilot testing on various devices is mandatory. In the case of problems, these either must be fixed before the course starts or must be mentioned before the course starts.\nThere are free, easy-to-use tools that can be used to check how a particular content will be displayed in different devices (tablets, smartphones, PCs, etc.). A good example of these is QuickTools screenfly, a webpage that – among many other features – can use different device emulators to display a given web content. We strongly recommend using one of these tools in order to design training contents for multiple devices.\nIn certain cases, an “exclude list” (naming devices that cannot be used in the course due to a specific reason) should be given to the participants (at the enrollment to the course). It is good praxis also to mention the reason why this device cannot be used (for example: screen too small, no suitable pointing device available, etc.).\nRecommendation: Some multimedia-based material is available as an executable file only. These types of programs depend on the operating system of the device being used and therefore such programs cannot be used with all devices. These products should be avoided and replaced by web-based material.\nThis content also should be tested on all possible devices including the usability of the multimedia tools.\nRecommendation: Check the usability of the multimedia tool in relation of the screen size and/or the necessary use of a pointing device (Further information can be found in the MOOC, Unit “Use of multiple devices”.\nAll external sources must be checked for availability before the course starts.\nRecommendation: The TIBL concept bases on the use of a MOODLE server as the training platform. All materials should be available there. If you use external sources (for example some specific web 2.0 interactive tool) you should care to make this material to be accessed via the training platform (for example by describing the assignment in the training platform and to put a link there) or by embedding the external material in the course (Two examples: the embedding of a YouTube video or the use of material from a cloud-based repository).\nThe necessary feedback must be prepared and available at the beginning of the course. This should cover\n- Feedback of trainees\nFeedback of trainers\nBoth types of feedback belong to the “feedback culture” and should be used in the evaluation of the course.\nHint: feedback to trainees is a crucial item for successful learning (as described in the training framework).\nRecommendation: If possible, define a feedback based on benchmarking. This means that the people giving feedback compare the course with given descriptions. The result can be used for amendment of the course (for the next edition). Further information is available in the chapter about evaluation (in the toolbox).","What is feedback?\nFeedback is information provided by an agent (e.g. teacher, peer, experience) regarding one’s performance or understanding of a task. An instructor or peer may provide corrective information, alternative strategies, or encouragement. Instruction and feedback run on a continuum -- both are needed to keep moving forward on course content. Providing correctional feedback is a form of instruction, making the two an intertwined process that takes on the form of new instruction, rather than only informing the student about correctness.\nFeedback needs to provide information relating to a specific task or process of learning to take on an instructional purpose (Sadler, 1989). Instructional feedback can be achieved in a variety of ways, such as motivating or engaging students, confirming to students when they are correct or incorrect, reiterating directions, or indicating an alternative approach or line of thinking.\nClearly, feedback can be a powerful tool. Students who receive informative feedback about a task and how to do it more effectively achieve higher learning outcomes than those students who receive praise, rewards, or punishment. Providing cues or reinforcement to learners in the form of audio, video, or computer-assisted instructional feedback is the most effective measure.\nThere are several ways to provide feedback to learners. Instructional designer and co-founder of E-learner Engaged, Anna Sabramowicz, describes some methods for providing learners with positive and effective feedback in the video below.\nThree Feedback Questions\nTeaching information to students involves providing constructive tasks and dialog and assessing and evaluating students’ understanding of this information in order to reach the next stage of teaching, which is done through feedback. Approaching feedback through three main questions can help provide the best feedback to the learner.\n- Where am I going? (Feed up)\n- How am I doing? (Feed-back)\n- Where to go next? (Feed forward)\nWhen both teachers and students seek answers to each of these questions learning goals can be achieved.\nWhere am I going? -- Feed Up: Clarify the Goal\nCritical to feedback is laying the foundation of instruction with learning goals. These provide the learners with a clear purpose or roadmap to attain information related to the task or performance, referred to as “success criteria.” When students understand the learning goal, they are more likely to focus on the task at hand. Outlining the learning goals and activities of how they will reach those goals provides students with the understanding of what the instructor’s expectations are so that students can focus on achieving the goals.\nWith assignments, providing clear expectations and instructions to the students will help them meet the requirements and allows the instructor to align the various assessments. For example, when it's clear that the purpose of a unit is to compare and contrast the branches of government, students know what to expect and the teacher can plan readings, collaborative projects, investigations, and assessments to ensure that students focus on content related to this goal.\nGoals inform individuals of the level of performance that is to be attained so that they can direct and evaluate their actions and efforts accordingly; therefore, feedback allows students to track their performance in relation to their goals and redirect their progress as needed. Feedback for goal-driven activities needs to be direct and students need to share a commitment to attaining them as they will be more likely to seek and receive feedback.\nHow am I doing? -- Feed Back: Respond to Student Work\nThe feed-“back” dimension covers the individual responses given to students about their work. Instructors provide information directly related to the performance task. Feedback is effective when it consists of information about progress and how to proceed next. Students often seek information about how they are doing and crave feedback as a self-regulatory tool to move forward successfully. Feedback can occur in an email, on a discussion post, or after formal testing. An instructor may find that emailing students progress reports can be the olive branch the students needed to ask specific questions on their success or failures of the instructional content.\nFor example, in a unit on writing high-quality introductions, a teacher gives students multiple opportunities to introduce topics using such techniques as beginning with a question or startling statistic, leading off with an anecdote, and so on. The teacher provides students feedback on each introduction they have written so the students revise that introduction and use the suggestions to improve their next attempt. Rather than simply noting mechanical errors, the teacher acknowledged areas of success and highlighted things students might focus on sharpening.\nWhere to go next?\nFeed-Forward: Modify Instruction\nInstruction is often sequential -- meaning instructors provide learning content and activities, students attempt these tasks, and these are then followed by more instruction and tasks. However, the consequence of this sequence is that giving constructive and effective feedback takes too long. Students are moving on to the next learning task before they receive feedback on the previous activity. Feed-“forward” can have some of the most powerful impacts on learning as it tries to enhance challenges, provide more self-regulation of the learning process, and a deeper understanding of what is or is not understood about the information.\nThe feed forward aspect of the formative feedback system is often left out. In an effective feedback system, teachers use assessment data to plan future instruction based on student performance. As the instructor analyzes student work, whether checking for understanding or using formative assessment, they use what they learn to modify their instruction. This demands greater flexibility in lesson planning because it means that teachers can't simply implement a set series of lessons.\nEven adult learners can struggle with content, which can become apparent during an activity, assessment, or discussion board. If there is an issue from multiple learners, it is time to re-evaluate the instructional materials and remediate where necessary. Remediation can occur through several avenues:\n- Redirection in a current discussion board\n- Creation of a new, clearer discussion topic to discuss the misinterpretation of a previous discussion\n- Feedback directly to the students who need it\nSometimes the content is predetermined for the instructor, which can provide students with a consistent experience. Courses are revised based on student and instructor feedback, which can lead to the reorganization of content to clarify any roadblocks along the way. In self-paced courses, feed-forward can be seen through adding clarification in a discussion post, following up directly with a student about the content, or providing personal examples to clarify confusion. It is important to focus on feed-forward as a teaching and learning opportunity for both the instructor and learners.\nIntegrating the Three Questions\nFeed-up, feed-back, and feed-forward work together across for levels of feedback which closes the gap between where students are and where they are aiming to be. The focus of feedback is critical and the level at which feedback is directed influences its effectiveness. The task, the process, self-regulation, and the “self” are the levels feedback can be directed.\nThe Focus of Feedback: The Four Levels\nThe focus of feedback is critical and the level at which feedback is directed influences its effectiveness. The task, the process, self-regulation, and the \"self\" are the levels feedback can be directed.\nAbout the task\nFeedback can be directed towards the correctness or incorrectness of the task or assignment. On this level, feedback may include directions to acquire more, different, or correct information, such as \"You need to include more information about the Gettysburg Address.\"\nFeedback can focus on the process used to create the assignment, which aims more at the processing of information or learning process required to understand or complete the assignment. For example, the instructor may suggest that the learner needs to revisit strategies covered in the materials.\nAiming feedback at the self-regulation level can boost confidence and self-evaluation skills. This feedback can influence self-efficacy and self-regulation, such that the students understand how to more effectively continue with the task. For example, the instructor can remind the student what they already know and direct them to check if they have included it, such as \"You already know the components of a hypothesis, check to see if your introduction paragraph has one.\"\nFeedback can be personal in the sense that it is directed to the \"self,\" which are often unrelated to task performance. This feedback includes \"you're a great student\" and \"that's a great response, well done.\"\nStructuring feedback appropriately is part of what makes it effective. It should reference the goal or objective that needs to be accomplished by the learner with tangible and actionable items that help obtain the ultimate goal. Only telling learners \"good job\" or \"you did that wrong\" does not benefit or help the learner obtain the module objective and move onto the next course goal. While specific and accurate feedback is important, it is not useful to the learner if it overwhelms them or is not understandable. Too much feedback can be counterproductive, whereas focusing on one or two key elements of performance can get the learner to focus.\nProviding students with feedback is only useful to learning when it is timely -- not immediate or delayed. Communicate to students when they can expect feedback on activities and assessments. If feedback is delayed for any reason, it is necessary to communicate this delay with students quickly.\nAs an instructor, it is important to evaluate learner performance as a reflection of teaching. Are there areas where multiple learners are struggling on the same concept? Were the instructions of the task communicated clearly? Could a concept been explained more clearly? Consistent, accurate, and trustworthy feedback helps learners adjust their performance.\nAs stated before, it is important for the instructor to allot enough time to grade items and provide vital feedback to learners. If the instructor falls behind with providing individual feedback to learners, communicate the delay with the learners while reiterating the activity or lesson objective and encourage the learners to keep moving forward with the course content.\nIn short, begin with a description about task performance, follow with guidelines for what to keep doing or change (not just the \"what\" but the \"how), and end with encouragement to persist.\nRubrics as Feedback\nAs mentioned, the importance of the feed-\"up\" step to feedback highlights expectations to learners. Providing learners with a rubric is part of the feed-\"up\" step and is an effective scoring tool that lists criteria and articulates the degree of quality for each criterion. Rubrics can improve learner performance by clarifying expectations and showing learners how to meet these expectations, because the rubric defines the level of quality expected out of them. In turn, this helps learners analyze their own work to self-assess areas of improvement.\nFor the instructor, rubrics save valuable time when evaluating a learner's work. Once items have been self- or peer-assessed, instructors find that the gaps have already been filled in and they can focus on providing more focused and itemized feedback. Also, rubrics provide the instructor with a way to justify grading based on the expectations and criteria outlined within the rubric, which eliminates instructor bias.\nGrading student assignments can be a daunting task, especially if the assessment criteria provided was vague and subjective. Providing students with rubrics, an authentic assessment tool, justifies and defends the grades assigned to a given activity. Authentic assessment focuses on complex and subjective assessments that correlate to real-world experiences, during which, the instructor observes students in process, provides feedback, monitors student use of the feedback, and adjusts instruction and evaluation accordingly.\nRubrics provide a formative type of assessment because it is an ongoing part of the whole teaching and learning process. With rubrics as an instructional tool, students become more focused and self-directed as expectations are laid out as a pathway to success.\nThe advantages of using rubrics in assessment are that they:\n- allow assessment to be more objective and consistent\n- focus the teacher to clarify his/her criteria in specific terms\n- clearly show the student how their work will be evaluated and what is expected\n- promote student awareness of about the criteria to use in assessing peer performance\n- provide useful feedback regarding the effectiveness of the instruction\n- provide benchmarks against which to measure and document progress\nRubrics can be created in a variety of forms and levels of complexity, however, they all contain common features which:\n- objective (performance, behavior, or quality)\n- range to rate performance\n- contain specific performance characteristics arranged in levels indicating the degree to which a standard has been met\nCreate a Rubric\nRubrics help instructors evaluate authentic, performance-based assessments that is tailored to an instructor's own curriculum and teaching style. In order to develop an effective rubric using the following:\nSteps in Rubric Development\n- Determine learning outcomes\n- Keep it short and simple (Include 4 - 15 items; use brief statements or phrases)\n- Each rubric item should focus on a different skill\n- Focus on how students develop and express their learning\n- Evaluate only measureable criteria\n- Ideally, the entire rubric should fit on one sheet of paper\n- Reevaluate the rubric (Did it work? Was it sufficiently detailed?)\nTerms to use in measuring range/scoring levels\nNumeric scale ranging from 1 to 5, for example\nAfter you write your first paragraph of the highest level, circle the words in that paragraph that can vary. These words will be the ones that you will change as you write the less than top level performances.\nPresence to absence\nComplete to incomplete\nMany to some to none\nMajor to minor\nConsistent to inconsistent\nFrequency: always to generally to sometimes to rarely\nBelow are links to rubric examples that can be customized to fit your assignment requirements. Once you have adjusted the rubric for a course activity, you may link to them within an assignment or include them in your syllabus."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:ed5b6ec4-a680-45d5-9f2f-d3b48a78976a>","<urn:uuid:3fbebf6c-889d-4620-b63d-e455b4ba3815>"],"error":null}
{"question":"Hi! I'm interested in the history of animation - could you explain how the first animated feature film was created and what animation techniques existed before it?","answer":"The first surviving animated feature film was The Adventures of Prince Achmed (1926) by German puppeteer Lotte Reiniger. It was created using 2D silhouette animation, where Reiniger designed and animated paper puppets by hand, photographing them frame-by-frame in a friend's garage over five years with a small crew of five people. She developed an innovative technique using different horizontal planes to create depth. Before this, early animation techniques included zoetropes (spinning cylinders with slots showing moving images) dating back to second-century China, and flip books invented in England in the mid-19th century. Early filmmakers like Georges Méliès discovered stop-motion animation by accident, while Émile Cohl pioneered drawn animation in 1908 with simple drawings of figures and objects.","context":["The oldest surviving animated feature was not made by Walt Disney, but by a German puppeteer named Lotte Reiniger who escaped Nazi persecution to move to London and make adverts for the British Post Office. Her film, The Adventures of Prince Achmed (1926), was a creative and technical masterstroke that would have a huge influence on the generation of animators that followed, including those working in the United States.\nBut with the rise of Disney and the dominance US film studios, Reiniger’s name has largely disappeared from sight. But in recent years, thanks to enthusiasts sharing her work on social media and the availability of her work online, her part in the development of animated films is becoming more widely known. As part of that rising recognition, Prince Achmed will be showing in 16mm at London’s Cinema Museum on October 25. The screening will be followed by a live podcast recording from academics at Fantasy-Animation.org reflecting on Reiniger’s status as one of animation’s great pioneers.\nThe story of Prince Achmed – and the animator who created it – highlights the important role women have played and continued to play within animation’s history, a role which is overlooked in favour of a story of powerful men.\nAt as young as six, Reiniger became fascinated with the craft of making silhouette cutouts. According to her biographers, as young child she would spend hours making paper characters, using them to stage dramatic retelling of fairy tales such as Snow White to her parents.\nReiniger’s generation was the first to grow up with cinema. The medium was invented just a few years before her birth and growing up in a large urban centre such as Berlin meant Reiniger grew up with access to the first film shorts. She was influenced by the pioneering special effects films of George Méliès, the French illusionist and film director, and sought a career for herself making her paper characters come to life on the screen through the process of stop-motion animation.\nReiniger would embark on the painstaking process of photographing her paper puppets frame by frame, performing tiny movements at each step of the process to create the illusion of movement through a technique that has since become one of the trademark features of Aardman’s Wallace and Gromit films.\nReiniger originally trained as an actor and was a member of an influential circle of innovative German artists that included Bertolt Brecht, Fritz Lang and G.W. Pabst, creating animated sequences for some of their more famous expressionist works, including a vivid dream sequence that featured in the first of Lang’s two-part fantasy epic Die Nibelungen (1924).\nReiniger’s contemporaries saw animation as a medium of abstract expressionism that could be used to address the immediate political and economic issues they faced within Weimar Germany.\nWhy do you read fairy tales?\n(animation by Lotte Reiniger) pic.twitter.com/bLzLOi2jTG\n— Into The Forest Dark (@ElliottBlackwe3) October 20, 2019\nBut Reiniger saw the value of animation in its capacity for imaginative storytelling. In 1923, she was given the opportunity to make an animated feature by a private investor named Louis Hagen. Reiniger choose to use this opportunity to adapt a work of folklore taken from One Thousand and One Nights – a decision that put her at odds with her contemporaries. Her fellow avant-garde artists felt Reiniger should be making films that directly addressed the turbulent politics of the time.\nReiniger focused her efforts on the technical and expressive possibilities offered by her signature style of 2D silhouette animation. She used her creative powers to bring to life onscreen a kind of fantasy narrative that cinema audiences had yet to see due to the limitations of photography.\nThe production of Prince Achmed was low-fi to say the least. Working with a crew of just five people, Reiniger designed and animated all of her silhouettes by hand, photographing the animation frame-by-frame in a friend’s garage. The process took five years for her to complete.\nThis gave her time to experiment with a number of technical and artistic processes. She colour tinted her film stock to create multicoloured backgrounds and incorporated background designs from avant-garde filmmaker Walter Ruttmann.\nPerhaps most significantly, she developed a technique designed to give the illusion of depth to the flat animation cell by placing her silhouettes on different horizontal planes stacked at alternating levels before photographing them from above. This technique was a close source of inspiration for Disney’s own multiplane camera, one of the key technical developments that has been said to have set Disney’s studio apart from his US rivals during its rise to dominance during the 1930s and 1940s.\nPrince Achmed opened to positive reviews in both Germany and in France, with a critic at the time describing the film as “visual poetry, a miracle to be experienced”. But as time went on, Reiniger’s achievements began to be diminished within a gendered version of film history. Her work was often dismissed by film critics for offering pretty escapism that was pleasant to look at but lacked the supposedly serious political intent of her male contemporaries.\nDr Caroline Ruddell, who is currently working on a project on the role of women in animation history, argues that this needs to change. “Reiniger is not part of the popular canon in the way she should be,” Ruddell suggests.\nOverlooked No More: #LotteReiniger, Animator Who Created Magic With Scissors and Paper—Even before Disney, Reiniger told fanciful stories in animated films with her hand-cut paper silhouettes. https://t.co/yd0d1HSsiP @nytimes pic.twitter.com/mZ9x184XaK\n— Women in the Arts (NMWA) (@WomenInTheArts) October 17, 2019\nTashi Petter, a PhD candidate at Queen Mary University, London, is researching Reiniger’s equally influential career working in London’s artistic and cultural scene during the 1930s. As Petter told me: “With the rise of social media, Reiniger is starting to gain more recognition as a crucial part of film/animation history”. Petter says that fans of Reiniger’s work are now finding spaces to talk to one another, while new audiences are becoming attracted to her innovative animation style thanks to the availability of her films online.\nNearly a century after her heyday, Lotte Reiniger is finally getting the recognition she deserves as one of the great innovators in movie history and is rewriting the outdated notion that cinema was a man’s world.\nLooking for more on films?","0 Items in Your Cart\nVault Guides are THE source for insider insight on career information and employer reviews. Shop Vault Guides\nExplore Industries /\nThe earliest animation devices were zoetropes: spinning cylinders with evenly spaced vertical slots through which a viewer could see a succession of images on the opposite inside wall of the cylinder. These images changed position slightly as the cylinder turned and thus appeared to be in motion. This invention has been dated back as far as second-century China but became popular in the West only as late as the mid-19th century.\nFlip books, in which readers riffle the pages so that a sequence of drawings appears to move, were invented in England around this same time, made possible partly by the development of cheap paper. Whereas zoetropes were circular and therefore best suited to repetitious, cyclical motions such as the galloping of a horse, flip books could portray a linear sequence.\nSome of the earliest creators of motion pictures, at the turn of the 20th century, experimented with animation. The French filmmaker Georges Méliès discovered stop-motion animation by accident when objects changed position in a scene he was shooting while the camera was halted by a malfunction. Another French filmmaker, Émile Cohl, pioneered drawn animation shortly afterward with his film Fantasmagorie (1908), which consisted mostly of simple drawings of human figures, animals, and other objects doing fanciful things. During the following decade, drawn animations grew more sophisticated, became known as cartoons, and began to be shown regularly in movie theaters.\nThe period from 1930 to 1960 is considered the golden age of hand-drawn animation. Soundtracks had become fully integrated with the visual images, moviegoers had come to expect cartoon shorts alongside feature films, and many series were built upon popular cartoon characters such as Felix the Cat, Mickey Mouse, Betty Boop, and Popeye the Sailor. After success with Mickey Mouse and a transition to color images, Walt Disney began to plan what became the first feature-length animation to be released in the United States, Snow White and the Seven Dwarfs (1938). The production almost bankrupted Disney's studio, but the finished film was a huge commercial and critical success, to be followed by Pinocchio, Fantasia, Bambi, and other releases.\nTelevision created a new outlet for animation, but animators who wanted to create enough material to fill a weekly series were hampered by the slow, labor-intensive nature of hand-drawn animation. The animation directors William Hanna and Joseph Barbera, after success in the movies with Tom and Jerry cartoons, solved this problem by using partial animation, in which most of the drawings remained static and only essential parts needed to change from one frame to the next. Their characters, such as The Flintstones and The Jetsons, had muzzle-like mouths so that the rest of the face (except the eyes) could remain stationary while they talked.\nTelevision was an early adopter of computer-generated animation for applications such as title sequences and dynamic weather maps.\nComputer-generated animations began appearing in feature films in the 1970s. One of the first instances was in Westworld (1973), where they were used to show the point of view of a robotic character. In the original Star Wars film (1977), they were displayed as part of a training exercise for the attack on the Death Star. In the 1980s, the technology was advanced enough to render computer-generated images (CGI) to represent real objects rather than schematics. For example, the spaceships that appeared in The Last Starfighter (1984) were all rendered by CGI. In the 1990s, CGI was able to represent human body movements and even skin texture with considerable realism.\nIn the first decade of the 21st century, some animators adopted a new technique, motion capture, to render body movements with greater realism than was possible with purely computer-generated images. To use this technique, the animator films a live actor in motion, usually covered in a black suit dotted with white balls. The computer tracks the motion of the balls in the film and then reassigns these movements to the corresponding locations on the body of an artistic creation, such as a monster. The Polar Express (2004) was the first entirely computer-animated 3-D film to use motion capture.\nAnimation has been important in video games since the late 1970s, when the games moved beyond purely geometrical images (as in Pong) and began to depict aliens, asteroids, and angry gorillas. At first, only arcade game machines had sufficient computing power to render the sophisticated graphics of the most detailed and fastest paced animations, but beefed-up home consoles eventually lured dedicated gamers away from the arcades.\nWhen desktop computers shifted from text-based to graphic interfaces in the 1980s, they improved on simple animations, such as hourglasses, and began to use more sophisticated animations to represent common operations, such as files being copied from one drive to another. Personal computers have also made it possible for amateurs to create complex animations and share them on sites such as YouTube. Animation continues to improve every year. Today's video games, such as Middle-earth: Shadow of Mordor (2014) and Call of Duty: Black Ops III (2015), feature characters with complex physical movements and facial expressions that are the most realistic and lifelike yet.\nJuly 24, 2017\nJuly 20, 2017\nJuly 19, 2017\nJuly 18, 2017\nFree sample resumes and expert advice\nFree sample cover letters and expert advice\nSample interview questions and expert advice\nGet networking tips and expert advice"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:f11ae797-50b9-46e6-bcf9-5c2a282743cc>","<urn:uuid:094b66f8-881f-4de9-acff-f13f65555419>"],"error":null}
{"question":"Do flaming linseed oil rags and boilers reach similar maximum temperatures during operation?","answer":"No, they have different maximum temperature ranges. Linseed oil rags used in stock adjustment should be kept under 300°F to prevent ignition, while industrial boilers can operate at much higher temperatures, as they heat water past its boiling point of 212°F/100°C to generate steam for various applications like power generation and heating.","context":["TTAG commentator Dyspeptic Gunsmith writes:\nThe Ithaca 37 is one of the last pump guns made with an all-steel receiver and nice wood stock. They’re very well built. Some people don’t care for the loading/ejection out of the same port, but for those who do, it works, and it serves righties as well as lefties equally well. NB how the price is up in the $1K area for a quality, US-made gun.\nWhat JWT describes in how the stock bites him is a result of the stock having no “toe-out.” This is a complaint that happens often in not only this shotgun, but in many other shotguns. I hear this complaint from men with larger pectoral and women with larger breast development. The solution is to either a) make the stock with ‘toe-out’ in the first place, or b) adjust the stock to have toe-out. So you could seek out a stock maker to make you a new buttstock, or adjust this one.\nI was taught how to adjust stocks to have toe-out by an old English gunsmith, he of the Birmingham gun trade, in an operation that, if owners of the guns were to see it, would probably make them hit the ceiling.\nYou clamp the gun in a fixture that holds the receiver and barrels firmly. The rear of the fixture will have a clamp that attaches to the butt of the stock (sometimes with screws) and allows a torsion to be put on the stock, pushing the toe out.\nThis technique works on only wood stocks, and the wood on that 37 you’re holding is quite nice.\nYou wrap the wrist of the stock in rags that are soaked in linseed oil that has been heated. Put a pan under the stock to catch excess oil. Keep soaking the rags in heated oil. The oil could be heated safely to, oh, about 250F+. Keep it under 300F to prevent the oil igniting in the heating vessel.\nOK, when you’ve got a layer of cotton rags around the wrist of the stock, and you’ve allowed the heat from the heated oil to work, try to toe-out the stock a bit more. In good, harder woods, you’ll not get much results – but you’ll feel that the wood is getting more pliable. What you need is a bit more heat.\nNone of the foregoing is what makes the owners hit the ceiling. This next part is.\nWhen you need more heat to get into the wood fibers, you light the outer layer of rag around the wrist of the stock on fire. Don’t let the flames get away from you, you just keep spooning more oil onto the rags. Keep the flame off the wood. This takes practice, so start with a walnut plank for your first practice piece of wood. Let the fire burn for, oh, a minute to 90 seconds, then blow it out. It goes without saying that you should have an extinguisher nearby, but let’s get that on the record.\nLet the heat of the fire work down through the saturated rags into the wood. Try putting more toe-out into the stock. As the heat works its way into the wrist, you’ll be able to gradually put more toe-out into the buttstock. If you don’t have enough toe-out yet, you re-light the flames. Lather, rinse, repeat. When you have the toe-out you desire, you pull off the rags, wipe off any excess oil, and leave the shotgun in the fixture to cool. Let it cool, oh, four+ hours in the fixture. You’re waiting for the wood fibers to set up again.\nAn alternative to this method is to use a steam generator, but you’ll end up with lots of water in the action as the steam condenses on the steel parts. If you go the steam route, you should probably pull the lockwork out of the action and spray the inside with WD-40 to keep water from penetrating into nooks and crannies in there. You’ll also find out that it takes quite a while to heat up the wood enough to get the toe-out you desire with steam, since the steam you can commonly generate outside a boiler will be limited in how hot it is.\nAll we’re doing here is what fine furniture makers have known for hundreds of years: you can bend or twist wood if you get the fibers hot enough to start them becoming supple and plastic. Hold the heated, now-plastic wood in the desired position, allow it to cool and you’re on your way to becoming the next Sam Maloof.\nI’ve done the flaming linseed oil rag technique a couple of times, and the owners who had “toe bite” really appreciate a stock that no longer punches them “right there” as they’d call it. I don’t understand why more shotgun manufactures don’t put at least a little toe-out onto their stocks. The two adjustments that people really seem to appreciate in a stock are cast-off and toe-out (for right-handed shooters).","Boiler – Wikipedia\n- A boiler is a closed vessel in which fluid is heated. The fluid does not necessarily boil. The heated or vaporized fluid exits the boiler for use in various processes or heating applications, including water heating, central heating, boiler-based power generation, cooking, and sanitation.\nWhat is a Hot Water Boiler? – Boiling Point – YouTube\nClick to view on Bing7:46\n9/7/2018 · Today on the Boiling Point, Ritchie talks with Steven Taylor about what hot water boilers are and what they are used for. These boilers offer an alternative to a steam boiler generating hot water for process or domestic.\n- Author: wareboilers\nLow Water in a Steam Boiler – Boiling Point – YouTube\nClick to view on Bing8:31\n2/10/2017 · Today on the Boiling Point, Ritchie talks with Steven Taylor about what happens to a boiler when the water level drops below the tubes. These situations are incredibly dangerous but also easily avoidable if you make sure …\n- Author: wareboilers\nWater-tube boiler – Wikipedia\nHigh Pressure Water Tube Boiler: The heated water then rises into the steam drum. Here, saturated steam is drawn off the top of the drum. In some services, the steam will reenter the furnace through a superheater to become superheated. Superheated steam is defined as steam that is heated above the boiling point at a given pressure.\nBoiler water treatment – Lenntech\nIn general, softening chemicals (phosphates, soda ash, caustic, etc.) are added directly to the fee-water at a point near the entrance to the boiler drum. They may also be fed through a separate line discharging in the feed-water drum of the boiler.\nTroubleshooting a Gas-Fired Hot Water Boiler\nA boiler is a bit more complex than a forced air furnace in that it has more parts, valves, and controls. However, gas-fired boilers are fairly reliable and, when problems do occur, they are usually related to the expansion tank or water circulation pump(s).\n- Author: Bob Formisano\nPPT – boiler PowerPoint presentation | free to download …\nWater Tube Boiler A water tube boiler is a type of boiler in which water circulates in tubes heated externally by the hot gases / flue gases. Water tube boilers are used for high-pressure boilers. Fuel is burned inside the furnace, creating hot gas which heats up …\nAbout – SG Water Dispensers Pte Ltd\nSG Water Dispensers Pte Ltd is your Singapore direct piping hot & cold water dispenser supplier. SGwater carry a wide range of POU point of use, direct piping pipe-in, bottleless instant auto refill type water purifier, water cooler, water boiler and water dispensers including 3 temperature hot warm cold clean filtered drinking water.\nWhat is a Boiler and How does it Work? | RealPars\nIn this article, we are going to discover what an industrial boiler is, and how it works. The function of a boiler is to either produce hot water or steam. In this article, we are going to discover what an industrial boiler is, and how it works. … The boiling point of water at atmospheric pressure is 100°C or 212°F.\nWater Boilers, Zip Water Heaters and Instant Hot Taps for …\nWater Boilers Direct is the hot drinking water division of leading hydration specialists, The WCD Group. We take pleasure and pride in delivering a first class service, excellent free advice before and after you buy, as well as bringing you a vast portfolio of premier instant hot water and beverage solutions at affordable prices.\nBoiler Water Conductivity Measurement Fundamentals\nBoiler Water Conductivity Measurement Fundamentals. As a boiler generates steam, any impurities which are in the boiler feedwater and which do not boil off with the steam will concentrate in the boiler water. Inside the boiler the heat generates steam bubbles inside the water.\nControlling TDS in the Boiler Water\nThere comes a point (depending on boiler pressure, size, and steam load) … For boiler water, however, a hydrometer can be used to obtain an approximate measurement of the dissolved solids, since for boiler water each increase of 0.000 1 relative density at 15.5°C is approximately equal to 110 ppm.\nWater treatment of steam boilers – SlideShare\nWater treatment of steam boilers 1. Water treatment-Steam Boilers Chandran Udumbasseri, Technical consultant email@example.com. Steam boilers Types There are two types of steam boilers 1. Fire tube boiler (hot combustion gas passes through tube with water surrounding this tube) 2.\nHigh-Purity Boiler Water pH Measurement\nThe cutoff point between low- and high-conductivity boiler water is 50µS/cm, which is the lowest conductivity at which conventional pH sensors have reliably been used. Below 50µS/cm the sensor of choice has traditionally been one purpose-built to measure pH in low-conductivity water, …\n3 Differences Between a Boiler and a Water Heater\n6/1/2015 · A boiler system heats water to the point that it boils, and the steam is used to transport heat throughout your house. 2. A water heater heats and stores water. Water is usually heated to approximately 125 degrees. It’s kept at a consistent temperature until it’s needed — aka when someone in your home takes a shower, washes the dishes or does …\nBASIC WATER TREATMENT OF STEAM BOILERS\n· PDF file\nboiler water chloride level should be monitored to ensure the blow down rate is correct. If the chloride level is higher than the recommended maximum, more blow down is needed. If it is low, excess blow down is occurring, resulting in wasted chemicals, water, and heat energy.\nBoiler Water Treatment – SlideShare\nTHERMAX LIMITED Contents Water Chemistry w.r.t. Boiler Problems associated w.r.t. Water chemistry Objective and importance of boiler water treatment Various programs in boiler water treatment Dosing of chemicals and monitoring 3. THERMAX LIMITED Water Chemistry 4.\nChapter 14 – Boiler System Failures | SUEZ\nThe boiler water often contains elements that become corrosive when concentrated far beyond normal values as a result of design problems. Many industrial boilers, for example, are treated in such a manner that low concentrations of caustic soda are present in the boiler water. The caustic can become corrosive to steel when the boiler water is …\nWater Handbook – Boiler Blowdown Control | SUEZ\nContinuous blowdown, as the term implies, is the continuous removal of water from the boiler. It offers many advantages not provided by the use of bottom blowdown alone. For instance, water may be removed from the location of the highest dissolved solids in the boiler water. As a result, proper boiler water quality can be maintained at all times.\nHow Power Plant Boiler Works?\nThe drum itself a large cylindrical vessel that functions as the storage and feeding point for water and the collection point for water and steam mixture. This is the largest and most important pressure part in the boiler and weighs in the range 250 Tons for 600 MW power plant. Water Walls\nWhat Is a Boiler Feed Water Treatment System and How Does …\nA boiler feed water treatment system is a system made up of several individual technologies that address your specific boiler feed water treatment needs. Treating boiler feed water is essential for both high- and low-pressure boilers.\nSteam Boiler | Working principle and Types of Boiler …\nSteam boiler or simply a boiler is basically a closed vessel into which water is heated until the water is converted into steam at required pressure. This is most basic definition of boiler.See more on electrical4u.com\nBoiler Feed Water Treatment for Industrial Boilers and …\nCity water is treated in a single-step VSEP treatment system followed by a final ion exchange polishing unit. The treated boiler feed water is then fed to the boiler via a supply pump. Figure 3 is a process block flow diagram showing integration of a vibratory shearing system with boiler feed water treatment at an industrial manufacturing facility.\nThe Normal PSI Reading for a Hot Water Boiler System | Hunker\nHot water boiler systems are equipped with a pressure gauge and an automatic pressure reducing valve that will add water to the system if the pressure drops. Despite the pressure reducing valve, owners should be familiar with the standard PSI of their hot water boiler system and should check the pressure reading regularly.\nDifferent Types of Boilers | Steam & Hot Water Boiler …\nDifferent Types of Boilers. Whether you are in the market to have a boiler installed in your home, need to know what kind of boiler is installed in a home you’re looking to purchase, or simply want to find out more about your own boiler, Hot Point Heating & Air Conditioning is here to help.\nBOILER FEED & PUMP SIZING\n· PDF file\nr) Elevation of boiler maximum water level above pump’s discharge port = 5 feet s) Liquid Specific Gravity = 0.953 t) Temperature of water in deaerator tank = 227 degrees F u) Vapor pressure in deaerator tank = 19.7 psi v) Pump materials all 316 stainless steel to resist treatment chemicals\nProperties of Boiler Water – Marine Engineering\nIn addition to the above values, the various water treatment companies will add further demands, depending on the method used for treatment of feed and boiler water. However, the most important point is that the above values or their equivalents are observed and that a regular (daily) test of feed and boiler water is carried out.\nCharacteristics of boiler feedwater – Lenntech\nFeed-water purity requirements therefore can vary widely. A low-pressure fire-tube boiler can usually tolerate high feed-water hardness with proper treatment while virtually all impurities must be removed from water used in some modern, high-pressure boilers. Only relatively wide ranges can be given as to maximum levels of alkalis, salt, silica …\nInstant Hot Water Dispensers for Your Office · Waterlogic\nWith specific measures in place to stop the boiler from boiling dry and programmable temperature options, you can be sure you have hot water when you need it, without overheating or damaging your system. Our commercial instant hot water dispensers can be used safely by everyone in your office, making them the perfect addition.\nBoiler Feedwater Treatment (Part I): Why Water Treatment …\nBoiler water carryover is the contamination of steam with boiler water solids. Common causes of boiler water carryover are: Bubbles or foam form on the surface of the boiler water and leave with the steam. This is due to high concentrations of insoluble or soluble solids in the boiler water.\n- Author: Bob Formisano\n- Author: wareboilers\n- Author: wareboilers"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:3d9f9a1e-4c2e-4ffa-8cb9-24952bfe062b>","<urn:uuid:d7017291-637a-49c3-bf48-bd2468823467>"],"error":null}
{"question":"How does the crease area and its rules compare between men's and women's lacrosse?","answer":"In both men's and women's lacrosse, the crease is a circular line around the goal that opposing players cannot enter. However, there are key differences in the rules. In women's lacrosse, the crease is 8.5 feet in diameter and players must treat it as a cylinder - they cannot break its plane with their bodies or sticks. If a goalie leaves the crease with the ball, they cannot step back in while still possessing it. A field player (called a deputy) can enter the crease to replace the goalie if their team has possession, but cannot block shots. In men's lacrosse, the crease rule is simpler - it's defined as an area where no player of the opposing team is allowed to go in or break the 'plane' of the crease.","context":["Like every other sport, lacrosse has its own terms that we use. It is important that all of us in the program understand these terms and use them to communicate with each other. Here are the most commonly used lacrosse terms.\nBOX - A 35 yard by 40 yard rectangle that starts at the end line and encompasses the goal.\nCHECKING - Checking can refer to hitting with either your body or your stick. The three best stick checks are the lift check, the poke check and the slap check. All of these checks involve making contact with the bottom hand of the ball carrier. Checks with your body involve hitting an opponent from the front between the waist and shoulders that is either in possession of the ball or within 5 yards of a loose ball.\nCRADLE - A method of moving your wrist in a motion that rocks the ball in your stick. This motion is important to keep the ball in your stick when running or having your stick hit. This is one of the most important basic skills to be learned.\nCREASE - The round line surrounding the goal. No player of the opposing team is allowed to go in or beak the “plane” of the crease.\nCLEARING - Moving the ball from your defensive side of the field to your offensive side of the field. Clearing typically involves using your goalie and 3 defensemen to create a 4 on 3 situation.\nDODGE - An offensive move designed to get by an opponent.\nEMO (Extra Man Office) - When the opposing team has one or more players in the penalty box and you have more men on the field than the other team. Also known as Extra Man Offense (EMO).\nFACEOFF - Faceoffs occur at the beginning of each game, after each goal and at the start of each quarter/half. Faceoffs involve two opposing center midfielders who meet at the center of the midfield line. Each player gets into a crouch position with their gloves touching the ground and the back of their sticks facing the opposing player. The referee places the ball between the two sticks. The face off begins when the referee blows his whistle.\nHOLE - The unmarked area in front of each goal. Offensively, this is where we will score the majority of our goals. Defensively, we need to cover every opponent to prevent good shots on our goalie.\nMAN-BALL - A Man-Ball situation occurs when there are 2 (or more) members of your team going after a ground ball against 1 man on the opposing team. In this situation one man will call “Man” and screen the opponent away from the ball by standing between the oppopnent and the ball, and the other player will call “Ball” and will scoop up the ball.\nMAN UP - See EMO\nMAN DOWN - When you have one or more players in the penalty box and you have less men on the field than the other team.\nMIDFIELD - The Midfield is the line that separates the offensive half of the field from the defensive half of the field. Players who play both offense and defense are called midfielders.\nOFF SIDES - When a team has less than 3 players on their offensive side of the field or less than 4 players on their defensive side of the field. If we have possession of the ball when offides is called by the referee, then we lose possession of the ball. If our opponent has the ball, then we are assessed a 30 seconds penalty.\nPICK - A pick is stationary position that is designed to free a teammate from his defensive man when he cuts off you.\nPOSSESSION - Possession is when a player has the ball in his stick. At the time of a face-off once possession has been established, the referee will yell “possession” and then Defensemen and Attackmen are allowed to go outside of the “Box”.\nRIDING - Defending against a team that is attempting to clear the ball.\nSCOOP - A method of using your stick to pick up a ball on the ground.\nSHOT - An attempt to score a goal.\nSQUARED UP - Defensive position where you are directly in front of the opposing player with your body and shoulders square in front of him, your knees are slightly bent and your stick is extended out toward your opponent in a slightly raised position.\nX - Position behind the goal. Getting the ball to “X” means getting the ball to a teammate behind the goal. Many offensive attack strategies actually start from behind the goal.\nPlayer to Player Communication\n“Ball Down” - what a player yells to inform teammates the ball is on the ground.\n“Here’s your help” - or \"Help\" - when a teammate is being covered and you get open, you put up your stick to receive a pass and yell “Here’s Your Help”.\n“Pick (Left or Right)” - when an opposing player sets a pick on one of your teammates you should yell “Pick” and the side the Pick is on to let him know.\n“Man/Ball” - When you and a teammate are going after a ground ball the player closest to the ball should yell “ball!” and attempt to scoop the ball; the other player should yell “man!” and blocks the nearest opponent from scooping the groundball.\n“I Got Your Left/Right” - A player should be communicating to his teammate that he is prepared to help him cover his man on either the Right or the Left side.\n“Switch” - If an opposing player moves or begins to move past a teammate (either through a Pick or a another offensive move) a player should yell “PICK” and each teammate should switch covering the opposing player they were covering.","The Field: The Goal Cage is 6’ by 6’. The circle around the goal is 8.5’ in diameter and is called the Crease. Field players must treat this line as a cylinder and cannot break the plane of this cylinder with their bodies or sticks. The goalie is allowed to cover the ball when the ball is inside the crease. If the goalie leaves the crease with the ball in her stick, she may not step back in while still in possession. If the goalie leaves the crease a field player may go into the crease and take her place if her team is in possession of the ball. This deputy (the unpadded field player’s new name in goal) is not allowed to block shots.\nThe other lines around the goal are known as the 12-Meter Fan and the 8-Meter Arc. The Center Circle is used for the draw, which happens after every goal and is done in a standing position with the ball placed between the backside of the two players’ sticks.\nRestraining Lines are lines at either end of the field that are used to control the number of players on offense or defense at any one time. See Players and Positions for more information.\nPlayers and Positions: There are 11 field players plus a goalie for each team. The lines on either side of the centerline are called restraining lines. Only seven field players from each team are allowed into the offensive side of their restraining line. This prevents congestion in front of the goal. Four \"attack\" players, have roles that are primarily offensive; shooting on goal and feeding cutters. They will typically stay behind their opponents restraining line. Three mid-field players, \"middies,\" are counted on to play equal parts offense and defense and they run the length of the field. Four \"defense\" players are defensive specialists. They tend to stay behind the restraining line when the ball is on offense. Four players (plus the goalie) must remain on the offensive or defensive side of the restraining line at all times thereby limiting the number of attackers or defenders. If those players cross the line, they are considered offsides and a penalty is given.\nEquipment: The ball is slightly smaller than a tennis ball and made of dense rubber. Sticks are between 35-1/2” to 43-1/4” long. The pocket is considered legal if the top of the ball is even with or above the sidewalls of the stick head when dropped into the pocket and slight pressure is applied. Mouth guards and eye protection are mandatory. Goalies wear a helmet with throat guard, gloves, and chest protector. (Many elect to wear additional protection).\nEvolution of Women's Lacrosse Rules: The women's lacrosse game has been modified significantly over the past ten years. Modifications include limiting the amount of players allowed between the two restraining lines on the draw to five players per team. Stick modifications have lead to offset heads, which allow the women's game to move faster and makes stick moves and tricks easier. In 2002, goggles became mandatory equipment in the United States (but not a requirement in international rules). In 2006, hard boundaries were adopted.\nWomen's vs Men's Rules: The rules of women's lacrosse differ significantly from men's lacrosse and are specifically designed to allow less physical contact between players. As a result of the lack of contact, the only protective equipment required is eyewear and a mouthguard. Although these are the only protective equipment, there are still many injuries due to accidental checks to the head. The pockets of women's sticks are shallower than those of the men, making the ball harder to catch and more difficult to shoot at high speed. Checking is different from men's lacrosse. See the detailed description below for more information. Finally, In women's lacrosse, players are not allowed to touch the ball with their body or cover the ball with their stick in order to scoop it into their stick or protect the ball from picked up by an opponent.\nStick Skills: Learning basic stick skills at the earliest ages is essential. Girls sticks have a shallow pocket, so for the ball to stay in the stick, players must master cradling the ball, ground ball pick up, passing and catching accurately:\n- Cradling: View teaching points and video links. Some cradling violations include: Covering - a player may not guard the ball with her foot or stick. Warding Off - cradling with one hand and using the other arm to ward off the defender. Illegal cradle - players cannot cradle too close to their head or body so as to make an otherwise legal check impossible.\n- Throwing: View video link.\n- Catching: View video link.\n- Checking/Modified Checking: Checking is the striking of an opponent’s stick head in an effort to dislodge the ball. Stick checks should be controlled, short, quick taps. There should not be a backswing. A defender may never check toward the head or body. Players may only check using the side of their stick. Using the flat of the head, is called a \"held check,\" is a foul and the opposing team will get the ball.\nThere are different rules for checking depending upon the age of the girls - it gradually increases to full checking by the time they are in high school:\nU9 - No stick checking, may use three pass and/or three second rule\nU11 - No stick checking, may use three pass and/or three second rule\nU13 - Modified checking below the shoulders in a downward motion away from the body\nU15 - Same as U13 unless officiated by two adult referees and agreed to by both teams\nWhen playing with modified checking or no checking the 3 second rule may apply. This means that if a defender is closely guarding the attacker who has possession of the ball for more than 3 seconds and the attacker does not make an attempt to switch hands, dodge, etc. the ref may award the ball to the defender.\nIt should be noted that stick-to-stick contact is not necessarily a violation of the no checking/modified checking rule. A defender who is holding her stick in good defensive position may force the attack player to cradle into her stick causing contact. This is not considered a stick check, as the attack player initiated the contact, not the defender. A similar situation would exist when the defender puts her stick up in an attempt to block or intercept a pass and the attacker makes contact while in the act of passing or catching the ball.\nSome common checking violations include: check to the head (all checks must be made away from the body), slash, dangerous follow through of check, dangerous contact, reach across the body, modified checking violation (ex: check above the shoulders when not playing with full checking) and checking an empty stick.\n- Shooting: View video link.\nGround Ball: One of the most important skills - as the girls are developing their skills the ball often ends up on the ground. Often times games are lost because of loss of possession and a team’s inability to win ground ball to regain possession. View video link. A player may not cover the ball when picking up a ground ball. A player may not “hold” another player’s stick down to prevent them from getting a ground ball.\nDefense: The goal of the defender is to be able to direct the attacker where they want them to go and to get them to turnover the ball. View video link.\nLength of Game: Women's games are played in two 25-minute halves. These 25 minutes are running time, except for after a goal is scored and on any whistle within the last two minutes of each half. While the whistle is blown, players are not allowed to move. In high school and middle school: If a team has a four-goal advantage the clock will run after goals.\nCenter Draw: The draw takes place at midfield to start each half and after every goal. The draw is a chance to gain possession. View video link. The ball must go higher than the midfielder’s head or else there is a redraw. Besides the players conducting the draw, 4 players from either team may stand on the edge of the circle. The other players must be behind their respective restraining lines. Players may move before the whistle but must remain outside the circle or behind the restraining line. A team with a 4 goal or more deficit may choose to take indirect free position at the center of the field in lieu of a draw. The opposing center must be 4 meters away at a 45° angle. The center with the ball may not score without first passing the ball.\nOut of Bounds: When a player with the ball steps outside the line while in possession of the ball, the ref will whistle and award possession to the opposing team. When a ball goes out of bounds, the team who was last to touch the ball loses possession of the ball. When a shot on goal goes out of bounds, the player closest to the ball when it crosses the boundary line will gain possession 2 meters inside the line.\nFouls: There are two types of fouls: Major and Minor. Major fouls usually pertain to offenses that are potentially dangerous. Minor fouls are less series such as checking an empty stick, covering a ground ball with a stick, or guarding the ball with ones feet. A Major Foul committed outside the 8-meter arc: For a major foul by a defender, the free position will be at the spot of the most recent foul. All players are cleared from the penalty lane. A Major Foul committed inside the 8-meter arc: All players’ bodies and sticks must be cleared from the arc and penalty lane. Player with the ball may have her crosse in the arc, her feet must be behind the arc. The goalkeeper may return to the goal circle as long as they did not commit the foul. All players must take the shortest route out. The player who was fouled will move to a hash mark on the 8-meter nearest to the spot of the foul. The player who fouled moves to the 12 meter fan directly behind the player taking the free position. For Minor Fouls: The player committing the foul must move 4 meters behind the player getting the free position.\nCharging: A player may not “push” her way through another player or players who have established their position. (Much like in basketball)\nGoal Crease violation: A player (other than the goalie) may not enter the goal crease with any part of their body or stick. If a player’s stick crosses the crease after a goal, the goal will not count. The goalkeeper may reach her stick out of her goal to bring the ball back as long as no part of her body is “grounded” outside the goal circle. She may also cover the ball to bring it into her stick as long as no opponent is in a position to play the ball. The goalkeeper MAY NOT bring the ball back into the crease if they had to leave the crease to get the ball.\nThree Second Violation: A player (either attack or defense) may not stand within the 8 meter arc for more than 3 seconds.\nShooting Space violation: One of the most difficult to understand, this rule is for the safety of the players. In order to avoid being called for a shooting space violation a defender must be within a stick’s length (with no arm extension) from the offensive player. View video.\nPenalties for women's lacrosse are assessed with the following cards:\n- The green card, given to the team captain, is for a delay of game.\n- The yellow card is for a first-time penalty and results in the player being removed from the field for five minutes (three minutes in high school competition). The team whose player gets the card must play with one less player until the penalty is over. A yellow card may also be given if a player has been warned by a ref but continues to commit the same foul.\n- The red card is the result either of two yellow cards or one unsportsmanlike behavior ruling, and causes the player to be ejected from the game. If the red card is for unsportsmanlike behavior, the player is also not permitted to play in the following game.\nClear: Any action taken by a player within the goal circle to pass or carry the ball out of the goal circle.\nCritical Scoring Area: An area 15 meters in front of and to each side of the goal and nine meters behind the goal. An eight-meter arc and 12 meter fan are marked in the area.\nCrosse (Stick): The equipment used to throw, catch, check and carry the ball.\nCrosse Checking: Stick to stick contact consisting of a series of controlled taps in an attempt to dislodge the ball from the crosse.\nDeputy: A player who enters the goal circle when the goalie is out of the goal circle and her team is in possession of the ball.\nDraw: A technique to start or resume play by which a ball is placed in between the sticks of two standing players and drawn up and away.\nEight-Meter Arc: A semi-circular area in front of the goal used for the administration of major fouls. A defender may not remain in this area for more than three seconds unless she is within a stick's length of her opponent.\nFree Position: An opportunity awarded to the offense when a major or minor foul is committed by the defense. All players must move four meters away from the player with the ball. When the whistle sounds to resume play, the player may run, pass or shoot the ball.\nFree Space To Goal: A cone-shaped path extending from each side of the goal circle to the attack player with the ball. A defense player may not, for safety reasons, stand alone in this area without closely marking an opponent.\nGoal Circle: The circle around the goal with a radius of 2.6 meters (8.5 feet). No player's stick or body may “break” the cylinder of the goal circle.\nGrounded: Refers to any part of the goalkeeper's or deputy's body touching the ground for support outside of the goal circle when she attempts to play the ball from inside the goal circle.\nIndirect Free Position: An opportunity awarded to the offense when a minor foul is committed by the defense inside the 12 meter fan. When the whistle sounds to resume play, the player may run or pass, but may not shoot until a defender or one of her teammates has played the ball.\nMarking: Being within a stick's length of an opponent.\nPenalty Lane: The path to the goal that is cleared when a free position is awarded to the attacking team.\nScoring Play: A continuous effort by the attacking team to move the ball toward the goal and to complete a shot on goal.\nStand on Whistle: All players must stop and stand still when the whistle blows to stop play. A player moving after the whistle can be called for a foul and change of possession awarded.\nSphere: An imaginary area, approximately 18 cm (seven inches) which surrounds a player's head. No stick checks toward the head are allowed to break the sphere.\nSubstitution: Is unlimited and may occur at any time. Substitutions must check in at the scorer/timer table and enter the field through the team substitution area. The player coming off the field must completely exit the field before her substitute may run onto the field.\n12 Meter Fan: A semi-circle in front of the goal used for the administration of minor fouls.\nWarning Cards: A yellow card presented by an umpire to a player is a warning which indicates that she will next receive a red card and be suspended from further participation if she continues to play dangerously and/or conduct herself in an unsportsmanlike manner. A green card is presented by an umpire to the team captain indicating a team caution for delay of game.\nUslacrosse.org is a great resource to find helpful information. The youth handbook can be found at the following link and you will receive one from US Lacrosse each year."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:111e9ec1-22d6-4825-9229-16e1d18fba95>","<urn:uuid:0b7836d7-21ff-4ae8-bc98-9f8137ec488d>"],"error":null}
{"question":"How do modern county plat requirements compare to Al Idrisi's methods for creating the Tabula Rogeriana?","answer":"Modern county plat requirements mandate specific legal documentation and surveys of land, focusing on precise boundaries, public improvements, and intended use of the property, with strict filing requirements to local authorities. In contrast, Al Idrisi created the Tabula Rogeriana through extensive personal travel and by interviewing visitors to different regions, combining their accounts with classical geographical information to document not just boundaries but also ethnic groups, socioeconomic factors, and other regional features. While both approaches aim for accuracy, modern plats focus on legal precision within small areas while Al Idrisi's method covered vast regions with broader cultural and geographical information.","context":["Plats and Surveys. What do these words mean? If you?ve recently purchased or acquired a tract of land and plan on dividing it, adding to it, or otherwise developing it, you will come across the phrase ?property plat.? A ?plat? is a plan or a map of a plot of land, especially used in construction site mapping. A plat map includes a description of the land and everything on it, including roads, boundaries, and real property. Learning about property plats can help you understand your rights as a landowner, and begin the development process within the legal boundaries of your county code.\nUnderstanding Property Plats\nProperty plats are necessary during the land development process ? particularly with the subdivision of a tract of land. Platting is a complex process that many lawyers, real estate developers, and even government officials still struggle with. The problem lies in the origin of platting laws. Plats are based in public law, not contract law. Subdivision platting laws are based on governmental legal concepts of ?police power,? or the right of the government to make laws necessary to protect the safety, welfare, and health of the public.\nWhile the average person will likely not have to deal directly with platting, it?s important for anyone who wishes to purchase, sell, or develop a lot to understand this concept. Plats are important because they give an accurate description of a section of land as well as the people, things, and access ways on the tract. Plats ensure property owners don?t trespass on another?s property, land for public use remains public, lots comply with zoning rules and restrictions, and all property owners have access to utilities. Plats also control the growth of cities.\nLocal governments require plats to keep track of all residential and commercial properties, their subdivisions, and the development of tracts of land. Without the platting system, there would be no legal documentation of the development of a property ? nor would there be laws holding property owners liable for what they do to their land and that of adjacent tracts. Plats may seem like tedious red tape, but they are necessary to preserve the law and order of lands.\nWhen Do You Need a Land Survey Plat?\nAny time you plan to subdivide a property outside the limits of a municipality into two or more parts, you must submit a subdivision plat to the correct city or county official, unless you meet an exception in your local subdivision ordinance. This applies whether you plan to divide the land into building lots, a subdivision, or an addition. A tract of land is only subject to platting requirements if the division is also to lay out parks, squares, alleys, streets, or other parts intended to be dedicated to public use or the use of other owners adjacent to the land.\nTexas has strict platting laws enforced on every development or division of land. Your county may require a filed plat for new plumbing or HVAC installation, as well as structural modifications. In Texas, platting requirements fall under local government code 212.004. This code states that a ?division of a tract? includes those made using a metes and bounds description in a deed of conveyance, a contract for a deed or a contract of sale, or any other method.\nExceptions include a division of land into parts greater than five acres, where each portion has access and no public improvement, as well as local county and city exceptions. Platting requirements will change depending on your city. For example, Dallas has particularly specific platting rules. Dallas requires a property plat for the following actions:\nCombining tracts of land\nCreating a building site\nDeveloping a planned district\nAdding vacant or abandoned property to a building site\nAmending or correcting errors in a previous plat\nEstablishing a shared access development\nYou do not need a plat if you are dividing property for transfer or ownership ? as long as the property is described using metes and bounds. However, this exception only lasts until someone requests a building permit for the property. Reach out to your county clerk?s office for an exact list of platting requirements and exemptions in your area.\nWhat Must a Property Plat Include?\nWhen you file a property plat with your city or county, you must follow certain rules for it to be properly recorded. First, you must describe the subdivision by metes and bounds. This is a system for describing land and real properties that uses physical features of the land?s geography, as well as distances and directions, to describe the boundaries of a piece of land.\nMetes refer to a boundary defined by the measurement of a straight run. One specifies a mete by a distance between terminal points, as well as a direction or orientation. Bounds are a more general boundary description. For example, describing a body of water, a stone wall, or an adjoining road are all bounds. A lawyer can write the metes and bounds description of your land if necessary.\nThe plat must also locate the subdivision with respect to a corner of the original survey, and state the dimensions of the subdivision and of each part of the tract intended for public use. This may include parks, alleys, or streets. The owner of the tract, or the owner?s agent, must acknowledge the plat; a process much like that of the acknowledgement of deeds. You must file and record the plat with the county clerk in the county where the tract of land is located.\nHow to Obtain a Property Plat\nPlat maps must be as accurate as possible. To create a plat map, one invests in a plat survey. A plat surveyor will study the land in question and describe it in prose, using plain words to recreate the area. Plat surveys are especially useful when describing large tracts of land, where an accurate estimate of boundaries would be too difficult to achieve. Landowners can hire a surveyor to complete the plat survey, compare measurements to the original property deed, and calculate the actual bounds of the property.\nAfter a plat survey, the owner of the land may find that the tract is actually larger or smaller than it is listed in other legal documents. Your property may already have a plat map on file in your county. Conduct an online public records search for plats, and discover the actual boundaries of your piece of property. Plats can open your eyes to parts of your property you didn?t realize you owned, and help you comply with county codes during subdivision or construction projects. Plats and Surveys.","Cartography is the art and science of map making. It requires a steady hand, attention to detail, and comprehensive knowledge of geography. Though most people can’t name one cartographer off of the tops of their heads, let alone a group of them, maps play an influential role in everyone’s lives. Here are famous cartographers who established themselves as some of the world’s best known and significant mapmakers.\nPtolemy wrote his Geographia around the year 150 AD which compiled existing knowledge about the world’s geography at the time. The work refered to a system of latitude and longitude, as well as a means of describing locations on earth based on astronomical observations from those areas. Ptolemy’s original maps from the work were never found, having presumably been lost over the years, but his work was descriptive enough that cartographers were able to recreate his observations in 1300 AD, and create the Ptolemy map. His world map is notable for its probable role in the Roman expansion. Ptolemy’s idea of using a latitude and longitude system had a significant impact on the work of later cartographers.\nThe first spot has to go to the man who coined the term geography, Eratosthenes who lived between 276 – 194 BCE. Eratosthenes came up with the word geography from the roots “geo” (the earth) and “graphein” (to write). He was also the first man ever to be able to calculate the size of the earth (with a remarkably low 2% error) using a measurement known as stades (taken from the length of a stadium, in this case, an Egyptian stadium), the earth’s axial tilt, and possibly even its distance from the sun. His greatest contribution was the concept of latitude and longitude. He created one of the earliest maps of the known world around 220 BC using parallels and meridians which indicated his understanding that the earth was round. The information used to create the map was compiled from early Hellenistic exploration including expeditions by Alexander the Great, Pytheas of Massalia, Megasthenes, Patrocles and Timosthenes of Rhodes. The map was created during his tenure as the chief librarian at the Library of Alexandria, which provide Eratosthenes with access to hundreds of thousands of scrolls gathered from around the known world. The world map was an accompaniment to his three-volume treatise entitled Geography. Unfortunately, the original map no longer exists and only fragments of Geography have been preserved.\nAbu Abd Allah Muhammad al-Idrisi al-Qurtubi al-Hasani al-Sabti, known simply as al Idrisi or Dreses. He was originally from Ceuta, a Spanish city inside the border of Morocco, and lived at the Sicilian court of King Roger II during the 12th century. He traveled extensively in his lifetime, and visited Spain, Portugal, France, England, and Anatolia while he was still a teenager. He created the Tabula Rogeriana (translated as the Book of Roger), which was a description of the world accompanied by a detailed map created in 1154. It required fifteen years for Al Idrisi to finish, and covered all of Eurasia, and part of northern Africa. In addition to the map, the Tabula Rogeriana included an exhaustive account of the geographical features, ethnic groups, socioeconomic factors, and other features of every area he drew. His information was gleaned from interviews with visitors to the areas he wrote about, as well as his own travels and inherited classical geographical information. In a time period when few people traveled more than five or ten miles from their homes, he had visited Spain, Portugal, France, Anatolia, and England by age sixteen, and traveled even more extensively later in life. The Tabula Rogeriana is his most famous work of geography and cartography, and was created for King Roger II of Sicily. The Tabular Rogeriana shows south oriented towards the top of the page. S. P. Scott, who, in 1904 wrote the History of the Moorish Empire, noted that” For three centuries geographers copied his maps without alteration.” This cartographer inspired Clark University’s Clark Labs to name its GIS software ‘IDRISI’ after him.\nThe Fra Mauro Map was created by the 15th-century Camaldolese monk Fra Mauro around 1450 AD who maintained a cartography studio at the Monastery of St. Michael in the then Republic of Venice. Fra Mauro’s mappamundi (world map) was created based on interviews with traveling merchants and is considered one of the finest pieces of medieval cartography in existence. It’s a large round map, around two meters in diameter, painted on vellum and stretched in a wooden frame. The map itself depicts the known world at the time- Europe, Asia, and Africa. One interesting feature of Fra Mauro’s map is that it is oriented with south at the top of the map, as opposed to the Ptolemy map, another well-known historical map. Fra Mauro chose this orientation instead of Ptolemy’s northern orientation because he felt that Ptolemy’s map was no longer accurate, having been created based on information gleaned from works of Ptolemy dating from long before much of the world had been thoroughly explored.\nGerardus Mercator, the man for whom the Mercator map projection is named, lived during the early 16th century, in Flanders. The Mercator map projection was created as a navigational tool in 1569. Though it doesn’t give viewers an accurate idea of the relative sizes of countries and oceans, it was the first map projection designed purely to simplify navigation. Mercator named his map, Nova et Aucta Orbis Terrae Descriptio ad Usum Navigatium Emendate (new and augmented description of Earth corrected for the use of navigation). Because the Mercator projection map is a cylindrical projection, the map does not have a consistent scale when applied to a round earth. This means that objects close to the poles become distorted, and appear much larger than they really are. Gerardus Mercator’s Mercator Map. This map is notable for being the first attempt to make a round earth look “right” on a flat surface. The problem inherent in representing a spherical shape on a flat plane is that things tend to get distorted. Lines of latitude and longitude, useful for navigating a globe, become warped and useless on a flat map. Mercator sought to account for this by keeping the lines straight, and distorting the size of objects closest to the poles. The result was the Mercator projection, an invaluable tool for navigation at sea. Because the Mercator projection allowed for straight lines, called loxodromes, it was much easier for ships’ navigators to use to chart a course, despite the trade-off of distortion.\nNicolas de Fer\nThe French cartographer Nicolas de Fer was less a scientist than he was an artist. De Fer lived during the 17th-18th centuries, and is known for producing over 600 very beautiful maps. His father, Antoine de Fer, started the family business of mapmaking and engraving, which Nicolas later took over. Though his maps probably wouldn’t have won any prizes for geographical accuracy, they were prized for their sheer beauty and decorative qualities. The beauty of his maps was enough to get Nicolas de Fer a royal appointment as the geographer for the French Dauphin, the Duke of Anjou.\nHenry Pelham was a Boston-born painter, engraver, and cartographer that lived during the late 18th century. As a staunch Loyalist, he wasn’t able to remain in the American colonies during the American Revolution, and was eventually forced to flee to Britain in August of 1776. There, he supported himself by teaching and acting as a cartographer and civil engineer. His county and baronial maps of Ireland are considered very important parts of Irish history from that period, and his letters to his family are held in high regard as historical documents from the American Revolution. His map, A Plan of Boston in New England with Its Environs, is considered one of the best Revolutionary War prints. Pelham created the map of Boston and its surrounding areas for use by British intelligence. His pass is reproduced in the upper left corner of the map and states “Permission to take a plan of the Towns of Boston and Charlestown and of the Rebel works round these places in doing of which he is not to be obstructed or impeded but has leave to pass and repass to and from.” The map was etched in London in 1777 by Francis Jukes using the relatively new method of aquatint.\nThough cartographers today don’t usually get the same kind of esteem that they did back when hand-drawn and engraved maps were expensive, prized pieces of art, cartography is still a very difficult discipline that’s equal parts art and science. Few cartographers are cartographers alone – being a cartographer usually goes hand-in-hand with being an artist, engraver, writer, or other creative professional. One thing all cartographers have in common is a fascination with the world around them. Al Idrisi turned his travels and interviews with other travelers into one of the greatest historical and cartographic works of his time, and Nicolas de Fer’s maps were just as much art as they were works of geographical research. Some of their work, like many of de Fer’s antique maps, have been lost to time. Other contributions, like the Mercator map projection, are still in use today."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:90ee1779-5cba-41e3-aed1-98a8928259d6>","<urn:uuid:109556a2-6d79-4694-a45b-646b6af92403>"],"error":null}
{"question":"I'm interested in understanding travel accommodations. How do passenger capacities and social dynamics compare between river cruise ships and a game of Flower & Scorpion? Could you explain the number of people involved in each case and how they interact?","answer":"River cruise ships typically accommodate between 120-175 passengers, with 60-85 cabins spread across 3-4 decks. This size allows passengers to either socialize or maintain privacy, with enough people to blend into a crowd but few enough for crew to recognize individuals. In contrast, Flower & Scorpion is designed for a much smaller group of 2-4 players, with specific tile distributions: 2 players get 6 tiles each, 3 players get 6 tiles each, or 4 players get 5 tiles each. While cruise passengers can choose their level of social interaction across various spaces like lounges, bars, and restaurants, Flower & Scorpion players must directly interact as they compete for chips and strategic positioning in a more intimate setting.","context":["Flower & Scorpion\nThis is a variation of Bergen for two to four players using a double six set of dominoes (28 tiles). As in Bergen, players score for making the free ends of the layout equal. In addition, two pairs of dice are thrown before each hand to determine the flower, which gives a bonus when the ends of the layout match it, and the scorpion, which gives a penalty when it is matched. The dice can also be used to determine the lead in the first hand if you wish. Joe Celko was told the game came from the Middle East or Morocco, where some local conventions were added to Bergen when the Germans and French occupied North Africa. The dice supposedly came from Backgammon sets which were common in bars in the region.\nThe game uses a double six domino set and two pairs of dice. The two pairs of dice are distinguished from each other by color, say red for the Scorpion and green for the Flower. The important point is to have the color retain their meaning for the entire game. The game is usually played with a small shallow bowl of small chips or match sticks for scoring.\nThe games is for 2, 3 or 4 players\n- 2 players get six tile each, leaving a boneyard of 16 tiles.\n- 3 players get six tile each, leaving a boneyard of 10 tiles.\n- 4 players get five tiles each, leaving a boneyard of 8 tiles.\nThe caravan master leads this round and is in charge of making the payoffs from the supply of chips. The player to his left will be the caravan master for the next round and the dice and bowl of chips will pass to him.\nThe leading player starts a caravan, or line of tiles which would be called a train in other games, with any tile he wishes. Other players have to add to either end of the caravan if they are able by matching ends in the usual manner. Doubles are played across, but are not spinners.\nA player not able to add onto the caravan must draw new tiles from the oasis until he can play or until the oasis is empty. If a player can neither play nor draw from the oasis, he passes his turn.\nPlay continues until someone dominoes or the game blocks.\nThe basic idea of scoring is to look at the two, three or four exposed ends of the caravan and try to make all possible pairs from the exposed halves. When some of those pairs are doubles, flowers or scorpions, the player scores or loses points.\nFor example, given two ends on a caravan, there is only one pair to inspect. Given a double and a single tile on the caravan, [a1, a2] and [?, b], the three possible pairs are [a1, a2], [a1, b] and [a2, b]. Given two doubles on the ends of the caravans [a1, a2] and [b1, b2], there are [a1, a2], [b1, b2], [a1, b2], and [b1, a2].\nIf a player sets a tile that makes the exposed ends of the layout equal, the player takes two chips. This is called a \"double header\" in Bergen; the term in this game is a \"dual\" instead.\nA player takes three chips from the bowl when his play makes a triplet from the exposed ends of the caravan. This is called a \"triple header\" in Bergen; the term in this game is a \"plural\" instead.\nIf the Flower and Scorpion are identical, then they cancel each other out and no special scoring is done in this round. They can, however, determine the winner of a blocked round.\nIf the Flower and Scorpion are different, the more likely case, then every possible pairing that results in a Flower gains one chip and every possible pairing that results in a Scorpion loses one chip.\nWhen a player plays the tile with the same two faces as the pair of dice in the Flower, he takes a chip. This is the short Flower. Flowers built from the end of the caravan are called a long Flower.\nWhen a player plays the tile whose ends match the Scorpion, he puts one chip into the bowl. This is the small Scorpion. A pair built from the exposed ends which makes a Scorpion is called a large Scorpion.\nIf the Flower is a double, then it is counted as both a dual and a Flower when played. For example, if the Flower is [6-6] and a player sets a train which leaves two sixes exposed, he scores 3 points: 2 for the dual and 1 for the long flower. If the next player adds the [6-6], leaving three 6's exposed, then the score is 6: three for the triple 6's, one for the Flower tile and one for each of the two long Flower pairs.\nIf the Scorpion is a double, then it is not counted as a dual or a plural when played. For example, if the Scorpion is [6-6] and a player sets this tile to form a train which leaves three 6's exposed, then the player loses three chips (one short and two long Scorpions) and gets no credit for the triplet.\nA player cannot go below zero chips and owe the bowl.\nPlay stops when someone dominoes or when a round is blocked. A round is blocked when nobody can play and there are at least two dominoes in the oasis. The winner of the round is determined by applying these rules in this order:\n- The player who dominoes.\n- When the round is blocked, player who has the Flower tile in his hand wins.\n- When the round is blocked and nobody has the Flower tile, the player who has the Scorpion tile in his hand wins.\n- When the round is blocked and nobody has either the Flower or the Scorpion tile, the player with lowest double in his hand wins.\n- Otherwise, there is no winner and no points are awarded.\nThe winner takes two chips from the bowl; there is no bonus or penalty for the Flower or the Scorpion at this point, since they were not played.\nThe game is won by the first player to get 20 or more chips.\nComments & Strategy\nDo not get so attached to the Flower and Scorpion that you forget to play a good game of Bergen. Notice that blanks cannot ever be part of the Flower or the Scorpion because dice do not have blanks.\nDoubles are still important in this variation, so you should hold them as long as possible to get a plural.\nHolding the Flower tile is a strong advantage for the player who has almost won the game because, unlike dual and plural points, it is always worth one point. It also guarantees two points if the round is blocked.\nHolding the Scorpion tile is risky. You might be forced to play the Scorpion tile during the game. You might avoid getting the penalty points by having it left in your hand when someone else dominoes. But it can be an advantage if the game blocks because it might give you the win for the hand if the Flower has been played.\nBe aware of chances to score a Flower on the ends of the caravan and get a quick single point.\nIf you can keep track of the tiles, you can force another player to make a Scorpion on the ends of the caravan. This is easier if one end of the caravan has frozen. That is, if the suit at one end of the caravan has been completely played on the table (i.e. it occurs eight times in the caravan), so players have to add new tiles to the other end.\nIt can be to your advantage to force a blocked round if you think that you can get the two chips for winning while preventing the other players from further scoring.","European River Cruising\nThe most convenient way to travel in\nLong sleek river\ncruisers give you a wonderfully convenient way of traveling\nThis pictures shows\nAmadeus Waterways' boat\nDurnstein, Austria on the Danube.\nPart 1 of a series on river cruising.\nPlease also see\nAll About European River Cruising\nHow to Choose Your\nEuropean River Cruise\nHow to Choose Your River Cruise\nOnce you've enjoyed a European\nriver cruise, you'll never again want to subject yourself to a\nbus tour, or the massive hassles of checking in and out of\nhotels, unpacking and repacking your bags, finding your own way\nto and from train stations, and/or driving yourself around in\nRiver cruising is the most\ncomfortable and convenient way to enjoy a European vacation.\nAnd because Europe's historic\ntowns and cities were often built on or close to the main\nrivers, you can see most of the major interesting places, as\nshort tours from the comfort of your cruise ship.\nWhy Choose a River Cruise\nSome people, who have never\nbeen on a river cruise, might consider a river cruise to\nbe confining - you travel at a slow speed on a river, and only\nget to see the sights on either side of the river. You are\nlocked into a fixed itinerary, and trapped on a small boat with\na large number of other people. Lastly, some people\nconsider river cruising to be very expensive.\nWhile there are\nunderstandable reasons for all these perceptions, the reality is\nvery different. Let's look at each point in turn.\nSpeed of travel\nRiver cruises commonly do\nmost of the traveling at night, while you are dining, enjoying\nyourself, and sleeping.\nCruises will include some\ncruising during the day, both to meet the need to be certain places at\ncertain times, and also to allow you to enjoy some daytime sightseeing,\nlooking at the beautiful scenery and lovely little towns, old\ncastles on nearby hilltops, churches, farms, and all the other\nelements of life in Europe.\nSpeed is not needed, and the\ngraceful rate of progress (typically 12 - 15 mph cruising speed)\nhelps convey a feeling of relaxed unhurriedness. Remember that Europe is very compact - you only have short\ndistances to travel between towns (and even between countries).\nYou don't need speed, and you don't want speed.\nYou see much more from a\nriver cruise boat than you do from a bus, train, or car, while\nhurrying from place to place. And you see these things\ntremendously more comfortably, either in your cabin, in a\nlounge, in a dining room, or up on one of the open decks.\nPlus, when you arrive at the\nvarious places you stop and visit, when you get off the boat,\nyou're able to sightsee exactly the same as any other way of\ntraveling. Well - not quite so exactly - you don't have to\nbother about packing your bags, checking out of the hotel,\ncarrying your bags to your form of transport, traveling to the\nnext place, getting to your hotel, checking in, carrying your\nown bags upstairs, and unpacking them again in your room!\nInstead you find yourself,\nas if by magic, close in to the center of town in most cases,\nand with helpful sightseeing material provided by the cruise\nship staff, and usually with some free sightseeing included.\nThis is marvelously convenient.\nOf course all cruises work\nto a fixed itinerary. But what is bad about this?\nYou can choose through very many different itineraries and\nselect the itinerary that most suits you, the same as you would\nif planning your travels any other way.\nAnd, once you've chosen your\nitinerary, that is all the hard work done. There's nothing\nmore to arrange once you've booked your cruise. Your\ntransportation, accommodation, sightseeing and meals are all\n(Note that the fixed\nitineraries can sometimes, but rarely, vary. If the river\nis flooding, or if the river is unusually low, there may be\na need to change the itinerary.)\nBoat size and passenger\nRiver cruisers are typically\nabout 300' - 440' in length and 35' - 40' in width. They\nhave three or four decks for accommodation, lounges, bars,\nrestaurants, and other public areas.\nThe ships usually have\nbetween 60 and 85 cabins, making total passenger capacity\nsomewhere between 120 and 175.\nChances are you'll find this\nto be neither too small nor too large. The ship itself is\nlarge enough to be interesting and not to feel confining, but\nnot too big as to make it a hassle to go from anywhere to\nanywhere else. Need to go back to your room? It is\nonly a minute or two from wherever else you are on board.\nAnd the number of fellow\npassengers is enough as to enable you to anonymously blend into\na crowd if you wish, but also is small enough so that ship's\ncrewmembers recognize you and talk to you as an individual,\nrather than as another faceless passenger.\nThe passenger count is\nsufficiently low as to make it easy to get to know other\npassengers when dining or in the lounges, and then to see and\nrecognize them again on future occasions. But there are\nalso enough passengers so that if you want to avoid someone, you\naren't always running into them!\nMost cruises seem to be\npriced in the range of about $200 - $300 a day. When you\nfirst see $2000 for a one week cruise, it might seem like a lot\nof money, especially when you compare it to what you'd pay for a\none week cruise in the Caribbean.\nA fairer comparison is to\ncompare a river cruise with what you'd pay traveling around Europe any other way, for a\nweek. A daily budget (share twin) for traveling on your\nown might include $125 for a hotel each night, $75 for a tour\neach day, $50 for transportation each day (a five day in 2 month Eurail Selectpass costs between $383 and $473 - much more than\n$50/day), and $50 for lunch and dinner (assuming breakfast was\nincluded in the hotel rate). In other words, regular\ntouring is going to cost $300 a day - the same as the upper end\nof all-inclusive cruising costs.\nNow consider the value of\nthe convenience of only having to pack/unpack and check in/out\nonce. How much extra would you pay for that?\nAnd consider also the value\nof traveling at night, freeing up more time during the days to\nsightsee, as well as the extra quality time and energy spared by\nskipping all the packing/unpacking etc. What is that worth to you?\nThere are plenty of other\npositive value-adds in a cruise (like the friendly English\nspeaking staff, free evening entertainment, and with some cruise\nlines, even free wine with dinner), but consider these as free\nbonuses. A simple comparison shows a cruise to be no more\nexpensive - and quite possibly less expensive - than traveling\naround on your own.\nRiver cruising is not\nexpensive. It is a very good value way to comfortably and\nconveniently see Europe.\nRiver and Ocean\nMuch of what we write here\nis focused on river cruising. But you should be aware that\nthere are also excellent ocean cruising opportunities in Europe.\nWith many European countries having a border on the sea, big\nship cruising is also a viable option - most commonly in\nwonderful cruises around the Mediterranean Sea, and also up\naround the Baltic areas.\nThere are plenty of positive\nreasons to consider an ocean based\nEuropean cruise as well as a river cruise. A number of the other big name cruise\ncompanies operate seasonal cruising itineraries in Europe.\nMore reasons to choose a river\nThe food is generally of a\nhigh quality on board, and dining is single seating, usually\nwith no pre-assigned tables. This gives you the\nflexibility to meet new people (or to avoid people) at meals,\nand also means you're not rushed out of the dining room as soon\nas dinner is finished to make way for a second group of diners.\nRiver cruises are almost\nalways calm and stable, with no problems caused by waves or\nRiver cruises have a range\nof daily activities on board - not nearly as extensive as full\nsized ocean liners, but enough to keep you occupied and\ninterested when you have nothing else to do.\nRiver cruises are 'timeless'\n- whereas travel by road or rail surrounds you with 20th and\n21st century technology, on the river, you're in a more natural\nenvironment. And because the rivers were the original\ntravel routes before roads, rail, and air, you get to see\nancient buildings and towns - and the parts of these towns\nfacing the river - that you'd miss when traveling by other\nDifferences between river\nbarges and river ships\nThis article is primarily\nabout cruising on a river 'ship'. A river ship is very\ndifferent to a smaller river barge.\nBarge cruising is on a very\nsmall boat, or sometimes a pair of boats (with one primarily for\npassenger accommodation and the other with dining rooms, lounge,\nand other public and work spaces. Typically a barge (or\npair of barges) carries only 6 - 20 passengers at a time.\nBarges meander along at a\nvery sedate speed, typically little faster than brisk walking\nBarge cruises are more expensive\n- partially due to the low number of passengers to share the\nfixed cost of operation and the high ratio of staff to guests,\nand partially due to a lot of deluxe inclusions. Many\ntimes, barges have an open bar, and for shore visits, will\nprovide private chauffeur driven car touring around the regions\nDue to the small number of\npassengers and crew, there are limited evening entertainment\nactivities on a barge.\nBarge cruising can also be a\nwonderful experience, but it is a very different type of\nexperience to river cruising, and the two should not be\nSee also our article on 'do\nit yourself' canal cruising in\nBritain for a third type of river/canal cruising experience.\nTypes of River Cruises in\nThere are several convenient\nways to categorize the wide range of different river cruising\nMost of the cruises tend to\nbe regional; only the very lengthy cruises cover extended areas.\nMost cruises are about a\nweek in length. Some companies offer longer cruises,\nwhich typically are two shorter cruises 'joined together' - for\nexample a company might offer a one week Black Sea to Budapest\ncruise and a one week Budapest to Nuremberg cruise, plus also\noffer a two week Black Sea to Nuremberg cruise.\nOne-way or roundtrip\nMost cruises tend to be\none-way, although a few are roundtrip - either in an out and\nback type manner, going back the way you came, or in a circle\ntrip manner, not repeating any part of the route.\nIf you're traveling on an\nout and back itinerary, don't think that it will be terribly\nboring returning back the way you came. As long as your\nshore stops aren't identical, the river scenery is often very\nmuch different when viewed from the other direction, and the\nparts of the river you see during the day (as compared to at\nnight) are likely to be different. Out and back\nitineraries are just as interesting as any other type.\nIn addition to standard\ncruises (which generally operate only in spring/summer/fall\nperiod) some cruise lines offer special seasonal cruising.\nThe two most common seasonal\nspecials are Tulip cruises around Holland, timed for the early\nspring when the tulips are in full flower, and the Christmas\nMarket cruises in Austria and Germany, timed for the period\nbetween Thanksgiving and Christmas, giving you a chance to shop\nat the various Christmas Markets along the Danube.\nBoth of these seasonal\ncruises are wonderful experiences.\nWhen to Travel\nOur preference is to travel\nin May or June. This gets you the maximum amount of\ndaylight every day, but beats the worst of the summer heat and\nthe summer crowds. Peak summer crowds can become quite\nclaustrophobic in some of the most popular destinations, and\nshould be avoided at all costs.\nAnd, talking about costs,\nsometimes you'll find the cruise fares are a bit lower in\nMay/June, too. Another reason to choose those months.\nAvoid July and August if at\nall possible, for the reasons above.\nEurope is a small region with many different wonderful places, all close to each other,\nand there's a strong temptation to try and see and do more while\nin Europe. For this reason, many people choose to add extra touring at the start or end of their cruise.\nSome cruises start off and\nhave the ship immediately leaving the city you joined it in, and\nsome cruises finish with you immediately leaving the ship upon\narrival in the destination city.\nOther cruises have the ship\nspending as many as three nights moored in your departure city\nbefore starting the traveling part of the cruise, and may spend\nas many as three nights moored in the arrival city before you\ndisembark (this is common with Russian cruises).\nObviously it makes more\nsense to add extra days at either end in the start and finish\ncities if the cruise itinerary doesn't already have you spending\na lot of time in those places.\nExtending before joining the\nWhether you are joining a\ncruise that stays in the departure city for several days or not,\nwe generally recommend you should plan to arrive in Europe at\nleast one day before your cruise starts. This gives you a\nprecious day up your sleeve in case you miss flights, or have\nflights cancelled, or have your luggage lost en route.\nIt also helps you unwind\nafter the long flight(s), and helps you get over the jet lag, so\nyou're at your best for when the cruise commences.\nWhere to Extend\nSometimes it is sensible to\nsimply stay in the city the cruise starts or finishes in and\nspend more time there. This minimizes your extra travel\nneeds, and in the case of a pre-cruise extension, gives you a\nsafety margin of extra time in case of problems getting to the\ncruise start point.\nBut if you're spending\nenough time in these places as part of the cruise, by all means\ntravel further afield as it suits you best.\nCruise company extension or\nCruise companies often offer\noptional extensions before or after the main ship-based cruise.\nThese can be convenient and definitely enjoyable, but usually\nyou can make similar arrangements on your own for a similar type\nof price. Some people use the extension itinerary ideas\nfrom the cruise brochures as a guide for what to add on their\nIf you're extending on you\nown, you'll probably find the cost is the same or lower, and\ndepending on your style of travel, you may find either doing it\nyourself or having everything arranged more convenient.\nLocations of River Cruises in\nClick on image for a\nlarger map to open in a new window\nIt is possible to cruise\nthrough much of Europe by river and canal, and to traverse\nEurope completely from the Black Sea to the North Sea.\nRiver cruises can be found on most of Europe's main rivers,\nincluding those listed in the table below :\n1771 miles in\nlength; the major commercial waterway in Europe\nStarts in Germany's Black Forest, empties into the Black\nSlovakia, Hungary, Serbia, Bulgaria, Romania\n1420 miles from the\nsouthwest of Russia, through Belarus and Ukraine, to the\n561 miles from\ncentral Spain, through Portugal and to the North\nPortugal and Spain\n724 miles from the\nCzech Republic, through Germany, to the Baltic Sea\n328 miles long, a\ntributary of the Rhine\n341 miles long,\ntraverses France and Luxemburg then into Germany and\njoins the Rhine\nGermany and France\n820 miles from SE\nSwitzerland, through Germany and Netherlands, to the\n300 miles from\nSwitzerland to the Mediterranean\n300 miles long,\nflows into the Rhône\n482 miles long,\nflows into the English Channel\n2300 miles - largest\nand longest river system in Europe. Starts\nnorthwest of Moscow and empties into the Caspian Sea.\nRead more in Parts 2 and 3\nthe second part, we talk about\nhow to choose the best\nEuropean river cruise - what to consider when selecting the best cruise\ncompany for your cruise, and the best airfare to get you there\nIn the third part, we discuss\nthe implications of how to choose the\nbest cabin on your river cruise. This of course will\nhave a major impact on your budget and cruise experience.\nIf so, please donate to keep the website free and fund the addition of more articles like this. Any help is most appreciated - simply click below to securely send a contribution through a credit card and Paypal.\n3 Feb 2006, last update\n02 Jul 2017\nYou may freely reproduce or distribute this article for noncommercial purposes as long as you give credit to me as original writer."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:bdef5f3f-50b6-4038-8a89-4b1a36c8ff12>","<urn:uuid:cd06aa03-e82d-4635-83aa-f8e489572306>"],"error":null}
{"question":"How do BioBricks technology enable cellular engineering innovations, and what regulatory requirements must be met to test these engineered cells in clinical trials?","answer":"BioBricks are DNA-sequence-based building blocks that can be assembled to create new cellular functions, similar to Lego bricks. They allow synthetic biologists to develop novel cell-based theranostic agents that combine diagnostic and therapeutic functions in a single system. However, to test such engineered cells in human clinical trials in the US, researchers must first obtain an Investigational New Drug (IND) designation from the FDA, which requires demonstrating safety through in vivo and in vitro laboratory studies. The clinical trials then proceed through multiple phases: Phase 1 tests safety in 20-100 healthy volunteers, Phase 2 evaluates efficacy and safety in hundreds of patients, Phase 3 involves placebo-controlled trials in hundreds to thousands of patients, and Phase 4 monitors the approved drug post-marketing.","context":["The paper in Nature Chemical Biology is here: http://go.nature.com/2ATAiI6\nBioBricks—DNA-sequence-based building blocks capable of being assembled to exert higher order functions (similar to ‘Lego®’ bricks)— have attracted attention from various synthetic biologists in the quest to create new cellular functions1. In our study just published in Nature Chemical Biology, we successfully showed that not only DNA fragments, but also signaling proteins themselves can be used as ‘Lego bricks’-like elements for creating new artificial cellular functions by carefully exploiting unique biophysical characteristics of each protein.\nOur lab has strong expertise in mammalian synthetic biology, and has developed novel cell-based theranostic agents that contain both diagnostic and therapeutic functions in a single integrated system; this work has enabled us to engineer cells that can be implanted in patients and sense disease biomarkers and secrete therapeutic effector proteins in situ, on demand. Typically, G protein-coupled receptors have been used as modules to sense soluble disease markers, and downstream signaling pathways have been rewired to transgene expression of therapeutic proteins. This cell-based theranostic approach is a very attractive strategy for next-generation personalized medicine, since it allows us to provide medical treatment with precise spatiotemporal control of the release of therapeutic agents2.\nHowever, for the therapy of certain types of diseases, including cancer, typical disease markers are not soluble, but expressed on the surface of the cells in the affected area. Therefore, to apply this ‘sense-and-actuate’ theranostic principle in this context, custom cell-contact-sensing output devices are necessary. As discussed in our paper, we wanted to implement such devices in non-immune cells to extend the applicability of cell-based cancer therapy. However, due to the shortage of design principles to transmit extracellular information into cells, it has been difficult to achieve this goal. To overcome this situation, we turned to ‘playing lego’ with signaling proteins.\nOur basic approach was to rewire a signaling pathway by hacking the biophysical movement of the signaling proteins; we thought that by conjugating a signaling effector (whose activity is usually controlled by translocation of a protein triggered by input A) to another protein (whose translocation is controlled by another input B) it might be possible to rewire the input of the signaling effector to input B.\nTheorising this concept was relatively easy—compared with finding a functional combination of signaling proteins that worked. Although the contents of the paper mostly described successful combinations, several other designs were also attempted. These included control of translocation of protease to release membrane bound transcription factor (Tango assay3-like system), and translocation of phospholipase C to the plasma membrane upon cellular contact by exploiting the movement of cytosolic protein ZAP70 (a downstream signaling component of T cell receptor (TCR) signaling that translocates to the plasma membrane upon phospholyration of the TCR4). However, these approaches were all unsuccessful under our settings.\nWhile discussing within the team to find a breakthrough solution, we changed the focus protein, and came up with an idea to conjugate intracellular domain of the protein CD45 to some inhibitory protein of a signaling pathway that is functional in non-immune cells. We thought it would be possible to control accessibility of the inhibitory protein to downstream signaling components along with biophysical segregation of CD45 triggered by cell-cell contact, thereby enabling OFF/ON switching of downstream signaling of the signal inhibitory protein upon cell-cell contact. In fact, as we looked more closely at the function of CD45, we found that it could be used as such a signal inhibitory protein, since CD45 was reported as an inhibitor not only for T cell signaling, but also for the JAK-STAT signaling pathways that function in a variety of cells5 (a fortunate finding). Thus, we came up with the basic design of the signaling device we developed, and finally this signal rewiring did work (Figure 1).\nMost of the synthetic biologists have focused on construction of precisely designed gene switches by DNA elements as building blocks toward the purpose of regulating cellular function at will. However, few researchers have exploited macro-scale (compared to protein size) interaction of proteins in the cells as building blocks for controlling cellular function to respond to physiological input from the extracellular environment. We are quite happy that we could contribute to the available design principles of artificial cellular functions by demonstrating that rational assembly of biophysical movement of signaling proteins in an ingenious manner can generate a new class of cellular functions.\nOf course, it was also pleasing that we could show the potential applicability of the cell-contact-sensing device developed in this study to future clinical application. However, my personal favorite point of this study is that we found that this new way of thinking (like playing Lego with protein elements) can indeed sometimes be functional in biological systems. I am now fantasizing what kind of other ‘beyond natural’ cellular functions can potentially be implemented with this kind of design…\nOur paper: Kojima, R. et al. Nonimmune cells equipped with T-cell-receptor-like signaling for cancer cell ablation. Nat. Chem. Biol. doi:10.1038/nchembio.2498.\nAcknowledgements and credit for the banner image\nI thank Dr. Charlotte Rivas for useful comments on this manuscript. Credit of the graphic should be addressed to Ryo Tachibana, a PhD student in the Graduate School of Pharmaceutical Sciences, the University of Tokyo.\n1. Shetty, R.P., Endy, D. & Knight, T.F., Jr. Engineering BioBrick vectors from BioBrick parts. J. Biol. Eng. 2, 5 (2008).\n2. Kojima, R., Aubel, D. & Fussenegger, M. Novel theranostic agents for next-generation personalized medicine: small molecules, nanoparticles, and engineered mammalian cells. Curr. Opin. Chem. Biol. 28, 29-38 (2015).\n3. Barnea, G. et al. The genetic design of signaling cascades to record receptor activation. Proc. Nat. Acad. Sci. U.S.A. 105, 64-69 (2008).\n4. James, J.R. & Vale, R.D. Biophysical mechanism of T-cell receptor triggering in a reconstituted system. Nature 487, 64-69 (2012).\n5. Irie-Sasaki, J. et al. CD45 is a JAK phosphatase and negatively regulates cytokine receptor signalling. Nature 409, 349-354 (2001).\nDrawn by Ryo Tachibana, reflecting the concept of our study.","New Drug Application\nThe Food and Drug Administration's New Drug Application (NDA) is the vehicle in the United States through which drug sponsors formally propose that the FDA approve a new pharmaceutical for sale and marketing. Some 30% or less of initial drug candidates proceed through the entire multi-year process of drug development, concluding with an approved NDA, if successful.\nThe goals of the NDA are to provide enough information to permit FDA reviewers to establish the complete history of the candidate drug. Among facts needed for the application are:\n- Patent and manufacturing information\n- Drug safety and specific effectiveness for its proposed use(s) when used as directed\n- Reports on the design, compliance, and conclusions of completed clinical trials by the Institutional Review Board\n- Drug susceptibility to abuse\n- Proposed labeling (package insert) and directions for use\nTo legally test the drug on human subjects in the U.S., the maker must first obtain an Investigational New Drug (IND) designation from FDA. This application is based on nonclinical data, typically from a combination of in vivo and in vitro laboratory safety studies, that shows the drug is safe enough to test in humans. Often the \"new\" drugs that are submitted for approval include new molecular entities or old medications that have been chemically modified to elicit differential pharmacological effects or reduced side effects.\nThe legal requirement for approval is \"substantial\" evidence of effectiveness demonstrated through controlled clinical trials. This standard lies at the heart of the regulatory program for drugs. Data for the submission must come from rigorous clinical trials.\nThe trials are typically conducted in three phases:\n- Phase 1: The drug is tested in 20 to 100 healthy volunteers to determine its safety at low doses. About 70% of candidate drugs advance to Phase 2.\n- Phase 2: The drug is tested for both efficacy and safety in up to several hundred people with the targeted disease. Some two-thirds of candidate drugs fail in Phase 2 clinical trials due to the drug not being as effective as anticipated.\n- Phase 3: The drug is typically tested in several hundred to several thousand people with the targeted disease in double-blind, placebo controlled trials to demonstrate its specific efficacy. Under 30% of drug candidates succeed through Phase 3.\n- Phase 4: These are postmarketing surveillance trials in several thousand people taking the drug for its intended purpose to monitor efficacy and safety of the approved marketed drug.\nThe legal requirements for safety and effectiveness have been interpreted as requiring scientific evidence that the benefits of a drug outweigh the risks and that adequate instructions exist for use, since many drugs have adverse side effects.\nMany approved medications for serious illnesses (e.g., cancer) have severe and even life-threatening side effects. Even relatively safe and well understood OTC drugs such as aspirin can be dangerous if used incorrectly.\nThe actual applicationEdit\nThe results of the testing program are codified in an FDA-approved public document that is called the product label, package insert or Full Prescribing Information. The prescribing information is widely available on the web, from the FDA, drug manufacturers, and frequently inserted into drug packages. The main purpose of a drug label is to provide healthcare providers with adequate information and directions for the safe use of the drug.\nThe documentation required in an NDA is supposed to tell the drug’s whole story, including what happened during clinical tests, what the ingredients of the drug formulation are, results of animal studies, how the drug behaves in the body, and how the company manufactures, processes and packages it. Currently, the FDA decision process lacks transparency, however, efforts are underway to standardize the benefit-risk assessment of new medicines. Once approval of an NDA is obtained, the new drug can be legally marketed starting that day in the U.S.\nOnce the application is submitted, the FDA has 60 days to conduct a preliminary review, which assesses whether the NDA is \"sufficiently complete to permit a substantive review.\" If the FDA finds the NDA insufficiently complete (reasons can vary from a simple administrative mistake in the application to a requirement to re-conduct testing), then the FDA rejects the application by sending the applicant a Refuse to File letter, which explains where the application failed to meet requirements.\nAssuming the FDA finds everything acceptable, they decide if the NDA needs a standard or accelerated review, and communicates acceptance of the application and their review choice in another communication, known as the 74-day letter. A Standard Review implies an FDA decision within about 10 months while a Priority Review should complete within 6 months. The decision comes in a Complete Response Letter.\nRequirements for similar productsEdit\nBiologics, such as vaccines and many recombinant proteins used in medical treatments are generally approved by FDA via a Biologic License Application (BLA), rather than an NDA. The manufacture of biologics is considered to differ fundamentally from that of less complex chemicals, requiring a somewhat different approval process.\nGeneric drugs that have already been approved via an NDA submitted by another maker are approved via an Abbreviated New Drug Application (ANDA), which does not require all of the clinical trials normally required for a new drug in an NDA. Most biological drugs, including a majority of recombinant proteins are considered ineligible for an ANDA under current US law. However, a handful of biologic medicines, including biosynthetic insulin, growth hormone, glucagon, calcitonin, and hyaluronidase are grandfathered under governance of the Federal Food Drug and Cosmetics Act, because these products were already approved when legislation to regulate biotechnology medicines later passed as part of the Public Health Services Act.\nMedications intended for use in animals are submitted to a different center within FDA, the Center for Veterinary Medicine (CVM) in a New Animal Drug Application (NADA). These are also specifically evaluated for their use in food animals and their possible effect on the food from animals treated with the drug.\n- \"The Drug Development Process\". U.S. Food and Drug Administration. January 4, 2018. Retrieved May 1, 2018.\n- \"The Drug Development Process. Step 4: FDA Drug Review\". U.S. Food and Drug Administration. January 4, 2018. Retrieved May 1, 2018.\n- Commissioner, Office of the. \"Public Health Focus - FDA and Marijuana\". www.fda.gov. Archived from the original on April 28, 2018. Retrieved April 30, 2018.\n- \"The Drug Development Process. Step 3: Clinical Research\". U.S. Food and Drug Administration. January 4, 2018. Retrieved May 1, 2018.\n- \"The Drug Development Process. Step 1: Discovery and Development\". U.S. Food and Drug Administration. January 4, 2018. Retrieved May 1, 2018.\n- Food, Drug, and Cosmetic Act, Section 505; 21 USC 355]\n- 21 CFR 201.5: Labeling Requirements for Prescription Drugs and/or Insulin\n- \"Daily Med:Current Medication Information\". Archived from the original on November 12, 2008. Retrieved October 10, 2007.\n- Liberti L, McAuslane JN, Walker S (2011). \"Standardizing the Benefit-Risk Assessment of New Medicines: Practical Applications of Frameworks for the Pharmaceutical Healthcare Professional\". Pharm Med. 25 (3): 139–46. doi:10.1007/BF03256855. Archived from the original on February 6, 2012.\n- Kathie Clark (December 15, 2009). \"Updates from the Regulators:FDA\". The eCTD summit. Archived from the original on July 16, 2011.\n- \"Merck KGaA Receives Refuse To File Letter From FDA On Cladribine Tablets New Drug Application\". medicalnewstoday.com. Archived from the original on March 5, 2010. Retrieved April 30, 2018.\n- \"Archived copy\". Archived from the original on March 8, 2010. Retrieved February 23, 2010.CS1 maint: Archived copy as title (link)\n- \"Cadence Pharmaceuticals Announces Priority Review and Acceptance of NDA Submission for Acetavance for Treatment of Acute Pain and Fever\". drugs.com. Archived from the original on July 11, 2017. Retrieved April 30, 2018.\n- \"FDA, CDER Office of Generic Drugs\". fda.gov. Archived from the original on May 28, 2009. Retrieved April 30, 2018.\n- \"C&EN: COVER STORY - BEYOND HATCH-WAXMAN\". pubs.acs.org. Retrieved April 30, 2018.\n- Henninger, Daniel (2002). \"Drug Lag\". In David R. Henderson (ed.) (ed.). Concise Encyclopedia of Economics (1st ed.). Library of Economics and Liberty.CS1 maint: Extra text: editors list (link) OCLC 317650570, 50016270, 163149563\n- Chapter 11: Prescription Drug Product Submissions in: Fundamentals of US Regulatory Affairs, Eighth Edition 2013"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:2e845b5d-560e-40db-a692-1cb19bc226bf>","<urn:uuid:533abac9-692c-499a-87d7-9a087ebf7ea3>"],"error":null}
{"question":"What role do automation and environmental factors play in commodity markets?","answer":"In automation, new technologies like robot-powered pizza making and automated fashion picking systems are being developed to manage rising production costs. For environmental factors, while energy innovation is driving down fossil fuel prices, agriculture faces a different dynamic - environmental imperatives for tree planting compete with farmland use, potentially supporting agricultural prices. Carbon pricing has already increased marginal sheep and beef country values by 35% in New Zealand due to forestry conversion potential. This suggests that while automation may help control costs, environmental factors could support agricultural commodity prices relative to energy prices.","context":["News briefs for the week take a look at cobot solutions for pick n’ pack fashion apparel SKUs, exosuit to manage spastic muscle conditions, homemade ingenuity in service robots, a 3D-printed cobot arm that actually “works”, and the 45-second, robot-made pizza in a moving truck.\nCobot solution: pick n’ pack fashion SKUs\nIf you are a cobot vendor and don’t yet have a solution for picking and packing apparel/fashion SKUs, then you’re missing out on a massive, global B2C eCommerce market segment, which is estimated at $752.5 billion (2020), and forecast for 2025 to be $1.1 trillion.\nApparel and fashion don’t ship on hangers. Most often apparel and fashion items are those SKUs that come to a packing area in shiny plastic packaging that’s highly reflective and difficult for vision systems to see; and therefore, difficult to recognize and sort…and, for that matter, to sort quickly and productively (like 700 SKUs an hour). The same shiny packaging might also present difficulties for a cobot’s gripper to properly pick up and sort.\nTherefore, a specially tailored solution to the apparel/fashion challenge is highly desirable. And Netherlands-based Smart Robotics (founded 2015), which is most definitely in the pick and pack cobot biz, has just announced theirs: the Smart Fashion Picker. How appropriate.\nSmart Robotics’ PR release claims that its Smart Fashion Picker “can pick a large variety of fashion items, which are placed in protective wrapping [the aforementioned [“shiny packaging”], straight from a cart or bin. Vision sensors, motion and task planning algorithms help the cobot to calculate what to pick next, as well as how to pick and place the item accurately into the next bin, and it can easily detect when the bin is empty or filled.”\nSmart Robotics says the Smart Fashion Picker is “one of the many improvements it has in the pipeline to make warehouse automation more versatile and dynamic in a highly performant logistics industry”.\n“We’re super excited to introduce this product to the wider market after successful implementation with our initial customers,” says Johan Jardenvall, CEO at Smart Robotics.\nOf course, only time will tell how effective the Smart Fashion Picker is on the job, but, for now, the vendor is presenting the desired tailored Pick N’ Pack system at the exact right time during a growth spurt in the industry.\nWhen exoskeletons become clothes: the exosuit\nThose who suffer from multiple sclerosis (MS), cerebral palsy (CP) or stroke don’t see an exoskeleton or exosuit, rather, they see a godsend!\nInvented in 2009 by Swedish chiropractor, Fredrik Lundqvist, the Exopulse Mollii Suit’s full-body exosuite, with 58 embedded electrodes, fire 20 times per second using targeted electric impulses in order to correct mobility problems. The exosuit is known officially as a drug-free, non-invasive spasticity management device with innovative, near-full-body neuro-stimulation. To Lundqvist, it’s a wearable robot.\nIn 2021, German med-tech giant Ottobock acquired Lundqvist’s Exoneural Network, buying 100% of the outstanding shares. Together with its previous acquisition of US-based SuitX (occupational exoskeletons), Ottobock is now the global leader of prosthetics, orthotics, and exoskeletons.\nLundqvist’s exosuit, still manufactured in Sweden, comes in 37 different sizes starting from 2–3 years of age and up to 5XL for men and women. Cost: $5800 or $540 rental per month.\nBetter still, a lifetime sentence of involuntary and continuous muscle spasms is 100% corrected by a wearable robot (see video).\nIn praise of homemade robots\nCOVID has prompted the spending worldwide of some $228 billion for purchases of endpoint devices, robotics, and drones…just in 2021 alone! But it doesn’t take billions of dollars, as a small town in Indonesia has shown, to create service robots that disinfect and tote food and supplies to those in lockdown. Ingenuity and localized innovation can go a long way.\nWith a robot’s head made from an old rice cooker, chest from a discarded television set, and its base from a junked toy car chassis, the residents of the Tembok Gede district of Surabaya, capital of the Indonesian province of East Java, are making due by basically dumpster-diving and improvising robotics gear.\nParticularly hard hit with COVID, especially the Delta variant (Indonesia has had 3.68 million COVID infections and more than 108,000 deaths), the villagers of Tembok Gede, under the direction of a few local engineering professors, constructed a service robot they named Delta.\nAseyanto (uses only one name), a neighborhood leader who heads the “Delta robot” project, said: “With this new Delta variant and the surging number of COVID-19 cases, I decided to turn the robot into one used for public services such as to spray disinfectant, deliver food and meet the needs of residents who are self-isolating.”\nAfter moving down the street to the home of a self-isolating villager, the robot’s speaker sends out the message “Peace be with you,” followed by “A delivery is here. Get well soon.”\nOperated by remote control, with twelve hours of battery life, Delta is now one of several robots made in the village that has won Tembok Gede a reputation for its creative use of technology.\n3D-printed robot arm that really works!\nThere are many 3D-printed robot arms available, but most are wonderful educational toys and not cut out for real work, like machine tending in a factory or piece picking in e-commerce.\nNot so for Real Robot One or RR1 from the laboratory of Pavel Surynek. Sure, it’s a prototype, but it’s a first, and may well lead to a near future of high-grade, 3D-printed robot and cobot arms becoming regular players in industrial robotics.\nThe criteria for a useful robot or cobot arms are precision, accuracy, repeatability, and strength; and the RR1 checks off all those boxes, which has been a near impossibility via any 3D-printing process, until now.\nRR1 (Real Robot One),” like most multi-axis robot and cobot arms, has stepper motors that rotate the joints. However, the RR1 is unique in that each motor has a 3D-printed planetary gearbox to increase torque and motor output resolution.\n“Those help the robotic arm lift quite a bit of weight and move with precision,” says Surynek. The RR1 has six joints, so it contains six of those gearboxes. The final degree of freedom is the end effector, which is a servo-driven gripper.”\nIn this drawing (see exploded view), the planetary gearbox consists of 3 planets—herringbone gears—and one middle gear connected to the motor axle.\nAmazingly, Surynek has pulled off the design engineering and manufacturing of this complex planetary gearbox with a 3D printer, which is a key development in making robot and cobot arms truly useful.\nRobot’s record-setting, 45-second pizza!\nFounded in 2019, by former SpaceX engineer Benson Tsai, (and 23 other former SpaceX employees), but not launching until the fall of 2022, new robot startup, LA-based Stellar Pizza, has trained its robots to knock out an oven-ready, large pizza in 45 seconds.\nThat’s record-setting speed for an autonomous machine, but still, a slowpoke compared to two-time world champion, Canadian Werner Lomker, who made three large pizzas in an impressive 47.56 seconds—equal to making each pizza in roughly 15 seconds!\nOf course, Stellar Pizza’s robots can work the dough, tomato sauce, and grated mozzarella circuit around the clock without stopping; Lomker’s productivity, on the other hand, has its limits.\nWith the global pizza industry worth $160 billion in annual sales (largest market Western Europe at $60 billion, followed by North America (U.S. and Canada) with $56 billion), there’s value in speedy, highly productive robots.\nAccording to Tsai, the main problem that Stellar Pizza solves is the rising food costs in the United States. The USDA’s inflation watch concluded that found there was more inflation for grocery and supermarket store purchases then for restaurants.\nStellar Pizza hopes to address this problem directly by selling directly to its target customer: people who want food quickly and conveniently. Stellar Pizza has raised $9 million from three VC firms.\n“The process to make them isn’t terribly different from that of pizzas made by human hands; first, a metallic arm lowers into a refrigerated box and grabs a ball of dough, depositing it onto a conveyor belt, where a disc lowers to press it into a 12-inch circle (the company makes the dough at its headquarters then loads it into the machine’s refrigerator in pre-portioned balls).\n“As the raw crust moves along on the belt, various machines dot it with tomato sauce, shake cheese onto it, add other toppings, then lift it into a 900-degree oven for baking. The pie’s journey is tracked by cameras and sensors, which can make adjustments as needed.”\nEach 12-inch, 45-second pizza, depending on topping choices, costs $7 to $10.\nBTW: All of this takes place in a moving truck!","Is the price of food catching up with energy?\nExploring the likely pathways of food versus energy commodities\n1. Will agriculture finally out-perform energy?\nThis Commentary explores the likely pathways of food vs. energy commodities in the 2020s and ‘30s. It explores key factors including renewable energy and synthetic food technologies that are likely to drive their relative performance. It concludes that prices of food commodities are likely to out-perform those of energy in the years ahead.\n2. Why may relative commodity prices matter in a post-Coronavirus world?\nThe world economy is currently reeling from the Coronavirus shock to the real economy (which via business losses will, in turn, hurt the financial and insurance industries). The responses of Governments to Coronavirus are significantly impacting discretionary sectors (particularly services)*. For example, it is estimated 25% of jobs in the UK are likely to be either made redundant or furloughed in 2020.\n* See the Craigmore commentary “Analysing a Possible Coronavirus Recession”\nTo counter falls (estimated at an annual rate of 10% to 20% for at least two quarters) in GDP of the advanced economies, widespread income support is being made available by governments to citizens. In the UK it is likely that the income of approximately 5 million citizens will be supported by the government to maintain their solvency and some of their spending power.\nFor this period the productive capacity of the UK economy will be smaller (due to the lost services of e.g. restaurants and tourism that were until very recently part of GDP). If we assume that those who are supported do not save all the hand-outs, the question then arises; on which goods and services will citizens spend this borrowed (or will it be printed) income?\nPutting it another way, since citizens will not be able to spend as much on foreign holidays or restaurants this year, where will they spend that part of their income they do not use to repair their bank accounts?\nAs before they will of course spend some of it on consumer staples, like food and energy. As before, if well supplied, prices of these will not necessarily increase. However, if the supply of energy or food falls short in coming months, then it is likely a population with the same spending power as before yet fewer things on which to spend it, may bid up any such “tight” commodities.\nThis is the reason why we focus in this Commentary on the likely path of supply and demand for commodities, to seek to identify which are likely to remain abundant, and which might go into short supply?\n3. What has been the history of food commodities vs oil?\nThe graph below plots the relative performance of oil, as the benchmark energy commodity, vs. a simple basket of US$ spot prices of Corn, Wheat, Soy, Live Cattle and Milk over the 50 years since 1970.\nAs can be seen oil significantly out-performed food commodities for most of this period creating a shift in the allocation of world incomes to the oil sector. While the wealth and influence of rural landowners languished. Basically “Dallas” replaced “Downton Abbey”.\nMore recently, however, oil prices have “returned to earth”. Indeed, as can be seen in the graph, indices of energy and food are level-pegging again in 2020, for the first time in 50 years (in fact this graph is a 2-year moving average showing an oil price of around $35. In fact, oil has traded as low as $20 in the last few days).\nFigure 1: After 1970 oil outperformed food – until 2020\nLooking more closely at the (less volatile) price of food commodities these spiked in 2010-2013 (a period of US and Russian droughts coming immediately after the world had eaten up the subsidy driven excess food stocks of the ‘80s and ‘90s). At the peak of this short boom prices had doubled to approximately twice their 2007 levels. They have since fallen back 25% when measured in US$.\nThe US$ has however risen more than 25% since then against many currencies (the US$ TWI index has risen 26% from 102 to 129 in the six years since 2014). As a result, as can be seen in the graph below, farm-gate prices received by most non-US farmers, including in New Zealand after its currency softened over the past three years (as it brought interest rates into line with those of other countries), are right “on trend” i.e. close to as good as they have ever been.\nFigure 2: NZ$ (blue) vs US$ (red) Dairy Prices\n4. What drives the “terms of trade” of commodities?\nPrices of both energy and food commodities are of course primarily determined by changes in the volumes of supply and demand. Increases in productive capacity should lower prices whereas increases in demand for a commodity should lift them. Until 1970 this situation applied in both the oil and the food markets. Large numbers of oil companies and large numbers of farmers competed to supply their commodities to growing markets. And large numbers of consumers paid intermediaries (oil and food companies) for the respective end products. This “Adam Smithian” arrangement remains the case in the (highly fragmented) agricultural markets i.e. prices there continue to be driven almost entirely by supply and demand. Particularly by supply since food demand tends to be fairly constant whereas production varies in the short term as a result of climatic, political and economic events (including, we may soon discover, measures designed to protect human health but which have unintended consequences in terms of labour supply to agriculture, and hence food production).\nHowever, from 1970, the energy markets were transformed when the major oil exporting nations, led by Saudi Arabia, began to influence supply, not in the traditional way (by allowing many companies to each make their own capacity decisions) but by “rigging the market” via the OPEC oil cartel. The cartel actively restricted supply to lift oil prices from $3 per barrel in 1970 to $40 by 1980. Such has been (until recently) the concentration of global supply of oil from the Middle East, that OPEC was able to heavily influence movements in the price of oil for 44 years, from 1970 until 2014. However, as can be seen from the brown line on the graph, since 2014 the price of oil has significantly fallen, especially in the past 10 weeks (when it has fallen from $63 per barrel for WTI Crude on 6th Jan to a price range between $20 and $30 in March, 2020).\n2014 was the year that oil (and gas) from fracking began to surge and put the US on track to become the largest oil producer in the world and a net energy exporter (which was achieved in 2019). This pressure, plus the rise of cheap and attractive renewable energy alternatives has effectively broken the OPEC cartel, further damaged by the withdrawal of Russia as an OPEC observer and ally. It seems likely, now that there are many competing suppliers each making their own decisions on volume of production, oil prices will return to being set, “at the cost of marginal production” (presumably of extraction via fracking in the US mid-west, west and south). Given how rapidly fracking and renewable energy technologies and regulations are advancing, it does not seem likely that these supply (and demand) decisions, at the margin, are now going to drive up the price of oil.\nTherefore, I conclude that abundant supplies of energy make it unlikely the current crisis will lead to sharp rises in prices of oil or other energy commodities. On the contrary, energy and especially oil are likely to remain cheap (and in so doing create a transfer of wealth from energy exporting countries to net importers). Before discussing prospects for food commodities in Section 6, the next Section will review the historic correlation of the two sets of commodity prices.\n5. Are farm commodity prices correlated with oil?\nAs we saw in the previous section, in the 1970s oil prices increased by over 12x. Over the same period farm commodity prices increased by 2.5x. In the 1960s and 1970s a “green revolution” in crop production was sweeping world farming, raising yields and efficiency via improved genetics, fertiliser and machinery. This meant that, although farm commodities kept pace with the high inflation of that decade (the US CPI also increased by 2x during that decade) they did not outperform in line with oil.\nAs can be seen in the graph below, this period of significant out-performance by oil in the 1970s was then replaced by 30 years, from 1984 until 2014, when oil and food commodities traded in lock-step. There were good reasons for that. The green revolution had made farming so efficient that in some countries the price of ethanol from crops was cheaper than that of ethanol from oil. The produce of the two industries traded on an effective “wholesale energy equivalence” basis for most of those three decades.\nFrom 2014, as noted, farm commodities, although soft (down about 25% in US$ terms), significantly outperformed oil, which fell 75% from $100 per barrel in early 2014 to, say, $25 today.\nFigure 3: Relatively Weak Correlation > Strong Correlation > Weak\nMight food prices now revert to their previous high level of correlation, i.e. “follow oil down”? This is the topic of the next sections.\n6. Supply factors for foods\nWe farmers are our own worst enemies. For two centuries we have over-produced and lowered our terms of trade:\nIn the nineteenth century we discovered the “new world” prairies, invented refrigeration, and massively lowered farm relative incomes (from about 70% to 10% of GDP) as a result.\nIn the last century we added only very little new land, and this continues. However, we have adopted technology after technology to produce more grain, meat and dairy from each of those hectares. These techniques have also increased productivity per labour unit and per $ of capital employed. Meanwhile subsidies to farmers in most wealthy countries protected farms against the odd bad harvest – encouraging further over-production.\nThose two centuries period of declining terms of trade for agriculture ended up in the year 2000 with farmers being rewarded, as the rest of the economy grew but our share did not, with farm incomes of less than 1% of GDP in the industrialised west.\nHowever, pleasingly for us farmers in the first 20 years of this century this “fall” in real prices of farm produce has levelled off and indeed real prices of farm produce has increased (the graph below replicates the indices from the first graph but deflated by US$ inflation i.e. shown as real prices).\nWhat is the reason for this improvement in farmers’ pricing power?\nFigure 4: Inflation Adjusted Commodity Indices\nThe two main reasons for improvements in real farm produce prices are:\nYield growth per hectare, while still increasing for some crops (e.g. US corn or NZ dairy), has stabilised across much of the farming sector e.g. average European cereal yields per hectare have been “stuck” at 7 tonnes for 20 years (graph below).\nFigure 5: Western European Cereal Yields\nDiets are significantly changing around the world as, for example, the E Asian middle-class rockets from 500 m people in the year 2010 to 1.5 bn now and heading to 3 bn in 2030. Despite concerns in the west with health and the environmental impacts of animal protein, these newly wealthy people want infant formula, yogurt, cheese, pizzas, chicken nuggets and hamburgers just as do their counterparts in the west. And, since average protein consumption in many of these countries is below the levels experts believe are optimal, it is hard to see how rich western countries, even if they are able to bring down their own (in many cases excessive) consumption of “land intensive” proteins, are going to be able to prevent the populations of these countries gaining the health benefits of moderate amounts of protein in their diets? Since it takes a lot more land to grow animal rather than vegetable protein estimates are that the production of the world farming industry will need to grow by 25-70% by 2050. In order to persuade the “marginal producer” of food to deliver this increase it is likely higher prices will be needed.\n7. Substitutes for “farmed foods”\nHopefully, for environmental reasons, some of the demand of this emerging middle class for protein will be met with synthetic foods, e.g. re-constituted soy into burgers, oats into milk substitute. However, it seems unlikely, as plant based and highly processed foods seldom have the same nutritional density as animal protein, that these synthetic proteins will take over the roles of especially the low-cost parts of the current protein value chain i.e. dairy farming (cheese and fresh milk are after all a big part of the diet of most vegetarians) and also white meat production. We will discuss red meat in the next and final section, below.\n8. Environmental factors\nMankind’s attempts to combat climate change are driving energy innovation to reduce energy costs and substitute production away from fossil fuels. As discussed in Section 2 this is likely to lower the terms of trade of energy products, especially fossil fuels such as oil.\nMany urge a parallel shift in protein production from livestock systems to vegetable protein and synthetic foods. And especially away from red meat, since ruminant animals are more polluting per kg of nutrition than animal protein from dairy, poultry or fish. If implemented (as discussed not all of an emerging middle class will accept limitations on their right to beneficial nutrition) this may, other things equal, also be a negative for the equilibrium prices of conventionally grown food.\nHowever, as readers will be aware, there are simultaneous environmental imperatives for planting trees. Conveniently these are most suited to the same class of land (extensive rangelands) that grow cattle. This fact may make the environmental calculus for farming different from fossil fuels. There are no other uses for oil wells and refineries than to produce and then distil their production into a series of chemicals, most of which will be burned. Forestry and other forms of biological sequestration (e.g. low input, low stocking rate regenerative farming to sequester carbon into pastoral soils) represent viable alternative land-uses in a world that is seeking to address climate change. Therefore, if a reduction in demand for farmed red meat does eventuate, then prices of these and other agriculture products will only fall if demand for land shifted to biological sequestration does not fully off-set reductions in demand for land for red meat.\nTo illustrate this point with a brief case study; Craigmore Sustainables, on behalf of our investors, has been an active purchaser of marginal sheep and beef country in NZ for afforestation i.e. livestock have been retired from this land and it has been planted with trees. The rise in the NZ carbon price (to around NZ$25 per tonne) over the past three years has lifted the price of such land from around $8,000 per hectare (fair value for pure sheep and beef growing) to now around $11,000 (fair value to forestry investors). If this approx. 35% gain in economic returns and land valuations is reflected in other regions as environmental measures are rolled out, we may find land use industries no longer accrue such a derisory share of GDP. It seems logical to this author, and borne out by NZ’s carbon forestry experience, that if mankind really wants to address climate change (and we must) then society will need to reward farmers with more than 1% of GDP. As soon as this is done then, as argued in the Craigmore commentary In “Praise of Trees”, the farm/forest industry is ready and willing to use our skills to off-set climate change using some of our land to sequester carbon, and enabling us to either sell the credits from this or, indeed, to produce carbon neutral produce.\nShifts in tastes toward (more land intensive) animal protein as well as population growth itself are forecast to require an approx. 50% extra production from the same amount of land in the next 30 years. This is achievable, but it seems logical that society, especially if we wish to raise our game to address climate change, will need to pay farmers more than the 1% of developed country GDP to get this.\nIt is too early to know whether the Coronavirus slump and massive governmental fiscal and monetary responses to it are likely to cause inflation (this risk will be the subject of a future Commentary), however, the above analysis suggests that if this occurs it is foodstuffs, not energy (or at least not fossil transport fuels i.e. oil) that would be more likely to exhibit those rising prices.\nEnvironmental imperatives do not, I believe, change this analysis since biological sequestration will compete with food for land resources, further “tightening” capacity of the global farming industry.\nIt is these factors, which may lift the real prices of farm produce in coming years, that should allow farm commodities to continue the break in their historic correlation with energy prices, which already began with the scaling up of fracking and renewable energy substitution, from 2014."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:51a22d33-4d97-47c2-af6c-369406117138>","<urn:uuid:6c6126f0-312a-4e44-91f6-4be818d417f4>"],"error":null}
{"question":"What are the main advantages of wireless temperature sensors over pliance glove sensors when it comes to measuring in hard-to-reach locations?","answer":"While both systems are designed for specialized measurements, wireless temperature sensors have specific advantages for hard-to-reach locations. Wireless sensors can operate without cables in difficult access points like chimneys and through extreme conditions such as high temperature and pressure. The pliance glove system, while advanced for hand force measurements, requires physical connection through fiber optic/USB interface. Wireless sensors can transmit data from many different locations to a central station, making them particularly suitable for monitoring multiple hard-to-reach points simultaneously.","context":["pliance® glove sensor\nDespite the progression of technology, many tasks in the workplace and in daily living must be performed with the hands. Furthermore, it remains a challenge to quantify the manner in which hands are being used and with what level of intensity. These measurements are very important; especially if a task requires repetitive motion, is exhausting, or requires high precision.\nThe ability to measure force distribution across the hand provides a perfect basis for the monitoring of hand forces during routine tasks, for the improvement of working conditions, and for the development of optimized hand tools.\nThe pliance® glove system offers state-of-the-art technology for force distribution measurement of the hand. The system consists of flexible and elastic measuring sensors, a multi-channel analyser, a calibration device, and a software package for Windows PC.\nThe pliance® glove system is primarily designed for measuring force distribution across the hand while using manufacturing and production tools. These measurements may be used to reduce stress on the hand either by providing ergonomic assessment and feedback during the occupational activities or by providing critical data for modifying and improving the standard working process and redesigning the tools.\nThe system may also be used in physical therapy practice for assessing manipulations or for hand rehabilitation. Furthermore, it is useful for research in biomechanics, orthopaedics, and robotics.\nAs is the case for all physical measuring systems, the most important part is the sensor technology. Just like all novel systems, the pliance® glove system is based on calibrated, accurate, and reliable capacitive sensors. The individual sensor elements are elastic and arranged in a matrix which conforms well to three-dimensional shapes.\nThe pliance® glove sensors are a combination of five to twelve finger sensors and one palm sensor consisting of 64 sensors in a matrix arrangement. All critical hand surfaces are covered and the grip force of the hand can be measured. The unique design of the sensors conforms to the shape of the hand allowing various handgrips to be tested in different situations.\nThe pliance® software operates as a Windows application and allows easy data collection of load distribution. Data can continuously be stored in online mode and handled with a configurable SQL database. Different levels of pliance® measuring software are available. Using novel scientific software the expert is also able to design the parameter configuration to specific needs and exchange data with colleagues via HTML protocol. Individual reports can be created by the user to allow quick and easy data analysis.\ntrublu® calibration device\nWith the aid of the trublu® calibration device, all pliance® sensors are individually and simultaneously calibrated with homogeneous air pressure. Alternatively, special weights are available to calibrate individual sensors. Calibration guarantees accurate and reproducible data collection.\nThe pliance® analyser can collect up to 1,024 sensors and is specially developed for mobile measurements. It has a wide range of options, such as master-slave synchronisation of several systems, dynamic amplification control, biofeedback by sound or vibration, and synchronisation of video systems.\nThe pliance® analyser technology allows individual calibration curves for each sensor and also individual dynamic amplification control and crosstalk suppression, resulting in very accurate and reproducible pressure values. The pliance® analyser communicates with the PC via fibre optic/USB interface or via\nFeatures of the pliance® glove system\n• Conforms perfectly to the hand grip\n• Monitors simultaneously different anatomical areas of the hand\n• Each sensor is individually calibrated\n• Can be synchronised with video and other systems like EMG or 3D motion analysis systems\n• 2D, 3D display, or isobar display\n• Allows manifold analysis of measurement data\n• Allows ASCII output\n• Manipulation of tools\n• Occupational medicine\nTechnical data of the pliance® system","Wireless temperature sensors are just as accurate as those connected by the traditional method, but they have the added advantage of allowing measurement in hard-to-reach places. Read the article and find out how they work, where to use them and what issues to think about before buying.\nA wireless temperature sensor is used to measure and transmit temperature information via a digital signal – without the use of cables or wires. The sensor relies on a battery-powered microprocessor that captures temperature information from a nearby area. The device then uses radio waves to transmit this data to a central device.\nThe value measured by the wireless temperature sensor is first transmitted via radio signal to a receiver, such as JUMO Wtrans. From there, the signal can be transmitted in digital or analog form for further processing to various devices such as controllers, automation systems, indicators or recorders, e.g. PLC controller JUMO variTRON 300.\nThe JUMO variTRON 300 central unit can be used as a receiver for JUMO Wtrans wireless sensors. Using a router module, input and output modules can be connected and thus create a complete system with an integrated PLC.\nThe JUMO WTrans B wireless sensor transmits a signal to the receiver. It can then be processed by control devices.\nIn many industrial applications, temperature measurement by the traditional method, based on connecting the temperature sensor by wire to other devices for indication, control, regulation or data recording, is difficult or impossible. This is due to, among other things:\nthe presence of moving parts that would damage the cables\ndifficult accessibility of measurement sites\nlack of cable infrastructure\nhigh cost of long cable routes\ndifficult accessibility through extreme conditions such as high temperature, pH, pressure, etc.\nIn such cases, an alternative is a wireless temperature sensor, which has no wiring and works on the principle of digital signal transmission from the sensor to the receiver.\nTemperature plays a key role in composting plants, as it kills dangerous organisms. According to the bio-waste regulation, it is required to document temperature measurements for all composting batches. What's more, this data must be available at all times to the relevant government verification body.\nSometimes these measurements are carried out manually by composting plant operators, but this generates high personnel expenses. A better solution is wireless temperature sensors, which provide continuous, non-invasive and direct measurement of the temperature in the treated material and automatic recording of these values.\nWireless temperature monitoring system system in the composting plant with wireless temperature probe and PLC controller\nMeasuring system for a composting plant consisting of wireless wTRANS sensors, receiver and JUMO variTRON 500 PLC controller\nWireless temperature measurement also plays an important role in the food industry. Wireless temperature probes can be found, for example, in:\ntanks with mixtures, e.g. for heating chocolate\nmeat production processes\nIn warehouse halls, there is often no cable infrastructure to connect sensors, so wireless temperature sensors are a suitable solution.\nOther examples of wireless sensor applications include:\nRotary ovens - they have moving parts, so cables of traditional temperature sensors would be damaged\nHard-to-reach measurement locations such as chimneys\nWireless temperature sensors are used in the meat industry in smokehouses and production processes, among others. At the image is wireless temperature probe for smoker and meat JUMO wTRANS T.\npossibility to monitor temperature in places difficult to access\ncontinuous monitoring of processes in hazardous environments and reporting of data to the operator at a monitoring facility located at a safe distance\nno wiring costs\ngood temperature accuracy\nlong-term stability of measurements\ncan be used to create a network to monitor many different locations from one station - centralized control of the factory\nIn order to choose the right wireless temperature sensor, you should first analyze the following questions:\nAccuracy and response time: How accurate does the measurement need to be and how quickly should it update?\nMost wireless sensors are as accurate as their wired counterparts; however, readings are usually transmitted every few seconds to conserve battery power. If immediate measurement is required, this should be taken into account when choosing a wireless transmitter, as some models may not offer the desired response time.\nRange: The range of wireless sensors varies widely. Some are designed for short-range indoor applications, while other sensors can transmit data to a receiver many kilometers away.\nRegardless of the sensors' capabilities, the range of a wireless signal is always limited by obstacles. Transmission through machinery, walls and structures degrades signal strength and reduces range capabilities. As a result, the range of a transmitter located indoors is usually much shorter than that of the same transmitter transmitting outdoors in a wide field.\nFrequency: European wireless products typically operate at 868MHz or 2.4GHz.\nWhat do you want to measure?\nWhere do you want to install it?\nHow are you going to power it?\nAre there any space limitations where you will be installing it?\nWhat approximate range do you plan to cover with the measurement?\nAt what frequency do you want to operate it?\nInsert your Text here."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:557d0367-44e2-4fb1-98ae-637d6a1cf93b>","<urn:uuid:a0a165f9-d2d5-4bbf-94f0-0d642a4eab0b>"],"error":null}
{"question":"How do climate change impacts differ between tropical and temperate agricultural regions, and what are the projected consequences for global food availability?","answer":"Tropical regions are more vulnerable to climate change than temperate regions for several reasons. First, tropical crops are closer to their high temperature tolerance limits and experience more temperature stress, despite lower projected warming. Second, insects and diseases, which are already more prevalent in warmer and humid regions, may become even more widespread. Additionally, tropical regions face greater economic and social constraints, including widespread poverty, inadequate technologies, and greater dependence on agriculture. Regarding global food availability, production potential is projected to increase with temperature rises of 1-3°C but decrease beyond that point. Food production changes tend to be less negative or even positive in mid and high latitudes, while simulations in low latitudes indicate more detrimental effects. These regional differences will affect food supply, demand, and consumption patterns globally, potentially leading to changes in international food trade flows.","context":["A slight problem in England could be that parts of the south e.g. London have been slowly sinking into the clay for centuries, so readings might not tell the whole story. When it comes to predicting climate change, most scientists use state-of-the-art supercomputers to model future trends. But researchers at the University of Sheffield are […]\nAgriculture is the basic activity by which humans live and survive on the earth. Assessing the impacts of climate change on agriculture is a vital task. In both developed and developing countries, the influence of climate on crops and livestock persists despite irrigation, improved plant and animal hybrids and the growing use of chemical fertilizers. The continued dependence of agricultural production on light, heat, water and other climatic factors, the dependence of much of the world’s population on agricultural activities, and the significant magnitude and rapid rates of possible climate changes all combine to create the need for a comprehensive consideration of the potential impacts of climate on global agriculture.\nTHE INFLUENCE OF CLIMATE CHANGE ON CROP PRODUCTION\nAt the basis of any understanding of climate impacts on agriculture lies the biophysical sciences. The rates of most biophysical processes are highly dependent on climate variables such as radiation, temperature, and moisture, that vary regionally. For example, rates of plant photosynthesis depend on the amount of photosynthetically active radiation and levels of atmospheric carbon dioxide (C02). Temperature is an important determinant of the rate at which a plant progresses through various phenological stages towards maturity. The accumulation of biomass is constrained by the availability of moisture and nutrients to a growing plant.\nNumerous studies have examined the impacts of past climatic variations on agriculture using case studies, statistical analyses and simulation models (e.g. Nix 1985; Parry 1978; Thompson 1975; World Meteorological Organization 1979). Such studies have clearly demonstrated the sensitivity of both temperate and tropical agricultural systems and nations to climatic variations and changes. In the temperate regions, the impacts of climate variability, particularly drought, on yields of grains in North America and the Soviet Union have been of particular concern because of their effects on world food security. In the tropics, drought impacts on agriculture and resulting food shortages have been widely studied, especially when associated with the failure of the monsoon in Asia or the rains in Sudano-Sahelian Africa. In the temperate regions, climatic variations are associated with economic disruptions; in the tropics, droughts bring famine and widespread social unrest (Pierce 1990).\nGLOBAL ESTIMATES OF AGRICULTURAL IMPACTS\nGlobal estimates of agricultural impacts have been fairly rough to date, because of lack of consistent methodology and uncertainty about the physiological effects of CO2. General studies of how climate change might affect agriculture include those of the National Defense University (1983), Liverman (1986), and Warrick (1988). Kane et al. (1989) broadly predicted improvements in agricultural production at high latitudes and reductions in northern hemisphere mid-continental agricultural regions. The IPCC (199Ob) concluded that while future food production should be maintained, negative impacts were likely in some regions, particularly where present-day vulnerability is high.\nAn international project of the US Environmental Protection Agency (EPA), “Implications of Climate Change for International Agriculture: Global Food Trade and Vulnerable Regions,” has been established to estimate the potential effects of greenhouse gas-induced climate change on global food trade, focusing on the distribution and quantity of production of the major food crops for a consistent set of climate change scenarios and CO2 physiological effects. Other goals of the project are to determine how currently vulnerable, food-deficit regions may be affected by global climate change; to identify the future locations of those regions and the magnitudes of their food-deficits; and to study the effectiveness of adaptive responses, including the use of genetic resources, to global climate change.\nAs part of the EPA project, crop specialists are estimating yield changes at over 100 sites in over 20 countries under common climate change scenarios using compatible crop growth models. The focus is on staple food crops: wheat, rice, maize, and soybeans. The crop models are those developed by the International Benchmark Sites Network for Agrotechnology Transfer (IBSNAT, 1990)—a global network of crop modelers funded by the U.S. Agency for International Development. The choice of the IBSNAT crop models was based on several criteria. First, the models simulate crop response to the major climate variables of temperature, precipitation, and solar radiation, and include the effects of soil characteristics on water availability for crop growth. Second, the models have been validated for a range of soil and climate conditions. Third, the models are developed with compatible data structures so that the same soil and climate data bases could be used with all crops.\nPreliminary national production changes for wheat based on IBSNAT crop model (Rosenzweig and Iglesias, et al., 1992). Results from individual sites have been aggregated according to rainfed and irrigated practice and contribution to regional and national production. The table shows national production changes for the three climate change scenarios with (555 ppm) and without (330 ppm) the physiological effects of CO2 on crop growth.\nIn general, these results show that the climate change scenarios without the physiological effects of CO2 cause decreases in estimated national production, while the physiological effects of CO2 mitigate the negative effects. Production declines occur in many locations, however, even with the compensating CO2 effects. Production changes tend to be less negative and even positive in some cases in countries in mid and high latitudes, while simulations in countries in the low latitudes indicate more detrimental effects of climate change on agricultural production. The UKMO climate change scenario (mean global warming of 5.2deg.C) generally causes the largest production declines, while the GFDL and GISS (4.0 and 4.2deg.C mean global warming, respectively) production changes are more moderate.\nWhen embedded in a global agricultural food trade model, the Basic Linked System (Fischer et al., 1988), the production change estimates based on IBSNAT crop model results will allow for projection of potential impacts on food prices, shifts in comparative advantage, and altered patterns of global trade flows for a suite of global climate change, population, growth, and policy scenarios.\nIn general, the tropical regions appear to be more vulnerable to climate change than the temperate regions for several reasons. On the biophysical side, temperate C3 crops are likely to be more responsive to increasing levels of CO2. Second, tropical crops are closer to their high temperature optima and experience high temperature stress, despite lower projected amounts of warming. Third, insects and diseases, already much more prevalent in warmer and more humid regions, may become even more widespread.\nTropical regions may also be more vulnerable to climate change because of economic and social constraints. Greater economic and individual dependence on agriculture, widespread poverty, inadequate technologies, and lack of political power are likely to exacerbate the impacts of climate change in tropical regions.\nIn the light of possible global warming, plant breeders should probably place even more emphasis on development of heat- and drought-resistance crops. Research is needed to define the current limits to these resistances and the feasibility of manipulation through modern genetic techniques. Both crop architecture and physiology may be genetically altered to adapt to warmer environmental conditions. In some regions it may be appropriate to take a second look at traditional technologies and crops as ways of coping with climate change.\nAt the regional level, those charged with planning for resource allocation, including land, water, and agriculture development should take climate change into account. In coastal areas, agricultural land may be flooded or salinized; in continental interiors and other locations, droughts may increase. These eventualities can be dealt with more easily if anticipated.\nAs climatic factors change, a host of consequences will ripple through the agricultural system, as human decisions involving farm management, grain storage facilities, transportation infrastructure, regional markets, and trade patterns respond. For example, field-level changes in thermal regimes, water conditions, pest infestations, and most importantly, quantity and quality of yields, may lead to changes in farm management decisions based on altered risk assessments. Consequences of these management decisions could result in local and regional alterations in farming systems, land use, and food availability. Ultimately, impacts of climate change on agriculture may reverberate throughout the international food economy and global society.\nAt the national and international levels, the needs of regions and people vulnerable to the effects of climate change on their food supply should be addressed. In many cases, reducing vulnerability to current climate variability should also serve to mitigate the impacts of global warming.\nThe UK Prime Minister, David Cameron, is convinced that ‘fracking’ – or hydraulic fracturing – of shale gas and oil is the answer to the country’s energy supply worries and economic troubles. However politicians such as Caroline Lucas, environmental groups and thousands of local protesters are more than aware of the downsides to this ‘dash for gas’ and are desperate to shout about them loud and clear. So what is fracking, what are the benefits and what are the risks?\nHorizontal hydraulic fracturing is an intensive method of fossil fuel extraction. Where gas or oil lies within the earth in layers or shales, there is the possibility of extracting it by pumping sand, chemicals and a huge amount of water into the shale and letting the fuel come up into a well. The horizontal method is new while conventional oil or gas drilling is vertical. The US has recently undergone a huge ‘fracking revolution’ and the UK has granted over 100 licenses for exploratory drilling while other countries such as China and Poland are also in the process of undergoing this process.\nThe benefits of fracking are largely the benefits to any kind of fossil fuel extraction. Our whole economy is powered by fossil fuel, so nations are always desperate to pull more and more out of the ground – especially as oil extraction around the world starts to decline. For the UK specifically, fracking would create a lot of profit, helping our economy to grow. It will also create jobs, and possibly improve our energy security, although this isn’t definite because the companies that frack will actually just sell the fuel to the highest bidder in Europe – as the whole EU has an integrated energy supply system. Cameron claims it will lower domestic gas bills – but there is evidence to the contrary. Caudrilla itself – one of the biggest companies involved – has admitted that the effect on gas bills will be marginal, if anything.\nSo there are some economic benefits. But there are many downsides. First off, this ‘dash for gas’ completely contradicts the 2008 Climate Change Act, as the continual investment in fossil fuels not only pollutes the atmosphere and deepens the climate change problem, it also diverts investment potential and public spending away from renewable energy technology.\nThere is also a risk of earth tremors – tiny earthquakes – caused by fracking sites. This happened at an exploratory site near Blackpool, causing significant public concern.\nFracking also has significant health risks, as the gas and chemicals can (and often do) leak into the groundwater of the area. This will not only put aquatic wildlife in jeopardy, but can also contaminate local drinking water. If this happens, the health and safety risks to local people are severe.\nThe industrialisation of the countryside is also an issue to many people who live in rural communities. Huge noisy trucks chugging through small villages create traffic congestion and localised atmospheric pollution.\nThe aesthetic value of the English landscape is also damaged by industrial fracking plants. Not only is this a shame in itself, but it will also lower local house prices, which in turn will have a negative economic effect on local areas. Households very near to a site may even have trouble getting house insurance.\nAt the moment both government and the public are massively divided on the fracking debate. It’s proving to be a very controversial issue, with campaign groups such as Frack Free Somerset and Frack Free Sussex cropping up to organise the opposition. The testing site in Balcomb, Sussex, has seen thousands of protesters from both the local area and all over the country camping out for days on end, desperate for their views to be taken seriously.\nIf he’s serious about democracy, David Cameron will listen to the people rather than the pound signs. Is that really too much to ask for?\nThe planet is undergoing one of the largest changes in climate since the dinosaurs went extinct. But what might be even more troubling for humans, plants and animals is the speed of the change. Stanford climate scientists warn that the likely rate of change over the next century will be at least 10 times quicker than any climate shift in the past 65 million years.","5.6.5 Food security and vulnerability\nAll four dimensions of food security, namely food availability (i.e., production and trade), stability of food supplies, access to food, and food utilisation (FAO, 2003a) will likely be affected by climate change. Importantly, food security will depend not only on climate and socio-economic impacts, but also, and critically so, on changes to trade flows, stocks and food-aid policy. Climate change impacts on food production (food availability) will be mixed and vary regionally (FAO, 2003b, 2005c). For instance, a reduction in the production potential of tropical developing countries, many of which have poor land and water resources, and are already faced with serious food insecurity, may add to the burden of these countries (e.g., Hitz and Smith, 2004; Fischer et al., 2005a; Parry et al., 2005). Globally, the potential for food production is projected to increase with increases in local average temperature over a range of 1 to 3°C, but above this it is projected to decrease. Changes in the patterns of extreme events, such as increased frequency and intensity of droughts and flooding, will affect the stability of, as well as access to, food supplies. Food insecurity and loss of livelihood would be further exacerbated by the loss of cultivated land and nursery areas for fisheries through inundation and coastal erosion in low-lying areas (FAO, 2003c).\nClimate change may also affect food utilisation, notably through additional health consequences (see Chapter 8). For example, populations in water-scarce regions are likely to face decreased water availability, particularly in the sub-tropics, with implications for food processing and consumption; in coastal areas, the risk of flooding of human settlements may increase, from both sea level rise and increased heavy precipitation. This is likely to result in an increase in the number of people exposed to vector-borne (e.g., malaria) and water-borne (e.g., cholera) diseases, thus lowering their capacity to utilise food effectively.\nA number of studies have quantified the impacts of climate change on food security at regional and global scales (e.g., Fischer et al., 2002b, 2005b; Parry et al., 2004, 2005; Tubiello and Fischer, 2006). These projections are based on complex modelling frameworks that integrate the outputs of GCMs, agro-ecological zone data and/or dynamic crop models, and socio-economic models. In these systems, impacts of climate change on agronomic production potentials are first computed; then consequences for food supply, demand and consumption at regional to global levels are computed, taking into account different socio-economic futures (typically SRES scenarios). A number of limitations, however, make these model projections highly uncertain. First, these estimates are limited to the impacts of climate change mainly on food availability; they do not cover potential changes in the stability of food supplies, for instance, in the face of changes to climate and/or socio-economic variability. Second, projections are based on a limited number of crop models, and only one economic model (see legend in Table 5.6), the latter lacking sufficient evaluation against observations, and thus in need of further improvements.\nTable 5.6. The impacts of climate change and socio-economic development paths on the number of people at risk of hunger in developing countries (data from Parry et al., 2004; Tubiello et al., 2007b). The first set of rows in the table depicts reference projections under SRES scenarios and no climate change. The second set (CC) includes climate change impacts, based on Hadley HadCM3 model output, including positive effects of elevated CO2 on crops. The third (CC, no CO2) includes climate change, but assumes no effects of elevated CO2. Projections from 2020 to 2080 are given for two crop-modelling systems: on the left, AEZ (Fischer et al., 2005b); on the right, DSSAT (Parry et al., 2004), each coupled to the same economic and food trade model, BLS (Fischer et al., 2002a, 2005b). The models are calibrated to give 824 million undernourished in 2000, according to FAO data.\n| ||2020 ||2050 ||2080 |\n| ||Millions at risk ||Millions at risk ||Millions at risk |\n|Reference ||AEZ-BLS ||DSSAT-BLS ||AEZ-BLS ||DSSAT-BLS ||AEZ-BLS ||DSSAT-BLS |\n|A1 ||663 ||663 ||208 ||208 ||108 ||108 |\n|A2 ||782 ||782 ||721 ||721 ||768 ||769 |\n|B1 ||749 ||749 ||239 ||240 ||91 ||90 |\n|B2 ||630 ||630 ||348 ||348 ||233 ||233 |\n|CC ||AEZ-BLS ||DSSAT-BLS ||AEZ-BLS ||DSSAT-BLS ||AEZ-BLS ||DSSAT-BLS |\n|A1 ||666 ||687 ||219 ||210 ||136 ||136 |\n|A2 ||777 ||805 ||730 ||722 ||885 ||742 |\n|B1 ||739 ||771 ||242 ||242 ||99 ||102 |\n|B2 ||640 ||660 ||336 ||358 ||244 ||221 |\n|CC, no CO2 ||AEZ-BLS ||DSSAT-BLS ||AEZ-BLS ||DSSAT-BLS ||AEZ-BLS ||DSSAT-BLS |\n|A1 ||NA ||726 ||NA ||308 ||NA ||370 |\n|A2 ||794 ||845 ||788 ||933 ||950 ||1320 |\n|B1 ||NA ||792 ||NA ||275 ||NA ||125 |\n|B2 ||652 ||685 ||356 ||415 ||257 ||384 |\nDespite these limitations and uncertainties, a number of fairly robust findings for policy use emerge from these studies. First, climate change is likely to increase the number of people at risk of hunger compared with reference scenarios with no climate change. However, impacts will depend strongly on projected socio-economic developments (Table 5.6). For instance, Fischer et al. (2002a, 2005b) estimate that climate change will increase the number of undernourished people in 2080 by 5-26%, relative to the no climate change case, or by between 5-10 million (SRES B1) and 120-170 million people (SRES A2). The within-SRES ranges are across several GCM climate projections. Using only one GCM scenario, Parry et al. (2004, 2005) estimated small reductions by 2080, i.e., –5% (–10 [B] to –30 [A2] million people), and slight increases of +13-26% (10 [B2] to 30 [A1] million people).\nSecond, the magnitude of these climate impacts will be small compared with the impacts of socio-economic development (e.g., Tubiello et al., 2007b). With reference to Table 5.6, these studies suggest that economic growth and slowing population growth projected for the 21st century will, globally, significantly reduce the number of people at risk of hunger in 2080 from current levels. Specifically, compared with FAO estimates of 820 million undernourished in developing countries today, Fischer et al. (2002a, 2005b) and Parry et al. (2004, 2005) estimate reductions by more than 75% by 2080, or by about 560-700 million people, thus projecting a global total of 100-240 million undernourished by 2080 (A1, B1 and B2). By contrast, in A2, the number of the hungry may decrease only slightly in 2080, because of larger population projections compared with other SRES scenarios (Fischer et al., 2002a, 2005b; Parry et al., 2004, 2005; Tubiello and Fischer, 2006). These projections also indicate that, with or without climate change, Millennium Development Goals (MDGs) of halving the proportion of people at risk of hunger by 2015 may not be realised until 2020-2030 (Fischer et al., 2005b; Tubiello, 2005).\nThird, sub-Saharan Africa is likely to surpass Asia as the most food-insecure region. However, this is largely independent of climate change and is mostly the result of the projected socio-economic developments for the different developing regions. Studies using various SRES scenarios and model analyses indicate that by 2080 sub-Saharan Africa may account for 40-50% of all undernourished people, compared with about 24% today (Fischer et al., 2002a, 2005b; Parry et al., 2004, 2005); some estimates are as high as 70-75% under the A2 and B2 assumptions of slower economic growth (Fischer et al., 2002a; Parry et al., 2004; Tubiello and Fischer, 2006).\nFourth, there is significant uncertainty concerning the effects of elevated CO2 on food security. With reference to Table 5.6, under most future scenarios the assumed strength of CO2 fertilisation would not greatly affect global projections of hunger, particularly when compared with the absolute reductions attributed solely to socio-economic development (Tubiello et al., 2007a,b). For instance, employing one GCM, but assuming no effects of CO2 on crops, Fischer et al. (2002a, 2005b) and Parry et al. (2004, 2005) projected absolute global numbers of undernourished in 2080 in the range of 120-380 million people across SRES scenarios A1, B1 and B2, as opposed to a range of 100-240 million when account is taken of CO2 effects. The exception again in these studies is SRES A2, under which scenario the assumption of no CO2 fertilisation results in a projected range of 950-1,300 million people undernourished in 2080, compared with 740-850 million with climate change and CO2 effects on crops.\nFinally, recent research suggests large positive effects of climate mitigation on the agricultural sector, although benefits, in terms of avoided impacts, may be realised only in the second half of this century due to the inertia of global mean temperature and the easing of positive effects of elevated CO2 in the mitigated scenarios (Arnell et al., 2002; Tubiello and Fischer, 2006). Even in the presence of robust global long-term benefits, regional and temporal patterns of winners and losers are highly uncertain and critically dependent on GCM projections (Tubiello and Fischer, 2006)."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:7225e486-e766-4614-8dcc-d86005d88307>","<urn:uuid:f8ca5935-4fbb-4575-980a-7a0d74c98f39>"],"error":null}
{"question":"Como profesionales de la fotografía del siglo XIX, what were the contrasting approaches to documenting historical events between Mathew Brady's Civil War photography and Edwin Starbird's Woods of Maine series, particularly regarding their methods, subject matter, and ultimate impact on historical documentation?","answer":"Mathew Brady focused on documenting the Civil War, creating a comprehensive photographic record of the conflict through portraits of generals, camp life, and battlefield aftermath. His work evolved from studio portraiture to outdoor documentary photography, including shocking images of casualties at battles like Antietam and Gettysburg. In contrast, Starbird's Woods of Maine series, comprising nearly 600 views, documented wilderness areas, sporting camps, and wildlife in regions like Rangeley Lake and Moosehead Lake. His approach required living in the woods for months, traveling by canoe or foot to capture remote locations. While Brady's work served as direct historical documentation of a major conflict, Starbird's series focused on preserving images of Maine's wilderness and sporting culture. Brady's work had immediate historical significance in documenting the Civil War, while Starbird's photography helped document Maine's natural and recreational heritage.","context":["Edwin R. Starbird Papers\nArranged and described by Brenda Howitson Steeves\nRaymond H. Fogler Library Special Collections Department\nIntroduction and Summary Information\nCollection Title: Edwin R. Starbird Papers.\nDates of the Collection: 1896, undated.\nProvenance: Gift from Muriel Sanford in August 2005 with additions in November 2009.\nCollection Number: MS 1058.\nBox Numbers: 1 (formerly new accession 2005-0812a and 2009-1103a).\nSize and Arrangement: The collection consists of one archival record carton of material (1.3 cubic feet).\nConservation Note: The collection has been re-housed in acid-free folders and an acid-free box, with the photographs housed in polypropylene sleeves.\nPreferred Citation: Edwin R. Starbird Papers, Special Collections, Raymond H. Fogler Library, University of Maine.\nRestrictions on Access and Use: Collection kept at Fogler Library's offsite storage facility. One week's notice required for retrieval.\nThe collection contains photographs taken by Edwin R. Starbird, a professional photographer in the late 19th and early 20th centuries specializing in views of the Maine woods.\nEdwin Roland Starbird, son of Amos and Mary Jane Gilkey Starbird, was born on September 15, 1853, in Freeman, Maine. Amos Starbird operated a sawmill there which made shipping boxes for various food products. He also had mills at New Portland and Solon, Maine, and in Apopka, Florida; the Florida mill made boxes for shipping fruit.\nEdwin Starbird started his working life as a teacher in towns near Freeman, but in the early 1880’s he began studying photography with Francis E. Stanley in Lewiston, Maine. Stanley, along with his twin brother, Freeman O. Stanley, was an inventor and photographer from Kingfield, Maine, who maintained a photography studio in Lewiston from 1880 to 1889. There he developed a formula which improved on the recently invented photographic dry plate process and began to manufacture the plates with his brother. The Stanleys then set up a factory in Watertown, Massachusetts, to manufacture the plates and later went on to develop and manufacture the Stanley Steamer automobile.\nAfter his training with Francis E. Stanley, Edwin Starbird returned to Farmington, Maine, and opened a studio with his brother, Albert W. Starbird, in 1882. Albert left the business within two years, giving Edwin full control of the studio, which did both portrait and landscape photography. He soon began to take stereo views and photographs of the area around Rangeley Lake, including the wildlife and sporting camps in the region. Around 1883 he began a series of photographs known as the Woods of Maine series which came to include nearly 600 views of the Rangeley area as well as Moosehead Lake, the West Branch of the Penobscot River and other wilderness places. Starbird described the series as “… Photographic views of scenes in the hunting and fishing resorts in the interior, or woods of Maine. It is intended that this series shall cover all points of interest to sportsmen and hunters.” This photo project, which lasted until around 1903, required Starbird to spend months living in the woods and traveling to remote areas by canoe or on foot.\nIn 1892 Starbird left Farmington and opened a business in Brunswick, Maine, where he continued to work until 1911. While there he photographed many buildings at Bowdoin College as well as other scenes in the nearby towns of Bath and Topsham.\nAfter closing his studio in Brunswick, Starbird moved to Apopka, Florida, where he died on July 23, 1921.\nScope and Content Note\nThe collection contains 62 photographs taken by Edwin R. Starbird, including 20 identified as part of his Woods of Maine series. It also contains a folder of Starbird family papers.\nThe photographs are arranged in the following categories: Woods of Maine series; Rangeley Lake views; Tim Pond and the Seven Ponds views; hunting photographs; photographs of Bath, Brunswick, Freeport, Topsham, etc.; other Maine photographs; and Florida photographs. The Box List which follows lists each photograph in the collection. Photograph titles enclosed in square brackets were either supplied by the donor, hand-written on the photograph, or abstracted from the catalog prepared by Starbird for the Woods of Maine. The significance of the numbers listed on some of the photographs is unknown. The majority of the photo mounts have “E.R. Starbird Photo, 78 Main St., Brunswick, Me.” inscribed on them.\nThe folder of Starbird family papers contains marriage certificates for Amos Starbird and Mary J. Gilkey, 1852; and for Edwin Starbird and Mattie A. Thompson, 1882; as well as a lease for a hall in Farmington, 1882, and other miscellaneous material.\nAdditional material was added to the collection in 2009, primarily photographs taken by Edwin Starbird of various members of the Starbird family both in Maine and in Florida. Details about people depicted in the photographs have been supplied by the collection’s donor, Muriel Sanford, and by Mary Elizabeth Wheeler of Apopka, Florida. Copies of legal and financial documents of the Starbird family as well as photographs of Strong, Maine are also included.\nWoods of Maine Series\n1 No.1362, Camp comfort at the Birches\n2 The Birches [on Students’ Island]\n3 No.1759, Cottages at the Birches [on Students’ Island]\n4 No.2423, [A cabin at Round Mountain Lake]\n5 No.2420, The cabins at Round Mountain Lake\n6 No.1718, [A scene at Beaver Pond; Ed. Grant paddling a sportsman]\n7 Snowman’s Beach at Kennebago [Lake]\n8 No.1973, View north from Bald Mountain Camps\n9 No.2183, The Rangeley Lake House\n10 No.1787, The south veranda of the Rangeley Lake House\n11 No.1695, [Golf course scene at Rangeley], c. 1\n12 No.1695, [Golf course scene at Rangeley], c. 2\n13 No.1110, A salmon weighing 11 lbs. caught in Rangeley Lake May 20th, 1896\n14 No.2172, Cascades near the Rangeley Lake House\n15 No.172, A moose on Baker Mountain\n16 King and Bartlett Lake from the dining room\n17 The wigwam at Camp Bemis, ME\n18 The new cottage at Students’ Island, ME\n19 Round Pond, Camp Bemis, ME\n20 [Round Mountain Lake from the outlet]\nRangeley Lake Views\n21 Up the Cupsuptic, from Camp Frye\n22 [Scene of a woman standing on shore, fish net, guns and rowboat]\n23 The new cottage [Three men and a woman out front]\n24 Looking toward the outlet from north east shore\n25 Looking up the lake, from near the outlet\nTim Pond and the Seven Ponds Views\n26 The new camp at Big North West Pond\n27 [Hunter with a deer in the snowy woods]\n28 [Man with a bear in the woods]\n29 [Man with a bear at camp]\n30 No. 2119, “Great fun, this!” [Two men with a deer in the snowy woods]\nPhotographs of Bath, Brunswick, Freeport, Topsham, etc.\n31 [Engine of the battleship “Georgia,” now building in Bath]\n32 [Battleship “Georgia,” Bath Iron Works]\n33 [Washington Street, Bath]\n34 [Washington Street, Bath, ME]\n35 [The Common, Bath, ME]\n36 [Public Library, Bath]\n37 [Lewiston Car Barn]\n38 [Searles Science Building, Bowdoin College, Brunswick, ME]\n39 [Searles Science Building, Bowdoin College, Brunswick, ME]\n40 [Memorial Hall, Bowdoin College, Brunswick, ME]\n41 [Entrance gates, Bowdoin College, Brunswick, ME]\n42 [View up the river from Brunswick, water tower across river]\n43 [Maine Street and Park Row, Brunswick, looking north to Cabot Mill]\n44 [Casco Castle, Freeport, ME]\n45 No.1927, The Edgemere at Pemaquid Harbor\n46 [Topsham at Brunswick from Merrymeeting Park; Cow Island, center]\n47 [Pejepscot Paper Mill, Topsham]\n48 Androscoggin River at Topsham, freshet of 1896\n49 [House in Topsham; present residence of Bela Norton, 1896]\n50 [Topsham High School, taken 1890s]\n51 [Topsham School – lower grades and high school, taken in the early 1890s]\n52 [Topsham church picnic]\nOther Maine Photographs\n53 [Lake scene]\n54 [Train track running through the woods]\n55 [Large cottage with eight men on the front porch]\n56 [A moose near West Branch Pond]\n57 [Saw Mill, Apopka(?), Florida]\n58 [Man in a melon field, Florida]\n59 [Housing near a two-story building, Florida]\n60 [Lumber mill scene near railroad tracks, Florida]\n61 [Railroad train on a railroad bridge, Florida]\n62 [Railroad train hauling logs, Florida]\n63 Edwin R. Starbird: family papers\nAdditions to the collection in 2009\n64 Photographs by Edwin Starbird, Farmington, Me. and Florida: unidentified people\n65 Hinds, Minnie Ola and Mabel Olive\n66 Unidentified (Hinds?)\n67 Mason, Zelotes H. (Florida)\n68 Starbird, Adelbert; Starbird, Austin Charles\n69 Starbird, Albert and Leola, Alwyn and Gladys\n70 Starbird, Amos and Mary Jane Gilkey Starbird\n71 Starbird, Austin Charles\n72 Starbird, Clinton\n73 Starbird, Edwin, Mattie (Thompson) and Ralph\n74 Starbird, Lionel\n75 Starbird, Percival Linwood\n76 Starbird, Ralph\n77 Starbird, Ralph and “Aunt Mattie”\n78 Starbird, Ralph and Edith\n79 Starbird family\n80 Miscellaneous family snapshots\n81 Starbird family legal and financial documents\n82 Strong, Maine: photographs and postcards\nFinding Aids for selected manuscript collections in the Special Collections Department at Fogler Library are accessible online in URSUS, in a browsable Guide to Manuscript Collections. Please contact Special Collections at email@example.com or (207) 581-1686 for further information.","Portraits of a Nation\nBy Robert Wilson. Bloomsbury. 288 pp. $28\nIf you were one of Washington’s elite in the late 1850s, sooner or later you would have found yourself at 350 Pennsylvania Avenue N.W., climbing a flight of wooden stairs to a skylighted room, outside of whose windows hung an enormous painted sign for all to see: “Brady’s National Photographic Art Gallery.”\nFrock-coated photographer’s assistants would have escorted you through the gallery itself—a long, comfortable room lined with photographs of the famous, the wealthy, and the merely congressional. Two more flights of stairs would have taken you to a dressing room with a marble washstand and, next to it, under a skylight, the sitting room, where a carved oak chair and a few props awaited you—books, a clock (which always read 11:52), a toga for the senatorially inclined, and a metal clamp to hold your head steady in front of the big, black-draped camera. Usually an assistant operated the camera, but if you were lucky, Mathew Brady, a small, bushy-haired Irishman with a pointed beard and a big nose, might step into the room at the last moment and arrange your pose.\nRobert Wilson, the author of a new biography of Brady, is editor of The American Scholar and a prolific essayist on American topics. In his excellent 2006 life of the American explorer Clarence King, Wilson took on the challenge of dealing with a dramatic, oversized, highly contradictory personality—on the one hand, a fearless conqueror of the wild and rugged Sierra Nevada, on the other, a witty sophisticate who felt perfectly at home among Henry Adams’s exquisite porcelains and watercolors, a beloved member of that most exclusive of 19th-century clubs, “The Five of Hearts.” To the delight of any biographer, King left a copious record of his exciting life—books, letters, numerous geological reports, memoirs by his many friends.\nBy contrast, Wilson’s new subject, Mathew Brady (always spelled that way), lived mostly in hotels, never traveled west of Virginia, and left no more than a handful of letters and newspaper interviews. But he was without question the most important figure in early American photography, and if the written record is scant, and if his personality appears to have been no more than affable, smiling, a little bland, the images he gave the nation before and during the Civil War are the work of a great and passionate artist.\nHe was born around 1823 near Lake George, New York, the son of an Irish immigrant. From childhood, he suffered very poor eyesight, an irony the future “sun drawer” recognized when he told a friend that, even as a boy, “I felt a craving for light.” At 16 or 17 he moved to New York City and, like a real-life Horatio Alger, quickly became acquainted with three classic American visionary hustlers—the artist and inventor Samuel F. B. Morse, the showman P.T. Barnum, and the merchant prince A. T. Stewart, whose “Marble Palace” on Broadway was America’s first department store. Soon enough, with his friends’ methods of self-promotion well in mind, Brady entered the new and wildly popular business of making and selling daguerreotypes. Commercial photography came in existence roughly at the same time that he arrived in the city, but when he opened his studio a few years later, in 1844, there were already numerous established studios catering to a public in love with the idea of cheap, faithful portraits of themselves—“sun drawings,” “sun pictures,” “heliographs,” as the earliest names for photography had it.Photography, Wilson shrewdly observes, was “among the first examples (along with the telegraph and the railway) of a phenomenon that has become almost commonplace in our time—an advance in technology that transforms rapidly from a state of inconceivable mystery . . . to something that everyone could and must have access to.”\nFor two decades, Brady enjoyed unmatched success as a photographer of the successful and the talked about. (With no evident trace of despair, Wilson notes that Brady “helped invent the modern idea of celebrity.”) He had a genius not only for improving the techniques of photography, but also for using light and contrast—there were a clarity and an authenticity to his portraits, people agreed, that no one else could achieve. Much of what we remember—what we see—of antebellum America comes from his unforgettable photographs of such notables as Edgar Allan Poe, General Winfield Scott, and the “Swedish Nightingale,” Jenny Lind. His beautiful and delicate photograph of the young Henry James with his father is worth volumes of scholarship.\nThis was no haphazard achievement. Brady’s ambition, as he himself declared, was “to form a gallery which shall eventually contain life-like portraits of every distinguished American now living.” But as Wilson reminds us, the word “photography” means “writing with light.” Brady’s ambition to form a national gallery evolved into something grander. Toward the end of his life, Walt Whitman recalled that he and Brady had “had many a talk together: the point was how much better it would often be, rather than having a lot of contradictory records by witnesses and historians—say of Caesar, Socrates, Epictetus, others—if we could have three or four or half a dozen portraits . . . that would be history—the best history—a history from which there would be no appeal.”\nWhen the war came, the historian seized his chance. Wilson traces in fascinating detail Brady’s enterprise of making a photographic record, from which there indeed would be no appeal, of the Homeric struggle to restore the Union. He is especially good on Brady’s close relationship with Abraham Lincoln—in all, Brady and his assistants gave us at least a dozen photographs of the president—and on his business rivalry with Alexander Gardner, his onetime assistant and partner. These two were hardly the only photographers of the war—hundreds of camera wagons and camera operators followed the Union armies wherever they went. (For a variety of reasons, photographers who attached themselves to the forces of the Confederacy took far fewer pictures.) The operators almost never attempted to photograph actual combat—their cameras required too long an exposure time—but their portraits of camp life and celebrated generals were on display all over the North.\nThen, at Antietam in 1862 and Gettysburg the following year, photographic journalism took a decisive step. Alongside bucolic images of men gathered around a cooking fire or stacking muskets, now came shocking images of gray corpses left unburied on the field, of soldiers bloody and torn, white bones and skulls. Gardner showed a particular affinity for such photographs—at Gettysburg, he actually moved a Confederate corpse to a more dramatic position and repositioned the dead soldier’s rifle beside him—but Brady excelled in the kind of portraits he had mastered before the war: faces and settings that revealed, as nothing else could, the essentials of a personality. As Wilson notes, he seems simply to have moved his portrait studio outdoors. In one great photograph, Ulysses Grant leans against a tree shortly after the last terrible, almost suicidal assault at Cold Harbor in 1864, an assault that Grant would confess in his Memoirs he had always regretted ordering. Yet even at such a moment, no one can miss the placid, mysterious, indomitable confidence in Grant’s expression, a deep inner character exposed by the artist to light.\nWilson analyzes a number of such photographs—he is especially good on Brady’s sly practice of including himself, Hitchcock-like, in some of his wartime pictures. In perhaps the best of these, taken after the Battle of the Wilderness in 1864, the photographer arranged the Union general Robert Potter and members of his staff according to height in front of their tents. The men, wearing hats, look at their bareheaded general, who stares grimly ahead at the camera. Above them, the silvery Virginia pines stretch to the sky. And to the right of the frame stands a well-dressed civilian, hand on hip, leg casually cocked. It is, as Wilson tells us, the author of the image: “Brady has posed himself as what he was, not the subject of the photograph but its presiding intelligence.”\nBrady’s life after the war makes for sad reading. Childless, a widower, he began a slow descent into poverty.Late in life he would claim that the ruinous expense of photographing the war had put him on the road to bankruptcy. Wilson makes it clear, however, that Brady’s own prodigality, a lifelong trait, was the real cause of his declining fortunes. For several more decades, he held on to his studio in Washington. But business fell away, and his efforts to sell his great collection of Civil War negatives came to little. He died in poverty in New York at the age of 72, without enough money for a headstone.\nThere have been other biographies of Brady and several fine discussions of his art—Mary Panzer’s Mathew Brady and the Image of History (1997), for example. But Wilson’s book is notable for its thorough, up-to-date narrative. And his responses to Brady’s work are criticism of a high order. In a beautiful passage about the Gettysburg photographs, he ponders a series of pictures of Brady studying the landscape where General John Reynolds had died—“the photos,” Wilson says, “introduce in an explicit way a human consciousness of the violence that had been played out in these now-serene fields.”\nTo the argument that such images are no more than a chemical recording of lights and darks, he replies that Brady in fact created what might be called “first-person photography.” The drama and intensity of his works make plain that “a photograph is not just the doings of a sunbeam, an objective rendering of a scene, but a view created, in effect, by an individual consciousness. . . . They steal photography from the sun.”"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:111e3b70-053a-4a65-8c79-741b8bf7d5bc>","<urn:uuid:f5ab49ad-d6a3-4f2a-b9f9-5425213f1cf5>"],"error":null}
{"question":"How do BRBX anti-vibration mounts compare to structural thermal break plates in terms of their shock absorption capacity and thermal insulation properties?","answer":"BRBX anti-vibration mounts primarily function as shock and vibration absorbers in machines producing high vibration levels, while also reducing friction and noise pollution. When selecting these mounts, their shock absorption capacity and ability to absorb almost all vibrations are crucial factors. In contrast, structural thermal break plates like Farrat TBK and TBL are specifically designed for thermal insulation, with different performance characteristics. For example, Farrat TBK has a characteristic compressive strength of 312 N/mm² and thermal conductivity of 0.187 W/m-k, while Farrat TBL has lower values at 89 N/mm² and 0.292 W/m-k respectively. These thermal break plates are primarily used between horizontal and vertical connections to prevent thermal bridging rather than for vibration control.","context":["Apart from absorbing vibrations, anti-vibration mounts have several other applications in a machine which are meant to complement the functions of the machine and thus making the machine’s output more efficient and of the best quality. Different anti-vibration mount has various applications.\nApplications of anti-vibration mounts.\n- Cone mounts\n- Act as vibration and sound absorbers which is their primary purpose.\n- Used in static parts of vehicles, machines, and engines.\n- Also, serve as shock absorbers in some cases.\n- Marine mount.\n- They are mostly used in marine vessels to reduce sounds and also to increase the lifespan of these machines.\n- Marine anti-vibration mounts are also used in rotating machines which produce a lot of vibrations and noise like water pumps, airplane engines, trains, marine-land units, industrial vehicles and machines among others.\n- BRBX anti-vibration mounts\n- Act as shock absorbers in machines.\n- Mostly used as vibration absorbers in machines that produce high levels of vibrations.\n- BRBX also help reduce friction in machines and thus increasing the machine’s lifespan.\n- The BRBX anti-vibration mounts also act as noise absorbers and thus reduce noise pollution.\n- BRB anti-vibration mounts also have the same applications as BRBX anti-vibration machines.\nFactors to put into consideration when purchasing anti-vibration mounts.\n- Shock absorption capacity; an anti-vibration mount that absorbs almost all the vibration and can withstand high levels is always the best as its performance levels are very high, and so is its lifespan.\n- Quality; high-quality anti-vibration mounts are the most effective as they absorb all vibrations produced by machines and therefore also reduce noise pollution. Moreover, high-quality anti-vibration mounts also help increase the machine’s efficiency at work as well as increasing the machine’s lifespan.\n- Cost; just like other machines, the quality of anti-vibration mounts increases with increase in the price, and therefore the most expensive anti-vibration mounts are always the best.\n- Type of machine; different anti-vibration mounts are designed for machines, and therefore it is always advisable to check on the kind of anti-vibration mount before making purchases. In most cases, the names of different anti-vibration mounts are engraved on either the mount’s cap or on its basement.\n- Ability to withstand large amounts of the load; in most cases, the anti-vibration mount is sandwiched between two parts of the machine. Therefore, the anti-vibration mount should possess sufficient strength to with carrying the substantial portions of the machine on the load size;\n- The material used in making the anti-vibration mount. The best anti-vibration mounts are made from metals and rubber of high quality and one that cannot break or leak vibrations.\n- Size; the size of anti-vibration mounts always vary and thus one should be cautious when buying anti-vibration mounts. The size of every anti-vibration mount is engraved on either the cap or on the mount’s basement adjacent to the mount’s name.\n- Availability; when buying an anti-vibration mount, it is advisable to purchase one that is readily available which makes future purchases for replacement easy and very fast. Moreover, purchasing directly from the manufacturer or a known dealer is also advised to avoid purchasing a counterfeit anti-vibration mount.","Farrat Structural Thermal Break Plates/Pads are high performance thermal insulators used between horizontal and vertical connections of internal and external elements to prevent thermal or cold bridging.\nStructural Thermal Breaks provide a simple, economical and extremely effective solution to meeting Part L of the Building Regulations by way of reducing heat loss and the risk of internal condensation. Farrat Structural Thermal Breaks have British Board of Agrement Certification [BBA]. This is important in a market where there are materials offered which have not had independent evaluation to ensure suitability in structural connections.\nFarrat Structural Thermal Breaks also meet the NHBC’s technical requirements.\nChanging Legislation in response to climate change and energy saving has meant that Farrat now supply Structural Thermal Break Plates for the UK and overseas market.\nConstantly driven by engineering excellence, we continue to lead the way in the development of the Structural Thermal Break Plate market with the following Certifications and membership:\nFarrat Structural Thermal Breaks have British Board of Agrèment Certification [BBA].\nFarrat Structural Thermal Breaks meet the NHBC’s Technical Requirements. This is referenced in the BBA Certification.\nFarrat is a member of BRE’s Certified Thermal Details and Products Scheme.\nFarrat Structural Thermal Breaks can be found on NBS Plus and NBS National BIM Toolkit and Library.\nFarrat operates under an ISO 9001:2008 Quality Assurance System. This also incorporates BBA’s Product Quality Plan.\nFarrat operates under an ISO 14001:2004 Environmental Management System.\nFarrat is a member of The Steel Construction Institute [SCI].\nFarrat Structural Thermal Breaks are manufactured from high performance materials. We only use materials specifically developed for use within the building envelope and have British Board of Agrement Certification [BBA] to ensure that designers and clients have confidence in the product which is used in structural connections. Every order is accompanied with a Certificate of Conformance.\nWe offer two grades, Farrat TBK and Farrat TBL.\n|Farrat TBK||Farrat TBL|\n|Characteristic Compressive Strength, fck (N/mm² , MPa)||312||89|\n|Design Value for Compressive Strength, fcd (N/mm² , MPa)||250||70|\n|Elastic Modulus (N/mm² , MPa)||4100||2586|\n|Water Absorption (%)||0.14||0.48|\n|Thermal Conductivity (W/m-k)||0.187||0.292|\n|Colour (may vary)||Amber||Black|\n|Thicknesses available (mm) +||5, 10 ,15 ,20 & 25||5, 10 ,15 ,20 & 25|\n|Thickness Tolerances (mm) ++||0 to +0.3||0 to + 0.25 (TBL5)\n+0.2 to +1.5 (TBL10)\n+0.3 to +2.5 (15, 20 & 25)\n|Maximum sheet size (mm)||2400 x 1200||2500 x 1250|\n|+||Multiple plates can be provided for applications where thicknesses greater than 25mm are required. Both materials can be supplied in non-standard thicknesses (please contact Farrat for further details).|\n|++||Farrat TBL can be supplied to tighter tolerances (please contact Farrat for further details).|\nFor further details, please refer to our Structural Thermal Breaks brochure.\nThere are few standard construction details between projects therefore detailing of the building envelope and penetrations can vary significantly. As a result, the calculation of thermal performance and compliance with requirements can be complex.\nThere are two aspects to the thermal performance of the building envelope; heat loss and condensation risk. Both issues are covered by Building Regulations and guidance on meeting them is provided in various Approved Documents (England and Wales), Technical Handbooks (Scotland) or Technical Booklets (Northern Ireland). These documents currently require heat loss and condensation risk to be assessed in accordance with the same British Standards, European Standards and BRE Publications.\nUnlike proprietary mechanical thermal break systems, the plate type thermal break is very simple to incorporate into most details. This flexibility means that it can be used for a wider variety of applications and is not restricted by the modular nature or the space required for proprietary mechanical systems. This flexibility also provides the Designer with greater freedom to develop a bespoke solution.\nThermal Design Considerations:\nHow thick does the thermal break need to be?\nIdeally the detail should be thermally modelled. This requires not only the members and connections, but the entire fabric of the envelope local to the connection to be included in the model. This applies to both mechanical and plate type thermal breaks.\nThis issue is often forgotten or considered late in the construction process and due to the cost and time implications, modelling is often not undertaken. However, modelling should be considered where:\n- the environmental conditions pose a greater risk (e.g. swimming pools)\n- the detailing of the planar elements local to the connection are considered to have an inferior thermal performance to that of the main building envelope\n- there is significant repetition of the same detail (e.g. balconies).\nIf thermal modelling is not undertaken, the following should be considered:\n- The thermal break should be located within the insulated zone of the building envelope.\n- Selection of the thickest thermal break (up to 25mm) considering cost, thermal performance and structural requirements (limitations).\n- Minimisation of the cross sectional area/ mass of the steelwork penetrating the building envelope where possible.\n- The performance of the connection detail against the BRE’s Certified Farrat details – information provided below.\nStainless steel bolts are sometimes specified for durability reasons. Isolation using normal methods may need to be considered because of bi-metallic action and corrosion. Isolation using thermal washers and thermal bushes will provide minimal additional thermal performance.\nPoint thermal bridge\nThe quantity which describes the heat loss associated with a single penetration is a point thermal bridge(χ-value, W/K). This is pronounced as ‘chi-value’. This is a property of the thermal bridge and is the rate of heat flow per penetration that is not accounted for in the U-values of the plane building elements containing the point thermal bridge.\nLinear thermal bridge\nThe quantity which describes the heat loss associated with a thermal bridge is its linear thermal transmittance (Ψ-value, W/m·K). This is pronounced as ‘psi-value’. This is a property of a thermal bridge and is the rate of heat flow per degree per unit length of the bridge that is not accounted for in the U values of the plane building elements containing the linear thermal bridge.\nThe Specifier will usually identify indoor and outdoor temperatures and relative humidity conditions under which condensation must not occur. Guidance on suitable conditions is given in BS 5250 Code of Practice for the Control of Condensation in Buildings. From these conditions it is possible to determine the allowable minimum temperature on the construction detail below which there would be a risk of condensation. Finite Element Analysis and similar analysis methods allow the temperature distribution to be predicted.\nThe temperature factor (f) is used to assess the risk of surface condensation or mould growth and is calculated under steady state conditions. To avoid problems of surface condensation or mould growth, the fRsi should not be less than a critical temperature factor (fCRsi).\nA range of appropriate critical temperature factors are identified in BRE Information Paper IP 1/06 and listed below:\n|Building type||Critical Temperature Factor (fCRsi)|\n|office, retail premises||0.50|\n|dwellings, residential buildings, schools||0.75|\n|sports halls, kitchens, canteens||0.80|\n|swimming pools, laundries, breweries|\nBRE Certified Thermal Products Scheme\nFarrat is a member of BRE’s Certified Thermal Details and Product Scheme.\nThe scheme database includes for both BRE Certified Thermal Details and Products and Government Accredited Details, and this provides a freely accessible and independently assessed and certified resource for users. The third-party BRE Global certification can distinguish products and services from their competitors, and give customers confidence about the thermal performance of the products.\n- storage buildings\n- retail premises\n- residential buildings\n- schools and sports halls\n- kitchens and canteens\n- swimming pools\nThermal Modelling Specialists\nGraeme A. Hannah (MEng) – Technical Manager\nBRE Certified Thermal Details and Products Scheme\nT: +44 (0) 1355 576 000 E: email@example.com\nAnnalisa Simonella – Director\nLoud1 Design | Building Physics\nT: +44 (0)7906 082 828 E: firstname.lastname@example.org\nDr. Richard Harris – Partner, Consultancy Department\nT: +44 (0)20 7565 7066 E: email@example.com\n- Thermal bridge in a link without a Farrat Thermal Break. The temperature of the steel is on the hot side of the outer-wall system (9.8°C) and heat loss (χ value) is 1,31W / K.\n- Distribution of temperature with Farrat Structural Thermal Break plate (TBK). The temperature on the hot side of the facade system has been improved to 16.5°C and the heat loss is limited to 0.35 W/K = 73% less heat loss.\nUnder the SCI Assessed Product Scheme the technical data and structural design methodology for Farrat Structural Thermal Breaks has been independently verified by the SCI. The design considerations are set out in the Farrat Structural Thermal Breaks Connections Guide.\nUnlike proprietary mechanical thermal break systems, the plate type thermal break is very simple to incorporate into most details. This flexibility means that it can be used for a wider variety of applications and is not restricted by the modular nature or the space required for proprietary mechanical systems. This flexibility also provides the designer with greater freedom to develop a bespoke solution.\nStructural Design Summary (steel connections)\nConnections that include thermal break plates should be designed in accordance with the relevant design standards (e.g. BS EN 1993-1-8) or industry guidance (e.g. SCI publications). The following additional checks should also be undertaken, check that:\n- the thermal break plate can resist the applied compression forces.\n- any additional rotation due to the compression of the thermal break plate (including allowance for long term creep) is acceptable.\n- the shear resistance of the bolts is acceptable given that there may be a reduction in resistance due to:\n- PACKS – Clause 22.214.171.124 of BS 5950-1 or clause 3.6.1(12) of BS EN 1993-1-8\n- LARGE GRIP LENGTHS – Clause 126.96.36.199 of BS 5950-1 or BS EN 1993-1-8\nStructural Design Considerations:\nThermal break plates are contained within the protective envelope of the building and in general Building Regulations do not require them to be fire protected or have a fire performance rating. Where the connection containing the thermal break requires fire protection then the following options can be considered:\n|Board Protection||A number of proprietary fire protection board systems are available on the market.|\n|Sprayed Fire protection||A number of proprietary sprayed fire protection systems are available on the market. The manufacturer should be consulted regarding the compatibility between the system and the thermal break materials. Alternatively consideration can be given to recessing the thermal break and providing a continuous fire protection strip (Nullifire etc.)|\nThe majority of thermal break connections are related to secondary elements only.\nThe Structural Engineer will consider robustness during the design process and will refer to local codes and standards. Where a thermal break is located within a key critical element this may need further analysis leading to either consideration of the complete loss of the thermal break or inclusion, for example, of a physical “fail safe”. The detailing of this can often be undertaken whilst maintaining the thermal performance of the connection.\nHandling on site\nThermal breaks are normally procured by the steel fabricator as part of the steel frame package on a project. The delivery from Farrat is normally co-ordinated with the steel work contractor erection schedule. They are delivered to site with each one labelled with a unique reference linked to the steel work contractors drawings.\nFor identification purposes Farrat TBK and TBL are different in colour. If it is essential to the project that both materials are used on the same project, Farrat normally advise that the connection arrangement (e.g. bolt positions) is unique to ensure that no errors are made during installation. This is in addition to Farrat’s normal labeling protocol.\nThe general handling requirements for thermal breaks should be in line with other component accessories expected to be handled with the primary steel work. This is covered in the NSSS: Section 8 Workmanship – Erection. The NSSS also sets out the requirements of the Quality Management System expected to be adopted by all competent steelwork contractors working on UK construction projects.\n- Structural Thermal Break Plate (TBK) with 4-hole connection, steel-to-steel.\nFarrat Structural Thermal Breaks can be used in a wide variety of applications where there is a structural requirement of the thermal insulation:\n- Steel to steel\n- Steel to concrete / masonry\n- Steel to timber\n- Concrete to concrete\n- Facade system connections to the primary frame\n- Brise Solei and Canopies\n- Roof plant room columns\n- Connections of external to internal primary building elements\n- Isolation of sub-structure & basement structure elements\n- External staircases or external balconies\n- Man-safe systems\n- BMU Systems\n- Connections to existing structures\nFarrat’s market leading Structural Thermal Break Plates & Pads (FTB) are high performance thermal insulators, used between horizontal and vertical connections of internal and external elements to prevent thermal/cold bridging.\n- Mechanical properties of the materials aligned for building applications\n- High or very high strength options\n- Low thermal conductivity (k)\n- Ability to be manufactured in 2D or 3D (i.e. recess, chamfer, etc.)\n- Variety of thicknesses available. Special thickness/tolerances available to specifiers\n- British Board of Agrement Certification [BBA]\n- A simple and effective solution to meeting Building Regulations\n- Not a proprietary modular mechanical system – so offers the designer scope to develop bespoke connection detailing\n- Supported by technically qualified staff\n- Supported by external organisations including BBA, NHBC, NBS and BRE.\n- Manufactured under Farrat’s ISO9001 & ISO14001 Systems\n- Manufacturing capacity allowing us to meet your lead time\nShould show a fully detailed connection or one communicating the design intent with a supporting specification (NBS or similar).\nIs normally responsible for ensuring that the connection meets the requirements of the Building Regulations Part L (SAP).\nDesign Output – Thermal performance/ Thickness (Farrat TBK or Farrat TBL).\nThe Structural Engineer\nIs normally responsible for designing the connection or providing a performance specification for the steel work fabricator.\nDesign Output – Strength (Farrat TBK or Farrat TBL)\nSample Specification for project using Farrat TBK – National Building Specification (NBS)\nNBS Clause: G10/ 350 Structural Thermal Break Connection Plate\n- Manufacturer: Farrat Isolevel Ltd, Balmoral Road, Altrincham, Cheshire, WA15 8HJ, Tel: +44 (0)161 924 1600, Fax: +44 (0)161 924 1616 www.farrat.com\n- Product Reference: Farrat TBK\n- Thickness: 25 mm\n- Plate Size: As Drawing number – or to be determined by the connection designer\n- Hole Size & Positions: As Drawing number – or to be determined by the connection designer\n- Certification – BBA\nPlease be aware there are cheaper materials on the market that the supply chain may provide as an alternative, but in our view not of equal performance or certificated for use in building applications (structural).\nTo enable us to provide a quotation, we will require the following information for each plate:\n- Material – Farrat TBK or Farrat TBL\n- Plate Dimensions\n- Plate Thickness\n- Size and Number of Holes\n- Any Special Requirements\n- Delivery Location\nTo accept orders, our manufacturing facility will require a fully dimensioned drawing with each plate type having a unique customer reference (drawing number).\nWe aim to start manufacturing within 3 working days of receiving the order and you will be advised of a despatch date.\nWe can very often start manufacturing sooner, and can work with you on very large orders to meet your programme and requirements.\n- Each plate has a label attached [Farrat/BBA]\n- Each order will be accompanied by a Certificate of Conformance under our British Board of Agrement Certification.\nQMU Graduate Centre\nType: TBK Thermal Breaks\nType: Structural Thermal Breaks\nType: Structural Thermal Breaks\nAIRC Cranfield University\nType: TBK Thermal Breaks\nNottingham Trent University\nType: TBL Thermal Breaks\nTo provide a quotation please submit the following information:\n- Material Type – Farrat TBK or Farrat TBL\n- Plate Dimensions & Thickness\n- Size & Number of Holes*\n- Delivery Postcode\nWe aim to start manufacturing within 3 working days from an order being placed.\n*A fully dimensioned drawing will be required for each type of plate with a unique reference prior to fabrication.\nStephen Blundell BEng(Hons) CEng MICE MIStructE\nTechnical Director – Structural Thermal Breaks\nDD: +44 (0)161 924 1600\nStructural Thermal Break Connections\nStephen is a Chartered Engineer with over 35 years experience in civil and structural engineering. As Technical Director for Structural Thermal Break Connections at Farrat, Stephen is the technical lead for thermal break systems and specification."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:4cd4bff6-8111-4d7e-b774-a5fdd328f569>","<urn:uuid:5418009a-1a97-44de-9c7c-a25030b5fa55>"],"error":null}
{"question":"Could you explain why Triton is considered one of the most fascinating bodies in the Solar System? I'm curious about its unique characteristics.","answer":"Triton is considered fascinating because it has an extremely young surface (possibly less than a few million years old) and may still be geologically active today. Despite having a maximum surface temperature of only 35 degrees above absolute zero, it features volcanoes and geysers that have transformed its surface, potentially within human history. These features were so remarkable that even Voyager scientists, who were accustomed to planetary surprises, were almost speechless when they discovered them.","context":["Today’s blog features a correction and some additional details on the new Triton map and movie blog posted a few days ago > stereomoons.blogspot.com/2014/08/triton-at-25.html. <\nFirst, a correction. The surface compositions of Triton and Pluto are indeed similar but not quite identical. Triton has nitrogen, methane, carbon dioxide and carbon monoxide on its surface, and probably some water ice, but of those ices Pluto does not have carbon dioxide or water ice that we can measure from Earth. What those differences may mean for geologic and atmospheric history no one can say as yet with confidence, but all the more reason for going to Pluto and someday back to Triton.\nAs a matter of personal opinion, I am sometimes asked which planets I’d like to see explored next. Europa is first on the list, but after that we have the ice giant planets Uranus and Neptune and their strange families of icy moons (including Miranda, Ariel, and Triton to name a few). These large bodies are distinct and different from the gas giants Jupiter and Saturn but have been visited only once, by Voyager 2 with instruments designed in the 1970s. What we could learn by going back has been amply demonstrated by the innumerable discoveries of Cassini at Saturn.\nTriton, whose surface may be younger than a few million years and may be geologically active today, is one of the most fascinating bodies on the Solar System. Its maximum surface temperature is only 35 degrees above absolute zero, and yet volcanoes and geysers have remade its surface, possibly within the lifetime of the human species. Even the Voyager scientists, who had become accustomed to surprises after the discoveries on Io, Ganymede, Titan, Miranda and the rest, were left almost speechless as Voyager made its final planetary visit. As Larry Soderblom exclaimed at the press briefing when he showed the first Triton images, “What a way to leave the Solar System!”\nT-shirt printed up to during the Neptune encounter 1989. The t-shirt and owner are now 25 years older.\nNeptune was fabulous too with its strange and dynamic cloud patterns and its odd, incomplete ring system. One of my first efforts in serious image processing was to reconstruct the Neptune ring high-phase-angle observations. These were the best images of the rings we got, but the long exposures saturated Neptune itself and created bright haloes that were difficult to suppress. Normally exposed Neptune crescent images were substituted but the heavy filtering required for the bright haloes also enhanced noise in the images. The end result was a montage showing a crescent Neptune and the entire ring system. This was done back in 1992 or 93, so I’m sure I or someone else could do a better job now. It is a composite of 5 (or 6?) different exposures taken at different times and distances from Neptune, but all the data are real.\n|Crescent mosaic of Neptune from Voyager 2 on departure, August, 1989.|\nTriton Map: Enhancement and 'Color'. The enhancement applied to the Triton map in the August 21 post was a modest contrast-stretch only; no differential color enhancement was applied. Surface brightness contrasts on Triton exist but are not as strong as on Pluto. The color does have a greenish cast in equatorial areas. This seems to be real, but there are ‘concerns’ with Triton’s color. First, the color images were sometimes smeared or noisy, due to long exposures under very low solar lighting intensity for which the cameras were not designed. This explains some of the splotchy color mottling that is apparent in a few areas. Secondly, there are some uncertainties in the photometric properties of Triton. Earth-based spectra of Triton obtained in the 1970’s and 80’s differ in the inferred visual color of Triton and it was not possible to get an exact color ‘calibration’ on Triton. We did our best, but the colors may not be only approximate, given the slightly different color sensitivity of the Voyager 2 camera.\nThe Triton map is suitable to drop into Google Earth or similar programs! You can now zoom and spin on Triton in any way you like.\nNeptune in the Movie. Several have asked why Neptune doesn’t appear in our movie. Several reasons, the most important of which is that we ran out of time for the August 25 anniversary. The second is that we compress almost 10 days of the encounter into 1 minute. Neptune would probably appear in 2, maybe 3 of those frames. We are looking into it. We know that Neptune and Triton do appear together in the sky about a day out from Triton, and again 6 days later, but do not appear in proximity to each other on the way in, apparently. We may attempt to add Neptune back in for a final version later this year."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:ed308921-dc2f-4441-bb4f-e01984f85367>"],"error":null}
{"question":"What subjects are covered in First Class Area Academy's house management training seminar?","answer":"First Class Area Academy's one-day house management seminar covers three main areas: 1) Duties of Jr. House Manager, House Manager and Estate Manager, 2) Successful communication between employee and employer, and employee to employer, and 3) Acquiring and retaining capable household staff.","context":["Household Management Courses\nLondon Academy for Household Staff Training, Polo and Tweed, Starkey International (US based), UK Home College (online course), Professional Domestic Services & Institute, Luxury Household Management (US based), First Class Area Academy (US based), and Perfect Household Staff provide short-term executive courses in Household Management. These eight reputable institutions provide at least one course in household management, and three of these institutions are US-based. However, only First Class Area Academy explicitly states that it offers a course that covers the pitfalls and challenges of building and running a household, and how to hire and manage a household team.\nIn order to identify short-term, executive education courses on how to hire and manage a household team (such as a butler, nanny, maid, chef, chauffeur, etc.), my colleagues and I attempted to identify seven to ten courses from trusted, reputable organizations and/or educational institutions that address the pitfalls and challenges of running a household and hiring domestic staff. We focused first on identifying offline courses offered in Europe, but expanded our research as needed. There are three limitations to our research. First, we included one online course, as our findings indicate it is offered by a reputable organization and will provide the requested educational information. Second, as we were unable to construct a robust list of courses exclusively based in Europe, we expanded our research to Canada and the United States. Third, while only one program, First Class Area Academy, indicates that it provides instruction in hiring and firing staff members, research into the curriculum and topics offered by the remaining providers indicate that they offer instruction on the areas of interest to you. We selected eight reputable, highly-rated organizations that provide instruction in household management, which are discussed below, in no particular order.\nLONDON ACADEMY FOR HOUSEHOLD STAFF TRAINING\nEstablished in 2004 by Jane Urquhart, the London Academy for Household Staff provides education for training new and existing Butlers, Couples, Housekeepers, and other private households, luxury hotels, chalets, villas, yachts and corporate environments where 7* standards are a prerogative in the UK and internationally. There are at least 4 types of training in the London Academy for Household Staff Training. These include:\n3. Skills Sessions.\n4. Bespoke Training.\nTailored to suit the student and the private household or corporate environment in which they are working, Management of a Household or Other Establishment course focus on running an establishment smoothly with minimum fuss and maximum efficiency. Topics include:\nManagement of a Household or Other Establishment course runs on a one-to-one basis, the cost is £200 + VAT for one day (9.30 – 4.30pm) including a light lunch. A certificate will be issued for those who attend the course.\nPOLO AND TWEED\nBased in Mayfair, London, Polo & Tweed provide a professional service in domestic staff recruitment and domestic staff training. Their network of clients includes private homes, hotels, restaurants, chalets and yachts.\nPolo & Tweed will provide their client with the following:\nTheir management courses include:\n1. Principals & Managers — Estate Training for Your Staff.\n2. Professional House Managers — Training for Your Career.\n3. Group House Management Training Classes.\n4. Private House Management Training Classes.\nStarkey International has been in the Private Service Industry for 36 years and is a well-known expert in teaching the “Relationship of Service” in the Private Service Industry in the U.S., Canada, Caribbean, Europe and Asia. Starkey teaches its students how to meet the service standards expected of the principal. It has developed and patented the Starkey Service Management System. Starkey Private Staff Training is customized in content to serve the individualized need. It curriculum include:\nUK HOME COLLEGE\nUK Home College, a Distance Learning and online college that specializes in e-learning, provides courses centered around Management, Health and Social Care, Travel and Tourism, Finance and Marketing along with a list of possible specializations within these departments. It courses make its graduates highly employable.\nProfessional Certificate in Household Management in this program students will learn World Class Service. Candidates will learn how to adapt this services to different styles of homes and family structures, the dimensions of Household Management, and explore how development practices are becoming essential for this skill. The course costs £50.\nPROFESSIONAL DOMESTIC SERVICES & institute\nProfessional Domestic Services & Institute. It has been in the Private Service industry for 25 years (1993-2018). It trains specialist Household Managers/Butlers, Administrative Assistants/Personal Assistants, Estate Managers, and Estate Housekeepers. And offers these household management programs:\nLUXURY HOUSEHOLD MANAGEMENT\nLuxury Household Management, the 2017 Recipient of the NMSU (New Mexico BizSprint Arrowhead Accelerator) program to launch successful businesses, was founded by Nancy Harris, a Household Manager with 10 years experience in the Private Service Industry. She has managed numerous estates with staff. Her previous clients include the families of the CEO of Coca-Cola and the family of Tyson Foods both of whom have been listed on Forbes Magazine's 400 wealthiest executives. Nancy Harris is the Head instructor of Luxury Household Management.\nFIRST CLASS AREA ACADEMY\nFirst Class Care partnered with The Charles MacPherson Academy to provide Chicago with a one-day seminar in House Management training course. It's a one-day seminar packed with information that will assist new and existing managers in the Household management industry.\nThe courses it offers include:\n1. Duties: A Jr. House Manager, House Manager and Estate Manager duties.\n2. Successful Communication: Successful communication between employee and employer, and employee to employer.\n3. Household staff: Acquiring and retaining capable household staff.\nPERFECT HOUSEHOLD STAFF\nPerfect Household Staff is an international Residence Management and Domestic Staffing Agency that provides a professional service that source for ideal employees to serve in households. It is based in London, and sources for residences in the UK and around the world.\nLondon Academy for Household Staff Training, Polo and Tweed, Starkey International (US based), UK Home College (online course), Professional Domestic Services & Institute, Luxury Household Management (US based), First Class Area Academy (US based), and Perfect Household Staff provide short-term executive courses in Household Management. First Class Area Academy provides a course that covers the pitfalls and challenges of building and running a household, and how to hire and manage a household team. UK Home College provides an online course on household management."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:77339f3a-801a-4b9c-b795-2528b4eba91a>"],"error":null}
{"question":"What means information in physics and why cannot be destroyed?","answer":"In physics, information is a property of how particles are arranged - for example, the same carbon atoms arranged differently can form either coal or diamond. According to quantum mechanics, information is indestructible - it can change form but never be lost. For instance, when paper burns, the information isn't lost - theoretically, if you could collect every atom in the ash and measure all the smoke and heat from the fire, you could reconstruct the original paper. The information is still there, just in a different form.","context":["Why Black Holes Could Delete The Universe – The Information Paradox\nBlack holes are the most powerful things in the universe, strong enough to rip whole stars into atom sized piecesWell, this is scary enough. They have an even more powerful and dark property: they might delete the universe itself. Black holes in a nutshellA black hole appears when an extraordinary amount of matter is concentrated in a tiny space. At their center, gravity is almost infinitely strong and whatever gets tooclose is ripped into its elementary particles. Not even light can escape black holes, and so we perceive them as spheres of blackness. If you were to fall into a black hole, nothing bad would happen until well after you crossed its outer border: the event horizon. You can imagine this as swimming in a river that ends in an enormous waterfall. As you float along, imperceptibly, the stream gets faster and faster, even if you can’t see the waterfall yet. You could swim to safety, until without even noticing it, you cross the point of no return. No matter how fast you try to swim now, the stream will pull you towards certain death. Nothing can escape a black hole waterfall once it gets too close. This border completely separates black holes from the rest of the universe: we can’t access them unless we’re willing to never return. So there’s no way of telling what’s really going on inside black holes,but we have a few ideas about what’s going on right at their very edges. Black holes radiate their mass away, like a hot pot on a stove losing its water as steam. This is called Hawking radiation. Black holes constantly lose an extremely tiny amount of their mass,a process that’s unbelievably slow. It will take a black hole with a mass of our sun 10,000 billion billion billion billion billion billion years to lose 0. 0000001% of its mass. This is happening constantly and unstoppably, and as it goes on it speeds up more and more. In the far far future when the last star in the universe has been dead for trillions of years,black holes will become tinier and tinier until they evaporate and disappear, leaving behind just a bit of radiation. But this is a problem, because in the process of disappearing black holes might delete something fundamental: information. 2 – What is information?Information is nothing tangible. It’s typically understood as a property of the arrangement of particles. What does this mean?Imagine a bunch of carbon atoms. Arrange them in a certain way and you get coal. Arrange them in a different way, and you get a diamond. The atoms are the same, what changes is the information. If we make this more complex and add in a few more atoms,we get a banana. Change the arrangement of the atoms, and we get a squirrel. The basic building blocks of everything in the universe are the same,and don’t care if they’re part of a bird or a rock or a cup of coffee. Without information everything in the universe would be the same. According to the theory of quantum mechanics information is indestructible. It might change shape,but it can never be lost: for example if you burn a piece of paper, you get ash. That ash will never become paper again. But, if you were able to carefully collect every single carbon atom in the ash, and measured the exact properties of the smoke and heatradiating from the fire, you could, in theory reconstruct the paper. The information of the paper is still in the universe. It’s not lost,it’s just hard to read. If you could somehow measure every single atom and particle and wave of radiation in the universe,you could see and track every bit of information there is. Hypothetically, you could see the entire history of the universe right back to the Big Bang. And here black holes trip us up. Information tells us how things are different from each other and what used to be what. Black holes do the opposite: they take different things and make them the same. They destroy information. This creates the information paradox, and this is a serious problem. The information paradoxIt’s fundamental for all our laws of physics that information can never be lost. Existing, not existing. Without information, everything is relative. When it comes to our understanding of reality, we need absolutes. How could we solve this paradox?There are a few possibilities. 1) Information is lost. Irretrievably and forever. This means we have to nix all our laws of physics, throwing out a lot of stuffthat’s worked very well so far and to start from scratchWhat those new laws of physics would look like, or what that means for us, nobody knows. This is a little frightening, but also kind of exciting. 2) Information is hidden. Maybe a little part of the black hole splits off and forms a baby universe. The information would be transferred into this new weird place, where we could never observe or interact with it,but technically it would not be lost. It’s like having a broken hard drive with all your family photos, that you could never access. Sure, it’s nice that they’ve not been deleted, but also not very helpful. Or maybe black holes don’t disappear completely after the end of their life cycles,but a little piece is left, an information diamond. like a clown car filled with an infinite amount of information clowns. But there’s a third option:Information is safe after all, not lost or hidden. Perhaps we’ve just been looking at this whole thing the wrong way. We know that black holes trap information and might delete it later, but we never thought about what they do with it in the meantime. Where do black holes store their information?Cosmic housekeepingLet’s create a black hole with dirty laundry. First, we fill up a room with laundry baskets: the more laundry you want to store, the more baskets you put in the room. But at some point every single basket is full, and the room is completely stacked, not a single extra sock fits in. The room is at maximum capacity. But if we still squeeze the sock in with a lot of energy and violence, the room collapses in on itself and forms a black hole. But the capacity of the room itself has not changed, fitting in more stuff or information is still impossible. So what happens if we throw more laundry into it?The room itself gets a little bit bigger to make space for the new information. It turns out a black hole grows its surface by a tiny pixel for each bit of information we throw into it. In a nutshell, more information means more surface area. The information gets painted on the surface, similar to what happens when we throw a stone into a pond. After the stone sinks to the bottom we can’t see it anymore,but we can tell that something went in from the ripples on the surface of the pond. Even the smallest black hole can store more information on its surface than all the data ever produced in human history. They do this by storing information in a type of pixel that is unbelievably tiny. Black holes are the ultimate hard drive. This is a bit like taking a paper back, and turning it into an e-book, two things that look completely different. But their content is the same — it’s just encoded and memorized in another way. Black holes swallowing stars and planets is a bit like transferring a whole library onto an e-reader. This solution is called the holographic principle, but if it’s correct then everything we thought we knew about the universe is wrong. The universe is a hologramIf information is actually stored on the boundary of a black hole, the Hawking radiation has a chance of learning about the informationencoded there, and can carry it away. So, information is not lost when black holes fade away,and we do not need to redo physics: the information paradox is resolved. But we still have to change our understanding of reality in a fundamental way. If everything that falls into the black hole is stored on its event horizon, that basically means that three-dimensional stuff is encoded on a flat surface. We have a name for this: a hologram. A hologram is like a 3D photo, a flat piece of plastic that encodes a three-dimensional image. A black hole is like a hologram, because everything inside it is encoded on its event horizon. A person inside a black hole will experience their usual three-dimensional life. but for us on the outside they are flattened images on the surface of the black hole. The consequence of this is counterintuitive, but stay with us for a moment. Black holes are very extreme objects, but they’re still bound to the same rules as everything else. So if this crazy duality between 2D and 3D works for black holes, then it might work for the whole universe, and you in it. Since a person inside a black hole would not realize that they’re encoded on a flat surface,we might share the same fate: you really might be stretched over a flat screen at the end of the universe. The science behind this is complicated and really weird, with toy universes to play with, string theory and a lot of maths. We’ll talk about this more in another video. Regardless of what the true nature of the universe really is, we just know that it’s strange and complicated,and we have to do a lot more physics to understand it. But black holes might be key to understanding the nature of reality itself. This video was supported by the Swiss National Science Foundation, and realized with the scientific advice of Alessandro Sfondrini."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:ae9de856-74f6-4930-b97d-93d40edccebf>"],"error":null}
{"question":"Hey there! I'm looking to replace my old CVBS camera but want to future-proof my system. What options do I have that support both CVBS and HD formats? 💭","answer":"Most skoopia cameras support both CVBS (PAL/NTSC selectable) and Full-HD video transmission. You can choose from industrial or medical AHD cameras, TVI cameras, or SDI OEM cameras. These cameras are based on Full-HD sensors, allowing digital zoom without compromising resolution in CVBS mode up to 2.x times, making them suitable for future upgrades while maintaining compatibility with existing CVBS systems.","context":["skoopia develops, engineers, produces and markets industrial and medical cameras. Each industrial camera has unique features, but it all starts with the video format the camera outputs. skoopia focuses on six types of OEM cameras:\nAHD cameras (industrial cameras)\nSDI cameras (industrial and medical cameras)\nIP cameras (industrial and medical cameras)\nBlock cameras (industrial and medical cameras)\nCVBS cameras (industrial and medical cameras)\nGlobal shutter cameras (industrial cameras)\nUse the underneath selection table to directly access the OEM camera overview with the required video output or find out more about the different video outputs by clicking the Why XX industrial cameras? link.\nWhy Full HD CVBS cameras ?\nWhy Global-shutter cameras?\nIndustrial AHD camera\nAHD camera stands for Analog High Definition camera. At this very moment, AHD is merely scratching the surface in industrial applications, while first medical applications are still to be implemented.\nAHD as video signal has several huge advantages, and is considered to be the 'de facto' replacement of CVBS video signals (PAL camera / NTSC camera) if higher resolution is required.\nKey advantages of AHD:\nLow latency (alike CVBS)\nLow bandwidth (approximately 30 times less than uncompressed Full HD)\nLong distance Full HD transmission without amplification (> 500 meter)\nHigh image quality (contemporary status (2020) up to 4K / 8MP AHD video solutions exist)\nFlexible cabling (twisted pair, CAT cabling, coax cabling)\nSupport of Up the Coax - A smart cabling solution allowing power as well as camera control signals to use the same wires as the video transmission.\nNext to AHD, also TVI and CVI exist. Where CVI has lower quality and starts to vanish, TVI is an equally good alternative to AHD. The key reason skoopia markets primarily AHD has to do with the origine of the standard.\nTVI is a video standard set by HikVision. AHD is a video standard set by NextChip. All skoopia cameras supporting AHD also support TVI.\nskoopia has generated several unique cameras on the base of special features developed in collaboration with NextChip, factual providing unique firmware to support e.g. VISCA protocol for AHD cameras next to the Pelco-D protocol which is found in AHD and TVI.\nConsider AHD industrial cameras whenever you want to transfer to higher resolution, while keeping the benefits of CVBS camera implementations. The vast majority of skoopia AHD cameras support CVBS next to AHD and TVI, making the transition even smoother.\nIndustrial SDI camera\nAt this very moment, SDI is vastly adopted in the medical and broadcast industry, and adoption in other industry is growing fast. However, it is still at its infancy in adoption compared to the medical and broadcast industry. Because of the near-identical technical features, this camera category is described in the Medical camera section.\nIndustrial IP camera\nIndustrial IP cameras as well as Medical IP cameras are found in numerous applications. Particularly if latency is less of an issue, IP cameras often provide the most pragmatic approach to implement a vision solution.\nIf latency is less important / acceptable, IP as video signal has several advantages over other digital transmission techniques.\nKey advantages of IP:\nEach camera has a unique address (IP address), making cabling (network) significantly easier if multiple cameras are involved (other video solutions need a dedicated peer to peer connection)\nHigh compression rates via Codec (H.265 /H.264 / MJPEG)\nMultiple streams possible\nMultiple viewing and storage locations of the video\nMultiple streams (resolution, frame rate, etc)\nFunctions as stand alone unit, no need for (local) receiving end. Local storage (often on SD card) possible.\nPoE support, allowing camera operation merely using standard CAT cable\nskoopia industrial IP cameras and medical IP cameras are based on the best in class components (Sony Starvis sensors) and Hi Silicon DSPs (Digital Signal Processors).\nThe standard range of skoopia IP cameras the 1/2.8\" sensor size of Sony Starvis Exmor R sensors.\nThe super senstive range of industrial IP cameras and particularly medical IP cameras are using Sony Starvis Exmor R sensors with 1/1.8\" sensors.\nskoopia industrial IP cameras and medical IP cameras are ONVIF-S compliant, making discovery and integration easy in all relevant camera management platforms (e.g. Milestone).\nskoopia industrial IP cameras are all 42mm by 42mm single board, with resolutions ranging from 2MP up to 12 MP. Moreover, skoopia provides an API (Application Programming Interface) which is identical for all Industrial IP cameras and Medical IP cameras, making your engineering investment adopting a skoopia IP OEM camera future proof.\nIndustrial Block camera\nBlock cameras or zoom blocks are a special type of industrial camera or medical camera. Although they could have all types of video output, making it an AHD camera, CVBS camera, Global Shutter camera, SDI camera or an IP camera, the differentiating aspect of a block camera or zoom block is the fact that it comes with a lens system that allows for automatic focus and optical zoom.\nThe industrial block camera or medical block camera market has long been dominated by larger corporations, providing Sony block cameras, Hitachi block cameras or Samsung block cameras to the market. However, these companies have completely stopped production or only focused on high resolution section, neglecting the need for CVBS, HD or Full HD optical zoom cameras.\nDigital zoom vs. Optical zoom\nZooming in offers the advantage to the human eye that objects at a distance can be represented at a larger visible size. Zoom can be done digitally. However, digital zoom is merely a partial representation of the full information set. An image displayed and stored having digital zoom implies information loss of the image or video captured outside of the zoomed-in area.\nOptical zoom -in contrast- makes use of a system of lenses allowing you to zoom in without losing any resolution. The optical lenses allow you to enlarge an object at a specific distance without resolution loss.\nskoopia 23Z36L-LL block camera\nIndustrial block camera versus industrial OEM board camera\nIf only limited zoom is required, board cameras can provide a viable alternative to zoom blocks. Particularly if the resolution required is significantly lower than the sensor resolution, digital zoom can provide benefits. This effect is most often used in environments where an analog signal (CVBS, PAL, NTSC) is used, and the resolution of the sensor allows for significantly more information.\nAdvantages of industrial board camera over industrial block camera:\nmore robust, as there are no moving parts (block camera mechanically changes the position of the lens system inside)\nsignificantly smaller size, as only a single board with a lens is required.\nAdvantage of industrial block camera over industrial board camera:\nresolution of the block camera remains intact using optical zoom, while the resolution of the output image decreases for a the industrial board camera.\nindustrial block cameras provide auto-focus, industrial block cameras will require either an auto-focus lens (so moving part) or a distance change of the OEM board camera to the lens, in both cases requiring the introduction of moving parts*.\n* To avoid this, OEM board cameras requiring a large Depth of Field (i.e be in focus from near (e.g. 10 mm) till infinity are using lenses with a very small aperture (e.g. F8.0)). The negative impact of this is the introduction of more noise in the image and a significant lower amount of light for generating the video (smaller aperture allows less light to enter the camera sensor).\nSo, as soon as resolution loss is not acceptable, and/or zoom factor is significant, the only (affordable) zoom alternative is optical zoom. Underneath table provides you an indication of the required signal resolution, an example sensor resolution, and a maximum digital zoom that is possible without resolution loss.\nRequired signal Sensor resolution Max Digital zoom *\nNTSC (640x480) Full-HD (1920x1080) 2.2x | 3.0x (V | H)\nPAL (756x576) Full-HD (1920x1080) 1.9x | 2.5x (V | H)\nHD (1280x720) 4K (3820x2160) 4.0x | 4.0x (V | H)\nFull HD (1920x1080) 4K (3820x2160) 2.0x | 2.0x (V | H)\n* Difference in (V | H) vertical versus horizontal is caused by the fact that CVBS (NTSC/PAL)\nis a 4:3 format, while higher resolutions (HD and above) have an aspect ratio of 16;9.\nFor larger zoom requirements with a good resolution, it can be easily seen that sensor resolution goes beyond the contemporary capabilities**.\nRequired signal Required Zoom Min. Sensor resolution\nNTSC (640x480) 10x 30 MP\nFull HD (1920x1080) 10x 200 MP\nFull HD (1920x1080) 20x 400 MP\nFull HD (1920x1080) 36x 720 MP\n** at realistic price points compared to pricing of optical zoom cameras\nFinally, a combination of optical and digital zoom without resolution loss can be ideal for settings in which only a CVBS signal is required, but e.g. previous models of analog block cameras (Sony FCB block cameras are the most common) offering e.g. 18x optical zoom and analog output. Although a 20x optical zoom block camera could be selected, this block camera (now using a 1/3\" sensor instead of the common 1/4\" sensor is too large in size. Using a 10x optical zoom block (e.g. skoopia 20Z10S-NF) in combination with 2.5x digital zoom, without resolution loss could provide an adequate solution. The industrial block camera is small enough to fit the existing housing, and offers the relevant zoom wihtout resolution loss.\nSony Block Camera\nskoopia block cameras for industry and medical are replacing Sony FCB block cameras as well as Hitachi and Samsung zoom blocks.\nProductivity of the person working / relying on the video provided by the block camera is crucial in any medical or industrial application. Although the block cameras from skoopia support Pelco-D (the most common control protocol to setup a camera), in industrial block camera and medical block camera business, direct control buttons are required. Here, Sony introduced VISCA, a control language that allows operators to directly access the registers of the block camera and drive commands via a single button.\nAll skoopia cameras are pin-compatible and to a large extend (cannot be 100% as we are using more contemporary DSP (digital signal processing) chips sets) factual registry commands.\nskoopia also offers VISCA compatible OEM board cameras. The reason is exactly the same as for the block cameras. Provide direct access tot he registry of the OEM board camera. Although industrial AHD board cameras or medical SDI board cameras will require less operator steered commands (no optical zoom is an obvious command not present), the benefits of direct commands are providing operator productivity improvements are substantial.\nIndustrial CVBS camera\nIndustrial PAL camera\nIndustrial NTSC camera\nCVBS cameras are still widely in use, particularly in industrial and medical environments. Often, frame grabbers and/or specific overlays are embedded in an integrated systems, making the transition to higher resolution and/or other aspect ratio (4:3 CVBS to 16:9 Full-HD) difficult.\nAlthough all skoopia cameras have a Full-HD or higher resolution, the vast majority supports CVBS, as skoopia understands that a transition to higher resolution impacts not just camera, but also cabling, overlays, monitors and storage.\nIf you consider a replacement CVBS camera, but want to be future proof, please check our portfolio of CVBS cameras that -next to CVBS (always PAL/NTSC selectable)- will support your preferred Full -HD video transmission for the future (whether it is an industrial or medical AHD camera, TVI camera or you prefer an SDI OEM camera. And even industrial IP cameras, with ever lowering latency or for areas where latency is no issue could be a solution direction.\nNext to board cameras, skoopia also offers a broad range of block cameras with CVBS (NTSC and PAL output).\nskoopia offers CVBS cameras with true CVBS output, ie. PAL (756 x 576 i50) and NTSC (640 x 480 i59,94). Although NTSC is often indicated as 480i60, it is important that you udnerstand that we support the true NTSC standard on all relevant block cameras, SDI cameras or AHD cameras, even though such modules work with digital formats, requiring an exact number of frames per second.\nAlso note that all skoopia cameras are based on a Full-HD sensor, and offer the ability to make use of digital zoom without compromising resolution in CVBS mode up to 2.x times (click here to sensor resolution versus digital zoom comparison table)\nIndustrial Global Shutter camera\nAll skoopia cameras are CMOS (Complementary metal–oxide–semiconductor) based. CCD (Charged Coupled Devices) production for sensors has been abandoned, as it provides inferior performance compared to CMOS (production wise, reliability wise etc).\nTherefore, CMOS as technology is found in near 100% of all sensors used for cameras, also for industrial cameras and medical cameras.\nCCD had one advantage over CMOS, as it intrinsically provided Global Shutter. To overcome this phenomenon, CMOS global shtuter sensors have been developed.\nFor your application, it is crucial that you check if you require global shutter, which is likely if you are looking at fast moving or fast rotating objects.\nWhy is this important: Global Shutter sensors, and therefore global shutter cameras are significantly higher priced if compared to Rolling shutter cameras. Rolling shutter is the default mechanism for CMOS sensors.\nGlobal Shutter camera explained\nIn Global Shutter mode, every pixel is exposed simultaneously at the same instant in time (see picture below). This is particularly beneficial when the image is changing from frame to frame. As such, Global Shutter cameras are primarily useful capturing fast moving objects.\nGlobal shutter contrasts with Rolling Shutter. Rolling Shutter is a method of image capturing not by taking a snapshot of the entire scene at single instant in time but rather by scanning across the scene rapidly, either vertically or horizontally (see picture below). In other words, not all parts of the image of the scene are recorded at exactly the same instant.\nThis produces predictable distortions, which are primarily seen imaging fast-moving objects or rapid flashes of light. This phenomenon is not seen with \"global shutter\" cameras in which the entire frame is captured at the same instance.\nRolling Shutter camera\nCMOS global shutter versus CCD global shutter\nMost CCD cameras are global shutter. The CCD however has an inherent disadvantage when it comes to frame rate and/or resolution.\nWhen a CCD camera's exposure is complete, the signal from each pixel is serially transferred to a single Analog-to-Digital Converter (A/D). The CCD’s frame rate/resolution is limited by the rate that individual pixels can be transferred and then digitized. The more pixels to transfer in a sensor, the slower the camera. Therefore, global shutter CCD cameras typically have a low resolution.\nWith lowest sensor resolution in Mass Production already being 1080p (Full-HD), CCD sensors, and as a consequence CCD cameras are only available for very special areas of applicaiton. CMOS has won the battle and conquered the world, even for Global Shutter applications."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:ad034d96-cb3b-45df-8d74-2ac75210883d>"],"error":null}
{"question":"What are the key financial measures needed for sustainable development in both Russian civil society organizations and Latin American institutions?","answer":"In Russia, the Third sector organizations primarily need to address insufficient funding, with 33% of responses identifying this as the main problem, along with lack of experience in project implementation (18%) and insufficient interaction with local government (12%). Additionally, 70% of organizations operate with small staff (up to 5 people) and rely heavily on volunteers. In Latin America, financial needs are specifically quantified at USD 77 billion to meet 2030 climate change goals, with public investment only able to cover about a quarter. The region has developed some financial instruments, with green bonds accounting for 2% of the global market, focusing on renewable energy, transport and land use. Both regions require systematic work and increased public-private cooperation, with Latin America particularly needing to follow credible sustainable trajectories guided by net zero pledges with science-based targets.","context":["The results of civil society sociological studies in the region and the non-profit organizations activities are presented in the article. Surveys were conducted in target groups: population, non-profit organizations representatives and local governments. It has been established that the non-profit sector, which acts as the civil society institutional basis, gradually occupies its important place in the economy structure and in social services system. At the same time, according to experts, the level of the Third sector development in Russia compared to other countries is much lower. At the same time, the assessments positive modality identifies the increasing non-profit organizations influence on the population and government consolidation. This is a favorable prerequisite for social stability establishment and the social conflicts elimination. At the same time, the social system is considered as chaotic in the sense that it acts as both a force aimed at preserving and maintaining the system integrity, and the forces leading to the central ties weakening and the disorder instrumentalization. Therefore, it justifies the new agenda facing the elite, the government and the active social part. The main conclusion is that it requires systematic work, taking into account the complexity of modern society and global challenges. Its general outline is presented in the article in the form of paradoxes. To eliminate the interests’ gap, an ongoing dialogue is needed, based on solvable local problems, the active formation of communications between all public life subjects based on both traditional and new forms of civil society organization and development.\nKeywords: Civil societynon-profit organizationscooperationsocial policyInternet\nIn the new reality the state administration system have to be turned towards the management of goals and projects, both economic and social, in the cooperation context. At the same time, the social system is considered as chaotic in the sense that it acts as both a force aimed at preserving and maintaining the system integrity, and the forces leading to the central ties weakening and the disorder instrumentalization. Civil society (with new communications technologies) contributes to the development of these trends. The question is: what is the trend and to what extent?\nThe government intends to simplify and standardize the social order, driving it into the format of such categories and institutions that make this order accessible and easy to control and manage. In this case, sociology uses the term “environmental misconception” (Maitland & Bauer, 2001; Brewer & Venaik, 2014), which means the objects group aggregated data incorrect use by transferring them to individual objects, or make them match the norms. Environmental misconceptions are the result of a logical error in the a priori or statistical data interpretation, where information about the phenomena (or subjects) nature is derived from a generalization of these phenomena (subjects) class to which they belong. It also includes the imposition of norms and other aspects that violate the social ecology. The challenge is to avoid these misconceptions and form the right agenda.\nThe social structure is extremely difficult. This accepted truth, by its obviousness virtue, does not seem to require proof and, as it were, can be used exclusively rhetorically. At the same time, today the whole world (including the Russian Federation and its subjects) has faced challenges that impose new requirements on the formation of a social order, a society structure capable of modernizing the country. Therefore, the subject of the study was a complex of civil society important aspects, as well as a new agenda facing the elite, the government and an society’s active part, which involves systematically considering social structure issues, taking into account the modern society and global challenges complexities.\nPurpose of the Study\nThe purpose of the study was to obtain information necessary for analyzing the civil society development problems, including analyzing the activities of the Third sector – non-profit organizations (NPOs), assessing the implementing socially oriented projects effectiveness and the level of public satisfaction with the services provided. The Republic of Karelia region was chosen as the object of the study. Using this example, an attempt was made at an empirical level to measure the effectiveness of the Third sector and civil society in general.\nThe survey was conducted in 2015 in 12 municipalities in the target groups: population, NGOs representatives, and local government representatives. A quota sample, reproducing the population structure, was compiled in order to conduct a survey for each urban district and municipal district. The census data were used (GKS, 2010).\nIt has been established that the non-profit sector, which acts as the civil society institutional basis, is gradually growing and occupies an important place in the economy and employment structure, in the social services system. At the same time, according to experts, the Third sector development level in Russia compared with developed countries is much lower. According to the Report on the Development of Civil Society Institutions in Russia (Nasriddinov, 2014), the NPO sector share in Russian GDP is 0.9%, while the same indicator for countries with a developed Third sector reached the point of 6.5%. The share of people employed in the third sector in Russia is 1.1%, abroad – is 7.1%, the share of SO NPOs from the total number of NPOs is 13.5% in our country, 60–70% – abroad. A comparison of the NPOs formal indicators in Russia and in the Republic of Karelia is presented in Table\nAccording to established data, 70% of Third sector organizations have a small number of fulltime employees – up to 5 people. In the Republic, 40% of organizations have 1-3 people in their staff. The labor scarcity is partly compensated by volunteers. In Karelia, the number of volunteers involved in the NPOs statutory activities implementation in the aggregate, reaches 18 thousand people. According to the Third sector representatives interviewed, the main problems in the NPOs activities are the third sector potential underestimation, bureaucracy in power structures, pressure from regulatory authorities, the authorities’ insensitivity to public inquiries, disunity of NPOs. According to the population, the main problems in NPO activities are insufficient funding (33% of responses), lack of experience in project implementation (18%), insufficient interaction with local government representatives (12%), lack of qualified personnel (8%) and the necessary equipment (8%). According to NPOs, the problems are the following: lack of financial resources (20% of responses), lack of sponsors support (12%), problems with placement (12%), lack of knowledge (10%), especially legal knowledge (7%), lack of enthusiasm (7%), etc. At the same time, generally positive attitudes have been formed among the population, which can become catalysts for the civil society development. High degree of uncertainty and inconsistency in assessing the civil society state and prospects was revealed. At the same time, certain tendencies were identified, which stipulate the following scenarios:\nThe main conclusion is that the civil society formation requires systematic work, outlined below. Conclusions are formulated in the form of paradoxes in order to emphasize a problem situation that requires an immediate solution. The first problem is\nA positive response (in table\nPolitical systems and their legitimation are strongly dependent on citizens’ support and legitimation (Iston, 2000) and their satisfaction with legal norms, authorities, the political system’s “inputs” and “outputs” structure. In other words, the shared beliefs of those who govern and manage the processes and results of management are crucial to the stability and political systems preservation (Sabatier, 1991; Holbig, 2013). Public participation (or non-participation) is one of the main elements that has been studied in detail and, accordingly, in the social sciences, a question has long been raised about the correct determining factors for analyzing public participation and the democratic institutions effectiveness (Deth, 2011; Etzioni, 1969; Verba & Nie, 1972) for the social order design . On the other hand, the community demonstrates the ability to resist categories imposed top-down.\nThe article is completed in the framework of the state assignment 0218-2019-0088.\n- Abraham, F., Mitina, O., & Houston, D. (2015). Chaos theory and the Internet in the era of postmodernism. Retrieved from: http://www.kinnet.ru/cterra/357/38.html\n- Beetham, D., Carvalho, E., Landman, T., & Weir, S. (2008). Assessing the quality of democracy: A practical guide. Stockholm: International IDEA.\n- Brewer, P., & Venaik, S. (2014). The ecological fallacy in national culture research. Organization Studies, 35(7), 1063–1086.\n- Deth, V. J. W. (2011). Is Creative Participation Good for Democracy? Creative Participation: Responsibility-Taking in the Political World. Boulder: Paradigm Publisher.\n- Etzioni, A. (1969). The Active Society. A theory of Societal and political Processes. Brutish J. of Sociology, 13(2). DOI:\n- GKS (2010). Results of the all-Russian census of 2010. Retrieved from: http://www.gks.ru/free_doc/new_site/ perepis2010/croc/perepis_itogi1612.htm\n- Holbig, H. (2013). Ideology after the end of ideology. China and the quest for autocratic legitimating. Democratization, 20(1), 61–81.\n- Iston, D. (2000). Categories system analysis policy. In Political Science: A Reader (pp. 319–331). Moscow: Gardariki.\n- Kuzminov, I. I. (2013). Strategy 2020: New Growth Model – New Social Policy. Moscow: Delo Publishing House.\n- Maitland, C. F., & Bauer, J. M. (2001). National level culture and global diffusion: the case of the Internet. In Culture, technology, communication: towards an intercultural global village. New York: State University of New York Press.\n- Nasriddinov, T. G. (2014). Report on the state of civil society in the Russian Federation for 2014. Moscow: Public Chamber of the Russian Federation.\n- Sabatier, P. A. (1991). Toward better theories of the policy process. PS: Political Science & Politics, 24(2), 147–156.\n- Tolstoguzov, О. V., & Pitukhina, M. A. (2017). Sustainability of Social Relations: Internal and External Factors. In Local Politics in a Comparative Perspective. Baden-Baden: Nomos.\n- Tolstoguzov, О. V., & Pitukhina, M. A. (2018). Migration Challenges to Socio-Cultural Space: Political Communication of Russian Migrants Living in Finland. Practical Geography and XXI Century Challenges. Moscow: Institute of Geography, Russian Academy of Science.\n- Verba, S., Nie, N. (1972). Participation in America. Political Democracy and Social Equality. Social Forces, 24(4). DOI: 10.2307/2576853\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.\nAbout this article\n21 January 2020\nPrint ISBN (optional)\nSociolinguistics, linguistics, semantics, discourse analysis, science, technology, society\nCite this article as:\nTolstoguzov*, O., & Pitukhina, M. (2020). Civil Society: Problems And Paradoxes. In D. Karim-Sultanovich Bataev, S. Aidievich Gapurov, A. Dogievich Osmaev, V. Khumaidovich Akaev, L. Musaevna Idigova, M. Rukmanovich Ovhadov, A. Ruslanovich Salgiriev, & M. Muslamovna Betilmerzaeva (Eds.), Social and Cultural Transformations in the Context of Modern Globalism, vol 76. European Proceedings of Social and Behavioural Sciences (pp. 3145-3152). Future Academy. https://doi.org/10.15405/epsbs.2019.12.04.424","By José Luis Reséndiz\nLatin America Analyst\nThe sustainable economic recovery in Latin America is a challenge that leaders may regret not to take up seriously and on time. Unfortunately, the violence and inequality crises from the last decades have led to the wrong perception that the pandemic is just a transitory issue. The region is facing a storm in which many of its leaders do not feel responsible for their actions, and citizens are much more concern about unemployment or hunger. Under such circumstances, what is the role of sustainable finance in Latin America during the pandemic?\nIn its Meditations, Rousseau argues that adversity teaches us lessons that come to our minds quite late, just after they might be useful. There is a risk that one of those lessons for Latin America would be the use of sustainable finance to promote a sustainable economic recovery or a “build back better” strategy. Accordingly, sustainable finance should be understood as an approach that incorporates climate, green and social finance while also adding broader considerations concerning the longer-term economic sustainability.\nIn developed markets, the finance industry seems to have passed a turning point towards a net-zero economy. The growing interest to include environmental, social and governance (ESG) considerations on investments demonstrate how sustainability can generate financial value. Ultimately, it is the only way humanity (and capitalism) can survive. The UN’s Intergovernmental Panel on Climate Change has been precise: we need to cut annual global emissions by half in the next years and hit net zero carbon by the middle of the century.\nIt is expected that Latin America will experience a 9% fall in GDP, which will not only trigger an economic downturn but also a rise in poverty for approximately four million inhabitants, most of them being indigenous and afro-descendants. To address these upcoming issues, governments must stop tweaking the edges of their traditional approaches. Instead, they should embrace a new way to mobilise public and private investments in which the environmental and social matters should be the centre.\nThe banking industry’s sustainable finance products are concentrated in five countries –Brazil, Colombia, Argentina, Mexico and Ecuador – due to regulatory and government support.  Brazil has become the leader in providing debt instruments linked with ESG considerations. Mexico has recently shown a strong signal to promote a green recovery by issuing its first sovereign sustainable bond in the middle of the pandemic. However, the issuance raised concerns about the lack of enforcement mechanisms, which may lead to greenwashing practices. Even though there is a growing interest on green bonds in Latin America, they accounted for just 2% of the global green bond market in 2019, mainly focused on renewable energy, transport and land use.\nApproximately USD 77 billion are needed to meet Latin America’s 2030 climate change goals. Public investment can cover only about a quarter, while private investors should be incentivised to fill the rest. Even though there is strong evidence that the right track is to follow a sustainable finance approach in the upcoming years, it is still unclear how Latin America will move forward to support the recent signals of progress. To remove this uncertainty, governments and businesses in the region should follow a credible sustainable trajectory guided by net zero pledges with science-based targets.\n International Capital Market Association. 2018. Sustainable Finance, High-level Definitions. bit.ly/2K2k323.\n Economic Commission for Latin America and the Caribbean. 2020. Estudio Económico\nde América Latina y el Caribe Principales condicionantes de las políticas fiscal y monetaria en la era pospandemia de COVID-19. bit.ly/2Wk65ep.\n Juan Camilo Mejia-Escobar, Juan David González-Ruiz, and Eduardo Duque-Grisales. 2020. “Sustainable Financial Products in the Latin America Banking Industry: Current Status and Insights”. Sustainability 12(14). bit.ly/38bN8zZ.\n Justin Villamil. 2020. Mexico’s ESG Bond Has Skeptics Questioning Do-Good Bona Fides. Bloomberg. bloom.bg/2WpFblj.\n Climate Bonds Initiative. 2019. Latin America & Caribbean Green finance state of the market 2019. bit.ly/2LJ6MvI.\n Luis Alberto Moreno. 2020. How green investment will help Latin America and the world fight climate change. bit.ly/3ahVzfR.\n Carlos Alberto Vargas, Andrés Guerrero Alvarado, Angélica Rotondaro and Vicente José Servigón Caballero. 2020. In Latin America, Impact Finance Demonstrates Remarkable Progress and Untapped Potential. Stanford Social Innovation Review. bit.ly/3r1nHd0."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:13abc6ca-9acd-4d0a-a2fc-12425d91b7ce>","<urn:uuid:411002e2-bd81-4641-9495-07c868ca09ed>"],"error":null}
{"question":"Can you tell me what IHMC and Apex Golf Tournament have in common regarding their celebrations of community?","answer":"Both IHMC and the Apex Golf Tournament serve as important community gatherings and celebrations in Canada. IHMC (Islamic History Month Canada) aims to celebrate and share Muslim heritage and contributions with fellow Canadians, while the Apex Golf Tournament functions as a homecoming celebration for Truro's Black community and is one of the longest-running annual gatherings of Black people in Atlantic Canada. Both events focus on bringing communities together to celebrate their heritage and contributions to Canadian society.","context":["A Rich Heritage\nA golden past with modern day examples that touch our lives daily, an acknowledgement to those who have contributed to the betterment of humanity throughout the ages, and an overall celebration of Islamic history summarizes Islamic History Month Canada [IHMC].\nCelebrations take place across Canada in October. Cities such as Kingston and Burnaby have annually proclaimed IHMC since the inauguration. The recognition of Islamic History Month is an exciting part of Canadian-Muslim history although the honour is not limited to IHMC only. For example, Burnaby’s City Council has proclaimed Black History Month, European Heritage Month, Diwali Month, and Barbados Heritage among other proclamations that have been issued to celebrate the diverse fabric of Canadians.\nBridge-building with understanding is one of the goals of IHMC. More importantly at a time when Muslim are being perceived as the ‘other’, it’s a time dedicated to showcasing Muslims’ contributions to humanity many may not be aware of. Previous IHMC themes have been Islamic Art and Architecture; Islamic Banking and Finance; Irrigation and Water systems; Medicine and Pharmacology; and Islam and the Environment. Islamic History Month Canada provides a time to focus on celebrating and educating the Muslim and non-Muslim community on the rich history, heritage and culture of Canadian Muslims and Muslims around the world.\nLearn, Share, & Celebrate\nIslamic History Month Canada (IHMC) in October aims to celebrate, inform, educate, and share with fellow Canadians the rich Muslim heritage and contributions to society: Contributions in sciences, humanities, medicine, astronomy, and other disciplines that have greatly benefited human progress. IHMC believes that through education and sharing positive stories, all Canadians can grow and connect in the best way possible.\nIn 2007, IHMC was graciously proclaimed by the honourable and departed, Mauril Belanger, MP, for Ottawa-Vanier. IHMC has been proclaimed in the cities of Kingston, Burnaby, and most recently in the province of Manitoba.\nIn 2013, IHMC became an independent body with a new Chair. The Board members are currently from Saskatoon, Regina, London, Ottawa, and Winnipeg.\nEvery individual effort is essential! IHMC can be successful only through the efforts of communities across Canada — Let us learn, share, and celebrate Islamic History Month in October this year!\nShare your event with IHMC today!\nStatements from Notable Muslim Canadians for Islamic History Month\nFaisal Kutty – Lawyer, Law Professor, Writer and Public Speaker\nIt is no secret that Canada as we know it today is only possible because of the contributions of all the great people that make up this wonderful nation. First and foremost we must recognize and honour the sacrifices and generosity of our First Nations people and then the contributions of the rest of our communities. Islamic History Month presents an opportunity to recognize and reinforce the important role of Muslim Canadians in the past, present and in shaping the future.\nThank you Islamic History Month Canada (IHMC) for your ongoing efforts to keep Islamic History Month continuing for almost 10 years. We must also be grateful to the Federal government for declaring the first Islamic History Month on October 25, 2007. Since then IMHC has been instrumental in sharing the rich Muslim heritage and collaborating with diverse communities across Canada.\nI urge all Canadians to take advantage of Canadian Islamic History Month to learn more about Muslims and to continue sharing information about our respective contributions. The more we know about each other the more we realize how much we have in common.\nAs we mark the 10th year of Islamic History Month in Canada, let me congratulate all those involved in giving Canadians an opportunity to learn about the positive contributions of Muslims to the fabric of our beloved country.\nIn a time where Muslims are experiencing such extreme levels of mistreatment, hatred and islamophobia around the world, I applaud Islamic History Month Canada for taking this bold and progressive step to remind Canadians and the world that Muslims are not what you see on your TV screens, but rather are our mothers and daughters, our sisters and brothers, our aunts and uncles, mothers, our nieces, nephews and our neighbours.\nEach and every individual regardless of ethnic background or creed deserves to live with honour, respect and dignity. This awareness month enables us to take one step closer to our vision of a completely inclusive and just society for all.\nCommunity Activist, Author and Director, Editor in Chief of Muslimsactually.com & Director of f Fund Development & External Affairs, International Development & Relief Foundation (IDRF)\nHind A. Al-Abadleh, Ph.D.\nCongratulations to IHMC on reaching a milestone, its 10th anniversary, of shining light on Muslims contribution to human civilization. Since its inception by visionary pioneers in the Canadian Muslim community, IHMC has been instrumental in highlighting how Islam as a religion continues to be an inspiration to many generations of Muslims of diverse cultures and ethnic backgrounds to innovate in the sciences, arts, architecture, medicine, and finance to name a few. As the Canadian Muslim community continues to grow and thrive, their positive contributions to the wider society will better Canada’s future in the face of the many environmental and political challenges facing the world today. My heartfelt congratulations and best wishes for an eventful October 2016.\nHind A. Al-Abadleh, Ph.D.\nDepartment of Chemistry and Biochemistry\nWilfrid Laurier University\nEvery year in October, Canadians have an opportunity to discover and celebrate the vibrant history of Islamic contributions to civilization. Given the current global climate defined by Islamphobia, war, and terrorism, the Islamic History Month Canada allows Canadians to see and appreciate the positive and beautiful side of Islamic history and heritage. It also helps Canadian Muslims rediscover and showcase their tradition of diversity, peaceful existence, community building, philanthropy, innovation, and service to society. The Islamic History Month Canada is also a time to remember the inspirational stories of Canadian Muslim pioneers, who laid the foundations of first cultural and religious institutions, as they helped build an inclusive and multicultural Canada.\nI truly believe in the potential of Islamic History Month Canada in bringing Canadian Muslims and their fellow Canadians together in celebrating the beauty of Islamic culture and heritage through awareness and engagement.\nTaha Ghayyur – Director of Communications, Sound Vision; Board Member, DawaNet & MuslimFest","1. Emancipation Day\nThe first-ever federally recognized Emancipation Day in Canada took place on Sunday, August 1.\nBoth the federal and provincial governments voted unanimously this past spring to commemorate the anniversary of August 1, 1834 when the British Emancipation Act went into effect. This granted freedom to all enslaved people in the British colonies, including what is now Canada.\nThere was a province-wide virtual walk/run, as well as events in Halifax, Guysborough County, Yarmouth, and Truro.\nRosemary Sadlier, former president of the Ontario Black History Society, spoke to The Examiner about her spearheading efforts to get both Black History Month and Emancipation Day federal recognition. Her efforts date back to the 1990s.\nIn addition to that, Sadlier talked about how the Haitian Revolution was the catalyst for Emancipation Day, how Emancipation Day was the catalyst for the Underground Railroad, and the negative impacts the Underground Railroad had on Black people living in Canada at the time.\nAdditional Emancipation Day events are scheduled to take place throughout the province throughout the month of August. On Sunday, August 8 there was a church service and an event at the Afrocentric Park in New Glasgow to commemorate Emancipation Day.\nEmancipation Day in Truro:\n2. Apex Golf Tournament returns\nAfter a hiatus last year because of COVID-19, the Apex Invitational Golf Tournament returns to Truro this weekend.\nThe tournament weekend, which doubles as a homecoming for Truro’s Black community, is one of the longest-running annual gatherings of Black people in all of Atlantic Canada.\nAccording to its website, the Apex Invitational Golf Association was founded in 1974 by a young Black resident of Truro named Darrell Maxwell:\nAt the time, Darrell was not only a member of the Truro Golf Club but was considered one of the best young golfers in Nova Scotia.\nDarrell’s idea was to promote Black golf in Truro, Nova Scotia by putting on a competitive golf tournament with some of the Black golfers residing there, as well as former Black golfers and caddies who had moved away to the larger cities to find work. This gathering would present an opportunity for these former Truro residents to return home after some time apart and get together to reminisce about old times. Darrell pursued this idea with his brother, Stan “Chook” Maxwell, who was the caddy master at that time. With the help of the Truro Golf Club professional and some prominent club members, the plan went ahead to organize this tournament which was initially called the Black Golf Tournament.\nDespite last year’s cancellation, the Apex’s all-Black committee was said to have still awarded its annual scholarships to Black students in the community.\nFor more info and past news items about the Apex Invitational Golf Association, click here.\n3. Gibson Woods teacher awarded\nSaltwire Media reported this week than an African Nova Scotia literacy/numeracy teacher was awarded with the Loran Teachers Building Leaders Award. Kathy-Ann Johnson, who is from the historical Black community of Gibson Woods, was nominated by a former student, Lily Bateman, who she taught in middle school.\nAccording its website, “Each year, graduating Loran Scholars are invited to show gratitude to the teachers who impacted their lives by nominating them for a Loran Teachers Building Leaders Award.”\nJohnson told Saltwire:\nHaving taught Lily in Grade 6, this being 10 years later, is extraordinary and impactful. For me, it confirms that my heart and the approach to my work is in the right place. I was emotional and felt very blessed and humbled to receive the nomination.\nI recall telling students that if I’ve taught them nothing else, I hope to at least have taught them to be positive and to grow and progress from where they are. She was always a happy, respectful, and talented student who was kind to everyone and always gave her best. It was her who inspired me to make my own sushi, as I recall.\nIn addition to Johnson, Bateman also nominated her high school teacher, Carrie Rand, who also received the award.\n4. Africville Exhibit in Halifax\nA free exhibit honouring Africville opened at the Halifax Convention Centre this week and will be open to the public until December.\nJuanita Peters, the executive director of the Africville Museum, spoke to CBC’s Information Morning on Wednesday:\nI hope people walk out feeling something significant and having some impression of what it might have been like to be in Africville.\nI’m hoping that [the exhibit] helps people understand what was wrong with the original story and what was created — and I say created because it was an environment that was created by the city — that created such hardships for people.\nFrom the moment that Angel started carving and crafting to today, this exhibit has just given us so much joy.\nSo we hope that when people come through, they get not just an education, but understand that these were the things that were important to people of Africville.\nWe heard that people felt a little overwhelmed and sometimes happiness can turn to sadness.\nIt’s that loss, and sometimes you may hear something that reminds you of something and it’s bittersweet.\n5. Future Stars of Basketball\nGlobal Halifax’s Alexa MacLean talked to Geoff Clyke and John Hum about a street basketball league for kids — Future Stars Basketball — they’ve organized. Clyke told Global:\nWe find that street basketball makes it tougher, it makes them grind a little bit harder, their families can watch after being locked up for the COVID for so long.\nAs the parents, we were feeling the stress, but as the kids they were starting to feel the stress — in the house, not being able to do the things they wanted to do. That’s why John and I we decided to put the program together.\nOur vision for the future is to have all university collages coaches lined up all around the fences, all of them here looking at the future of our players that are here in Nova Scotia.\nMacLean said that Clyke worked with the city ahead of restrictions being lifted in order to secure court time ahead of time. In the video of the report, the league is seen operating outdoors at Oxford Junior High School in Halifax.\n6. Jermaine Colley hosts Aries Spears (In Case You Missed It)\nLast week, East Preston’s Jermaine Colley, who’s a Black podcaster and standup comedian, released episode 7 of his podcast In Case You Missed It. This episode featured special guest celebrity comedian and TV and movie star, Aries Spears. Spears is known for his role in the movie Jerry McGuire and for his starring role on the sketch comedy series Mad TV.\n“My big bro Aries Spears dropped by to bless my new podcast,” Colley said when asked about the episode. “We chopped it up about a few current events in the culture and cracked a lot of jokes along the way.”\nLinks to the eight-episode-to-date podcast series (not necessarily suitable for all listeners) are available here.\nIn addition to In Case You Missed It, the Black In The Maritimes podcast released their latest episode this week titled ‘Round table about the N word‘."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:ba0e994b-0392-4491-a573-498886e220ad>","<urn:uuid:364c7110-2a10-4d68-80ce-1b1be343c710>"],"error":null}
{"question":"What are the key differences between CGI architectural visualization and traditional building photography in terms of flexibility and technical limitations? 🤔","answer":"CGI architectural visualization and traditional building photography have several key differences. CGI allows creation of images before construction begins and offers unlimited control over elements like materials, lighting, weather and camera angles. It can show buildings from any perspective and create hypothetical scenes not possible to photograph. In contrast, traditional building photography is limited by physical constraints - you need the actual building, proper lighting conditions, and clear views without obstructions like parked cars. Photography also requires careful camera handling, proper exposure settings, and consideration of elements like sun position to avoid flare. While CGI can create any desired scene, photography depends on finding suitable real-world conditions and locations to capture good images.","context":["Whether for property marketing, product and prototype visualisation or character animation, computer generated imagery (CGI) offers the potential to create stunning, imaginative visuals from almost limitless perspectives and with loads of possibilities.\nArchitectural illustration & artist’s impressions\nArchitects, property developers, house builders and estate agents use architectural illustrations to show the potential of a development before it is built.\nThe finished illustration can be used for a variety of purposes:-\n- To sell the design to a client\n- Property marketing\n- Encourage investment\n- Support planning submissions\nPhoto-realistic artist’s impressions help clients and other parties to see what a project will look like before building starts, with accurate depictions of materials and finishes. They can even show what a building project will look like in different weather or lighting conditions. The advantages are numerous:\n- An accurate architectural illustration can inform decisions on materials and finishes before materials are purchased, allowing changes to be made without incurring excessive costs.\n- High quality artist’s impressions show a building in its environment, with landscaping giving a true idea of what the finished property will look like.\n- Incorporated into a location photograph, the property CGI can be used to create a photo-montage, showing the new property in its proposed position.\n- For development of existing properties, CGI techniques can be used to show alterations and enhance site photographs.\n- Architectural illustrations can be created to allow viewing from a variety of angles.\nOur talented CGI artists create amazing artist’s impressions and architectural illustrations of all types of buildings and infrastructure developments, from houses to industrial estates.\nWalk-throughs, fly-rounds and animations can be produced in a variety of styles to create stunning videos for sales and marketing presentations in marketing suites or online.\nNormally created from architectural drawings and materials schedules, architectural illustrations can also be produced from verbal briefs and references. Our CGI studio is equally at home working with a variety of origination materials, helping you to create the final high quality artist’s impression that you need.\nArchitectural visualisation fills the photography gap\nThere are many situations where the ideal scene or location is simply not available to be photographed. This is where photo-realistic architectural visualisation can fill the gap.\nParticularly useful for building materials manufacturers, infrastructure developers and energy companies, imaginary or hypothetical scenes can be created to show typical product application areas, even incorporating products within the scene.\nCreative photo-realistic visualisations provide an eye-catching, highly attractive presentation.\nRealistic artist’s impressions of interiors can be created from mood boards and material samples or to your own design criteria, incorporating bespoke furniture, decorative finishes and fabrics.\nWalk-through and animations of house and property interiors add appeal to sales and marketing materials for presentations, showrooms, marketing suites and websites.\nUnlimited potential of artist’s impressions\nCombining 3D product modelling with artist’s impressions of interiors and/or exteriors, site photography or library images enables building materials manufacturers to insert their products into photo-realistic settings, offering a viable alternative to studio photography.\nThis can offer huge savings over traditional studio photography as it does not require the design and construction of roomsets. It also provides amazing control over all elements of the image, including product finishes, interior decor, building materials, lighting, weather and camera angles.\nYou don’t have to wait for your products to be manufactured, saving time and costs, and allowing you to get marketing material prepared well ahead of product launch dates.\nAnd you don’t have to be in the studio (or on site) to supervise!\n3D rendered product modelling\nProduct and prototype visualisation provides 3D rendered images from detailed CAD drawings or the simplest of briefs. Again, animation options range from simple fly-rounds to complete working models.","Learning patterns/Improving your building photography/ja\nWhat problem does this solve?\nYour photos of buildings don't seem to come out as well as you'd hoped. Is it your camera? Is it the way you take the photos?\nWhat's the solution?\nHere's the good news: the quality of those photos can be greatly improved if you keep in mind a few simple tips. You don't need an expensive camera to take good shots – just a little knowledge and care when taking the image.\nよくある失敗 - カメラの構え方\nTake care to keep the camera straight. Spending ten seconds on composition (how the visual elements are arranged in the photo) and alignment can greatly improve your picture quality. And in spite of what you may see others doing, always hold your camera or your mobile phone in both hands when composing and taking the picture.\nCamera tilt is the single biggest problem with mobile phone uploads, and it's worth repeating: always hold your camera or your mobile phone in both hands when composing and taking the picture.\nWhile there are applications for later editing pictures by rotating them, it's much better to get it right in the first place.\nBuilding partly obscured\nTry to avoid objects that obstruct the view of your subject. Choose a different angle, or (if the obstruction – especially parked cars – might go away) come back later. If you absolutely can't avoid cars, for privacy reasons ensure that numberplates are not visible (the number plates had to be blurred out to display this image).\nIf your camera creates a timestamp watermark, please turn off that feature beforehand.\nDon't chop off the top of the building or, as some people like to say: \"make sure you get the cross on top of the steeple\". It can ruin a good picture if the viewer expects to see something that has been chopped off.\nThink about how far the edges of the building are from the frame. If the entire building won't fit in the frame, walk back if you can, or try a different angle. If there's nowhere from which you can take a good picture, concentrate on something else instead, such as some of the smaller architectural details.\nDon't chop off the bottom of the building unless it's unavoidable. It's easy to forget the bottom when you point your camera up to get the top of the building.\nIf the entire building won't fit in the frame, move back if you can, or try a different angle. If there's nowhere from which you can take a good picture, concentrate on something else instead such as some of the smaller architectural details.\nAvoid pointing the camera too close to the sun. The amount of flare in this photo makes the picture unusable, even though the sun itself is outside the frame.\nTo avoid camera movement, especially in dark church interiors, use the font or a pillar as a support. Blurred images such as this are of little use. Even outside, occasionally we're lucky enough to have something solid like a bench or fence conveniently located; if so, try resting your camera on it for greater stability, and see if the composition and alignment work from that angle.\nThis picture seems to have been taken one-handed; remember the rule about always holding your camera in two hands? Notice the annoying chopped-off candle holder on the left; again, think about each element in relation to the edges of your photo.\nCommon errors – auto settings\nThe camera has automatically exposed the sky correctly, but that leaves the building much too dark. If your camera has exposure adjustment, increase the exposure by about one stop and try again. If that doesn't solve the problem, take the picture from another angle and try to keep the amount of sky to a minimum.\nOverexposure like this is not very common on modern cameras. If you see it, check your camera settings: the \"exposure compensation\" may have been set incorrectly.\nCheck what your camera is focusing on\nHere, the camera autofocus has latched on to the lamp to the left, rather than the building.\nCommon errors – subjects\nPlease don't take photos of signs\nPlease don't upload photographs of posters, noticeboards, signs, modern murals, or anything else having text or two-dimensional images that might be copyright-protected. That applies even to text or images in a public place – even if everyone else is taking pictures of them. Ancient wall paintings in churches are fine: they have no copyright protection.\nHere, the copyright-protected text and images have been blurred out. It's OK if your photo of a building incidentally includes an unobtrusive sign that appears small in the final image, but if the sign appears large or prominent, please take the photo from another angle.\nNo posing please\nPlease don't upload images of people posing, even if there's a nice building in the background!\nThat looks a lot better!\nThe signs are small in the composition and don't cause a copyright problem. Maybe you can do even better than this?\nWhen to use\n- Individually uploaded photos of local buildings\n- Wiki Loves Monuments, 'Wiki Takes ...' events and 'Wiki Scavenger Hunts'\n- Compositional techniques, in the English Wikipedia.\n- \"Picture composition\" in the Danish Wikipedia (Google translation into English).\n- \"Composition\" in the Finnish Wikipedia, which goes into greater detail concerning lines and directionality (Google translation into English)."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:bd69217d-683d-4e51-bfac-fddbf45e5fea>","<urn:uuid:05ff742a-91b3-47fb-a038-253dc0b9430c>"],"error":null}