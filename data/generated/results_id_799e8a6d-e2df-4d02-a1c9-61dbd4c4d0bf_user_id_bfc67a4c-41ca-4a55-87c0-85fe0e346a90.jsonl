{"question":"What are the differences in treatment approaches between pancreatic cancer and hepatitis B?","answer":"Pancreatic cancer treatment involves three main approaches: surgery to remove the cancer, radiation therapy using high-dose x-rays, and chemotherapy using drugs to kill cancer cells. The treatment success depends heavily on early detection. In contrast, hepatitis B treatment is primarily supportive, focusing on rest, fluids, and adequate nutrition, as most adult cases resolve on their own. While pancreatic cancer requires active intervention, hepatitis B may not need specific medication in acute cases, though chronic cases might require antiviral medications. Additionally, hepatitis B can be prevented through vaccination, while there is no preventive vaccine for pancreatic cancer.","context":["Cancer of the pancreas (pancreatic cancer)\n- What is cancer of the pancreas?\n- What causes cancer of the pancreas?\n- What are the symptoms of cancer of the pancreas?\n- How is it diagnosed?\n- How is pancreatic cancer treated?\n- What does the future hold for those with cancer of the pancreas?\nWhat is cancer of the pancreas?\nPancreatic cancer is the development of abnormal cells in the pancreas, which results in a tumour. The pancreas makes pancreatic juice, which help to digest food in the small intestine. When pancreatic cancer spreads (metastasises) it usually travels through the lymphatic system. Cancer cells can also be carried through the bloodstream to the liver, lungs, bone or other organs.\nWhat causes cancer of the pancreas and can it be prevented?\nThe precise cause is not known. Smoking is thought to be a contributing factor. A 2010 US study suggested that people who drink two or more soft drinks per week may be at an increased risk of developing cancer of the pancreas. An earlier study in 2006 found that those who consumed 400mgs of vitamin D on a daily basis reduced their risk of the disease by 43%.\nWhat are the symptoms of cancer of the pancreas?\nCancer of the pancreas is hard to diagnose because the organ is hidden behind other organs. Also, the signs of pancreatic cancer are like many other illnesses.\n- Jaundice may occur, where the whites of the eye and skin become yellow.\n- Urine may become darker.\n- Back and abdomen pain, especially after eating.\n- Nausea, appetite loss, and weakness.\n- Unexplained weight loss.\n- Unexplained depression.\nHow is it diagnosed?\n- Blood, urine and stool test may be required.\n- An ultrasound may be used, where sound waves are used to find tumours. The sound waves are converted by the ultrasound machine into a visual image of the tumour.\n- A CT scan may be ordered, which is an x-ray capable of giving detailed pictures of the pancreas.\n- An ERCP may be also be done. During this test, a flexible tube is put down the throat, through the stomach and into the small intestine. The doctor can see through the tube and inject dye into the drainage tube (duct) of the pancreas so that the area can be seen more clearly on an x-ray. During an ERCP, the doctor may also take a biopsy (remove some cells) for further testing.\n- An angiogram may be ordered, which is a special x-ray of blood vessels.\nA surgical exploration of the abdomen (laparotomy) may be required, which allows the doctor to look at the abdominal organs.\nHow is pancreatic cancer treated?\nCancer of the pancreas is very hard to control. It can be cured only when it is found at an early stage, before it has spread. However, there are treatments for all patients with cancer of the pancreas, which can improve the quality of a person's life by controlling the symptoms and complications of the disease. Treatment depends on the age and sex of the patient as well as the type and size of the tumour. Three kinds of treatment are used:\n- Surgery (taking out the cancer or relieving symptoms caused by the cancer).\n- Radiation therapy (using high-dose x-rays or other high energy rays to kill cancer cells).\n- Chemotherapy (using drugs to kill cancer cells).\nWhat does the future hold for those with cancer of the pancreas?\nThe earlier pancreatic cancer is detected, the more successful treatment is likely to be. However the survival rate for pancreatic cancer after five years is poor. Around 380 people are diagnosed with pancreatic cancer in Ireland every year. The earlier the disease is detected, the more successful treatment is likely to be. However only a quarter of Irish people with pancreatic cancer can receive treatment, largely due to late diagnosis of the disease. Across Europe, the five-year survival rate from pancreatic cancer is less than one person in 20.\nBack to top of page","Hepatitis B is a liver infection and is spread by sharing infected bodily fluids. This condition often does not show symptoms but chronic cases can cause serious damage.\nWhat is hepatitis B?\nHepatitis B is an infection of the liver caused by the hepatitis B virus (HBV). There are other hepatitis viruses (A, C, D, E) that may behave and be transmitted differently. HBV causes swelling and inflammation of the liver that prevents its normal function. HBV can cause a short-term, acute illness that lasts up to a few months. It can also cause a lifelong illness, which more commonly develops in children.\nSince many people may not have symptoms or do not realize they are infected, their illness is often not diagnosed. Presenting symptoms include fever, fatigue, loss of appetite, nausea, vomiting, abdominal pain, and dark urine among others.\nTreatment is much like that for a regular infection, with plenty of rest, fluids, and adequate nutrition provided at home or from a medical team in more severe cases. Hepatitis B can be prevented by a variety of measures.\nYou should visit your primary care physician. Hepatitis B infection, if confirmed by a blood test, is treated with prescription antiviral medication.\nFree, private, and secure to get you the best way to well. Learn about our technology.\nHepatitis b symptoms\nAccording to the Centers for Disease Control and Prevention (CDC), in 2016, more than 3000 cases of acute hepatitis B were reported in the USA. Just 30 to 50 percent of adults who contract hepatitis B experience symptoms, and many people infected with hepatitis B virus do not realize they are infected, and therefore commonly not reported. The CDC estimates the actual number of acute hepatitis B cases was almost 21,000 in 2016.\nThese symptoms can appear from eight weeks to five months after initial exposure to the virus. The average time from exposure to onset is three months. It is important visit your healthcare professional immediately if there is any suspicion of infection. This is also important because the virus can still be unknowingly transmitted to others. Symptoms of acute hepatitis B include:\n- Loss of appetite\n- Abdominal pain\n- Dark urine\n- Clay-colored bowel movements\n- Joint pain\n- Jaundice: This is yellow coloring of the skin or eyes.\nDeveloping the lifelong form\nIn the acute form of HBV, symptoms usually last several weeks, but can last as long as six months. About 95 percent of adults exposed to HBV have the acute form, while the remaining 5 percent develop the lifelong form. In children, the statistics are reversed; 95 percent of children will develop the lifelong form of hepatitis B, while just 5 percent will have the acute form of the infection.\nHepatitis b causes\nHepatitis B is caused by a variety of factors that contribute to its spread. However, it is important to also understand how HBV is not spread. Certain individuals do have characteristics that place them at a higher risk of developing HBV.\nHow HBV is spread\nHepatitis B results when the hepatitis B virus enters the bloodstream. The only way to become infected is to be exposed to blood, semen or other body fluid that is infected. People can become infected from the virus via:\n- Birth: Spread can occur an infected mother to her baby during birth\n- Sex with an infected partner\n- Sharing needles or syringes with an infected person\n- Sharing items: This includes toothbrushes, razors or other medical equipment that accesses the bloodstream (a glucose stick, for example) with an infected person\n- Direct contact: Spread can occur from touching blood or open sores of an infected person\n- Exposure to blood: Spread can occur from needlesticks or other sharp instruments of an infected person\nHow HBV is not spread\nThe common thread among the causes above is that they involve contact/transmission of body fluids. Hepatitis B virus is not spread via airborne transmission or contact transmission if there is no blood contact involved. For example, hepatitis B virus is not spread via:\n- Food or water\n- Sharing eating utensils\n- Hand holding\nWho is most likely to be affected\nGiven the ways that HBV is spread from person-to-person, there are specific populations that are at a greater risk for contracting the disease even though anyone can get HBV. These populations include:\n- Infants born to infected mothers\n- People who inject drugs or share needles, syringes, or other drug equipment\n- Sex partners of people with HBV\n- Men who have sexual contact with men\n- People who live with a person who has HBV\n- Healthcare and public safety workers exposed to blood\n- Hemodialysis patients\nFree, secure, and powered by Buoy advanced AI to get you the best way to better. Learn about our technology.\nTreatment options and prevention for hepatitis b\nTreatment options include methods of testing and methods to restore fluid and nutrition levels as well as hospitalization, if necessary. There are also important preventative methods that are highly effective.\nThe first step to treating or preventing HBV infection is to see a doctor for testing. A blood test can help to tell whether you have immunity, or an acute or chronic hepatitis B infection.\nRestoring fluids and nutrition\nThere is no curative medication for HBV, and most infections in adults resolve on their own. Rest, fluids, and adequate nutrition will help this process. More severe cases of acute hepatitis B will require hospitalization to provide appropriate fluid and nutrition supplementation while the infection resolves.\nFortunately, HBV is a preventable disease, and an effective vaccine has been available since 1981. The hepatitis B vaccine is safe and effective in all age groups. It is a series of shots given over several months. You can also do the following in to reduce your risk of HBV:\n- Safer sex: Condoms act as a protective barrier against bodily fluids such as blood and semen that could be infected with the virus. Condoms can reduce your risk of contracting HBV, they do not eliminate the risk completely.\n- Know the HBV status of sexual partners: Don't engage in unprotected sex (sex without a condom) unless you're absolutely certain your partner isn't infected with HBV or any other sexually transmitted infections.\n- Be cautious about body piercing and tattooing: If you get a piercing or tattoo, look for a reputable shop. Ask about how the equipment is cleaned. Make sure the employees use sterile needles. If you can't get answers, look for another shop.\n- Avoid shared, re-used, or dirty needles: Use a sterile needle each time you inject anything and never share needles.\nIf a HBV infection does resolve on its own, the affected individual becomes immune and cannot become infected with HBV again.\nIndividuals with chronic HBV infection are at an increased risk for liver damage, cirrhosis, liver failure, liver cancer, and death. Chronically infected individuals will need to follow up closely with a liver doctor. Some antiviral medications are available that can help to suppress the infection.\nWhen to seek further consultation for hepatitis b\nIf you think that you have been exposed to HBV but are unsure of your vaccination status, call your physician immediately. They may be able to perform tests that will indicate your vaccination and exposure status.\nYou may be eligible for an injection of an antibody that may help protect you from HBV if it is given to you within 12 hours of exposure. Because this treatment only provides short-term protection, you also should get the hepatitis B vaccine at the same time, if you never received it.\nQuestions your doctor may ask to determine hepatitis b\n- Have you lost your appetite recently?\n- Have you experienced any nausea?\n- Are you sick enough to consider going to the emergency room right now?\n- Any fever today or during the last week?\n- Have you been feeling more tired than usual, lethargic or fatigued despite sleeping a normal amount?\nSelf-diagnose with our free Buoy Assistant if you answer yes on any of these questions.\n- Surveillance for Viral Hepatitis - United States, 2016. Centers for Disease Control and Prevention. Published October 1, 2018. CDC Link\n- Trpo C, Chan HLY, Lok A. Hepatitis B Virus Infection. The Lancet. Published June 18, 2014. The Lancet Link\n- Elgouhari HM, Abu-Rajab Tamimi TI, Carey WD. Hepatitis B Virus Infection: Understanding Its Epidemiology, Course, and Diagnosis. Cleveland Clinic Journal of Medicine. 2008;75(12):881-889. NCBI Link\n- Komatsu H, Inui A. Hepatitis B Virus Infection in Children. Expert Review of Anti-Infective Therapy. 2015;13(4):427-450. NCBI Link\n- Wilkins T, Zimmerman D, Schade RR. Hepatitis B: Diagnosis and Treatment. American Family Physician. 2010;81(8):965-972. AAFP Link\n- Tang LSY, Covert E, Wilson E, Kottilil S. Chronic Hepatitis B Infection: A Review. JAMA Network. 2018;319(17):1802-1813. JAMA Link"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:018e19e7-ee71-4ad4-98b4-da448bbbc4da>","<urn:uuid:d843a744-a7e6-4e74-a027-4b893c95b293>"],"error":null}
{"question":"How did Rembrandt's artistic journey evolve from Leiden to Amsterdam, and how does his work exemplify Baroque painting characteristics?","answer":"Rembrandt's artistic journey began in Leiden with his early famed series of allegorical paintings of the senses, and later developed in Amsterdam where he created works like 'Self-Portrait with Shaded Eyes' and 'Minerva in Her Study'. His work clearly exemplifies Baroque characteristics through his use of dramatic light effects and chiaroscuro. As a Baroque artist, Rembrandt's paintings evoked emotion and passion, utilizing the period's characteristic deep colors and intense light and dark shadows. His work influenced other artists in his circle and helped establish him at the forefront of an artistic movement characterized by a deep interest in humanity and daily life.","context":["Louvre Abu Dhabi sheds light on Dutch Golden Age\nABU DHABI - After its successful inaugural shows last year, the Louvre Abu Dhabi has set the pace for a second season by unveiling 95 artworks by Dutch masters under the exhibition appropriately titled “Rembrandt, Vermeer and the Dutch Golden Age: Masterpieces from the Leiden Collection and the Musee du Louvre.”\nThe exhibition, the largest of Dutch masters from the 17th century in the Gulf region, surveys Rembrandt’s artistic journey in his native Leiden and Amsterdam as well as his relationships with rivals and peers, including Johannes Vermeer, Jan Lievens, Ferdinand Bol, Carel Fabritius, Gerrit Dou, Frans van Mieris and Frans Hals.\nExhibition patrons are especially privileged because it coincides with the first display of the museum’s most recent acquisition, Rembrandt’s oil sketch “Head of a Young Man, with Clasped Hands: Study of the Figure of Christ” (circa 1656).\nOther highlights are works by Vermeer — “The Lacemaker” (Musee du Louvre) and “Young Woman Seated at a Virginal” (Leiden Collection) — that were painted on canvas cut from the same bolt. These are displayed side by side for what is thought to be the first time in 300 years.\nThe exhibition is drawn primarily from the Leiden Collection, one of the largest and most significant private collections of art from the Dutch Golden Age, interspersed with masterpieces from the Musee du Louvre’s Dutch collection. Loans from the Rijksmuseum and the Bibliotheque Nationale de France complete the presentation.\nHistory has cast the Dutch Golden Age as a brief period of the 17th century when the Dutch Republic, newly independent from Spain, established itself as a world leader in trade, science and the arts and became the most prosperous country in Europe.\nWealth derived from dominance in worldwide trade also resulted in one of the most productive periods in the making of art. Paintings and art objects were widely collected and traded, resulting in the proliferation of Dutch works in museums and collections across the world.\nRembrandt and Vermeer established themselves at the forefront of the artistic movement characterised by a deep interest in humanity and daily life.\n“In 2019, the UAE is celebrating the Year of Tolerance, which is testament to our long-standing tradition of nurturing a culture of openness and exchange. The exhibition… illustrates not only the importance of cross-border cultural collaborations but also how artistic creativity has always been at the heart of great historic moments,” said Mohamed al-Mubarak, chairman of the Department of Culture and Tourism in Abu Dhabi.\nLouvre Abu Dhabi Director Manuel Rabate affirmed the “commitment to bringing key moments in art and history to a new global audience and further cement the museum’s mission to become a centre for cultural exchange.”\nThe exhibition unfolds through six sections exploring the heart of the Dutch Golden Age; Rembrandt’s beginnings in Leiden; Rembrandt in Amsterdam; Fine Painting in Leiden; Picturing Everyday life in the Dutch Republic; and Historical Lessons and Tales of Morality.\nRembrandt’s early famed series of allegorical paintings of the senses drawn at the start of his career in Leiden and paintings created later in Amsterdam are on display. They include “Self-Portrait with Shaded Eyes” and “Minerva in Her Study,” his monumental history painting of the goddess (both from the Leiden Collection).\nThe works are displayed alongside paintings by other masters from Rembrandt’s artistic circle, illustrating the influence that these remarkable artists had on each other’s work.\nViewers also get an insight into the artistic traditions that flourished in Leiden and the wider Netherlands during this period, including the development of a new school of artists, called the fijnschilders (fine painters), who focused on painting portraits, character studies, history paintings and exquisitely rendered scenes of daily life.\nThe Leiden Collection, founded in 2003 by American collectors Thomas S. Kaplan and his wife, Daphne Recanati Kaplan, consists of approximately 250 paintings and drawings and represents one of the largest and most important assemblages of 17th-century Dutch paintings in private hands.\nHighlights from the Leiden Collection include Vermeer’s “Young Woman Seated at a Virginal,” Dou’s “Scholar Interrupted at His Writing,” Lievens’s “Boy in a Cape and Turban” and Rembrandt’s “Young Lion Resting.”\nAmong the highlights from the Musee du Louvre’s collections are: Dou’s “Self-Portrait with Palette in a Niche,” Bol’s “Rebecca and Eliezer at the Well” and an engraved nautilus shell (circa 1660-80).\nAlongside the exhibition, which is to close May 18, Louvre Abu Dhabi has announced a cultural programme featuring film screenings curated by Emirati artist Hind Mezaina, a pop-up costumed performance in the museum galleries as well as talks and workshops.","Baroque painting is the painting associated with the Baroque cultural movement. The movement is often identified with Absolutism, the Counter Reformation and Catholic Revival, but the existence of important Baroque art and architecture in non-absolutist and Protestant states throughout Western Europe underscores its widespread popularity.\nBaroque painting encompasses a great range of styles, as most important and major painting during the period beginning around 1600 and continuing throughout the 17th century, and into the early 18th century is identified today as Baroque painting. In its most typical manifestations, Baroque art is characterized by great drama, rich, deep colour, and intense light and dark shadows, but the classicism of French Baroque painters like Poussin and Dutch genre painters such as Vermeer are also covered by the term, at least in English. As opposed to Renaissance art, which usually showed the moment before an event took place, Baroque artists chose the most dramatic point, the moment when the action was occurring: Michelangelo, working in the High Renaissance, shows his David composed and still before he battles Goliath; Bernini's Baroque David is caught in the act of hurling the stone at the giant. Baroque art was meant to evoke emotion and passion instead of the calm rationality that had been prized during the Renaissance.\nAmong the greatest painters of the Baroque period are Velázquez, Caravaggio, Rembrandt, Rubens, Poussin, and Vermeer. Caravaggio is an heir of the humanist painting of the High Renaissance. His realistic approach to the human figure, painted directly from life and dramatically spotlit against a dark background, shocked his contemporaries and opened a new chapter in the history of painting. Baroque painting often dramatizes scenes using chiaroscuro light effects; this can be seen in works by Rembrandt, Vermeer, Le Nain and La Tour. The Flemish painter Anthony van Dyck developed a graceful but imposing portrait style that was very influential, especially in England.\nThe prosperity of 17th century Holland led to an enormous production of art by large numbers of painters who were mostly highly specialized and painted only genre scenes, landscapes, Still-lifes, portraits or History paintings. Technical standards were very high, and Dutch Golden Age painting established a new repertoire of subjects that was very influential until the arrival of Modernism.\nThe Council of Trent (1545–63), in which the Roman Catholic Church answered many questions of internal reform raised by both Protestants and by those who had remained inside the Catholic Church, addressed the representational arts in a short and somewhat oblique passage in its decrees. This was subsequently interpreted and expounded by a number of clerical authors like Molanus, who demanded that paintings and sculptures in church contexts should depict their subjects clearly and powerfully, and with decorum, without the stylistic airs of Mannerism. This return toward a populist conception of the function of ecclesiastical art is seen by many art historians as driving the innovations of Caravaggio and the Carracci brothers, all of whom were working (and competing for commissions) in Rome around 1600, although unlike the Carracci, Caravaggio persistently was criticised for lack of decorum in his work. However, although religious painting, history painting, allegories, and portraits were still considered the most noble subjects, landscape, still life, and genre scenes were also becoming more common in Catholic countries, and were the main genres in Protestant ones.\nThe term \"Baroque\" was initially used with a derogatory meaning, to underline the excesses of its emphasis. Others derive it from the mnemonic term \"Baroco\" denoting, in logical Scholastica, a supposedly laboured form of syllogism. In particular, the term was used to describe its eccentric redundancy and noisy abundance of details, which sharply contrasted the clear and sober rationality of the Renaissance. It was first rehabilitated by the Swiss-born art historian, Heinrich Wölfflin (1864–1945) in his Renaissance und Barock (1888); Wölfflin identified the Baroque as \"movement imported into mass\", an art antithetic to Renaissance art. He did not make the distinctions between Mannerism and Baroque that modern writers do, and he ignored the later phase, the academic Baroque that lasted into the 18th century. Writers in French and English did not begin to treat Baroque as a respectable study until Wölfflin's influence had made German scholarship pre-eminent.\nA rather different art developed out of northern realist traditions in 17th century Dutch Golden Age painting, which had very little religious art, and little history painting, instead playing a crucial part in developing secular genres such as still life, genre paintings of everyday scenes, and landscape painting. While the Baroque nature of Rembrandt's art is clear, the label is less use for Vermeer and many other Dutch artists. Flemish Baroque painting shared a part in this trend, while also continuing to produce the traditional categories.\nNotable Baroque painters\n- William Dobson (1611-1646)\n- George Jamesone (1587–1644)\n- Godfrey Kneller (1646–1723)\n- Peter Lely (1618–1680)\n- Daniël Mijtens (1590–1648)\n- John Michael Wright (1617-1694)\n- Rembrandt (1606–1669)\n- Hendrick Avercamp (1585–1634)\n- Gerard ter Borch (1617–1681)\n- Adriaen Brouwer (1605–1638)\n- Aelbert Cuyp (1620–1691)\n- Gerrit Dou (1613–1675)\n- Jan van Goyen (1596–1656)\n- Frans Hals (1580–1666)\n- Meindert Hobbema (1638–1709)\n- Gerard van Honthorst (1592–1656)\n- Pieter de Hooch (1629–1684)\n- Willem Kalf (1619–1693)\n- Pieter van Laer (1599–1642)\n- Gabriël Metsu (1629–1667)\n- Adriaen van Ostade (1610–1685)\n- Jacob van Ruisdael (1628–1682)\n- Salomon van Ruysdael (1602–1670)\n- Pieter Jansz. Saenredam (1597–1665)\n- Johannes Vermeer (1632–1675)\n- Jan Steen (1626–1679)\n- Václav Hollar (1607–1677)\n- Karel Škréta (1610–1674)\n- Petr Brandl (1668–1735)\n- Václav Vavřinec Reiner (1686–1743)\n- Peter Paul Rubens (1577–1640)\n- Anthony van Dyck (1599–1641)\n- Jacob Jordaens (1593–1678)\n- Jan Brueghel the Elder (1568–1625)\n- Frans Francken the Younger (1581–1642)\n- Clara Peeters (1594–1657)\n- Gerard Seghers (1591–1651)\n- Frans Snyders (1579–1657)\n- David Teniers the Younger (1610–1691)\n- Adriaen van Utrecht (1599–1652)\n- Cornelis de Vos (1584–1651)\n- Valentin de Boulogne (1591–1632)\n- Philippe de Champaigne (1602–1674)\n- Laurent de La Hyre (1606–1656)\n- Georges de La Tour (1593–1652)\n- Charles Le Brun (1619–1690)\n- Le Nain brothers :\n- Eustache Le Sueur (1617–1655)\n- Claude Lorrain (1600–1682)\n- Pierre Mignard (1612–1695)\n- Hyacinthe Rigaud (1659–1743)\n- Nicolas Poussin (1594–1665)\n- Simon Vouet (1590–1649)\n- Cosmas Damian Asam (1686–1739)\n- Adam Elsheimer (1578–1610)\n- Johann Liss (1590–1627)\n- Sebastian Stoskopff (1597–1657)\n- Federico Barocci (1535–1612)\n- Jacopo Chimenti (1554–1640)\n- Giovanni Battista Paggi (1554–1627)\n- Antonio Tempesta (1555–1630)\n- Bartolomeo Cesi (1556–1629)\n- Alessandro Maganza (1556–1640)\n- Bernardo Castello (1557–1629)\n- Lodovico Cigoli (1559–1613)\n- Enea Talpino (1559–1626)\n- Bartolommeo Carducci (1560–1610)\n- Caravaggio (1571–1610)\n- Guercino (1591–1666)\n- Annibale Carracci (1560–1609)\n- Guido Reni (1575–1642)\n- Orazio Gentileschi (1563–1639)\n- Artemisia Gentileschi (1592– c. 1656)\n- Agostino Carracci (1557–1602)\n- Ludovico Carracci (1555–1619)\n- Pietro da Cortona (1596–1669)\n- Gregorio Preti (1603-1672)\n- Francesco Cozza (1605-1682)\n- Mattia Preti (1613–1699)\n- Andrea Pozzo (1642–1709)\n- Salvator Rosa (1615–1673)\n- Giovanni Battista Tiepolo (1696–1770)\n- Josefa de Óbidos (1630–1684)\n- José Claudio Antolinez (1635–1675)\n- Alonso Cano (1601–1667)\n- Juan Carreño de Miranda (1614–1685)\n- Claudio Coello (1642–1693)\n- Juan van der Hamen (1596–1631)\n- Juan Bautista Mayno (1569–1649)\n- Juan Bautista Martínez del Mazo (1612–1667)\n- Bartolomé Esteban Murillo (1617–1682)\n- Antonio de Pereda (1611–1678)\n- Francisco Ribalta (1565–1628)\n- José de Ribera, Lo Spagnoletto (1591–1652)\n- Juan de Valdés Leal (1622–1690)\n- Diego Velázquez (1599–1660)\n- Francisco Zurbarán (1598–1664)\n- Counter Reformation, from Encyclopædia Britannica Online, latest edition, full-article.\n- Counter Reformation, from The Columbia Encyclopedia, Sixth Edition. 2001–05.\n- Helen Gardner, Fred S. Kleiner, and Christin J. Mamiya, \"Gardner's Art Through the Ages\" (Belmont, California: Thomson/Wadsworth, 2005)\n- For example, in French calling Poussin Baroque would be generally rejected\n- \"Getty profile, including variant spellings of the artist's name\". Getty.edu. 2002-12-11. Retrieved 2012-02-13.\n- Gombrich, p. 420.\n- Belkin (1998): 11–18.\n- His Lives of the Painters was published in Rome, 1672. Poussin's other contemporary biographer was André Félibien.\n- W. Liedtke (2007) Dutch Paintings in the Metropolitan Museum of Art, p. 867.\n- Panofsky, Erwin (1995). \"Three Essays on Style\". The MIT Press. p. 19.\n- Often described as Saint Bartholemew, martyred in similar fashion, but now recognized as St Philip. See Museo del Prado, Catálogo de las pinturas, 1996, p. 315, Ministerio de Educación y Cultura, Madrid, No ISBN\n- Belkin, Kristin Lohse (1998). Rubens. Phaidon Press. ISBN 0-7148-3412-2.\n- Belting, Hans (1994). Likeness and Presence: A History of the Image before the Era of Art. Edmund Jephcott. University of Chicago Press. ISBN 0-226-04215-4.\n- Mark Getlein, Living With Art, 8th edition.\n- Gombrich, E.H., The Story of Art, Phaidon, 1995. ISBN 0-7148-3355-X\n- Christine Buci-Glucksmann, Baroque Reason: The Aesthetics of Modernity, Sage, 1994\n- Michael Kitson, 1966. The Age of Baroque'\n- Heinrich Wölfflin, 1964. Renaissance and Baroque (Reprinted 1984; originally published in German, 1888) The classic study. ISBN 0-8014-9046-4"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:f5a53eed-fc9a-45ea-916d-fe6d2be27deb>","<urn:uuid:e9306bd1-8419-4f65-a1ba-3d47036d0aee>"],"error":null}
{"question":"As an avid wildlife enthusiast, what are the key biodiversity features that distinguish El Yunque National Forest from the Nantahala National Forest region?","answer":"El Yunque National Forest has the greatest biodiversity among national forests and is unique as the only tropical rain forest managed by the Forest Service. It is home to notable species like the endangered Puerto Rican parrot and the coqui tree frog. The Nantahala National Forest, in contrast, is characterized by mountain and valley ecosystems with elevations ranging from 1,200 to 5,800 feet, and offers recreational fishing opportunities for species like spotted bass, striped bass, catfish, crappie, and sunfish in Lake Chatuge.","context":["U.S. Forest Service planning teams must complete rapid assessments of ecosystem conditions on national forests and the effects on those ecosystems (such as this one at Cedar Lake) from stressors, such as climate change. U.S. Forest Service photo\nThis post is part of the Science Tuesday feature series on the USDA blog. Check back each week as we showcase stories and news from the USDA’s rich science and research portfolio.\nFrom South Carolina’s coastal plain to the North Carolina mountains to the tropics of Puerto Rico to the southern Sierra Nevada region of California, climate change is on the minds of forest planners.\nThat’s because U.S. Forest Service planning teams in these areas are among the first to revise their land and resource management plans under the 2012 Planning Rule. To help them in their planning, land managers from the Francis Marion, Nantahala, Pisgah, El Yunque, Inyo, Sequoia, and Sierra national forests will turn to a web-based tool known as the Template for Assessing Climate Change Impacts and Management Options.\nForest Plans help guide the management of national forests and are typically revised every 10 to 15 years. The plans help ensure that national forests and grasslands continue to meet the requirements of the National Forest Management Act—for clean air and water, timber and other forest products, wildlife habitat, recreation and more. Read more »\nEl Yunque National Forest in Puerto Rico. At 28,000 acres, it’s the smallest national forest and the only tropical rain forest the Forest Service owns, boasting the greatest biodiversity among national forests.\nThis post is part of the Science Tuesday feature series on the USDA blog. Check back each week as we showcase stories and news from USDA’s rich science and research portfolio.\nEl Yunque National Forest in Puerto Rico is unique for the U.S. Forest Service. At 28,000 acres, it’s the smallest national forest and the only tropical rain forest managed by the Forest Service, boasting the greatest biodiversity among national forests. Read more »\nIn 2010, the U.S. Mint began issuing a series of quarters, such as this one from El Yunque National Forest, featuring national forests along with other sites highlighting ’America the Beautiful.’ Photo courtesy of the U.S. Mint\nImagine going to the grocery store and getting a national forest quarter as your change and holding onto it as a collector’s item. That can happen now because of the recent release of the El Yunque National Forest coin. The coin features the endangered Puerto Rican parrot and the coqui tree frog amongst tropical vegetation. Read more »\nPuerto Rican Parrot, one of the 10 most endangered birds in the world.\nDeep amid the dense greenery of a rain forest, U.S. Forest Service scientists are nursing a special patient back to health.\nThe patient is on pain medication, but lucid enough to ruffle his emerald green feathers and fill the room with angry squawks when a biologist removes him from an incubator. It is a Puerto Rican parrot with a broken leg, a serious injury for one of the world’s most endangered bird species. Read more »","Late Summer Getaway – Jackrabbit Mountain\nJackrabbit Mountain Recreation Area is located in the Tusquitee Ranger District of the Nantahala National Forest. The Recreation Area is on a wooded peninsula of scenic 6,950-acre Lake Chatuge, with a backdrop of high mountain peaks.\nLake Chatuge is touted as the crown jewel of Tennessee Valley Authority’s system of lakes and is an impoundment of the Hiwassee River. Similar to a Swiss alpine lake setting, the lake is filled with a bounty of fish, from spotted, white and striped bass to catfish, crappie and sunfish.\nSome of the sites and infrastructure within the campground were updated in recent years. There are no water or electric hookup sites. There are hot showers and drinking water available in each camping loop.\nThere is a 13-mile system of trails designed to accommodate mountain bikers ranging from the beginner to the expert rider while still providing for hiking use. This trail system has a trailhead with restroom, parking lot, drinking water, picnic tables and information board.\nJackrabbit Mountain Hiking Trail loops around the peninsula for 2.4 miles, offering spectacular views of the lake for hikers. Mountain biking is not allowed on this trail. Several marinas in the area rent boats and other watercraft.\nEach campsite has a picnic table and fire grill. Some sites have tent pads. Some sites can accommodate 32’ or longer RVs.\nAlcoholic beverages are prohibited in Jackrabbit Recreation Area.\nWhat Will I See?\nSpectacular views along Lake Chatuge. Some lakefront sites, picnic pavilion in campground: first-come, first-served.\nSeveral hiking and biking trails, as well as campgrounds with amenities for convenience.\nWhat Should I Know?\nAmenities: 100 campsites, drinkable water, 2 picnic shelters, a swimming beach, 2-mile hiking trail, 13-mile mountain biking trail system, a boat ramp, fully accessible showers and flush toilets, and an amphitheater. For reservations, call 877-444-6777 or visit www.recreation.gov. For additional information, contact the Tusquitee Ranger District Office at 828-369-5152.\nOperating Season: May 1- Sept. 30.\nFee: Camping fees range from $15 to $30 per night.\nHow Will I Get There?\nTake Hwy 64 East towards Hayesville NC for 9 miles. From Hayesville, go 6.2 miles, and then turn right on NC 175. Travel 2.5 miles, and turn right on NC 1155.\nTake US 64 East for 6.2 miles, and turn right onto NC 175. Go 2.5 miles. Turn right onto SR 1155.\nTake GA75North for about 3.5 miles, to the NC-GA state line, where it turns into NC175. Proceed 1.5 miles, and turn left onto SR1155.\nTake I-40 West to Canton. Take Exit 27 onto 74West/19-23South. At Exit 81, take 23-441South to Franklin. In Franklin, take 64West for 31 miles, and turn left on NC175. Go 2.5 miles. Turn right on SR1155 to the recreation area.\nAbout Nantahala National Forest\nThis forest lies in the mountains and valleys of southwestern North Carolina. Elevations in the Nantahala National Forest range from 5,800 feet at Lone Bald in Jackson County to 1,200 feet in Cherokee County along Hiwassee River below Appalachian Lake Dam. “Nantahala” is a Cherokee word meaning “land of the noon day sun,” a fitting name for the Nantahala Gorge, where the sun only reaches to the valley floor at midday. With over a half million acres, the Nantahala is the largest of the four national forests in North Carolina. The national forest is divided into three ranger districts: Cheoah, Nantahala and Tusquitee."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:26fa7207-32c1-4a52-8026-2c759daca1d8>","<urn:uuid:8c965e8c-cc66-426e-bae3-78e859fa557c>"],"error":null}
{"question":"How does the concept of patient autonomy differ between traditional Japanese medical ethics and modern Western medical standards?","answer":"In traditional Japanese medical ethics, influenced by Bushido, patient autonomy is not explicitly addressed as a principle, and the system emphasizes virtues like loyalty to the group and politeness in doctor-patient relationships. This contrasts sharply with modern Western medical standards, where patient autonomy is considered fundamental. Western medicine generally prohibits medical paternalism for legally competent persons, requiring their informed consent for treatments. This is illustrated in cases like Dax Cowart, where Western medical ethics ultimately supported his right to refuse treatment despite doctors believing it was in his best interest. The only exception in Western practice is 'weak paternalism,' which permits temporary interference to determine if a patient is competent to make decisions.","context":["Nishigori, Hiroshi MD, MMEd; Harrison, Rebecca MD; Busari, Jamiu MD, PhD, MHPE; Dornan, Tim MD, PhD, MHPE\nDr. Nishigori is associate professor, Center for Medical Education, Kyoto University, Kyoto, Japan.\nDr. Harrison is associate professor, Division of Hospital Medicine, Oregon Health & Sciences University School of Medicine, Portland, Oregon. She was also visiting associate professor, International Research Center for Medical Education, the University of Tokyo, from April to July 2009.\nDr. Busari is associate professor, Department of Educational Development and Research, Faculty of Health, Medicine and Life Sciences, Maastricht University, Maastricht, the Netherlands.\nDr. Dornan is professor of medical education, Department of Educational Development and Research, Faculty of Health, Medicine and Life Sciences, Maastricht University, Maastricht, the Netherlands.\nFunding/Support: None reported.\nOther disclosures: None reported.\nEthical approval: Reported as not applicable.\nPrevious presentations: Earlier versions of this essay were presented at the 39th annual meeting of the Japan Society for Medical Education, Iwate, Japan, July 27, 2007, and in a preconference workshop, Association of Medical Education in Europe 2007, Trondheim, Norway, September 25, 2007.\nCorrespondence should be addressed to Dr. Nishigori, Center for Medical Education, Kyoto University, Yoshida-konoe-cho, Sakyo-ku, Kyoto, 606-8501, Japan; telephone: (075) 753-9325; fax: (075) 753-9339; e-mail: email@example.com.\nMedical professionalism has recently become a core topic in medical education,1 which is reflected in a growing body of literature discussing curriculum development, teaching, and the evaluation of professionalism in physicians’ development.2 The physician charter proposed by the American Board of Internal Medicine Foundation, American College of Physicians Foundation, and the European Federation of Internal Medicine3–5 is frequently cited as an authoritative document about professionalism. The charter’s 3 principles (primacy of patient welfare, patient autonomy, and social justice) and 10 professional commitments5 are widely endorsed by international professional associations, colleges, societies, and certifying boards. However, we believe that the discussion of professionalism should also be related to the world’s different cultures and social contracts, respecting local customs and values even when they differ from Western ones.6 This is because the role of the physician is subject to cultural differences and dependent on the nature of the particular health care system in which medicine is practiced.6 As medical professionalism has mostly been considered from a Western perspective, there is a need to examine how the same, similar, or even different concepts are reflected in a wider range of cultural contexts.7\nIn Japan, professionalism is being discussed in a variety of forums, such as the Professionalism Committee of the Japanese Society of Medical Education and the Japanese Society of Internal Medicine.8 Questions have been raised about how to translate the mostly Western concepts of professionalism presented in international publications into the Japanese setting as well as how to tell the international community about unique Japanese concepts relating to medical professionalism. The very term professionalism, whose first meaning is “the conduct, aims, or qualities that characterize or mark a profession or a professional person,”9(p930) largely reflects a Western concept; there is no corresponding word in most Asian languages, including Japanese.10\nTo describe the views of some Japanese doctors on professionalism-related concepts, we have chosen Bushido as a value system, because it has much in common with Western virtue ethics; its meaning, “the way of the warrior,” is comparable to that of professionalism. It is a historical Japanese code of personal conduct originating from the ancient samurai warriors. In this article, we introduce the concepts of Bushido, compare them with Western concepts of medical professionalism—as represented by the physician charter mentioned earlier—and present views of some doctors now working in Japan about the continuing relevance (and occasional nonrelevance) of Bushido to their medical practices.\nThe Concepts of Bushido\nAlthough there are many books written about Bushido, the one by Inazo Nitobe,11 Bushido: The Soul of Japan, published in English in 1900, is a classic that is highly referenced in the international community. Nitobe describes Bushido as the code of moral principles that the knights (samurais) were required or instructed to observe. It is likened to chivalry and the noblesse oblige of the ancient warrior class of Europe. As in the martial arts of judo or karate, Bushido has a basis in Buddhism, Confucianism, and Shintoism.12 Though some cultural experts and scholars argue that the influence of Bushido on Japanese society has lessened,13 others say that the spirit of Bushido remains in the minds and hearts of the Japanese people.9,14 Although Bushido is not specific to medicine, some argue that it continues to influence the behavior of modern Japanese doctors.15\nThe seven principal virtues\nThe seven principal virtues in Bushido are rectitude (gi), courage (yu), benevolence (jin), politeness (rei), honesty (sei), honor (meiyo), and loyalty (chugi). Below, we describe each virtue in more detail. We have presented the virtues in the order given by Nitobe.\nThe first virtue, rectitude (gi), is considered the most fundamental virtue of the samurai. It is the way of thinking, deciding, and behaving in accordance with reason, without wavering. In a medical setting, this guides the doctor to what she or he should be doing; therefore, it is analogous to the concept of professionalism itself. It is also similar to the concept of altruism, as rectitude is usually meant as the antonym of seeking personal benefit. Furthermore, because the same Chinese character is used for rectitude and justice in Japanese writing, the concept of rectitude is also tied in with the concept of social justice. In an e-mailed survey of Japanese doctors that we carried out in 2012,* 117 of the 133 respondents (88%) agreed (by answering “strongly agree” or “agree”) that gi exists in their daily practices. Representative comments are “It is the value of justice or morality for doctors,” “I have sacrificed my private life because I am a doctor (so there is gi in me),” and “I would not work as a hospital doctor if I pursued financial benefit.”\nCourage (yu), the second virtue, meaning the spirit of daring and bearing (i.e., how one stands, walks, and behaves), is defined as doing what is right in the face of danger. In Bushido, the concept that righteous action speaks louder than words is highly valued. Whilst there is no analogous concept in the physician charter, yu can be understood to mean being unafraid to put the principles of professionalism into practice. Eighty-three respondents (62%) agreed to the existence of this virtue in their daily practices. Representative comments are “Doctors who went to rescue people suffering from the 2011 Great East Japan Earthquake and Tsunami are good examples” and “Recently, I find it difficult to practice yu, as patients became more and more demanding.”\nThe third virtue, benevolence (jin), encompasses the concepts of love, sympathy, and pity for others and is recognized as the highest of all the attributes of the human soul. For doctors, that means practicing “medicine as a benevolent art,” as one respondent to the survey expressed it. The concept of benevolence is expressed as “patient welfare” or “altruism” in the physician charter, though the Bushido concept of jin is more emotional and linked to empathy. In the survey, 123 respondents (93%) agreed that jin is alive in their clinical practices. Representative comments are “Jin is absolutely necessary!!!” and “There is no medical practice without jin.”\nPoliteness (rei), the fourth virtue, is defined as respectful regard for the feelings of others. Nitobe11 said that rei “suffers long, and is kind; envieth not, vaunteth not itself, is not puffed up; does not behave itself unseemly; seeks not her own; is not easily provoked; takes no account of evil.” Although it may be one of the most influential concepts of the doctor–patient relationship in Japan, politeness is not described in the physician charter. We suggest that it is analogous to a commitment to maintaining appropriate relationships. In the survey, 111 respondents (83%) agreed that rei is important in their clinical practices. Representative comments are “I always try to show rei to patients,” “I cannot do medical practice without rei,” and “We must show rei as a member of society. It is a virtue that goes beyond medicine.”\nThe Chinese character for honesty (sei) combines the characters for “word” and “perfect.” The phrase bushi no ichi-gon means “the word of a samurai,” which is a guarantee of truth. This fifth virtue is a counterpart to the commitment to honesty in the physician charter, though Bushido places greater emphasis than the charter on spoken words. Therefore, doctors in Japan may be embarrassed when they orally tell something important to patients and then have to change it (which happens in daily clinical practice). In the survey, 94 respondents (71%) agreed that sei is important in their clinical practices. Some representative comments are “In most cases, I tell my patients the truth even though it is a bad news,” “Sei is fundamentally important,” and (a contrasting view) “Sometimes the end justifies the means.”\nHonor (meiyo), the sixth virtue, is recognized as the ultimate pursuit of goodness. Nitobe11 wrote, “The sense of meiyo could not fail to characterize the samurai, born and bred to value the duties and privileges of their profession.” By writing that “Death involving a question of meiyo was accepted in Bushido as a key to the solution of many complex problems,” Nitobe tried to explain the meaning of hara-kiri and seppuku; both are the classical types of suicide for samurai. We could draw a parallel with the concept of commitment to professional responsibilities, although there are some clear differences between the two; for example, doctors do not have to kill themselves as a result of unprofessional behavior. In the survey, 86 respondents (65%) agreed that meiyo is important in their clinical practices. Representative comments are “I feel meiyo to be a doctor,” “I do not know.… I do not care much about meiyo, to be honest,” and “Recently, I feel meiyo has lessened.”\nIn contrast to the individualism of the West, the Japanese have long valued loyalty (chu-gi) to the needs and interests of the group (e.g., family or hospital staff), placing the group’s needs above their own needs and interests. Bushido says that the interests of the family and the interests of its members are inseparable. Indeed, institutional loyalty is one of the factors that has encouraged Japan’s health care workforce to display values of altruism in patient care. Yet in the survey, only 62 respondents (47%) agreed that chu-gi is important in their clinical practices. Representative comments are “I feel chu-gi to my boss and the hospital I am working for,” “I weigh my personal benefit against the institutional one where I belong,” and “Recently, I feel fewer and fewer doctors feel chu-gi.”\nComparing Bushido and the Physician Charter\nBy comparing Bushido with the physician charter, we found that there are omissions, nuances, and blendings of words that create differences between the two in addition to the differences that are clearly there, which reminds us how meanings can be altered in translation and interpretation. Nevertheless, comparisons of Bushido and the physician charter can provide fresh insights into the understanding of professionalism. The charter calls for altruism from doctors, a concept that has a long tradition in Western thought.16 The Japanese way of upholding the primacy of patient welfare is to practice a blend of rectitude, benevolence, and loyalty. Similarly, although the concept of social justice per se may not prevail in the Japanese health care system, the concepts of rectitude, honor, and loyalty together represent social justice. For example, when these virtues work together within a universal health care system (i.e., one that covers everyone), they can motivate physicians to eliminate discrimination in health care.17\nThere are also several commitments in the physician charter that are not present in Bushido, because the charter describes medical professionalism, whereas Bushido describes a code of conduct for people in general. This difference can be seen in Table 1, which presents a comparison of the 7 virtues of Bushido and the 3 principles and 10 commitments of the physician charter.\nConcepts in Bushido That Differ From Contemporary Views of Professionalism\nExamples of differences\nAlthough Bushido is intrinsically intertwined with the values of Japanese society,2 it has concepts that differ from—and sometimes conflict with—contemporary notions of professionalism. Bushido does not address the autonomy of the individual, which is one of the principles in the physician charter. This omission may encourage paternalistic relationships between doctors and patients in Japan; such relationships are increasingly considered unacceptable in Western culture. In the chapter on self-control, which is an associated virtue in Bushido, Nitobe11 stated that the Japanese were required not to show their emotions to others, expressed in the phrase “He shows no sign of joy or anger.” Although this behavior might be considered an appropriate trait in Japan, it can cause miscommunication between doctors and patients. Inequality between men and women is another example, rooted in Bushido, where the roles of women were classified as naijo, the “inner help” of the home. With the recent influx of women into Japanese medicine, such challenges as maintaining work–life balance (e.g., through flexible work arrangements) are transforming the traditional concept of Japanese work ethics and provoking intergenerational discussions that are shaping the present-day view.18\nFactors behind the differences\nMany factors account for differences between Bushido and the physician charter. An important factor is religion. For example, although the social contract between medicine and society is very important in discussing medical professionalism,6 the concept of a “social contract” is foreign to Japanese culture (and probably other non-Judeo-Christian cultures), as it implies a covenant based on Judeo-Christian principles. Another example: In Confucian cultures like Japan, young people are expected to show respect for elderly people. Demonstrating politeness (rei) to an older person calls for a type of deference on the part of a doctor that might not be seen in Western cultures.\nThe extent to which a culture values individualism is another factor influencing the professionalism of doctors.19 Loyalty (chu-gi) motivates doctors to give greater weight to the interests of the group in cultures like Japan’s that value a collective approach.\nA final, and very basic, difference between the physician charter and Bushido is that the former is founded in an ethical system based on duty and rules, whereas Bushido is founded in virtue ethics,20 which concerns the character of the actor. In ethical systems based on duties and rules, one judges whether a course of action is ethical or not according to its adherence to ethical principles, focusing on doing, whereas virtue ethicists judge whether an action is ethical according to the character trait the actor embodies, focusing on being. Virtue ethics has attracted increased interest in the field of general philosophy in recent years and has also entered into discussions about medical professionalism,21 such as this one.\nA Call for Multicultural Perspectives on Professionalism\nWhile recognizing that Bushido was in full force at a particular time and place in Japanese history and culture and is by no means a comprehensive ethical system, we suggest that its concepts are applicable to discussions of medical professionalism for those teaching and practicing in Japan today, and merit further study. Given the pace of globalization, which can easily cause the hegemonic imposition of Western culture and discourage cultural diversity, we hope this article will encourage a richer discussion, from the viewpoints of different cultures, on the meaning of professionalism in today’s health care practice.\nAcknowledgments: The authors wish to thank Dr. Yoshiyasu Terashima for taking part in a symposium, “Bushido and Medical Professionalism in Japan,” with Hiroshi Nishigori at the 39th annual meeting of the Japan Society for Medical Education. The authors also wish to thank Dr. Gordon Noel, Dr. Graham McMahon, Mr. Christopher Holmes, and Prof. Kimitaka Kaga for reviewing the manuscript.\n* We sent our survey to 422 practicing physicians registered in a doctors’ directory in Japan. We asked them to rate, on a five-point scale, the extent to which the seven virtues of Bushido are still alive in their daily clinical practices and to add comments explaining their responses. Cited Here...\n1. Stern DT, Papadakis M. The developing physician—becoming a professional. N Engl J Med. 2006;355:1794–1799\n2. van Mook WNKA, de Grave WS, Wass V, et al. Professionalism: Evolution of the concept. Eur J Intern Med. 2009;20:e81–e84\n3. Medical Professionalism Project. . Medical professionalism in the new millennium: A physicians’ charter. Lancet. 2002;359:520–522\n4. Blank L, Kimball H, McDonald W, Merino JABIM Foundation; ACP Foundation; European Federation of Internal Medicine. . Medical professionalism in the new millennium: A physician charter 15 months later. Ann Intern Med. 2003;138:839–841\n6. Cruess SR, Cruess RL, Steinert Y. Linking the teaching of professionalism to the social contract: A call for cultural humility. Med Teach. 2010;32:357–359\n7. Ho MJ, Lin CW, Chiu YT, Lingard L, Ginsburg S. A cross-cultural study of students’ approaches to professional dilemmas: Sticks or ripples. Med Educ. 2012;46:245–256\n8. Miyazaki J, Bito S, Obu S Inside a Pocket of a White Coat: Considering Medical Professionalism in Japan [in Japanese]. 2009 Tokyo, Japan Igaku shoin\n9. Merriam-Webster’s Collegiate Dictionary. 1993;93010th ed Springfield, Mass Merriam-Webster, Inc\n11. Nitobe I Bushido: The Soul of Japan. 2012 New York, NY Kodansha USA\n15. Hori M. Medical professionalism in Bushido [in Japanese]. Sogo Rinsho. 2008;57:1497–1498\n16. Coulter ID, Wilkes M, Der-Martirosian C. Altruism revisited: A comparison of medical, law and business students’ altruistic attitudes. Med Educ. 2007;41:341–345\n17. Campbell JC, Ikegami N The Art of Balance in Health Policy: Maintaining Japan’s Low-Cost, Egalitarian System. 2008 New York, NY Cambridge University Press\n18. Nomura K, Yano E, Fukui T. Gender differences in clinical confidence: A nationwide survey of resident physicians in Japan. Acad Med. 2010;85:647–653\n19. Triandis HC Individualism and Collectivism (New Directions in Social Psychology). 1995 Boulder, Colo Westview Press\n21. Bryan CS, Babelay AM. Building character: A model for reflective practice. Acad Med. 2009;84:1283–1288","Paternalism is the interference with people's liberties or autonomy \"for their own good\" or to \"prevent their harm\" irrespective of the preferences of the person whose liberty is being curtailed . A powerful way to understand the issues and controversies about paternalism in medicine is to consider the case of Dax Cowart, who was severely injured after a gas explosion caused second and third degree burns over 67 percent of his body . A 1974 film, shot 10 months after the accident, shows Cowart undergoing painful but life-saving treatments. The film mostly consists of Cowart's interviews with Robert White, a psychiatrist at the University of Texas at Galveston. Burn unit doctors told White that Cowart was irrational and depressed and needed to be declared incompetent so his mother could be appointed his legal guardian and authorize surgery on his hands.\nUnlike doctors in the burn unit, the surgeons refused to operate unless Cowart gave informed consent or was declared legally incompetent and a court-appointed surrogate authorized surgery. Dr. White and another psychiatrist found Cowart to have the capacity to make his own medical decisions and refused to participate in the process to have him declared legally incompetent. Yet Cowart's doctors were still unwilling to honor Cowart's refusals of treatments. Eventually Cowart agreed to the surgery because, he said, he believed that it was the fastest route out of the hospital, where he could reestablish control over his life .\nThe difficulty with paternalism for legally competent persons is that, first, someone's sincere belief about what is good for another person may be wrong. With the best intentions people may be mistaken about what harms or benefits others . Doctors were wrong in assuming what values were most important to Cowart and in predicting that he would regain some vision and use of his hands, be able to dress himself, and attend to his personal needs. Second, limiting the liberty of competent persons offers insufficient respect for their autonomous actions or their ability to make decisions for themselves. People find it intrinsically valuable to plan their own lives and live as they wish . Third, there is utility or instrumental value in letting people live as they wish because competent people generally are the best judges of what is best for them and because we learn from each other's successes and failures . In deciding for ourselves, moreover, we develop our potential as autonomous persons, gain respect from others, and do not feel thwarted. Paternalism is generally considered an unwarranted interference with the liberties of people who can act autonomously because it undercuts what they want for themselves and their liberty to live out their lives as they wish as long as they do not interfere with others. Current laws and policies generally do not permit medical paternalism for legally competent persons.\nSometimes we are not sure whether persons are competent who, in our view, are about to harm themselves. In such cases, it seems appropriate, perhaps even a moral duty, to interfere to determine if the person is competent. For example, a seemingly competent person may want to fly from a 15th story window, eat poisonous mushrooms, or walk into a minefield. Weak paternalism permits interference with the liberty of others to determine whether they are competent or capable of making a rational choice [7-9]. Most people would argue that it is justifiable to interfere with persons about to harm themselves to determine if they have the capacity to look after their interests, understand the consequences of what they are doing, or act voluntarily. Weak paternalism honors the autonomous decisions of competent persons while also protecting people who may be acting nonautonomously or on insufficient information. Weak paternalism is especially important in medicine since it extends more protection to people who are impaired by such things as illness, ignorance, drugs, or fear.\nWhen Cowart was first taken to the hospital he demanded that the clinicians let him die. Since Cowart was disoriented and hallucinating, his doctors could justify treating Cowart over his objections, using this widely defended principle of weak paternalism. As the months went by and he became articulate, clear in his reasoning, and unwavering in his refusals of burdensome treatments, it became increasingly difficult for physicians to use the defense of weak paternalism.\nCowart is now a lawyer defending patients' rights. He insists that, while he is happy to be alive, he was not treated with respect, and his competent refusal should have been honored. He was more accurate in predicting the severe limitations with which he lives and claims that the result was not worth his suffering. Honoring a patient's competent refusal of a burdensome treatment does not constitute participation in a suicide, as some doctors feared. One would hope that Cowart's doctors would have recommended or even implored him to consider life-saving treatments or meet persons with disabilities who were living full and happy lives. Still, they crossed a legal and moral line in treating this highly competent man against his will without even a court hearing.\nFirst-year medical students at the Brody School of Medicine watch the film about Cowart, Please Let Me Die, in our medical humanities course . It introduces them to issues of competency, informed consent, and paternalism. Most, if not all, students agree with the psychiatrists but struggle with the difficult choice faced by his doctors. Compassion seems to lead them in one direction and respect for liberty in another. There is no conflict between the need to protect sick people and to honor their self-determination when they authorize recommended treatments or hospitalizations. The problems arise when we cannot simultaneously do what we think is best for people and also respect their refusal of treatment or hospitalization, and solutions often depend on competency determinations.\nPaternalism is justifiable if someone lacks the capacity to look after his or her interests. Some form of protection is justified or even obligatory when people cannot make decisions for themselves, suffer incapacitating illnesses, show involuntary self-destructive behavior, or make choices so inappropriate to their own established life goals that we doubt their autonomy. Interference seems justified in the presence of people's nonautonomous, self-destructive behavior or when they resort to acts that are irrational, unreasonable, and uncharacteristic. Thus, paternalism (some prefer the less sexist word \"parentalism\") is sometimes a duty in medicine, and clinicians have to decide when they should act like good parents and help people who cannot look out for themselves.\nFor example, temporary involuntary commitment of a patient may ultimately enlarge that person's liberty . Civil commitment laws for persons considered dangerous to themselves are paternalistic in the sense that they interfere with the liberty or autonomy of such persons for their own good or to prevent harm. The justification for these laws is that people sometimes lack the capacity to act in their own interest. When people are very ill, they are \"not themselves\" and are not choosing autonomously. As a society, we can even adopt paternalistic laws for competent adults, such as requiring motorists to wear seatbelts, motorcyclists to wear helmets, prohibiting swimming in dangerous areas, and requiring parents to protect their children. Doctors, however, are private citizens and cannot restrain the liberties of others simply because they do not like competent patients' decisions.\nLimiting the liberty of others can be justified if they lack capacity to make the relevant decision (paternalism), if they pose harm to others (the harm principle), or if their behavior is so bizarre that we should intervene to allow time to determine if their actions are autonomous and informed (weak paternalism). Interference with the liberty of adults requires a heavy burden of proof to show they are incapacitated, incompetent, or a threat to themselves or others. It requires proving that the probability and magnitude of the possible harm merits the interference and that the means used are effective and the least restrictive means available [10-11].\nDworkin G. Paternalism. In: Audi R, ed. Cambridge Dictionary of Philosophy. New York: Cambridge University Press; 1995: 564.\nPlease Let Me Die [film]. Galveston: University of Texas, Department of Psychiatry; 1974.\nDax's Case [film]. New York: Concerns for Dying; 1985.\nKopelman L. Moral problems in psychiatry: the role of value judgments in psychiatric practice. In: Veatch R, ed. Medical Ethics.2nd ed. Boston, MA: Jones and Bartlett Publishing Company; 1997: 275-320.\n- Dworkin, G. Autonomy and behavior control. Hastings Cent Rep. 1976;6(1):23-28.\nMill JS. On Liberty. Himmelfarb G, ed. Harmondsworth: Penguin; 1974.\nFeinberg J. Legal paternalism. Can J Philosoph. 1971:105-124.\nFeinberg J. Freedom and behavior control. In: Reich WT, ed, Encyclopedia of Bioethics v 1. New York: The Free Press; 1978:93-101.\nVanDeVeer D. Paternalistic Intervention: The Moral Bounds of Benevolence. Princeton: Princeton University Press; 1986.\n- Kopelman L. On the evaluative nature of competency and capacity Judgments. Intl J Law Psychiatr. 1990;13(4):309-329.\nBeauchamp TL. Paternalism. In: Post SG, ed. Encyclopedia of Bioethics. 3rd ed. MacMillan Library Reference; November 2003:1983-1989."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:c26b6de2-29dc-4d18-af7e-b8a095466f62>","<urn:uuid:00c4e4ca-ea98-4d56-931b-9084229df81e>"],"error":null}
{"question":"How do vitamin D supplementation and physical exercise differ in their effects on memory and cognitive function in older adults?","answer":"Vitamin D supplementation and physical exercise affect memory and cognition differently. High-dose vitamin D (4000 IU/d) specifically improves nonverbal (visual) memory after 18 weeks of supplementation, particularly in individuals with insufficient baseline levels (<75 nmol/L), while verbal memory and other cognitive domains show no significant improvement. In contrast, physical exercise shows broader cognitive benefits, particularly improving tasks involving information processing and executive control, such as attention, dual-task performance, task-switching, decision-making, and response inhibition. Exercise also has additional brain-related benefits, including preservation of brain volume, especially in the hippocampus, and enhanced patterns of neural activation during executive control and semantic memory tasks. Exercise induces neurogenesis and angiogenesis in the brain, particularly in the hippocampus, which may contribute to these cognitive benefits.","context":["Experimental Gerontology. Vol 90, April 2017, pg 90-97, https://doi.org/10.1016/j.exger.2017.01.019\nJacqueline A Pettersen\n- Cognition of Alzheimer’s patients improved by daily 4,000 IU of vitamin D – RCT Jan 2015\n- Reversal of cognitive decline with multitherapy (not monotherapy) – Sept 2014 Omega-3, etc.\nCognitive category starts with the following\nVery brief summary of Cognitive decline\nTreatment : Vitamin D intervention slows or stops progression\nPrevention : Many observational studies - perhaps Vitamin D prevents\nOmega-3 both prevents and treats cognition\nWonder the benefits if both Vitamin D AND Omega-3 were to be used\nsee also Alzheimers-Cognition - Overview\nOverview Parkinsons and Vitamin D\nSearch VitaminDWiki for dementia anywhere in text 1190 items Jan 2019\nOverview Schizophrenia and Vitamin D\nSearch VitaminDWiki for \"WHITE MATTER\" 53 items as of Jan 2017\n37 minute podcast Vitamin D and the brain Vitamin D Council Sept 2014\nIncludes discussion by Dr. Cannell and Dr. David Llewellyn\n• Novel trial of high versus low dose vitamin D3 on multiple cognitive domains\n• High dose vitamin D (4000 IU/d) improved nonverbal (visual) memory after 18 weeks.\n• 25(OH)D levels < 75 nmol/L at baseline may confer more benefit with supplementation.\nInsufficiency of 25-hydroxyvitamin D [25(OH)D] has been associated with dementia and cognitive decline. However, the effects of vitamin D supplementation on cognition are unclear. It was hypothesized that high dose vitamin D3 supplementation would result in enhanced cognitive functioning, particularly among adults whose 25(OH)D levels were insufficient (< 75 nmol/L) at baseline.\nHealthy adults (n = 82) from northern British Columbia, Canada (54° north latitude) with baseline 25(OH)D levels ≤ 100 nmol/L were randomized and blinded to High Dose (4000 IU/d) versus Low Dose (400 IU/d) vitamin D3 (cholecalciferol) for 18 weeks. Baseline and follow-up serum 25(OH)D and cognitive performance were assessed and the latter consisted of: Symbol Digit Modalities Test, verbal (phonemic) fluency, digit span, and the CANTAB® computerized battery. Results: There were no significant baseline differences between Low (n = 40) and High (n = 42) dose groups. Serum 25(OH)D increased significantly more in the High Dose (from 67.2 ± 20 to 130.6 ± 26 nmol/L) than the Low Dose group (60.5 ± 22 to 85.9 ± 16 nmol/L), p = 0.0001. Performance improved in the High Dose group on nonverbal (visual) memory, as assessed by the Pattern Recognition Memory task (PRM), from 84.1 ± 14.9 to 88.3 ± 13.2, p = 0.043 (d = 0.3) and Paired Associates Learning Task, (PAL) number of stages completed, from 4.86 ± 0.35 to 4.95 ± 0.22, p = 0.044 (d = 0.5), but not in the Low Dose Group. Mixed effects modeling controlling for age, education, sex and baseline performance revealed that the degree of improvement was comparatively greater in the High Dose Group for these tasks, approaching significance: PRM, p = 0.11 (d = 0.4), PAL, p = 0.058 (d = 0.4). Among those who had insufficient 25(OH)D (< 75 nmol/L) at baseline, the High Dose group (n = 23) improved significantly (p = 0.005, d = 0.7) and to a comparatively greater degree on the PRM (p = 0.025, d = 0.6).\nNonverbal (visual) memory seems to benefit from higher doses of vitamin D supplementation, particularly among those who are insufficient (< 75 nmol/L) at baseline, while verbal memory and other cognitive domains do not. These findings are consistent with recent cross-sectional and longitudinal studies, which have demonstrated significant positive associations between 25(OH)D levels and nonverbal, but not verbal, memory. While our findings require confirmation, they suggest that higher 25(OH)D is particularly important for higher level cognitive functioning, specifically nonverbal (visual) memory, which also utilizes executive functioning processes.\n|1325 visitors, last modified 20 Nov, 2017, URL:","In the fields of neuroscience and cognitive science, human cognition is broadly defined as a component of brain function that includes information processing, memory, attention, perception, language, and executive function related to decision making (DM) and the initiation or inhibition of behavior. In the context of sport and exercise psychology, researchers have been interested in the possible benefits of increased leisure-time physical activity (PA) and the performance of acute and chronic exercise on various aspects of cognitive function, from infants to older adults. The focus of this entry is human older adult cognitive function. The effects of exercise training and leisure-time PA on magnetic resonance imaging (MRI) derived measures of brain structure and function related to cognitive performance among healthy older adults and those who are at increased risk for cognitive decline and dementia are also discussed.\nAcute and Chronic Exercise in Healthy Adults\nThe performance of a single session of exercise results in improved cognitive performance in healthy younger adults. However, the type, duration, and intensity of the exercise are important, as well as the timing of the cognitive performance during the exercise and after the exercise has ended. In addition, some types of cognitive function improve more than others. During exercise, impairments in cognitive function can occur for complex cognitive tasks, especially during exercise that also may be more cognitively demanding, for example, running compared to stationary cycling. However, evidence for cognitive improvement during exercise may occur for simple tasks that involve rapid DM or require a fast reaction time (RT), and in tasks that are very well learned and, thus, can be performed without much thought or planning. After exercise has ended, improvements in cognitive performance, especially tasks that involve information processing, memory encoding, and memory retrieval, occur over the first 15 minutes and then dissipate thereafter. It is presumed that the heightened physiological arousal during the recovery period contributes to these effects. It is unknown if these effects of acute exercise also occur in healthy older adults or those with cognitive impairments.\nExercise, Physical Activity, and Older Adult Cognition\nAmong healthy older adults, greater levels of cardiorespiratory fitness and periods of exercise training (other than acute bouts) are associated with better cognitive function. These effects are largest for cognitive tasks that involve information processing and executive control, such as attention and performance during a dual task, efficient switching between different types of tasks, DM, and response inhibition.\nThere is some evidence that beneficial effects of exercise on cognitive function are stronger in those who are at genetic risk for Alzheimer’s disease, the most common cause of dementia. Apolipoprotein E (APOE) allele status is related to risk for Alzheimer’s disease (as well as cardiovascular disease) through its handling of cholesterol. Possession of one or two copies of the apolipoprotein-epsilon4 (APOE-e4) allele increases the risk of future cognitive decline up to 10 times compared to noncarriers of the APOE-e4 allele. However, engaging in moderate levels of leisure time PA substantially reduces the risk of future cognitive decline in APOE-e4 allele carriers, equal to the risk for noncarriers (the other variants being the most common e3 allele, and the protective and less common e2 allele).\nVery little information exists regarding whether these beneficial effects extend to those with existing cognitive impairment. Individuals diagnosed with a very early stage of Alzheimer’s disease, termed mild cognitive impairment, may benefit from exercise training in their ability to perform a semantic fluency task on a day they did not exercise (e.g., naming as many animals or fruits as possible in 30 seconds) but not in tasks that involve episodic memory (e.g., learning a list of words and then being able to later, after doing other tasks, recall that list without any reminders). However, these effects have not been replicated or shown in large samples.\nMagnetic Resonance Imaging to Measure Brain Function and Brain Structure\nMRI is a tool that can be used in research to assess brain function and brain structure. There are multiple modalities of MRI that can be used (termed multimodal MR imaging) to assess differences between groups of individuals or the effects of interventions, such as exercise, on brain function. Functional magnetic resonance imaging (fMRI) is the most commonly used modality and depends on the blood-oxygen-level-dependent (BOLD) signal—derived from differences in oxygenated and deoxygenated hemoglobin presumed to reflect oxidative metabolism in nerve cells. The BOLD signal is an indirect, but validated, estimate of relative neuronal activation. The absolute rate of cerebral blood flow can be measured using a MRI technique called arterial spin labeling, and using radioactive contrast agents (e.g., gadolinium) cerebral blood volume can be estimated. Structural information about the brain, such as the volume of gray matter, white matter, and cerebrospinal fluid compartments of the cerebrum, can also be obtained using MRI. The structural integrity of brain white matter fiber tracts can be measured using a MRI technique called diffusion tensor imaging (also diffusion-weighted imaging), which assesses the diffusion characteristics of water molecules, which in a healthy person are constrained to diffuse along the boundaries of the intact myelinated white matter fiber bundle. Finally, MR spectroscopy can be used to measure the concentrations of certain metabolites or markers of neurotransmitter function in a single voxel (small three-dimensional cubes of brain tissue). The use of MRI in the context of sport and exercise psychology is appealing; however, caution is warranted as very little is known about the physiological effects of exercise on fundamental MRI signals that may occur independently from, but could appear as, alterations in neuronal firing or cerebral blood flow.\nEffects of Physical Activity and Exercise on Multimodal Magnetic Resonance Imaging Outcomes\nAfter robust growth in synaptic connections and brain volume during maturational development, the volume of the brain gradually decreases from roughly the age of 30 years until death. This decline in brain volume contributes to normal age-related cognitive decline, but brain atrophy is accelerated in neurodegenerative diseases such as Alzheimer’s disease, Parkinson’s disease, and Huntington’s disease. There is accumulating evidence that modifiable lifestyle behaviors, such as PA, may help to preserve brain tissue and promote a neural or cognitive reserve, and perhaps the maintenance of cognitive abilities, into old age.\nUsing MRI to examine the structural volume of the brain, several studies have demonstrated the benefits of PA and cardiorespiratory fitness as a method to preserve brain volume. The greatest effects of exercise on brain volume have been shown in the hippocampus, a medial temporal lobe brain region critical to all learning and memory processes; a region that is an early first target of Alzheimer’s disease neuropathology. People who self-report being more physically active, as well as people who possess greater cardiorespiratory fitness, tend to have greater brain volume in several additional brain regions, including the parietal, frontal, prefrontal, and subgenual cortices. Importantly, exercise training may result in an increase in the volume of the hippocampus in healthy older adults. This exercise training effect appears to be stronger in the anterior portion of the hippocampus, which is known to show more severe atrophy in Alzheimer’s disease. Among healthy, cognitively intact, older adults who are at increased genetic risk for Alzheimer’s disease, greater levels of self-reported PA reduces the risk of future cognitive decline and helps to preserve (but not increase) hippocampal volume over 18 months. However, it is not known if leisure time PA will lead to reduced rates of Alzheimer’s disease diagnosis. In patients previously diagnosed with early stage Alzheimer’s disease, greater cardiorespiratory fitness is associated with greater brain volume. However, it is currently unknown if exercise training will help preserve brain tissue volume over time in older adults diagnosed with Alzheimer’s disease, or if these effects translate into a slowing of disease progression.\nEvidence from fMRI experiments suggests that PA and cardiorespiratory fitness are associated with enhanced patterns of neural activation during executive control and semantic memory tasks. For example, in one study, healthy older adults completed a flanker task during the scan, which consisted of indicating the direction a central target arrow was pointed among flanking arrows that were congruent (>>>>>) or incongruent (>><>>) with the direction of the target. This task involves attention and visual information processing as well as inhibition of motor responses during the more difficult incongruent condition. Older adults who had greater cardiorespiratory fitness and others who had completed a 6-month walking exercise intervention (compared to the less fit and the stretching exercise controls groups, respectively) showed greater activation in areas involved in executive control, including the right middle frontal gyrus and superior parietal lobule, and lesser activation in the anterior cingulate cortex, a region activated in response to unexpected conflict and adaptations to attentional control processes. In another study, older adults completed a famous name discrimination task. In this task, the participant makes a right index finger button press to indicate the name is famous (e.g., Frank Sinatra) and a right middle finger button press to indicate the name is not famous (e.g., Rebecca Hall). Older adults, even those with cognitive impairment, perform the task very well with about 90% accuracy. Only correct trials are included in the analysis in order to remove activation related to errors in memory performance. In the analysis of the brain activation response, a “famous” minus “unfamiliar” metric is calculated in order to remove brain activation related to the common sensory and motor aspects of the two name conditions, thus providing a measure of activation related to semantic memory retrieval. Greater levels of self-reported PA were associated with a greater spatial extent and a greater intensity of neural activation in several brain regions involved in semantic memory. Furthermore, these effects were much greater in the more physically active participants who possessed a genetic risk for Alzheimer’s disease with the APOE-e4 allele. Larger effects of PA on brain amyloid levels, measured using positron emission tomography, have also been reported in APOE-e4 allele carriers. Accumulation of amyloid plaque in the brain is a hallmark feature of Alzheimer’s disease, and the early accumulation of brain amyloid is greater in APOE-e4 allele carriers, even prior to any symptoms of memory loss. Physically active APOE-e4 allele carriers showed substantially lower brain amyloid than those who were less physically active, levels that were similar to those who did not possess the genetic risk factor. Thus, exercise and PA may help preserve brain volume in regions involved in memory and executive function and may help preserve the ability to activate these regions to perform cognitive tasks. These effects are hypothesized to provide a cognitive or neural reserve that may provide protection against potential neural insults or neuropathological processes. Despite a genetic disposition to develop Alzheimer’s disease in APOE-e4 allele carriers, PA may promote cognitive resilience and the ability to maintain intact cognitive function and functional independence with age.\nExercise-Induced Angiogenesis and Neurogenesis\nThe possible neurophysiological mechanism(s) for the effects of PA on brain function have been well characterized using animal models. The most well-known finding is that exercise induces neurogenesis, the birth and growth of new nerve cells, in the hippocampus. Exercise in rodents produces increases in brain-derived neurotrophic factor (BDNF) and BDNF messenger RNA in the hippocampus and dentate gyrus. These exercise-induced neurotrophic effects in the hippocampus are hypothesized to contribute to the mnemonic benefits of exercise on memory. An important concomitant of neurogenesis is angiogenesis, the birth and growth of new blood vessels or capillaries, and exercise has been shown to induce angiogenesis in rodent motor cortex. The neurogenic and angiogenic effects of exercise are coupled and likely combine to affect cognitive function. In younger healthy adults, exercise training led increased cerebral blood volume in the dentate gyrus, an effect also observed in mice along with markers of hippocampal neurogenesis. Importantly, these effects are related to improved memory performance.\n- Colcombe, S. J., Kramer, A. F., Erickson, K. I., Scalf, P., McAuley, E., Cohen, N. J., et al. (2004). Cardiovascular fitness, cortical plasticity, and aging. Proceedings of the National Academy of Sciences USA, 101(9), 3316–3321.\n- Erickson, K. I., Voss, M. W., Prakash, R. S., Basak, C., Szabo, A., Chaddock, L., et al. (2011). Exercise training increases size of hippocampus and improves memory. Proceedings of the National Academy of Sciences USA, 108(7), 3017–3022.\n- Etnier, J. L., Nowell, P. M., Landers, D. M., & Sibley, B. A. (2006). A meta-regression to examine the relationship between aerobic fitness and cognitive performance. Brain Research Review, 52(1), 119–130.\n- Intlekofer, K. A., & Cotman, C. W. (2012, June 30). Exercise counteracts declining hippocampal function in aging and Alzheimer’s disease. Neurobiology of Disease. doi: 10.1016/j.nbd.2012.06.011\n- Smith, J. C., Nielson, K. A., Woodard, J. L., Seidenberg, M., Durgerian, S., Antuono, P., et al. (2011). Interactive effects of physical activity and APOEepsilon4 on BOLD semantic memory activation in healthy elders. Neuroimage, 54(1), 635–644.\n- Smith, J. C., Nielson, K. A., Woodard, J. L., Seidenberg, M., Verber, M. D., Durgerian, S., et al. (2011). Does physical activity influence semantic memory activation in amnestic mild cognitive impairment? Psychiatry Research, 193(1), 60–62.\n- Smith, J. C., Paulson, E. S., Cook, D. B., Verber, M. D., & Tian, Q. (2010). Detecting changes in human cerebral blood flow after acute exercise using arterial spin labeling: implications for fMRI. Journal of Neuroscience Methods, 191(2), 258–262.\n- Smith, P. J., Blumenthal, J. A., Hoffman, B. M., Cooper, H., Strauman, T. A., Welsh-Bohmer, K., et al. (2010). Aerobic exercise and neurocognitive performance: a meta-analytic review of randomized controlled trials. Psychosomatic Medicine, 72(3), 239–252."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:28e5d7f5-bd3b-4cff-b1cf-cac0e4c8b196>","<urn:uuid:20176c76-f431-4164-adb3-99dc6f0d6a23>"],"error":null}
{"question":"What are the key differences between an Insulation Energy Appraisal and a Home Performance Energy Audit in terms of their assessment scope and diagnostic tools used?","answer":"Insulation Energy Appraisals and Home Performance Energy Audits differ in several aspects. Insulation Energy Appraisals focus specifically on mechanical insulation systems, examining pipe and equipment sizes, insulation types, process temperatures, and energy costs. They utilize infrared cameras to capture heat loss and the 3E Plus program for calculations. In contrast, Home Performance Energy Audits take a broader 'whole-home' approach, examining multiple systems including the building envelope, HVAC equipment, insulation, and windows. They employ various diagnostic tools including infrared cameras, blower door tests for air leakage assessment, and duct work leakage testing, along with safety tests for HVAC equipment including carbon monoxide and combustion appliance back-draft testing.","context":["Insulation Energy Appraisals: The Economics of Insulating for Energy Savings\nAn Insulation Energy Appraisal provides a data-backed analysis for facility/plant managers and owners to better understand how a properly selected, specified, designed, installed, and maintained mechanical insulation system can save money and enhance system performance. Whether the appraisal includes a system, a mechanical room, or the entire plant or campus, it will clearly illustrate the value of mechanical insulation.\nFor more than a decade, the National Insulation Association’s (NIA’s) insulation appraisal process quantifies the amount of energy and dollars a facility or process is losing with the current in-place insulation system and demonstrates how more efficient systems could:\n• Save energy;\n• Improve process control and efficiency;\n• Reduce fuel costs; and\n• Contribute to a cleaner environment through reduced emissions.\nThrough visual inspection, interviews, calculations, and analysis, an appraiser thoroughly evaluates a facility’s insulated and uninsulated systems (or the specific scope that has been identified). The thermal performance of insulated piping and equipment will be compared to any uninsulated or under-insulated piping and equipment in the facility. Based on the analysis, the appraiser will document the Btus/dollars/emissions that users are saving/losing with the current system, as well as the potential savings and reduced emissions an insulation upgrade, replacement, or maintenance program could provide. Additional upgrade benefits include reducing the strain of equipment and extending its life, or in some cases eliminating the need to purchase additional equipment to maintain system efficiency or temperature. The customized final report will identify recommendations for insulation optimization and calculate the potential return on investment (ROI).\nBoth new construction and existing buildings can benefit from an energy appraisal for uninsulated and under-insulated systems. For example, in new construction, items such as flange valves, flanged pairs, valves, and condensate receivers, and pumps, are routinely left uninsulated. Andrew Martin, President of I-Star Energy Solutions, a subsidiary of the Irex Contracting Group, conducts insulation energy audits and building envelope assessments throughout the United States. Andrew notes that in newer buildings when you add up the square footage of the surface area of those uninsulated items, it adds up to a significant amount of energy loss.\n“We use infrared cameras on our assessments, and customers are shocked. . . when we show them infrared/digital images and calculations and show them what the actual loss is. With new construction, they assume that’s the way it’s supposed to be, there’s nothing else that can be done with it,” Andrew adds.\nWhen assessing older facilities or systems, I-Star sees the same things they see in new facilities, plus they see damaged insulation or places where the insulation has been removed for maintenance issues of some sort and never replaced. Andrew notes, “That’s where we find a lot more scope in the older buildings because of maintenance. For example, if a valve leaked, they would remove the wet insulation, but then never replace it.”\nWhat to Expect during the Assessment\nThe first step in the appraisal process is an interview and discussion with the facility personnel who know the facility well. This will determine the scope of the appraisal, the scope of a facility’s energy usage and energy distribution systems, and the cost to operate. The appraiser will also review the facility layout and facility drawings, if available, and determine the major sources of energy serving the facility.\nOften the facility walk-through takes place on a follow-up visit. During this step, the appraiser will measure and document all applicable pipes, ducts, and equipment, both insulated and uninsulated or damaged. The appraiser may also point out areas of concern with uninsulated areas or damaged insulation, such as personnel safety, regulatory compliance, corrosion under insulation, process control, or impact on adjacent equipment.\nWhen I-Star conducts an assessment, they ask for the process temperatures of the systems, the hours of operation of the system, type of fuel, fuel costs, and the efficiency of the system. “Once we have those 5 things, we can do our calculations and report back to them,” Andrew says. Regarding how long the on-site assessment takes, it depends on the size of the building. “If we are in a hospital or larger building with 12 stories and multiple mechanical rooms, it could be multiple days. If there’s only 1 building, it may only take a few hours. We don’t tie anybody up—we are on our own,” Andrew notes.\nDepending on the scope of the appraisal, the certified appraiser will look at:\n• Pipe and equipment sizes, locations, and geometries;\n• Types of installed insulation and jacketing or protective materials;\n• Condition of the current insulation systems;\n• Ambient temperature;\n• Process temperatures;\n• Wind velocity;\n• Design relative humidity values;\n• Annual hours of operation;\n• Scheduled downtimes;\n• Current thicknesses of insulation;\n• Energy sources;\n• Efficiency of each energy unit;\n• Type of energy used;\n• Cost of energy;\n• Process and instrument drawings (if available); and\n• Insulation specifications.\nCreating a Customized Report\nOnce the appraiser has completed the interview with facility staff and the facility walk-through, he or she analyzes the data using resources—such as the insulation calculators developed by NIA and the National Institute of Building Sciences as part of the Mechanical Insulation Design Guide (www.insulation.org/resources/system-designmidg)—and the 3E Plus® program, which was developed by the North American Insulation Manufacturers Association (NAIMA). Calculations include:\n• Heat gain or heat loss (actual fuel dollar loss);\n• Surface temperature requirements;\n• Condensation control;\n• Heat loss efficiencies versus bare pipe;\n• Payback period or Return on Investment (ROI); and\n• Emission reductions.\nFor I-Star, once they have gotten the customer information and completed their on-site assessment, they can generate the final report in approximately 10 business days.\nWhat Will the Final Report Cover?\nThe appraiser will present a customized report and explain all financial savings information and energy and environmental data. Reports may also include infrared images that visually capture heat loss.\nThe final report will provide detailed and well-documented analysis, including:\n• Fuel cost savings with your current insulation system, potential upgrades for the system, and potential savings with an insulation upgrade;\n• Environmental impact in terms of reduced CO2, NOx, and other greenhouse gases resulting from increased energy savings and reduced fuel consumption;\n• Energy (Btu) loss/gain from uninsulated surfaces in the facility;\n• Energy (Btu) loss/gain from insulated surfaces in the facility; and\n• Btu or energy loss/gain from a pipe or vessel if it is insulated to the most thermally efficient, yet cost effective, thickness determined by the 3E Plus program.\nThe appraiser may identify recommendations based on the analysis and discuss the potential ROI from an insulation upgrade or replacement or from implementation of a timely and effective mechanical insulation maintenance program. If requested, the appraiser can also provide a professional estimate regarding any insulation recommendations.\nPutting the Information into Action\nWhen asked what customers typically do upon receiving their report, Andrew sees several typical responses. “Some say, ‘Wow, that’s a lot of loss. We’ll try to budget for it in the future.’ Those who are ready to take action may say, ‘This is great. I can take it up the ladder. I have the proof I need to get the funds.’” Andrew says people generally know there is a problem, but don’t know the extent of it.\nWhen systems are properly insulated following an assessment, users will immediately notice a drop in their energy usage. Andrew added, another “thing everyone sees is a drop in temperature in the mechanical room itself just because when you’ve insulated all the steam systems or whatever it may be, the ambient temperature drops 10–15 degrees at times and that’s what the site personnel notice right on the spot.”\nAndrew affirms, “Main thing, even though you may not think there’s an opportunity, at least allow someone who’s in the business of mechanical insulation and energy assessments to take a look instead of just saying, ‘We don’t have any energy loss here.’ There’s energy loss in every building.”\nNIA currently has nearly 500 certified Insulation Energy Appraisers throughout the United States, and approximately 325 work for NIA member companies. Certified appraisers receive intensive training through NIA’s Insulation Energy Appraisal Program. They can provide an accurate appraisal of your current insulated systems and recommend the steps necessary to achieve the best energy and cost savings and emission reduction possible. To find a NIA Certified Insulation Energy Appraiser in your area, please visit http://tinyurl.com/jr4wzf8.\nThis article was published in the December 2016 issue of Insulation Outlook magazine. Copyright © 2016 National Insulation Association. All rights reserved. The contents of this website and Insulation Outlook magazine may not be reproduced in any means, in whole or in part, without the prior written permission of the publisher and NIA. Any unauthorized duplication is strictly prohibited and would violate NIA’s copyright and may violate other copyright agreements that NIA has with authors and partners. Contact firstname.lastname@example.org to reprint or reproduce this content.","- What is Home Performance\n- Find a Contractor\n- About Us\n- Take the Home Quiz\nHome Performance & Energy Audits\nWhat is home performance contracting?\nHome performance contracting is all about making existing homes more comfortable, safer, healthier, more durable, and more energy efficient. BPI Certified Professionals and BPI GoldStar Contractors use proven building science concepts to examine the whole home and how different systems within it interact. These systems include the building envelope (shell), heating, ventilation and air conditioning (HVAC) equipment, insulation, windows, the occupants themselves, and more.\nWhat services might be included in a home performance project?\nHome performance projects start with a comprehensive energy audit of the house to identify problems and trace them to their root cause. The technician may use an infrared camera to visualize temperature differences in different areas; conduct a blower door test to assess air leakage in the building envelope; and test for leakage in the duct work of your heating and cooling system. They may also perform other safety tests on HVAC equipment, including carbon monoxide and combustion appliance back-draft testing. Then the contractor prescribes and prioritizes improvements – from must-do to nice-to-have – that fit your budget. These repairs might include:\n- Air sealing the building envelope, including compartmentalizing the attic or basement from the conditioned living space\n- Weather-stripping doors and/or windows\n- Adding insulation\n- Sealing ductwork\n- Tuning up HVAC equipment\n- Upgrading to energy efficient HVAC equipment\n- Upgrading ENERGY STAR®-rated appliances and lighting\nCan an investment in home performance help make my house more comfortable and lower my energy bills?\nYes. In fact, that’s one of the primary reasons to get an energy audit. Drafts, uneven temperatures (including hard to heat or cool rooms), and extreme (too damp or too dry) or inconsistent humidity levels are all problems best solved with a whole-home approach that examines and improves the interaction between different systems in the house.\nAccording to the Department of Energy, the potential energy savings from reducing drafts in a home may range from 5% to 30% per year, and the home is generally much more comfortable afterward. To estimate how much money you could save by making home performance upgrades, take our home quiz.\nAre there any incentives available to help me pay for this?\nYes! Click here to find incentives that can cover or help with the cost of whole-home assessments and home performance improvement projects.\nBPI GoldStar Contractor\nWhat type of training do BPI Certified Professionals receive?\nTraining is optional and is administered by a coast-to-coast network of training organizations separate from BPI, including private companies, community colleges, and local not-for-profit agencies. Individuals hoping to get BPI certified learn the house-as-a-system approach that focuses on the relationships between different components within the home. They also learn how to identify problems at the root cause and prioritize and provide solutions that improve energy efficiency, health, comfort, and safety of homes. After they are certified, they are required to participate in ongoing continuing education requirements to stay on top of emerging issues, technologies, and best practices.\nWhat is the difference between a BPI Certified Professional and a BPI GoldStar Contractor?\nHow can I check the status of a company that represents their business as a BPI GoldStar Contractor?\nBeware of imitators! Only current BPI GoldStar Contractors with full credentials are listed on the BPI website. If a contractor claims that they are a BPI GoldStar Contractor, but they are not listed on BPI's Contractor Locator Tool, then they are not actually a BPI GoldStar Contractor.\nCommon Building Problems\nI’m worried about mold and indoor air quality. Can a home performance project help protect my family's health and safety?\nYes. One common problem that home performance retrofits correct is uncontrolled air leakage through the building envelope. Warm, moist, conditioned air passes from the living space through the walls on its way outside. When that warm air reaches the cooler temperatures within the wall cavity or inside a window frame, it drops the moisture on condensing surfaces, which can help contribute to mold. The American Lung Association® Healthy House® guidelines require homes to be more airtight to improve unplanned moisture movement. Although some stories in the media attribute indoor air quality problems to houses being built too tightly, the reality is that homes need proper mechanical ventilation to ensure clean air is coming into the home where it is supposed to.\nYour BPI Certified Professional or BPI GoldStar Contractor will locate where air leakage is occurring during the energy audit. Then, as part of the retrofit project, they will air seal and ensure your mechanical ventilation system is working as it should to draw clean air into your home.\nCan I tackle a home performance project on my own?\nThere are some aspects you can probably tackle alone, but whole-home energy audits are best performed by a qualified technician for two reasons:\n- Properly diagnosing home performance problems and prescribing solutions requires a building science specialist background that most homeowners do not have.\n- The job requires special (and often expensive) diagnostic tools, such as a pressurization blower door and infrared camera, that are not likely to be found in the average homeowner’s tool kit.\nHere's what you can do to prepare for an energy audit:\n- Make a note of potential home performance problems. Walk through your house and look for clues. Are there damp spots? Is dust collecting or is the carpet looking dirty near the baseboards? Is the snow melting in a weird pattern on the roof? Does the furnace make a funny noise or produce a strange smell when it’s running hard? Is one room particularly hot or cold? Which windows seem to carry the most condensation?\n- Make a list of any work that’s been done and when it was done.\n- Pull together all your energy bills for the last year."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:416a8a0b-a39a-4fe8-a707-72519d24dd71>","<urn:uuid:20870d9c-1b3e-4056-b968-ba48ed572f8f>"],"error":null}
{"question":"What are the psychological effects of workplace transitions, both in modern remote work and Papua New Guinea's post-independence period?","answer":"In remote work settings, employees struggle with psychological challenges including social isolation, difficulty disconnecting from work, and sleep problems, with 42% of remote workers experiencing sleep disruptions compared to 29% of office workers. Similarly, Papua New Guinea's post-independence literature reveals psychological struggles with transition, as writers grappled with changing identities, social changes, and conflicts between traditional and modern ways of life. Both contexts show how major workplace and societal transitions can impact mental wellbeing and create a need for coping mechanisms.","context":["|Provincial flags outside the National Library, Port Moresby.|\nWhat has Independence done to me as a Papua New Guinean, a writer, and scholar of indigenous cultures? Surely, this question must be asked by many conscientious Papua New Guineans.\nPapua New Guinea as a postcolonial nation struggles to free itself from a colonized history, more particularly from the neocolonial practices and influences of its former colonizer. Achieving political Independence has never freed Papua New Guinea completely from Australia.\nAustralian influence in Papua New Guinea is deeper than perceived at the political level. Australia continues to play a major part in the economic, social, and political development of Papua New Guinea. The relationship between Australia and Papua New Guinea is often tested, but always maintained through diplomatic dialogues and other political processess.\nEarly Papua New Guinea writings tackled Australian colonialism with ferment and nationalist fury to the extent of achieving Independence without bloodshed. The literature of that period is fueled by such political necessity.\nAfter Independence Papua New Guinean writers disappeared, except for a few committed ones, who continue to write. Two notable figures of the period are Russell Soaba and Paulias Matane, who continue to write literary and non literary works beyond the 2000s. Soaba continues to write and teach literature at the University of Papua New Guinea.\nPaulias Matane continues to write non fiction works after he moved away from the Aimbe fiction series. His interests in writing led him to publish many non fiction works throughout the years, even after becoming the then Governor General of Papua New Guinea. Grand Chief Sir Paulias Matane also assisted many Papua New Guineans to publish their books. He continues to impress all of us.\nThese gentlemen are, to many of us, the younger generation, embodiment of a legacy that refuses to go away. They used their writing to speak about their conditions before and after Independence. Reading their works helps us to understand our own lives.\nThe writings of the 1980s to the present are about this neocolonial presence, dependent relationship, as well as about the lack of critical reassessment of the changing experiences in postcolonial Papua New Guinea. Papua New Guinean writers are concerned with diverse issues of identity, social change, economic change, cultural change, the movement between village and urban areas, experiences of growing up, adolescence, education, unemployment, wantok systems, and conflicting cultural situations. Their writings are about the contemporary experiences, the traditional cultures and customs, and immediate past they seemed to have lost in the transition from a stone-age culture to one of electronic media networking. The mundane to important events in the lives of Papua New Guineans are concerns of contemporary writers. Literary expressions are inspired from the personal experiences of writers and anecdotes of other Papua New Guineans. This experience of Papua New Guineans is similar to those of other Pacific Islanders as already noted.\nLiterary culture developed in different phases in Papua New Guinea. The first phase characterizes dissent, protest, and anticolonial resistance. The period between 1968 and 1975 marks this phase. The second phase, between 1980 and 2000, covers the village pastoral and sociological literature. In this phase Papua New Guineans wrote out of the need to assess their conditions of living, of existence, to make sense of the world around them, and to revive the experiences of an earlier era.\nWriting struggled to survive against polarized national developmental priorities and civil conformity since Independence. The third phase is a combination of the previous phases and the independent emergence of new voices of a new generation. The third phase, between 1990 and 2000 came about as a result of Papua New Guineans reading the works of earlier writers and seeking out avenues to speak for themselves, about their experiences, and their visions for a democratic society. The later category makes use of new literary structures, both appropriated and experimental in style, to represent their experiences as Papua New Guineans.\nPapua New Guineans re-imagined themselves in their writings. They create various discourses about themselves. Papua New Guineans realize the process of rethinking and re-evaluation of some of the inherited values or those created by Papua New Guineans need urgent critical attention. The methods and procedures used for investigating and conducting research of Papua New Guinea cultures need to be reframed so as to produce a balanced critical reading of Papua New Guinea literature. Papua New Guineans need distinctive signposts to navigate through the many inroads created in their lives. All these are politically invested. The localized struggles and their responses to the globalized economy make them more vulnerable than is imagined. Accepting the passive, non active, unquestioning life is a form of conformity and cultural paralysis. Papua New Guineans can articulate their experiences in radical and progressive ways.\nPapua New Guinea is a hybridized postcolonial society with a fusion of diverse cultures, modern global influences, and the result of a synthesis of multicultural experiences. Questions of nationalism after Independence are raised every so often, suggesting that, perhaps it has served its purpose at the time of its emergence. Constant internal conflicts, uncontrolled social disorder, cultural conflicts, violence, ethnic differences, stagnant views, and rampant corruptions, poor governance, and massive squandering of royalties from its mineral and natural resources often stun the growth of nationalism. Nationalism, in most cases, is evoked by elites as the self-appointed guardians of their people’s interests. Nationalism is not what it claims to represent in Papua New Guinea as it fails to eliminate the ethnocentricism fueling regionalism within a national boundary.\nLiterature and politics have a unique relationship to each other. So long as literature continues to be useful to people it maintains its political function. It is difficult to resist viewing the political and ideological overtones present in Papua New Guinea writing. No writers are free of the social, political, economic, and cultural influences of their societies. Writers are creatures of their societies. Hence, a writer’s work carries with it the social and political value and responsibilities of his or her society.\nHappy celebration to all Independent Papua New Guineans.","Telstra and Westpac are the latest companies to encourage staff to work from home, just a few months after some of them returned to the office.\nWorking from home for extended periods can leave employees feeling socially and professionally isolated. When people work from home, they have fewer opportunities to interact and acquire information, which may explain why remote workers feel less confident than their office-based counterparts.\nResearchers also report working from home (WFH) is linked to negative physical health outcomes such as increased musculoskeletal pain and weight gain, as well as exhaustion.\nIf you are still working from home or your employer has just reinstated it, the good news is there are evidence-backed tips that can help overcome the challenges. Here are seven tips to navigating the coming weeks and months.\n1. Maintain your connections\nA chief complaint in surveys about working from home is social isolation. We miss connecting with our colleagues and friends.\nLoneliness has significant implications for our work, with research showing work loneliness can result in emotional withdrawal, which ultimately leads to deteriorating performance and wellbeing, as well as poorer health.\nNow lockdown restrictions have ended, maintaining connection is easier. Planning regular meet-ups with colleagues is an easy and effective way to overcome the social isolation felt working from home. Infection risks can be lessened by wearing respirators when you can’t socially distance. You should also stay home if you’re sick.\nSome companies are now also implementing walking meetings. As well as connecting with others, it’s an easy way to get some exercise as well as the stress-reducing benefits of nature. In one study, walking was shown to increase creativity by 81%.\n2. Tidy up regularly\nWhile a messy desk has helped win a Nobel prize and may be helpful for creativity, removing clutter is recommended for a lot of the other types of tasks we undertake in an average workday. A clutter-free desk may reduce the cognitive load on our brains, making us more productive.\nResearchers have found clutter influences employees’ thinking, emotions and behaviours. These factors affect decision-making, relationships, stress, eating choices and even sleep.\n3. Limit Zoom meetings and reduce “pings”\nAs technology platforms proliferate, so does the overload and distraction for our brains. After more than two years of WFH, the prospect of yet another Zoom meeting may well be uninspiring.\nThere are a few things we can do. Switch off notifications if possible, and ask whether each meeting really needs to happen. Using document sharing and email can sometimes replace meetings. A good old-fashioned telephone call may also be a good alternative. During a phone call, we only have to concentrate on one voice and can walk around, which can help thinking.\n4. Ask for feedback\nWondering how we are doing on the job undermines one of the key psychological drivers of our work, a sense of competence. It might be harder to gauge how your manager thinks you’re tracking with expectations, if you’re socially distant.\nObtaining feedback is vital for employees to develop this sense of competence, so make sure you ask for regular feedback.\n5. Create a WFH space\nResearch suggests replicating what you might have in the office can be a good way to control or mark out a work space at home. Having a proper desk does actually matter.\nWhile few of us will have something as incredible as a musical puzzle desk, we can start with a desk that is both functional and attractive.\nA flat surface, ergonomic chair, and suitable lighting can reduce problems such as eye strain, muscular pain or stiffness and back injuries, as well as decreasing fatigue.\n6. Identify restorative spaces\nSpaces that promote psychological and emotional detachment from work are also important. Restorative spaces, such as lounge areas, cafes, nature rooms and meditations spaces have begun to emerge in office settings in recent years.\nSuch spaces have been shown to support mental and physical replenishment.\nTaking a break on your favourite couch or in a sunny spot during the workday is an important part of maintaining wellbeing and productivity – not something to feel guilty about.\n7. Find ways to disconnect\nIt can be hard for employees who are working from home to switch off, particularly if we don’t have a dedicated home office space.\nAround half of employees increase their work hours when WFH. Not being able to switch off can have implications beyond the work day.\nA study from 15 countries found 42% of individuals who worked from home had trouble sleeping and woke up repeatedly in the night, compared to only 29% of individuals who always worked in the office.\nMany workers enjoy not having to commute to the office, but there is a potential downside to losing the “transition time” involved in travelling from home. We might use this time to separate private issues from work ones, to prepare for the day ahead or process the one just passed.\nIn addition to practical considerations such as shutting down software and finalising tasks, research shows using defined end-of-day rituals can help achieve psychological detachment, emotional regulation of the nervous system and reduce physiological stress.\nInstead of commuting, meditation, journaling, listening to music, engaging in hobbies or pleasurable activities, or undertaking exercise can give us a mental break, so we aren’t still thinking about work hours later.\nMore than two years into a forced global experiment, we now know a lot more about the benefits and challenges of working from home. Implementing these simple, evidence-backed strategies can make a big difference to our wellbeing.\nRepublished with permission of World Economic Forum. Read the original article."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:5d3ea6fa-38db-4c70-bb41-ccc5bcd763f4>","<urn:uuid:867500f1-ea8a-45e8-a9b4-de6b87f4cec7>"],"error":null}
{"question":"I'm interested in small-scale food production. How do the initial testing processes compare between making chocolate in Georgian times and modern product development?","answer":"In Georgian times, chocolate testing was done by roasting beans at around 140 degrees centigrade and grinding them on a warmed metate stone at about 80 degrees, using manual tools and basic equipment like iron pans. In modern product development, like at Heinz Wattie's, testing is done in a kitchen laboratory using both ordinary utensils and specialized equipment like mini 'Silverson' mixers and kitchen whizzes to imitate industrial machinery. Both processes involve careful temperature control and manual processing, but modern development includes additional scientific testing like pH, brix, and salt levels.","context":["How a new product is developed in the laboratory\nHow does the Product Development team start?\nSometimes the new recipe can be based on something Heinz Wattie's already do. A strawberry and apricot jam might use the same basic recipe as strawberry jam with the addition of apricots. Sometimes it is completely new, such as a creamy bacon sauce in a pouch.\nFor something new, the Product Development team may use recipe books, try competitor's products and look at their ingredient list, or sometimes they may simply make a new recipe up. Packaging Development are consulted for the most suitable type of can, pouch, plastic bag size, PCU (portion control unit which is a single serve unit, such as the butter that you get on an airline) material or plastic/glass bottle for the product (this is discussed when determining the feasibility of the product, but confirmed once development starts). The Thermal Processing Department are consulted to determine the critical factors affecting the cooking of the product and to ensure the product will be safe (that is, does not contain any microbes that cause sickness).\nHow do they make it in the kitchen?\nSmall samples are made in the Product Development Laboratory, in a kitchen using ordinary utensils to mix and stir. Some ingredients, such as spices, sugar, salt, vegetables, are the same as you might use at home. Other ingredients are specially modified or refined for use in processed food, such as modified starch, which can withstand high temperatures and high acidity, where ordinary flour or cornflour cannot. Sometimes flavours are added to improve the flavour of the product. In the case of pet food, vitamins and minerals must be added and calculations carried out to ensure that each recipe is a complete and balanced feed, because for some animals, such as guide dogs in training, our products are their sole source of nutrition. All ingredients are carefully weighed and recorded. The small batches are made to a specific volume or weight.\nThe Product Development team tries to imitate the factory as closely as possible throughout the development of the product by stirring with spoons, mixing dry ingredients with water with a mini 'Silverson' (the brand name of an industrial machine manufacturer – in this case this refers to a mixing machine) and imitating a Liquiverter (an industrial blender) with a kitchen whizz. Sometimes even using a whisk to imitate the effect the pump would have on a product. If homogenising, steam injection heating, or larger kettles (23, 40, 100 or 250 litres) are required, the Heinz Wattie'spilot plant has small-scale versions of the factory. The same tests that are performed in the factory are performed in the lab – consistency or thickness, pH or acidity, brix (dissolved solids), salt levels.\nPilot plant cooking\nHow is the packaging done?\nOnce the small batch is prepared, the product is filled into the correct packaging – a can, bottle, polythene bag, pouch or PCU. The Heinz Wattie's Product Development team has: a machine that puts the lids (caps) on the cans; a heat sealer for pouches and plastic bags; an iron for induction seals on bottles and for heat sealing PCU foil – yes, that is an ordinary old household iron.\nAnd the processing?\nProducts that only require cooling are cooled in a bucket or sink of cold running water. Products that need a retort or cooker process are processed in a pilot plant retort. The pilot plant retort is a mini version of a factory retort that can do steam or hot water processes and also has an attachment to imitate a continuous cooker. The retort processes are monitored to ensure the product is sufficiently sterilised just like the factory.\nWhat happens next?\nVery rarely is the first sample good enough to proceed with to the next stage. Ingredients may be added or taken out, quantities of ingredients may be changed, and the process may be made longer or shorter (still ensuring a safe thermal process).\nMany samples are sent to Marketing or the customer to get stakeholder feedback and make improvements. The number of samples that go back and forth can vary enormously from less than five up to more than 50! The same steps are done each time to ensure the Product Development team are getting a good idea of how consistent the test results are and how reliable the thermal process is. This helps to build up enough results to set specifications for the factory.\nThroughout the development, the costing is updated to ensure Heinz Wattie'scan still make a profitable product.\nOnce the product is approved by Marketing or the customer, the Product Development team can move to the next stage.","Skip to 0 minutes and 11 secondsHello, my name is Christopher Hew, and I'm from the University of Reading. Nice to meet you. Oh, very nice to meet you. My name's Robert Hoare. I'm from the Historical Royal Kitchens team at Hampton Court Palace. How was chocolate made during the Georgian period? The process starts with taking the pods off the tree, and then fermenting them on basically the forest floor. And then drying them off. And then we have them in this form. And these have been processed abroad. They had to be fermented and dried and everything. And that takes quite a while. But we didn't know that. So we get the beans like this. And then we roast them.\nSkip to 0 minutes and 44 secondsAnd we roast them in front of fires, sometimes just with iron pans, sometimes with things like this. Great big cylinders that you put onto the spit. And once it's roasted, it gets to be like these. These are roasted beans. These are already roasted beans. Already roasted beans, which I did this morning. And we roast them-- not a terribly high temperature compared to coffee. I've done these at 140 degrees centigrade. Are they edible? They are. Would you care to try them? Yes, please. Thank you.\nSkip to 1 minute and 17 secondsIt's crunch-- it's chocolatey. Very chocolatey. But very dry. And what happens next is the amazing thing, because you crunch these up, and then you put them on the metate stone, which is warmed. But only warmed to about 80. It's heated below. EIther using a charcoal stove or in our case, you can have a tray with sort of charcoal embers in. And then you use this to crunch them. You crunch the beans. And after a while, they sort of liquefy. It's really strange. Suddenly the cocoa butter comes out. And they become liquid? Yeah, it becomes really fatty. Back home in Indonesia, we have a similar device as well. Oh, do you?\nSkip to 2 minutes and 0 secondsBut we use it to grind up herbs and chillies, usually. And it's much more rough in texture. And so it grinds the herbs very, very finely, as compared to probably this. So what you do is you get it until you think it's ready. You sort of scrape it off with your special scraper. And you scrape it onto bits of paper, like this. And when you scrape it onto bits of paper like this, this is now called a chocolate cake. Called cake because of their shape. Cake of soap. It was just an old thing for a round thing. And then some people say keep them for a month. Some people say don't keep them for a month.\nSkip to 2 minutes and 37 secondsIt's a sort of trade secret thing. And then, typical Georgian, you boil this up in-- over here, in England, milk. But not only milk. Chocolate wine, chocolate port-- Oh, chocolate port. Which is very nice, yes. You have to serve it up quickly. But because it's still got the fat in there, and it's still got a slight graininess, you serve it in one of these. This is a classic chocolate pot. And the reason it's got this, it's because you want to-- it's basically a whisk. built in. And so what you do is you give it a whisk up. And get it all nice and frothy, so when it is poured out, it's also sort of nice and homogenised.\nMaking chocolate the Georgian way\nThis Week we’ve focused on the role of chocolate in showcasing power, wealth and kingship during the Georgian era.\nIn this video, Robert Hoare, Food Historian and member of the Historic Kitchens Team at Hampton Court Palace explains the traditional, Georgian method for preparing a ‘cake’ of chocolate from raw chocolate nibs. These cakes were then taken and mixed with hot water, milk, or alcohol to make a delicious range of chocolate drinks.\nRobert is joined by Christopher Hew, a Food and Nutritional Science student from the University of Reading. Christopher is keen to find out how these ‘cakes’ of chocolate are prepared and to take a sample back to the labs at Reading, where he plans to analyse the aromas released from drinking chocolate prepared using different ingredients, as part of his final year research project.\nFigure 1: Georgian Chocolate cakes © Historic Royal Palaces\nIf you’d like to see the full process of making chocolate, watch this recreation, also made in the Chocolate Kitchen at Hampton Court Palace on YouTube.\n© University of Reading and Historic Royal Palaces."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:99387ace-2fbd-4c8b-99ac-f14c3533b9c0>","<urn:uuid:d623bcf8-d4fb-4be2-aa4a-cd32fb62c2e5>"],"error":null}
{"question":"List the key differences between fine triangle chip carving and spline joints in terms of their basic geometric elements.","answer":"Fine triangle chip carving uses right-angled triangles of 1/4 inch or less, created through three specific cuts: two push/plunge cuts at 65 degrees and one pull cut, all made with a chip knife. Spline joints, on the other hand, use thin strips (usually of hardwood or plywood) that fit into matching grooves on joining surfaces - these are straight, linear elements rather than triangular, and the grooves are typically cut using a table saw to create matching kerfs.","context":["By L. S. Irish\nChip carving has become for the woodworker an art form complete in itself. This technique uses triangular shaped cuts that create intricate and intriguing designs dancing across the surface of the wood. Patterns for chip carving are defined by a few basic elements, as the triangle or free form line, yet by varying the size and position of these elements new patterns are created.\nBecause the chip carving technique employs such a few shapes it would seem that the patterns created would be static, however this is far from true. The triangles can be long narrow shapes or very short and fat in appearance, even curves are included within the triangles outlines. Chip carving is extremely appropriate for ornate jewelry boxes, shelf supports, and bread boards that will decorate your kitchen.\nThis technique is basically created with the chip knife. I highly recommend Moor’s Chip Carving Knives. This set of three are specifically designed for chip carving work and will last you a life time.\nTwo Types of Chip Carving\nChip carving as an art takes two direction. First is the pattern that is created from triangular shapes or traditional chip carving. Free Form chip carving is made with fine cut lines that vary in their shape and direction. In the Section of mistakes you will see a recipe box that has a Free Form bird pattern on the lid accented with a Traditional chip pattern along the sides. Traditional chip carving can be broken down further into fine chip carving and large chip carving.\nFine Triangle Chip Carving\nWith fine chip carving most of the right angled triangles that you will be working on are about 1/4″ or less in size. So the first two basic cuts are not pull cuts but instead a push or plunge motion into the wood.\nThe knife is held at a 65 degree angle to the wood surface with the blade of the knife facing you. You will be making this first cut toward you. Let the tip of the knife rest at point A, then push or plunge the knife into the wood toward point B. This creates a cut into the chip that is deepest where the point enters and shallow at the intersection of the next angle of the chip. Back the knife out of the cut, do not pull it out in the direction of the line. This will create a fine cut past the end of the line where you do not want it.\nFlip the knife over in your hand, the blade will now be facing away form you. Turn your work so that it is opposite it’s original position. You are ready for the second cut which will be a push cut into the wood. Begin again at point A, holding the knife at 65 degrees. Again, push or plunge the knife into the wood along the chip line toward point C. Then bring it straight out from the cut. Just as with the first cut, the second one is deepest at point A and becomes shallow toward the next intersection.\nThe third cut is a pulled cut. Return the knife to a position in your hand where the blade faces you. Place the tip of the blade at Point B, 65 degrees. Now slowly push the knife into the wood, pulling it along the line. As you reach the center of the line begin to bring the knife out of the wood until just the tip makes the cut at Point C.","Take a comprehensive look at the most common wood joints.\nThe language of the joiner is filled with words that we know well from ordinary usage but here have new and distinct meanings: Lap, edge, butt, and finger joints are technical terms to woodworkers. Joinery jargon gets still more complicated when you add in some other kinds of joints, like mortise-and-tenon, tongue-and-groove, dovetail, dowel, dado, spline, and rabbet. Not to mention such combination joints as cross laps, dado rabbets, dovetail laps, and keyed miters.\nYet this is, to say the least, a rather incomplete list of wood joints. With the introduction of the biscuit or plate joiner, any number of these joints are strengthened or varied thanks to the presence of the little, football-shaped wafers.\nDon’t be intimidated by all these possibilities. Try thinking of them as an embarrassment of riches. Pretty soon you will find that it’s fun to figure out which will work best for a given project or a particular application.\nIf you are just making your first foray into the land of the joiners, you’d probably do best to start with a simple joint like a dado or a rabbet. (If you’ve ever made anything, you’ve almost certainly made a butt joint already.) A picture frame typically uses a miter joint, so perhaps you’ve done that, or would like to try.\nSo here they are, the basic kinds of wood joints, in something approaching simplest-to-hardest order.\nButt Joint. When you join two squared-off pieces of wood, you’ve made a butt joint, whether the workpieces are joined edge to edge, face to face, edge to face, or at a corner. A butt joint is the simplest to make, requiring little shaping beyond cuts made to trim the workpiece to size. As with all joints, however, the surfaces to be joined must fit together tightly; if they don’t, a block plane may be used to smooth the end grain. Glues, nails, screws, dowels, and other fasteners may be used to secure a butt joint.\nMiter Joint. As you know from the miter box and the miter gauge on your table saw, a miter cut is basically an angle cut (though if you consult your dictionary, you’ll get told something like, “A miter is an oblique surface shaped on a piece of wood or other material so as to butt against an oblique surface on another piece to be joined with it.”).\nTo put it another way, a miter joint is a butt joint that connects the angled ends of two pieces of stock. The classic example is a picture frame, with its four butt joints, one at each corner, with the ends of all the pieces cut at a forty-five-degree angle, typically in a miter box.\nThe miter joint has two signal advantages over a butt-corner joint: First, no end grain shows, making for a more regular and attractive joint; second, the surface for gluing is increased. Miter joints may also be fastened with nails, screws, dowels, or other mechanical fasteners.\nRabbet Joint. A rabbet (or rebate, as it is also known) is a lip or channel cut from the edge of a workpiece. A typical rabbet joint is one in which a second piece is joined to the first by setting its end grain into the rabbet. Rabbet joints are frequently used to recess cabinet backs into the sides, or to reduce the amount of end grain visible at a corner.\nThe rabbet joint is much stronger than a simple butt joint, and is easily made either with two table or radial-arm saw cuts (one into the face, the second into the edge or end grain) or with one pass through a saw equipped with a dado head. A router or any one of several traditional hand planes, including a plow plane, will also cut a rabbet. Glue and nails or screws are frequently used to fasten rabbet joints.\nDado Joint. When a channel or groove is cut in a piece away from the edge, it’s called a dado; when a second piece set snugly into it is joined to the first with nails, glue, or other fasteners, a dado or groove joint is formed. Some cabinetmakers differentiate between groove and dado joints, insisting that grooves are cut with the grain, dadoes across. Whatever you want to call them, grooves or dadoes are cut easily with a dado head on a radial arm or table saw.\nThe dado joint is perfect for setting bookshelves into uprights, and can be fastened with glue and other fasteners.\nLap Joint. A lap joint is formed when two pieces have recesses cut into them, one recess in the top surface of one piece, the second in the lower surface of the other. The waste material removed is usually half the thickness of the stock, so that when the shaped areas lap, the top and bottom of the joint arc flush.\nLap joints are used to join ends (half-laps) or mitered corners (miter hall-lap). Dovetail shaped laps are sometimes used to join the ends of pieces to the midsection of others (dovetail half-laps).\nLap joints can be cut with dado heads, as well as with standard circular sawblades on radial- arm or table saws. Gluing is usual, though other fasteners, including dowels or wooden pins, are also common with lap joints.\nSpline Joint. A spline is a thin strip, usually of wood, that fits snugly into grooves on surfaces to be joined. Miter, edge-to-edge butt, and other joints may incorporate splines. Once the surfaces to be joined have been cut to fit, a table saw can be used to cut matching kerfs.\nThe spline itself adds rigidity to the joint, and also increases the gluing area. As most splines are thin, they are usually made of hardwood or plywood.\nTongue-and-Groove Joint. Flooring, bead-board, and a variety of other milled, off-the-shelf stock are sold with ready-made tongues and grooves on opposite edges. The edges can also be shaped with table or radial-arm saws; in the past, matching hand planes did the job.\nFor finish work, nails are driven through the tongues of the boards, and the groove of the next piece is slid over them (“blind-nailing“). For rougher work, as with certain kinds of novelty siding and subroof or sheathing boards, the stock is face-nailed. Glue is used only infrequently, as one of the chief advantages of a tongue-and-groove joint is that it allows for expansion and contraction caused by changes in temperature and moisture content.\nMortise-and-Tenon Joint. The mortise is the hole or slot (or mouth) into which a projecting tenon (or tongue) is inserted. Most often, the mortise and tenon are both rectilinear in shape, but round tenons and matching mortises are to be found. The mortise-and-tenon joint is harder to shape than other, simpler joints (both pieces require considerable shaping), but the result is also a great deal stronger.\nFinger Joint. Also known as a drawer or box joint, this one is most often seen in drawer joinery. Interlocking rectangular “fingers” are cut into the end grain of drawer sides and ends.\nThough precise cutting of the fingers is essential, finger joints require only relatively simple ninety-degree cuts that can be made by hand or using a router, radial-arm, or table saw.\nFinger joints, like dovetail joints, are sometimes used as a decoration, adding a contrasting touch as well as strength to the joined pieces.\nDovetail Joint. Occasionally, there’s a bit of poetry even in the workshop. As early as the sixteenth century, this joint was identified by its resemblance to bird anatomy. A thesaurus of the period termed the joint “A swallowe tayle or dooue tayle in carpenters works, which is a fastning of two piece of timber or bourdes together that they can not away.”\nThe dovetail is one of the strongest of all wood joints. It’s also one of the most challenging to make, requiring careful layout and the investment of considerable cutting and fitting time. Its shape is a reversed wedge, cut into the end grain of one piece, that fits into a corresponding mortise on a second workpiece. Dovetails are traditionally used to join drawer sides and ends and, in the past, for many kinds of casework furniture.\nThe good news is that there are some jigs on the market (though they’re hardly inexpensive) that make layout and cutting dovetails a snap. The jig is generally used along with a router with a dovetail bit."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:eef2a21f-477d-4af0-85cd-a7f02c4cc3ce>","<urn:uuid:2bcc6260-0655-4fa9-a414-ac78bc1f3294>"],"error":null}
{"question":"For my materials analysis work, I'm wondering what advantage spark spectrometry in argon has over arc excitation in air?","answer":"Spark spectrometry in argon atmosphere prevents sample oxidation and provides better repeatability than arc excitation in air. This is particularly important when conducting analyses involving several thousand sparks, as it prevents fundamental changes in the sparking conditions during the analysis.","context":["A spark is a discontinuous type of discharge that occurs in several phases. Initially, the energy needed for the spark discharge is supplied by a reservoir capacitor of a given capacitance. The interval between two sparks is used to recharge the capacitor (1). Every spark then has to be ignited, for which a high-voltage pulse of several kilowatt is used (2). This creates an almost constant burning voltage between the electrode and the sample (3). The spark then runs for several hundred µs.\nStrictly speaking, both sparks and arcs play a role in spark spectrometry. A continuous arc can be created with relatively simple excitation generators in an air atmosphere. For quantitative analyses, though, spark generation in an argon atmosphere provides numerous advantages. For instance, spark spectrometry with argon prevents sample oxidation and thus any fundamental changes in the sparking conditions during an analysis involving several thousand sparks. Spark excitation therefore delivers a substantially higher degree of repeatability than arc excitation. The latter named method plays a role in mobile spectrometers used for material recognition.\nAs a rule, the sample material is conditioned prior to the analysis phase. During this pre-sparking phase, the sample material is homogenised, for which higher spark energies are typically used than for measuring phases (HEPS – High Energy Pre-Sparking).\nIn the early years, the excitation conditions of a spark discharge used to be characterised by three parameters, charge capacity C, inductivity L and resistance R, and the first excitation generators filled entire rooms. Apart from the analytical sparking distance, a wear-prone auxiliary sparking distance was also typically used. However, the ability of this kind of excitation generator to produce constant discharge parameters was limited. To ensure an acceptable degree of long-term stability of the analytical data, it was often necessary to perform control or corrective measures (recalibration) when using such excitation generators. On top of that, it proved to be difficult to create numerous different discharge forms, as are absolutely necessary when using multi-matrix devices. Nowadays, excitation generators can be produced using semiconductors and no longer require an auxiliary sparking distance. They are characterised by a high degree of repeatability, good long-term stability and no maintenance needs. “Digitalisation” has become something of an inflationary term in descriptions of spark generators.\nAn electrically generated spark under an argon atmosphere can be used to excite a large number of elements. Plasma temperatures of over 10,000 K are reached. The resulting spectra are called “spark spectra” and display numerous atomic and ionic lines. Spark excitation can even be used to analyse non-metals like N or O. The alternative arc excitation, which requires several Ampere of current, creates plasmas of lower temperatures and delivers little spectral background and only few ionic lines (“arc spectrum”).The field of spark spectrometry uses the advantages of this method by generating arc-like discharge forms for the detection of several elements.\nThe many advantages of spark spectra come at the expense of several negative boundary conditions that have an effect on the optical layout and the evaluation algorithms. In comparison to low-pressure discharges, spark spectrometers deliver relatively broadened emission lines with a half-width value of about 20 pm. As a result of the optically dense spark plasma, non-excited atoms can absorb light, which leads to a self-absorption effect that increases proportionately with the number atoms of the same element present in the plasma. As a consequence, spark spectrometric calibration functions hardly ever display linear relationships. Furthermore, calibrating spark spectrometers is a complex task due to unavoidable line overlaps and the corrective calculations that are necessary as a result. Hundreds of calibration standards often have to be measured for a single metallic matrix so that optimal calibration functions can be calculated for all element channels.\nOBLF GDS III Spark Generator\nThe electrical energy needed to create the spark in the spark stand is delivered by the so-called spark generator.When generating this excitation energy it is critical to ensure that the power functions provided are extremely reproducible and can be individually modulated to suit the various requirements of the analysis tasks – in the most extreme case for every analytical program and every exposure time. This knowledge having spawned an early wish to use digital technology to contain these problems, it then found fertile ground at OBLF: we already began producing our spark generators exclusively on the basis of semiconductor technology at the beginning of the 1980s – a time when our competitors still had to use an auxiliary sparking distance. As a result of this, our development work to produce a digital spark generator made rapid progress and enabled the successful integration of this technology into OBLF spectrometers at a very early stage of digitalisation. Generators of this kind are called GDS spark generators (Gated Digital Source) and have by now become a firm fixture of the analytical landscape. In fact, they have been state-of-the-art for some years now. From an analytical point of view, these digital spark generators constitute a new dimension because they can deliver the requisite special power function in practically limitless form. As a result, they no longer fit into the usual definition schema with parameters like capacitance, inductance, Ohmic resistance, spark succession frequency or increase of the pre-sparking energy (HEPS). Such parameters are irrelevant when using GDS spark generators because they do not contain any classic discharge circuits that would limit their controllability. Apart from guaranteeing superior flexibility, the digital technology of these spark generators also guarantees extremely reproducible results and naturally renders them maintenance-free.\nOBLF GDS Spark Generator – Technical Specifications:\n|Discharge characteristics:||digitally definable from medium voltage sparks to arc-type|\n|Control variables:||discharge characteristics and duration|\n|Technology:||gated current technology|\n|Spark succession frequency:||up to 1 kHz|\n|Control:||all parameters via optical link|\n|Parameter definition:||fully via spectrometer software|\nread more »"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"sensitive"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:3afb5ef2-3176-4d5d-943b-3e11fbe9f83f>"],"error":null}
{"question":"What are the key differences between a business analyst and a data analyst in terms of their primary job responsibilities?","answer":"Business analysts conduct studies and evaluations to help businesses operate more efficiently, focusing on analyzing company aspects and suggesting improvements. They often need strong critical thinking to draw meaningful conclusions from data to help businesses make choices. Data analysts, on the other hand, work more closely with tech and mathematics, focusing on quantitative analysis. They gather and clean data, apply statistical models, and develop visualizations. While both roles help businesses thrive through information analysis, data analysts tend to be more focused on technical tasks while business analysts lean toward business awareness and process improvement.","context":["Business Analyst vs. Data Analyst: Analyzing These Commonly Confused Careers\nIf you take the job titles by their face value, you could guess that “business analyst” means someone who analyzes businesses and “data analyst” means someone who analyzes data. But wouldn’t a data analyst still be part of a business? Wouldn’t a business analyst still use data?\nSo what differentiates these careers? What do these professionals do? If you’re wondering about the role of a business analyst versus a data analyst for how it might work into your own future—you’re looking in an optimistic direction.\n“It’s no secret that the amount of data in the world is growing exponentially, and this greatly applies to the data that companies work with,” says Artem Melnikov, senior business analyst at MightyCall. Melnikov says companies today understand that decisions should be made on various kinds of data to supplement a manager’s subjective point of view and experience.\n“This leads to decision-making known as data-driven and data-informed,” Melnikov explains. And in the hope that data-based decisions will lead to smarter, more efficient and more lucrative choices, companies across all industries are taking note.\nIt’s safe to say that both business analysts and data analysts will have a relationship with data into the foreseeable future. But learning a bit more about what these two job titles represent and how they compare can help you understand the way companies navigate the new data capabilities of today.\nIf you are interested in finding your career somewhere in this arena, more details can help you choose a path forward. We combined research with expert advice to flesh out these commonly confused careers.\nBusiness analyst vs. data analyst: Differentiating the job descriptions\nBusiness analysts conduct studies and evaluations, design systems and procedures, conduct work simplification and measurement studies to assist a business in operating more efficiently and effectively, according to the Department of Labor.1 Their goal is to analyze aspects of a company, often using data sets, to find vital information and suggest improvements, fix problems and move the company’s operations forward.\nAs you can probably imagine—that general goal can take on a myriad of tasks depending on the company. “I work for a creative agency so my role as a business analyst is extremely unique,” says Holland Martini, director of data strategy at Grey Group. “My data sets include typical metrics such as sales, spend and media activity, but get partnered with more eccentric data points such as fashion trends, cultural biases, food habits, etc. …”\nJeff Neal, business analyst for The Critter Depot, has unique tasks and challenges based on his company as well. “We ship live insects across the country, so I’m always churning through data, to determine the probability of ‘transit survival’.” Neal’s business analysis often involves finding patterns in what might cause insects to perish during transit, to come up with options on how to avoid or mitigate those scenarios.\nWhile your employer or client’s specific needs could be all across the charts, business analysts usually need to rely on lots of critical thinking to draw meaningful conclusions from the data they have and help the business make choices.\nMartini explains that the importance of business analytics is rising. “Even architects and fashion designers are no longer willing to make decisions based solely on gut reaction to trends,” Martini says. “Everyone wants a smart proof point that substantiates their idea, and this has drastically expanded the market.”\nData analysts tend to lean a little closer to tech and mathematics. “In the purest sense, data analysis is a quantitative discipline—hard numbers and statistics flow into conclusions,” says Micah Melling, director of data science at Spring Venture Group.\nMelling explains that data analysts must first define the purpose of the analysis they are performing, being mindful of the bigger picture. Then, they gather the data they believe relevant, which requires technical skill sets. “We are often tasked with cleaning the data and placing it into a usable format as well,” Melling says. “After this point, we can dive into applying statistical models and developing visualizations.”\n“At the end of the day, my role is clarity—a concise summary of key learnings backed by data,” Melling says. In this phase of the role, business analysis and data analysis basically cross over. Both career avenues are culling information to help their businesses thrive.\nWhat skills are needed to be a business analyst vs. data analyst?\nWe combed through job postings from the last year to show you the most in-demand skills for each of these job titles to help you compare and contrast their attributes.\nBusiness analyst skills:2\n- Business analysis\n- Business process\n- Project management\n- SQL (structured query language)\n- Communication skills\nData analyst skills:2\n- Data analysis\n- Data quality\n- Data management\nYou can see that top skills for data analysts in the last year included more systems and software familiarity, while top skills for business analysts lean toward business awareness. But our experts say these roles really do cross over frequently—even to the point of becoming indistinguishable for many professionals.\nAfter all, many business analysts will need to learn more than one or two technical skills to make their work happen. And data analysts often need to understand the businesses they serve and the purpose of the data they are finding. “Both the business analyst and data analyst roles are pretty similar—to crunch numbers and discover patterns and relationships,” Neal says. “The titles are often used interchangeably.”\nHow do you choose between becoming a business analyst versus a data analyst?\nIf you are looking at the possibility of a career as a business analyst or data analyst—you might be making decisions early on in your education that would better complement one or the other. The vast majority of both business analyst and data analyst job postings asked for applicants with a bachelor’s degree.2\nSo how can you choose which career to pursue? “While the roles often overlap, the advice I would give is to decide how creative you want to be with your insights,” Martini says. “A business analyst is responsible for taking data and making insightful and creative actions to drive business, which often mixes the arts and sciences.”\n“A strictly data analyst is responsible for finding unique ways to solve a problem,” Martini says. “Both are fun!”\nJason Morphett, technical director of Purple Toolz, points out that many business analysts will have roles centered on finance, and many data analysts will have programming-heavy jobs. “I would advise anyone trying to decide between business and data analytics to consider where their interests lie,” Morphett says. “In all cases, be one step ahead of the game and get familiar with data science.”\nIf you see yourself craving creative opportunity and out-of-the-box thinking, maybe business analysis will offer more flexibility with that. And if you crave the hard-and-fast parameters of finding information in the most unbiased way possible, Melling says data analysis can offer that.\n“At times, data analysts need to focus solely on the technical side and temporarily throw out interpretability. Intellectual honesty is paramount. This mindset allows you to dig into a challenging problem and find a solution free of constraints.”\nChoosing a side—for now\n“The appetite for data analysis know-how will only increase,” Melling says. “In particular, versatility of skills will be in demand. Being able to move from technical practitioner to storyteller represents a comparatively rare skills set.”\nAs data analysts supplement their skills with business acumen, and business analysts branch into deeper technical ability—these two titles might very well merge into one career. While the blurred lines between these job titles might be a little frustrating for those who like black-and-white answers, the good news is that there are multiple viable paths into these roles.\nFor instance, a Business Management program that incorporates data analysis fundamentals can give you a broader understanding of business operations. On the flip side, a Data Analytics program that dives deep into the technical skills used for advanced data analysis paired with business experience could be an effective combo as well.\nNo matter your initial leanings for how to get there, you’ll want to learn more about the role of a data analyst before making a decision. Our article “What Does a Data Analyst Do? Exploring the Day-to-Day of This Tech Career” can help shed some light.\n1Bureau of Labor Statistics, U.S. Department of Labor, Occupational Employment Statistics, [accessed July, 2019] www.bls.gov/oes/. Information represents national, averaged data for the occupations listed and includes workers at all levels of education and experience. Employment conditions in your area may vary.\n2Burning-Glass.com (analysis of 85,690 data analyst job postings and 192,308 business analyst job postings, Jul. 01, 2018 - Jun. 30, 2019).\nPython is a registered trademark of The Python Software Foundation.\nTableau is a registered trademark of Tableau Software."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:a44520f6-2a1e-437b-93db-ba71bfa98823>"],"error":null}
{"question":"What is the Goldfish method of continuous solvent extraction and how does it differ from the Soxhlet method?","answer":"The Goldfish method is a continuous solvent extraction technique that differs from the Soxhlet method in the design of its extraction chamber. In the Goldfish method, the solvent trickles through the sample rather than accumulating around it.","context":["Continuous Solvent Extraction The Goldfish method is similar to the Soxhlet method except that the extraction chamber is designed so that the solvent just trickles through the sample rather than building up around it. In: 'Advances in Lipid Methodology - Two' , pp. Chromatographic analysis of ether-linked glycerolipids, including platelet-activating factor and related cell mediators. Lipids are an extremely diverse group of compounds consisting of tri-, di- and monoacylglycercols, free fatty acids, phospholipids, sterols, caretonoids and vitamins A and D. In:: 'Contemporary Lipid Analysis, 2nd Symposium Proceedings', pp. Cast, Sheffield Academic Press 1998. Lipids also were used as molecules to probe the basic mechanisms of ion fragmentation following electron ionization.\nA Practical Approach' edited by R. In: Applications of Modern Mass Spectrometry in Plant Science Research Proc. Kramer 16 Investigation of Protein-Lipid Interactions by Vibrational Spectroscopy, E. A simple test to determine the ability of lipids to withstand cold temperatures without forming crystals, is to ascertain whether or not a sample goes cloudy when stored for 5 hours at 0oC. Rheology is the science concerned with the deformation and flow of matter. Accelerated Solvent Extraction The efficiency of solvent extraction can be increased by carrying it out at a higher temperature and pressure than are normally used.\n. There have been several approaches to deal with complex mixtures of molecular species. In: 'Advances in Lipid Methodology - One' edited by W. Solvent Extraction The fact that lipids are soluble in organic solvents, but insoluble in water, provides the food analyst with a convenient method of separating the lipid components in foods from water soluble components, such as proteins, carbohydrates and minerals. Another curious feature is that the phospholipids and glycerolipids are observed as alkali metal adducts Na + and K + as well as the protonated species.\nIn: Analyses of Fats, Oils and Lipoproteins, pp. Normally, oxidation can take a long time to occur, e. In practice, the efficiency of solvent extraction depends on the polarity of the lipids present compared to the polarity of the solvent. This minireview will be reprinted in the 2011 Minireview Compendium, which will be available in January, 2012. The absence of reference standards for the different variants of structure observed in a molecular species series, e.\nResults given are the mean ± S. Many of these compounds are suspected xenoestrogens. Confirmation of conjugated linoleic acid isomers by capillary gas chromatography-Fourier-transform infrared spectroscopy. Nevertheless, over-consumption of certain lipid components can be detrimental to our health, e. Composition, Structure and Function', pp.\nThe selection of appropriate methods of ionization for analysis of various substances is also considered. B Injection in open-tubular supercritical fluid chromatography is usually accomplished using a dynamic-flow-splitting or time-splitting technique. The analysis of these lipids has always been a challenge because of the low quantities made within tissues or cells. Methods of Analyzing Lipid Oxidation in Foods 5. Perkins, American Oil Chemists' Society, Champaign, U. There have been several approaches to deal with complex mixtures of molecular species.\nChristie, Oily Press, Dundee 1996. Author by : Magdi M. Methods of determination in lipids. A sample is mixed with a combination of surfactants in a Babcock bottle. In: Analyses of Fats, Oils and Lipoproteins, pp.\nAnalysis of tocopherols by gas-liquid and high-performance liquid chromatography: a comparative study. Nevertheless, given the problems with accuracy, these measurements remain precise and useful when comparing changes of the same lipid molecular species within an experimental series. Rapid scanning tandem mass spectrometers are capable of quantitative analysis of hundreds of targeted lipids at high sensitivity in a single on-line chromatographic separation. The flask is heated and the solvent evaporates and moves up into the condenser where it is converted into a liquid that trickles into the extraction chamber containing the sample. Remarkable images are appearing as to the regional distribution of lipids in tissues such as brain and kidney as well as large biological structures such as embryos and entire organisms such a mouse.\nIt provides a broad overview of the applications of ionic liquids in various areas of analytical chemistry, including separation science, spectroscopy, mass spectrometry, and sensors. Advances in planar chromatography for the separation of food lipids. Perkins, American Oil Chemists' Society, Champaign, U. Microwaves: their potential applications in lipid chemistry. Size exclusion chromatography applied to the analysis of lipoproteins. The lipids are extracted from the food sample and then dissolved in an ethanol solution containing an indicator. This figure has been reprinted with permission from Elsevier."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:1708ec4d-9003-4973-bc44-89dbb825c03a>"],"error":null}
{"question":"What are the key differences between right-to-work laws and at-will employment in terms of their implications for workers?","answer":"Right-to-work laws and at-will employment are distinct concepts with different implications. Right-to-work laws focus on union membership, prohibiting agreements between employers and unions that require employees to join or pay union dues as a condition of employment - workers can choose whether to join a union. In contrast, at-will employment means employers or employees can terminate the employment relationship for any reason or no reason at all, with exceptions for violations of state or federal law. At-will employment is the standard in almost every state, while right-to-work laws vary by state and specifically impact union participation rather than termination rights.","context":["Casey Sipe | , , ,| By\nDiscovery? Deposition? Jurisdiction? If you don’t have a background in law it can be difficult to grasp these legal terms, but as HR professionals, it’s essential that we do so. Here is a short and sweet guide to the core parts of the legal process.\nThe Legal Process\nA plaintiff almost always begins a lawsuit with a Complaint. Every court will have its own specific rules, but for the most part Complaints are similar. They are broken down into number paragraphs, setting forth the allegations. Complaints will also enumerate the different “causes of action” or alleged wrongs that have occurred (i.e. breach of contract, negligence, fraud, etc.). In some circumstances, a lawsuit may be initiated with a Writ of Summons. For example, in Pennsylvania a Writ of Summons does nothing more than inform the defendant (or defendants) that the plaintiff intends to file a lawsuit against them and does not state the alleged wrongs committed. A Writ of Summons is most often used where the plaintiff seeks to beat the statute of limitations.\nAn Answer is the formal answer to a Complaint filed by the defendant (or defendants). Like a Complaint, it is broken up into numbered paragraphs, each corresponding to a numbered paragraph in the Complaint. Generally, it is not enough for the Answer to simply state “denied” to each allegation. Rather, the Answer must provide more specific denials for each allegation or a statement that the defendant reasonably investigated the allegation but does not possess the requisite information to admit or deny it. In addition to replying to the Complaint’s allegations, an answer will usually contain “new matter,” which sets forth certain defenses that the defendant possesses.\nIn some situations, when a plaintiff sues a defendant, the defendant finds that they have a cause of action against the plaintiff as well. Typically, the defendant will allege the counterclaim and its supporting facts in the Answer. The use of counterclaims allows for the more efficient administration of the courts, rather than encouraging a defendant to file a an additional lawsuit that will need to administered separately.\nIn some cases, a Complaint (or Answer) is faulty or violates the Rules of Civil Procedure (the rules governing how a civil lawsuit is conducted). In this case, the party receiving the Complaint (or Answer) files Preliminary Objections (also known as POs). POs are also set forth in numbered paragraphs and state each problem with the pleading in question. If a party does not raise a specific objection, they cannot bring it up later. It is truly use it or lose it.\nDiscovery is a part of the process that most non-lawyers find the most perplexing. It is an opportunity, guided by specific rules, for each party to attempt to find out information about the other party and their case. The scope of discovery is quite broad, and a party may inquire into information that it would not actually be permitted to use at trial. Discovery typically takes the form of interrogatories, requests for production of documents and depositions.\nInterrogatories are written questions that must be answered under oath. The party answering interrogatories must answer truthfully, but they are only required to provide the information that is requested, nothing more. As a result, crafting interrogatories in a manner so that they elicit the information required is a difficult task.\nRequest for Production of Documents\nRequests for production of documents are exactly what they sound like. One party asks the other party for documents. For example, in an employment discrimination case, the plaintiff may request all company emails that discuss the plaintiff.\nA deposition allows a party (or more likely their attorney) to ask questions, in person, of another party or some other witness in front of a court reporter. The court reporter takes down the testimony verbatim and creates a written transcript of the entire deposition. Often depositions are used for a dual purpose: gathering information about the other side’s case and creating a record to prevent the other party from varying their story. If variations occur, it can be used to undermine a witness’s credibility at trial.\nSummary judgment, which is requested by a motion and brief, requests the court to find in favor of one party before a trial takes place. The bar for granting summary judgment is very high. Essentially, the party is asking the judge to decide that, given the facts in evidence, no jury could possibly find against them. Summary judgment is difficult to win, and therefore, it isn’t requested often.\nStatute of Limitations\nEvery lawsuit must be brought within a certain period of time from the date the injury occurred (or was discovered). Every cause of action has a different limitation of time, generally between six months and 20 years, with most falling between one and four years. Due to the time limitations imposed by the statute of limitations, it is best to investigate a lawsuit as soon as you discover you or your business have been harmed in some way.\nJurisdiction is determines whether or not a court has the power to render a judgment and is divided into two different types: personal and subject-matter. A court has personal jurisdiction if the party has enough contact with the geographic area (i.e. county, state or district). For example, if you have an office in Pennsylvania, you most likely can’t be sued in Utah, if you don’t do business in Utah. A court has subject-matter jurisdiction if statute allows it to hear that type of case. For example, a bankruptcy court only has subject-matter jurisdiction to hear bankruptcy cases.\nHearsay is a report of another person’s statement, and as a result is of suspect accuracy and authenticity. Hearsay is generally not admissible in court, but there are numerous exceptions (which would put you to sleep if i discussed them at length). Many witnesses have a difficult time not saying “Joe told me…” which will result in the other party objection.\nRight to Work\nRight to work states (not to be confused with at-will employment) are those with statutes prohibiting agreements between employers and unions which require either membership or payment of union dues as a condition of employment. In right to work states, an employee may work for an employer and decide if he or she would like to join a union. In non-right to work states, that same employee may be required to join the union and pay union dues, regardless of whether they want to join the union or pay the dues, in order to be hired or retain their job.\nIn almost every state, employment is at-will. In short, it means the employer or employee may terminate the employment relationship for any reason or no reason at all. Of course, there are exceptions, most prominently, an employer may not terminate the employment relationship in violation of state or federal law.","- 1 What is the right to work law in simple terms?\n- 2 Can you be fired for no reason in Wisconsin?\n- 3 Does right to work mean I can be fired for any reason?\n- 4 Who Benefits From right to work laws?\n- 5 What states do not have the right to work law?\n- 6 Do I have the right to work?\n- 7 Is wrongful termination hard to prove?\n- 8 What are the 5 fair reasons for dismissal?\n- 9 What are the 3 exceptions to employment at will?\n- 10 What to do when your boss is trying to fire you?\n- 11 Can I sue my employer for firing me?\n- 12 Can I sue my employer if I’m fired for being sick?\n- 13 Does Right to Work hurt unions?\n- 14 What are the pros and cons of right to work laws?\n- 15 Is Right to Work Good or Bad?\nWhat is the right to work law in simple terms?\nA right-to-work law gives workers the freedom to choose whether or not to join a labor union in the workplace. This law also makes it optional for employees in unionized workplaces to pay for union dues or other membership fees required for union representation, whether they are in the union or not.\nCan you be fired for no reason in Wisconsin?\nWisconsin is an at- will employment state, which means that an employer can fire an employee unless the reason for the termination is unlawful.\nDoes right to work mean I can be fired for any reason?\nThe right-to-work doctrine, originally established in the National Labor Relations Act (NLRA) of 1935, gives employees the option to refrain from engaging in collective activity such as labor organizing and union representation. The employment relationship can be terminated for any reason or no reason at all.\nWho Benefits From right to work laws?\nRight-to-Work States Encourage Economic Growth Both companies and workers benefit from a better economy, as wages and corporate earnings increase. Studies have found that right-to-work laws increased manufacturing employment by approximately 30 percent.\nWhat states do not have the right to work law?\nThere are also some counties and municipalities located in states without right-to-work laws that have passed local laws to ban union security agreements.\n- New Hampshire.\n- New Mexico.\nDo I have the right to work?\nMore than half of U.S. states have enacted so-called ” right to work ” laws that guarantee no person can be compelled to join a union or pay union dues, as a condition of employment. Such laws exist at both the state and federal level. The Taft-Hartley Act is a federal right to work law that was enacted in 1947.\nIs wrongful termination hard to prove?\nUnless blatant, wrongful termination is difficult to prove and requires the employee to document as much as possible and seek effective legal representation from experienced attorneys.\nWhat are the 5 fair reasons for dismissal?\n5 Fair Reasons for Dismissal\n- Conduct/Misconduct. Minor issues of conduct/misconduct such as poor timekeeping can usually be handled by speaking informally to the employee.\n- Statutory illegality or breach of a statutory restriction.\n- Some Other Substantial Reason (SOSR)\nWhat are the 3 exceptions to employment at will?\nThe three major common law exceptions are public policy, implied contract, and implied covenant of good faith. The at- will presumption is strong, however, and it can be difficult for an employee to prove that his circumstances fall within one of the exceptions.\nWhat to do when your boss is trying to fire you?\nWhat should I do if my boss wants me to quit?\n- Recommit to performance. Employees should identify areas where they can improve immediately and display their commitment to the company’s objectives.\n- Don’t hold a grudge or gossip.\n- Rewrite the terms.\n- Improve your quality of life away from work.\nCan I sue my employer for firing me?\nYes, you can sue your employer if they wrongfully fired you. But you need to know if your employer actually broke the law, and you need to determine how strong your case is. All too often, people want to sue for being fired when the company had a legitimate reason to fire them. Not every firing is illegal.\nCan I sue my employer if I’m fired for being sick?\nEmployees have the right to be reinstated once their leave is over, with a few limited exceptions. So, if you were out sick for a serious health condition as defined by the FMLA, and your employer fired you because of it, you may have a legal claim for wrongful termination.\nDoes Right to Work hurt unions?\nWhile Right-to-Work states do not require all beneficiaries of union contracts to pay dues or be members, the union itself must represent all workers under that contract the same.\nWhat are the pros and cons of right to work laws?\nPro: Unions advocate for higher wages and better benefits. Pro: Political organizing is easier with union support. Con: Unions require dues and fees. Con: Unions may make it more difficult to promote and/or terminate workers.\nIs Right to Work Good or Bad?\nBy reducing workers’ voice, right-to-work laws drive down the wages and reduce the economic well -being of all workers—union and nonunion alike. At the same time, right-to-work laws make it easier for more of the economy’s gains to flow straight to the country’s wealthiest people."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:3336fd4f-2892-4618-9b9d-81df125114fd>","<urn:uuid:89dbeb03-e7f0-46eb-ad90-86c52ccd9f2f>"],"error":null}
{"question":"How do safety measures compare between the USA High School Clay Target League and the Daisy National BB Gun Championship Match?","answer":"Both programs prioritize safety but implement it differently. The USA High School Clay Target League has maintained a perfect safety record with no reported injuries since its inception in 2001 and requires school administration approval for participation. The Daisy National BB Gun Championship Match emphasizes safety through mandatory classroom gun-safety education before touching a gun, and competitors must take a safety test that accounts for 20% of their final score, with 19 competitors achieving perfect scores in the recent competition.","context":["What started off as simple mentor program at a local gun club in Plymouth, MN, back in 2001 has become something greater than anyone could have ever imagined. Youth clay target shooting has experienced a boom in recent years unlike any other shooting sport. Since its humble beginnings as a program for youth seeking guidance on how to shoot clay targets, the USA High School and Clay Target League has matured to become a premier conduit for new scattergun shooters that are eager to compete against their peers.\nThe Minnesota School Clay Target League, which would eventually snowball into the USA High School and College Clay Target League, was founded by Jim Sable at the Plymouth Gun Club in 2001 as an after-school youth mentoring program. With simple intentions to recruit younger members to the gun club, it progressively became something more. When a nearby school district was searching for mentors to assist a teenager girl in learning the art of trap shooting, things really began to heat up. What began with just one individual quickly turned into two, four and eventually an entire team.\nWith the popularity present, the League reached out to other area schools and helped them form teams so that competitions could be held. In 2008, once all required policies and procedures were met, the Minnesota State High School League was created in response to the growing interest of students in the sport. That first year there were only three teams and 30 students participating. By last year, there were a whopping 12,000 Minnesota teens competing on 349 clay target teams—now that’s a major increase.\nAfter the Minnesota State High School League took off, the USA High School Clay Target League was established in 2012 in order to expand to other states. Last year, the League introduced a college shooting program under the USA College Clay Target League banner. This new League will officially make its nationwide debut this September.\nThis clay target program has proven to be highly popular as one of the fastest-growing youth extracurricular activities in the U.S. with participation in 30 states. Today, the League boasts close to 30,000 student-athletes on over 900 teams expected to participate during the 2018-2019 school year.\nAnd not only can the League boast of such great popularity, but it can also pride itself on being the safest sport available in schools today, with no reported injuries since the League’s inception in 2001. Additionally, in order to participate each team must have the approval of their school’s administration. Thus, the League is the only 100 percent school-approved clay target shooting program in the United States.\nAvailable to boys, girls and also adaptive students in grades 6-12, the League allows participants the opportunity to compete on the same team and possibly make it to the State Championships for the ultimate recognition. The program therefore attracts student-athletes who have passed a comprehensive firearm safety education course to be a part of a “virtual” competition among high school teams throughout each state with minimal travel costs for families. Conferences are determined by team size rather than geographic location for fair competition and athletes earn scoring points as determined by their performance and ranking against all athlete score’s within their team’s conference. The team score and overall standing are tallied by adding the earned points from qualifying athletes which are subsequently posted on the League’s website. Student-athletes and their families can track their individual and team performances on their computer or via mobile devices.\nWith the League’s mission being safety, fun and marksmanship—in that order―the League always has safety as its number one priority. Everything the League does is in the best interest of the students, who reaping the benefits of the program and love every second of it. Undoubtedly, this program will continue to grow and more students will have the privilege of participating in one of the fastest growing sports for young people in the nation.","The DaisyNational BB Gun Championship Match (DNBBGCM) has become an important tradition among shooters: Anyone who becomes involved in competition BB gun shooting and attends the event is instantly hooked, as evidenced by the number of 20-, 25- and 30-year coaches who returned to lead the next generation of shooters. It's the premier 5-meter, 4-position BB Gun match in the world, but its primary focus is and always has been teaching safety, enjoying camaraderie and having family-friendly fun. The DNBBGCM is held annually at the John Q. Hammons Convention Center in Rogers, Arkansas, and brings together the best BB Gun shooters in the country aged 8 to 15. Teams must qualify for the event by coming in first, second or third in a state NRA-sanctioned match. Although everyone who attends the DNBBGCM is already a winner, at the end of the 2018 “Daisy Nationals” it was the Wyandotte County 4-H team out of Kansas City, Kansas, that took the medal podium to accept gold medals and the traveling first place trophy. In a nail-biting competition, Wyandotte County 4-H took the win by a single point over the Pierre Junior Shooters out of Pierre, South Dakota, the team that won the last two National Championships.\nErica Berger, shooting for the Gallatin Valley Sharpshooters from Bozeman, Montana, won the individual shooter National Championship, scoring just one-tenth of a point over Allee Smith (Pierre Junior Shooters). Third to first in the individual division was separated by just .4 points.\nMore than 50 teams from across the nation competed in this year’s National Championship. Teams are comprised of five shooters, and often, two alternates. Alternates shoot for their own National Championship, as do “Champions,” shooters who competed last year. If competitors shoot in this year’s event, they must return as Champions for a year before being allowed to shoot with their regular team the following year.\nThis year’s Match, held June 30-July 3, brought approximately 2,000 shooters, former champions, coaches and parents to Northwest Arkansas. Teams came from as far as Georgia, Oregon, Virginia, Wisconsin and south Texas. Any community or educational organization can form a team as long as it follows the curriculum outlined by Daisy, which features hours of classroom gun-safety and education work prior to ever touching a gun. In fact, before the match each competitor must take a test covering gun safety and match rules, and their score on that test makes up 20 percent of their final score. This year 19 competitors made a perfect score on the test.\nAs always, the 53rd annual event featured plenty of fun, action and emotion beyond the shooting competition. The Barter Bar, in which competitors bring items representing their states (which can include everything from homemade crafts and wildlife-related items to cans of “Potted Possum” meat and professional sports jerseys) and swap them amongst the crowd.\nOther events include a Parade of Champions at the opening ceremonies in which many team wears costumes or coordinated clothing or custom T-shirts of their own design. This year, Academy Sports and Outdoors sponsored these side contests and presented the winners with $500 gift cards.\nThe Match also includes a Painted Gun contest. Each competitor must shoot the Daisy 499B Champion BB Gun, the most-accurate 5-meter competition BB gun in the world, and may decorate it any way they choose. Many guns look as sharp as sports cars or follow a particular theme. First, second and third place winners were awarded Academy Sport and Outdoors gift cards in the amount of $300, $200 and $100 respectively.\nDaisy also inducts a longtime leader in the BB gun shooting sports into its Hall of Fame. This year, Jim Eberwein, who has been a coach in Kansas for more than 30 years, and is responsible for expanding and growing the program throughout the Sunflower State, was inducted into this exclusive club.\nA final highlight of the event was an appropriate send off for a Daisy employee who has planned, organized and administered the event for the past 15 years, Denise Johnson. Known by competition BB gun shooters and coaches as the “BB Gun Lady,” she’s become a well-known figure in the industry and is much-loved by the community. She retires after 39 years with Daisy.\nWhile the program instills responsibility, control and focus on the shooter (especially for youth who have focus or attention issues), and to reach the National Championship takes many hours of hard work, this final event pays off with four days that are just for the shooters. In addition to the cheers and tears of competition, the fun and camaraderie creates lifelong memories.\nFor more information on the event and instructions on how to create a team to compete in this exciting match, go to www.daisy.com."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:0b221392-be93-4b1b-a839-6ee77379fee5>","<urn:uuid:1bb5b958-128d-4269-8838-f3a7414146a5>"],"error":null}
{"question":"How do the quality control procedures compare between standard chemical water analysis and MPN testing when ensuring accurate results?","answer":"In chemical water analysis, quality control involves control acceptance criteria where recovery of parameters must fall within specific ranges. If results fall outside these ranges, they are considered suspect and while reportable, cannot demonstrate regulatory compliance. When methods lack specific QC acceptance criteria, control limits of +/- three standard deviations around the mean of at least five replicate measurements are applied. For MPN testing, quality control is built into its three-step verification process: the presumptive test checks for initial indication of coliforms through gas production and color change, the confirmed test verifies results using selective media to rule out false positives from non-coliform bacteria, and the completed test provides final verification through additional culturing and gram staining. Both methods require careful control of testing conditions, though MPN has a larger margin of error compared to chemical analysis.","context":["Sample preservation could partially resolve the next trouble. A common technique is preserving samples chilly to sluggish the rate of chemical reactions and stage improve, and analyzing the sample at the earliest opportunity; but this merely minimizes the improvements instead of stopping them.[nine]:43–forty five A beneficial process for determining impact of sample containers all through delay involving sample collection and analysis entails planning for 2 artificial samples beforehand with the sampling occasion. One sample container is full of water known from prior analysis to have no detectable number of the chemical of desire.\nThe composition in the nutrient typically incorporates reagents that resist The expansion of non-goal organisms and make the goal organism simply identified, frequently by a colour improve during the medium. Some recent methods consist of a fluorescent agent so that counting with the colonies is usually automatic. At the conclusion of the incubation period of time the colonies are counted by eye, a process that normally takes a few moments and would not require a microscope because the colonies are generally a handful of millimetres throughout.\nWater chemistry analyses are completed to determine and quantify the chemical components and Qualities of water samples. The type and sensitivity of the analysis depends on the objective of the analysis and also the predicted use of the water. Chemical water analysis is completed on water Employed in industrial processes, on squander-water stream, on rivers and stream, on rainfall and on The ocean. In all circumstances the outcome with the analysis delivers information and facts that can be used to make conclusions or to provide re-assurance that situations are as predicted. The analytical parameters picked are picked out being appropriate for the choice making procedure or to determine suitable normality.\nDetailed water quality answers For water high-quality monitoring, we provide multiple methods for that analysis of ingesting water, surface area water, groundwater, and wastewater to adjust to legislations and regulations.\nAbsorbance measurements are used to quantify the concentration of solutions and gases (as explained With this set up) that take in light-weight in the media that transmits gentle.\nIn some instances, analytical costs may possibly raise somewhat because of changes in solutions, but these improves are neither major, nor unique to small governments. This rule merely approves new and revised variations of testing treatments, and new sample assortment, preservation, and holding time necessities.\nAs soon as the lab has concluded testing your water, you may get a report that looks similar to Determine 1. It will eventually consist of an index of contaminants tested, the concentrations, and, in some instances, spotlight any trouble contaminants. A vital function with the report could be the units utilized to evaluate the contaminant amount with your water. Milligrams for every liter (mg/l) of water are employed for substances like metals and nitrates. A milligram for each liter can be equivalent to at least one portion for every million (ppm)--that's just one aspect contaminant to one million elements water.\nNo sizeable revisions ended up designed into the proposed MDL method. Some adaptability was extra to the course of action, as is reviewed in Section II.K previously mentioned.\nIn ingesting water materials the reason for unacceptable good quality can similarly be determined by cautiously focused chemical analysis of samples taken through the distribution process. In manufacturing, off-spec merchandise could possibly be right tied again to unanticipated modifications in moist processing phases and analytical chemistry can establish which levels could possibly be at fault and for what cause.\nWhen samples display elevated amounts get redirected here of indicator bacteria, even more analysis is frequently carried out to search for particular pathogenic germs. Species commonly investigated while in the temperate zone include Salmonella typhi and Salmonella Typhimurium.\nIntertek developed and formation water analytical screening companies include things like ten ion compositional analysis to determine inclination for scaling and corrosion.\nThe Home Water Analysis package is an extensive take a look at kit that enables you to swiftly and simply exam your ingesting water right-at-home for many contaminants and conditions. Risk-free water is essential to Anyone's well being. PurTest will help you know very well what's within your water and what difficulties should be solved. Checks Water Analysis bundled: 1 exam Each and every for germs, guide, & pesticide.\nOnce the recovery of any parameter falls exterior the standard Handle (QC) acceptance standards within the pertinent strategy, analytical outcomes for that parameter during the unspiked sample are suspect. The final results really should be reported but can not be utilized to demonstrate regulatory compliance. If the strategy isn't going to comprise QC acceptance criteria, control limits of +/− a few conventional deviations across the indicate of no less than five replicate measurements must be applied. These top quality Manage prerequisites also implement to the Normal Solutions, ASTM Solutions, and other methods cited.\nMDL samples certainly are a reference matrix, which include reagent water, spiked having a known and steady amount with the analyte.) Formerly, laboratories ended up identified to run all in their ready MDL samples on quite possibly the most delicate instrument, and afterwards use that MDL for other devices.","Last Updated on January 8, 2020 by Sagar Aryal\nWater Quality Analysis by Most Probable Number (MPN)\n- Most probable number (MPN) analysis is a statistical method based on the random dispersion of microorganisms per volume in a given sample.\n- In this method, measured volumes of water are added to a series of tube containing a liquid indicator growth medium.\n- The media receiving one or more indicator bacteria show growth and a characteristic color change. Color change is absent in those receiving only an inoculums of water without indicator bacteria.\n- From the number and distribution of positive and negative reactions, the MPN of indicator organisms in the sample may be estimated by reference to statistical tables.\n- MPN test is completed in three steps:\n- Presumptive test\n- Confirmed test\n- Completed test\nThis test, a specific enrichment procedure for coliform bacteria, is conducted in fermentation tubes filled with a selective growth medium (MacConkey lactose broth), which contain inverted Durham tubes for detection of fermentation gas. A series of lactose broth tubes are inoculated with measured amounts of the water sample to be tested. The series of tubes may consist of three or four groups of three, five or more tubes.\nThe main selective factors found in the medium are lactose, sometimes a surfactant such as Na-lauryl sulfate or Na-taurocholate (bile salt), and often a pH indicator dye for facilitating detection of acid production, such as bromcresol purple or brilliant green. The selective action of lactose occurs because many bacteria cannot ferment this sugar, whereas coliform bacteria and several other bacterial types can ferment it. The surfactant and dye do not inhibit coliform bacteria, whereas many other bacteria, such as the spore formers, are inhibited.\nThis test serves to confirm the presence of coliform bacteria when either a positive or doubtful presumptive test is obtained.\n- A loopful of growth from such a presumptive tube is transferred into a tube of brilliant green lactose bile (BGLB) 2% broth (or other lactose broth) and incubated at 35°C for 48 hours. This is a selective medium for detecting coliform bacteria in water, dairy, and other food products. A selective agent in the medium is lactose. The broth tube also contains a Durham tube to detect gas production.\n- A plate of LES Endo agar (or EMB agar) is streaked with a loopful of growth from a positive tube, and incubated at 35°C for 18–24 hours. Typical coliform bacteria (E. coli and Enterobacter aerogenes) exhibit good growth on this medium and form red to black colonies with with dark centers or a sheen. Salmonella typhi exhibits good growth but the colonies are colorless. S. aureus growth is inhibited altogether.\nThis test helps to further confirm doubtful and, if desired, positive confirmed test results. A typical coliform colony from an LES Endo agar plate is inoculated into a tube of brilliant green bile broth and on the surface of a nutrient agar slant. They are then incubated at 35°C for 24 hours. After 24 hours, the broth is checked for the production of gas, and a Gram stain is made from organisms on the nutrient agar slant. If the organism is a Gram-negative, nonspore-forming rod and produces gas in the lactose tube, then it is positive that coliforms are present in the water sample.\nObjectives of Most Probable Number (MPN)\n- To enumerate the number of bacteria present in the drinking water by MPN method.\n- To identify the bacteria present in the drinking water sample.\nProcedure of Most Probable Number (MPN)\nI. Presumptive Test\n- Prepare MacConkey purple media of single and double strength in test tubes with Durham’s tube and autoclave it.\n- Take three sets of test tubes containing five tubes in each set ;one set with 10 ml of double strength (DS) other two containing 10 ml of single strength (SS) .\n- Using sterile pipettes , transfer 10 ml of water to each of DS broth tubes . Transfer 1 ml of water sample to each of 5 tubes of one set of SS broth and transfer 0.1 ml water to five tubes of remaining last set of SS broth tubes.\n- Incubate the tubes at 37°C for 24 hours.\n- After incubation , observe the gas production in Durham’s tube and color change of the media.\n- Record the number of positive results from each set and compare with standard chart to give presumptive coliform count per 100 ml water sample.\nII. Confirmed Test\nSome microrganisms other than coliforms also produce acid and gas from lactose fermentation. In order to confirm the presence of coliform, confirmatory test is done. For this, a loopful of suspension from a positive tube is inoculated into 3 ml lactose-broth or brilliant green lactose fermentation tube and to an agar plate (EMB agar or Endo Agar) or slant.\nA. Inoculation of the lactose-broth\n- Incubate the inoculated lactose-broth fermentation tubes at 37°C and inspect gas formation after 24 ± 2 hours.\n- If no gas production is seen, further incubate up to maximum of 48 ±3 hours to check gas production.\nB. Inoculation in media slants\n- Take a loopful of suspension from a positive tube and inoculate on the agar surface.\n- The agar slants should be incubated at 37°C for 24± 2 hours.\n- Colonies must be examined macroscopically.\nIII. Completed Test\n- Transform a typical coliform colony from the agar plate into a tube of brilliant green bile broth with placed Durham’s tube and on the surface of a nutrient agar slant.\n- Incubate at 35°C for 24 hours.\n- After 24 hours, check the broth for the production of gas, and perform Gram staining for organisms on the nutrient agar slant.\nResults of Most Probable Number (MPN)\nA. Presumptive Test\nPositive: The formation of 10% gas or more in the Durham tube within 24 to 48 hours, together with turbidity in the growth medium and the colour change in the medium constitutes a positive presumptive test for coliform bacteria, and hence for the possibility of fecal pollution.\n- The test is presumptive only, because under these conditions several other types of bacteria can produce similar results.\nNegative: No growth or formation of gas in the Durham’s tube.\nB. Confirmed Test\nPositive: Formation of gas in lactose broth and the demonstration of a coliform-like colony on the EMB agar indicate the presence of a member of the coliform group in the sample examined.\nColiforms produce colonies with greenish metallic sheen which differentiates it from non-coliform colonies (show no sheen). Presence of typical colonies on high temperature (44.5 ±0.2) indicate presence of thermotolerant E.coli.\nNegative: The absence of gas formation in lactose broth or the failure to demonstrate coliform-like colonies on the EMB agar.\nPositive: The presence of gas in the brilliant green bile broth tube and Gram-negative, non-spore-forming rods on NA slant constitutes a positive completed test for the presence of coliform bacteria, which, in turn, infers possible contamination of the water sample with fecal matter.\nNegative: Absence of growth and gas formation in the broth. Absence of gram negative, non-sporing rods on Gram staining.\nUses of Most Probable Number (MPN)\n- It is commonly used in estimating microbial populations in soils, waters, agricultural products.\n- The technique is particularly useful with samples that contain particulate material that interferes with plate count enumeration methods.\n- It has also been suggested as a consideration for an alternate method to trend environmental monitoring studies.\n- It is also useful for counting bacteria that reluctantly form colonies on agar plates or membrane filters, but grow readily in liquid media.\nAdvantages of Most Probable Number (MPN)\n- Ease of interpretation, either by observation or gas emission\n- Sample toxins are diluted\n- Effective method of analyzing highly turbid samples such as sediments, sludge, mud, etc.\n- Permits samples that cannot be analysed by membrane filtration.\nLimitations of Most Probable Number (MPN)\n- Poor accuracy and precision associated with MPN counts usually means that the method is one of last resort — to be considered only when other counting methods are inappropriate.\n- Laborious and expensive in terms of materials, glassware and incubator space.\n- It has relatively a large margin of error.\n- Cappuccino J.G. and Sherman N. 2008. Microbiology: A Laboratory Manual, 8th ed. Pearson Benjamin Cummings, San Francisco, CA, USA"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:0c6440bf-afeb-4e78-89f2-3e9060ba3b5e>","<urn:uuid:0281b081-3e12-4e59-a6bc-8bf6420bfd35>"],"error":null}
{"question":"What are the practical benefits of laminate countertops, and what maintenance challenges do they face?","answer":"Laminate countertops are economical, versatile, and allow for easy cuts and modifications without special tools. They're inexpensive, come in various colors and patterns, have smooth surfaces that are easy to clean, and complement any home decor. However, they have notable maintenance challenges: they can be easily scratched or chipped and are impossible to repair. They also have low heat resistance and will wear out with heavy use.","context":["Plastic laminate countertops serve as an economical and versatile alternative to stone and solid surface countertops. Made by binding sheets of plastic laminate to a plywood substrate, these countertops also allow for easy cuts and modification, without the special tools and techniques required to cut granite and other tough materials.\nCutting New Countertops\nIf your new laminate countertop doesn't quite fit within your kitchen, or you want to repurpose an old countertop for a new application, you'll need a circular saw equipped with a fine-tooth blade. The finer the teeth on the blade, the more likely you'll be able to cut the laminate without chipping or cracking the surface. Look for blades designed specifically for cutting laminate, or choose the finest plywood-cutting blade you can find. Flip the section of countertop upside down, then mark the area you plan to cut using a pencil or a strip of tape. Guide the saw smoothly and evenly through the cut, and have a friend hold the cut section until you've finished sawing through the countertop. Allowing the cut section to simply fall away as you cut may lead to splintering or cracking of the laminate. Once you've made your cut, use a sheet of 220-grit sandpaper to smooth away sharp or rough edges.\nCutting Countertops in Place\nIt may be possible to cut countertops that are already in place to accommodate new appliances or cabinets. Use masking tape to mark the area you plan to cut, then choose your saw. Try a rotary tool equipped with a cutting wheel or a jigsaw equipped with a fine tooth blade, and cut slowly and carefully through the countertop. Make sure you have someone to support the section you're cutting off so it doesn't fall before you've finished cutting and crack the laminate.\nInstalling a Sink\nIf you want to install a sink in your countertop, you will need to cut a piece of plastic laminate to accommodate the vessel. Some sinks come with templates to guide your cuts, while others suggest using the sink itself as a template. Mark the area where you need to make your cuts using a marker or tape, then double-check all measurements. Drilling 1/2-inch holes in each corner of the marked area will help guide your cuts. Use a saber saw or jigsaw equipped with a fine-tooth blade to complete the project.\nCutting Plastic Laminate Sheets\nPlastic laminate can chip or peel over time, particularly along the edges of a countertop. Cutting away this piece of laminate countertop and replacing it with a new piece of laminate can restore the look of your kitchen. Using a straightedge as a guide, score the damaged piece of laminate using a utility knife. While a standard blade may do the job, choose a blade designed for cutting plastic laminate for best results. Once you've cut away the damaged section, choose a new sheet of laminate to match the existing countertop finish. Mark the section you need to cut, then cut it with the finished side facing up using a fine-tooth hacksaw.\n- Jupiterimages/Polka Dot/Getty Images","Cabinetry is an important aspect of kitchen planning. So if you are planning a complete kitchen overhaul or just a modest facelift, any slight changes in the counter, sink and faucet can make a difference in your kitchen. However, the greatest difference can come about with changes in your countertop, the workbench of the kitchen.\nThe choices of countertop material are vast. You also don’t have to stick to one type of countertop material for your kitchen. Here are some options for countertops, and a bit more information to help you make an educated choice when you are remodeling your kitchen.\n1. Wood Counters\nWood countertops, often made of hardwood such as maple or oak, come in a wide range of colours and finishes. One popular type of wood countertop is face-laminated wood countertop or butcher block. Other types are oiled wood planks and lacquered wood. Wood counters can give your kitchen a beautiful warm inviting look that is functional.\nHowever, scars and stains are always inevitable, and wood is susceptible to bacterial contamination. Fortunately, wooden surfaces can be cleaned quite easily, and worn surfaces can be renewed by sanding and resealing.\nWood countertops can also be customized. You should note that wooden countertops must be installed and maintained properly. Therefore, you will need a skilled carpenter.\n2. Concrete Counters\nConcret countertops offers versatility in any type of kitchen style (modern or traditional); the different finishes add to its timelessness. You can craze, grind, polish, stamp, and stain it. You can even embed object in it. You can create the desired feel from your finish. So, it does not have to be rough, gray or monotonous; instead, it can be smooth and polished, and it can look coloured sandy, or rocky.\nA great advantage of concrete is that it is durable, it would not dry-rot. Even though concrete is tough, it is not resistant to stains or scratches, especially if the concrete is unsealed. Concrete surfaces is discoloured or made slightly rough by acidic liquids, such as wine, lemon juice, and vinegar. Therefore, an epoxy sealer and routine maintenance can help keep your concrete countertop in pristine condition. Also, you will need to keep hot pots away from the concrete surface. For this reason, if you think you are not a fan of the wabi-sabi look, and you firmly believe a countertop needs to be kept immaculate and new-looking you may not like concrete countertops.\nConcret countertops have a rustic and simplistic appearance. It should be noted that concrete is heavy, and it is a good idea to survey the conditions of the floor and cabinetry that will support it.\n3. Ceramic Tile Counters\nA ceramic tile countertop can give your kitchen an elegant or rustic look. It is one of the most prefered countertop style, because its is durable, inexpensive, attractive, and easy to install. Ceramic tiles come in a variety of colours, textures, and sizes, which lends itself to all sorts of attractive accents. Some are hand-painted and can add to a distinctly personal appeal in your kitchen.\nCeramic tile surfaces are heat resistant, but it can be uneven and likely to be damaged or damage dishes or cookware. The tiles can easily chip or crack and the grout lines can stain if not sealed regularly. You can minimize stains in the grout if you use grout that contains a latex additive and apply a grout sealer after the grout cures and reapply once a year thereafter.\nYou should note that porcelain tiles are the least of all tiles likely to stain; it is also more expensive than the other types of tiles. Other type of tile countertops are made of marble and stone materials (such as slate and limestone).\n4. Laminate Counters\nAnother common countertop surface is laminate; it is often referred to as high-pressure decorative laminate or plastic laminate. Plastic laminate is quite durable, has a smooth surface, and it is easy to clean. It is inexpensive, and comes in a variety of colours, patterns, and finishes (gloss, stain, and textured). It complements any home decor.\nHowever, laminates can be easily scratched or chipped and are impossible to repair. Therefore, you should avoid laminates that are dark coloured, too textured, or deeply embossed with patterns. It also has a low resistance to heat. Plastic laminate is easy to install and inexpensive to maintain, but will wear out with heavy use.\n5. Solid Surface Counters\nYou will find solid surface countertops an attractive choice, because they can be custom-made to your specification, are quite durable and easy to clean, and scratches and burn marks can be sanded out and polished. Solid surface countertops are available in a limited selection of colour and patterns for your decor. They are also expensive to buy at first, but inexpensive to maintain.\n6. Stainless Steel Counters\nIf you are going for the contemporary kitchen look, you would like the stainless steel countertops. A stainless steel countertop is a good choice because is heat and stain resistant, durable, easy to clean, and can be custom build to your specifications. However stainless steel is expensive, and it can be scratched easily. There are other metal type countertops, such as galvanized sheet and copper sheets. Stainless steel is most suitable where food preparation is concerned.\n7. Granite Counters\nGranite counters are an expensive choice for the homeowner. It is beautiful and comes in a wide variety of colours. It is durable, functional, and easy to clean. It could last a lifetime. However, it can crack if improperly installed.\nThere are a lot more options in the market for kitchen countertops, those presented here are the most popular selections. Then, there are countertops which are made from recycled materials, such as reclaimed wood or recycled lumber, recycled glass (beer bottles, stemware, traffic lights, etc.), and paper. Recycled-material countertops are a fashionable choice for the environmentally conscious homeowner. However, their manufacturing require extensive labour and are therefore expensive. This option is quite new, so methods on repairing any damage done to the material may not be fully known yet. Another downside to recycled material countertops is that there may be undesirable components in the products.\nAll pictures courtesy www.photobucket.com"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:c5347a7b-39d8-4530-83f9-126a687bd3d0>","<urn:uuid:bfebb806-5887-4888-af12-b09e6b8f9a0a>"],"error":null}
{"question":"What was William Eggleston's reception at the Museum of Modern Art in New York in 1976?","answer":"When William Eggleston's color work was exhibited at the Museum of Modern Art in New York in 1976, many critics reacted negatively. Notably, Hilton Kramer of The New York Times dismissed the show as bland and boring.","context":["The first American edition of The Americans sold just over 1,000 copies, and the hail of criticism prompted Frank to temporarily abandon photography for film-making.\nIn 1976, when William Eggelston’s colour work was exhibited at the Museum of Modern Art in New York, many critics were appalled for different reasons. Hilton Kramer of The New York Times famously dismissed the show as bland and boring. In both instances, the work ran counter to the received consensus about what constituted serious photography.\nPhotography moves fast though, and often it takes a critical eye to recognise what a photographer – or group of photographers – is actually doing when they first show us the world in a different way. To this end, the writer and curator, John Szarkowsy, was instrumental in recognising Eggleston’s new way of looking at vernacular America, just as he was instrumental in defining a new harder-edged aesthetic in the work of Diane Arbus, Lee Friedlander and Gary Winogrand, when he staged the New Documents exhibition in New York in 1967.\nThe work of all of the aforementioned photographers, though, to one degree or another, adhered to Henri Cartier-Bresson’s definition that “to photograph is to hold one’s breath, when all faculties converge to capture fleeting reality”. In a world where photographs are increasingly made rather than taken, that definition no longer applies. (Neither, though, did it really apply to, say, Warhol when he was shooting his Polaroid portraits or to photographic-artists like Cindy Sherman or Gregory Crewdson.) We are now living in a world where photography is constantly conflated with the photographic and that this has ramifications beyond the purely semantic, not least in the question: what makes a good photograph?.\nConsider this year’s Deutsche Borse Prize, which rewards a “significant contribution to the medium of photography either through an exhibition or publication in Europe” throughout the previous year. Of the shortlisted artists – Mishka Henner, Cristina du Middel, Broomberg & Chanaran and Chris Killip – only the work of Killip adheres to Henri Cartier-Bresson’s suddenly old-fashioned description. Killip is, in a very real way, the token photographer in a photography prize that attempts to define contemporary photography. The rest are artists who make photographic work that, in a term beloved of the Deutsche Borse organisers, ‘interrogates” the medium.\nIn this context, the question “what makes a good photograph and how do we decide?” is an interesting one. We take for granted that we can apply it to Killip’s work using tried and tested criteria: composition, lighting, the capturing of a decisive or illuminating moment, the recording of a time and a place in an often striking or suggestive single image or linked series of images. How, though, does it apply to, say, one of Mishka Henner’s appropriated Google Street View images of sex workers? Is appropriation of this kind another already tired conceptual conceit? Were they good photographs before they were appropriated, blown up and put on the wall? Are they good photographs because they tell us something about how we live now in a constantly mediated world where surveillance is commonplace and the notion of privacy so contested that it seems somehow to have been redefined? Is the work good because the thought process behind it is as interesting as the actual images? Or, are we simply asking the wrong question here? (And, should we, indeed, be asking the same question of a Mishka Henner image that we ask of a Chris Killip photograph?)\nAs a writer on photography, I don’t think it is my role to answer these sort of questions – and the one big one raised here – but rather to raise them in a kind of ongoing discussion about photography, its uses and its meanings. One senses, too, that these questions are already academic. Artists do what they do and the rest of us try to keep up or make sense of what they do. What I can say is that it certainly looks to me like photography, (the taking of a photograph) is currently being superseded by the conceptually photographic (the ways in which photographs can be used: manipulated, re-appropriated, made into fictions, made to interrogate photography.) This is happening at a time when, as one cursory look at Flickr, Facebook ,Tumblr or Instagram will tell you, everyone is taking – and sharing – photographs all the time to a degree where the sheer weight of the numbers has long since become meaningless. (In January, Instagram reported that it had 90 million users and that 50 million images were being uploaded daily.)\nAs we struggle to make sense of the sheer scale of this global digital image bank, photography seems to be losing its meaning (even as it remains perhaps the only medium to keep up with this digitalised, mediated, overloaded world.) Or, is it that photography is losing some of it’s traditional meanings once again as new ones arrive to challenge and, in some instances, bemuse us. Whatever, I would like to think that, even as we are living in a moment of fiercely accelerating culture, a great photograph remains just that. It alerts us to something about ourselves, our lives, our world, our way of thinking.\nWho decides on – or confers – that greatness is another question entirely, and currently the curator, as the musician and thinker, Brian Eno, predicted back in the early 1990s is the key cultural figure, more important in many ways than the artist. It is, for instance, the curatorial thrust towards conceptualism that arguably drives contemporary photography more than anything else, and that is as much to do with the market as with aesthetic decisions. (Consider the recent vogue for post-Gursky large scale prints, whether or not the work suits the scale.)\nConversely, there has been the recent attendant growth in importance of the photo-book as an artwork in, and of, itself. It is how most people now absorb photography and often without ever thinking that they need to see an exhibition of the same work. Photography reinvents and reinvigorates itself continuously and in often surprising ways. It asks of us that we stay alert and ask new questions of it as a form. The big question – what makes a good photograph? – is always predicated on a number of smaller questions about what it is that we want from a photograph. And, what we want photography to do and say in a world overwhelmed by photographic images.\nPublished on 15 July 2013\nCommissioned by Photoworks"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:789ff6d9-0f87-4a99-9762-1b5d44d47996>"],"error":null}
{"question":"What's the difference between how Enterprise Service Bus and Apache Kafka handle message persistence?","answer":"In Enterprise Service Bus (ESB), messages are primarily handled for immediate routing and mediation between services, with error messages being logged or forwarded to error-tracking services as needed. In contrast, Apache Kafka retains messages on topics even after they've been read by consumers, with configurable message lifetime. Kafka's approach allows multiple consumers to read the same messages, and messages aren't deleted after consumption. This persistence model makes Kafka particularly suitable for event streaming and real-time data processing scenarios, while ESB focuses on immediate message routing and transformation.","context":["The main role of an Enterprise Service Bus (ESB) is to act as the backbone of an organization’s service-oriented architecture. It is the spine through which all the systems and applications within the enterprise (and external applications that integrate with the enterprise) communicate with each other. As such, an ESB often has to deal with many wire level protocols, messaging standards, and remote APIs. But applications and networks can be full of errors. Applications crash. Network routers and links get into states where they cannot pass messages through with the expected efficiency. These error conditions are very likely to cause a fault or trigger a runtime exception in the ESB.\nUsing fault sequences\nWSO2 ESB provides fault sequences for dealing with errors. A fault sequence is a collection of mediators just like any other sequence, and it can be associated with another sequence or a proxy service. When the sequence or the proxy service encounters an error during mediation or while forwarding a message, the message that triggered the error is delegated to the specified fault sequence. Using the available mediators it is possible to log the erroneous message, forward it to a special error-tracking service, and send a SOAP fault back to the client indicating the error or even send an email to the system admin.\nIt is not mandatory to associate each sequence and proxy service with a fault sequence. In situations where a fault sequence is not specified explicitly, a default fault sequence will be used to handle errors. shows how to specify a fault sequence with a regular mediation sequence.\nWhenever an error occurs in WSO2 ESB, the mediation engine attempts to provide as much information as possible on the error to the user by initializing the following properties on the erroneous message:\nWithin the fault sequence, you can access these property values using the\nget-property XPath function. Sample 4 uses the log mediator as follows to log the actual error message:\n<property name=\"text\" value=\"An unexpected error occured\"/>\n<property name=\"message\" expression=\"get-property('ERROR_MESSAGE')\"/>\nNote how the ERROR_MESSAGE property is being used to get the error message text. If you want to customize the error message that is sent back to the client, you can use the makefault mediator as demonstrated in Sample 5: Creating SOAP Fault Messages and Changing the Direction of a Message.\nThis section describes error codes and their meanings.\nTransport error codes\n|101000||Receiver input/output error sending|\n|101001||Receiver input/output error receiving|\n|101500||Sender input/output error sending|\n|101501||Sender input/output error receiving|\n|101504||Connection timed out (no input was detected on this connection over the maximum period of inactivity) |\n|101506||NHTTP protocol violation|\n|101508||Request to establish a new connection timed out|\n|101510||Response processing failed|\nIf the HTTP PassThrough transport is used, and a connection level error occurs, the error code is calculated using the following equation:\nThere is a state machine in the transport sender side, where the protocol state changes according to the phase of the message.\nFollowing are the possible protocol states and the description for each:\n|REQUEST_READY (0)||Connection is at the initial stage ready to send a request|\n|REQUEST_HEAD(1)||Sending the request headers through the connection|\n|REQUEST_BODY(2)||Sending the request body|\n|REQUEST_DONE(3)||Request is completely sent|\n|RESPONSE_HEAD(4)||The connection is reading the response headers|\n|RESPONSE_BODY(5)||The connection is reading the response body|\n|RESPONSE_DONE(6)||The response is completed|\n|CLOSING(7)||The connection is closing|\n|CLOSED(8)||The connection is closed|\nSince there are several possible protocol states in which a request can time out, you can calculate the error code accordingly using the values in the table above.\nFor example, in a scenario where you send a request and the request is completely sent to the backend, but a timeout happens before the response headers are received, the error code is calculated as follows:\nIn this scenario, the base error code is\nCONNECTION_TIMEOUT(101504) and the protocol state is\nError code = 101504 + 3 = 101507\nThis section describes the error codes for endpoint failures. For more information on handling endpoint errors, see Endpoint Error Handling.\n|303000||Load Balance endpoint is not ready to connect|\n|303000||Recipient List Endpoint is not ready|\n|303000||Failover endpoint is not ready to connect|\n|303001||Address Endpoint is not ready to connect|\n|303002||WSDL Address is not ready to connect|\nFailure on endpoint in the session\n|309001||Session aware load balance endpoint, No ready child endpoints|\n|309002||Session aware load balance endpoint, Invalid reference|\n|309003||Session aware load balance endpoint, Failed session|\n|303100||A failover occurred in a Load balance endpoint|\n|304100||A failover occurred in a Failover endpoint|\nReferring real endpoint is null\n|305100||Indirect endpoint not ready|\nCallout operation failed\n|401000||Callout operation failed (from the callout mediator)|\n|401001||Blocking call operation failed (from the Call mediator when you have enabled blocking in it).|\n|401002||Blocking sender operation failed (from the Call mediator when you have enabled blocking in it).|\nCustom error codes\nEndpoint Custom Error - This error is triggered when the endpoint is prefixed by\nFor information on best practices for handling errors in WSO2 ESB, see WSO2 ESB by Example - Best practices for error handling on the WSO2 ESB.","Introduction to Apache Kafka: Fundamentals and Working\nThis article was published as a part of the Data Science Blogathon.\nHave you ever wondered how Instagram recommends similar kinds of reels while you are scrolling through your feed or ad recommendations for similar products that you were browsing on Amazon? All these sites use some event streaming tool to monitor user activities. Kafka is one of the most popular events streaming platforms out there.\nBig companies like Netflix, LinkedIn, Uber, Walmart, and Airbnb use Kafka for a lot of things. Such as,\n- Netflix uses Kafka to provide tv show recommendations in real time.\n- Walmart uses Kafka for the real-time inventory management system.\n- For Uber’s technology stack, Kafka is considered the cornerstone. It uses Kafka for many things, such as computing the cab fare in real time depending on the demand, destination, and availability of cabs.\nApart from these, many other companies use Kafka for large-scale streaming analysis, log analysis, message brokerage services, etc. A real-time event-driven system is essential when every user activity is valuable and every second has a financial cost.\nSo, in this article, we will learn about the fundamentals of Apache Kafka and how it works. We will learn how to set up Kafka servers with Zookeeper and KRaft from the terminal and use the Kafka python client library to read and write events to topics.\nWhat is Apache Kafka?\nApache Kafka is a distributed, real-time streaming platform for large-scale data processing. Organizations use Kafka for real-time analytics and building event-driven architectures and streaming pipelines to process data streams.\nIn 2010, a group of engineers at LinkedIn started working on a tool to handle high amounts of data produced daily. In 2011, considering its usefulness, the technology was open-sourced. Jay Kreps, one of the co-creator, named it Kafka after the author Franz Kafka as he liked Kafka’s writings. Later on, in 2012, Kafka graduated from the Apache Incubator.\nHow does Kafka work?\nKafka has various components like Topics, Producers, Consumers, and Brokers that make Kafka a scalable, fault-tolerant and durable event-streaming platform. So, let us understand how each component works one by one.\nCommon Definition of Terms\nAn event is a real-world phenomenon. A Kafka event has an event key, values, time-stamp and optional metadata.\n- The event key is specific to an event. It can be a number or string value. For example, if we want to store the GPS data of cabs, we may assign the cab ID to the event key.\n- The event value field stores the data we want to write to the topics.\n- A timestamp is added to an event\nThe publish-subscribe messaging is a messaging pattern where the sender sends messages to a destination, and whoever subscribes to that destination receives messages. In this case, the central destination is called the topic.\nEvent streaming is an evolution of pub-sub messaging where messages are stored and transformed as they occur, in addition to typical pub-sub capabilities. An event stream is a continuous flow of events from sender to destination.\nKafka Topics and Partitons\nA Kafka topic is similar to a database table and is used to store data. You can create as many topics as you want, such as a “tweets-data” topic to hold tweets data. Topics are partitioned to allow data to be spread across multiple brokers for scalability. Partitions are log files that hold the actual data. A topic can have many partitions, which can also be configured programmatically.\nWhen a new event is published, it gets appended to one of the partitions. Events with the same event key are appended to the same partition. An event key could be a string or a number, for example, a user id. Kafka guarantees the order of events; consumers will read the events in the same order as they were written on that topic.\nEvery time a message is published on a Topic, an offset is added. An offset is an integer value that identifies a record or event inside a partition. Earlier records will have smaller offsets compared to later ones.\nsource: Kafka log\nApache Kafka Producers\nProducers are the applications that send data to Kafka topics. A producer can be any data source, such as Twitter, Reddit, Logs, GPS data etc. A producer client library can assist in ingesting the data from these sources to respective topics.\nThe records from Applications are written to the partitions. All the messages sharing the same event key are written to the same Kafka partition. If the events lack keys, the messages are written to topics in a round-robin fashion (p0->p1….->p0).\nApache Kafka Consumers\nKafka Consumers are the applications or systems that consume the data from the topics. Applications integrate a Kafka client library to read data from the topics.\nTo read data from Kafka, consumers first need to subscribe to a topic and then can read from single or multiple partitions. Consumer tracks its progress by using the offset of the last message processed.\nThe message order is guaranteed if the consumer consumes messages from a single partition. While reading from multiple partitions may not conserve ordering.\nUnlike traditional messaging platforms, events in topics are not deleted after being read. The lifetime of these events can be configured.\nConsumers identify themselves with a consumer group name, and each message published on a topic is delivered to one consumer instance within each consumer group that is subscribed to it.\nKafka guarantees ordering across multiple consumer instances by assigning partitions to consumers of the group., such that a single consumer in the group reads a single partition. This also ensures load balancing as there are multiple consumers.\nKafka Brokers, Topic Replication, and Controller\nKafka brokers are servers that store data. Kafka usually operates in clusters, which are made up of one or more brokers. Topics are partitioned and stored on multiple brokers. To ensure that Kafka is fault-tolerant, multiple copies of topic partitions are kept on different brokers. The default number of copies is 3, but this can be changed programmatically. So, in case one broker dies, data will persist on other brokers having the replicas.\nTo achieve high throughput, partitions are evenly distributed across brokers. Here’s a diagram to understand the architecture.\nTopic Replication and Controller\nEach active Kafka cluster has special brokers called controller nodes. The Zookeeper/Raft Protocol is responsible for controller selection. We know that each partition has multiple replicas determined by the replication factor. This controller node elects a leader for each of these partitions.\nA leader partition is a partition responsible for writing operations. Producers publish records only to the leader partition, and follower partitions replicate data from the leader. This reduces the overhead of writing to multiple partitions.\nWhenever a new topic is created or deleted, Kafka runs an algorithm to select a leader from the partition. The first replica always gets the preference. The distribution of partition leaders is even across brokers. So if you have 3 partitions and 3 brokers, each broker hosts a leader partition. This will spread the read/write load. When the leader fails, follower brokers take their place.\nTopics with leaders and followers are distributed evenly across brokers.\nNow you have an idea of how Kafka works. Let’s see how to set up a Kafka server from the terminal.\nApache Kafka With Zookeeper\nKafka uses Zookeeper to keep track of all the brokers in a cluster. The Zookeeper is responsible for following actions.\n- Notify Kafka when a broker dies, a new broker joins, a topic is deleted or created, etc.\n- Responsible for identifying and Electing the leader broker of a partition.\n- Metadata management, Permission, and configuration management of topics.\nSetup Kafka with the Zookeeper\nTo set up a Kafka cluster from the shell, you will need to perform the following steps:\n- Download and install the Apache Kafka binary files from the Apache website.\n- Extract the downloaded files and move them to the Kafka installation directory.\n- Start the Zookeeper server by running the\n- Start the Kafka server by running the\n- Create a new Kafka topic by running\nbin/kafka-topics.sh --topic test-events --bootstrap-server localhost:9092.\n- Start a Kafka producer by running\nbin/kafka-console-producer.sh --topic test-events --bootstrap-server. localhost:9092. Now write anything to the Kafka topic you just created.\n- Start a Kafka consumer by running the\nbin/kafka-console-consumer.sh --topic test-events --bootstrap-server localhost:9092. This will show you everything you have published to the test-events topic.\nAfter performing these steps, you will have a single-node Kafka cluster running on your machine. You can test the consumer and producer by running their respective scripts. The best thing about Kafka is both the producer and consumer are independent of each other, failure of one will not impact the other in any way.\nApache Kafka With KRaft\nSome flaws in Zookeeper’s implementation kept developers always wanting more.\n- With Zookeeper, Kafka clusters can only have 200,000 partitions.\n- A high-level leader election, while a broker joins or leaves, overloads the Zookeeper server slowing down the entire process.\n- Zookeeper security lags behind Kafka’s security.\nDue to these reasons, an alternate solution for metadata management and leader elections was created. As Kafka’s metadata are logs only, these logs can be consumed by Kafka brokers as internal metadata topics. In short, Kafka used itself to store metadata. A protocol called Raft was used for controller election, hence the name KRaft.\nSet-up Kafka with KRaft\nTo use Kafka with Kraft, create a cluster UUID.\nFormat Log directories\nbin/kafka-storage.sh format -t $KAFKA_CLUSTER_ID -c config/kraft/server.properties\nStart Kafka server\nThen, you can create topics and publish and consume events as usual.\nWorking with Kafka Using Python\nFirst of all, install Apache Kafka using pip.\npip install kafka-python\nSending messages to a Kafka topic\nThe following example shows how to use the\nKafkaProducer class to send messages to a Kafka topic:\nfrom kafka import KafkaProducer producer = KafkaProducer(bootstrap_servers=['localhost:9092']) # Send a message to the 'test' topic producer.send('test', b'Hello, Kafka!') # Flush the producer to ensure all messages are sent producer.flush()\nReceiving messages from a Kafka topic\nThe following example shows how to use the KafkaConsumer to receive messages from a Kafka topic:\nfrom kafka import KafkaConsumer consumer = KafkaConsumer( 'test', # consume messages from the 'test' topic group_id='my-group', # consumer group to join bootstrap_servers=['localhost:9092'], # Kafka broker address ) # Consume messages from the 'test' topic for message in consumer: print(message.value)\nProcessing messages in real-time with Kafka Streams\nKafka also provides a stream processing library called Kafka Streams, which allows for creating real-time streaming applications that can process data from Kafka topics.\nHere is an example of using Kafka Streams to count the number of messages in a Kafka topic in real-time:\nfrom kafka import KafkaConsumer from kafka.streams import KafkaStreams # Create a Kafka consumer to read from the 'test' topic consumer = KafkaConsumer( 'test', group_id='my-group', bootstrap_servers=['localhost:9092'], ) # Create a KafkaStreams instance to process the 'test' topic streams = KafkaStreams(consumer) # Define a function to process each message in the stream def process_message(message): # Increment a counter for each message received counter += 1 # Consume messages from the 'test' topic and process them with the function defined above streams.foreach(process_message)\nWhere to use Apache Kafka?\nKafka can stream and process website events, e-commerce data, IoT sensor data, and Micro-services generated logs.\nSome use cases of Kafka are\n- Real-time data pipeline: Kafka can help build a real-time data pipeline to ingest a large amount of data and make it available for real-time processing.\n- Streaming Data Analytics: Data from various sources can be ingested and analyzed to drive business and product decisions.\n- Event-Driven Architectures: Kafka can be used to create event-driven architectures that allow for immediate processing and action on data as it is generated.\n- Microservices: Kafka can be used as a communication channel between microservices. It can also retrieve and process large amounts of logs generated by various microservices.\nKafka’s ability to handle large amounts of data and support real-time processing makes Kafka an essential tool for those who need to process large quantities of data quickly and reliably.\nKey takeaways from the article\n- Kafka is a publish-subscribe event streaming tool for real-time data processing.\n- Its distributed nature makes it a low-latency, Highly available, fault-tolerant, robust tool for processing large amounts of real-time data.\n- Kafka has three core components Topics, Producers, and Consumers.\n- In future stable releases, Apache Kafka will no longer need Zookeeper but instead will leverage its capacity to store metadata logs in an internal topic and use the Raft protocol for controller election.\nSo, this was all about the basics of Apache Kafka.\nThe media shown in this article is not owned by Analytics Vidhya and is used at the Author’s discretion."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:b50a1b4e-52f6-48d1-9e15-b2ede69d79d1>","<urn:uuid:5a32a411-36f1-40a3-8adb-7604867d5f9c>"],"error":null}
{"question":"As a parent of an athletic child, I'm concerned about foot conditions. Do both flatfoot and Sever's disease share ibuprofen as a treatment option?","answer":"Yes, both conditions can be treated with ibuprofen (NSAIDs). For pediatric flatfoot, NSAIDs like ibuprofen may be recommended to help reduce pain and inflammation. Similarly, for Sever's disease, nonsteroidal anti-inflammatory drugs (NSAIDs) such as ibuprofen help reduce the pain and inflammation.","context":["What Is Pediatric Flatfoot?\nFlatfoot is common in both children and adults. When this deformity occurs in children, it is referred to as “pediatric flatfoot,” a term that actually includes several types of flatfoot. Although there are differences between the various forms of flatfoot, they all share one characteristic — partial or total collapse of the arch.\nMost children with flatfoot have no symptoms, but some children have one or more symptoms. When symptoms do occur, they vary according to the type of flatfoot. Some signs and symptoms may include:\n- Pain, tenderness, or cramping in the foot, leg, and knee\n- Outward tilting of the heel\n- Awkwardness or changes in walking\n- Difficulty with shoes\n- Reduced energy when participating in physical activities\n- Voluntary withdrawal from physical activities\nFlatfoot can be apparent at birth or it may not show up until years later, depending on the type of flatfoot. Some forms of flatfoot occur in one foot only, while others may affect both feet.\nTypes of Pediatric Flatfoot\nVarious terms are used to describe the different types of flatfoot. For example, flatfoot is either asymptomatic (without symptoms) or symptomatic (with symptoms). As mentioned earlier, the majority of children with flatfoot have an asymptomatic condition.\nSymptomatic flatfoot is further described as being either flexible or rigid. “Flexible” means that the foot is flat when standing (weight-bearing), but the arch returns when not standing. “Rigid” means the arch is always stiff and flat, whether standing on the foot or not.\nSeveral types of flatfoot are categorized as rigid. The most common are:\n- Tarsal coalition. This is a congenital (existing at birth) condition. It involves an abnormal joining of two or more bones in the foot. Tarsal coalition may or may not produce pain. When pain does occur, it usually starts in preadolescence or adolescence.\n- Congenital vertical talus. Because of the foot’s rigid “rocker bottom” appearance that occurs with congenital vertical talus, this condition is apparent in the newborn. Symptoms begin at walking age, since it is difficult for the child to bear weight and wear shoes.\nThere are other types of pediatric flatfoot, such as those caused by injury or some diseases.\nIn diagnosing flatfoot, the foot and ankle surgeon examines the foot and observes how it looks when the child stands and sits. The surgeon also observes how the child walks and evaluates the range of motion of the foot. Because flatfoot is sometimes related to problems in the leg, the surgeon may also examine the knee and hip.\nX-rays are often taken to determine the severity of the deformity. Sometimes an MRI study, CT scan, and blood tests are ordered.\nTreatment: Non-surgical Approaches\nIf a child’s flatfoot is asymptomatic, treatment is often not required. Instead, the condition will be observed and re-evaluated periodically by the foot and ankle surgeon. Custom orthotic devices may be considered for some cases of asymptomatic flatfoot.\nIn symptomatic pediatric flatfoot, treatment is required. The foot and ankle surgeon may select one or more approaches, depending on the child’s particular case. Some examples of non-surgical options include:\n- Activity modifications. The child needs to temporarily decrease activities that bring pain as well as avoid prolonged walking or standing.\n- Orthotic devices. The foot and ankle surgeon can provide custom orthotic devices that fit inside the shoe to support the structure of the foot and improve function.\n- Physical therapy. Stretching exercises, supervised by the foot and ankle surgeon or a physical therapist, provide relief in some cases of flatfoot.\n- Medications. Nonsteroidal anti-inflammatory drugs (NSAIDs), such as ibuprofen, may be recommended to help reduce pain and inflammation.\n- Shoe modifications. The foot and ankle surgeon will advise you on footwear characteristics that are important for the child with flatfoot.\nWhen Is Surgery Needed?\nIn some cases, surgery is necessary to relieve the symptoms and improve foot function. Foot and ankle surgeons perform a variety of techniques to treat the different types of pediatric flatfoot. The surgical procedure or combination of procedures selected for your child will depend on his or her particular type of flatfoot and degree of deformity.\nThis information has been prepared by the Consumer Education Committee of the American College of Foot and Ankle Surgeons, a professional society of 5,700 podiatric foot and ankle surgeons. Members of the College are Doctors of Podiatric Medicine who have received additional training through surgical residency programs. The mission of the College is to promote superior care of foot and ankle surgical patients through education, research and the promotion of the highest professional standards. Copyright © 2004, American College of Foot and Ankle Surgeons, www.acfas.org","Sever?s disease is a painful condition of the heel affecting children, usually at the beginning of the growth spurt in early puberty. It is caused by inflammation at the growth plate at the back of the heel, adjacent to the Achilles tendon attachment. This is one of the most common causes of heel pain in school-aged children. Physically active children aged between eight and fourteen years old are most at risk of developing pain from Sever?s disease. It is common among children involved in soccer, little athletics, gymnastics, basketball and netball but can affect children involved in any running or jumping activity. Boys seem to be more commonly affected than girls.\nThe more active a child is then the greater the chance of suffering from Sever?s disease. Poor foot function such as flat feet causes the calf and Achilles to work harder and pull on the growth plate leading to Sever?s disease. Tight calves or Achilles is common in growing children and can increase tension on the growth plate.\nThis is a condition that affects the cartilage growth plate and the separate island of growing bone on the back of the heel bone. This growth plate is called the physeal plate. The island of growing bone is called the apophysis. It has the insertion attachment of the Achilles tendon, and the attachment of the plantar fascia. This island of bone is under traction from both of these soft tissue tendon and tendon-like attachments.\nYour podiatrist will take a comprehensive medical history and perform a physical examination including a gait analysis. The assessment will include foot posture assessment, joint flexibility (or range of motion), biomechanical assessment of the foot, ankle and leg, foot and leg muscle strength testing, footwear assessment, school shoes and athletic footwear, gait analysis, to look for abnormalities in the way the feet move during gait, Pain provocation tests eg calcaneal squeeze test. X-rays are not usually required to diagnose Sever?s disease.\nNon Surgical Treatment\nThe primary method of treating Sever?s disease is taking time off from sports and other physical activities to alleviate the pressure on the heel bone. During the healing period, your child?s doctor may also recommend physical therapy or any type of exercise that involves stretching and strengthen leg muscles and tendons. Wrapping ice in a towel and placing it under the child?s heel will also help to alleviate and reduce pain and swelling.\nThe surgeon may select one or more of the following options to treat calcaneal apophysitis. Reduce activity. The child needs to reduce or stop any activity that causes pain. Support the heel. Temporary shoe inserts or custom orthotic devices may provide support for the heel. Medications. Nonsteroidal anti-inflammatory drugs (NSAIDs), such as ibuprofen, help reduce the pain and inflammation. Physical therapy. Stretching or physical therapy modalities are sometimes used to promote healing of the inflamed issue. Immobilization. In some severe cases of pediatric heel pain, a cast may be used to promote healing while keeping the foot and ankle totally immobile. Often heel pain in children returns after it has been treated because the heel bone is still growing. Recurrence of heel pain may be a sign of calcaneal apophysitis, or it may indicate a different problem. If your child has a repeat bout of heel pain, be sure to make an appointment with your foot and ankle surgeon."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:63f2b0c2-0632-48fb-a46d-c0372dd026d3>","<urn:uuid:5286895c-bb33-4594-a526-d33c0449e9f9>"],"error":null}
{"question":"I've been diagnosed with prostate cancer and am researching treatment approaches. Can you compare how PSA testing and biopsy results influence the choice between active surveillance versus seed implantation therapy?","answer":"PSA testing and biopsy results are key factors in treatment decisions. The PSA, rectal exam, and biopsy report are described as the 'big three' that define the disease's aggressiveness and are imperative to treatment decisions. For seed implantation (brachytherapy), the entire prostate gland needs treatment because microscopic cancer cells may be present throughout, even if biopsies in certain areas were negative. Active surveillance ('doing nothing') is always presented as a valid option, but the decision between this and active treatment like brachytherapy should be based on understanding the specifics of your disease through these diagnostic tests.","context":["TABLE OF CONTENTS\nIntroduction – Now you know and the process begins 1\nAn arrow in your quiver.\nPreface – So you have your own disease.\nPart One – The basics 9\nProstate cancer and the male – The perfect storm.\nThe PSA, rectal exam, and biopsy report – The “big three” are the essence of prostate cancer and define the aggressiveness of the disease. An understanding of each is imperative to “The Decision.”\nThe anatomy of the prostate – Water and nerves.\nTreatment options – Usually, but not always, a major inconvenience.\nPart Two – ‘Who are you?’ and why it matters 39\nGeneral health, age, and years at risk – What’s best for the cancer may not be best for you.\nYour PSA, rectal exam, and biopsy report – Understanding the specifics of your disease is key to making the right decision.\nBias – Do you have a dog in this fight?\nCure-driven – Most aggressive treatment is your priority.\nRisk-driven – Least chance of side effects is your priority.\nLifestyle-driven – Ease of treatment is your priority.\nInternet/family/friends – Apples to apples/Prostates to prostates.\nMisconceptions and half-truths – Don’t let “a little knowledge” get you.\nMcHugh Decision Worksheet – Have you learned enough about yourself and your prostate cancer to answer these questions intelligently?\nPart Three – Putting it all together – Methods for making “the decision” 111\nGetting your priorities straight – Paper covers rock.\nYou’ve got good health and all options are open to you – Evaluating your underlying health is an important part of the decision process.\nEvaluating the negatives – Picking your poison.\nBest case/worse case scenarios – Evaluating the potential outcome of your decision from different perspectives may be of help to you.\nHow I made my decision – It’s not about doing what I did; it’s about making your decision the way I did.\nHow you should make yours – Now it’s your turn.\nDecision Cheat Sheet – Tally up all the factors to see where you stand.\nPart Four – You’ve decided 129\nYou want it out – Open vs. Robotic.\nYou want radiation – External beam alone, seeds alone, or combination therapy.\nYou want to do something else – Just because it’s new doesn’t mean it’s better.\nYou want to do nothing – Surveillance therapy is always an option.\nRepresentative case studies – How other’s decisions played out for them.\nPart Five – The best laid plans oft go astray 157\nThe luck factor – I’d rather be lucky than good, and I’d rather have good luck than make the right decision.\nEpilogue – Now that you know, tell others 169\nShared joy-twice joy – Shared sorrow-half sorrow\nExtras – Always give ‘em a little more than they paid for\nPatients do the funniest things.\nDid you know?\nProstate Stories – Only a urologist could write a story in which the main character is a prostate.\nDiaper Diaries – A screenplay\nDid I treat you fairly?\nI have good news. I have prostate cancer.\nJohn, I heard you got cancer!","What patients should expect from Palladium-103 brachytherapy (seed implantation for prostate cancer)\nIn order to treat prostate cancer, radioactive Palladium-103 or Iodine-125 seeds are placed directly into the prostate gland, using either after loading needles with a special \"gun\" or preloaded needles.\nBoth of these seeds give off low-energy x-rays, and the majority of the radioactivity is released within a short period of time. Only each seed irradiates a small volume of prostate tissue, and therefore many seeds have to be placed throughout the prostate to cover the entire gland and the cancer site. Because of the low x-ray radiation energy released, radiation exposure to adjacent normal organs is reduced.\nThe radioactive seeds treat the entire prostate gland because the microscopic cancer cells may be present at different sites within the gland, even though the biopsy in the general area was negative. The number of seeds implanted into the prostate for treatment depends on the size and shape of the prostate gland. On average, the number, of seeds implanted is approximately 100.\nIn performing brachytherapy, the doctor places a biplaner ultrasound probe in the rectum to image the prostate. The biplaner ultrasound, along with fluoroscopy, gives a multidimensional view of the prostate on several TV screens. These images are then used to accurately place the needles and to space the seeds in the prostate gland. No surgical incision is required.\nNeedles are advanced through an area of skin (behind the scrotum and in front of the rectum) into the prostate with the aid of:\n- A template attached to the ultrasound probe and,\n- A computer plan designed specifically for the size of the patient's prostate.\nRadioactive seeds are then deposited through the needle into the prostate gland. The seeds are permanently placed in the prostate gland. Depending on the radioactive seeds that are selected, they give off radiation for 3 months to a year. Both the probe and needles are removed when the procedure is completed. Cystoscope is done to evaluate the urethra and the bladder and to retrieve any seeds found in the bladder.\nAbout radioactive seed implantation for prostate cancer\nInternal radiation therapy, also called Interstitial Radiotherapy or Brachytherapy involves implanting radioactive seeds into the prostate gland to treat cancer.\nIn selected men, this option offers quick, minimally invasive treatment with good cancer control. Small radioactive rice-sized pellets or \"seeds\" (Palladium 103 or Iodine 125) are placed directly into the prostate gland and give off a known amount of radioactivity into the surrounding prostate tissue. In this way, radiation is placed as close as possible to the cancerous cells so that the pelvic organs are exposed to less radiation than with external beam radiation.\nSeeds are implanted without a surgical incision. They are inserted through the skin of the perineum (just behind the scrotum and in front of the anus) using small pre-loaded needles. General or spinal anesthesia is used. Each seed is carefully placed in a predetermined location and depth. Placement is guided by a template attached to an ultrasound probe and a computer plan is designed specifically for the size of the patient's prostate. Placement is monitored in real time with ultrasound and fluoroscopy.\nThe permanent seeds give off radiation continually for an extended period of time. The amount of time that the seeds remain radioactive depends on the dose and what type of radioactive material is used. For example, the half-life of Palladium-103 is 17 days. That means that the prostate receives half of the dose in the first 17 days; then one quarter in the next 17 days. The useful dose will have been delivered in three to four half-lives.\nThe entire gland gets treated because microscopic cancer cells may be present at different sites in the prostate even though the biopsy in the general area was negative. The seeds irradiate a small volume of tissue so several seeds have to be placed to cover the entire gland. The number of seeds used can range from 40 to 150, depending on the size of the prostate gland.\nPotential side effects\nAny medical treatment may cause side effects or put you at risk for a more serious and/or permanent complication.\nYou may experience a few, none, or (very rarely) all of these side effects. Most will disappear or lessen with time. Also, if other types of treatment are given in conjunction with radiation therapy, side effects may be more frequent and/or more severe than if radiation therapy alone had been given.\nProbable side effects\n- Pelvic discomfort\n- Urinary frequency (feeling the need to urinate frequently Urinary urgency, or feeling the need to urinate right away)\nPossible side effects\n- Burning during urination\n- Urinary retention (passing urine, but unable to fully empty the bladder)\n- Urinary obstruction (unable to pass urine)\n- Rectal bleeding\n- Sexual dysfunction\n- Urinary incontinence (unable to control bladder)\n- Bowel incontinence (unable to control bowels)\nRare side effects\n- Rectal injury requiring surgery\nSome foods and liquids (acidic food or amino acid groups) can be slightly irritating to the bladder, causing increased urinary frequency, discomfort, and a slower stream.\nGenerally, it is not necessary to eliminate these foods from the diet, but you may wish to decrease the amount, particularly if you are having a lot of symptoms.\n- Acidic Foods\n- Alcoholic beverages\n- Cranberries and juice\n- Apples/apple juices\n- Grapes/grape juice\n- Carbonated beverages\n- Chilies/spicy food\n- Citrus fruits and drinks\n- Tea and Coffee including decaf\n- Vitamin B complex\n- Low acid fruits: pear, apricots, papaya, watermelon\n- For coffee drinkers: KAVA (low acid instant), cold brew from Starbucks\n- For tea drinkers: non-citrus herbal, Sun brewed tea"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:4bfd32c3-448f-4833-a820-eef5e4c5bbe3>","<urn:uuid:b731c2e0-fb2c-49fc-aa89-fe2569a2e354>"],"error":null}
{"question":"I noticed genetic health risk info and smoking data - which has stronger evidence for changing behavior: DNA test results or lung cancer statistics?","answer":"Based on the evidence, lung cancer statistics have stronger evidence for changing behavior than DNA test results. The genetic risk studies were found to have no significant effects on smoking cessation and were predominantly at high or unclear risk of bias, with low quality evidence. In contrast, lung cancer mortality statistics show clear behavioral impacts - as smoking decreased among men in the last 30 years, lung cancer mortality peaked in the 1980s and has been declining since then. State-specific data also demonstrates this relationship - states with higher smoking rates like Kentucky (29.1% male smokers) had much higher lung cancer death rates (111.5 per 100,000 men) compared to states with lower smoking rates like Utah (10.4% male smokers, 33.7 deaths per 100,000 men).","context":["Does genetic risk information improve healthy behavior? Let’s not throw out the baby with the bath water!Posted on by\nIn a recent systematic review with meta-analysis, Hollands et al evaluated the impact of communicating genetic risk information on risk-reducing health behaviors and motivations for behavior change. The authors reviewed 18 studies with 7 behavioral outcomes, including smoking cessation, diet and physical activity. They found no significant effects of communicating DNA based risk estimates on smoking cessation, diet or physical activity. They also found no effects on other behaviors (including alcohol use, medication use, sun protection, and screening programs). The study generated interest and press coverage. One widely expressed sentiment is to point out “the limits of personalized medicine” emphasizing that “knowing genetic risk of disease doesn’t motivate people to change their behavior.”\nA closer look at the review gives plenty of reasons to pause and reflect on the limitations of the studies themselves. The review might at first seem to prematurely condemn a nascent field that is still in infancy and requires additional rigorous research. However, the authors of the review pointed out the limitations of available evidence, and stated that “studies were predominantly at high or unclear risk of bias, and evidence was typically of low quality.”\nIn a series of letters to the editor, further explanations of study limitations were offered. They are worth repeating here. Janssens explains that communicating risk estimates based on one or a few single nucleotide polymorphisms with weak disease risk associations is hardly expected to change health behavior for complex diseases that are caused by interactions between many genetic and environmental factors. Most genetic tests included in the review are already outdated. Currently, more genetic variants for different diseases are available which can improve the predictive ability of genetic tests.\nBurton et al point out that we already know that there is a paucity of evidence linking risk communication, genomic or otherwise, to the motivation of sustained behavior change. Motivating behavior change is extraordinarily difficult, and there is no reason to suggest that genomic information will have a greater influencing power than non-genetic information such as personal, physiological or biomarker data – even though these more traditional factors are widely considered to have utility. Furthermore, there is little evidence that the accuracy of the risk prediction for these diseases will be improved. Thus, for any individual, the disease risk estimate will be only slightly higher or slightly lower than the population “average” risk, making the case of stratified or individualized disease prevention less than compelling.\nHay et al point out the limitations of studies included in this review such as selection biases, relatively small sample sizes, the methods of risk communication, and the failure to consider gene-environment interactions.\nThe bottom line is that a rigorous review of methodologically-limited studies can be misleading. The field of genomic risk communication is at an early stage, and this is a time of rapid change in risk information. So let us not throw out the baby with the bathwater. Rather than dismissing genomics on the basis of limited data, we should focus on generating appropriate evidence on how the combination of genomic and other personal and environmental information can be effectively used to drive behavioral and medical interventions that improve health and prevent disease.","In the United States, smoking-related illnesses accounted for an estimated 443,000 deaths each year between 2000 and 2004.[1,2] (Also available online.) On average, these deaths occur 12 years earlier than would be expected, so the aggregate annual loss exceeds 5 million life-years. These deaths are primarily due to smoking’s role as a major cause of cancer, cardiovascular diseases, and chronic lung diseases. The known adverse health effects also include other respiratory diseases and symptoms, nuclear cataract, hip fractures, reduced female fertility, and diminished health status. Maternal smoking during pregnancy is associated with fetal growth restriction, low birth weight, and complications of pregnancy. It has been estimated that at least 30% of cancer deaths and 20% of all premature deaths in the United States are attributable to smoking.\nTobacco products are the single, major avoidable cause of cancer, causing more than 155,000 deaths among smokers in the United States annually due to various cancers. The majority of cancers of the lung, trachea, bronchus, larynx, pharynx, oral cavity, nasal cavity, and esophagus are attributable to tobacco products, particularly cigarettes. Smoking is also causally associated with cancers of the pancreas, kidney, bladder, stomach, and cervix and with myeloid leukemia.[4,6]\nSmoking also has substantial effects on the health of nonsmokers. Environmental or secondhand tobacco smoke is implicated in causing lung cancer and coronary heart disease. Among children, secondhand smoke exposure is causally associated with sudden infant death syndrome, lower respiratory tract illnesses, otitis media, middle ear effusion, exacerbated asthma, and respiratory effects such as cough, wheeze, and dyspnea.\nEnvironmental tobacco smoke has the same components as inhaled mainstream smoke, although in lower absolute concentrations, between 1% and 10%, depending on the constituent. Carcinogenic compounds in tobacco smoke include the polycyclic aromatic hydrocarbons (PAHs), including the carcinogen benzo[a]pyrene (BaP) and the nicotine-derived tobacco-specific nitrosamine, 4-(methylnitrosamino)-1-(3-pyridyl)-1-butanone (NNK). Elevated biomarkers of tobacco exposure, including urinary cotinine, tobacco-related carcinogen metabolites, and carcinogen-protein adducts, are seen in passive or secondhand smokers.[7,9-11]\nIn 2011, 21.6% of adult men and 16.5% of adult women in the United States were current smokers. (Also available online.) Cigarette smoking is particularly common among American Indians and Alaska Natives. The prevalence of smoking also varies inversely with education, and was highest among adults who had earned a General Educational Development diploma (49.1%) and generally decreased with increasing years of education. (Also available online.) From 2000 to 2011, significant declines occurred in the use of cigarettes among middle school (10.7% to 4.3 %) and high school (27.9% to 15.8%) students. (Also available online.) Cigarette smoking prevalence among male and female high school students increased substantially during the early 1990s in all ethnic groups but appears to have been declining since approximately 1996.[14,15] (Also available online.)\nThe effect of tobacco use on population-level health outcomes is illustrated by the example of lung cancer mortality trends. Smoking by women increased between 1940 and the early 1960s, resulting in a greater than 600% increase in female lung cancer mortality since 1950. Lung cancer is now the leading cause of cancer death in women.[14,16] In the last 30 years, prevalence of current cigarette use has generally decreased, though far more rapidly in males. Lung cancer mortality in men peaked in the 1980s, and has been declining since then; this decrease has occurred predominantly in squamous cell and small cell carcinomas, the histologic types most strongly associated with smoking. Variations in lung cancer mortality rates by state also more or less parallel long-standing state-specific differences in tobacco use. Among men, the average annual age-adjusted lung cancer death rates from 2001 to 2005 were highest in Kentucky (111.5 per 100,000), where 29.1% of men were current smokers in 1997, and lowest in Utah (33.7 per 100,000), where only 10.4% of men smoked. Among women, lung cancer death rates were highest in Kentucky (55.9 per 100,000), where 28.0% of women were current smokers, and lowest in Utah (16.9 per 100,000), where only 9.3% of women smoked.References\n- American Cancer Society.: Cancer Facts and Figures 2013. Atlanta, Ga: American Cancer Society, 2013. Available online. Last accessed May 2, 2013.\n- Centers for Disease Control and Prevention (CDC).: Current cigarette smoking among adults - United States, 2011. MMWR Morb Mortal Wkly Rep 61 (44): 889-94, 2012. [PUBMED Abstract]\n- Nelson DE, Kirkendall RS, Lawton RL, et al.: Surveillance for smoking-attributable mortality and years of potential life lost, by state--United States, 1990. Mor Mortal Wkly Rep CDC Surveill Summ 43 (1): 1-8, 1994. [PUBMED Abstract]\n- U.S. Department of Health and Human Services.: The Health Consequences of Smoking: A Report of the Surgeon General. Atlanta, Ga: U.S. Department of Health and Human Services, CDC, National Center for Chronic Disease Prevention and Health Promotion, Office on Smoking and Health, 2004. Available online. Last accessed May 9, 2013.\n- Centers for Disease Control and Prevention.: Targeting Tobacco Use: The Nation's Leading Cause of Death 2005. Atlanta, Ga: CDC, 2005.\n- Ontario Task Force on the Primary Prevention of Cancer.: Recommendations for the Primary Prevention of Cancer. Toronto, Canada: Queen's Printer for Ontario, 1995.\n- U.S. Department of Health and Human Services.: The Health Consequences of Involuntary Exposure to Tobacco Smoke: A Report of the Surgeon General. Atlanta, Ga: U.S. Department of Health and Human Services, Centers for Disease Control and Prevention, Coordinating Center for Health Promotion, National Center for Chronic Disease Prevention and Health Promotion, Office on Smoking and Health, 2006. Also available online. Last accessed May 9, 2013.\n- Cinciripini PM, Hecht SS, Henningfield JE, et al.: Tobacco addiction: implications for treatment and cancer prevention. J Natl Cancer Inst 89 (24): 1852-67, 1997. [PUBMED Abstract]\n- Finette BA, O'Neill JP, Vacek PM, et al.: Gene mutations with characteristic deletions in cord blood T lymphocytes associated with passive maternal exposure to tobacco smoke. Nat Med 4 (10): 1144-51, 1998. [PUBMED Abstract]\n- Benowitz NL: Cotinine as a biomarker of environmental tobacco smoke exposure. Epidemiol Rev 18 (2): 188-204, 1996. [PUBMED Abstract]\n- Hecht SS: Human urinary carcinogen metabolites: biomarkers for investigating tobacco and cancer. Carcinogenesis 23 (6): 907-22, 2002. [PUBMED Abstract]\n- Centers for Disease Control and Prevention (CDC).: Vital signs: current cigarette smoking among adults aged ≥18 years with mental illness - United States, 2009-2011. MMWR Morb Mortal Wkly Rep 62 (5): 81-7, 2013. [PUBMED Abstract]\n- Centers for Disease Control and Prevention (CDC).: Current tobacco use among middle and high school students--United States, 2011. MMWR Morb Mortal Wkly Rep 61 (31): 581-5, 2012. [PUBMED Abstract]\n- Jemal A, Thun MJ, Ries LA, et al.: Annual report to the nation on the status of cancer, 1975-2005, featuring trends in lung cancer, tobacco use, and tobacco control. J Natl Cancer Inst 100 (23): 1672-94, 2008. [PUBMED Abstract]\n- Johnston LD, O'Malley PM, Bachman JG: Monitoring the Future: National Survey Results on Drug Use, 1975-2001. Volume I: Secondary School Students. Bethesda, Md: National Institute on Drug Abuse, 2002. NIH Pub. No. 02-5106. Also available online. Last accessed February 15, 2013.\n- U.S. Preventive Services Task Force.: Guide to Clinical Preventive Services: Report of the U.S. Preventive Services Task Force. 2nd ed. Baltimore, Md: Williams & Wilkins, 1996."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:4a011415-a9b4-4448-aab1-fffe706661f5>","<urn:uuid:b70ae121-7787-4492-8c7c-992922ebaf4e>"],"error":null}
{"question":"How do St. John's wort and licorice differ in their regulatory oversight and standardization approaches?","answer":"St. John's wort and licorice have different approaches to standardization in their preparations. For licorice, there are specific standardized extracts containing 20% glycyrrhizinic acid, with precise dosage recommendations ranging from 250-500 mg three times daily. For St. John's wort, which is sold as a dietary supplement in the US, standardization varies as the FDA does not regulate dietary supplements the same way as medicines. This means that how well St. John's wort works or its side effects may differ among brands or even within different lots of the same brand.","context":["By: Nikki Nies\nLicorice (Glycyrrhiza glabra) is a plant, most commonly associated with flavorings in food, beverages and tobacco. However, the root is used to make Eastern and Western medicine.\nLicorice can be used for:\n- Digestive issues: heartburn, indigestion, GERD, stomach ulcers, colic, ongoing inflammation of the stomach’s lining-chronic gastritis\n- Sore throat\n- Canker sores\n- Infections from bacteria or viruses\n- Systemic lupus erythematosus (SLE)\n- Liver disorders\n- Food poisoning\n- Chronic Fatigue Syndrome (CFS)\nIt can be used in many forms:\n- Dried root: 1 – 5 g as an infusion or decoction (boiled), 3 times daily\n- Licorice 1:5 tincture: 2 – 5 mL, 3 times daily\n- Standardized extract: 250 – 500 mg, 3 times daily, standardized to contain 20% glycyrrhizinic acid\n- DGL extract: 0.4 – 1.6 g, 3 times daily, for peptic ulcer\n- DGL extract 4:1: chew 300 – 400 mg, 3 times daily 20 minutes before meals, for peptic ulcer\nIf one has the following disease states or situations, use of licorice should not be used: liver disease, pregnancy and breastfeeding, high blood pressure, hypertonia, low potassium levels in the blood (hypokalemia), kidney disease, surgery, sexual problems in men and/or hormone sensitive conditions (i.e. breast cancer, uterine cancer, ovarian cancer and/or endometriosis).\nNatural licorice can increase cortisol concentration, leading to increased sodium retention, potassium excretion, high blood pressure (aka hypertension) and/or an increase in sodium reabsorption. These changes can antagonize the action of diuretics and antihypertensive medications. Some herbs have a stimulant laxative effect (i.e. aloe vera, castor oil, senna and rhubarb) should be avoided to lower potassium in body.\nFurthermore, use of certain medications can negatively interact with licorice.\n|Medication||Use||Potential interaction with licorice|\n|Warfarin (Coumadin)||Slow blood clotting||Licorice may increase breakdown; decrease effectiveness of warfarin, which may increase the risk of clotting|\n|Cisplatin (Platinol-AQ)||Treat cancer||Licorice may decrease how well cisplatin works|\n|Digoxin (Lanoxin)||Treats atrial fibrillation and heart failure||Large amounts of licorice can decrease potassium levels, which can inhibit digoxin’s effectiveness|\n|Ethacyrnic Acid (Edecrin); Furosemide (Lasix)||Treats edema; diuretic||When etharynic and licorice are taken together, may cause potassium to become too low|\n|Furosemide (Lasix)||Treats edema||When furosemide and licorice are taken together, may cause potassium to become too low|\n|Medications associated with the liver (i.e. cytochrome P450 2C9, cytochrome P450 3A4, CYP3A4, phenobarbital, dexamethasone)||Liver issues||Licorice may change how the liver breaks down medications, may increase/decrease effects of medications|\n|Antihypertensive drugs (i.e. captopril, enalapril, losartan, valsartan, amlodipine, hydrochlorothiazide, Lasix)||Treats high blood pressure||Might decrease effectiveness of medications for high blood pressure|\n|Corticosteroids (i.e. hydrocortisone, dexamethasone, methylprednisone, prednisone)||For inflammation||Some medications for inflammation can decrease potassium in the body; when corticosteroids are taken in conjunction with licorice, can decrease potassium in the body too much|\n|Diuretics (i.e. Lasix, Diuril, Thalitone, HCTZ, Microzide)||Water pills||In conjunction with licorice, diuretics can decrease potassium in body too much|\nLastly, when taking licorice, drinking grapefruit juice may increase licorice’s ability to cause potassium depletion. Licorice can increase sodium/water retention and increase blood pressure. Licorice can be a great solution to certain disease states, however, take caution with use of licorice if you’re on medications. Best to check with your primary care physician if it is safe to use licorice.","St. John's Wort\nWhat is St. John's wort?\nSt. John's wort ( Hypericum perforatum) is a plant with yellow flowers that people in European countries have used for centuries to treat mild to moderate depression. In the United States, it is sold as a dietary supplement and can be found at health food stores and pharmacies.\nWhat is St. John's wort used for?\nSt. John's wort is used in the short-term treatment of mild to moderate depression.\nIt may take up to 2 to 3 weeks for St. John's wort to improve depressive symptoms. Not all preparations of St. John's wort are the same. A standardized form means the amount of St. John's wort is the same in every capsule.\nIs St. John's wort safe?\nSt. John's wort causes fewer side effects (such as digestive discomfort or headaches) than antidepressant medicines, although it may cause a rash with sun exposure.\nSt. John's wort may interact with medicines used to treat some other illnesses, such as AIDS. It is important to let your doctor or pharmacist know if you want to try St. John's wort so that he or she can determine whether it might interfere with other medicines you are taking.\n- Do not take St. John's wort while you are taking other antidepressants. You may overmedicate yourself, resulting in serious negative side effects. Always talk with your doctor before you take any herbal remedies to treat depression or other conditions.\n- Do not take St. John's wort while you are taking protease inhibitors (PIs) or nucleoside reverse transcriptase inhibitors (NRTIs) for the treatment of HIV infection.\n- Do not take St. John's wort while you are pregnant or breastfeeding.\nThe U.S. Food and Drug Administration (FDA) does not regulate dietary supplements in the same way it regulates medicines. A dietary supplement can be sold with limited or no research on how well it works.\nAlways tell your doctor if you are using a dietary supplement or if you are thinking about combining a dietary supplement with your conventional medical treatment. It may not be safe to forgo your conventional medical treatment and rely only on a dietary supplement. This is especially important for women who are pregnant or breastfeeding.\nWhen using dietary supplements, keep in mind the following:\n- Like conventional medicines, dietary supplements may cause side effects, trigger allergic reactions, or interact with prescription and nonprescription medicines or other supplements you are taking. A side effect or interaction with another medicine or supplement may make other health conditions worse.\n- Dietary supplements may not be standardized in their manufacturing. This means that how well they work or any side effects they cause may differ among brands or even within different lots of the same brand. The form you buy in health food or grocery stores may not be the same as the form used in research.\n- The long-term effects of most dietary supplements, other than vitamins and minerals, are not known. Many dietary supplements are not used long-term.\nOther Works Consulted\n- Bongiorno PB, Murray MT (2013). Hypericum perforatum (St. John's wort). In JE Pizzorno, MT Murray, eds., Textbook of Natural Medicine, 4th ed., pp. 833–841. St. Louis: Mosby.\n- Linde K, et al. (2008). St. John's wort for major depression. Cochrane Database of Systematic Reviews (4).\n- St. John's wort (2010). In A DerMarderosian et al., eds., Review of Natural Products. St. Louis: Wolters Kluwer Health.\n- Szegedi A, et al. (2005). Acute treatment of moderate to severe depression with hypericum extract WS 5570 (St. John's wort): Randomised controlled double blind non-inferiority trial versus paroxetine. BMJ, 330(7490): 503–508.\nCurrent as of: December 20, 2019\nAuthor: Healthwise Staff\nMedical Review: Adam Husney MD - Family Medicine"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:764e8e4e-9c43-48ae-be6b-852b1f77c8da>","<urn:uuid:2d3367da-9304-4d3b-9017-06b8bb565cb5>"],"error":null}
{"question":"How do modern music recording projects preserve cultural heritage, and what challenges arise in collecting original vinyl recordings?","answer":"Modern music recording projects like Natural Rhythm follow historical precedents like Alan Lomax's work, documenting and preserving American folk and roots music across multiple locations. These projects create new anthologies that serve as windows into musical evolution, though they can only capture a small portion of live performances. Meanwhile, in the vinyl collecting world, particularly for northern soul, original recordings face preservation challenges. The condition of vinyl records can vary greatly - some may look damaged but sound fine for DJing, while others might appear mint but sound terrible. Additionally, the high demand for original recordings has driven prices up significantly, with some rare northern soul 45s selling for £5000-£10000+, making preservation and access increasingly difficult.","context":["The 100 Club is one of the UK's longest running northern soul allnighters. In addition to being a legendary allnighter, it has also been responsible for creating its own mini-collectors scene. At each allnighter, those attending get a \"free\" 7\" single which usually features otherwise unreleased on 7\" records. They are generally great quality, and as a result, have become highly collectable in their own right. Also, because they are often the only vinyl release, they are classed as \"original vinyl\" and if someone wants to play them out at an original only venue, then they can.\nAdy Croasdale, via Kent records (part of Ace Records) has discovered some truly incredible unreleased northern soul sounds from the vaults over the years, and they are still showing up. As a result, 100 Club anniversary northern soul singles are highly sought after.\nNorthern Soul records for sale between £10 and £100.\nYou may find a lot of great soul records for sale in this price bracket. Also bear in mind that you might find a great record that sells for more than £100 in top condition offered more cheaply because it's not in quite so good shape. When buying northern soul records - especially if buying to DJ with - many collectors are more concerned about whether or not a record is an original and how it sounds as opposed to how it looks.\nTechnically, a record should be graded based on it's visual condition - though there may also be some added information about the sound quality. It's perfectly possible to find a record that is in VG condition and even sometimes G condition that actually sounds fine - especially for DJing with - as long as the sound of the record is loud and clear. (It's also possible to find a record that looks mint but sounds terrible due to it being either a bad pressing, or a styrene record that has been destroyed with a damaged stylus).\nPlaying records out can often lead to a more forgiving sound when a record has some surface noise as the sheer volume of the music can often drown out pops and crackle that would be very noticable if played at home. Hence the phrase \"fine for DJing\" which is often used where a record sounds a lot better than it looks.\nIt's fair to say that some records are so in-demand that as long as they play fine, (good enough to DJ with or better) then they might sell for as much as a near mint copy. Supply and demand is a hugely influential force in northern soul record collecting.\nYou don't have to be a big spender to appreciate the sound of northern soul played from a vinyl 45 record - or a 7\" single as we say in the UK. You can buy a lot of great records for under a tenner if you are not overly concerned about it being a reissue (which normally means it is an officially licensed re-release of a record) or even a pressing (which is northern soul record collecting speak for a bootleg - where the rights holder hasn't been paid for the reissue). Pressings were rife in the 70s and still are - and some are collectors items in their own right (and so might sell for a lot more that £10!)\nIf you are DJing with vinyl, you will find there is a massive hornets nest to poke if you want to argue the toss about whether or not you should be allowed to DJ with reissues or bootlegs. Some people do, and lots of venues (and punters) aren't bothered as long as it sounds OK, and equally, lots of other venues (and punters) care very very much if bootlegs and reissues are played out and fervently believe that original vinyl only should be played at northern soul venues. You could argue that as long as people know what to expect, it's up to them - but as I said, it's a massive topic that has been the subject of hundreds of online threads and debates and I'm not going to re-hash it here!\nHaving said that, you can, of course, find some original northern soul 45s for sale for under £10 - not all records sell for massive high prices. So the records you find here won't necessarily all be reissues or bootlegs.\nAnyway, you're here because you want northern soul records that are under £10. Here you go.\nMake sure you check the condition of the record before you order.\nNorthern Soul over £100 for sale\nThe value and the price of a northern soul 45 can vary from week to week in some cases. Rare soul 45s go up and down in popularity and over the last 10 years or so, the general trend is that prices have gone up and up. Classic \"top 100\" northern soul oldies which have always been considered to be rare have particularly increased in value to the point where, a bit like London house prices, if you were starting to collect \"from scratch\" now, you'd need to have quite a lot of money if you wanted to try and buy the very top end of the northern soul 45 scene- which can often change hands these days for £5000-£10000+ in some cases.","Natural Rhythm is a multimedia music recording project that follows the path of the Great Migration from New Orleans to Memphis, St. Louis, and Chicago, emulating the work of Alan Lomax.\nIn the summer of 2014, Abigail Nover and Tegan McDuffie drove 1,580 miles in a ten-year-old Toyota Corolla with no air conditioning, following the path of the Great Migration. Undertaking a project named “Natural Rhythm,” they were also following the footsteps of John and Alan Lomax. In the twentieth century John and Alan Lomax traveled the country and the world recording music for The Library of Congress. They recorded greats like Leadbelly, Woody Guthrie, Jelly Roll Morton and preserved countless traditions that became available to the public. These archives serve as resource for scholarly endeavors, but include many recordings of enduring popularity and influence.\nAlan Lomax’s two major journeys through the US took place in 1937 and again in 1959. Despite the fact that his work and methodology are highly controversial, his recordings have become staples of the understanding American folk and roots music. Since these trips, however, fieldwork of American folksong has become less common and certainly less popular, creating a gap in recorded sources. So, more than half a century later, Nover and McDuffie hit the road filled with curiosity and determination to document the current state of American folk music. Although the project was more Gonzo than Lomax, it examines how music has changed since the depression era and folk revival, and provides us with clues about what the future may hold.\nIn order to identify how American folk and roots music has evolved in the past fifty-five years, they traveled from New Orleans to Chicago, recording music and interviews to create the beginning of a new anthology of today’s American music. Over six weeks of travel, they visited and recorded in Louisiana (New Orleans, Opelousas, Eunice, Plaissance, Laffayette), Mississippi (Greenville, Merigold, Clarksdale, Greenwood, Como, Cleveland), Tennessee (Memphis), Missouri (St. Louis, Webster Groves), and Illinois (Chicago). They recorded thirty-one sessions, accumulated fifty tracks of music, and twenty-eight interviews. The people and music recorded represent a large spectrum. The youngest person recorded was thirteen and the oldest was ninety-three. The music included in the anthology covers genres including blues, bluegrass, jazz, ragtime, country, and old time.\nThe breadth of the project, as it stands currently, is wide, but ultimately incomplete. The project only scratches the surface and calls attention to a need for a much larger and more in-depth project to further record and especially to analyze the current, and swiftly changing state of American folk and roots music. To continue the project, it would be necessary to record in more regions of the United States and for longer periods of time in each location. With the amount of time and money allotted, this project serves its purpose as a small collection of examples through which to begin to analyze and understand the state of folk and roots music in 2014.\nThe project creates a new context that allows for examination. The scholar and media theorist, Jonathan Sterne writes in his seminal book, The Audible Past, “Recording is a form of exteriority: it does not preserve a preexisting sonic event as it happens so much as it creates and organizes sonic events for the possibility of preservation and repetition.” Sterne’s apt description hits an important point. This project and Lomax’s alike are new creations that can be used as a point of comparison, but cannot account for the totality of the live performances that they captured. The collection can, however provide a window not only into the past, but also into what lies ahead for continued evolution of American music.\nThe way we consume music on a daily basis has changed drastically in the past decade with the advent of social media, including YouTube, Spotify, iTunes, and the increasing commonality of headphone use. The availability of music has increased both in terms of ease of access and extensiveness of available resources. People have access to music from virtually any location and time period so long as they know how to search for it on the Internet. Technology is becoming a new form of oral tradition. So what does this mean for American folk music today? This anthology explores the answer to that question.\n Sterne, Jonathan. The Audible Past: Cultural Origins of Sound Reproduction. Durham: Duke UP, 2003. 332. Print.\n'Natural Rhythm' is a sponsored project of Fractured Atlas, a non-profit arts service organization. Contributions for the charitable purposes of 'Natural Rhythm' must be made payable to Fractured Atlas only and are tax-deductible to the extent permitted by law.\n'Natural Rhythm' is sponsored by the Undergraduate Research Office of Carnegie Mellon University in the form of a Summer Undergraduate Research Fellowship."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:a9723144-5a01-4929-8339-d3270b8c7c95>","<urn:uuid:e23e9dcd-fb9a-4bfd-8288-7d57633bba3b>"],"error":null}
{"question":"How do crowdsourcing techniques in cultural institutions compare to citizen science initiatives in environmental monitoring in terms of their approaches to public participation and data collection?","answer":"Both approaches engage the public in contributing valuable data but have distinct implementation methods. Cultural institutions like libraries and museums use crowdsourcing to gather new knowledge through activities like transcription, description, and categorization of cultural heritage resources. They focus on creating platforms for audience engagement and deep connection with cultural heritage through online collaboration. Meanwhile, environmental monitoring initiatives like Namibia's EIS use citizen science through specific tools like the Atlasing program, where participants record and monitor biodiversity by entering plant and animal sightings via website or mobile apps. Both systems aim to democratize access to information and rely on public participation to gather large amounts of data, but they operate in different domains - cultural heritage versus environmental science - and utilize different specific tools and methodologies for data collection and validation.","context":["‘This webinar will explore crowdsourcing techniques used increasingly by organizations and institutions seeking to gather vast amounts of new knowledge and participation from online contributors.\nCrowdsourcing techniques are increasingly being utilized by organizations and institutions—including libraries and museums—seeking to gather vast amounts of new knowledge and participation from online contributors. In this fast-paced hour-long introduction, you’ll get a handle on “Crowdsourcing Fundamentals” from leading voice in the field Mia Ridge, along with first-person accounts from two exemplar crowdsourcing projects (NYPL, Zooniverse). Learn the basics about implementing crowdsourcing techniques, securing funding, engaging users, and assessing the quality of crowdsourced data, as well as the advantages and challenges of utilizing crowdsourcing.\nThis webinar is part of the newly formed Crowdsourcing Consortium for Libraries and Archives (CCLA). Funded by the Institute of Museum and Library Services (IMLS), the goal of CCLA is to forge national/international partnerships to advance the use of crowdsourcing technologies, tools, user experiences, and platforms to help libraries, museums, archives, and more.’\nSlides, video and chat notes are available on the OCLC’s page.\nMuch of this comes from my PhD research and my previous work in museums, and I’m grateful to everyone who’s commented in person or on twitter so far, particularly as it helps me understand the best ways to explain the Participatory Commons and the research underlying it for different audiences.\nRidge, Mia; Lafreniere, Don and Nesbit, Scott (2013). Creating deep maps and spatial narratives through design. International Journal of Humanities and Arts Computing, 7(1-2) pp. 176–189.\nAbstract: An interdisciplinary team of researchers were challenged to create a model of a deep map during a three-day charette at the NEH Institute on Spatial Narratives and Deep Maps. Through a reflexive process of ingesting data, probing for fruitful research questions, and considering how a deep map might be used by different audiences, we created a wireframe model of a deep map and explored how it related to spatial narratives. We explored the tension between interfaces for exploratory and structured views of data and sources, and devised a model for the intersections between spatial narratives and deep maps. The process of creating wireframes and prototype screens—and more importantly, the discussions and debates they initiated—helped us understand the complex requirements for deep maps and showed how a deep map can support a humanistic interpretation of the role of space in historical processes.\nAbstract: Crowdsourcing, or “obtaining information or services by soliciting input from a large number of people,” is becoming known for the impressive productivity of projects that ask the public to help transcribe, describe, locate, or categorize cultural heritage resources. This essay argues that crowdsourcing projects can also be a powerful platform for audience engagement with museums, offering truly deep and valuable connection with cultural heritage through online collaboration around shared goals or resources. It includes examples of well-designed crowdsourcing projects that provide platforms for deepening involvement with citizen history and citizen science; useful definitions of “engagement”; and evidence for why some activities help audiences interact with heritage and scientific material. It discusses projects with committed participants and considers the role of communities of participants in engaging participants more deeply.\nI was asked to share some of the lessons I’ve learnt from building digital participation projects in museums and from my research on crowdsourcing in cultural heritage for the London Museums Group blog following my talk at their “Museums and Social Media” event on 24 May at Tate Britain.\nThey were published at ‘Tips for digital participation, engagement and crowdsourcing in museums by Mia Ridge‘, but as the site doesn’t seem to be loading I’ve re-posted it below. I think most of what I wrote then holds up, but today I’d add a third bonus tip – plan to ingest the results of your crowdsourcing tasks into whatever internal systems are necessary to appropriately integrate and re-share the enhanced or new data.\nTo pinch from my headings, I discuss the advantages of digital engagement; challenges for museums – new relationships, new authorities, dissolving boundaries; 6 tips for designing digital participation experiences in museums; 2 bonus tips for designing crowdsourcing projects in museums.\nSite abstract: “Museums have increasingly been joining the global movement for open data by opening up their databases, sharing their images and releasing their knowledge. Mia Ridge presents a brief history of open cultural data projects, explores some reasons why some data is relatively under-used and looks to the future of open cultural data”.","There are few countries in the world that have better access to environmental information all in one place than Namibia. Indeed, in this regard we are the envy of many countries, including so-called developed or industrialised nations. And it all happens through Namibia’s environmental one-stop-shop, the Environmental Information System (EIS)\nAbout 15 years ago, four friends and colleagues – environmental scientists, programmers and IT specialists – decided to set up an electronic, web-based environmental information system and service for Namibia. With the support of the IT, Internet and web provider company Paratus, the EIS was born. Over the years, as small amounts of funding were secured to invest in the system, the EIS slowly grew in size and significance. It started with an electronic, on-line library. Today, this is Namibia’s largest, most comprehensive and accessible library of environmental material – and it is free to anyone in the world who has access to the Internet.\nA key objective of the EIS was to democratise access to environmental information. Whether you are a student, researcher, conservation manager, policymaker, learner or an interested member of the public, you can access the eLibrary from Windhoek, Warmbad, Katima Mulilo or anywhere in the world, including on your smartphone. Another feature of the eLibrary is that, in addition to published articles, it contains a large amount of “grey” literature: important reports, data sets, theses, presentations and other material that is not otherwise readily accessible.\nThe eLibrary has a user-friendly search interface. If you are interested, for example, in Climate Change, you simply enter those words in the search box and a list of all relevant reports and publications will be produced. You can then click on the articles that interest you and read them online or download them for free. If a listed article is not on the EIS, it will direct you via a link to the journal that published the article.\nThe EIS also has an easy Upload function so that you can submit your publications and reports into the system. They will be checked (to screen out inappropriate material such as advertising), key worded and loaded. This is one of the most effective ways of getting your work widely read and known, as the eLibrary is used by researchers, students, NGO and government staff, the private sector and more, both within Namibia and internationally.\nThe second component added to the EIS was a “citizen science” public-participation altasing programme to record and monitor Namibia’s biodiversity (see screenshot below). You can enter plant and animal sightings via the website or the Atlasing in Namibia App (available on Play Store for Android devices and here for iOS devices). It currently has sections for recording mammals, reptiles, amphibians, butterflies, invasive alien plants and breeding birds.\nBy bringing Namibia's biodiversity data together, we can enhance their value and usefulness. Comparisons become possible across space and time, between and across species. To this end, several substantial datasets have been incorporated into the Atlasing system. Data from the system are freely available and have been used for many purposes including the upcoming Red Data book on Namibian carnivores.\nOther components of the EIS include a photo library of landscapes in Namibia to monitor changes over time, a birds and powerlines interactive tool and the Cuvelai-Etosha river basin interactive tool.\nThe latest addition to the EIS is a scientific journal, the Namibian Journal of Environment, which publishes peer-reviewed scientific articles as well as editor-reviewed articles that facilitate quick publication of field notes and related observations.\nAs Namibia’s one-stop-shop for the environment, the EIS provides 1) an easy-to-use platform for submitting environmental information; 2) a facility where this important information is curated and managed; 3) a place where scientists and others can submit papers and observations for publication; and 4) a place to go for all your environmental information needs. After 15 years of development and growth, we are pleased to present the EIS as an important tool to promote wise environmental management in Namibia – please use it."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:887eb44a-621e-43fa-ab27-2d4e7726a2e8>","<urn:uuid:79fc417d-87f1-4509-b679-49922924f42e>"],"error":null}
{"question":"I want to support local farmers through my clothing choices. What are the environmental benefits of organic farming methods, and how do they connect to sustainable fashion production?","answer":"Organic farming provides several environmental benefits, including protecting the environment by minimizing soil degradation and erosion, decreasing pollution, and optimizing biological productivity. It promotes biological diversity, recycles materials, and relies on renewable resources in locally organized systems. For sustainable fashion production, these organic farming practices help eliminate pesticides and toxic chemicals used in processing clothing. The organic clothing production process has a lower carbon footprint as it consumes less fuel and energy and emits fewer greenhouse gases. Additionally, eco-friendly processing helps reduce water and electric use and toxic runoff by using non-chlorine bleach, silicon-free softeners and low-impact dyes.","context":["Introduction to Organic Farming\nOrganic farming is a method of crop and livestock production that involves much more than choosing not to use pesticides, fertilizers, genetically modified organisms, antibiotics and growth hormones.\nOrganic production is a holistic system designed to optimize the productivity and fitness of diverse communities within the agro-ecosystem, including soil organisms, plants, livestock and people. The principal goal of organic production is to develop enterprises that are sustainable and harmonious with the environment.\nThe general principles of organic production, from the Canadian Organic Standards (2006), include the following:\n- protect the environment, minimize soil degradation and erosion, decrease pollution, optimize biological productivity and promote a sound state of health\n- maintain long-term soil fertility by optimizing conditions for biological activity within the soil\n- maintain biological diversity within the system\n- recycle materials and resources to the greatest extent possible within the enterprise\n- provide attentive care that promotes the health and meets the behavioural needs of livestock\n- prepare organic products, emphasizing careful processing, and handling methods in order to maintain the organic integrity and vital qualities of the products at all stages of production\n- rely on renewable resources in locally organized agricultural systems\nOrganic farming promotes the use of crop rotations and cover crops, and encourages balanced host/predator relationships. Organic residues and nutrients produced on the farm are recycled back to the soil. Cover crops and composted manure are used to maintain soil organic matter and fertility. Preventative insect and disease control methods are practiced, including crop rotation, improved genetics and resistant varieties. Integrated pest and weed management, and soil conservation systems are valuable tools on an organic farm. Organically approved pesticides include “natural” or other pest management products included in the Permitted Substances List (PSL) of the organic standards. The Permitted Substances List identifies substances permitted for use as a pesticides in organic agriculture. All grains, forages and protein supplements fed to livestock must be organically grown.\nThe organic standards generally prohibit products of genetic engineering and animal cloning, synthetic pesticides, synthetic fertilizers, sewage sludge, synthetic drugs, synthetic food processing aids and ingredients, and ionizing radiation. Prohibited products and practices must not be used on certified organic farms for at least three years prior to harvest of the certified organic products. Livestock must be raised organically and fed 100 per cent organic feed ingredients.\nOrganic farming presents many challenges. Some crops are more challenging than others to grow organically; however, nearly every commodity can be produced organically.\nThe world market for organic food has grown for over 15 years. Growth of retail sales in North America is predicted to be 10 per cent to 20 per cent per year during the next few years. The retail organic food market in Canada is estimated at over $1.5 billion in 2008 and $22.9 billion in the U.S.A. in 2008. It is estimated that imported products make up over 70 per cent of the organic food consumed in Canada. Canada also exports many organic products, particularly soybeans and grains.\nThe Canadian Organic Farmers reported 669 certified organic farms in Ontario in 2007 with over 100,000 certified organic acres of crops and pasture land. This is an annual increase of approximately 10 per cent per year in recent years. About 48 per cent of the organic cropland is seeded to grains, 40 per cent produces hay and pasture and about five per cent for certified organic fruits and vegetables. Livestock production (meat, dairy and eggs) has also been steadily increasing in recent years.\nThe main reasons farmers state for wanting to farm organically are their concerns for the environment and about working with agricultural chemicals in conventional farming systems. There is also an issue with the amount of energy used in agriculture, since many farm chemicals require energy intensive manufacturing processes that rely heavily on fossil fuels. Organic farmers find their method of farming to be profitable and personally rewarding.\nConsumers purchase organic foods for many different reasons. Many want to buy food products that are free of chemical pesticides or grown without conventional fertilizers. Some simply like to try new and different products. Product taste, concerns for the environment and the desire to avoid foods from genetically engineered organisms are among the many other reasons some consumers prefer to buy organic food products. In 2007 it was estimated that over 60 per cent of consumers bought some organic products. Approximately five per cent of consumers are considered to be core organic consumers who buy up to 50 per cent of all organic food.\n“Certified organic” is a term given to products produced according to organic standards as certified by one of the certifying bodies. There are several certification bodies operating in Ontario. A grower wishing to be certified organic must apply to a certification body requesting an independent inspection of their farm to verify that the farm meets the organic standards. Farmers, processors and traders are each required to maintain the organic integrity of the product and to maintain a document trail for audit purposes. Products from certified organic farms are labelled and promoted as “certified organic.”\nIn June 2009, the Canadian government introduced regulations to regulate organic products. Under these regulations the Canadian Food Inspection Agency (CFIA) oversees organic certification, including accreditation of Conformity Verification Bodies (CVBs) and Certification Bodies (CBs). This regulation also references the Canadian Organic Production Systems General Principles and Management Standards (CAN/CGSB-32.310) and the Organic Production Systems – Permitted Substances List that were revised in 2009.\nThe Canadian organic regulations require certification to these standards for agricultural products represented as organic in import, export and inter-provincial trade, or that bear the federal organic agricultural product legend or logo. (Figure 1) Products that are both produced and sold within a province are regulated by provincial organic regulations where they exist (Quebec, British Columbia and Manitoba).\nFigure 1. Canadian Agriculture Product Legend (logo)\nThe federal regulations apply to most food and drink intended for human consumption and food intended to feed livestock, including agricultural crops used for those purposes. They also apply to the cultivation of plants. The regulations do not apply to organic claims for other products such as aquaculture products, cosmetics, fibres, health care products, fertilizers, pet food, lawn care, etc.\nFood products labelled as organic must contain at least 95 per cent organic ingredients (not including water and salt) and can bear the Canada Organic logo. Multi-ingredient products with 70 per cent to 95 per cent organic product content may be labelled with the declaration: “% organic ingredients”. Multi-ingredient products with less than 70 per cent organic content may identify the organic components in the ingredient list.\nExported products must meet the requirements of the importing country or standards negotiated through international equivalency agreements. Products exported to the U.S. must meet the terms of the Canada-U.S. equivalency agreement signed in June 2009. All products that meet the requirements of the Canada Organic Regime can be exported to the U.S. with the exception that agricultural products derived from animals treated with antibiotics cannot not be marketed as organic in the U.S. Canada is also exploring other international equivalency agreements with other trading partners to enhance trade opportunities for export and to assure the organic integrity of imported products.\nWhen considering organic certification, know the requirements and accreditation(s) needed in the marketplace where your products will be sold. When comparing certification bodies, make sure they have the certification requirements and accreditations needed to meet market requirements. As a minimum certification bodies should be accredited under the Canadian Organic Products Regulations. Some markets may require accreditation or equivalency agreements with countries in the European Union, or with the Japanese Agricultural Standard (JAS), Bio-Swisse or other international organic certification systems. As Canada develops international equivalency agreements the need for the certification body to have these international accreditations will diminish.\nFor more information on certification and links to Canadian regulations and standards see the Organic Agricultural section of the OMAFRA website at www.ontario.ca/organic or the CFIA website at www.inspection.gc.ca.\nThe first few years of organic production are the hardest. Organic standards require that organic lands must be managed using organic practices for 36 months prior to harvest of the first certified organic crop. This is called the “transition period” when both the soil and the manager adjust to the new system. Insect and weed populations also adjust during this time.\nCash flow can be a problem due to the unstable nature of the yields and the fact that price premiums are frequently not available during the transition since products do not qualify as “certified organic.” For this reason, some farmers choose to convert to organic production in stages. Crops with a low cost of production are commonly grown during the transition period to help manage this risk.\nCarefully prepare a plan for conversion. Try 10 per cent to 20 per cent the first year. Pick one of the best fields to start with and expand organic acreage as knowledge and confidence are gained. It may take five to 10 years to become totally organic, but a long term approach is often more successful than a rapid conversion, especially when financial constraints are considered. Parallel production (producing both organic and conventional versions of the same crop or livestock product) is not allowed. Use good sanitation, visually different varieties, individual animal identification and other systems to maintain separation and integrity of the organic and conventional products. Good records are essential.\nIn organic production, farmers choose not to use some of the convenient chemical tools available to other farmers. Design and management of the production system are critical to the success of the farm. Select enterprises that complement each other and choose crop rotation and tillage practices to avoid or reduce crop problems.\nYields of each organic crop vary, depending on the success of the manager. During the transition from conventional to organic, production yields are lower than conventional levels, but after a three to five year transition period the organic yields typically increase.\nCereal and forage crops can be grown organically relatively easily to due to relatively low pest pressures and nutrient requirements. Soybeans also perform well but weeds can be a challenge. Corn is being grown more frequently on organic farms but careful management of weed control and fertility is needed. Meeting nitrogen requirements is particularly challenging. Corn can be successfully grown after forage legumes or if manure has been applied. Markets for organic feed grains have been strong in recent years.\nThe adoption of genetically engineered (GMO) corn and canola varieties on conventional farms has created the issue of buffer zones or isolation distance for organic corn and canola crops. Farmers producing corn and canola organically are required to manage the risks of GMO contamination in order to produce a “GMO-free” product. The main strategy to manage this risk is through appropriate buffer distances between organic and genetically engineered crops. Cross-pollinated crops such as corn and canola require much greater isolation distance than self-pollinated crops such as soybeans or cereals.\nFruit and vegetable crops present greater challenges depending on the crop. Some managers have been very successful, while other farms with the same crop have had significant problems. Certain insect or disease pests are more serious in some regions than in others. Some pest problems are difficult to manage with organic methods. This is less of an issue as more organically approved biopesticides become available. Marketable yields of organic horticultural crops are usually below non-organic crop yields. The yield reduction varies by crop and farm. Some organic producers have added value to their products with on-farm processing. An example is to make jams, jellies, juice, etc. using products that do not meet fresh market standards.\nLivestock products can also be produced organically. In recent years, organic dairy products have become popular. There is an expanding market for organic meat products. Animals must be fed only organic feeds (except under exceptional circumstances). Feed must not contain mammalian, avian or fish by-products. All genetically engineered organisms and substances are prohibited. Antibiotics, growth hormones and insecticides are generally prohibited. If an animal becomes ill and antibiotics are necessary for recovery, they should be administered. The animal must then be segregated from the organic livestock herd and cannot be sold for organic meat products. Vaccinations are permitted when diseases cannot be controlled by other means. Artificial insemination is permitted. Always check with your certification body to determine if a product or technique is allowed in the Permitted Substances List and the organic standards. Organic production must also respect all other federal, provincial and municipal regulations.\nOrganic produce can usually qualify for higher prices than non-organic products. These premiums vary with the crop and may depend on whether you are dealing with a processor, wholesaler, retailer or directly with the consumer. Prices and premiums are negotiated between buyer and seller and will fluctuate with local and global supply and demand.\nHigher prices offset the higher production costs (per unit of production) of management, labour, and for lower farm yields. These differences vary with commodity. Some experienced field crop producers, particularly of cereals and forages, report very little change in yield while in some horticultural crops such as tree fruits, significant differences in marketable yield have been observed. There may also be higher marketing costs to develop markets where there is less infrastructure than for conventional commodities. Currently, demand is greater than supply for most organic products.\nOrganic farming can be a viable alternative production method for farmers, but there are many challenges. One key to success is being open to alternative organic approaches to solving production problems. Determine the cause of the problem, and assess strategies to avoid or reduce the long term problem rather than a short term fix for it.\nCOG – Canadian Organic Growers Inc.\n323 Chapel St., Ottawa ON K1N 7Z2\nPhone: (613) 216-0741, 1-888-375-7383\nEFAO – Ecological Farmers Association of Ontario\n5420 Highway 6 North,\nRR 5, Guelph, ON N1H 6S2\nPhone: (519) 822-8606\nOMAFRA – Ontario Ministry of Agriculture, Food and Rural Affairs\n1 Stone Road W., Guelph, ON N1G 4Y2\nAgr. Information Contact Centre\nOACC- Organic Agricultural Centre of Canada\nNova Scotia Agricultural College\nBox 550, Truro, Nova Scotia, B2N 5E3\nPhone: (902) 893-7256, Fax: (902) 893-3430\nGuelph Organic Conference\nFor information contact:\nTomás Nimmo, Box 116,\nCollingwood, ON L9Y 3Z4\nPhone: (705) 444-0923, Fax (705) 444-0380\nOCO - Organic Council of Ontario\nRR 5 Guelph, ON N1H 6J2\nPhone: (519) 827-1221, Fax: (519) 827-0721\n© Queen's Printer for Ontario, 2015\nThe information on this page was written and copyrighted by the Government of Ontario. The information is offered here for educational purposes only and materials on this page are owned by the Government of Ontario and protected by Crown copyright. Unless otherwise noted materials may be reproduced for non-commercial purposes. The materials must be reproduced accurately and the reproduction must not be represented as an official version. As a general rule, information materials may be used for non-profit and personal use.","Organic clothing, fashioned from natural fibers, by local farmers and indigenous groups around the world\nA sample of Suzanne MacFadyen's most recent organic, sustainable clothing line hangs at Arrowhead Clothing in Portland.\nThe pertinent style of clothing varies for each geographical area due to personal and cultural preferences, religious obligation, and the obvious, climate. However, while clothing may vary, it is nevertheless a necessity for human survival. The importance of organic clothing lies in that it would help eliminate potential pesticides and the toxic chemicals that are used in processing clothing. It would create more job opportunities on land where material is grown and/or animals are herded and in the businesses making threads, dyeing the clothing, making cloth, and finishing garments. By choosing to wear organic clothing one can support local farmers, indigenous and cultural groups involved in creating these fabrics and pieces.\nAccording to the US National Organic Standards Board's National Organic Program, \"organic\" is defined as agricultural products produced in accordance with Organic Foods Production Act and the NOP Regulations. The principal guidelines for organic production are to use materials and practices that enhance the ecological balance of natural systems and that integrate the parts of the farming system into an ecological whole.Organic agricultural practices cannot ensure that products are completely free of residues (pesticides can drift over from nearby fields), but methods are used to minimize pollution from air, soil and water (Organic Trade Association). The primary goal of organic agriculture is to optimize the health and productivity of interdependent communities of soil life, plants, animals, and people (Organic Trade Association).\n- 1 Context within NORA\n- 2 Understanding Current Patterns of Abundance and Scarcity\n- 3 Approaches to Creating Greater Abundance\n- 4 Links and Stories\n- 5 References\nRelationships to Needs\nWearing clothing is not only enforced by law, but also by societal and cultural norms. It is essential for a number of reasons, including that it provides security by protecting individuals from physical harm like weather and in most cases from emotional and mental harm like ridicule from peers; clothing can even hide body dimorphism giving peace of mind and security. In general, clothing allows self-expression that can contribute to a contemplative/spiritual connection with the self. A meaningful livelihood can take the form of the production of organic fibers, or a job in the fashion industry. Also, by becoming involved in the field of organic fashion, one not only becomes an active participant in economic decision-making about the sustainable use of resources, but also finds new opportunities to learn from those who are experienced. For example, an individual may work with a designer and receive first hand experience, then later apply newly gained knowledge to their own work thereby creating a new product or design for others.\nRelationships to Organizational Forms\nThe producers of fibers, yarn, cloth, and finished garments may all sell their products either through individual sales or through committed services or sales (if there is a long-term contract between producers and their commercial buyers). However, final sale to the customers is usually through individual sales. Immediately following the production of goods, sellers need to travel either locally or to other parts of the world to showcase and sell the product. Most everyone involved in the process of creating organic clothing, i.e. local farmers, indigenous groups, small enterprises, can negotiate over prices while being in charge of producing and maintaining their product.\nGroups of producers can also self-provisioning at the level of a household or in the context of community solidarity (where different members of a local community or neighborhood are involved in different parts of the production process). Rebecca Burgess is just one example of a woman who has grown and designed her own wardrobe with the help of individuals within a 150 mile radius of her home in Northern California. Her story is one instance where this sort of self-provisioning and community responsibility takes place. If there is a high degree of local self-sufficiency, individuals could opt to cease to manufacture goods for sale.\nOrganic farm production involves the management of natural resources such as sheep, and the land and water needed to rear them.\nOnce knowledge about the products spread and distribution methods are developed, networks may be established between friends or between individuals involved in organic clothing production and use. It is important however to prevent the emergence of power imbalances and hierarchies, for example by avoiding the creation of bottlenecks that can be used to control the system.\nRelationships to Resources\nThe resources necessary for creating organic clothing come from the land, minerals, living things, and intangibles. The land provides grazing area for animals as well as land for crops like cotton. Water is essential for both agriculture and in the manufacturing processes (for example, for dyeing). Minerals may be used to create pigments and decorative designs on clothing as well as be imitated to produce color. Living things like crop plants and animals are central to organic clothing. For example, sheep's wool is essential in a lot of clothing. Cow hide may also be used as a trim, and organic cotton can be the replacement for regular cotton. There are countless resources that can be obtained from living things. Lastly, intangibles like knowledge of craftsmanship, patterns and designs, are the basis for the creation of clothing. Without intangibles there would be no clothing. Needless to say, no resource can be compromised. Land provides nutrition, minerals decorate the clothing, living things are what make the clothing, and intangibles place all the material together to create the final product.\nUnderstanding Current Patterns of Abundance and Scarcity\nWhy Choose Organic Clothing?\nOpting to buy clothing from retailers who embrace sustainable options such as organic or fair trade products can eliminate many of the issues with nonorganic clothing. A few of the issues with nonorganic clothing, as explained in more detail by the Forum for the Future, are: the use of pesticides and toxic chemicals, inefficient water use in cotton production, and serious human rights abuses. In order, the world's cotton farmers use around 2 billion dollars of chemical pesticides, of which at least 819 million dollars worth are classified as 'hazardous' by the World Health Organization. Although there are huge variations between and within countries in pesticide use per acre, on average, almost 1 kilogram of pesticides is applied for every hectare under cotton. A recent study revealed that the average Indian cotton farmer suffers three instances of pesticide poisoning over a single season, sometimes leading to death. Secondly, because cotton is a thirsty crop the Aral Sea in central Asia, once the world's 4th largest inland body of water, has been depleted to 15 percent of its former volume. Lastly, in states like Uzbekistan where cotton production is controlled, forced labor, unfairly low wages and violence, imprisonment, and intimidation for everyone attempting to leave the factory tends to be common.\nOne of the strengths is the relationship to resources. Because organic clothing comes from the land, minerals, and living things it is fairly accessible. In addition, these materials can be available, as shown by Rebecca Burgess, within a 150 mile radius of the home. However, if transportation and trade relationships are planned effectively, buying or selling goods overseas can promote beneficial interactions between different individuals and communities. Another strength is the independence from the larger manufacturers, allowing for self-provisioning. Therefore, an individual can control when to begin or cease the manufacturing of their goods; an individual who is independent from the larger manufacturers is also able to control costs and the process of how organic clothing is made – eliminating the chance for clothing to be wrongly processed or contaminated by pesticides.That being said, eco-friendly processing does not compromise workers' health and helps reduce water and electric use and toxic runoff, e.g. non-chlorine bleach, silicon-free softeners and low impact, azo-free dyes (Hae Now). Strict testing also ensures the absence of contaminants like nickel, lead, formaldehyde, amines, pesticides, and heavy metals (Hae Now). In general, manual farming and organic practices have a lower carbon footprint as the entire process consumes less fuel and energy and emits fewer greenhouse gases (Hae Now).\nThe promotion of organic clothing also has a few weaknesses. An issue that may arise is inadequate knowledge of individual societies, specifically about the craftsmanship, patterns, and design – the basis for the creation of clothing. Because each society is unique in its geographic area, climate, religious obligation, and societal and cultural norms it can create limitations on material, available methods to fabricate clothing, and of course on resources. Also, organic farming can involve higher costs because it may be more labor intensive. However, the amount of product being produced is also important in that lower volume of production in a commodity chain can lead to increased transaction costs per item throughout the commodity chain, and thus increase costs while restricting the diversity of products.\nBarriers to the Success of Organic Clothing\nOrganic clothing is limited by the short supply of organic product that is certified by national or international organic certification boards (e.g., in the US, the US Department of Agriculture's accreditation program, which requires that all organic fabrics follow the National Organic Program Regulations). Therefore,organic clothing is only sold in a few places where the demand is strong. Because supply is limited, organic clothing is often times sold online, or in small scale boutiques which are not easily accessible.\nCosts of organic farming\nOrganic fiber production suffers from similar constraints as the organic production of food crops, which is that industrial farmers do not pay for the damage they cause to the commons we all share: the water, the atmosphere (modern farming adds more carbon dioxide to the atmosphere than it absorbs), the soils (which are damaged for future generations), the ecosystem (damaged by excessive use of agrochemicals), and human health. Organic farmers however have to take more care of their land and have to pay for their certification as \"organic,\" which leads to costs industrial farmers do not have to pay. This puts organic farmers at a disadvantage in the marketplace.\nAccording to the Organic Trade Association's 2012 Press Release, the \"cost per acre to grow organic cotton ranged from $350/acre to $650/acre, with an average cost/acre of $440. Most survey respondents reported receiving $1.50 per pound for organic cotton, with prices ranging from as high as $2.40…or to a low of $1.35\" (Organic Trade Association). Compared to organic cotton, the National Cotton Council of America reports industrial cotton, between the years of 2009-2011, to range from $364.37/acre to $639.88/acre; with dollars per pound to go as low as $0.59 per pound and as high as $0.92 per pound.\nThus, the high cost of organic cotton is a major reason for failure in the market. However, that is not to say that standard cotton is inexpensive. Both organic and standard cotton must be fertilized and transported-and most recently there has been an increase in transport costs due to an increase in oil prices (Organic Cotton Prices). Yet, organic cotton eliminates the need for pesticides, therefore eliminating one cost.\nProblems of Distribution\nThe commodity chains for organic clothing in many cases have yet to be organized. This means that there is a lot of work involved in simply establishing market relationships for all aspects of the business, competing with the conventional clothing business, where such relationships already exist, and are negotiated for huge amounts of garments at a time. Therefore, organic clothing does not enjoy the economies of scale found in most clothing distribution, even for high-end fashion items.\nAs a result, there is limited variation or diversity among products. Consumers want the next best thing, and they want it to be unique, just like the desire to have the latest gadgets and electronics, and having access to the coolest and newest IT clubs. This all becomes necessary to achieve social status. That said, variation allows multiple people to have the same product yet maintain that they are different and \"hard to get\". The \"boutique\" nature of organic cotton, and organic products in general, could be used as a selling point (though in a niche market). It means that no other person is likely to wear the same product, and while this creates the diversity that some people are searching for it also explains why organic cotton is more expensive than industrially grown cotton – the small production.\nDue to the limitations in the production line, small producers like DeLorme and MacFadyen, owners of Brook There and the clothing line, Arrowhead Clothing, are limited to what mills are offering off the shelf. While it forces them to be creative it creates a number of limitations. Large brands often times support companies who attempt to promote a cause by using organic clothing, but even they are subject to an increase in costs, especially market costs, when continuing accounts whose prices have risen on merchandise (Lapowsky). This causes companies to drop brands.\nHowever, authors of Fashioning Sustainability: A Review of the Sustainability Impacts of the Clothing Industry, argue that besides the above reasons, there are several other issues that prohibit the success of sustainable fashion:\n- Fashion consumption – the increasing number of fashion items that we buy and then dispose of.\n- The intensity of cotton production requiring lots of energy, water and pesticides.\n- Working conditions across the supply chain from cotton production to sweatshops (Murray).\nApproaches to Creating Greater Abundance\nBecoming culturally aware, therefore knowing where certain types of materials exist and likewise what patterns of craftsmanship take place in those areas.\nRecognizing sustainable resources, and purchasing products made of these resources so that there is less environmental destruction\nRefashion or reuse clothing by purchasing from second hand stores. Also possible is to take old clothing and style them into new pieces by cutting and designing a new style.\nEstablishment of small farms, specifically where textile fibers can be grown and animals like sheep herded\nTeaching of craftsmanship, designs and patterns\nPotential ways to lessen or solve some of these weaknesses include: offering workshops like those provided by Rebecca Burgess in her community that would be able to teach young and old about restoration of resources, mass production by networking and slowly expanding can help increase production, and beginning small then, funding allowed, increase number of individuals farming can – while momentarily increasing labor costs – will also increase production if networking is successful.\nReplacement of cotton with either organic cotton or a natural material like hemp. \"Hemp has many redeeming qualities…it is four times stronger than cotton, twice as resistant to abrasion, and more resistant to mildew, soiling, shrinkage and fading in the sun. In addition, hemp plants need little irrigation and significantly less pesticide or other chemicals\" (Forum for the Future). According to the HIA, Hemp Industries Association, Hemp legislation is planning to be introduced in Colorado to help create regulations for hemp farming. \"Canada on the other hand has reestablished their hemp industry in 1998 and and it has grown over the past 15 years from just over 5,000 acres to over 50,000 acres licensed for hemp cultivation in 2012\" (HIA).\nStandards for Fairtrade cotton that would help ensure a fair price for cotton producers, therefore somewhat lessen the overall cost of cotton (Forum for the Future).\nLinks and Stories\nEileen Fisher, designer of one of the most well known fashion lines in America, speaks out about simplistic wardrobe collaborations and what her company is doing for the environment and human rights.\nEileen Fisher's website for information regarding her stand for the environment, with links to organic cotton, organic linen, hemp, recycled fibers, and much more.\nA Successful, Organic Clothing Line : An article proving that organic clothing is not only beautiful, but obtainable. Whether through purchase or self creation.\nThe Fibershed Project : A clip about one woman, Rebecca Burgess, who has grown and designed her wardrobe within 150 miles of her home. She calls her project the \"Fibershed Project\" because like a foodshed or watershed, the 150 mile radius of her home is big enough to provide for all the fibers and dyes necessary to create a diverse wardrobe.\nThe Fibershed Marketplace : A website for individuals wanting to support The Fibershed Project. On the website individuals have the ability to purchase handmade goods or fabric and accessories to make their own clothing.\nFashion Futures 2025 : Is a call for a sustainable fashion industry that would provide clothing in say a climate shift or in a future where there is a possibility of water shortages. The project is also part of a larger website similar to the commons.\n150 mile wardrobe: local fiber, real color, Gandhi economy. (December 10 2013).\nMore information regarding Rebecca Burgess, creator of the Fibershed Project- a 150 mile wardrobe.\nDay, K. (August 5 2012). Maine women finding success with organic, sustainable clothing lines.\nForum for the Future (Stephanie Draper, Vicki Murray,and Ilka Weissbrod). (March 2007). Fashion Sustainability.\nHae Now: Organic Tree. (October 3 2013).\nHIA: About the HIA: Mission & Goals. (October 8 2013). The Hemp Industries Association (HIA) is a membership-based nonprofit trade group, which represents the interests of the hemp industry and encourages the research and development of new products made from industrial hemp, the low-THC oilseed and fiber varieties of Cannabis. The organization formed in 1994 and is the only U.S. nonprofit trade group representing actual hemp businesses.\nLapowsky, I. (2012). United By Blue's eco-friendly values sent costs soaring.Inc, 34(3), 99-101.\nNational Cotton Council of America. (19 December 2013).\nOrganic Cotton Prices. (5 December 2013).\nOrganic Trade Association. (October 8 2013).\nComment posting has been disabled on this doc."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:e6ddb9b3-68e1-422b-adc0-49ee5dd47471>","<urn:uuid:b2e3a774-2bb6-4ad6-8086-6272a04d9646>"],"error":null}
{"question":"How do the scientific approaches to timing in chronopharmacology compare to timing research in human dynamics models for social networks?","answer":"Chronopharmacology studies optimal timing for medication administration as a branch of chronobiology, taking a systematic scientific approach to biological rhythms. In contrast, human dynamics models for social networks focus on analyzing temporal patterns of human behavior and interactions, using mathematical models to study bursts and heavy-tailed statistics in human activities. While both fields examine timing, chronopharmacology is more focused on biological rhythms and medical applications, whereas human dynamics modeling emphasizes statistical patterns and mathematical modeling of social behavior.","context":["- About this Journal ·\n- Abstracting and Indexing ·\n- Aims and Scope ·\n- Annual Issues ·\n- Article Processing Charges ·\n- Articles in Press ·\n- Author Guidelines ·\n- Bibliographic Information ·\n- Citations to this Journal ·\n- Contact Information ·\n- Editorial Board ·\n- Editorial Workflow ·\n- Free eTOC Alerts ·\n- Publication Ethics ·\n- Reviewers Acknowledgment ·\n- Submit a Manuscript ·\n- Subscription Information ·\n- Table of Contents\nDiscrete Dynamics in Nature and Society\nVolume 2012 (2012), Article ID 678286, 13 pages\nA Hybrid Human Dynamics Model on Analyzing Hotspots in Social Networks\n1Beijing Key Laboratory of Intelligent Telecommunications Software and Multimedia, Beijing University of Posts and Telecommunications (BUPT), Beijing, China\n2Chongqing Engineering Laboratory of Internet and Information Security, Chongqing University of Posts and Telecommunications (CQUPT), Room 4029, No. 2 Chongwen Road, Nanan District, Chongqing 400065, China\n3School of Computer and Communication Sciences, Swiss Federal Institute of Technology (EPFL), Lausanne, Switzerland\nReceived 5 July 2012; Accepted 12 September 2012\nAcademic Editor: Garyfalos Papaschinopoulos\nCopyright © 2012 Yunpeng Xiao et al. This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.\n- F. A. Haight, Handbook of the Poisson distribution, John Wiley & Sons, New York, 1967.\n- A. L. Barabási, “The origin of bursts and heavy tails in human dynamics,” Nature, vol. 435, no. 7039, pp. 207–211, 2005.\n- Z. Dezso, E. Almaas, A. Lukacs, B. Racz, I. Szakadat, and A.-L. Barábasi, “Dynamics of information access on the web,” Phys Rev E, vol. E73, Article ID 066132, 2006.\n- W. Hong, X. P. Han, T. Zhou, and B. H. Wang, “Heavy-tailed statistics in short-Message communication,” Chinese Physics Letters, vol. 26, no. 2, Article ID 028902, 2009.\n- Q. Yan, L. Yi, and L. Wu, “Human dynamic model co-driven by interest and social identity in the microblog community,” Physica A, vol. 391, pp. 1540–1545, 2012.\n- J. Yu, Y. Hu, M. Yu, and Z. Di, “Analyzing netizens' view and reply behaviors on the forum,” Physica A, vol. 389, no. 16, pp. 3267–3273, 2010.\n- T. Zhou, H. Kiet, B. Kim, et al., “Role of activity in human dynamics,” Europhysics Letters, vol. 82, no. 2, pp. 28002–28006, 2008.\n- A. Vázquez, J. G. Oliveira, Z. Dezsö, K. I. Goh, I. Kondor, and A. L. Barabási, “Modeling bursts and heavy tails in human dynamics,” Physical Review E, vol. 73, no. 3, Article ID 036127, pp. 1–19, 2006.\n- A. Vázquez, “Exact results for the Barabási model of human dynamics,” Physical Review Letters, vol. 95, no. 24, Article ID 248701, pp. 1–4, 2005.\n- P. Blanchard and M. O. Hongler, “Modeling human activity in the spirit of Barábasi's queueing systems,” Physical Review E, vol. 75, no. 2, Article ID 026102, 2007.\n- L. Dall'Asta, M. Marsili, and P. Pin, “Optimization in task-completion networks,” Journal of Statistical Mechanics: Theory and Experiment, vol. 2008, no. 2, Article ID P02003, 2008.\n- Z. Deng, N. Zhang, and J. Li, “Inuence of deadline on human dynamic model,” in Dynamic Model of Human Behavior, J. L. Guo, T. Zhou, N. Zhang, and J. M. Li, Eds., p. 2934, Shanghai System Science Publishing House, Hong Kong, 2008.\n- X. P. Han, T. Zhou, and B. H. Wang, “Modeling human dynamics with adaptive interest,” New Journal of Physics, vol. 10, Article ID 073010, 2008.\n- M. Shang, G. Chen, S. Dai, et al., “Interest-driven model for human dynamics,” Chinese Physics Letters, vol. 27, no. 4, Article ID 048701, 2010.\n- T. Zhou, Z. Zhao, Z. Yang, et al., “Relative clock verifies endogenous bursts of human dynamics,” Europhysics Letters, vol. 97, p. 18006, 2012.\n- N. Johnson, M. Spagat, J. Restrepo, et al., “From old wars to new wars and global terrorism,” arXiv:physics/0506213, 2005.\n- N. Johnson, M. Spagat, J. A. Restrepo, et al., “Universal patterns underlying ongoing wars and terrorism,” arXiv physics: 0605035v1, 2006.\n- N. Johnson, Complexity in Humuan Conflict, Springer, New York, NY, USA, 2008.\n- S. Galam and S. Moscovici, “Towards a theory of collective phenomena: consensus and attitude changes in groups,” European Journal of Social Psychology, vol. 21, no. 1, pp. 49–74, 1991.\n- S. Galam, “Rational group decision making: a random field Ising model at ,” Physica A, vol. 238, no. 1–4, pp. 66–80, 1997.\n- S. Galam, “Sociophysics: a review of galam models,” International Journal of Modern Physics C, vol. 19, no. 3, pp. 409–440, 2008.\n- A. Clauset, L. Heger, and M. Young, “Substitution and competition in the israelpalestine conflict,” Chinese Physics Letters, vol. 27, p. 068902, 2010.\n- A. Vazquez, “Impact of memory on human dynamics,” Physica A, vol. 373, pp. 747–752, 2007.\n- J. F. Zhu, X. P. Han, and B. H. Wang, “Statistical property and model for the inter-event time of terrorism attacks,” Chinese Physics Letters, vol. 27, no. 6, Article ID 068902, 2010.\n- J. G. Oliveira and A. Vazquez, “Impact of interactions on human dynamics,” Physica A, vol. 388, no. 2-3, pp. 187–192, 2009.\n- Y. Wu, C. Zhou, M. Chen, J. Xiao, and J. Kurths, “Human comment dynamics in on-line social systems,” Physica A, vol. 389, no. 24, pp. 5832–5837, 2010.\n- D. J. Watts and S. H. Strogatz, “Collective dynamics of 'small-world9 networks,” Nature, vol. 393, no. 6684, pp. 440–442, 1998.\n- A. Barábasi and R. Albert, “Emergence of scaling in random networks,” Science, vol. 286, no. 5439, pp. 509–512, 1999.\n- W. Zachary, “An information ow model for conict and fission in small groups,” Journal of Anthropological Research, vol. R 33, no. 4, pp. 452–473, 1977.\n- G. A. Miller, “The magical number seven, plus or minus two: some limits on our capacity for processing information,” Psychological Review, vol. 63, p. 8197, 1956.\n- A. Baddeleyi, “Developments in the concept of working memory,” Psychological Bulletin, vol. 101, p. 353, 1994.","We always assumed that in taking medication it would not matter when you take the pill: morning, afternoon or evening.\nBut science proved us wrong: chronopharmacology is a relative new branch of science in which the timing of application of a medication is studied. Chronopharmacology is a new branch at the tree of chronobiology, the study of the rhythms in biology. An even newer branch is chrononutrition, which basically studies what the optimal timing is for food or nutritional supplements.\nBut why is there no chronotraining which would research the effects of different times of training in relation to the adaptation to training. In other words, it would answer the question: does it matter when I train?\nAnd I can answer that question for you now: YES, it does……\nAnd probably to quite a large extent. This might not be interesting for the recreational athlete, but for the elite athlete it is, In the end it might make a huge difference in efficiency of training, progress and performance. Since the elite athlete should be looking for that very small margin making the difference between winning and losing, between success and failure, between gold and silver.\nOne simple factor might already help us here: it’s called chronotyping. Are you a morning-type or an evening-type, a lark or an owl? Do you wake up fresh and sparkling, full of energy and wanting to train before breakfast? Or do you need time to wake up, warm-up and preferably train late in the afternoon. And of course there are people to whom it does not make a difference when they feel best, no extreme morning- or evening-types.\nA simple guideline from experience and theory: train when you feel best.\nThe exception to this rule is when you are preparing from a tournament or for competitions at a certain given time. It would not be smart to always train in the evening while all of a sudden you have to go through qualification rounds in the morning or always train in the morning while your competition is late in the evening. You might have trouble adapting in time to perform at your best at a time that your body and mind aren’t used to.\nI used to coach an elite female athlete who was an extreme morning-type, waking up in the morning and ready to go! And during day time you could sense and measure her energy slowly disappearing. At the other hand, in the same period I used to coach an Italian athlete who was rather an evening type: waking up late, go for a late and small breakfast: coffee and a roll, then relax, wait for lunch, relax and train in the late afternoon, seeming to gain more energy as the day progressed. Since I believed in chronotyping, I coached them at separate times of the day. The first mentioned in the morning and the second one in the evening. It made my working days longer, but it was worth it!\nImplementation of this is not always as easy as shown above.\nFirst of all many coaches deal with teams and we can’t just train every player at a different time. Secondly, we also depend on the availability of the facilities, suppose you’re a morning-type but the weight room is only available in the afternoon.\nBut still those who coach individual athletes have to think about chronotyping since a lot of effort and money is spent on increasing the small improvements, like equipment, nutrition, training methods, etc. Training at the right time of the day, for that athlete might be even more efficient.\nIt’s not only the time of the day, but also the week of the month, think about the menstrual cycle of female athletes,. And also the months of the year, since adaptation to training is different in summer and winter, probably due to the effect of sunlight and the impact on the hormonal systems.\nThere are even shorter cycles like ultradian cycles, e.g. the BRAC or Basic-Rest-Activity-Cycle of approximately 90-120 minutes. It’s probably something we are all unconsciously familiar with e.g. in case of attention span in students or the duration of a workout.\nNow how do you know the chronotype of an athlete?\n1. ask, observe, recall or measure when the athlete is performing at his/her best in morning time or evening time, one can look at training data like times, distances, weights, points\n2. do the Horne or the Munich Morningness-Eveningness Questionaire or MEQ\nIn the past a lot of research has been done on the influence of time on mental and physical performance e.g. in case of shift work, sleep deprivation and jet-lag in industrial or military setting.\nAlso in Russia and the former GDR sport scientists have spend time to look at this, but in the West this factor seems to be ignored.\nNow, like stated above, research is catching up again and in the medical field and in biology we start to see the impact of optimal timing of interventions. How long will it take before we in sports performance will pick this up too? My guess……. too long\nOschutz, H: Chronobiologie im Sport; Leistungssport, No.4,1991, pg.12-15.\nAdam, K; Tomeny, M; Oswald, I: Physiological and psychological differences between good and poor sleepers; J.Psychosom.Res. Vol.20, No.4, 1986, pg.301-316.\nKlein, R; Armitage, R: Rhythms in human performance: 1.5 hour oscillations in cognitive style; Science, Vol.204, June 22 1979, pg.1326-1328.\nKleitman, N: Basic rest-activity cycle 22 years later; Sleep,Vol.5, No.4, 1982, pg. 311-317.\nHill, D.W; Cureton, K.J; Collins, M.A; Grisham, CS.C: Diurnal variations in responses to exercise of “morning types” and “evening types”; J.Sports Med.Phys.Fitness Vol 28, No.3, 1988, pg.213-219.\nHommel, H.H; Luck, P: Chronobiologische Grundlagen und ihre Bedeutung fuer die Sportmedizin; Med.und Sport Vol.25, No.8,1986, pg.239-242.\nHecht, K; Poppei, M: Chronomedizinsche Aspekte der psycho-nervalen und physischen Leistungsfaehigkeit; Med.und Sport. No.12, 1977, pg. 377-386.\nHildebrandt, G: Chronobiologie in der Medizin; Arztezeitschr. Naturheilverf. Vol.26, No.3, March 1985, pg. 135-152.\nWinget, C.M; DeRoshia, C.W; Holley, D.C: Circadian rhythms and athletic performance; Med.Sci.Sports Exerc. Vol.17, No.5, 1985, pg. 498-516.\nRoenneberg, T; Wirz-Justice, A; Merrow, M: Life between Clocks: Daily Temporal\nPatterns of Human Chronotypes; J. Biol.Rhythms, Vol. 18 No. 1, February 2003, 80-90.\nTamm, A.S; Lagerquist, O; Ley, A.L; Collins, D.F: Chronotype Influences Diurnal Variations in the Excitability of the Human Motor Cortex and the Ability to Generate Torque during a Maximum Voluntary Contraction; J.Biol.Rhytms, Vol.24, No.3, 2009, pg.211-224.\nTeo, W; Newton, M.J; McGuigan, M.R: Circadian rhythms in exercise performance: Implications for hormonal and muscular adaptation; J.Sports Sci.Med. No.10, 2011, pg.600-606.\nPostolache, T.(Ed.): Sports Chronobiology; Elsevier Clin. in Sports Med.\nDrust, B; Waterhouse, J; Atkinson, G; Edwards, B; Reilly, T: Circadian rhythms in sports performance- an update; Chronobiol.Int., Vol.22, No.1, 2005, pg.21-44.\nShibata, S; Tahara, Y: Circadian rhythm and exercise; J Phys Fitness Sports Med, Vol.3, No.1, 2014, 65-72.\nCappaert, T.A: Review: Time of Day Effect on Athletic Performance: An Update; J.Strength Cond.Res. Vol.13, No.4, 1999, pg.412–421.\nDi Cagno, A; Battaglia, C; Giombani, A; Piazza, M; Fiorilli, M; Calcagno, G; Pigozzi, F; Borrione, P: Time of Day – Effects on Motor Coordination and Reactive Strength in Elite Athletes and Untrained Adolescents; J.Sports Sci. Med. No. 12, 2013, pg. 182-189.\nReilly, T; Waterhouse, J: Sports performance: is there evidence that the body clock plays a role? Eur.J. Appl.Phsyiol.Vol.106, 2009, pg.321-332.\nCarandente, F; Montaruli, A; Roveda, E; Calogiuri, G; Michielon, G; La Torre, A: Morning or evening training: effect on heart rate circadian rhythm; Sport Sci. Health, 1, 2006, pg. 113–117.\nPereira, R; Machado, M; Ribeiro, W; Russo, A.K; de Paula, A; Lazo-Osorio, R.A: Variation of explosive force at different times of the day; Biol.Sport. Vol.28, No.1, 2011, pg.3-9."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:af01777c-fdeb-490d-9ea0-c92b0dec62a8>","<urn:uuid:04471abc-fe3c-49eb-93fb-8dbe7028ad24>"],"error":null}
{"question":"What are the typical symptoms of immune thrombocytopenia and congenital spherocytosis?","answer":"Immune thrombocytopenia symptoms include easy bleeding, bruising, small red/purple spots (petechiae), and purple bruise-like spots (purpura). In rare cases, it can cause brain bleeding. Congenital spherocytosis presents with general features of anemia, jaundice, splenomegaly, and can lead to gallstone formation. Both conditions can have aplastic crisis, though in spherocytosis this is accompanied by folate deficiency and hemosiderosis.","context":["What Is Immune Thrombocytopenia?\nImmune thrombocytopenia — or immune thrombocytopenic purpura (ITP) — happens when the immune system, which fights germs and infections, attacks the body's platelets. Platelets are cells that stop bleeding by forming blood clots. Without enough platelets, kids with the condition bleed easily.\nIn most children, immune thrombocytopenia (throm-buh-sye-tuh-PEE-nee-uh) goes away within 6 months. But sometimes it can last longer, or come back after going away.\nWhat Are the Signs & Symptoms of Immune Thrombocytopenia?\nA child with immune thrombocytopenia may have:\n- bleeding that happens easily, such as:\n- bleeding under the skin that leads to:\n- easy bruising\n- small red or purple spots on the skin called petechiae (peh-TEE-kee-eye)\n- purple spots that look like bruises called purpura (PURR-pyur-ah)\nVery rarely, immune thrombocytopenia can cause bleeding in the brain (a stroke).\nWhat Causes Immune Thrombocytopenia?\nImmune thrombocytopenia happens when the immune system attacks platelets. Viral infections often trigger this in children. Less commonly, another illness or autoimmune disease or a medicine can trigger ITP. Often, it isn't clear what triggers the immune system to attack platelets.\nWho Gets Immune Thrombocytopenia?\nMost cases of childhood immune thrombocytopenia happen in kids 1–7 years old. But it can happen in older kids and teens. Usually, the child is otherwise healthy and feels well.\nHow Is Immune Thrombocytopenia Diagnosed?\nTo diagnose immune thrombocytopenia, doctors:\n- asks questions\n- do an exam\n- do blood tests\n- do a platelet count\n- make sure the other blood counts (red blood cells and white blood cells) are normal\n- look for signs of infection\n- check for other causes of low platelets\nHow Is Immune Thrombocytopenia Treated?\nTreating immune thrombocytopenia depends on how severe the symptoms are. Children who only have bruising and red pinpoint spots may not need any treatment.\nWhen needed, treatments may include:\n- medicines that stop the immune system from attacking platelets, such as:\n- an IV injection of antibodies (immunoglobulins or rituximab)\n- medicines to help the body make more platelets\n- surgery to remove the spleen because the spleen is where the platelets are removed from the blood. This is done only when a child has serious symptoms that don't improve with other treatments.\nWhat Can Parents Do?\nWhile they have immune thrombocytopenia, kids need to:\n- avoid sports and activities (such as bike riding and contact sports) that could lead to injury and bleeding\n- not take medicines that contain ibuprofen (such as Motrin or Advil) or aspirin, which make bleeding more likely\nMost children with immune thrombocytopenia recover fully within a few months. Help your child by:\n- going to all doctor's appointments\n- following the doctor's advice on which activities are OK and which to avoid\n- contacting the doctor and going to a hospital right away if your child has a head injury\n- making sure your child avoids any medicines as your doctor recommends\n- calling the doctor if your child has new symptoms of bleeding, bruising, or red or purplish spots on the skin","OBJECTIVESBy the end of this lecture you will be able to know the followings:\n-Definition and Etiology of Thalassemia, Sickle cell anemia, G6PD, and congenital spherocytosis.\n-Diagnosis, and investigations of these types of anemias.\n-Prevention and management of these diseases\nIn hemolytic anemia there is shortened life span of RBCs (normal= 120 days).\n1.Increased breakdown of Hemoglobin leading to :A. Increased serum bilirubin (mainly indirect).\nB. Increased urobilinogen in urine and stercobilinogen in faeces.\nC. Increased serum iron and decrease in iron binding capacity.\n2.Bone marrow compensatory reaction which lead to:A. Accelerated erythropoiesis (reticulocytosis more than 2%), reversed M/E ratio.\nB. Expansion of bone configuration seen clinically and in X-ray especially in the skull (hair on end appearance).\nCauses of Chronic Hemolytic Anemia:\nA. Corpuscular causes :\n1. Membrane defect :\nA. Hereditary spherocytosis.\nB. Hereditary elliptocytosis.\n2. Enzyme defect :\nA. G6PD. deficiency.\nB. Pyruvate kinase deficiency.\n3. Hemoglobinopathies :\nA. Thalassaemia (quantitative β-chain defect).\nB. Sickle cell anaemia (qualitative β-chain defect).\nB- Extracorpuscular causes:1- Immune:-Iso-immune: Rh and ABO incompatibility. Auto-immune: (e.g SLE).\n2- Non- immune:-\n-Paroxysmal nocturnal hemoglobinuria\n-Infections (Malaria, toxoplasmosis, septicemia).\n-Toxic: snake venom.\nIt is a chronic haemolytic anaemia transmitted as autosomal dominant trait due to dysfunction of a cell membrane protein “spectrin” making the cell more permeable to Na+ influx result in increase intracellular water → swelling & spherocytosis.\nThe spherocyte is relatively rigid and passes with difficulty through the minute apertures between splenic cords and sinuses and thus becomes easily destroyed.\nAfter splenectomy the haemolysis improves, although the biochemical and morphological abnormalities are not corrected.\nClinical Picture:-Onset in infancy and may be a cause of neonatal jaundice.\n-General feature of anemia, jaundice, splenomegaly, aplastic crisis and gall stone formation.\n-Folate deficiency, and hemosiderosis.\nLaboratory Finding:-Anemia: normocytic normochromic.\n-Reticulocytosis (count from 5 up to 20%).\n-Spherocytes in peripheral film.\n-Increase osmotic fragility: spherocytes are destroyed in higher concentration of saline than normal RBCs.\n-Bone marrow: erythroid hyperplasia .\nDifferential diagnosis:--From other congenital hemolytic anemias.\n-Autoimmune hemolytic anemia: comb's test is positive.\nTreatment:1- Supportive: packed red cell transfusion for severe anemia. Folic acid supplementation.\n2- Splenectomy invariably produces a clinical cure.\nSplenectomy in early childhood may causes increased susceptibity to infection, the operation should be delayed if possible, until 5-6 years old.\n3- Splenectomized children should receive:\npneumococcal and hemophilus influenzae b vaccines.\nProphylactic long acting penicillin.\nGlucose-6-phosphate dehydrogenase deficiency(Favism, Drug induced Hemolytic Anemia)\nX-linked recessive disorder. Males more than females.\nAge : may present in the neonatal period.\n-More than 200 variant of the enzyme could be recognized in different localities in the world.\nA. G6PD.(A):- Common among American, enzyme activity in 5-15% of normal.\nB. G6PD.(B):- Common in Mediterranean area, where enzyme activity is less than 5% and lead to haemolytic attack.\nC. G6PD. (C):- Common in Chineese.\nPathogenesis:-G6PD is Responsible for Production of NADPH and renewal of reduced glutathione (GSH) in the red cell membrane.\n-GSH protects red cells from oxidants by reducing them.\n-G6PD deficiency → ↓NADPH →↓ GSH →↓ protection of RBCs from oxidants.\n-Exposure of G6PD deficient RBC to an oxidant → impaired elimination of oxidant → oxidation of Hb → oxidized Hb is denaturated and deposited as Heinz inclusion bodies in RBCs lead to ↓ red cell integrity → hemolysis.\nExamples of oxidant agents :-Fava beans (Contains L-Dopa)\n-Infections: Due to release of oxidants from active phagocytes . -Viruses; Respiratory, hepatitis, infectious mononucleosis -Bacterial pneumonias.\n-Metabolic: Diabetic Ketoacidosis.\n-Drugs:- -Antipyretics: Aspirin, phenacetin\n-Sulfonamides. – Antimalarials. -Nitrofurans\n-Antibiotics: chloramphenicol – Para-aminosalicylic acid, -synthetic vitamin k, Naphthalene - Methylene blue.\nClinical picture: Favism: ingestion of fava beans → acute hemolytic anemia\nNeonatal hyperbilirubinemia of unconjugated type: there is hemolysis plus immaturity of hepatic enzymes.\nSigns and symptoms: Acute anemia (Marked pallor, irritability, etc.)\nNausea vomiting and epigastric pain\nHemoglobinuria (if hemolysis is severe) –Mild splenomegaly.\nJaundice with dark orange colored urine (urobilin) and dark stools\nDeath → may occur in cases of severe hemolysis\nCourse: spontaneous recovery from the attack is the rule.\nLaboratory: Normochromic normocytic anaemia, Heinz bodies in blood smear. Diagnosis is confirmed by estimation of G.6.P.D activity in RBC, better around few weeks after the attack.\nTreatment: -Avoid offending agents e.g infection.\n-In severe cases, emergency packed RBC transfusion.\n-Vitamin E (400-800 U/day) protect against hemolysis and antioxidant.\n-In severe neonatal jaundice: exchange transfusion.\nHemoglobin is a tetramer formed of 4 polypeptide chain with a heme group attached to each chain.\nEach chain is controlled by a special sequence gene which is activated and inactivated in a special sequence.\nTypes of Globin Chain (α. β. γ. δ): Each chain is controlled by 2 sets of gene\nTypes of Hb.:\nHbF, (2α +2 γ).\nHbA1 (2α + 2 β).\nHbA2 (2α +2δ).\nAt birth After 6 months\nHbF : 65%. HbF: 2%.\nHbA1 : 34%. HbA1:95%.\nHbA2: <1% HbA2: 3%\nSickle Cell Disease\nIt is transmitted as an incomplete autosomal dominant trait.\n-Homozygotes (both parents are affected) →sickle cell anemia 85-95% HbS+ 5-15% HbF+ 2% HbA2.\n-Heterozygotes (only one parent is affected) →sickle cell trait 55-60% HbA+ 25-45% HbS + 2-3% HbA2 .\nThis is a benign condition usually there is no anemia sickling does not occur except if patient is exposed to hypoxia.\n-HbS is similar in structure to HbA except that at position 6 of the β-polypeptide chain, glutamic acid is replaced by Valine.\nClinical picture: Onset after 6 months and is expected only when patient is homozygous for sickle cell gene.\n1- vaso-occlusive (thrombotic or painful) crisis: Vaso-occlusive is due to occlusion of small blood vessels → distal ischemia and infarction ,this is usually preceded by infection or spontaneously.\n-Hand-foot syndrome (sickle dactylitis). -Bone crisis.\n-Severe abdominal pain.\n-Acute painful hepatomegaly + direct hyperbilirubinemia.\n-Repeated painful splenic infarcts → autosplenectomy.\n-Cerebral occlusion → strokes, acute hemiplegia.\n-Acute episodes of painless hematuria.\n-Acute chest syndrome.\n-Acute episodes of painful priapism.\n2. Aplastic Crisis :-Aplasia of the bone marrow during or following parvovirus 19 infection.\n-Decrease in reticulocytic count.\n-Lasts about 10-14 days and usually recovers spontaneously.\n3-Sequestration crisis: Pooling of the blood become, for no obvious reasons sequestrated in spleen, leads to massive splenomegaly.\nShock collapse and may be dehydration.\nIn late childhood this splenomegaly diminish in size as result of the multiple infarcts this leads to hyposplenism considered as functional asplenia with liability of overwhelming infection caused by pneumococcal, H. influenza infection and salmonella osteomyelitis.\n4-Hyperhemolytic crisis:May be precipitated by bacterial infection and presented by anemia, jaundice and fever.\nLaboratory investigation:-Normocytic normochromic anemia -RBC show sickling under low O2 tension. -Hb electrophoresis studies:\n*HbS (80-95%), HbA2 (2-3%) and HbA (5-15%) in sickle cell anemia.\n* HbS (25-45%), HbA2 (2-3%) and HbA (55-60%) in sickle cell trait.\n-Bone marrow aplasia during an aplastic crisis.\nTreatment:◦ Treatment and prevention of infection by :\nProphylactice long acting penicillin.\nPolyvalent, pneumococcal, and haemophilus infleunza b vaccine every 2 years.\n◦ Blood transfusion correct anemia and prevent crisis.\n◦ Vigorous treatment of crisis:\nPainful crisis: give hydration, packed RBC, analgesic,vasodilator, (L carnitine, nitric oxide inhalation).\nAplastic crisis: give packed RBC.\nSequestration crisis, plasma expander, blood transfusion.\nβ – Thalassemia major (cooley's anemia):\nβ - Most common around meditranian sea. In β - thalassemia there is impaired production of β -chain due to mutation of one gene \"heterozygous\" or both genes homozygous\".\nAccording to degree of β – chain deficiency β thalassmia is divided into:\nΒo no β-chain synthesis no HbA only HbF and HbA2.\nβ+ little β-chain synthesis, so HbA is scanty and more HbF and HbA2.\nβ++ more β chain synthesis → thalassemia intermediate.\nβ+++ normal in individual.\nThalassemias are hereditary chronic haemolytic anemias caused by impaired or absent synthesis of α or β globin chain.\nClinical picture: Age of presentation usually start at the age of 6-12 months.\nClinical evidence of haemolytic anemia, pallor, jaundice hepatosplenomegaly mongoloid facies (enlarged head, prominent malar bone, frontal bossing, depressed nasal bridge protrusion of maxilla ± exposure of upper teeth due to hyperplasia of B.M.)\n◦ Haemolytic crisis precipitated by infection by parvovirus B19.\n◦ Megaloblastic crisis due to folic acid deficiency\nCardiomegally and hemic murmur.\ndelayed puberty due to endocrinal disturbances.\nComplication:- Increase susceptabilty to infection\nIron overload (haemochromatosis) due to increase iron absorption from gut and excess iron from repeated transfusion .iron accumulate in vital organ leads to heart failure, hepatomegally .D.M., bronze skin.\nAnemic heart failure. -Hypersplenism. -Pathological fracture.\nComplications of repeated blood transfusion: e.g. hepatitis C.\nDiagnosis: Hypochromic microcytic anemia with anisocytosis, poikilocytosis, target cell.\nReticulocytic count: increased nucleated RBCs in peripheral blood.\nIncrease serum iron with decrease serum iron binding capacity.\nHb electrophoresis: (increase HbF).\nB.M. examinations show erythroid hyperplasia.\nX-ray bone of skull → hair on end apearance due to widening of diploid space. -Cardiomegaly may be present.\nEvident diagnosis: Genotyping or P.C.R. (Polymerase Chain Reaction) to detect the mutation on chromosome 11.\n-Weak mutation → thalassemia intermedia\nStrong mutation → thalassemia major\nTREATMENT1- Symptomatic treatment:\n1- Packed RBC transfusion 10-15ml/kg/dose every 4-6wks.\nBetter to transfuse washed RBCS by saline.\nBetter to transfuse (neocyte) rich with hemoglobin\nBetter to transfuse from single donor.\nBlood transfusion to keep Hb above 12gm/dl in order to:\na- Permit normal activity with comfort.\nb- Improve crisis.\nc- Minimize cardiac dilatation.\nd- Prevent skull bone changes.\n2- Iron chelation:\nDesferroxamine (desferal) given S.C. by small iron infusion pump in dose 30-50mg/kg/dose 5 days/week. Vit C potentiate the action of desferal. -Recently oral chelating agent deferazirox (Exjade) 20-30mg/kg/dose.\n3- Folate, 1mg Daily to compensate B.M. over activity.\n4- Low iron diet with increase tea drinking (decreasing iron absorption)\n5- Indications of splenectomy:\n◦ Secondary hypersplenism.\n◦ Huge spleen, cause dragging pain\n-Splenectomy should be done after the age of 5 yr.\nto guard against severe infection.\n-Prophylactic pneumococcal and meningococcal vaccination should be given 2wk. before operation.\n-long acting penicillin prophylactic 1,200,00 I Monthly.\n6- New modalitiy: α chain stimulation by hydroxyurea, β hydroxybuterate to cause increase HbF that decrease ineffective erythropoiesis.\nII. Curative therapy\nB.M. transplantation from HLA identical of non affecting sibling\ngene therapy by insertion of normal gene into B.M. stem cell\nβ Thalassemia minor (Heterozygous ) It may be asymptomatic or symptomatic:\n-Mild anemia (Hb 9-11 g/dl) accentuated during infections.\n-Anemia is microcytic hypochromic and does not respond to iron therapy.\n-No jaundice or other signs of hemolysis.\n-Hb electrophoresis: Hb A is predominant, Hb A2 is increase (<4%), Hb F may be slightly increase.\nβ Thalassemia intermedia :\nAffects 2-10 % of homozygous patients and is characterized by:\nPatients have higher ability to produce β –chain and high production of γ-chains.\nGenetic, morphologic and biochemical features: like β-thalassemia major.\nAnemia is milder (Hg level 7-10g/dl).\nPallor, jaundice, facial bony changes, splenomegaly and hepatomegaly are present.\nNo growth retardation and no hypogonadism.\nTreatment: -Blood transfusion is usually not needed and given only when required.\n-Decrease iron absorption from GIT (e.g. drink a cup of tea after each meal).\n-Chelation therapy and folic acid supplementation.\nα Thalassemia There is decrease synthesis of α chain and Hb A, F, A2 genetic deletion can affect one or more of the gene represent for α chain synthesis.\n1- Deletion of one gene → silent carrier.\n2- Deletion of two gene → α thalassemia trait.\n3- Deletion of three gene → HbH.\n4- Deletion of four gene → hydrops fetalis → miscarriage of non viable fetus,\nCOURTESY:DR. SHAHENAZ M. HUSSIEN\nDOWNLOAD THIS LECTURE"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:706e082e-fc83-4ae1-8415-b992030176ec>","<urn:uuid:b557f14f-647a-4f44-ac22-7819c3ae6ce1>"],"error":null}
{"question":"How do Palestinian and Indian cuisines differ in their use of spices, particularly in terms of intensity and specific spice combinations?","answer":"Palestinian cuisine uses milder spices compared to Indian cuisine, employing what's called '7 spice' or spice mix that includes cinnamon sticks, black pepper, allspices, and bay leaves. These spices are used to add flavor and eliminate meat odors. In contrast, Indian cuisine employs a more complex and intense array of spices, including specific combinations like garam masala (which contains cardamoms, cloves, peppercorns, and cumin seeds) and panch phoren (a mixture of cumin, fenugreek, nigella, fennel, and mustard seeds). Indian dishes also extensively use spices like turmeric, curry leaves, coriander powder, and saffron for both flavor and color.","context":["|Maqluba-Palestinian traditional dish (Photo: Internet)\nReporter: Hi Saleem! Welcome to our weekly show: Food delight. First of all, what is the most popular dish in Palestine that every Palestinian can cook and eat?\nSaleem: We have one dish called Maqluba, which means up-side-down. Maqluba is a Palestinian dish, in which we use, rice, vegetables, chicken, lamb or beef, it depends on your flavor. It is very flexible; you can use different kinds of ingredients.\nReporter: What is special about Maqluba?\nSaleem: The most special thing about this dish is the way how we set up the ingredients inside the pot. First, after we fry some veggies like eggplants, cauliflowers, potatoes, carrots, or whatever you like, you can put any kinds of veggies you like. You just fry half of the time, not for fully-cooked. So you just fry for about three minutes. Then we put the veggies inside the pot first, then we put the meat. The meat is already boiled with the spices and olive oil. We use some Arabic special spices including cinnamon sticks, black pepper, allspices, bay leaves… We call it 7 spice or spice mix. We use the spices to add more flavor and the meat is tastier. The spices also kill the smell of the meat.\n|(Photo: Saleem Hammad)\nReporter: As you said making maqluba requires several steps including pre-cooking and pre-seasoning the meat and vegetables before layering everything in the pot, right? Typically, vegetables are fried before layering.\nSaleem: Yes, you are right. We set up veggies, then the meat, then the rice and we add the water. The water we use is chicken broth. We usually boil the chicken for about 30 to 40 minutes and get the broth, which is very tasty. And we add the chicken broth, but we use less chicken broth than when we normally cook only rice because we the water coming out from the meat and veggie as well. We leave it to cook for about fifteen minutes like normal rice and then when we want to serve this dish we make it up-side-down, so the veggies, the meat, will be on the top and the rice is on the bottom. And the way it looks is like a cake, and it is very delicious, healthy because it has different ingredients in its veggies, meat and rice.\nReporter: Yummy! I really want to try to make Maqluba myself at home. Do you often make Maqluba in Vietnam?\nSaleem: I have invited many of my Vietnamese friends to try this dish and everybody love it. They like the way we serve it. They like the taste of it. Do you know that sometimes it is not easy for Vietnamese people to eat the foreign dishes especially the dish comes from the Middle East or Indian food which has a lot of harsh smell? In Palestine we use less spice than in India.\nReporter: If I can’t find all the Arabic spices to make Maqluba, can I use some Vietnamese spices instead?\nSaleem: As for me, after living in Vietnam for ten years, I understand how Vietnamese like to eat so even sometimes I use Nuoc Mam, fish sauce to add the Vietnamese flavor to my dish make Vietnamese people feel like it is a close thing to them. Well, it is not Vietnamese food but it is easy to eat.\nReporter: Thank you very much Saleem for your creative Maqluba recipe. I will definitely try to cook Maqluba one day. We hope you can share more recipes of Palestinian food on another VOV24/7 Food Delight. Good bye.","Do you know your Indian spices? Supermarkets outside India often sell little jars vaguely labeled ‘curry masala,’ the contents of which are used to imbue a generic ‘Indian’ flavor to the dish under preparation. However, each Indian spice serves a specific purpose, often related to both health and taste. Consider this a basic guide: Indian Spices Explained!\nIndian Spices Decoded\nA Glossary of the Most Important Indian Spices Used :\nThis yellow powder is a staple in dishes across North India and South India, and yellow lentils are further colored with a pinch of it. While it’s the powder which comes in use to flavor cooking, the root form is ground into a paste and used as a ‘facepack’ for the bride and groom before their wedding day, since turmeric also lends the skin a healthy glow. An entire ceremony revolves around it!\nOur Top Tip: While cooking, if you nick yourself while chopping, sprinkle a tad of turmeric on the gash to stop the blood flow – it’s an antiseptic!\nThis flavorful spice traces its roots to India. This comes from the bark of an evergreen tree. Cinnamon is used to give both an aroma and taste to a dish. It is generally used in pulao (a rice dish), biriyani (a rice dish) and often forms the ‘tadka’ or basic concoction of spices, seeds, onion and tomato for heavy lentil dishes, such as chickpeas and black beans. Cut off a small chunk and pop in hot oil as you begin cooking.\n3 Curry Leaves\nThese are dark green flat leaves that are used in a dish to impart flavor. It is widely used in South Indian cooking. Some people even use it without crushing and it is often grown in backyards. It is widely used in palyas (vegetable-based dishes, flavored with mustard seeds and often with grated coconut) and lentil dishes like sambar as a final garnish.\nBlack cloves are tiny and shaped like a tiny spear with a head on top. They have a distinctive flavor and are often used right at the start of a dish preparation, popped into boiling oil along with other big spices: whole peppers, bay leaves, cinnamon and cardamoms.\nOur Top Tip: A traditional Indian cure for mild toothache has been to pop a clove into your mouth and nurse it there: it acts as a mild analgesic!\n5 Cardamom (black and green)\nBlack cardamom is a well-known spice used to impart a strong taste and aroma. Just one big one is often enough to flavor biryanis, meat stews and pulao (flavored rice). It’s popped into hot oil at the start of a preparation.\nGreen cardamom is much smaller than the black variety and often used in conjunction with tiny black cloves to begin a dish preparation. Its distinctive flavor requires only a basic use of it: four tiny pods will flavor a pot of chickpea lentils! It’s also used to flavor tea, lending its name to ‘elaichi (green cardamom) chai’.\nSaffron is one of the costliest spices used in Indian cooking. It is used to flavor a dish and also as a coloring agent. It is widely used in Indian desserts like kheer. It is generally mixed with milk and then added to the kheer. It lends an orange color. In Indian cooking, saffron is generally added at the end of the preparation of a dish.\n7 Coriander Powder\nCoriander powder is one of the most extensively used ingredients in Indian cooking. Almost all Indian dishes have Coriander Powder as an ingredient, added alongside the turmeric, cumin and chili powders. It is also used a lot in marinating chicken, lamb, prawns, fish etc. when a pinch of the powder is smeared onto the chopped slices or meats in question. Tandoori dishes rely on coriander (or dhania) powder as an important ingredient, as you will no doubt discover during your India vacations.\nGinger is widely used in Indian cooking. Popular Indian dishes like kadai paneer, kadai chicken, mixed vegetables have this spice. Ginger and garlic are often used together at the start of a preparation but for many Indians who don’t use garlic for religious reasons, it’s shreds of ginger that can flavor a range of lentils from south-Indian sambars to north Indian ‘kali’ or black dal (lentils). Ginger is also used to flavor Indian tea. It also has a lot of medicinal properties. Hot water or hot tea infused with ginger is said to cure a nasty cough. It is also used to remove stomach acidity and just might work wonders during your India vacations.\nThis ingredient is used a lot in almost all Indian curries and dals and meat dishes as well. It is used as whole cumin seeds, popped into boiling oil as you begin cooking and often additionally in its powdered form as well, in conjunction with the coriander, turmeric and chilly powders. For yogurt dishes, such as raitas and dahi vadas (savory donuts with yogurt), a sprinkling of chili and cumin powders on the finished dish indicates that they are ready to be consumed!.\n10 Fennel Seeds\nThese are consumed mainly after having a meal as a mouth freshener. They are also used in masala chai (flavored Indian tea) to flavor it. It is used in the tempering of Indian dishes.\nComplex combinations of Indian Spices\nSometimes basic spices are combined to create more complex combinations, such as garam masala and panch phoren, two of our favorites!\n11 Complex Indian Spices: Garam Masala\nGaram masalas drizzled on to several North Indian dishes, usually dry preparations such as, say, cauliflower-potato or a kadai chicken. It often comprises ground spices including cardamoms, cloves, peppercorns, and cumin seeds and is available as a distinctive, deeply flavorful dark brown powder.\n12 Complex Indian Spices: Panch Phoren\nPanch phoren is a whole spice combination used in most East Indian Bengali cooking. It’s popped into boiling oil right at the start of preparation and is a mixture that blends cumin, fenugreek, nigella, fennel, and mustard seeds. It is to Bengali dishes what mustard seeds are to south Indian dishes and jeera seeds are to North Indian dishes: an initial whole seed that is used as a base flavor.\nDiscover the ultimate spice of life with a tailor-made culinary adventure with Enchanting Travels!"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:930d33a3-b86e-4a6d-ab16-0149d9e5f597>","<urn:uuid:d95e4b1d-7779-460b-8754-f620af6b668d>"],"error":null}
{"question":"How does the recognition of revenue in top line performance compare to the tracking of expenses and profit in variance analysis?","answer":"Revenue, known as the 'top line', appears as the first item in income statements on the credit side of trading accounts. This initial revenue recognition forms the starting point for financial analysis. In variance analysis, businesses compare budgeted amounts to actual results monthly, investigating the largest dollar amount variances first to make improvements. This analysis helps track expenses and ultimately affects the 'bottom line' (net profit), which appears as the final item in the Profit and Loss account. Through variance analysis, companies can identify issues like excessive costs - for instance, when actual costs exceed budgeted amounts by a significant percentage, allowing for corrective actions to maintain profitability.","context":["Income, turnover, revenue are terms used synonymously to mean the amount of money that an organisation receives from its activities like sale of products, providing services to customers etc. Depending on the nature of the organisation and the type of activity it is involved in the revenue streams are varied\nSale or Products, Providing Services are the activities most common to business organisations. Taxes, Duties, Fees etc are the major sources of revenue for Governments. Donations, Grants, Subscriptions, etc are some of the sources of revenue for non-profit organisations.\nThe terms Revenue and Sales or Turnover are interchangeably used. This makes sense only when sales are expressed in terms of value and not in terms of quantity.\nGross Revenue and Net Revenue are terms which are indicative of Gross Sales and Net Sales after setting off sales returns\nRevenue would be meaningful only when it is expressed in relation to a period. Say the revenue is Rs. 5 crores is would not make much sense unless we express the period involved.\nSaying the revenue for the last month is Rs. 5 Crores does sound meaningful.\nTop Line and Bottom Line\nRevenue is often referred to as top line since it is the first item that we consider in preparing the income statements or accounts. On the Credit Side of the Trading account we find sales generally towards the top as the first or second item.\nSimilarly Net Profit (revenue left after deducting all expenses) is termed \"Bottom Line\". In the Profit and Loss account, Net Profit/Loss is the last item that appears towards the end.\nEven in an income statement (which is nothing but the Trading and Profit & Loss a/c prepared in a form suitable for financial analysis) we start by considering the gross sales (i.e. gross revenue) and end with arriving at the net profit.\nrealized when goods and services are exchanged for cash or receivables (debtors).\nrealizable when assets received in exchange for goods and services are readily convertible to cash or receivables (debtors).\nearned when the duties to be entitled to compensation are performed.\nRecognising revenue implies the act that would make the organisation consider that they have earned the revenue involved in the transaction. Based on when the revenue is recognised there are two types of accounting systems (1) Cash Basis of Accounting and (2) Accrual Basis or Mercantile System of Accounting\nCash Basis Accounting\nUnder cash basis accounting revenues are recognized and earned only when cash is received irrespective of when and how the services were performed or goods delivered.\nTo put it in different terms, the cash basis of accounting asks you to take into consideration all those incomes/gains that have been received in cash or other assets and expenses/losses that have been paid out in cash or other assets during the accounting period in consideration.\nAccrual or Mercantile Basis Accounting\nUnder accrual or mercantile basis accounting, revenues are recognized and earned when they are realized or realizable irrespective of when the cash is received.\nTo put it in different terms, the accrual basis of accounting asks you to take into consideration all those incomes/gains and expenses/losses pertaining to the accounting period for which you are trying to ascertain the profits and losses irrespective of whether the incomes are received in cash or not and the expenses are paid out in cash or not.\nHybrid System of Accounting\nThis is not a system of accounting on its own. It is a combination of the Cash Basis Accounting and Accrual Basis Accounting. This system is based on the concept of conservatism.\nUnder the hybrid system of accounting, incomes are recognised as in Cash Basis Accounting i.e. when they are received in cash and expenses are recognised on accrual basis i.e. during the accounting period in which they arise irrespective of when they are paid.\nWhat Basis/system to follow?\nThe basis of accounting to be followed is dependent on the attitude and outlook of the organisation. If organisations have a conservative attitude, they may adopt the hybrid system of accounting.\nThe traditional accounting systems used to adopt the cash basis of accounting. Organisations which are to abide by the various regulations imposed by the various acts under which they are regulated are mostly required to adopt the Mercantile System of Accounting which is supposed to reveal the information relating to the organisation in a more appropriate manner than the cash basis of accounting.","How healthy is your business?\nIf you can’t answer this question with hard numbers, then you don’t really know. The most successful companies have systems in place to make sure that profit is prioritized, expenses are controlled and stakeholders are regularly updated about about progress.\nHere are seven ways to see how much you’re really spending and earning.\n1. Review Your Chart of Accounts\nYour chart of accounts is a listing of each account that you use for transactions as well as the account number. In order to track your expenses and profitability, your chart of accounts needs to be both comprehensive and easy to understand.\nIf you don’t review and update the chart of accounts each month, your financial statements may not be clearly stated. The balance sheet, for example, should separate assets into current and long term categories, and an income statement must separate operating income from non-operating income. The chart of accounts helps you generate the right information for these two financial statements.\nThe importance of this cannot be understated. Assume, for example, that Patty manages Marshall Landscaping, a business that generates $10 million in annual sales. Marshall’s work includes landscaping, tree services, and fence installation, and Patty considers these three areas to be profit centers.\nIf Patty’s chart of accounts uses the account number #7000 for total revenue, for example, Marshall should have subaccounts for each profit center. Assume that account #7100 is landscaping, #7200 is tree service, and fence installation is assigned #7300. Every revenue and expense category should have a subaccount for each profit center, so that Marshall can calculate the profit of each service it provides.\nEach revenue and expense account should also have a specific description, and no expense accounts should be labeled as “miscellaneous”. This policy forces management to assign each expense to a specific category.\n2. Automate Your Income and Expense Tracking\nUse accounting software that allows you to link your bank account and credit card activity directly into your accounting records. This task is important because it ensures that you’ll capture all of your business activity for the month.\nEntering information manually can be a problem, since it’s a time-consuming process. Given the amount of input required, many businesses put off entering monthly transactions until after the month is over. This delays the firm’s ability to generate month-end accounting reports and financial statements meaning that you might not discover problems until much later. Manual input also increases the risk of human error.\n3. Use Accrual Accounting for\nMaintaining the chart of accounts allows the business to use accrual accounting to track expenses and profit. Accrual accounting posts revenue when it is earned and records expenses when they are incurred. Revenue transactions are matched with the expenses that were incurred to generate the revenue.\nThis is different than cash basis accounting, which posts transactions based on cash inflows and outflows. Every business should use accrual accounting, because cash basis accounting can create misleading financial results.\nMarshall, for example, uses accrual accounting for its June month-end payroll. Workers will not be paid for the last week of June until July 3rd. To properly account for payroll on June 30th, Marshall posts a $70,000 debut to wage expenses and a $70,000 credit to accrued wages payable. When employees are paid on July 3rd, Marshall debits (reduces) accrued wages payable $70,000 and credits cash $70,000 to pay employees.\nAccrual accounting posts the wage expenses to the period when the work is performed (June). Cash basis accounting, on the other hand, posts the wage expenses when the payroll is paid in cash (July). Accrual accounting posts revenue and expenses to the proper time period, which allows Patty to track profitability.\n4. Use Variance Analysis to Find Opportunities for Improvement\nEvery business should have a formal budgeting process that produces a budget for the upcoming year. Many companies start the budget process before year end, but don’t complete the process until after the new year starts. But in order to track every dime of expenses and profit, the budget must be in place on January 1st of the new year.\nSuccessful companies also compare the budgeted amounts to actual results at the end of each month, and use that analysis to make improvements to the business. This process is called variance analysis, and it helps the business stay on track during the year.\nAn owner should review variances and investigate the largest dollar amount variances first. This strategy allows the owner to make changes that will make the biggest financial impact.\n5. Correct Pricing and Estimates on a Regular Basis\nAssume that Marshall purchases mulch for landscaping. When Marshall bids on landscaping work, it includes a mulch cost per square foot. Patty reviews her landscaping costs for May and notices that the actual mulch cost per square foot is 15% higher than the budget.\nPatty investigates the variance and finds that some new workers are using too much mulch on each job. Patty can train her employees and reduce the mulch expense to the budgeted amount going forward.\n6. Study Your Profit Margin Ratio\nPerforming variance analysis will keep a business on track, and increases the chances of reaching the budgeted profit total. After Patty reviews all of the variances, she can analyze profitability by profit center. If, for example, the tree service business generates a 15% profit, rather than the 12% budgeted profit amount, Patty should be able to find out why. It may be that tree service labor costs have declined below the budgeted rate per hour, which generates a higher profit.\nProfit margin is a great tool that allows you to compare profitability between profit centers. Profit margin is defined as (net income) divided by (sales), and this ratio measures the amount of profit earned on each dollar of sales.\nAssume, for example, that the landscaping division has a $400,000 profit on $4 million in sales, and that the tree service division earns a $450,000 on $3 million in sales. The landscaping division has a 10% profit margin, while the tree service division earns 15%.\nThe profit margin ratio allows you to compare the profitability of product lines with different levels of sales. Since the tree service profit margin is higher, Patty may decide to shift the firm’s marketing focus to the more profitable division.\n7. Automate Your Accounting\nManaging a business requires you to make dozens of decisions each day, and it may be difficult to monitor your financials. If you don’t stay on top of your expenses, however, your business profitability may suffer.\nAs a business owner, you want to work on your business rather than in your business. QuickBooks helps 4.3 million businesses save time and money by automating a number of important financial tasks. Try it free for 30 days."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:e93b2190-2f66-41c2-baf3-388775f4d6c6>","<urn:uuid:2efade40-ad42-4d72-a5dd-4ffbb4299b15>"],"error":null}
{"question":"In non-literate societies, how long does it take for words to change so much that linguistic relatedness cannot be traced through cognates?","answer":"In non-literate societies, words change so quickly that after five to eight thousand years, not enough cognates can be traced back to establish linguistic relatedness.","context":["Linguistic Phylogenies Are Not the Same as Biological Phylogenies\n(Note: This post is jointly written by Martin Lewis and Asya Pereltsvaig)\nA key assumption of Bouckaert et al. is that the diversification and spread of languages operates so similarly to the diversification and spread of biological organism that the two processes can successfully be modeled in the same manner. The parallels between organic and linguistic evolution are indeed pronounced. Both processes entail replicating codes that continually change, giving rise to novel varieties that increasingly differ from their progenitors over time. As a result, “phylogenetic trees,” showing descent from common ancestors, are a common feature of both evolutionary biology and linguistics.\nBut despite their similarities, organic evolution and linguistic evolution are in many ways highly dissimilar. Encoding information for communication is not the same as encoding information that generates life: language is vastly more fluid and complex than the genetic code; individual languages are much less clearly differentiated from each other than are species; and language is a social phenomenon, given to influences largely irrelevant for biological evolution. The key differences can be summarized as follows: biological evolution is unconstrained but governed by natural selection (any mutation can happen, but which mutations remain in the pool depends in large part on natural selection), whereas linguistic variation (seen in terms of deep grammatical properties) is constrained by a system of parameters but is not subject to natural selection. As a result, the branching trees of linguistic descent are merely analogous to the phylogenetic diagrams of biological evolution, and do not indicate the same kind of relationships.\nAlthough organic evolution operates through a much more restricted set of message-carrying units than does human language, it nonetheless produces diversity at a much deeper level. Given the biological constraints of the human brain/mind (as of yet less than fully understood), there are only so many ways in which any given language can be structured. To be sure, the number of possible human languages, both extant and extinct, as well we those that may arise in the future, is vast, but all human languages appear to be “variation on a theme,” guided by the same parameters. Some languages have as few as two vowels (Ubykh, Northwest Caucasian) and others as few as six consonants (Rotokas, North Bougainville); other languages may have as many as 20 vowels (e.g. the Taa language, spoken in Botswana and Namibia, is reported by some sources to have as many as 20 or even 30 vowels, depending on analysis) and as many as 84 consonants (as in Ubykh; the Taa language is reported to have 87 consonants under one analysis, 164 under another). But crucially, all languages differentiate vowels from consonants and use both. Some languages put verbs before subjects and objects, while others place them at the ends of sentences, but all languages have verbs, subjects and objects.* Some languages can build sentence-long words packed with of numerous prefixes, infixes, or suffixes, while others use stand-alone, stripped-down words to do the grammatical work of expressing tense, number etc., but all languages make words from morphemes—and all construct sentences. As a result of this limited space of possibilities, completely unrelated languages evolving on their own often come to share major grammatical traits.\nLinguistic evolution, unlike that of the biological realm, moves at a rapid clip. In non-literate societies, words change so quickly that after some five to eight thousand years not enough cognates can be traced back to establish linguistic relatedness. In the same time span, grammatical structures can undergo wholesale transformations, and sound inventories can change drastically as well. As a result, even clearly related languages can have next to nothing in common with each other, and can only be linked through investigations into their ancestors. Hindi and English, two of the three most widely spoken Indo-European languages, are dissimilar in almost every respect.** On casual inspection, Hindi would seem to have more in common with the non-Indo-European languages of the Indian sub-continent than it does with English.\nThus, relatedness at the family level and overall linguistic similarity often fail to correspond. Maps showing major language patterns typically bear little if any resemblance to maps depicting linguistic families. Even something as seemingly basic as word order correlates poorly with lines of descent. For example, Indo-European languages can be SVO (subject-verb-object; marked by red dots on the map to the left), such as English, Romance, and most Slavic languages (but Sorbian, a Slavic language, is SOV); SOV (marked by blue dots), such as the Indo-Iranian languages (yet Kashmiri is SVO); or VSO (marked by yellow dots), such as the Insular Celtic languages (yet Cornish is SVO). Some other families, such as Austronesian, have an even greater variability in the basic word order: Niuean is VSO, Malagasy is VOS, Rotuman is SVO, and Tuvaluan is OVS.\nSimilarly, features of morphological typology (how words are formed from morphemes) often cross-cut connections established by common descent. Whereas Proto-Indo-European, like most of its daughters, was a synthetic language (building words from multiple non-root morphemes), English and Afrikaans are relatively analytical (with low ratios of morphemes to words), which gives them a certain affinity with Mandarin Chinese (a highly analytical language). As discussed in an earlier GeoCurrents post, isolating languages are found in Africa (Hausa, an Afroasiatic language), Asia (Vietnamese, Austroasiatic), Oceania (Rapanui, Austronesian), and the Americas (Kipea, Kiriri). In phonology as well, similar patterns obtain, as sound inventories often fail to show systematic correspondences with language families. The Indo-European languages of South Asia, for example, are in many respects more phonologically similar to the Dravidian languages of the same region than they are to most other IE language. One of the characteristic phonological markers of the region, the rich inventory of retroflex consonants, is also scattered across the rest of the world, found in about 20 percent of all languages belonging to a wide variety of families.\nOne of the best ways to appreciate the relative insignificance of language families in regard to the global distribution of such features is to explore the maps that can be generated on the WALS website, such as the one reproduced above. Few if any of these maps bear much resemblance to the familiar depiction of the world’s major language families.\nAgain, the contrast with biological evolution is stark. The farther removed organisms are from each other on the tree of life, the fewer genes they necessarily share. Even when convergent evolution results in similarities between distantly related organisms, the parallels are relatively superficial. As a result, modern genetic inquiry can establish precise levels of biological relatedness, a process that has revolutionized taxonomy over the past few decades. In the biological realm, moreover, the farther one moves up different branches of evolutionary descent, the more distinctive the organisms found along it generally become. Chordates (the phylum that includes vertebrates) share a distant common ancestor with echinoderms (sea stars and their relatives), and some tunicates, primitive members of phylum Chordata, might be mistaken by unschooled observers for sea lilies in phylum Echinodermata. (Tunicates more generally look like unrelated jellyfish and other cnidarians; a few could be mistaken for rocks, but such rocks disconcertingly bleed when cut open.) But no one would ever mistake any mammal with a sand dollar, a sea cucumber, or any other echinoderm, animals characterized by radial rather than bilateral symmetry. The two phyla have simply evolved in strikingly different directions. If linguistic evolution worked in the same manner, it is questionable whether translation between distant languages would even be possible. Moreover, the disparate patterns of spatial distribution of deep grammatical properties, such as the ones illustrated by the WALS maps, would not be found.\nIn language, deep grammatical properties can radically change, often taking on the same forms as those encountered in wholly unrelated tongues. As a result, linguistic relationships are often anything but obvious, and can only be discerned though intensive study; significantly, such hidden connections can hold true even for relatively recently emerged languages. A fluent speaker of the major Germanic languages, for example, might be nonplused to learn that Frisian is more closely related to English than it is to Dutch. Yet according to some specialists, even Low German is “phylogenetically” closer to English than it is to (High) German—even though Low German is generally regarded as a mere dialect (or group of dialects) of German!\nLinguistic evolution is only vaguely analogous to organic evolution for a variety of reasons, but a crucial factor is the fact that vastly less sharing occurs across biological lineages. We now know that genes can jump from one species to another, but the process is relatively rare; in this realm, change generally occurs as a result of random mutations acted upon by natural selection, not from the borrowing of elements from other species. When it comes to languages, however, sharing is ubiquitous. Languages are almost always borrowing words, and sometimes they adopt grammatical properties of other languages as well. At times, two completely unrelated languages essentially merge to create a hybrid tongue. To be sure, linguists are almost always able to determine which language contributed more elements and more basic structures, and hence should count as the parent tongue. (It should be noted that the use of the terms “parent” and “daughter” in relation to languages is misleading since, unlike in the biological realm, where individual organisms are discrete, the transition from “parent” to “daughter” language is always gradual.) When it comes to creole languages, however, such determinations are not always easy. In regard to grammar, different creoles of completely different parentage are often more similar to each other than they are to any of their source languages. In some instances of mixed languages, admixtures of vocabulary, grammar, and phonology run so deep that linguists abandon the quest for unambiguous classification. Cappadocian Greek, for example, is slotted by the Wikipedia into the seemingly impossible “Greek-Turkish” language family. Does Indo-European therefore encompass this language? Other sources, such as the Ethnologue, place this language in the Greek branch of the Indo-European family, but Turkish influences on Cappadocian Greek are pronounced: it has certain sounds that have been borrowed from Turkish, as well as vowel harmony; it has developed agglutinative inflectional morphology and lost (some) grammatical gender distinctions; and its basic word order is SOV. And Cappadocian Greek is by no means the only example of such a thoroughly “mixed language.” In the biological realm, in contrast, such mixtures are so obviously impossible that they have generated their own nonsense genre, as exemplified by Sara Ball’s delightful flip-book, Crocguphant.\nLinguistic family trees must therefore be taken as often showing lines of partial descent, unlike the phylogenetic diagrams of organic evolution. To gain a more complete understanding of linguistic relatedness, it is necessary to complement language families with other kinds of connections. The various languages of a Sprachbund, or a linguistic convergence area, for example, derive from different families, yet nonetheless come to share many features through long histories of mutual interaction. One must also consider linguistic strata, which take into account the influences imposed by one language on another. The role of a linguistic substratum, derived from a previously existing language that was later supplanted by another tongue, can be profound. In many cases, such linguistic substrates were instrumental in generating subfamilies; the Germanic languages, for example, are distinct from other Indo-European languages not merely because they drifted in their own particular direction, but also because that acquired a major substrate from another (unknown) language family. Sometimes, the ghostly presence of a long extinct language or language family can be detected through such substrates. Vedic Sanskrit, for example, was definitely an Indo-European language, but it was influenced not only by the preexisting Dravidian and Munda languages of the Indian subcontinent, but also by an unknown substrate deemed by Colin Masica “Language X.”\nA useful alternative to the linguistic tree is the so-called wave model, or Wellentheorie, originally devised to explain some of the characteristics of the Germanic languages that seemed to defy the phylogenetic approach. In wave theory, fluid dialect continua replace the stable, geographically bounded languages required by models predicated on direct descent from ancestral tongues. Here, innovations can occur at any points within a dialect continuum; such changes then spread outward in a circular manner, eventually dissipating as the distance from the innovation center increases.*** If a bundle of innovations substantially overlap and become entrenched, a new dialect, or even language, can be said to have emerged. But according to wave theory, such a “language” is still best viewed as an “impermanent collection of features at the intersections of multiple circles.”\nWave theory does recognize, however, the fact that a single language/dialect can appropriate an entire dialect continuum, subordinating more localized speech forms and eventually driving them into extinction, as indeed was the case in regard to Standard German over most of Germany. Such a process, however, generally requires the power of the state or of some other overarching institution. Such geographically expansive and culturally potent organizations, however, are a feature of the relatively recent past; for most of humankind’s existence, the institutions necessary for producing linguistic standardization over broad areas were lacking. We are so used to the modern world of mass communication over vast distances and of language-standardizing governments and educational systems that we easily forget that in earlier times, and in many remote areas to this day, different linguistic environments prevailed. Overall, we suspect that for most of human history, the wave theory more accurately captures the process of language change than does the standard phylogenetic model. Yet in the most general terms, the two models complement each other relatively well.\n*Debate does rage, however, about whether the so-called “non-configurational languages” such as the Australian language Warlpiri, have subjects and objects in the same sense as the more familiar, “configurational” languages like English or French. The reader is referred to Baker (2001) for evidence of subject-object asymmetries in such non-configurational languages.\n**For example, Hindi makes a phonemic distinction between aspirated and unaspirated voiced stops, has fusional case/number morphology, subject-object-verb word order, postpositions, and uses the ergative-absolutive alignment in the preterite and perfect tenses; English, in contrast, has no aspirated voiced stops (and does not use aspiration phonemically at all), has largely abandoned fusional morphology, has lost the case system except with pronouns, employs a subject-verb-object word order, uses prepositions rather than postpositions, and is characterized by nominative-accusative alignment.\n***Ironically, the diffusion analogy of Bouckaert et al. may be best suited to describing dialectal continua rather than divergence and expansion of languages and language families; we shall return to this point in a forthcoming post.\nBaker, Mark C. (2001) The Natures of Nonconfigurationality. In Mark Baltin and Chris Collins (eds.) The Handbook of Contemporary Syntactic Theory. Oxford: Blackwell. Pp. 407-438.\nFind this post valuable? Please pay it forward by sharing it with others:"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:0959cf51-db21-4eab-b91f-9acf332eb87f>"],"error":null}
{"question":"As maritime history researcher, what were the key contrasts between Venice's and Omiš Pirates' control of the Adriatic Sea?","answer":"While the Omiš Pirates controlled the Adriatic Sea from Split to Dubrovnik during the 12th and 13th centuries, attacking merchant ships of rich cities like Venice, Venice itself had a much broader and longer-lasting maritime dominance. Venice conquered Dalmatia in 1000, expanded its territory in the eastern Adriatic after capturing Constantinople in 1204, and maintained significant maritime power until its decline began with the discovery of America in 1492 and the Cape route to the East in 1497.","context":["NATIONAL PARK KRKA\nIf you decide to visit the breathtaking waterfalls of the River Krka, near Šibenik, you will certainly not regret it. The main attraction of Krka National Park is its 7 waterfalls. Skradinski Buk is the longest waterfall on the Krka River and one of the best known natural beauties of Croatia. The perfect beauty of the powerful river flow and its relaxing atmosphere are a natural wellness center that will soothe and refresh you, bring you back to life. Visovac Lake is situated inside the national park. In the middle of the lake you will find Visovac Island, which is home to a Franciscan monastery.\nSPLIT - CITY CENTER\nA monumental palace turned into a city – this is how we can summarize the essence of Split, the most important urban center of middle (central) Dalmatian coast. Split is the largest city in Dalmatia and second largest city in Croatia.\nDue to unique and magnificent architecture of Diocletian’s Palace and the entire historical center, Split was chosen as a UNESCO World Heritage Site in 1979. The city center is a true ”history keeper”. Elements of ancient Roman Empire as well as Middle Ages are seen in the structure of historical buildings and the city square. Walking around the central area of Split will take you back to the past. With each step you open the door to centuries long gone and get to know the architecture history: the ancient Peristyle Square, medieval Romanesque Church of Saint Martin, Gothic palace, Renaissance and Baroque facades of the noblemen’s houses…\nOther interesting sites in Split are: the Temple of Jupiter – a place where Croatian kings were baptized, Cathedral of St. Duje (named for the patron saint of Split) and monastery of Saint Anthony near Poljud Stadium.\nIn addition to the notable examples of historical architecture, Split is famous for its aliveness. Numerous socio-cultural happenings like film and music festivals, sports events, rich nightlife, shops, fruit and vegetable markets, restaurants and coffee shops make this city exceptionally dynamic and alluring. It is worth to highlight Ultra Europe Festival, popular music event held in Split. Every year, the festival is attended by up to 100 thousand young people from more than hundred countries around the globe.\nIf you want, you can create your own sightseeing map here: Do it yourself sightseeing map of Split\nJust north of Split, there is another Dalmatian UNESCO-listed town – Trogir.\nThis town was founded by Greek colonists from the Island of Vis in the 3rd century BC. Its current name is actually a modern variant of historical Tragurium. The town is situated on an island between Čiovo and Central Dalmatian coast, in the Bay of Kaštela, and is connected to the mainland by a stone bridge. Trogir is known as one of the best-preserved Romanesque-Gothic towns in Central Europe.\nTrogir is certainly not lacking architectural distinctiveness, and one the finest examples is Radovan’s Portal, a unique monumental work of the great medieval architect Master Radovan. He signed his name on the main portal where he is celebrated as “the best of all in this art“. The magnificent portal of Cathedral of St. Lawrence was done in the 13th century, and is famous for its sculptures.\nThe old town core was formed between the 13th and 15th century inside defense walls that were restored by Venice in the 15th century. Venetians built two additional forts: Castle Kamerlengo and the Tower of Saint Marc.\nThe narrow stone streets reflect the rich history of the city and tell stories from centuries long ago, wonderfully illustrated by interesting art collections that include notable masterpieces. Due to an abundance of cultural-historical heritage, as well as natural beauty, Trogir is extremely popular among tourists.\nBrač is an island just opposite Split. It is famous for its Zlatni rat (Golden Cape) Beach that looks like a triangle, and is listed among the ten most beautiful beaches in the world. The tip keeps changing its shape constantly due to the influence of winds, waves and sea currents. Due to very favorable winds, Zlatni rat is popular among windsurfers.\nAnother town with typical Dalmatian architecture. Hvar is a very popular destination among celebrities from all over the world.\nAuthentic Mediterranean atmosphere, abundance of sunny days (Hvar is famous for being the sunniest isle of the Adriatic), soothing climate, rich traditional Dalmatian gastronomy and various happenings during the summer season make this island a must-see tourist attraction.\nThis picturesque city is situated 20 km south from Split, and it will tell you stories from years ago about pirates!\nDuring the 12th and 13th centuries, Omiš Pirates ruled the Adriatic Sea from Split to Dubrovnik and attacked merchant ships of rich cities like Venetia. In the 15th century, the city fell under the rule of Venetian Republic and became an important administrative and military center, protected by defensive walls. The traces of that time are still present in the contemporary architecture of the city.\nOne of the attractions related to Omiš is also the Cetina River, as well as its canyon. The abundant natural beauty offers fantastic opportunities for an active vacation.","If you came to this page directly and do not see a navigation frame on top, please go to the home page.\nVenice is certainly one of the most fascinating places in Europe. It is located in the centre of the Laguna Véneta, the largest of the northern Italian lagoons. The waters on average are only 50 cm deep and are riddled with numberless islands and sandbanks. The lagoon was formed during tens of thousands of years by the rivers Sile, Brenta and Piave. Their sediments formed the extended streches of land closing the lagoon towards the Gulf of Venice. Only three openings, called 'porti', allow the exchange of water with the open sea. The historical centre of Venice developed on numerous islands in the centre of the lagoon. During the 13th century the people of Venice started to replace their wooden buildings by stone edifices. In order to build their magnificent palaces uncountable numbers of oak trees were felled at the Dalmatian coast (which since that time is largely bare of vegetation). The trees were cut into poles that were driven into the underwater ground to support the wood, brick and marble layers that served as foundations. The historic centre of Venice is composed of about 10,000 buildings, separated by 150 canals, which are spanned by 411 bridges. Venice and its lagoon were listed as a UNESCO World Heritage site in 1987 (see also list of other UNESCO heritage sites).\nThe early period of of the history of Venice began with Attila's Hun invasion of the mainland in 452 AD, which caused the population of the countryside to take refuge on the islands of the lagoon and augment the local population of fishermen. The first council of 12 townships in the lagoon was formed in 466 and the mainland settlements were abandoned in 568 during Alboin's Lombard invasion. The election of the first Doge in 727 finally marks the foundation of the Republic of Venice. The settlement at Rivo Alto (Ri'Alto), the location of modern Venice, was consolidated in 810 during the invasion of king Pippin of Italy. After the turn of the first millenium, the period of expansion of the Venetian empire began. Dalmatia was conquered in 1000, follwed by the capture in 1204 of Constantinople by Venice and the 4th Crusaders, which again increased the territory of Venice in the eastern Adriatic. The period between 1255 and 1380 was marked by four wars with the rivalling republic of Genova (Genoa). During the 14th and 15th century Venice mainly expanded on the mainland: Treviso in 1339/1388, Padua, Vicenza and Verona in 1404, Aquilea and Friuli in 1418/1421, the bay of Cattaro (Kotor) with the exception of Castelnuovo (Herceg Novi) in 1420, Brescia and Bergamo in 1428, Rovigo and Polesine in 1484. Constantinople was lost to the Ottomans in 1453, but Cyprus was conquered in 1489. The discovery of America by Columbus for Spain in 1492 and the discovery of the Cape route to the East for Portugal by da Gama in 1497 marked the beginning of the period of decline of the Venetian power. The Leage of Cambrai, a confederation formed in 1508 by the Papal States, France, Spain, Hungary and others, inflicted a decisive defeat upon Venice in 1509. The Ottoman empire finally reconquered Cyprus in 1571, Crete in 1669, and Morea in 1716. The invasion of the French army under Napoleon in 1797 and the surrender of the Great Council of Venice on the 12th of May of that year ended the existence of the Republic of Venice after 1,070 years. Between 1815 and 1866 Venice, together with the rest of Venetia and Friuli, was part of the Austrian kingdom of Lombardy and Venetia, which in 1859/1866 became part of Italy (see also the map of the Austrian possessions in Italy 18151866).\nThe Basilica Cattedrale Patriarchale di San Marco [left] is certainly one of the most famous landmarks of Venice.\nThe original church on this spot was begun in 829 and was consecrated in 832 in order to enshrine the remains of St. Mark the Evangelist,\nwhich had been brought to Venice from Alexandria in Egypt. Thereupon, St. Mark replaced St. Theodore\nas the patron saint of the city and his attribute, the winged lion, became the official symbol of the republic. The first basilica was\ndestroyed during a popular revolt in 976, but was subsequently replaced by the present church, which was completed in 1071.\nThe relics of St. Mark had been lost in the fire of 976 but had miraculously \"reappeared\" when the new church was consecrated in 1094.\nThe edifice was built in Byzantine style on a ground plan of a greek cross surmounted by five domes. Throughout the following centuries\nthe church was enriched by numerous artworks. The famous four bronze horses on the west façade were brought to\nVenice from Constantinople in 1204 during the 4th Crusade. They were originally placed in the Arsenal of Venice and were set up\non the exterior of the basilica in the mid 13th century. The sculptures were removed to Paris by\nNapoleon, but were returned to the church in 1815. The interior of the church is decorated by numerous mosaics on gold ground,\nthe floor is inlaid with marble and glass. The screen separating the choir and the nave holds marble statues, which count as\nmasterpieces of Venetian Gothic sculpture. The church originally was not a cathedral but served as the Doge's chapel.\nIt did not become a cathedral until 1807 when the seat of the patriarchs was transferred from Castello to Venice.\nA note on the labelling on the glass no. 1533 [above left]: Although the glass is labelled in Italian, it is apparent that the craftsman who drew the original drawing for the print was not familiar with the Italian language because \"Chiesa S. Marco\" is falsely spelled 'Marko' instead of 'Marco'.\nThe Lion of Venice [centre] is an ancient bronze winged lion sculpture in the Piazzetta San Marco, which came to symbolise the city — and of course of one of its patron Saint, St. Mark — after its arrival there in the 12th century. The Lion of Venice surmounts one of two large granite columns in the Square, thought to have been erected about 1268, bearing ancient symbols of the two patron saints of Venice. The Lion sculpture has had a very long and obscure history, probably starting its existence as a winged lion-griffin statue on a monument to the god Sandon at Tarsus in Cilicia (Southern Turkey) about 300 BC. The figure, which stands on the eastern column, at some point came to represent the “Lion of Saint Mark”, traditional symbol of Saint Mark the evangelist.\nThe figure standing on the western column [right, barely visible] is\nSan Giorgio Maggiore [background] is a Benedictine church on the island of the same name located in the lagoon\nopposite the Piazzetta. The island was occupied by the Venetians already in the Roman period. By 829 it had a church consecrated to St. George; thus, it was\ndesignated as San Giorgio Maggiore to be distinguished from San Giorgio in Alga (an island in the Venetian lagoon).\nThe San Giorgio Monastery was established in 982. The present church was built by Andrea Palladio between 1566 and 1610.\nThe campanile (belltower) was first built in 1467, but fell in 1774. It was rebuilt in neo-classical style in 1791.\nIn the early 19th century, after the Republic fell, the monastery was almost suppressed and the island became a\nfree port with a new harbour built in 1812. It became the home of Venice's artillery.\nSt. Mark's Campanile [below, no. 2778: left]\nis the bell tower of St Mark's Basilica (see above), located in the Piazza San Marco. It is one of the most recognizable symbols of the city.\nThe tower is 98.6 metres (tall, and stands alone in a corner of St Mark's Square, near the front of the basilica. It has a simple form, the bulk of which is a\nfluted brick square shaft, 12 metres wide on each side and 50 metres (tall, above which is a loggia surrounding the belfry, housing five bells. The belfry is\ntopped by a cube, alternate faces of which show the Lion of St. Mark and the female representation of Venice (\"la Giustizia\", Justice). The tower is capped by a\npyramidal spire, at the top of which sits a golden weathervane in the form of the archangel Gabriel.\nThe initial 9th-century construction, built on Roman foundations, was used as a watch tower or lighthouse for the dock, which then occupied a substantial part of\nthe area which is now the Piazzetta. Construction was finished in the 12th century. Seriously damaged by a fire in 1489 that destroyed the wooden spire, the campanile\nassumed its definitive shape in the 16th century thanks to the restorations made to repair further damage caused by the earthquake of March 1511.\nThe work was completed on 6 July 1513, with the placement of the gilded wooden statue of the Archangel Gabriel.\nThe original Campanile inspired the designs of other towers worldwide, especially in the areas belonging to the former Republic of Venice. Almost identical, albeit smaller,\nreplicas of the campanile exist in the Slovenian town of Piran and in the Croatian town of Rovinj; both were built in the early 17th century\nIn July 1902, the north wall of the tower began to show signs of a dangerous crack that in the following days continued to grow. Finally, on Monday, 14 July, around 9:45 am,\nthe campanile collapsed completely. Remarkably, no one was killed. Because of the campanile's position, the resulting damage was relatively limited.\nThe same evening, the communal council approved over 500,000 Lire for the reconstruction of the campanile. It was decided to rebuild the tower exactly as it was, with some\ninternal reinforcement to prevent future collapse. The new campanile was inaugurated on 25 April 1912, on the occasion of Saint Mark's feast day, exactly 1000 years\nafter the foundations of the original building had allegedly been laid.\nThe Doge's Palace [left, no. 2778: centre], built in Venetian Gothic style, also is one of the main landmarks of the city of Venice.\nThe palace was the residence of the Doge of Venice, the supreme authority of the Republic of Venice. In 810 the seat of government was moved from the island of Malamocco to the area of the present-day Rialto;\nHowever, no traces remain of that 9th century building as the palace was partially destroyed in the 10th century. A new palace was thenbuilt in the late 12th century out of fortresses,\none façade to the Piazzeta, the other overlooking the St. Mark's Basin. Although only few traces remain of that palace, some Byzantine-Venetian architecture characteristics can still\nbe seen at the ground floor. The new Gothic palace's constructions started around 1340, focusing mostly on the side of the building facing the lagoon. Only in 1424, did Doge Francesco Foscari\ndecide to extend the rebuilding works to the wing overlooking the Piazzetta, and with a ground floor arcade on the outside, open first floor loggias running along the façade, and the\ninternal courtyard side of the wing, completed with the construction of the Porta della Carta (1442). In 1483, a violent fire broke out in the side of the palace overlooking the canal, where the\nDoge's Apartments were. An entire new structure was raised alongside the canal, stretching from the ponte della Canonica to the Ponte della Paglia. Another huge fire in 1547 destroyed\nsome of the rooms on the second floor, but fortunately without undermining the structure as a whole. Refurbishment works were being held at the palace when on 1577 a third fire destroyed\nthe Scrutinio Room and the Great Council Chamber. In the subsequent rebuilding work it was decided to respect the original Gothic style. As well as being the ducal residence, the palace\nhoused political institutions of the Republic of Venice until the Napoleonic occupation of the city in 1797, when its role inevitably changed. Venice was subjected first to French rule,\nthen to Austrian, and finally in 1866 it became part of Italy. Over this period, the palace was occupied by various administrative offices as well as housing the Biblioteca Marciana and\nother important cultural institutions within the city. By the end of the 19th century, the structure was showing clear signs of decay, and the Italian government set aside significant funds\nfor its restoration and all public offices were moved elsewhere, with the exception of the State Office for the protection of historical Monuments, which is still housed at the palace's\nloggia floor. In 1923, the Italian State, owner of the building, entrusted the management to the Venetian municipality to be run as a museum. Since 1996, the Doge's Palace has been part of\nthe Venetian museums network, which has been under the management of the Fondazione Musei Civici di Venezia since 2008.\nThe Grand Canal (Canal Grande) is the major water-traffic corridor of the city. It is the final section of the river Brenta.\nThe banks of the Grand Canal are lined with more than 170 buildings, most of which date to 13th to the 18th century and demonstrate the welfare and art\ncreated by the Republic of Venice. The noble venetian families faced huge expenses to show off their richness in suitable palazzos.\nBecause most of the city's traffic goes along the Canal rather than across it, only one bridge crossed the canal until the 19th century, the Rialto Bridge (see below).\nThere are currently two more bridges, the Ponte degli Scalzi and the Ponte dell'Accademia. A fourth, controversial bridge (Ponte della Costituzione) designed by\nSantiago Calatrava was opened in 2011, connecting the train station to the vehicle-open area of Piazzale Roma. As was usual in the past, people can still take\na ferry ride across the canal at several points by standing up on the deck of a simple gondola called traghetto.\nMost of the palaces emerge from water without pavement. Consequently, one can only tour past the fronts of the buildings on the grand canal by boat.\nThe Basilica di Santa Maria della Salute (Our Lady of Health) [background right], commonly simply known as the Salute,\nstands on a narrow finger of land between the Grand Canal and the Bacino di San Marco.\nIn 1630 Venice experienced an unusually devastating outbreak of the plague. As a votive offering for the city's deliverance from the pestilence, the Republic of Venice\nvowed to build and dedicate a church to Our Lady of Health (or of Deliverance, Italian: Salute). The church was designed in the then fashionable Baroque style by\nBaldassare Longhena, a pupil of the Venetian architect Andrea Palladio, and construction began in 1631. Most of the objects of art housed in the church bear references\nto the Black Death. The dome of the Salute was an important addition to the Venice skyline and soon became emblematic of the city, inspiring artists like Canaletto,\nThe Rialto Bridge (Ponte di Rialto) is one of the best-known sights of Venice. It was built in 15881591 by Antonio da Ponte who had won the contract against such eminent architects as Michelangelo and Palladio. The marble bridge (48 m long, 22 m wide, span height 7.5 m) which spans the Canal Grande at its narrowest spot remained the only bridge across the canal for more than 250 years until the Accademia Bridge was built in 1854. Each of the two bridge heads was built on a foundation of 6,000 piles. The bridge has three walkways: two along the outside ballustrades and a wider, central one between two rows of small shops.\nThe tower depicted in the right background of the picture on glass no. 2987 [near left] belongs to the\nGlasses no. 2986, 2987 and 2988 (see already above) are labeled as souvenirs for\nAs the three glasses are typical souvenir items it is not likely that"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:640b6eea-642d-4e03-8cf0-1bfe87cb7319>","<urn:uuid:dd44c1f1-7085-41c7-92c8-4b71547ff894>"],"error":null}
{"question":"As a fish market enthusiast, I'd like to know what interesting findings came from studying the Fulton Fish Market competition?","answer":"A 1995 study of the Fulton Fish Market revealed that perfect competition was not apparent in the market. The research found that traders appeared to charge different prices to different ethnic groups, suggesting price discrimination practices rather than purely competitive pricing.","context":["|Alma mater||Princeton University 22|\n|Thesis||Who pays more? Essays on bargaining and price discrimination. (1994)|\n|Doctoral advisor||Orley Ashenfelter|\nKathryn Graddy is a professor of economics and the dean of Brandeis International Business School at Brandeis University. She is the Fred and Rita Richman Distinguished Professor in Economics at Brandeis University. Her research interests include the economics of art, culture, and industrial organization.\nGraddy received a BS in mathematics and a BA in Russian language from Tulane University, an MBA from Columbia University, and a PhD in economics from Princeton University. Prior to her position at the University of Oxford, she was appointed as an assistant professor of economics at the London Business School. Afterwards, she was a research fellow at Jesus College and then at Exeter College.\nAfter leaving Oxford, Graddy joined Brandeis University in 2007. She chaired the university's department of economics from 2011 to 2014 and directed the PhD program at Brandeis International Business School (IBS) from 2015 to 2016. In 2013, she was made the Fred and Rita Richman Distinguished Professor in Economics. She was named senior associate dean of IBS in 2016, a position she held until she became dean in 2018.\nAs a graduate student, Graddy spent a month shadowing a trader at the Fulton Fish Market to analyze competition within fish markets. In her 1995 study, she described how the traders appeared to charge different prices to different ethnic groups, and perfect competition at the market was not apparent.\nIn a 2017 paper co-authored with Princeton University economist Carl Lieberman, the two economists concluded that \"artists, in the year following the death of a friend or relative, are on average less creative than at other times of their lives.\" The results were based on an analysis of thirty-three French Impressionist painters and fifteen American artists born between 1910 and 1920. The results, which were featured in the popular press, followed an earlier working paper by Graddy that found that visual artists' creative output suffered during periods of bereavement.\n- Ashenfelter, Orley; Graddy, Kathryn (September 2003). \"Auctions and the Price of Art\". Journal of Economic Literature. 41 (3): 763–786. doi:10.1257/002205103322436188.\n- Joshua D. Angrist; Kathryn Graddy; Guido W. Imbens (July 2000). \"The Interpretation of Instrumental Variables Estimators in Simultaneous Equations Models with an Application to the Demand for Fish\". The Review of Economic Studies. 67 (3): 499–527. doi:10.1111/1467-937X.00141. ISSN 0034-6527. Wikidata Q108879150.\n- Beggs, Alan; Graddy, Kathryn (Autumn 1997). \"Declining Values and the Afternoon Effect: Evidence from Art Auctions\". The RAND Journal of Economics. 28 (3): 544. doi:10.2307/2556028.\n- Beggs, Alan; Graddy, Kathryn (May 1, 2009). \"Anchoring Effects: Evidence from Art Auctions\". American Economic Review. 99 (3): 1027–1039. doi:10.1257/aer.99.3.1027.\n- Graddy, Kathryn (1995). \"Testing for Imperfect Competition at the Fulton Fish Market\". The RAND Journal of Economics. 26 (1): 75. doi:10.2307/2556036.\n- ^ a b c \"Kathryn Graddy named dean of Brandeis University International Business School\". Brandeis University. May 1, 2018. Retrieved December 5, 2020.\n- ^ \"Kathryn Graddy receives honorary doctorate in Copenhagen\". Brandeis International Business School. March 22, 2018. Retrieved November 17, 2021.\n- ^ \"Kathryn Graddy named the Fred and Rita Richman Distinguished Professor of Economics\". Department of Economics Blog. Brandeis University. January 31, 2013. Retrieved November 17, 2021.\n- ^ \"Kathryn Graddy\". Brandeis International Business School. Archived from the original on December 5, 2020. Retrieved December 5, 2020.\n- ^ Rocheleau, Matt (November 27, 2018). \"In today's market, could the stolen Gardner art be worth $1 billion?\". The Boston Globe. Retrieved May 1, 2021.\n- ^ Ehrenfreund, Max (May 12, 2015). \"Why this Picasso sold for $180 million and what it tells us about the super rich\". The Washington Post. Retrieved May 1, 2021.\n- ^ Carvajal, Doreen (July 13, 2015). \"Danh Vo and Bert Kreuk's Legal Battle Pits Artist Against Collector\". The New York Times. Retrieved May 1, 2021.\n- ^ Harford, Tim (June 22, 2007). \"What do fish markets teach us about the economy?\". Slate. Retrieved December 5, 2020.\n- ^ \"How deep are your pockets?\". The Economist. June 30, 2012. Retrieved December 5, 2020.\n- ^ Jacobs, Tom (December 5, 2017). \"Contrary to Cliché, Misery May Inhibit Creativity\". Pacific Standard. Retrieved December 5, 2020.\n- ^ Voon, Claire (December 6, 2017). \"\"Tortured\" Artists Are Actually Less Creative, Study Suggests\". Hyperallergic. Retrieved December 5, 2020.\n- ^ Malvern, Jack (December 24, 2015). \"There's no need to suffer for your art\". The Times. Retrieved December 5, 2020.\n- ^ Clark, Nick (December 22, 2015). \"An academic has debunked the myth of the tortured artist\". The Independent. Retrieved December 5, 2020.\n- ^ Blakemore, Erin (January 8, 2016). \"Grief May Not Make Artists Better\". Smithsonian. Retrieved December 5, 2020."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:2908348b-2a22-4b67-b49b-ee4905289e20>"],"error":null}
{"question":"What are the success factors and evaluation metrics for manufacturing collaborations?","answer":"Manufacturing collaborations require several key elements for success. According to collective impact theory, these include having a common agenda, mutually reinforcing activities, continuous communication, shared measurement systems, and backbone support. For evaluation, organizations should use metrics that assess both process and impact, including community change outcomes like policy achievements and program expansions, as well as capacity outcomes such as new skill development and partnerships. Additionally, practical evaluation should include risk management metrics, such as insurance coverage adequacy, provisions for self-funded losses, and potential areas of exposure or hidden liabilities in shared equipment and processes.","context":["Hi! My name is Amy Hilgendorf and I am the Associate Director for Engaged Research at the University of Wisconsin-Madison Center for Community and Nonprofit Studies (the CommNS). We specialize in community-based action research and evaluation partnerships with grassroots and nonprofit groups and offer support to others who do this work.\nIn recent years, we have partnered with county-based and statewide coalitions that are seeking to address childhood obesity by applying a model of collective impact. John Kania and Mark Kramer first characterized collective impact as consisting of five key conditions that can help unite multi-sector collaborative efforts towards greater community impact than what isolated efforts can achieve. Those five conditions are: a common agenda, mutually reinforcing activities, continuous communication, shared measurement systems, and backbone support. The coalitions we work with have found the collective impact model offers valuable guidance for the kinds of processes that will set them up for achieving impact, but questions remain about how to actually evaluate the impacts of collective impact.\nThe Collective Impact Forum is an online hub of information, resources, and peer networking related to collective impact. The searchable resources section includes a host of “Evaluation” resources. One tool is the Guide to Evaluating Collective Impact by Hallie Preskill, Marcie Parkhurst, and Jennifer Splansky Juster. While much of this guide focuses on evaluating the process of collective impact, the third part lists suggested behavior changes and systems changes that may result from collective impact initiatives and provides ideas of indicators and approaches for evaluating these changes.\nWe have found it critical to remember that collective impact is not necessarily a new concept, but rather one that has emerged from a long tradition of collaborative and coalition practice and thinking. Literature on this topic stretch back more than 30 years, especially in the community psychology field, and includes theory and practical tools for assessing the process and impact of collaborative work.\nIn particular, the Community Coalition Action Theory developed by Fran Butterfoss and Michelle Kegler synthesizes much of this research to suggest how coalition practices can lead to different kinds of community impacts. These theorized impacts include community change outcomes, such as policy achievement and program expansions; community capacity outcomes, like new skill development and new partnerships; and, over time, the health and social outcomes that are the target of the coalition’s work. Additionally, we have found that Michelle Kegler and Deanne Swan’s efforts to empirically test the relationships in this theory offers especially useful guidance for “connecting the dots” between evaluation of coalition processes, including implementation of collective impact practices, and evaluation of community impacts.\nThe American Evaluation Association is celebrating Community Psychology (CP) TIG Week with our colleagues in the CP AEA Topical Interest Group. The contributions all this week to aea365 come from our CPTIG members. Do you have questions, concerns, kudos, or content to extend this aea365 contribution? Please add them in the comments section for this post on the aea365 webpage so that we may enrich our community of practice. Would you like to submit an aea365 Tip? Please send a note of interest to firstname.lastname@example.org. aea365 is sponsored by the American Evaluation Association and provides a Tip-a-Day by and for evaluators.","To remain competitive, today’s manufacturers and automotive companies must produce more at a faster pace. These companies must evolve their production processes — and even their business models — while grappling with costly technological advances and vast labor shortages.\nThis pressure is leading many in the industry to form strategic alliances — sometimes with competitors — to keep pace with heightened customer expectations. Doing so, however, comes with its own set of challenges, making risk management and insurance critical components of these arrangements.\nA Changing Landscape\nManufacturing and automotive companies are facing digital disruptions that have forced them to transform production processes to better meet customer demands, including delivering more product variety and shortening production cycles. Robotics, 3D printing, nanotechnology, and blockchain are just a few of the digital solutions allowing manufacturers to adjust to changing customer needs.\nAutomakers, in particular, have been challenged to adapt. Not only are they expected to go to market with new and improved models at an unprecedented rate, they are now attempting to solve for driverless cars and are fighting for space in the rideshare market to diversify their business. It’s likely that none of this would be possible without 3D prototyping, connected factories, or the tech-driven shared economy fueling the rideshare marketplace.\nAt the same time, technology is increasing competition, creating demand for more skilled labor and presenting evolving privacy, security, and safety issues. Adding to these challenges is the integration of new technology into manufacturing processes, which can be costly and complex; solutions can quickly become outdated, and updating one network or system often requires altering others so they are compatible.\nManufacturers face great risk from what is often deemed the “fourth industrial revolution.” They are susceptible to intellectual property theft by cyber hackers, business interruption from system shutdowns, and financial losses and reputation damage from hefty development costs for products that do not meet revenue expectations.\nTeaming Up for Better Results\nThese mounting risks are leading manufacturers and automotive companies to form strategic alliances, which are formal partnerships whereby two independent companies remain separate entities but share resources or collaborate on projects for their mutual benefit.\nSuch strategic alliances often involve intertwining supply chains, technology, production locations, and/or finances. For instance, some automakers share technology and facilities to develop autonomous driving systems and electric vehicle platforms, while many food and beverage manufacturers share distribution networks and facilities to expand their reach.\nThese alliances can be beneficial, allowing collaborating companies to:\n- Improve product development: Two minds are better than one. When two similar or complementary companies collaborate, they can bring different resources to bear, which can lead to real transformation. Rather than creating “separate but equal” standard products, they can unite to create truly innovative products that will move the needle for both companies.\n- Reduce costs: Innovation is not cheap, but it can cost less in a strategic alliance. The aligned parties can split the cost of procuring or implementing new technology and share facilities, which can reduce redundant capital expenditures for expensive technology and equipment at separate locations, and benefit from economies of scale along their supply chains. Ultimately, if a collaborative project fails, it usually costs less than if the parties were going it alone.\n- Enter new markets and grow more easily: Strategic alliances can speed up research, development, and production of new products and up the momentum on distribution of longstanding products debuting in new markets. Collaborating parties can piggyback off one another’s already established production or distribution platforms in specific locations. As such, companies are able to overcome production barriers and local cultural and operational obstacles that would otherwise hamper speed to market.\nPerforming Your Due Diligence\nUltimately, strategic alliances can produce better outcomes at a lower cost, taking companies from barely surviving to thriving in today’s competitive marketplace. However, such partnerships are not without potential challenges. In fact, not properly managing a strategic alliance could actually increase your risk.\nUnfortunately, strategic alliances often fail. Nearly half of the respondents to a 2014 study on strategic alliances by the CMO Council and Business Performance Innovation Network reported strategic alliance failure rates of 60% or more.\nRecently, two major automakers partnered to trade one’s hybrid technology for the other’s access to a particular geographical territory. Both parties failed to agree upon terms and deliver on their promises. This not only resulted in a failed alliance, but also prompted international arbitration.\nMeanwhile, two large telecommunications manufacturers jointly acquired a portion of another smaller telecom company and made a significant investment in a burgeoning industry. But the industry never took off because consumers were not interested, and the alliance failed within just one year of their agreement.\nThese examples highlight how strategic alliances can be difficult to manage, despite their potential value. However, with adequate due diligence and proper planning, potential partners can avoid common missteps and instead find allies with compatible business cultures, aligned objectives and strategies, and agreeable terms regarding their operational business arrangements and how profit pools will be shared.\nDue diligence should go beyond high-level company culture and business strategy research, taking into account potential partners’ risk management and insurance programs as risk and insurance can become especially murky in strategic alliances. For instance, which company’s insurance responds first in the event of a claim? Even if the answer to that question is straightforward, how will partners manage through the frustration or financial burden if one party seems to shoulder disproportionately more risk?\nWhen considering a strategic alliance, organizations should take the following steps to conduct effective risk and insurance due diligence:\n- Evaluate a potential partner’s approach to risk management, loss control, and claims management.\n- Review insurance policies to identify any gaps in coverage.\n- Determine the extent to which deductibles or retentions will affect the quality of earnings.\n- Assess the adequacy of provisions for self-funded losses on the balance sheet.\n- Examine property schedules and create a risk map to determine if shared equipment, processes, and activities, would occur at inherently risky locations.\n- Identify potential areas of exposure or hidden liabilities.\n- For cross-border alliances, consider any unique in-country risk management and insurance requirements.\n- Assess — qualitatively and quantitatively — a potential partner’s risk profile, including benchmarking insurance programs and reviewing financial security of current and historical insurers.\n- Develop pro-forma insurance cost projections with respect to a partner’s total cost of risk.\n- Quantify historical liabilities (for example, from self-insured programs) and identify any insurance-related one-off costs that could affect an alliance.\nAcquiring such information can be much more manageable with access to data and analytics and modeling tools. Guidance from insurance advisors with expertise in manufacturing, strategic alliances or similar arrangements, and any new geographic territories being explored for an alliance can substantially ease this process. And while it may seem like a significant undertaking, risk and insurance due diligence can reduce uncertainty and help prevent surprises. It can also help obtain a clear picture of the value of the liabilities and assets being shared — potentially enhancing strategic alliance agreements, operational costs, and corporate governance.\nTechnology and digitization continue to disrupt manufacturing and automotive industries. Amidst such industry dynamics, strategic alliances may be one of the few ways manufacturers and automotive companies can keep up with the demands to expand product offerings while shrinking production timelines. And while adapting a strategic alliance business model might involve some growing pains, it can also result in real transformation for the manufacturing and automotive industry, helping these companies thrive in an ever-changing environment."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:73151d26-a51d-49f2-a772-170aaa74739c>","<urn:uuid:1ff5e8fe-1623-4a81-9e07-2b441bc6f4cc>"],"error":null}
{"question":"How do Sunday market schedules in central time zones compare with vertical farm production cycles?","answer":"While traditional markets like those featuring Greek music operate on specific schedules (such as Sunday afternoons from 4:00-6:00 PM Central time), vertical farms operate continuously 24 hours a day, 7 days a week. These indoor farms use LED lighting systems to control night-and-day cycles artificially, allowing for constant production regardless of time of day or season. This enables facilities like AeroFarms to achieve crop yields that are 75 times greater per square foot than traditional methods on an annual basis.","context":["Music from the Hills (Music)\nWith John Uhlemann\nSun Dec 4th 2011 4.00pm–6.00pm\nR=listener request. N=music new at the station. L=local music. Your purchases using the Buy it! links\nTime zone: central\nGiorgos Demelis, Stavros Kouskousidis, & band “Hasapiko tis polis” from To Sygrótima Thamiris (HXO 1999) —the \"polis\" of the title is Constantinople; this style of music originally ame from Asia minor and the Rebetika traditionBuy it!\nSophia Bilides “From Athens (Apó tin Athina / Azizie)” from Greek Legacy (E. Thomas Recordings 2006) —This recording originally came out in 1991, and has been reissued for obvious reasons. This song is from the Island tradition.Buy it!\nManos Hadjidakis “The Lantern (laterna music)” from Never On Sunday (Liberty 1983) —Hadjidakis is the composer, but this is a street organ playing this melody. these were common in years past , but were banned for a while. Many pieces were composed for them. as well as favorite tunes of the time re-set for this mechanical device.Buy it!\nGrigoris Maninakis “Γλυκεια Μου Περσεφονη” from H Mpompa (Horizon productions 2002) —We end this seet of Rebetic music with this Hasapiko-style song. this group came to the University of Missouri-St. Louis campus a few years ago, which is where I got the recording.Buy it!\nGiorgos Demelis, Stavros Kouskousidis, & band “I Agapi einai karfitsa (karsilama)” from To Sygrótima Thamiris (HXO 1999) —a dance, sometimes done in couples, in 9/8 time. It is done in Turkey and in both rural Thrace and in the cities of GreeceBuy it!\nVangelis Papanastasiou “Koulouriastos” from Thrace - Songs and rhythms of Thrace (Dodoni 2001) —\"koulouriastos\" means \"to stir around\", and refers to the way the line is folded and coiled up and then uncoiled.Buy it!\nGiorgos Parenis (vocal & oud), Nikos Alvanopoulos (klarino) “Hasapia medley-1 (Thelo Na Pantrefto)” from Horeftikos Omilos Thessalonikis Thrakiotiko Glenti 2 (alpha records 2007) —this and the next 2 \"tracks\" are actually the same long take, but artificially cut up into tracks. The dance is a simple fast 3-measure structure that forms the basis for so much Balkan dancing.Buy it!\nGiorgos Parenis (vocal & oud), Nikos Alvanopoulos (klarino) “Hasapia medley-2 (Ehe Gia Panta Gia)” from Horeftikos Omilos Thessalonikis Thrakiotiko Glenti 2 (alpha records 2007)Buy it!\nGiorgos Parenis (vocal & oud), Nikos Alvanopoulos (klarino) “Hasapia medley-ending (Organiki Hasapia)” from Horeftikos Omilos Thessalonikis Thrakiotiko Glenti 2 (alpha records 2007)Buy it!\nVangeli & Taki Betziou “Hasaposerviko” from Halkina me to Sygrotima tou Vangeli & Taki Betziou (Ziogas o.e.) —almost a field recording, this family band from Macedonia is playing the same 2/4 time hasapiko just heard, but in Macedonian style. They use a well-known Serbian kolo tune for the danceBuy it!\nXanthippi Karathanassi “My love sent word to me (Me Minise I Agapi MOU - Halkidiki, Macedonia)” from Songs and tunes of Macedonia (Crete University Press 1992) —a beautiful slow song from Greek Macedonia. This recording has a 99-page booklet, with music, words, and essays, in both Greek and English. There is not a bad cut on the CD.Buy it!\nΠέτρος Γα'ι'τάνος “Κυρά Μαρία” from Σαν Τα Κρύα Τα Νερά (Mercury 2000) —this begins with a \"brass\" and davul (large couble headed drum) un-metered section in imitation of the slow dances favored by men in the region. It is interesting that this intro should be presereved, even though it had to be done on synthesizer...Buy it!\nLike everything else, times are approximate. Spinitron and this station are not liable for errors or omissions.","In case you haven’t noticed, the world’s population is drastically moving. In two directions: upwards and inwards. By 2050 we will need to feed 9.6 billion people. A whopping 133% more than today’s 7.2 billion and 2200 times as many mouths than the population of New Zealand right now. It’s moving inwards too. Today, more than half of us live in densely populated urban environments. By 2050 66% (or 6.3 billion) of us will. This means one thing. An unprecedented demand for food – in urban areas.\nA scary thought, given food production and agriculture are already shockingly taxing on the natural environment – and in many cases – our health too. From water, energy and land use to the nasty issues surrounding fertilizers and pesticides, transportation and waste, conventional agriculture is far from earth friendly.\nSo it comes as no surprise that safeguarding global food security and sustainability for 9.6 billion people will mean doing more with less. And with the masses moving into our city centres, it will also mean doing things differently. Vastly differently. Especially when it comes to producing land and resource intensive (water and fertilizer) staples like horticultural land-based foods, including leafy greens, fruit and veggies. Because, while we can 3D print and bioengineer almost anything now (including the recently hyped Impossible Burger and other forms of juicy beef patties and protein alternatives made from real bovine or plant cells), one hard fact remains. We aren’t making any more land. And 3D printing can only go so far.\nMuch like the changing dynamics of our population itself, this will likely mean moving arable crop production inward (i.e. into urban centres) – and more importantly upward (i.e. into the ether). Think high-rise urban indoor farming – on a mass scale. A mixture of hydro, aqua and aeroponics – and at lofty heights.\nSome serious food for thought for New Zealand’s largely pasture and rural based agricultural and horticultural sectors valued at over 7.6 billion dollars – and nearly $4.3 of this export revenue (according to Industry publication Fresh Facts).\nData Science Meets Horticulture.\nIndoor (and vertical) farms are essentially highly controlled and automated multi-level growing environments that use a combination of software analytics, energy efficient LED lighting, sensor controls and closed loop moisture and nutrient recirculation systems to allow growers to monitor, nourish and grow plants like lettuces, greens and even fruit now using a fraction of – or no- resources such as water, soil and environmentally dubious fertilizers and pesticides. All year round. 24 hours a day. No matter what season and without a drop of real sunlight in sight. Yes, the veritable disruption of traditional field ag as we know it. How will our Kiwi field growers cope with such significant technological and social change?\nAnd before you naysayers out there think the world’s city dwellers will revolt at such a sterile, technologically complex concept such as industrial food production, think again. We’ve already seen how the “Impossible Burger” and Beyond Meat’s succulent meats range, including meatballs, patties and chicken strips are flying off the shelves in supermarkets throughout the States. Why would fruit and veg (which is actually the real thing) be any different? Something our traditional Kiwi horticultural farmers and the entire agricultural industry would be wise to take note of.\nThe arguments for high-rise farming are profound, to say the least. Indoor and vertical farming help to provide some very real and practical solutions to some of most pressing problems facing agriculture today, while being a highly profitable business model. And it’s happening already. In leaps and bounds.\nMirai, the world’s largest sensor-controlled hydroponic indoor vertical farm located in Japan, uses a closed nutrient recirculation system that requires 99% less water than traditional farming methods. Yes, 99% less. Spanning 25,000 square feet of vertical garden beds, its 17,500 energy efficient LED lights adapted with wavelengths to control the night-and-day cycle and accelerate growth mean highly productive and nutrient dense plants. 10,000 lettuces in a day productive – all bacteria and pesticide free.\nAeroFarms and Green Sense Farms, two (of many) US based mass scale vertical farming companies making huge inroads in this emerging market are also yielding similar environmental benefits. AeroFarm’s custom UV light spectrums, endless patented growth algorithms and 30,000 data points monitored by plant scientists mean it is able to produce leafy greens, herbs and lettuces en mass, 24/7, using almost no traditional field inputs in sight – including soil. “We use about 95 percent less water to grow the plants, zero soil, about 50 percent less fertilizer as nutrients and zero pesticides, herbicide, fungicides,” says David Rosenburg, AeroFarm CEO and co-founder. “Plants don’t need soil, they need nutrients. And they don’t need sun, they need spectrum of light”. According to Rosenburg, plants receive the perfect amount of moisture and nutrients misted directly onto their roots in a completely controlled environment. Sacré Bleu!\nAnd Green Sense Farm’s two industrial-sized, climate-controlled growing rooms outside of Chicago (each equipped with seven 12-meter tall grow towers) have over 14,000 LEDs that require less climate control, use less energy and absolutely no GMOs. Its cool burning green LED production modules mean lights can be placed closer to the plants, allowing for more levels to be stacked and more bang for your energy buck.\nCritics of vertical farming worry that the energy use involved in such growing techniques might increase the carbon footprint by an order of magnitude. However, with the energy efficiency of LED lighting increasing exponentially, and as solar powered renewable energy is rapidly reaching mainstream in major markets globally, this trepidation might become redundant before we know it.\nIn world with a constantly growing base of discerning consumers concerned about environmental and sustainability impacts of the food they are consuming, how will Kiwi field farmers compete with such staggeringly impressive resource efficiencies?\nThe Sky’s your Limit\nBut if environmental benefits aren’t enough to convince you of the real value of vertical farming then the numbers probably will. According to Caleb Harper, principal research scientist for the Open Agriculture (OpenAG) Initiative and CityFarm at MIT’s Media Lab, 30-40% of our diet (including greens, tomatoes and capsicums) could be produced in urban or peri-urban environments in the very near future – and it would be a lot better for us if it was.\nDubbed the “Vegetable Factory” of Japan, Mirai’s completely automated (including humidity, temperature, CO2, and irrigation) multi-level system spanning half the size of a football field is already 100 times more productive per square foot than traditional methods.\nRosenburg says AeroFarm’s indoor leafy crops grow twice as fast as regular field seeds inside its controlled environment and the company has specifically designed customisable stackable modules to get even more greens per square foot. At 12 levels high, this means crop yields that are 75 times greater per square foot than traditional field methods annually. What’s more, AeroFarm’s newest operation built in Newark, New Jersey, spans 70,000 square feet (6,503 square meters) of growing space 30 feet high – double that of its smaller (already productive) farms and leading Japanese competitors. “It has the capacity to grow just under 2 million pounds of baby greens annually” says Rosenburg.\nSignificant vertical economies also allow companies like AeroFarms to spread out operational costs (like HVAC, lighting and rent) amongst a much larger product base as well as reduce the cost of goods sold to the end consumer. “We sell at the same price that supermarkets buy from field farmers in the category of organics which is typically about a 20% premium” says Rosenburg. With layer after layer of thriving verdant greens sprouting day in – day out, the company says it is already cash-flow positive.\nAnd Green Sense Farms’ unique vertical stacking system in its 30,000-square foot (2,800 square metre) Chicago facility can distribute produce within 100 miles to over 20 million people. “At capacity, we’re producing about three to four million pounds of fresh produce a year” says CEO Robert Colangelo.\nFigures not to be sniffed at by even the most productive of Kiwi growers. Such unheard of levels of food productivity (volume) using a relatively miniscule footprint of land will be a godsend when considering the 9 billion people we’ll soon be having to cater for.\nWhat’s more, thanks to exponential improvements in robotics technology, Mirai and US vertical farms, including FarmedHere, along with AeroFarms, Green Sense Farms and Spread are also developing the technology to become robot-run farms devoid of human error. With Mirai already (human) hand harvesting over 10,000 heads of fresh lettuce, the mind boggles at just how many more greens these companies will be able to produce with robots in tow.\nTastebuds Gone Wild\nStill not sold? Controlled indoor farms (vertical included) – also mean crops are protected from insects, diseases and aren’t exposed to brutal weather conditions like drought and flooding – the bane of many farmers’ existence – making toxic chemical use and crop yield instability a thing of the past for growers. And for the consumer, this means healthy and nutritious food. Indoor farms like Aero, Mirai and Green Sense are entirely pesticide, herbicide, insecticide and fungicide free. Good news for health-conscious punters like me, since the last time I checked, no one wants to order a side of herbicide or chemicals with their salad.\nAeroFarms have taken this one step further by focusing as much on consumer nutrition as it does on low environmental impact. Greens are scrupulously macro & micro monitored for nutrient density, providing all the minerals and vitamins you could ask for. And the customers are lapping it up. Even top upmarket US restaurateurs, who would once have shunned the idea of buying indoor crops as part of their chic garden to table, local produce offerings, are sold on the stuff. “They taste the way greens are supposed to taste, the way I remember them tasting as a kid. There’s a flavour profile and if you put them up against greens from another big supplier, there’s no comparison in the freshness factor and in visual appeal” says Steven Yglesias, owner of a popular restaurant in Newark’s Ironbound neighbourhood.\nNo Nasty Food Miles\nBut perhaps one of the biggest advantages of vertical farming, at least when it comes to feeding the urban masses, is that you can build an indoor farm anywhere and to whatever size and height you want (within city limits). Without a ray of sun or speck of fertile NZ soil in sight. The sky’s your limit – quite literally. Mirai’s plant factory is located in an old semi conductor factory in eastern Japan’s Miyagi Prefecture – close to masses of urban mouths. Green Sense Farm’s green bounty is grown beneath 30 foot ceilings in a leased warehouse centrally located in an industrial park 40 miles outside of Chicago, a massive food distribution hub. And AeroFarm’s 9th domestic facility (yes 9th!) was built in a nondescript former paintball and laser tag facility in Newark, NY.\nIt also opens up the whole “buy local and fresh” model to the masses, traditionally a luxury only for the higher income earning echelons of society. Consumers everywhere will soon be able to eat nutrient dense fresh produce without the food miles. “When we build a farm, we become a part of the local community, bringing new life, to old buildings and reducing transportation miles” says Rosenburg.\nWhich begs the million-dollar question for our ag industry: Why buy clean green Kiwi produce when you can buy local and environmentally benign alternatives with no food miles attached? With indoor farms being built inside of, or in the fringes of massive cities, the need to transport (what once was) fresh and highly perishable farm produce (often) thousands of kilometres to urban consumers becomes close to ZERO. A huge plus for consumers and the environment, given the Tesla of the trucking world and mass scale electric transportation logistics have yet to reach mainstream.\nMoreover, what happens to our horticultural exports altogether (to the tune of $4.27 billion plus in 2015) as companies like Green Sense and AeroFarms rapidly gain stronger footholds in the lucrative markets we sell into? Green Sense has already signed a partnership to operate 20 mass scale vertical operations in China. The first farm is set to produce 750,000 to 1 million heads of lettuce and about 1.5 million leafy greens per year.\nOf course, not all of New Zealand’s horticultural exports are leafy greens suitable for inner city high rises. We export far more palatable plant based goodness than this. But it does beg the question as to how such technologies and bioscience could potentially disrupt these profitable hort subsectors too, and in a way that could possibly erode all our field pasture competitiveness we have steadily worked on over time. Our vital export revenue mainstay may be given a hideous shake up. Whether we like it or not.\nWhere to for Kiwi growers?\nWith stats like this, it is clear that vertical farming could play a key role in helping to avert a looming global food and environmental crisis – and one day become the future of agriculture. And with good reason.\nThe unheard of volume (productivity) of nutritious food that can be produced locally on the same footprint of land using a fraction of resources will be vital for feeding the 9 billion mouths we’ll soon have to cater for. And for areas where extreme weather conditions and resource scarcity routinely threaten their agricultural livelihood (think the sub-Saharan Africa, Middle East, United States and Southeast Asia) indoor farming techniques will very likely rise to vertical heights.\nThe question New Zealand now needs to ask itself is: As agrarian outdoor growers, what is our role in this rapidly changing new indoor, urban food producing dynamic? Yes, vertical farming may presently have found a niche in producing smaller lightweight crops that don’t require a huge amount of indoor real estate, all of which affords our fruit and larger crop growers some breathing space. For now. But we all know how quickly technologies can develop to shake up the status quo and erode industries overnight. Do we need to shift our focus away from bulky crops ill-suited to indoor warehouses for the future, or should we focus on plant varietals that labs and technology simply won’t be able to mimic in an indoor setting? Do we need to be riding on the coat tails of indoor farming’s technological success by investing in similar operations and technology? Or does New Zealand need to give up the ghost completely, let go of our entrenched desire to be leaders in agriculture and focus on new industry development altogether?\nThe answers remain to be seen. But such questions are highly worthy of asking – and planning for – right now."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:5ec6be68-908b-4f28-8f35-5dae14d53ea0>","<urn:uuid:f298393e-a09c-4ed1-9bf5-29662d9e86a3>"],"error":null}
{"question":"How do the geological formations of Mocona Falls and Angel Falls differ in terms of their vertical drops?","answer":"Angel Falls and Mocona Falls have vastly different vertical drops and geological characteristics. Angel Falls holds the record as the world's highest uninterrupted waterfall with a height of 3,211 feet (being 3 times as tall as the Eiffel Tower). In contrast, Mocona Falls has a much smaller vertical drop of 10-12 meters at its true waterfall section, though it creates a total drop of 25 meters over a 3-kilometer distance through its rapids system, formed by a unique submerged canyon in the Uruguay River that can reach depths of 100 meters.","context":["Since it’s that time of year when people start venturing outdoors looking for some adventure, we thought we’d highlight one of the most breathtaking natural wonders of our Earth… Waterfalls! There’s something pretty majestic about water cascading down a mountain from hundreds of feet in the air, so going to a waterfall should definitely be on your bucket list. Don’t know which one to visit first? Let us help you out.\nLocation: Kaieteur National Park, Guyana\nHeight: 741 ft\nIf you’re looking for a waterfall that will literally take your breath away, Kaieteur Falls is the place for you. Not only is it one of the most powerful waterfalls in existence, but it also holds the top spot for tallest single drop waterfall in the world! Located in the lush and isolated rainforest of the country of Guyana in South America, Kaieteur Falls a truly stunning sight. The natural wonder is 4 times taller than Niagara Falls and has an average flow rate of 23,400 cubic feet per second (meaning it packs quite the punch in terms of water speed and volume!)\nLocation: Southern Region, Iceland\nHeight: 197 ft\nThis well-known waterfall is almost too picturesque to believe! The best part about Seljalandsfoss has to be the path that takes you behind the waterfall- you’ll get soaking wet from the falls, but with a view like the one pictured above, the last thing you’ll be thinking about it is how drencehd you are. It’s located in south Iceland and is a must-see for anyone traveling in the country!\nLocation: Canaima National Park, Venezuela\nHeight: 3,211 ft\nThis waterfall needs to be at the top of the list for every waterfall-chasing person out there. Holding the record as the world’s highest uninterrupted waterfall, Angels Falls is a pure masterpiece of Mother Nature. The journey to get to it involves a few flights, trek through an isolated jungle, and a river boat ride, but on the other side of the somewhat long travel process lies a waterfall that is 3 times as tall as the Eiffel Tower and 15 times as tall as Niagara Falls!\nLocation: Niagara Falls, New York, USA\nHeight: 167 ft\nEven though we’ve used this waterfall’s height as a less sizable comparison to others on this list, it’s still very impressive and one of the most beautiful in the world! Not only is it one of the most widely known waterfalls in North America, but also one of the most easily accessible ones. No matter where you view Niagara Falls, from the US or Canada, is provides stunning panoramic views that will surely make your jaw drop. This marvelous waterfall deserves to make everyone’s bucket list!\nLocation: Border of Zimbabwe/Zambia\nHeight: 355 ft\nHands down one of the most awe-inspiring and unbelievable views in the universe. Victoria Falls is the largest singular waterfall in the world, with an impressive height of 355 feet, width of over a mile, and an average flow of 1 million liters per second! According to the tourism website, the noise of Victoria Falls can be heard from a distance of 25 miles and can be seen from over 30 miles away. Local tribes used to call the waterfall Mosi-o-Tunya, which means “the smoke that thunders”.\nLocation: Border of Argentina/Brazil\nHeight: 269 ft\nThe Iguazu River contains endless waterfalls that make up the largest waterfall system in the world! Accessible from both Brazil and Argentina, these incredible waterfalls stretch across over 3 miles and provide unbeatable views at every point. The main attraction however, is the Devil’s Throat, a horseshoe shaped bend that houses the largest water curtain in the whole system and includes 14 waterfalls. The Iguazu Falls are a marvel that deserved to be seen and celebrated in person.\nLocation: Yosemite National Park, California, USA\nHeight: 2,425 ft\nThis magnificent waterfall is one of the crowning jewels in Yosemite National Park. It’s comprised of three sections: Upper Yosemite Fall at 1,430 feet, the middle cascades at 675 feet, and Lower Yosemite Fall at 320 feet. There are endless ways to view the falls, from hiking trails to shuttle stops, to help ensure everybody can get a glimpse of its magnificence. For the explorers out there, you can hike to the top of the falls. It’ll take a lot of patience and physical exertion, but if you’re one to appreciate the beauty of nature, the view cannot be beat.\nHeight: 105 ft\nGullfoss is a uniquely beautiful and wild waterfall in southwest Iceland. It falls in two steep tiers and is one of the largest volume waterfalls in all of Europe. Gullfoss translates to “Golden Waterfall” and it got its name because, as this Iceland travel website explains, on sunny days the water takes on a golden color from the sediments from the glacial water its made from. If you’re ever in Iceland, make visiting Gullfoss a priority- it’s relatively close to Rekyjavik and is easy to locate since it’s a popular tourist attraction (for obvious reasons!)\nBan Gioc-Detian Falls\nLocation: Border of China/Vietnam\nHeight: 197 ft\nAsia’s hidden gem is currently the 4th largest waterfall along a national border, behind three other waterfalls on our list: Iguazu Falls, Victoria Falls, and Niagara Falls. The three-tiered beauty is a popular destination for travelers looking to be blown away by its massive web of waterfalls. You can ride a bamboo raft right up to the falls, take a dip in the pools, or relax under the canopies of the jungle. No matter what visitors do when they’re at the Ban Gioc-Detian Falls, they leave knowing they’ve witnessed a mesmerizing natural wonder!\nAre there waterfalls across the world that you think deserve to be on this list? Tell us in the comments below!","The peculiar canyon of Uruguay\nJust like the 170 km distant Iguazu Falls, Mocona Falls have formed in the basaltic Paraná Plateau, which formed in Cretaceous period.\nThe enormous, 1,838 km long Uruguay river has an interesting peculiarity – on the bottom of river channel is a deep and narrow canyon, which is submerged and not visible, when standing at the bank of river. In the upper part of the river, up to Mocona Falls, this trench takes approximately 15 – 30% of the width of the river and is very deep. There are places where it is even 100 m deep!\nSpecialists consider that this deep trench formed during the last Ice Age, when the climate was dry and river was narrower. Now this canyon is submerged with much more powerful stream and, thanks to the power of the stream, is not silted with sediments.\nOnly in some places this submerged canyon becomes visible in daylight – and one of them is Moconá Falls.\nThe big fall\nMocona Falls divide Uruguay River in Middle and Upper Uruguay and are located 1,215 km from the mouth of river. Here the river flows along one rim of the trench and falls into it.\nMoconá in Guarani means – \"to swallow everything\", but Yucumã – \"the big fall\". Somehow the first name is more used in Argentina, but the second one – in Brazil.\nRapids or falls?\nMocona Falls are seen only during low water and are not visible some 150 days per year. This is not typical for \"pure-breed\" waterfalls and is more characteristic for rapids. Water level in the river may change in any time of the year – sometimes the enormous river rises per 5 metres over 24 hours.\nThus a traveller might face a situation where during his journey to the the falls the level has rised and falls are not visible – visitors are recommended to ask for advice of locals before coming.\nEndless wall of water\nBut – if a traveller is lucky, he will experience an unforgettable sight.\nAlthough most of the river here is located in Argentina, the best vistas open from Brazil (again – a similarity to Iguazu Falls!).\nIf a visitor stands at the middle of falls, he sees a unique sight – seemingly endless wall of falling water which surrounds the observer for 180 degrees.\nThe width of falls is subject to water level. Some sources mention that the falls might be up to 3,000 m wide, many – that it is wider than 2 kilometres. More conservative sources give 1,800 m.\nBut even the most conservative figures mean that Moconá Falls are among the widest waterfalls of the world. The mighty Iguazu Falls are definitely wider – 2,682 m, but… that’s it, the only waterfall which is definitely wider. Technically there are even wider falls in the world, but these are interrupted with larger and smaller islands.\nMany sources mention that Mocona Falls are some 25 m high. This is not entirely true – this figure shows the drop of the river over the distance of some 3 kilometres.\nThe height of the vertical drop at low water can reach 10 – 12 metres. Thus – if we consider that Moconá Falls are rapids – over 3 km distance these rapids make 25 m difference in river level. But the height of true, vertical waterfall reaches 12 metres.\nAround the falls has been preserved the pristine Atlantic forest. In Brazilian side this is the last place in Rio Grande do Sul state, where still lives jaguar.\nA nature protected area here was first established in Brazil – Turvo State Park was established in 1947 and now has an area of 17,491 ha. In Argentina a reserve was created in 1967, in 1991 was established Moconá Provincial Park.\nUnfortunately this amazing natural monument is endangered by the planned Garabí hydroelectric dam.\n- Daniela Mariel Inés Kröhling, Martin Iriondo, A particular canyon excavated in the large Uruguay River channel (South America), International Geological Congress Oslo 2008. Accessed in 15.02.2012.\n|Coordinates:||27.1496 S 53.8877 W|\n|Rating:||(4 / 5)|\n|Address:||South America, Misiones in Argentina and io Grande do Sul in Brazil, on Uruguai River|\n|Alternate names:||Saltos de Yucumã (Portuguese)|\n|Height:||10 – 12 m|\n|Width:||1,800 – 2,700 m|\nDiscover Earth’s most beautiful and fascinating natural landmarks. From the spectacular granite domes of Yosemite to the reefs of the Bahama Banks and the ice sheets of the Antarctic, this is an unparalleled survey of the world’s natural treasures.\nt’s time to get off the beaten path. Inspiring equal parts wonder and wanderlust, Atlas Obscura celebrates over 700 of the strangest and most curious places in the world.\nTalk about a bucket list: here are natural wonders—the dazzling glowworm caves in New Zealand, or a baobob tree in South Africa that’s so large it has a pub inside where 15 people can drink comfortably."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:8696e428-42ba-4038-a521-d23961200766>","<urn:uuid:9f921608-1a9d-4af8-850c-c047b3243f29>"],"error":null}
{"question":"What are the benefits of focused single-task work, and how does chronic multitasking affect our brain's capacity for attention?","answer":"Focused single-task work allows full utilization of the brain's working memory capacity, enabling all available neurons to concentrate on one task. When attention is focused on a single task, cognitive performance improves and tasks are completed more efficiently. Conversely, chronic multitasking appears to create lasting negative effects on the brain's ability to focus, even during single-task situations. Research indicates that people who frequently multitask develop increased distractibility and diminished ability to organize information and filter out irrelevance. This conditioning is particularly concerning as it appears to create a permanent reduction in attention capacity, essentially 'dumbing down' cognitive abilities even when attempting to focus on one task.","context":["Presentation on theme: \"Top Study Tips with Richard Spacek Being a modification of “Bjork’s Big Seven,” developed by Dr. Robert Bjork, Professor of Psychology, University of California,\"— Presentation transcript:\nTop Study Tips with Richard Spacek Being a modification of “Bjork’s Big Seven,” developed by Dr. Robert Bjork, Professor of Psychology, University of California, Los Angeles, and supplemented by my own research..\n1 False. fMRI has repeatedly shown several brain areas at work during activities There is no unused “reserve,” only some degree of specialization\n2 False. Polyphasic sleep (more than two sessions per night) Usually the result of illness Often supplemented by daytime napping\n3 False. 1993 study suggested better “spatial task performance” Result of mood, not congnitive improvement Learning a musical instrument does contribute to self-regulation and possible cognitive ability\n4 False. Wakefield’s study was fraudulent and was withdrawn He was stuck from the British Medical Register\n5 False. Repairs are constantly carried out “neuroplasticity” can help recovery in some serious cases\n6. False. In certain areas, new neurons are formed constantly: hippocampus and olfactory bulb Cells constantly regenerated\n7 True. “spacing effect” is well documented current research focuses on finding the ideal spacings\n8 False—BUT... Damage to dendrites will occur as a result of chronic abuse Vit. B1 deficiency can cause Wernicke-Korsakoff syndrome\n9 False. Only physical trauma can create a hole in your brain Key brain regions in drug addicted people may be reduced in size Some drugs interfere with the way cells send and receive messages\n10 False. Crossword puzzles and similar games can help you learn words and improve specific skills, but will not enhance overall brain function\n11 True. regular exercise can help maintain memory and general cognition Aerobic exercise increases blood flow to the brain, lessening rate of tissue loss during aging John Ratey, Harvard: increases production of brain-derived neurotrophic factor (BDNF)\n12 False. Both hemispheres work together in almost every cognitive task that has been studied Something as broad (and ill- defined) as “creativity” is not centered in one hemisphere.\n13 False. Arzi et al. (2012) determined that sleeping subject could learn smells—but that’s about it!\nBjork’s Big Seven Bjork combined years of research into a system of key study activities I have built on this\nThe Big Seven 1.Concentrate 2.Interpret & Understand 3.Organize & Structure Information 4.Space & Repeat 5.Test, Retest/Generate, Retrieve 6.Organize Time 7.Recognize Physical Factors\n1. Concentrate Ever BLANK OUT while reading? Ever notice that a lecture has moved on to a completely different topic while you “tuned-out”? Ever driven miles while your mind wandered?\nConcentrate If you feel yourself slipping right now. Stop! Stand up; stretch Tap your head three times while muttering “think”, “think”, “think” Now refocus. Feel better?\nAttention, Please! Decades of attention research show us 1.Dividing your attention between multiple tasks is inefficient (no multi-taking) 2.Attention cannot be sustained indefinitely\nMulti-tasking Multi-tasking is inefficient... often an attempt to combine necessary with desired tasks turn off the stereo leave the residence tell yourself that if you study/practice for 20 min. effectively then you’ll spend 5 minutes... doing those other things you might otherwise be doing\nLecture Multitasking “significant negative correlation between in-class phone use and final grades... corresponding to a drop of 0.36 plus or minus 0.08 on a 4-point scale where 4.0 = A” “students cannot multitask nearly as effectively as they think they can”\nExceptions “separate perceptual domains” Allows one to read while walking on a treadmill without interference Writing while singing NOT so successful\nLimits of Concentration 2. A person can concentrate for a limited amount of time. The duration of attention differs from person to person, from task to task. You will know when you’ve reached your limit: your mind will start to wander\nLimits of Concentration For lectures, give yourself a pep talk beforehand: “Even the most boring lecture will end eventually.” When you find yourself beginning to lose attention, try to think of a question to ask the instructor and write it down Write one down now!\nLimits of Concentration Study: more difficult (or more boring) subjects may require more breaks Your ability to sustain focus tends to increase with practice Fatigue, illness will decrease endurance\nLimits of Concentration Example: You have 50 minutes to study You will actually learn more by studying for 45 of those 50 minutes and then taking a short physical break for 5 min.\nLimits of Concentration If you don’t take a break, your brain will go on one without you anyway... Possibly during the most important part of study For every 50 minutes, study, practice, or self-test for 25, take a break for 5, and then start again for 20.\nLimits of Concentration DON’T FORGET TO START AGAIN! For the next 50 min. time period, you might notice diminishing returns from that 5 min. break. You might find that you need a 10 min. break Eventually your stamina will increase\n2. Interpret & Understand Read the following: 1.The exposure was insufficient because of the weather conditions. 2.The crash was due to the keys sticking. 3.The numbers slid down because of the crisis abroad.\nInterpretation Interpretation can be thought of deep processing Now try to recall the three sentences that you just read on the previous slide. Can you do it? Fill in the blanks....\nInterpretation Remembering the sentences was probably difficult They seemed meaningless That which cannot be interpreted, cannot easily be recalled\nInterpretation Read the sentences again: 1.The exposure was insufficient because of the weather conditions. (Taking a picture) 2.The crash was due to the keys sticking. (Computer break-down) 3.The numbers slid down because of the crisis abroad. (Stock-market)\nInterpretation Context provides comprehension Now try to recall the three sentences again.\nInterpretation Other factors: you saw them before (repetition). You tried to recall them once already (retrieval practice) Repetition and retrieval practice are both crucial for learning & memory Research suggests that the sentences with “clues” are easier to remember even at first sight, because you were able to interpret the sentences.\n3. Organize/Structure List the months of the year— write them down. How long did that take you? Did you get them all?\nOrganization/Structure Pretty trivial, pretty easy? Now list the months of the year alphabetically. How long did that take? Are you sure that you got them all?\nOrganization/Structure A change in organization is a change in information\nOrganization/Structure Preview the chapters of textbooks Look over section headings how chapters are organized How many of you read the chapter summaries at the beginning (or end) of the chapters before you begin reading?\n4. Space/Repeat You have 4 hours to study for tests in Class A & Class B. What do you do? Do you spend two hours on Course A and then two hours on Course B? Study Course A for an hour, then Course B for an hour, then Course A for an hour, then Course B.\n4. Space/Repeat Spacing your study in this way is an easy way to increase variability of encoding Spacing your study increases retention\n4. Space/Repeat Each time you study something, you will encode the information slightly differently. Especially when time intervenes between the two study sessions. “encoding variability”\nGoing Postal: Optimal Spacing BPO: teaching typing to postal workers Option 1: take workers off their jobs and give intensive typing training Option 2: combine training with job and brief practice each day\nTyping Sessions 1.Two 2-hour sessions per day (total of 4 hours) 2.One 2-hour session per day (total of 2 hours) 3.Two 1-hour sessions per day (total of 2 hours) 4.One 1-hour session per day (1 hour)\nResults: Acceptable Standard Attained 1.80 hours 2.78 hours 3.78 hours 4.55 hours Also, group 4 retained their competence better than group 1\n“SPACING EFFECT” Remember the curve of decay: Knowledge initially declines rapidly, dropping by 80-90% within a week Review material before forgetting Brief reviews produce benefits Spaced recall promotes long term memory: separate instances of effortful recall!\nMicro-distribution practice Spaced presentation enhances memory Landauer/Bjork method: Test new item after short delay As item becomes better learned, gradually extend the practice interval Sample learning sequence:\nMicro-Distribution Teacher Stable = l’écurie Stable? Horse = le cheval Horse? Stable? Horse? Grass = l’herbe Grass? Stable? Horse? Grass? Church = l’église Church? Learner l’écurie Le cheval L’ écurie Le cheval L’herbe L’ écurie Le cheval L’herbe l’église\n5. Test, Retest When you flip through your textbook taking note of the organization before you begin to read the chapter, try to generate questions....\nBjork’s Advice Step number one: Take out your highlighting pen. Step number two: Make sure your highlighter has plenty of ink. Step number three: Throw away your highlighter!!!\nRead, Write; Don’t Highlight! Testing is BETTER Read a paragraph or two and test yourself: try to summarize Check the textbook to make sure that you have correctly summarized the information Correct as needed\nGenerate, Retrieve! All the time, try to make sense out of what you are learning (interpretation) Using this very powerful technique may double if not triple your reading time—but it will produce results.\nGenerate, Retrieve! Retrieval practice. How will you know you are ready to take the test and that you will do well on the exam? PRACTICE RETRIEVING THE INFORMATION BEFORE THE TEST!!!\nGenerate, Retrieve! Retrieval practice provides very effective feedback. Immediate knowledge of performance Even better, retrieval practice makes the information more likely to be remembered the next time you try to retrieve it!\nTest Stable Horse Grass Church Write the French for.... Stable = l’écurie Horse = le cheval Grass = l’herbe Church = l’église\n6. Organize Time “time management and self- testing were generally stronger predictors of... academic performance than aptitude”\nTime Thief: Video Games “81% of American youth report playing at least once per month C. 9% of 8-18 year olds are pathological users “consistent negative associations between liking to play violent video games and school performance”\nOrganize Time Schedules reduce stress, ensure performance They must be designed realistically and they must suit you schedules build habits and habits can work for you!\nFight Procrastination Beat procrastination with a limited commitment Promise to spend just 25 minutes on a large task Once you are working, delay quitting—perhaps 10 minutes Extend as necessary\nTM Workshop For more on Time Management, attend our next TM workshop Or download our slideshows: www.unbwritingcentre.ca/Workshops\n7. Study Environment a)Light b)Day vs. Night c)Height d)Distractions, internal and external\na. Daylight Lighting (100 watts times 2 or actual daylight Improves concentration, makes study more efficient Improves mood\nFour Oaks Elementary Wholly “daylit” school Scores 7% ABOVE average Destroyed by fire Students moved to trailers Scores 10% BELOW average New daylit school: 9% above average\nb. Day vs. Night Most are more efficient in daytime Try to reserve low priority tasks for the night The later the hour, the lower the challenge should be “Sleep on it” tasks\nc. Height Big debate: standing vs. sitting Standing increases alertness Probably less injurious to health Ideal: have at least one work space at which you can stand part of the time\nd. Distractions Use the “spider technique” to overcome your response to a distractor Make a “symbolic” response Transition to none at all\nThe End What about those questions you wrote down?\nWant to know more? go.unb.ca/wss www.unbwritingcentre.ca/Workshops","by Michael W. Taft\nYou’re working on a paper, and the kids need dinner. Music is playing in one room, and the tv is on in the other. You don’t want to miss tonight’s episode of Futurama, so you peek at that while you type in your notes, getting up every once in a while to stir the pasta. Maybe you’re checking your email, keeping an eye on your Facebook and Twitter feeds, and playing some Candy Crush in there, too. No problem. Hey, you’re a modern person, able to scoff at the narrow insistence on single-minded attention that kept our ancestors hidebound. And you think you’re pretty good at keeping all these balls in the air at once.\nThe fact is, however, that you’re not. You actually suck at multitasking, and you just don’t know it. Stanford Professors Clifford Nass and Anthony Wagner ran a study in 2009 which intended to show just how people who are good at multitasking (high multitaskers) were different from people who focus on one thing at a time (low multitasking). The core of successful multitasking, according to Nass, is to be able to focus on what is important and ignore what is irrelevant. The results of were a surprise:\nWe were absolutely shocked. We all lost our bets. It turns out multitaskers are terrible at every aspect of multitasking. They’re terrible at ignoring irrelevant information; they’re terrible at keeping information in their head nicely and neatly organized; and they’re terrible at switching from one task to another.\n…We’re troubled, because if you think about it, if on the one hand multitasking is growing not only across time, but in younger and younger kids we’re observing high levels of multitasking, if that is causing them to be worse at these fundamental abilities — I mean, think about it: Ignoring irrelevancy — that seems pretty darn important. Keeping your memory in your head nicely and neatly organized — that’s got to be good. And being able to go from one thing to another? Boy, if you’re bad at all of those, life looks pretty difficult.\nAnd in fact, we’re starting to see some higher-level effects [of multitasking]. For example, recent work we’ve done suggests we’re worse at analytic reasoning, which of course is extremely valuable for school, for life, etc. So we’re very troubled about, on the one hand, the growth, and on the other hand, the essential incompetence or failure. …\nOne would think that if people were bad at multitasking, they would stop. However, when we talk with the multitaskers, they seem to think they’re great at it and seem totally unfazed and totally able to do more and more and more. We worry about it, because as people become more and more multitaskers, as more and more people — not just young kids, which we’re seeing a great deal of, but even in the workplace, people being forced to multitask, we worry that it may be creating people who are unable to think well and clearly.\nSo not only were the high multitaskers bad at multitasking, they were also convinced that they were good at it. And their ability to perform basic cognitive functions, such as paying attention, keeping memory organized, and analytical reasoning, was bad.\nImagine twenty years from now, you are on trial for a murder you didn’t commit. (Or maybe you were texting while driving.) As you sit before a jury of your peers, you can’t help but notice that most of them are streaming Netflix movies on their cellphones. The judge misses one of your lawyer’s objections because she’s texting her lover, but your lawyer doesn’t care because he’s the one she’s texting. Your only hope is that the police at the door are watching porn videos on their wristwatches and won’t notice when you sneak out.\nThe multitasking future is a joke. The ability to concentrate attention on one task at a time is essential to high-level human cognition. Concentrated attention is also one of several core skills that meditation trains (the others are equanimity and sensory clarity). Each time you bring your wandering attention back to the task at hand, you are building your attention power. And each time you let yourself get distracted by a flood of irrelevancies you are building what I call your distractability.\nThere is only so much RAM in your brain, a capacity that is called working memory. When all of your attention is focused on a single task, then most or all of working memory is absorbed in that task. You can bring all your available neurons to the table, so to speak. When you are distracted, however, there is less attention available for the job at hand. You are bringing far fewer neurons to the table. The net effect is that you have dumbed yourself down: actually lost IQ points in the sense of intelligence available for work. As Nass says in an interview:\nNow, if they’re born multitaskers, we can say to them, “You know, you shouldn’t multitask because you’re going to be bad at it.” But if they’re made multitaskers, and we’re in a world where multitasking is being pushed on more and more people, we could be essentially undermining the thinking ability of our society. …\nAnd frankly, we’re seeing this across the world, from the least developed countries to the most developed countries. Multitasking is one of the most dominant trends in the use of media, so we could be essentially dumbing down the world.\nThat’s a terrifying thought.\nIt’s very scary. And it’s one of the reasons we’re so excited about this research and why so many other people are getting excited.\nPeople never bothered to look at what we call chronic multitaskers. What they would do is they’d make people do five things at once and say: “Ha ha! They’re not as good as if they do one thing at a time.” Not a big shock, I think. What we decided to do is ask the question, what’s happening if you’re doing this all the time, even when you’re not multitasking? So if we take a multitasker and say, “Now just focus on this,” can they? As a professor and as a teacher, we think a lot about how do you teach kids who can’t pay attention or are distracted by irrelevancy or don’t keep their memory neatly organized? It’s a scary, scary thought.\nAnd, in fact, you already hear professors and others talking about changes in the way kids write, so that instead of writing an essay, they write in paragraphs, because what happens is, they write a paragraph, and they say, “Oh, now I’ll look at Facebook for a while.” Or they write a paragraph and say, “Oh, chance to play poker,” or whatever other activity they want, or to do all of these at once.\nSo what we’re seeing is less of a notion of a big idea carried through and much more little bursts and snippets. And we see that across media, across film, across, in Web sites, this idea of just do a little bit and then you can run away.\nWe were at MIT, and we were interviewing students and professors. And the professors, by and large, were complaining that their students were losing focus because they were on their laptops during class, and the kids just all insisted that they were really able to manage all that media and still pay attention to what was important in class — pick and choose, as they put it. Does that sound familiar to you?\nIt’s extremely familiar. … And the truth is, virtually all multitaskers think they are brilliant at multitasking. And one of the big new items here, and one of the big discoveries is, you know what? You’re really lousy at it. And even though I’m at the university and tell my students this, they say: “Oh, yeah, yeah. But not me! I can handle it. I can manage all these,” which is, of course, a normal human impulse. So it’s actually very scary. …\nSo who are these kids that you picked [for your study] to come in here today?\nWe picked the kids at Stanford who are multitasking a whole lot. So on a college campus, most kids are doing two things at once, maybe three things at once. These are kids who are doing five, six or more things at once, all the time.\nSo they’re the kids who are texting while talking with people, while working on their papers, while chatting on multiple sessions. They’re the kids who are playing multiple games on their screen while they’re doing Facebook, while they’re talking, while they’re doing all these other things. So these are the extreme kids, the kids who are at the very, very high end of that. …\nAnd do these kids think they’re pretty good at it?\nYeah. They all seem to think they’re really good at it. In fact, what’s ironic is when we talk with people who multitask all the time, those who don’t — even though our research suggests the ones who don’t would actually be better at it — they’re the ones who are sure they’re really bad at it. And the ones who do it all the time and are sure they are great at it are really bad at it. So it’s a real question: What’s going on?\nI know with myself that I’ve started multitasking much, much more. And it’s not that I necessarily think that I’m good at it, but … my sense is that I can function in a world in which I have to multitask. But I recently had myself analyzed by an interruption scientist. … She watched me for a whole day, and she said that at the end of the day, I hadn’t spent more than three minutes on a single task, and that really chilled me.\nIt should be chilling. Our brains aren’t really built for that. We evolved in a world in which there [were] very few things to look at at one time, or, more precisely, very few things that weren’t related. So if you were out hunting an animal, yeah, you might look at a lot of things, but they were all about hunting that animal. Now what we see is people trying to use information in a totally unrelated way. And that’s not how your brain, or anyone’s brain, is built.\nSo what gets lost?\nSome things that we know get lost are, first of all, anytime you switch from one task to another, there’s something called the “task switch cost,” which basically, imagine, is I’ve got to turn off this part of the brain and turn on this part of the brain. And it’s not free; it takes time. So one thing that you lose is time.\nA second thing you lose is when you’re looking at unrelated things, our brains are built to relate things, so we have to work very, very hard when we go from one thing to another, going: “No, not the same! Not the same! Stop it! Stop it!” It’s why people who aren’t multitaskers, like me, often experience when we’re typing and someone walks up and starts talking with you — you’ve probably had this — you start typing their words and go, “Ah, what happened?” And that’s because your brain loves to mix. So we’re spending a lot of time trying to beat down this combining brain we have. …\nAt the end of the day, it seems like it’s affecting things like ability to remember long term, ability to handle analytic reasoning, ability to switch properly, etc., if this stuff is, again, … trained rather than inborn. If it’s inborn, what we’re losing is the ability to do a lot of things that we’re doing. We’re doing things much, much poorer and less efficiently in time. So it’s actually costing us time.\nOne of the biggest delusions we hear from students is, “I do five things at once because I don’t have time to do them one at a time.” And that turns out to be false. That is to say, they would actually be quicker if they did one thing, then the next thing, then the next. It may not be as fun, but they’d be more efficient.\nYou’re confident of that?\nYes. There’s lots and lots of evidence. And that’s just not our work. The demonstration that when you ask people to do two things at once they’re less efficient has been demonstrated over and over and over. No one talks about it — I don’t know why — but in fact there’s no contradictory evidence to this for about the last 15, 20 years. Everything [as] simple as the little feed at the bottom of a news show, the little text, studies have shown that that distracts people. They remember both less. Studies on asking people to read something and at the same time listen to something show those effects. So there’s really, in some sense, no surprise there. There’s denial, but there’s no surprise.\nThe surprise here is that what happens when you chronically multitask, you’re multitasking all the time, and then you don’t multitask, what we’re finding is people are not turning off the multitasking switch in their [brain] — we think there’s a switch in the brain; we don’t know for sure — that says: “Stop using the things I do with multitasking. Focus. Be organized. Don’t switch. Don’t waste energy switching.” And that doesn’t seem to be turned off in people who multitask all the time.\nSo are you suggesting that by multitasking all the time, we are actually changing our brains and making our brains worse at focusing on one thing?\nThere’s a good chance. We don’t know for sure, because it also could be that people are born to multitask. That is, they’re born with the desire to do all these things, and that’s making them worse. But there is reason to worry at least, and believe that.\nOne of the other worries is, we’re seeing multitasking younger and younger and younger. So in a lovely study, someone showed that when infants were breastfeeding and the television was on, infants were doing a lot of television watching. Now, if we think about it, the way that we think that breastfeeding evolved the way it did is the distance from the mother’s face to the infant is the perfect focal distance. The voice is one that’s very attractive.\nWell, if you think about it, what is television filled with? Faces and voices. What do babies love? Faces and voices. So now, at a time when we believe that children learn intense concentration, they’re being drawn away. Then as they get older, as they get to 3 or 4, we started feeling guilty that we put kids in front of the TV as a baby-sitter. So what did we do? We didn’t turn off the TV. We started giving them toys, books, etc., while they’re watching TV. So what are we telling them? We’re telling them, “Don’t pay attention; do many things at once.” Well, it may not then be surprising that years later, that’s how they view the media world.\nMultitasking actually trains you to be less attentive, which means you will be effectively less intelligent. Each decision you make will have the benefit of less and less brainpower to inform it.\nBy reducing distractions in your environment, on the other hand, you will slowly train yourself to concentrate your attention—deeply and fully—on one thing at a time. Focused attention is a learnable skill, and one that can grow much stronger than most people realize. With strong concentration power, effective intelligence grows, because the raw number of neurons you can use on a project is larger. And there is no better way to build concentration than by the practice of meditation.\nSo in daily life, try as much as possible to limit distractions. Turn off every input that you can turn off. Allow your attention to rest on one task at a time. Not only will you do a much better job, you will also feel less anxious.\nRemember: Multitasking is multi-failing.\ncopyright © 2011 by Michael W. Taft"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:1359f439-354d-4ab6-ab9f-8ce69690120f>","<urn:uuid:f12793f1-3798-4d78-920f-f2b7f570b3bc>"],"error":null}
{"question":"How do mining facilities' water treatment approaches differ between active operations and wastewater management, particularly in terms of environmental protection measures?","answer":"Active mining operations and wastewater management employ different but complementary approaches to water treatment and environmental protection. In active operations, measures include building settling ponds at mining areas, treating water run-off, maintaining pH stability between 6-9, and monitoring water quality at rivers, ponds, wells, and mine waste daily. For wastewater management, the focus is on treating effluents containing heavy metals, hydrocarbons, grease, and solids through specialized processes like mining oil separators and pump stations before discharge into the environment. Both approaches aim to prevent contamination of water streams and ensure regulatory compliance.","context":["Corporate Social Responsibility\nCommunity Development Programme\nWe have made substantial efforts to integrate with the local population in the vicinity where our mines are located, as we believe that these efforts are integral to the stability and development of our business.\nAs part of its community development programme initiative, we engage in activities aimed at fostering community relationship and empowering such communities, such as implementing various programmes to provide economic, technical, social, infrastructure, educational and healthcare assistance to the local communities.\nWe have provided the local communities with new employment opportunities by recruiting/employing and training local residents to work in fields that demand high skills, such as heavy equipment operators and drivers.\nWe also provide capital, technical, training and marketing support to assist local residents to set up small businesses, such as livestock farms, or seek employment in local industries.\nReclaimed land is also transferred back to the local communities for community development purposes.\nWe have also contributed to the social and cultural welfare of the local communities by establishing Islamic study groups in mosques and contributing the necessary facilities and materials, and collecting donations to organise religious celebrations and benefit orphans.\nWe perform public works development and maintenance, such as developing and constructing new wells, improving school facilities and infrastructure and providing school buses and repairing roads, bridges and public facilities (some of which were major roads and bridges connecting villages or functioning as the main route of activity).\nTo promote cultural values, we sponsor and participate in traditional events and social functions, such as Indonesia’s Independence Day celebrations.\nWe recognise the importance of education, and in 2011, we provided scholarships to 230 students from schools located in the province of South Kalimantan. In addition, we also distributed spectacles to students and teachers.\nWe had also arranged for free mass cleft lip surgery for children and adults from Jakarta and West Java in cooperation with Obor Berkat Indonesia, as well as providing various medicines for the West Sumatra Police.\nWe are committed to continuing to develop community relations by expanding its diverse CSR initiatives.\nEnvironmental Conservation Programme\nWe are mindful that certain aspects of its mining operations may have an environmental impact on our surroundings. Therefore, with the view to conserve and preserve the surrounding development, we have made sustainable development an integral part of our CSR programme and have strived to implement environmental protection policies.\nOur ECP initiatives include :\n- Designating an environment superintendent and other special officers to conduct daily monitoring of the environment, mine reclamation and rehabilitation and vegetation;\n- Reclaiming used land and thereafter developing cash crops, fish farms or recreational facilities;\n- Building a settling pond at the mining areas;\n- Treating water run-off from mining areas in settlement ponds;\n- Maintaining the pH stability of the water in the settling pond (pH of water in the settling pond ranges between 6 to 9);\n- Closely monitoring the water released from the settling ponds;\n- Providing workshops containing drainage systems for holding waste oils before disposal;\n- Cooperating with the sub-contractors to perform the management and storage for hazardous and toxic materials and waste;\n- Conducting daily and weekly inspections and monthly evaluations to minimise the production of mine waste and the size of disturbed areas;\n- Closely monitoring the water quality at rivers, ponds, wells and mine waste in the mining areas daily with monthly evaluations undertaken by the laboratory;\n- Performing air/emission and ambient and noise tests by the laboratory per semester;\n- Monitoring noise and dust levels and air quality at the mining areas and at intervals along haul roads;\n- Controlling coal dust by placing fixed water sprays at intervals around the coal stockpiles;\n- Monitoring the level of erosions at slopes and embankments in particular; and\n- Monitoring the acidity of overburden.","ammel process for the treatment of ammonia in low,• mining effluent or process streams containing ammonia derived from the use of ammonia based blasting powder and/or the oxidation of cyanide • tertiary treatment for municipal waste water treatment plants (mwtp) and lagoon systems • process streams related to steel, fertilizer and chemical industries.wastewater treatment for the mining industry | fluence,conventional biological activated sludge treatment requires air compression, the process that by far uses the most electricity during treatment. fluence’s innovative membrane aerated biofilm reactor (mabr) , however, delivers a 90% aeration energy savings with passive aeration at atmospheric pressure, making domestic wastewater treatment at mining camps more sustainable..treatment methods for mining and ore processing wastewaters,as a rule, wastewater treatment will involve the selective removal of one or more substances rather than bulk separation of all of the dissolved matter from the wastewater. the latter is usually an expensive operation at high dissolved solids concentrations although product or water recovery may provide sufficient incentive in some instances..wastewater treatment process in mining flow diagram,treatment process in a wastewater treatment plant (wwtp). as the treatment the influent flow rate, the influent pollutants including the total suspended solids (tss) and cbod, are different data-mining algorithms are used to obtain accurate models... a flow diagram of a typical wwtp process is shown in figure 1.1. read more.\nNOTE: You can also send a message to us by this email [email protected], we will reply to you within 24 hours. Now tell us your needs, there will be more favorable prices!\nthe regulation around the world for heavy metals has become more rigorous in recent years and today the removal process is a required feature in all mining sectors. metals in the wastewater are usually found in their salt forms which lead to the fact that most of the metals are totally soluble in the liquid phase and are difficult to remove by simple filtration or other simple physical processes.,treatment systems for mining sites | cleanawater,mining wastewater produced by geographically isolated mines is generally discharged to pondage. mining wastewater is rich in heavy metals, hydrocarbons, grease and solids, and these foreign particles must be processed in mining oil separators and via pump stations accordingly, prior to their discharge into the environment.\neffective with low and high concentrations of metals – can be used in mining wastewater and industrial wastewater (high concentrations, often 1000 times more than in mining water) treatment applications; precipitation process is exothermic – the process needs less energy and generates heat which can be recovered for use in other processes in cold climates, such as e.g. in the nordic countries and canada,how to manage and treat water during mining - hydrochem,processing ore requires a huge amount of water to clean off any dirt or debris. if sites used pure water, ultimately it would lead to an increase in the amount of wastewater. using recycling dirty water for ore processing, the mine can minimise their treatment requirements. all\nmining wastewater treatment mining affects fresh water through its heavy use when processing ore. to prevent contaminated water from entering the water streams, water must be treated to remove potentially dangerous impurities. brenntag offers solutions for mining wastewater treatment, the removal of contaminants from water used during mining.,wastewater treatment & reuse - mining | xylem south africa,managing the mining wastewater in a tailings pond requires more than just pumps. xylem customizes processes that provide nonpotable reuse water for further treatment using xylem-engineered uv treatment, ozone wastewater treatment, or biological treatment systems. conserve resources with ozone wastewater treatment.\nthe plant is designed to treat up to 120 m³/h of process solution containing up to 1,500 parts per million of dissolved copper. the plant allows the site owner to maintain compliance with regulatory limits for residual cyanide levels in tailings ponds when processing ore with high cyanide soluble copper content, while recovering up to 99% of the copper from the wastewater stream.,mining industry wastewater treatment systems » ecologix,mining industry treatment process wastewater from extractive operations ( bauxite, coal, copper, gold, silver, diamonds, iron, precious metals, lead, limestone, magnesite, nickel, phosphate, oil shale, rock salt, tin, uranium, molybdenum) has to be properly managed in order to prevent any water or soil pollution arising from acid or alkaline leaching of heavy metals.\nthis chapter focuses on sustainable mining wastewater treatment technologies with emphasis on gold mining wastewater. it discusses the technical and environmental challenges associated with mining effluent treatment, process conditions for optimum plant performance, efficiency, limitations and the economics of treating mining wastewater.,research of acid mine wastewater treatment technology,mining process is the first word processes of mineral resources industry, including mining process in open pit mining technique and mine mining process. mining waste water treatment process, according to management process, is divided into two kinds: mining industrial wastewater and acidic mining waste water. the mining industry wastewater mainly comes from component cooling water, such as the\nmining at isc, we have a broad range of experience creating customized water solutions for the mining industry including: wastewater treatment; boiler and cooling water treatment; process water treatment; pre-treatment; effluent water treatment; proprietary chemistries; no matter what your mining industry water treatment challenge is, isc can help.,mining wastewater treatment | acid water from mining,mining wastewater treatment avoids environmental damage from minery acid water. preventative and active measures are to be taken with best technologies. in regards to treatment technologies, it is important to note that on numerous occasions a single procedure does not prove sufficient, resulting in the necessity for a combination of\nwastewater treatment and zero liquid discharge in mining. a mine generates large amounts of highly concentrated wastewater due to contact between water and various types of minerals. the origin of these effluents can be found in the distinct processes undertaken in mining, in addition to drainage from rainfall.,mining and mineral processing wastewater treatment,these products can be used in a variety of applications such as geotextile dewatering, eliminating sludge odors in wastewater treatment. it can be sprayed directly into stacks and rooms, atomized into the atmosphere as a barrier, and drip fed into sewage streams, used at any malodorous site, added to cleaning water, or added to inks or adhesives.\nmeeting process water requirements – treatment of water supply to your mining facilities to ensure it is suitable for use in production process systems. improving the efficiency of your operations – each solution we create is optimised to ensure long-term efficiency in all areas of mine water treatment, enabling you to control your operating costs over the lifetime of each facility.,mine water treatment | veolia water technologies,operating mines, as well as abandoned mine sites, generates acid mine drainage which requires water treatment to prevent adverse impacts to rivers and streams. veolia has developed a proprietary process known as densesludge™ to reduce sludge management costs and improve the operating efficien cy of treatment plants.\nthe wastewater treatment process according to the present embodiment is a process in which a first neutralizing agent is added into sulfuric acid-acidic wastewater containing aluminum, magnesium, and manganese to separate an aluminum hydroxide precipitate, then a second neutralizing agent and a oxidizing agent are added to separate a manganese precipitate, and then a third,wastewater minimization and reuse in mining industry in,process wastewater the process wastewater is generated by the wash down facilities in the workshop, wash down bay, coal conveyor cleaning, the dust suppression facility, oil storage and diesel filling area, and on-site mineral processing facility. the likely contaminants are oil, coal, and other solid materials.\nthrough a proprietary advanced oxidation process, ecosphere’s patented, chemical-free ozonix® water treatment technology can help mining companies protect both their assets and the environment through cost-effective and environmentally responsible disinfection, oxidation and,wastewater treatment in mining |,wastewater treatment in mining. mining can demand the use of large volumes of water; a commodity that is valuable and in some cases, rare. to maximise the process efficiencies and to minimise water consumption, the optimisation of water use and re-use is important. in the mining industry, water is used to extract minerals (e.g. coal and iron),\nbiological wastewater treatment for mining: lessons from the power industry. many coal-fired and nuclear power plants in the u.s. are supported by steam electric power. this generation involves processes that require the discharge of water with constituents such as arsenic, lead, mercury, selenium, chromium and cadmium.,mining | wastewater treatment for mining | cleanawater,from designing equipment specific to your mining industry, to manufacturing and installing it, we provide a start to finish service. most of our off-the-shelf wastewater treatment units come fully wired and with complete plumbing. you can easily retrofit them in your mining application, and minimise the amount of downtime ordinarily needed.\na volume of 2.0 l of mining wastewater intensified with 100 mg cn − dm −3 (at initial ph of 12) were introduced into a glass tank and then, recirculated through the electrochemical cell using a peristaltic pump. samples of 75 ml were collected each time and after settling time of 30 min, the supernatant liquid was filtered through a 0.20 µm filter for the determination of cyanide and other,mining & metals | processed water solutions/wastewater,with cutting-edge water treatment and process solutions, samco is ready to help protect your equipment and increase mineral recovery while reducing and economically treating your waste. we have extensive experience serving companies like yours to help facilitate better-operating systems and higher productivity by efficiently purifying feedwater, separating out contaminants, and optimizing machinery.\nfluence’s innovative aerobic wastewater treatment technology, the membrane aerated biofilm reactor (mabr), cuts power requirements by 90% from conventional activated sludge aeration systems, which can provide environmentally responsible decentralized treatment for domestic wastewater at mining camps. mabr’s passive aeration design allows,wastewater treatment for the mining industry,treatment can be challenging the mining industry uses large amounts of water in its processes, creating a two-fold challenge for mine owners and operators who wish to: • maximize process water recycling to reduce source water demand • achieve high quality treatment to meet stringent environmental discharge requirements. mine wastewaters often contain high total dissolved solids\nmodeling and optimization of wastewater treatment processes faces three major challenges. the first one is related to the data. as wastewater treatment includes physical, in the temporal nature of the process. different data-mining algorithms are used to obtain accurate models. the last challenge is the optimization of the process models,mining sewage | aquasol water treatment solutions,installed to treat wastewater from housing developments, resorts, lifestyle villages, caravan parks, nursing homes, mine site camps and most other developments that require on-site sewage treatment. the process incorporates anaerobic and aerobic treatment along with return activated sludge to facilitate biological wastewater treatment to effluent"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:23ba524b-5d48-4ca0-a790-1fa546cbf894>","<urn:uuid:9e80b3c6-0bec-41a8-b945-1ceab810d932>"],"error":null}
{"question":"What are the innovative artistic techniques being used in contemporary art, and how do museums address the challenges of preserving these complex works?","answer":"Contemporary artists are employing diverse innovative techniques, from using 3D scanners to create fragmentary body representations, to developing generative simulations for landscape scenes, and creating hybrid artworks that combine sculpture, robotics, image, and sound. Some artists work with environmental sensors and LED installations, while others experiment with virtual reality and complex digital projections. Regarding preservation, museums have developed specific strategies to maintain these works. Conservation departments carefully document the creation process, assess media elements, manage display equipment, and maintain extensive communication with artists. They must handle various technological components, from external hard drives and tapes to vintage equipment like 35mm slide projectors, ensuring both the media content and display technology remain functional for future exhibitions.","context":["The major art schools participate in initiating trends. The Art and Technology Studies department of the School of the Art Institute of Chicago is a leading example. By analyzing significant works by several artists who have carried out studies there, we can begin to understand the range and impact of the practices under consideration and to affirm the excellence of its teaching.\nCarlos Fadon Vicente, Vector 10b, 1989.\nSince the mid-1970s, Carlos Fadon Vicente has constantly questioned the medium in which he works, ranging from still to moving images, captured or broadcast. Fadon won a Leonardo Pioneer Award in 2018, largely in recognition of the experiments that he started in the department of Art and Technology Studies in 1989, when he seized a color printer at SAIC to produce his series Vectors. Working in total symbiosis with a PaintJet (HP's first color printer) Fadon even accepted printing errors. Further, he incorporated them at a time when computing, in becoming generalized, already symbolized the perfection of images. In a form of letting go, of which serendipity only has the secret, he in a way became the first viewer of a work being produced before his eyes. This makes the Brazilian artist one of the pioneers of what many years later would come to be known as glitch art, a trend that became particularly appreciated by younger generations for whom the extreme perfection of images, controlled in all regards, ends up being boring.\nJason Salavon, The Top Grossing Film of All Time, 1 x 1, 2000.\nThe works of Jason Salavon, which are included in the collections of the Metropolitan Museum of Art, the Whitney Museum of American Art, and the Art Institute of Chicago are familiar to us. Yet, without their titles they would be, for the most part, pure abstractions. When the artist squeezes every frame of James Cameron’s movie Titanic into a single image, we use our memories of this feature film to distinguish between the light tones of the early sky and the darker ones of the climactic shipwreck. The Top Grossing Film of All Time, 1 x 1 (2000), offers us the visualization of all the hues of this same film through a large number of pixels that, strangely, fit so perfectly, even for those who have not seen the film. With Every Playboy Centerfold, The Decades (normalized) from 2002, it is the body of women and the male gaze that is in question. Salavon merges, decade by decade, the centerfold of a magazine that everyone knows, at least by reputation. Each image is a fusion of 120 bodies—resolutely pictorial representations that evoke imprints, and more precisely that of the Shroud of Turin. In this case, the art lover appreciates the quality of the blurs that translate the depth of the images while the sociologist deciphers the evolution, over time, of the poses or morphologies emerging from the amalgam of codified representations. Through the synthesis and use of data, Jason Salavon offers us distanced interpretations of whole fragments of our popular cultures.\nSusan Collins, Excavation, 2012.\nThe work of Susan Collins is about duration. The duration of processes that allow her, for example, to capture the images from the Seascape (2009) series pixel by pixel, line by line. Her work references the tradition of plein-air painting and the origins of photography. Her seascapes merge temporalities, focusing on luminosities while the scanning inherent in the acquisition process reinforces the extreme horizontality of the points of view. Collins has also dealt with architecture, ranging from the intimate to the monumental. In one example, Excavation (2012), Collins projected a film of an archaeologist digging with a trowel on the floor of the historic All Saints Church in Harewood (United Kingdom). The scene cannot be reliably located in the past or future and, covering just a few slabs, Collins addresses only a handful of privileged spectators. Conversely, Brighter Later (2013), performed at the Radcliffe Observatory in Oxford, addresses the greatest possible number. From sunrise to sunset, her luminous installation informs the surrounding populations about climatic variations including temperature, pressure and rainfall. At a time when each individual responds to the slightest alert from their connected objects, Collins has chosen to create an interface that provides collective observation.\nByeong Sam Jeon, The men with five tongues, 2016.\nEven the most mundane objects, creatively assembled and in surprising quantities, are likely to intrigue us. Artist Byeong Sam Jeon reveals the magic that can be found in the objects that fill our daily lives by skillfully multiplying them, as in a 2015 installation in which he covered the facade of an abandoned tobacco factory with nearly 500,000 CDs. The choice of such a support is interesting because it fascinated its users in the 1990s by the amount of data contained. Then, one innovation replacing another, it ended up disappearing from the shelves of our supermarkets— places of accumulation par excellence. But what is interesting about the CD Project is the fact that the CDs were offered by ordinary people, thus confiding some personal data to the artist. No one knows the nature of the data that make up the silver dress of an industrial building where the repeated gestures of the workers of yesteryear still resonate. Beyond the monumentality of Byeong Sam Jeon's installations, there is a symbolic dimension, as is again the case with The men with five tongues (2016). This work brings together, in relative darkness, a hundred fans that the public triggers when approaching. The title of the work encourages us to reconsider the violence of those who are protected by online anonymity within this new agora that is the public space of social media.\nPhilomène Longpré, Cereus, Queen of the Night, 2009-2013.\nPhilomène Longpré’s Cereus, Queen of the Night (2009- 2013) is a hybrid artwork at the crossroads of sculpture, robotics, image, and sound. This makes it, in a way, a total work of art, a concept born with German romanticism in the 19th century and celebrated in the 1950s by the artists affiliated with Black Mountain College. In the center of Cereus, Queen of the Night, there is a woman who performs, and the title of this immersive installation suggests her standing. The video sculpture creates the context of her environment that each viewer interprets in their own way. Formally recalling either a flower or a cocoon, the robotic elements also position it as a technical object. The sculpture protects and, at the same time, encloses the image, making this Queen of the Night the prisoner of the object that allows it to exist. A key element lost in reproductions, the video sculpture Cereus, Queen of the Night reacts to changes in its immediate environment, and the sounds inherent in the mechanized movements of its translucent petals add to the music of the piece when it is played in front of the audience. Cereus, Queen of the Night is an augmented video that exemplifies the expanded cinema theorized by Gene Youngblood as early as 1970, renewing it with the technology of our time.\nSophie Kahn, Periode de Clownisme, F, 2014.\nSophie Kahn’s subject of study is the human. She experiments as much with the possible representations of both still and moving images as she does with sculpture. Her favorite tool is a 3D scanner, with which she captures body fragments that are never really immobile. The act of capturing three-dimensional models generally requires poses that once again recall the origins of photography, although Kahn refers more explicitly to the history of radiography. The lack of points, lines, surfaces or matter summon the idea of the unfinished in painting, while the invisible parts of her sculptures correspond to the white or black monochrome backgrounds of her prints or sequences. Though some faces seem to have been burned and limbs mutilated, there is no suggestion of suffering whatsoever. When it comes to whole bodies, they appear to us reassembled, as in natural history museums. Sophie Kahn's work is about capturing life to reveal an element of eternity that lies dormant in each of us. These elements of eternity reveal themselves intermittently, because it is death that gives life its full meaning—a sentiment expressed by past civilizations through the creation of funerary masks, shadow portraits of those whose three-dimensional avatars are eternal.\nSamuel Adam Swope, Banana Mission; a monkey behavioral study, 2010.\nSamuel Adam Swope combines the natural and the technological in a unique way. Most recognized for his aerial art, he constructs and controls aesthetic systems that work with air, and are often themselves airborne. For Swope, aerial art “frames air, giving it a perceptible and systematic volume”. He often creates objects or environments that facilitate novel situations that oscillate between playful and poetic, introducing an uncanny apparatus to challenge cultural constructs, manipulate norms and produce ephemeral spectacles, which are then complemented by skillful documenting and storytelling. This approach is evident in Swope’s technological-natural hybrid Banana ‘copter, a flying banana created for the project Banana Mission; a monkey behavioral study (both 2010). Banana Mission was filmed throughout Hong Kong, from its emergence in the open market to its encounter with feral monkeys at Kam Shan Country Park (also known as Monkey Mountain). Reversing his approach and bringing natural-technological hybrids into the gallery, Ecotone (2017) constructs a narrative around a flying creature, assembled from a mixture of plant and electronic constituents, that is transformed into a ghostly apparition. Each of these works plays with the intersection of technology and the non-human, framing these encounters through an anthropomorphic lens to explore our place within these complex hybrid systems.\nJoshua Mosley, Jeu de Paume, 2014.\nJoshua Mosley is an artist who practices another expanded form of cinema by combining ancient processes from the pre-cinematographic period, such as stop motion, with the most advanced post-production technology. As a director, he produces animated short films whose subjects are unusual. They are often presented as animation loops in contemporary art events, such as the 2014 Whitney Biennial which featured Mosely’s Jeu de Paume. As for the public, it can only let itself be carried away by stories in which the real appears sublimated. We never really know what we are observing as materials ranging from clay to models intermingle with media combining drawing, watercolor and photography with computer graphics. Joshua Mosley is also an artist who leaves clues in his worlds. Clues that disturb our perception of scenarios borrowed from the history of the human sciences. In Dread (2007), for example, French philosophers Jean-Jacques Rousseau and Blaise Pascal discuss the human condition. The artist also details his production process on his website where we can see how his aesthetic depends on the intersection of techniques and technologies.\nJohn Gerrard, Western Flag (Spindletop, Texas), 2017.\n3D engines are to video games what post-production is to cinema. With their appearance in the 1990s, the general public finally discovered the attractions of immersion combined with interaction. As the power of machines continues to grow, the renditions of these applications have continued to approach a certain idea of reality. During the 2000s, John Gerrard — another graduate of the Art and Technology Studies department at SAIC - focused on the narrative potential of generative simulations. His landscape scenes, without beginning or end, allow contemplation in infinite variations using camera trajectories whose extreme slowness is immediately noticeable as the opposite of the pace of video games, the aesthetics of which the artist readily borrows. In 2009, at the 53rd Venice Biennale, the art world was seized by his large-scale projected video landscapes reminiscent of the American Great Plains region. More recently, Gerrard’s echoes the challenge facing humanity—global warming—by virtually planting his Western Flag (2017) in the Lucas Gusher, in Spindletop, Texas. The work is literally “on Texas time,\" which symbolized economic development through a natural resource that would change the world: oil. The flag, which usually expresses pride and whose fabric has been replaced by thick black smoke, evokes the end of a world. An end where unbridled consumption can only lead to an announced catastrophe that we still hope we can avoid.\nStephanie Andrews, Ghost Forest, 2016.\nSince its origins, cinema has developed in two distinct ways, one of which is more industrial and the other, resolutely experimental. A department dedicated to experimentation with creative technology in an art school must prepare its students simultaneously for such orientations, as distinct as they may be, at least in appearance. Stephanie Andrews works at the boundary between commercial cinema and experimental art. For example, she began as a technical director for some of the most successful animated films, such as A Bug’s Life and Toy Story 2, during the late 1990s. Her latest research has led her to offer audiences virtual reality experiences. Becoming the main actors of Ghost Forest (2016), audiences know that they are in two different places at the same time: in the indoor space where they wear a VR headset and in that of a virtually reconstructed nature that they perceive all around them. With Shards (2017), the experience becomes progressively more complex when the user is immersed in the empty space of a three-dimensional monochrome. During their experience they have the opportunity to observe the fragments of the multiple worlds which virtually surround them. Is the user witnessing a future in which the film industry joins that of video games?\nAndrea Polli, Energy Flow (source Jared Rendon Trompak), 2016.\nAndrea Polli is an environmental artist who, through her often monumental creations, sounds the alarm. In 2015, Polli’s light installation Particle Falls addressed the world’s most prominent political actors by appropriating the wall of a facade adjoining the Mona Bismarck American Center in Paris. At the same time and in the same city, the 21st United Nations Conference on Climate Change was taking place and we were awaiting important political decisions for universal action on global warming. The projected work represents a cascade of blue water symbolizing purity. But, activated by a nearby sensor, it is transformed into a burst of flames when the air quality indicates an excessive presence of fine particles in the environment. Contemporary threats are invisible or well hidden, and air pollution is one of them. Particle Falls warns us about the quality of the air we breathe while contemplating it. Since 2016, Polli’s Energy Flow has been enhancing Pittsburgh's Rachel Carson Bridge with a myriad of LED lights powered by wind turbines attached to the structure of the bridge. Thus, from sunset to sunrise, the people of Pittsburgh are visually informed of the unseen potential energy of the passing winds. Artists will inevitably have a role to play in the necessary energy transformation that companies, cities or states must initiate without delay all around the world.\nTrevor Paglen, Untitled (Reaper Drone), 2010.\nIf there is an artist of the invisible it is Trevor Paglen, who uses photography, among other mediums, to reveal what we usually cannot, or do not know how to, see. In the images of his series Limit Telephotography initiated in 2007, the secret military infrastructures that he presents us are less interesting than the blur due to the thick layer of atmosphere, heat and dust that protects them from being seen. The real subject in these same shots is the distance that prevents approaching them. In Untitled (Reaper Drone) from 2010, the subject ‘drone’ can appear to us as a pretext to photograph the morning or evening skies that evoke both the painter William Turner and the photographer Ansel Adams. However, in each of these images there are tiny details informing us of the contemporary threats of constant surveillance or possible strikes. Likewise, the NSA-Tapped Undersea Cables, North Pacific Ocean (2016) remind us of the materiality of the internet network that companies and states share.\nHuong Ngo, Reap the Whirlwind, 2018.\nLet's end this non-exhaustive case study with an exhibition of Huong Ngo that brings us back to Chicago in 2018. Her title, Reap the Whirlwind, evokes the idea of consequences to be inflicted, in this case, those relating to the French colonial past in Indochina. Based in Chicago, the artist also works in France and Vietnam, creating works which depict the fragments of a personal story that she assembles with historical events or characters, like Nguyen Thi Minh Khai, a young communist activist who lost her life at the age of 31 in a colonial prison. In Reap the Whirlwind, documentation is essential, exhibited aside other pieces as part of the artwork. The figure of the concubine evoking the relationship of a young Indochinese woman with a French man, possibly one of power, is central. The concubine, in her relation to the colonist who in a certain way she controls, is considered as politically engaged in the artist's books, which draw in the spectator's body. For it is by placing one's hands on the pages of the volumes that the heat temporarily reveals the characters, like so many family secrets that are hard to evoke before forgetting them again. Huong Ngo contributes to an emerging trend in contemporary art that shifts the boundaries between the artwork and its documentation.\nWhat all these artists have in common is that they studied in the department of Art and Technology Studies at the School of the Art Institute of Chicago, which, over the past fifty years, has constantly evolved to best stimulate art students or emerging artists from around the world. They not only continue the research of their predecessors, they initiate new trends in art that uses the technology of their time.","Media conservation is responsible for the audio, 35mm slide, performance, software, video, and film-based artworks in MoMA’s collection, caring for them in collaboration with colleagues across the Museum’s departments including Audio Visual, Curatorial, Information Technology, and Registrar. The first conservation position at MoMA was created in 1959 for a paintings conservator, and since then the Conservation Department has evolved to include specialists in sculpture, paper, photography, conservation science, and most recently media, in 2007.\nBy their nature, media-based works rely on technology for creation and exhibition. Today we are all too aware of its rapid changes that make these works inherently fragile or at least unstable, and their long-term preservation problematic. The core role of media conservation is to help manage these changes over time while respecting the artist’s intent. Although no artwork is ever the same and an artist’s opinion can differ from one work to another, conservators working with these complex objects have over the last two decades devised broad strategies to tackle the challenges of these works.\nToday, when a newly acquired media-based work arrives at the Museum, most commonly what we receive in the “box” is some type of media carrier (external hard drive, tape, optical disc), a certificate, and a set of installation instructions. In the case of Ten Thousand Waves (2010) by Isaac Julien, an epic nine-channel video work, we simply received two 8-terabyte hard drives at acquisition. In short we receive the essence of the work.\nAs conservators it is our job to take these raw ingredients and ensure that not only are they fresh and in good condition, but also, to extend the metaphor, that they perform as expected when thrown into the pot at exhibition time.\nBroadly speaking there are three main strands to our approach. Starting with the tangible media element we gather as much contextual information as possible, such as how it was created, recorded, produced, or programmed. Then we carefully watch and listen to the entire duration of each media element. Once these have been assessed and documented, like any collection work, appropriate storage conditions are required.\nSecond, we review the installation guidelines, which outline how the media and the artist’s intent should be translated and realized through display equipment in the gallery space.\nThe display equipment is no less important than the media, and for some works the equipment also functions as sculpture, as in Berlin Startup Case Mod: Rocket Internet (2014) by Simon Denny. Here the artist incorporates a 40” LED flat screen monitor, specifically the UE40F6500 model manufactured by Samsung.\nOlder and contemporary works often employ “vintage” technology such as Sorry (2005–12) by Luther Price consisting of 80 handmade transparencies displayed with a 35mm slide projector.\nThere are also works where the technology is not noticeable to the casual viewer, but the artist may for example have specified that the video be projected with a cathode ray tube projector, an obsolete piece of equipment which can only be supported as long as supplies last. This is the case for Deadpan (1997) by Steve McQueen. Understanding the fundamentals of technology, its historical context, how it works, troubleshooting, maintenance, and storage of the equipment—all these challenges sit alongside the preservation of the actual media.\nRecently I was struck by a comment made by my colleague Lynda Zycherman, a sculpture conservator at MoMA, during a talk she gave about a series of sculptures by Pablo Picasso, one of which is in MoMA’s collection entitled Glass of Absinthe (1914). Art historical research and scientific analysis had helped to unlock clues regarding the sculptures’ original state, materials, and construction. But one question remained: Which bronze-casting foundry had produced them? Lynda exclaimed, “If only the artist was still alive I could just ask him!” Since the majority of media-based works in MoMA’s collection are contemporary, we in Media Conservation are in the fortunate position of being able to do exactly that most of the time, and a significant part of the job involves communicating with artists and their representatives. In this sense, we work in reverse and attempt to anticipate such questions by de-constructing, researching, and documenting as much as we can now in order to save future colleagues from the frustration that Lynda described. This is our third angle. It is of course futile to imagine that every aspect of every work can be fully documented, but records of e-mail correspondence, telephone calls, and conservation-based interviews with the artists themselves play a central role in our efforts to maintain the integrity of their art. Media works are complex and multifaceted and it is the sum of these different strands, together with the voice of the artist, which enable us to maintain their authenticity now and into the future.\nInterested to discover more? This is the first of four blog posts looking at the work of Media Conservation, which over the coming weeks will look in detail at some of our activities including the migration of tape-based works to digital video files by Assistant Media Conservator Peter Oleksik, and the design and build of our digital art vault, by Digital Repository Manager Ben Fino-Radin. Stay tuned!\nIn the meantime, here are some links for additional information on media conservation:\nMatters in Media Art\nTime-Based Media Conservation at the Solomon R. Guggenheim Museum\nElectronic Media Group of the American Institute for Conservation of Historic & Artistic Works"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:438e6a19-cc85-46d6-938e-250eb5e062b9>","<urn:uuid:21b82693-77a6-4622-90ee-066d83236d10>"],"error":null}
{"question":"How do traditional Celtic wind instruments differ in their portability and performance settings?","answer":"Celtic wind instruments show notable differences in their portability and performance contexts. The flute is versatile in performance settings, being effective as both solo material and in ensemble playing with chord instruments like guitar. In contrast, the uilleann pipes, while also played indoors, are specifically designed to be played in a seated position and require more setup, as they use a small set of bellows strapped around the waist and right arm to inflate the bag. This bellows system, while making the instrument less portable, offers the advantage of allowing some pipers to converse or sing while playing, and helps maintain better tuning by providing dry air to the reeds.","context":["There are currently no items in your cart.\nCeltic Music For Flute arranged by Jessica Walsh. For flute. Saddle stitch. Celtic, Irish and Folk. Beginner/Intermediate. Flute solo book and performance CD. Standard notation, chord names and introductory text. 38 pages. Duration 70 minutes. Published by ADG Productions (AD.ADG043-CD).\nISBN 1882146875. With standard notation, chord names and introductory text. Celtic, Irish and Folk. 9x12 inches.\nCeltic Music For Flute by Jessica Walsh contains 44 tunes for flute which can be played solo or with a chord instrument. This great collection includes tunes from Scotland, Ireland, England, Wales, and the Isle of Man. The melodies were chosen from more than four hundred years of Celtic music. Most of the pieces have been extended with variations, which make the tunes a more suitable length for performances and are a lot of fun for the flutist to play. In addition, they will introduce the beginner to the joys of improvising in Celtic music, in which ornamenting and augmenting melodies is a tradition.Every tune in the book is included on the digitally mastered CD. Though the tunes are very effective as solo material, Jessica chose to record most of them with guitarist Allan Alexander. Allan has rendered the chords artfully, and the result is a really lovely CD which is a pleasure to listen to. For the player who is new to Celtic music, the recording will give insights into the melodies, and it will allow the flutist who has no access to a chord instrument to hear the harmonic progressions. For those who know and love these wonderful tunes, it will be a joy to hear them played by two musicians who obviously love to play this music together.\n- Follow My Highland Soldier\n- Blythe Was the Time\n- Carpenter's Morris\n- The Gentle maiden\n- O as I Was Kissed Yestreen\n- Arrane ny Skeddan (Song of the Herring)\n- Song of the Falcon Chief\n- Hush! The Waves Are Rolling In\n- Hunting the Hare\n- Fisherman's Lilt\n- The Foggy Dew\n- The Cuckoo's Nest\n- I Left Him on the Mountainside\n- Cold and Raw\n- Road to Listonvarna - The Morris Dance\n- Silent, O Moyle\n- Suo Gan\n- Kemp's Jig\n- Young Catherine\n- My Thousand Times Beloved\n- Pat's Missing Finger\n- Lassie, Lie Near Me\n- Shule Agra\n- The Butterfly\n- The Swan\n- MacCrimmon's Lament\n- Come, Give Me Your Hand\n- Send 'em Running\n- Carolan's Farewell\n- The Chase (from Into the Hills)\n- Niel Gow's Lament for the Death of His Second Wife\n- The Queen's Dream\n- Pastheen Fionn\n- The Exile Song\n- Fear a' Bhata (the Boatmen)\n- Parting Forever\n- Ye Banks and Braes of Bonnie Doon\n- Mary's Lament\n- Hush My Babe, Lie Still and Slumber\n- An Emigrant's Daughter\n- Johnny Cope\n- Manx Lullaby\n- Blind Mary\nCeltic Music For Flute\n- Explain exactly why you liked or disliked the product. Do you like the artist? Is the transcription accurate? Is it a good teaching tool?\n- Consider writing about your experience and musical tastes. Are you a beginner who started playing last month? Do you usually like this style of music?\n- Feel free to recommend similar pieces if you liked this piece, or alternatives if you didn't.\n- Be respectful of artists, readers, and your fellow reviewers. Please do not use inappropriate language, including profanity, vulgarity, or obscenity.\n- Avoid disclosing contact information (email addresses, phone numbers, etc.), or including URLs, time-sensitive material or alternative ordering information.\n- We cannot post your review if it violates these guidelines. If you have any suggestions or comments on the guidelines, please email us.\n- All submitted reviews become the licensed property of Sheet Music Plus and are subject to all laws pertaining thereto. If you believe that any review contained on our site infringes upon your copyright, please email us.\nCeltic Music For Flute\nTell a friend (or remind yourself) about this product. We'll instantly send an email containing product info and a link to it. You may also enter a personal message.\nWe do not use or store email addresses from this form for any other purpose than sending your share email.\nNow you can also view your purchased titles on your iPad with our\nfree Sheet Music Plus Digital Print Viewer app.\nWith Digital Print, you can print from your computer immediately after purchase, or wait until its convenient. And our software installation is easy - we'll guide you through the simple steps to make sure you have Adobe Flash Player, Adobe AIR and the Sheet Music Plus AIR application on your computer.\nSee all System Requirements for Digital Print\nNote: Our Digital Print items are purchased solely through our website. By purchasing a Digital Print item, you are authorized by the publisher to print out the purchased number of copies for that particular title. Thus, we are unable to fax, mail or email digital titles in any form or file format (including PDF) as it violates copyright law.\nGoogle Chrome Users: The most recent Google Chrome update is not compatible with Adobe AIR. We recommend using an alternate browser (such as Internet Explorer, Safari, or Firefox) in order to download Adobe AIR and print your music.\nDigital Print - Available Instantly!\nIf you have any questions, comments or concerns, please contact us at email@example.com.\nInstrument: Flute Solo Sheet Music\nFormat: Listening CD","The first bagpipes to be well-attested to for Ireland were similar, if not identical, to the Scottish Highland\npipes that are now played in Scotland. These are known as the \"Great Irish Warpipes\". In Irish and\nScottish Gaelic, this instrument was called the p铆ob mh贸r (\"great pipe\").\nThe uilleann pipes (pronounced \"illyun\" - not \"yooleeun\" ), originally known as the Union pipes,\nare the characteristic national bagpipe of Ireland. The uilleann pipes bag is inflated by means of a small set of\nbellows strapped around the waist and the right arm. The bellows not only relieves the player from the effort\nneeded to blow into a bag to maintain pressure, they also allow relatively dry air to power the reeds, reducing the\nadverse effects of moisture on tuning and longevity. Some pipers can converse or sing while playing as well.\nWhile the warpipe was alive and well upon the battlefields of France, the warpipe had almost disappeared in\nIreland (as a result of its outlaw by the English. The union or uilleann pipe required the joining of a bellows\nunder the right arm, which pumped air via a tube to the bagpipe under the left arm, with bellows.\nThe uilleann or union pipes developed around the beginning of the 18th century, the history of which is here\ndepicted in prints of carvings and pictures from contemporary sources. At about the same time the Northumbrian\nsmallpipe was evolving into its modern form, early in the 18th century; a tutor of the 1750s calls this early form\nof the uilleann pipes the \"Pastoral or New bagpipe.\" The Pastoral pipes were bellows blown and played in either a\nseated or standing position. The conical bored chanter was played \"open,\" that is, legato, unlike the uilleann\npipes, which can also be played \"closed,\" that is, staccato. The early Pastoral pipes had two drones, and later\nexamples had one (or rarely, two) regulator(s). The Pastoral and later flat set Union pipes developed with ideas on\nthe instrument being traded back-and-forth between Ireland, Scotland and England, around the 18th and early 19th\nThe earliest surviving sets of uilleann pipes date from the second half of the 18th century but it must be said\nthat dating is not definitive. Only recently has scientific attention begun to be paid to the instrument and\nproblems relating to various stages of its development have yet to be resolved.\nThe uilleann pipes are distinguished from many other forms of bagpipes by their sweet tone and wide range of\nnotes - the chanter has a range of two full octaves, including sharps and flats - together with the unique blend of\nchanter, drones, and \"regulators.\"\nThe regulators are equipped with closed keys which can be opened by the piper's wrist action enabling the piper\nto play simple chords, giving a rhythmic and harmonic accompaniment as needed. There are also many ornaments\nbased on multiple or single grace notes.\nThe chanter can also be played staccato by resting the bottom of the chanter on the piper's knee to close off\nthe bottom hole and then open and close only the tone holes required. If one tone hole is closed before the next\none is opened, a staccato effect can be created because the sound stops completely when no air can escape at\nThe uilleann pipes have a different harmonic structure, sounding sweeter and quieter than many other bagpipes,\nsuch as the Great Irish Warpipes, Great Highland Bagpipes or the Italian Zampognas. The uilleann pipes are often\nplayed indoors, and are almost always played sitting down.\nBack To The Top Of The Uilleann Bagpipes Page"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:c9e1b19f-2163-42b3-970a-1cfe9255f28d>","<urn:uuid:12f84d81-86dd-4445-819f-7b0623ff8ef0>"],"error":null}
{"question":"Are STEM Fest and SheTech Explorer Day comparable in terms of their event structure and activities?","answer":"Both events offer interactive STEM activities but have different structures. STEM Fest features organization booths, breakout sessions, interactive activities, and STEM challenges, with specific activities like bridge building, robotics, and 3D printing. SheTech has a more modular design with three main components: Workshops (24-person groups led by industry mentors), TechZone (company booths with hands-on activities), and TechChallenge (team-based problem-solving). While both events are interactive, SheTech offers more flexibility in its structure, allowing organizers to choose between full-day, half-day, or individual component formats.","context":["What causes combustion? What prevents bridges from collapsing? How are robots made? Students in elementary and secondary school have a natural curiosity about these and many other questions about how the world works; however, research shows that by 4th, 5th, and 6th grade, interest begins to fade as more difficult formulas and computations become part of the curriculum.1\nSTEM events provide students with opportunities to see and interact with Science, Technology, Engineering, and Mathematics (STEM) in fun and engaging ways, stimulating their curiosity and interest. With the help of professionals in these fields, schools can create events that will engage not only students, but also parents and the entire community.\nIn 2014, Highland View Academy in Hagerstown, Maryland, U.S.A., under the direction of Ophelia Barizo, created a new STEM Department. The program offers students an interdisciplinary approach to STEM and utilizes problem-based learning. Students learn how to apply principles to solve problems they will encounter living in the digital age.2 They also have opportunities for internships, field trips, STEM-related community service and outreach, special courses (robotics, app development, project-based learning, and AP computer science), and networking with STEM professionals.3\nSTEM Fest is sponsored by this department to create a school- and community-wide culture of appreciation for STEM and cultivate and nurture in younger students an interest and desire to study in these areas. The annual event, coordinated by Ophelia Barizo with the help of STEM teachers Colleen Lay, Lisa Norton, and Myrna Nowrangi, is in its fifth year.\nThe first STEM Fest took place in October 2014. Organizers invited several Federal STEM organizations such as the U.S. Department of Energy (DOE), the National Oceanic and Atmospheric Administration (NOAA), the National Aeronautics and Space Administration (NASA), the National Institutes of Health (NIH), the National Science Foundation (NSF), the U.S. Department of Agriculture (USDA), the National Security Agency (NSA), and many other private STEM organizations to have booths and representatives present.4\nAt the booths, organizations showcased the many exciting aspects of STEM careers and gave away resources, posters, information sheets, and other materials for teachers, students, and homeschoolers. The event was such a success that since that time, STEM Fest has expanded to include many more representatives from STEM-related organizations, breakout sessions, interactive activities, STEM challenges (activities), and more.\nDuring breakout presentations, professionals working in STEM industries share cutting-edge research. For example, in 2016, the festival featured breakout sessions from presenters such as Walt Sturgeon, nationally known mycologist and photographer who has written several books on mycology (the study of fungi), and is chief mycologist for the North American Mycological Association. Sturgeon spoke on “Wild Mushrooms, a World of Wonder at Our Feet.”\nThe 2016 STEM Festl also featured Natalie Harr, an award-winning educator and former Einstein Fellow and Presidential Awardee in Math and Science Teaching, who has been on two research trips to the Antarctic. Harr talked about her experiences traveling to the Antarctic Peninsula to study the world’s southernmost insect, the wingless midge (fly) Belgica antarctica. In addition, Maria and Chris Esquela shared the work of e-NABLE, a global network of volunteers using 3-D printing to create fingers and hands for children and adults in the underserved areas of the world.5\nMany STEM representatives from programs at private and public colleges and universities also attend STEM Fest, giving students an opportunity to hear what is available in terms of degree and certificate programs. At the booths, resources, posters, information sheets, DVDs on science-related topics, and much more are given away. Both teachers and parents who homeschool have found these to be very useful.\nThe HVA STEM Department also prepares hands-on, interactive activities for children, teens, and attendees such as: bridge building; robotics with Ozobots, Spheros, and Dash and Dot robots; 3-D printing; 3Doodler; making slime, silly putty, and polymer snow; several STEM toys for a Kids Korner; and many other items. Craig Trader’s Chaos Machine, a massive collection of tubes, tracks, lifts, and motors that move marbles around, is a hit with everyone, especially the children.\nRefreshments are sold during the event, and the proceeds go to support the STEM program. The family-friendly, free event continues to be a success. The conference-wide event brings students from neighboring Adventist schools, private schools, and public schools. From the program’s inception, the attendance has averaged around 400 people.\nFive Tips for Organizing a STEM Event\n- Attend STEM conferences whenever possible. Visit the exhibit halls to get ideas for STEM events.\n- Network with STEM professionals. These experts are excellent resources for booths and breakout sessions, or may know someone who is an excellent presenter for breakout sessions.\n- Prepare for the event several months in advance. It takes time to make phone calls, write e-mails and letters to possible presenters for breakout sessions, arrange for people to run STEM booths, and secure a place in the school’s calendar of events. Some people will not be able to participate for a variety of reasons, so organizers will need to keep expanding the list of contacts and reach out to other potential participants.\n- Order materials for STEM hands-on activities and challenges several months in advance to be sure they arrive in time and can be screened and organized more efficiently (e.g., making sure all the necessary materials are included and that the experiments and challenge activities work as they should).\n- Organize volunteer participation a few weeks before the event. Invite parents, board members, and students to help set up the gymnasium and breakout rooms for the event, man the booths, conduct hands-on activities, and sell food for STEM fundraising.\nIf you plan early, get a sufficient number of contacts for the event, and organize well, your school will be on its way to a successful STEM event that will be educational and fun for everyone.\nOphelia Barizo, “STEM Fest: Fun for Everyone!” The Journal of Adventist Education 80:4 (October-December 2018): 41-43, 45. Available at https://jae.adventist.org/en/2018.4.8.\nNOTES AND REFERENCES\n- Yu Xie, Michael Fang, and Kimberlee Shauman, “STEM Education,” Annual Review of Sociology 41 (August 2015): 331-357: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4712712/; EnCorps, “The STEM Crisis in American Schools,” EnCorps STEM Teachers Program: https://encorps.org/about/stem-crisis/.\n- Highland View Academy STEM Program: https://www.hva-edu.com/admissions/stem.\n- Schools outside the Washington, D.C., area could invite scientists and engineers from STEM programs at local colleges and universities to make presentations or connect with local manufacturing companies that hire in STEM-related fields. For example, one regular participant at the HVA STEM Fest is a leading manufacturing company that makes lift equipment. Additional STEM Fest participants could come from private STEM organizations, hospitals, laboratories, wildlife/animal preserves and parks, zoos, and botanical gardens.\n- Ophelia Barizo, “STEM Fest Held at Highland View Academy,” https://www.heraldmailmedia.com/news/education/stem-fest-held-at-highland-view-academy/article_17b6c6e8-ea02-55d0-9409-9c059ea6e3d4.html.","Let's Get Started\nInterested in organizing a SheTech Explorer Day in your area? Getting started is easy. Here we share some tips on these topics to make organizing SheTech helpful.\n- Organizing Team\n- SheTech Modular Structure\n- Planning Resources\nSheTech explorer is an industry and education partnership. So your team must include an industry lead and an education lead. It is rare to find this both in a single person. You should plan 3 to 4 months ahead and should plan on a core team of 4 people. You should plan on weekly team meetings and regular contact with WTC to make sure everything runs smoothly.\nOrganizing steps include:\n- Notify us that you want to organize SheTech. Signup here:\n- Recruit your core team\n- Confirm location and date\n- Sponsor and education outreach\n- Industry mentor signups\n- Student signup\n- Execute a fun and successful SheTech Explorer Day!\nCore Team Responsibilities include (can combine roles if desired):\nSheTech Point of Contact (POC): Organize core team, Liaison with Women Tech Council, Track project schedule and budget, Manage event promotion overall - PR, photographer\nSite Lead: Secure location and date, Manage venue logistics, meeting space, parking, lunch vendor, Coordinate event volunteers\nIndustry Lead: Recruit Black Diamond, Trekker, and TechZone sponsors, Manage sponsor relationships and prizes, Get Industry mentors and TechZone companies\nEducation Lead: Work with educators and parents to recruit students, Manage student registration\nContent Lead: Find keynote speaker, Manage TechChallenge, TechZones, and Workshops content, Utilize WTC “out of box” resources as needed\nModular SheTech Design\nSheTech has three basic content elements:\nWorkshops: Girls learn in groups of 24 from an industry mentor about a STEM topic. These are fun, interesting, and most importantly, interactive workshops. Some examples of workshops are:\n- Nothing Fits Better Than a Perfect Pair of Genes\n- How science and technology help solve crimes\n- 3D Graffiti\n- MATH + ORIGAMI: The Odds are Always in Favor of FUN\n- Diamonds are a Girl’s Best Friend: The Chemistry of Diamonds\n- Is that Robot winking at you?\n- 3-2-1 Blast Off\nTechZone: This is an exploration zone where industry companies provide booths in a large area of hands-on STEM ideas. Some fun ideas have included\n- LED bracelets\n- Chemistry of makeup\n- Instant snow\n- 3D doodlers\n- Fog machines\nTechChallenge: All the attendees form teams of 10 (including an industry mentor) and solve a real-world STEM challenge. The girls get to experience teamwork, problem-solving, and pitching their solution to industry judges. What’s more fun than seeing your own ideas come to life and winning a prize for your great thinking!\nYou get to choose. SheTech was specifically designed so that organizers could structure our SheTech elements based on what works best for them.\nFor example, some sites may want to do a full SheTech day like this:\nTechZone/Workshops in morning (3 hours)\nTechChallenge in afternoon (2 hours)\nOr some might do a half-day SheTech like this:\nTechZone for 1 hour\nTechChallenge for 2 hours\nOr some organization want to “plug in” a SheTech element to a STEM event they are already doing and focus just on girls.\nExisting STEM event\nTechChallenge 2 hours after event\nEither way, you get to choose how to run your own event. We’ve also provided a number of “out of the box” resources if you choose to use them.\nAs an organizer, you get access to basic program support from Women Tech Council, most of which will be available through this website:\n- Event planning\n- Best practices for organizing\n- Event templates and marketing documents\n- Event planning checklist\n- Event registration system\n- Budget management\n- Out of the Box content for:\n- Techzone exhibits\n- Workshop curriculum\n- TechChallenge scenarios\nWe also help you promote your event by providing:\n- Promotional Store for branded signage, T-shirt, swag bags, lanyards\n- Dedicated SheTech landing page for your event for promotion and visibility\n- Included in Media Programs including PR and social media distribution\nSignup to become an Organizer!\nWhy Should You Become a SheTech Organizer?\nHigh school girls in 9th-12th grade, get a release day from school, spend time on campus, learn about tech jobs and get access to internship and scholarship resources\nMeet Industry Mentors & Connect to Internships\nJoin over 200 industry mentors. Learn from them, work with them on a design challenge, present to them and learn about high school internship opportunities.\nHere Are a Few of the Innovative Exhibits You'll See\nHack, design games, build a robot, experience VR, make a light up ring, dive deep into DNA, 3D printing, make smog and use a fog machine."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:76524a21-b8f9-463b-a795-b5133990d7e3>","<urn:uuid:f045c03c-e366-43dc-8e30-0ead87f3206e>"],"error":null}
{"question":"Looking into genetic research breakthroughs - yeast-based genome modifications vs cancer predisposition studies?","answer":"Yeast-based genome modifications achieved the milestone of growing and modifying bacterial genomes (M. mycoides) in yeast cells, then successfully transplanting them back into bacterial cells, establishing a complete cycle of bacterial genome engineering. Cancer predisposition studies, on the other hand, have identified crucial mutations in genes like BRCA1, BRCA2, and CHEK2, leading to important discoveries about hereditary cancer risks. The yeast research focused on developing methods for bacterial genome manipulation, while cancer studies utilized techniques like massively parallel sequencing to discover novel cancer-predisposing genes and explore their clinical implications.","context":["FOR IMMEDIATE RELEASE\nJ. Craig Venter Institute Researchers Clone and Engineer Bacterial Genomes in Yeast and Transplant Genomes Back into Bacterial Cells\nNew methods allow for the rapid engineering of bacterial chromosomes and the creation of extensively modified bacterial species; should also play key role in boot up of synthetic cell\nROCKVILLE, MD — August 20, 2009 — Researchers at the J. Craig Venter Institute (JCVI), a not-for-profit genomic research organization, published results today describing new methods in which the entire bacterial genome from Mycoplasma mycoides was cloned in a yeast cell by adding yeast centromeric plasmid sequence to the bacterial chromosome and modified it in yeast using yeast genetic systems. This modified bacterial chromosome was then isolated from yeast and transplanted into a related species of bacteria, Mycoplasma capricolum, to create a new type of M. mycoides cell. This is the first time that genomes have been transferred between branches of life — from a prokaryote to eukaryote and back to a prokaryote. The research was published by Carole Lartigue et al in Science Express.\nHamilton Smith, M.D., one of the leaders of the JCVI team said, \"I believe this work has important implications in better understanding the fundamentals of biology to enable the final stages of our work in creating and booting up a synthetic genome. This is possibly one of the most important new findings in the field of synthetic genomics.\"\nThe research published today was made possible by previous breakthroughs at JCVI. In 2007 the team published results from the transplantation of the native M. mycoides genome into the M. capricolum cell which resulted in the M. capricolum cell being transformed into M. mycoides. This work established the notion that DNA is the software of life and that it is the DNA that dictates the cell phenotype.\nIn 2008 the same team reported on the construction of the first synthetic bacterial genome by assembling DNA fragments made from the four chemicals of life — ACGT. The final assembly of DNA fragments into the whole genome was performed in yeast by making use of the yeast genetic systems. However, when the team attempted to transplant the synthetic bacterial genome out of yeast into a recipient bacterial cell, all the experiments failed.\nThe researchers had previously established that no proteins were required for chromosome transplantations, however they reasoned that DNA methylation (a chemical modification of DNA that bacterial cells use to protect their genome from degradation by restriction enzymes, which are the proteins that cut DNA at specific sites) might be required for transplantation. When the chromosome was isolated direct from the bacterial cells it was likely already methylated and therefore transplantable due to it being protected from the cells restriction enzymes.\nIn this study, the team began by cloning the native M. mycoides genomeinto yeast by adding a yeast centromere to the bacterial genome. This is the first time a native bacterial genome has been grown successfully in yeast. Specific methylase enzymes were isolated from M. mycoides and used to methylate the M. mycoides genome isolated from yeast. When the DNA was methylated the chromosome was able to be successfully transplanted into a wild type species of M. capricolum. However, if the DNA was not first methylated the transplant experiments were not successful. To prove that the restriction enzymes in the M. capricolum cell were responsible for the destruction of the transplanted genome the team removed the restriction enzyme genes from the M. capricolum genome. When genome transplantations were performed using the restriction enzyme minus recipient cells, all the genome transplantations worked regardless of if the DNA was methylated or not.\n\"The ability to modify bacterial genomes in yeast is an important advance that extends yeast genetic tools to bacteria. If this is extendable to other bacteria we believe that these methods may be used in general laboratory practice to modify organisms,\" said Sanjay Vashee, Ph.D., JCVI researcher and corresponding author on the paper.\nThe team now has a complete cycle of cloning a bacterial genome in yeast, modifying the bacterial genome as though it were a yeast chromosome and transplanting the genome back into a recipient bacterial cell to create a new bacterial strain. These new methods and knowledge should allow the team to now transplant and boot up the synthetic bacterial genome successfully.\nThe research published today by JCVI researchers was funded by the company Synthetic Genomics Inc., a company co-founded by Drs. Smith and Venter.\nKey Milestones/Ethical Issues Background on JCVI's Synthetic Genomics Research\n1995: After sequencing the M. genitalium genome (published in 1995), Dr. Venter and colleagues begin work on the minimal genome project. This area of research, trying to understand the minimal genetic components necessary to sustain life, started with M. genitalium because it is a bacterium with the smallest genome known that can be grown in pure culture. This work was published in the journal Science in 1999.\n2003: Drs. Venter, Smith and Hutchison (along with JCVI's Cynthia Andrews-Pfannkoch) made the first significant strides in the development of a synthetic genome by assembling the 5,386 base pair genome of bacteriophage φX174 (phi X). They did so using short, single strands of synthetically produced, commercially available DNA (known as oligonucleotides) and using an adaptation of polymerase chain reaction (PCR), known as polymerase cycle assembly (PCA), to build the phi X genome. The team produced the synthetic phi X in just 14 days.\n2007: JCVI researchers led by Carole Lartigue, Ph.D., announced the results of work published in the journal Science, which outlined the methods and techniques used to change one bacterial species, Mycoplasma capricolum, into another, Mycoplasma mycoides Large Colony (LC), by replacing one organism's genome with the other one's genome. Genome transplantation was the first essential enabling step in the field of synthetic genomics as it is a key mechanism by which chemically synthesized chromosomes can be activated into viable living cells.\nJanuary 2008: The second successful step in the JCVI teams' journey to create a cell controlled by synthetic DNA was completed when Gibson et al published in the journal Science, the synthetic M. genitalium genome.\nDecember 2008: Gibson et al published a paper in Proceedings of the National Academy of Sciences (PNAS) describing a significant advance in genome assembly in which the team was able to assemble in yeast the whole bacterial genome, Mycoplasma genitalium, in one step from 25 fragments of DNA. The work was funded by the company Synthetic Genomics Inc. (SGI). The team is still working to boot up the synthetic cell using all the knowledge gleaned from their previous work.\nSince the beginning of the quest to understand and build a synthetic genome, Dr. Venter and his team have been concerned with the societal issues surrounding the work. In 1995 while the team was doing the research on the minimal genome, the work underwent significant ethical review by a panel of experts at the University of Pennsylvania (Cho et al, Science December 1999:Vol. 286. no. 5447, pp. 2087 — 2090). The bioethical group's independent deliberations, published at the same time as the scientific minimal genome research, resulted in a unanimous decision that there were no strong ethical reasons why the work should not continue as long as the scientists involved continued to engage public discussion.\nDr. Venter and the team at JCVI continue to work with bioethicists, outside policy groups, legislative members and staff, and the public to encourage discussion and understanding about the societal implications of their work and the field of synthetic genomics generally. As such, the JCVI's policy team, along with the Center for Strategic & International Studies (CSIS), and the Massachusetts Institute of Technology (MIT), were funded by a grant from the Alfred P. Sloan Foundation for a 20-month study that explored the risks and benefits of this emerging technology, as well as possible safeguards to prevent abuse, including bioterrorism. After several workshops and public sessions the group published a report in October 2007 outlining options for the field and its researchers.\nAbout the J. Craig Venter Institute\nThe JCVI is a not-for-profit research institute in Rockville, MD and La Jolla, CA dedicated to the advancement of the science of genomics; the understanding of its implications for society; and communication of those results to the scientific community, the public, and policymakers. Founded by J. Craig Venter, Ph.D., the JCVI is home to approximately 400 scientists and staff with expertise in human and evolutionary biology, genetics, bioinformatics/informatics, information technology, high-throughput DNA sequencing, genomic and environmental policy research, and public education in science and science policy. The legacy organizations of the JCVI are: The Institute for Genomic Research (TIGR), The Center for the Advancement of Genomics (TCAG), the Institute for Biological Energy Alternatives (IBEA), the Joint Technology Center (JTC), and the J. Craig Venter Science Foundation. The JCVI is a 501 (c) (3) organization. For additional information, please visit http://www.JCVI.org.\n# # #\nJCVI Media Contact: Heather Kowalski, 301-943-8879, hkowalski(AT)jcvi.org","Discovery and Characterization of Novel Cancer Predisposition Alleles\nThe laboratory serves as the translational research arm of the Clinical Genetics Service in the Department of Medicine, and also collaborates closely with colleagues in the Cancer Biology and Genetics Program of the Sloan Kettering Institute. A historical interest of the laboratory has been the study of genetic isolates and the insights the study of founder mutations provide into the phenotypic spectrum of hereditary disease in all populations. The lab discovered the most common BRCA2 mutation, and was among the first to describe the most common BRCA1 mutation associated with susceptibility to breast and ovarian cancer(1–3), as well as the most common MSH2, APC, and BLM mutations associated with colon cancer in Ashkenazi Jews(4, 6, 8, 10). The lab also played a leading role in discovering the common CHEK2 mutation in this population (16, 20).\nThe description of these recurring mutations has had broad implications on the management of all populations at hereditary risk for cancer. This work included description of other cancer risks associated with BRCA mutations, including ovary(9) colon(12), and prostate(13) cancer and, under the leadership of clinical lab members Drs. Mark E. Robson and Noah Kauff, studies defining the clinical behavior and optimal management of women and men carrying BRCA mutations(7, 11, 14, 15, 18, 22, 23, 25, 27). Population genetic studies have also defined the haplotype structure of Ashkenazim(5, 17, 24, 26), leading to one of the first genome-wide association studies (GWAS) of breast cancer(19) and the discovery of a novel cancer risk allele on chromosome 6(21). A large international GWAS of modifiers of risk of BRCA2 mutations discovered a novel locus on chromosome 10(28), and a follow up to this study used the “onco-chip” to scan approximately 10,000 BRCA2 carriers on protocols worldwide. That study discovered the first modifying locus that is not by itself a breast susceptibility locus.\nThe lab has been focused on utilizing massively parallel sequencing to identify novel cancer predisposing genes in humans. This work involves families with hereditary breast, ovarian, colon, renal, and other solid tumors, as well as lymphoma, myeloma, and acute lymphocytic leukemia. The work is supported by significant grants from the Starr Consortium, the Beene Foundation, and a recent RO1 grant from the National Cancer Institute.\nOther translational projects have focused on the integration of genetic testing for both common and rare variants into clinical management(29, 30), as well as on the clinical utility of genome-wide association studies(31). Clinicians in the laboratory, including Drs. Robson and Kauff and Zsofia K. Stadler, are extensively exploring the clinical translation of germline genetics of breast, ovarian, colon and other cancers. Pharmacogenomic studies, led largely by Dr. Robert J. Klein as well as clinical fellows rotating in the laboratory, are also in progress for bladder, prostate, lung, ovarian, pancreatic, and other cancers. All of the genotyping in the laboratory, which incorporates next-generation sequencing, is overseen by Dr. Vijai Joseph, who has worked closely with colleagues at the Cold Spring Harbor Laboratories and the Broad Institute as part of ongoing collaborations. Recent translational studies have included a second cancer GWAS(32), identification of a novel de novo mechanism of germline susceptibility to cancer(33), and the identification of a homeobox gene-associated breast cancer risk(34). A major effort is under way to devise approaches to translate germline whole exome associated risks for cancer and other diseases. This effort will serve as a forerunner to clinical availability of the personalized genome(35, 36)."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:01b3dcb7-e33c-4dcd-a1eb-491246965037>","<urn:uuid:699e775b-d986-4aab-89f6-ca12fc585315>"],"error":null}
{"question":"Can you tell me what happens when someone goes to Psychiatric Emergency Services at St. Catharines Site? What is process?","answer":"At the St. Catharines Site, the Psychiatric Emergency Service (PES) is a four-bed unit in the Emergency Department where the team provides crisis management, psychiatric assessment and treatment to people in emergency situations. After assessment, individuals are either discharged from PES or admitted to one of the Acute Inpatient Mental Health units.","context":["Psychiatric Emergency Services (PES)\nThe Psychiatric Emergency Service is a dedicated four-bed unit in the Emergency Department at the St. Catharines Site. The PES team provides crisis management, psychiatric assessment and treatment to people presenting with an emergency. Individuals are either discharged from PES or admitted to one of our Acute Inpatient Mental Health units.\nPES supports mental health and addictions care in the Emergency Departments at the Greater Niagara General Site and the Welland Site through the use of video conferencing.\nSexual Assault and Domestic Violence (SADV)\nThe Sexual Assault and Domestic Violence team provides confidential and individualized treatment to people who have experienced a sexual assault or are victims of intimate partner abuse. The team consists of nursing for the immediate emergency care following an assault and the social work team who provide trauma counselling. Click here for more information about SADV services across the province.\nThe PICU provides a higher level of observation and care for patients who require more intensive care. The unit balances safety, low-stimulation and therapeutic approaches to care for individuals with a higher level of need.\nPatients may be discharged directly from PICU with follow-up in the community or in the Outpatient Program at the hospital. Some patients may require a longer length of stay in one of our Acute Inpatient Mental Health units.\nThese units provide 24-hour psychiatric assessment, short-term treatment and stabilization for adults who cannot be fully treated on an outpatient basis. Most admissions occur through the Emergency Department. The typical length of stay on an acute unit is approximately 7 to 10 days, and patients may be referred for a longer stay depending on their unique situation or may be discharged with follow-up plans either in the community or in the Outpatient Program at the hospital.\nThis unit provides more specialized care for patients requiring a longer period of care. The focus is on assisting individuals to improve functional abilities while their treatment plan is being further supported to promote continued wellness after discharge. Follow-up care is organized in the community or at the hospital, based on individual needs.\nThe Transition to Independent Unit is used to support a step-down approach to discharge that allows individuals to live in an independent environment that is a bridge between inpatient care and community. The focus during the brief Transition to Independent Living stay is on community reintegration and focused rehabilitation goals.\nThe CL Service provides mental health and addictions consultation to medical, surgical and critical care areas across Niagara Health. Individuals are provided with follow-up care, which may include discharge to the community with initiation of referrals or admission to inpatient mental health when medical issues are stabilized.\nNiagara Health offers outpatient mental health services at the St. Catharines Site, Greater Niagara General Site and Welland Site. Typically, outpatient services are for individuals needing short- and longer-term assessment, treatment and support. Referrals come from community health care providers, family physicians, the Emergency Department or from inpatient services.\nNiagara Health is expanding our ability to provide services from a distance. With the COVID pandemic, it has become a necessity. Virtual care will be a bigger part of our services moving forward.\nNiagara has an excellent system in place to provide virtual appointments. This includes, using Ontario Telemedicine Network to connect people through video with their healthcare providers, therapeutic groups and telephone supports - providing more prompt care and reducing the need for patients to travel.\nTo ensure you are referring to the most appropriate program that best suits the needs of the client, please review the outpatient services provided below:\nNiagara Health works with many mental health and addictions community partners...learn more.\nPort Colborne New Port Centre\nPhone: 905-378-4647 ext. 32542\nDouglas Memorial Site in Fort Erie\nPhone: 905-378-4647 ext. 50102\nSt. Catharines (264 Welland Avenue)"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:396e3028-f0a8-40ba-bd3c-bee87fa30fbb>"],"error":null}
{"question":"What happens to an asset's value when it becomes fully depreciated?","answer":"When an asset becomes fully depreciated, it has expended its full depreciation allowance and only its salvage value remains. The salvage value is the book value of an asset after all depreciation has been fully expensed.","context":["A fully depreciated asset has already expended its full depreciation allowance where only its salvage value remains. The salvage value of an asset is based on what a company expects to receive in exchange for selling or parting out the asset at the end of its useful life. Salvage value is the book value of an asset after all depreciation has been fully expensed. The cost approach uses the costs for materials and labor needed to repair an asset, minus any depreciation. Residual ValueResidual value is the estimated scrap value of an asset at the end of its lease or useful life, also known as the salvage value. It represents the amount of value the owner will obtain or expect to get eventually when the asset is disposed. The scrap value is the product’s raw materials that the manufacturer will sell off as scraps in cost accounting.\nNext, the annual depreciation can be calculated by subtracting the salvage value from the PP&E purchase price and dividing that amount by the useful life assumption. You will find the depreciation expense used for each period until the value of the asset declines to its salvage value. Depending on the depreciation method used, the value of the camera at the end of those 7 years is the salvage value of that asset. Alternatives under the new system are much more limited in number, the differences between them are greater, and the choices may be more critical. If you are in a higher tax bracket than you expect to be in later years, you might choose the accelerated rate option because a faster depreciation method will result in earlier tax savings.\nCost AccountingCost accounting is a defined stream of managerial accounting used for ascertaining the overall cost of production. It measures, records and analyzes both fixed and variable costs for this purpose. The company found out that the useful life of this equipment is ten years, and at the end of 10 years, the value of the equipment would be $10,000. Most of the Business are dependent on machines and their business is highly dependent on the productivity of the existing machines. Nature, quality, the effectiveness of their products is highly dependent on the way of production of the products. Thus, all the above criteria have to be fulfilled to make the product cost effective and efficient both for the consumer’s and manufacturer’s point of view.\nBuy slightly damaged cars from Salvage King online\nAnnual straight line depreciation for the refrigerator is $1,500 ($10,500 depreciable value ÷ seven-year useful life). Useful life is the number of years your business plans to keep an asset in service. It’s just an estimate since your business may be able to continue using an asset past its useful life without incident.\nWhat is salvage value also known as?\nScrap value is also known as residual value, salvage value, or break-up value. Scrap value is the estimated cost that a fixed asset can be sold for after factoring in full depreciation.\nSalvage value as a concept was eliminated , and a specific use0.ful life has been assigned to every depreciable asset. Consequently, the owner or his/her accountant can no longer select a useful life that either fits a particular individual’s situation or helps plan current and future depreciation deductions. Instead, each asset is assigned a specific useful life by placing it in one of a small number of asset classes.\nFor a more accelerated depreciation method see, for example, our Double Declining Balance Method Depreciation Calculator. The book value refers to how much a given asset is worth on the company’s accounting records (i.e., how much it’s been depreciated).\nMANAGING YOUR MONEY\nThe result will always be lower than the current market value of the vehicle. If the cost of repairs surpasses this amount, the car is written off as a loss. Keep in mind that the vehicle’s repaired trade-in worth and salvage value differ. If the vehicle has a five-year life, according to the IRS, and is purchased at USD 30000, then the straight-line reduction method reduces the car’s value by about USD 5000 per usage year.\n- The farmer would consequently place the new combine on the depreciation schedule at a basis of $64,000 and calculate depreciation on that amount, but would list the market value of the machine at $72,000.\n- By the end of the PP&E’s useful life, the ending balance should be equal to our $200k salvage value assumption – which our PP&E schedule below confirms.\n- The wrong estimation may result in the wrong depreciation expense.\n- ABC expects to then sell the asset for $10,000, which will eliminate the asset from ABC’s accounting records.\n- In brief, salvage value is the appraised worth of a commodity or manufactured object that has reached its lowest point of depreciation.\n- The depreciation formula helps businesses calculate the amount they can depreciate each year so as to spread the cost of an asset over its useful life.\nYou want your accounting records to reflect the true status of your business’s finances, so don’t wait until tax season to start thinking about depreciation. You might have designed the asset to have no value at the end of its useful life. Perhaps you hyper-customized a machine to the point where nobody would want it once you’re through with it. Even some intangible assets, such as patents, lose all worth once they expire. The Financial Accounting Standards Board recommends using “level one” inputs to find the fair value of an asset. In other words, the best place to find an asset’s market value is where similar goods are sold, or where you can get the best price for it.\nSample Full Depreciation Schedule\nThe IRS has defined like-kind to refer to the nature, character, use, or purpose of the property. Any truck or any other machine is like-kind property as to any other machinery. A trade of farmland for apartment buildings or timberland is also a like-kind exchange because all of these forms of property are held to generate income. However, a trade of machinery for land, livestock of one sex for those of the other sex, an old bull for a young bull, and other such trades https://www.bookstime.com/ are not like-kind. The IRS rules are strict and should be checked before handling the transaction either as a taxable or a nontaxable exchange. The buyer paid $3,000 in cash and traded a bull having a fair market value of $2,000 and a book value of $1,000. The farmer would consequently place the new combine on the depreciation schedule at a basis of $64,000 and calculate depreciation on that amount, but would list the market value of the machine at $72,000.\nWe plan to provide the entirety of our clients with as much data concerning any inquiries or worries that they may have. Check out salvage king eshop, our latest inventory, and get in touch with our experts to buy the used or salvage car you want. A vehicle purchased for USD might be worth USD 5000 after five or seven years of regular use.\nWhat is the Salvage Value of My Car\nIf the swather was purchased October 1, then $100,000, or 67%, of total asset purchases, were made during the last quarter and the mid-quarter convention applies. Like the straight line method, if the asset was held for a portion of the year, the amount of depreciation claimed must be adjusted.\nBut then you have to consider other wear and tear, everything from fender benders to major accidents, that can further reduce your vehicle’s value. Sometimes these vehicles are repaired and resold; other times, they give them to a local junkyard to part out and then scrapped. Other times, the salvage car applies more loosely to vehicles with extremely high mileage or significant mechanical issues beyond repair. The most common salvage car definition applies to cars with a salvage title, which denotes a vehicle that’s been in a significant accident and been written off or declared a total loss by insurers. Annual depreciation is equal to the cost of the asset, minus the salvage value, divided by the useful life of the asset. Stay updated on the latest products and services anytime, anywhere. The closing value for year one is calculated by subtracting the depreciation from the opening value of the asset.\nBenefits of using Depreciation Formula Calculator\nEnter the number of years of the asset’s useful life in the “Number of Years” box. The salvage value calculator uses this formula to minimize the loss. The seller agreed to accept $50,000 in cash, $4,000 in grain, and $2,000 in labor services for the tractor. Salvage King offers a vast selection of used and slightly damaged vehicles for immediate purchase.\n- Salvage King offers a vast selection of used and slightly damaged vehicles for immediate purchase.\n- Residual value refers to the estimated worth of an asset after the asset has fully depreciated.\n- This method also calculates depreciation expenses based on the depreciable amount.\n- To avoid taxation of that gain, the farmer could roll over that gain into the new combine.\n- Table 8 shows the Alternate MACRS percentages for several of the most often used recovery periods.\nNet salvage valuemeans the salvage value of the property retired after deducting the cost of removal. Net salvage value means the salvage value of the property retired, after deducting the cost of removal. Net salvage value means salvage value of property retired less the cost of removal. Net salvage value means the salvage value of property retired less the cost of removal. Net salvage valuemeans the sal- vage value of property retired less the cost of removal.\nHowever, each year following a during-the-year purchase must also be adjusted. The adjustment process will be illustrated in a later example, but for now just accept that the adjustment process is awkward—so awkward, in fact, that most practitioners avoid the SYD method entirely. Simply put, the time to make depreciation decisions has passed us by; those decisions must be made in the year of purchase. Under each method, be careful to not depreciate below the salvage value originally specified.\nSalvage your bookkeeping\nGenerally Accepted Accounting Principles require accrual accounting method businesses to depreciate, or slowly expense over time, fixed assets instead of booking one expense on the purchase date. Under most methods, you need to know an asset’s salvage value to calculate depreciation.\nHow do you calculate ROI for a project?\n- ROI = (Net Profit / Cost of Investment) x 100.\n- ROI = [(Financial Value – Project Cost) / Project Cost] x 100.\n- Expected Revenues = 1,000 x $3 = $3,000.\n- Net Profit = $3,000 – $2,100 = $900.\n- ROI = ($900 / $2,100) x 100 = 42.9%\n- Actual Revenues = 1,000 x $2.25 = $2,250.\nThe straight line method of depreciation is the simplest method of depreciation. Using this method, the cost of a tangible asset is expensed by equal amounts each period over its useful life. The idea is that the value of the assets declines at a constant rate over its useful life. This means that even if you have bought an asset second-hand, machinery or computer hardware, for example, your purchase price is the value at the time of acquisition of the asset. When setting up depreciation, this is the amount needed to begin applying the depreciation method. At this time, it seems best to use straight line recovery on buildings and other real property. The recapture of all depreciation as ordinary income when sold makes using the accelerated rate much less attractive.\nThere are three ways of determining the dollar amount of salvage value. In the first, we need to estimate the number of years an asset will be usable. Then we look at the sale price of similar assets of the same age on the market currently. Salvage value (also often referred to as ‘scrap value’ or ‘residual value’) is the value of an asset at the end of its useful life. Say you’ve estimated your 2020 Hyundai Elantra to have a five-year useful life, the standard for cars.\nTables of percentages are provided to compute annual depreciation amounts (see Table 3 for 3-, 5-, and 10-year percentages and Table 4 for 15-, 18-, and 19-year percentages). The accelerated rates shown in the tables are spelled out in the law, so all you do is multiply them by your original basis.\nAdam Luehrs is a writer during the day and a voracious reader at night. He focuses mostly on finance writing and has a passion for real estate, credit card deals, and investing.\nThe cost and installation of the machinery of new come from the bank balance of the company. To calculate straight line depreciation for an asset, you need the asset’s purchase price, salvage value, and useful life. The salvage value is the amount the asset is worth at the end of its useful life. Whereas the depreciable base is the purchase price minus the salvage value. Depreciation continues until the asset value declines to its salvage value. It is worth remembering that companies owning several costly fixed assets such as vehicles, medical equipment and other heavy machinery may consider buying residual value insurance. This insurance helps to minimise asset-value risk by assuring the post-useful-life value of assets enjoying proper maintenance.\nDetermining the Salvage Value of an Asset\nIf the item is traded for another, it leaves a smaller depreciable basis on the replacement. If the item is sold and is depreciable personal property, the gain on the sale is recaptured as ordinary income to the extent of depreciation allowed or allowable; the rest is capital gain.\nNet salvage valuemeans the salvage value of property retired less the cost of removal. Calculating residual value requires two figures namely, estimated salvage value and cost of asset disposal. Residual value equals the estimated salvage value minus the cost How to Calculate Salvage Value of disposing of the asset. Once the company no longer uses the asset or the revenue generated from it has become rather unpredictable. Residual value is the projected value of a fixed asset when it’s no longer useful or after its lease term has expired.\nOther choices are to select straight line depreciation over the alternate recovery period. The same election must be made on all items in a given class purchased during the year. A list of agricultural assets and their ACRS, MACRS, and Alternate MACRS lives are presented in Table 5. Like ACRS, if you bought a tractor and installed a sprinkler irrigation system in the same year, you have to use the same recovery period and method for both. When 3-, 5-, or 7-year class assets are disposed of, any gain, to the extent of prior depreciation taken, is to be recaptured as ordinary income.\nThe impact of the salvage value assumption on the annual depreciation of the asset is as follows. If the salvage value is assumed to be zero, then the depreciation expense each year will be higher, and the tax benefits from depreciation will be fully maximized. The salvage value is the remaining value of a fixed asset at the end of its useful life. TheSalvage Value refers to the residual value of an asset at the end of its useful life assumption, after accounting for total depreciation."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:a32a0423-dd3a-4671-959e-424d870a069b>"],"error":null}
{"question":"Which activity takes longer: the Siem Reap countryside bike tour or the Mekong River cruise from My Tho to Siem Reap?","answer":"The Mekong River cruise from My Tho to Siem Reap is much longer, lasting 8 days, while the Siem Reap countryside bike tour is only approximately 5 hours long.","context":["5 hours (Approx.)\nHotel pickup offered\nOffered in: English\nGood for avoiding crowds\nSpend half a day exploring the rural countryside around Siem Reap with this bicycle tour. After being fitted with a bicycle and helmet, your English-speaking guide will lead you through back roads, across rice fields and farms, and into small local villages where you'll learn about how locals live, first hand.\n- Get off the beaten path and explore the Cambodian countryside\n- Ride through villages and rice fields on a mountain bike\n- Round-trip transfers from your Siem Reap hotel are provided\n- Snacks and drinking water are included\nSaved to wishlist!\nHotel pick up & Drop Off\nProfessional local English speaking guide\nMountain bike & Helmet\nLocal snacks and fruits\nDeparture & Return\nDeparture PointTraveler pickup is offered\nYou will pick up at 7:20 AM for a prompt departure at 7:30 AM. Please provide your hotel name while booking for the pick-up.\n- Confirmation will be received at time of booking\n- Not wheelchair accessible\n- Infant seats available\n- It is recommended to bring sunscreen, sunglasses, comfortable clothing and shoes for cycling\n- Kid sized bikes, tag-alongs and child seats are available\n- Most travelers can participate\n- Distance: (15km - 20km)\n- This tour/activity will have a maximum of 8 travelers\n- OPERATED BY Siem Reaper Travel\nYou can cancel up to 24 hours in advance of the experience for a full refund.\n- For a full refund, you must cancel at least 24 hours before the experience’s start time.\n- If you cancel less than 24 hours before the experience’s start time, the amount you paid will not be refunded.\n- Any changes made less than 24 hours before the experience’s start time will not be accepted.\n- Cut-off times are based on the experience’s local time.\nLearn more about cancellations.\nFrequently Asked Questions\nThe answers provided below are based on answers previously given by the tour provider to customers’ questions.\nWhat is the policy on face masks and attendee health during Bike the Siem Reap Countryside?\nA:The policies on face masks and attendee health are:\n- Face masks required for travelers in public areas\n- Face masks required for guides in public areas\n- Face masks provided for travelers\n- Temperature checks for travelers upon arrival\nWhat is the policy on sanitization during Bike the Siem Reap Countryside?\nA:The policies on sanitization are:\n- Hand sanitizer available to travelers and staff\n- Regularly sanitized high-traffic areas\n- Gear/equipment sanitized between use\n- Transportation vehicles regularly sanitized\nWhat is the social distancing policy during Bike the Siem Reap Countryside?\nA:The policies on social distancing are:\n- Social distancing enforced throughout experience\n- Contactless payments for gratuities and add-ons\nWhat measures are being taken to ensure staff health & safety during Bike the Siem Reap Countryside?\nA:The policies on staff health & safety are:\n- Guides required to regularly wash hands\n- Regular temperature checks for staff\n- Paid stay-at-home policy for staff with symptoms\nCompare Similar Experiences\nBike the Siem Reap Countryside\nExplore Similar Things to Do\nMountain Bike ToursHalf-day ToursWalking & Biking ToursCultural & Theme ToursOutdoor ActivitiesLuxury & Special OccasionsTours & SightseeingSiem ReapMountain Bike Tours - Angkor WatHalf-day Tours - Angkor WatMountain Bike Tours - Phnom PenhHalf-day Tours - Phnom PenhMountain Bike Tours - Ko ChangHalf-day Tours - Ko ChangCategory\nAngkor Village Apsara Theatre Tours & TicketsKampong Phluk Tours & TicketsSiem Reap Old Market (Phsar Chas) Tours & TicketsSiem Reap Art Center Tours & TicketsCambodia Landmine Museum Tours & TicketsPhnom Kulen National Park Tours & TicketsWar Museum Cambodia Tours & TicketsAngkor Night Market Tours & TicketsWat Preah Prom Rath Tours & TicketsAngkor National Museum Tours & Tickets","DAY 1: SAI GON - MY THO (L/D)\nBid farewell to the hustle and bustle of Saigon you will go to My Tho for embarkation. My Tho is 70km South of Saigon and located on the upper Mekong river in Vietnam. Enjoy a cocktail reception at the Funnel Bar and Lounge followed by a Welcome Dinner on the Sun Deck. Drop anchor just outside of My Tho for the night. Overnight on the Jahan Cruise\nDAY 2: CAI BE - SA DEC - CHAU DOC (B/L/D)\nThe day – in fact everyday – starts with an invigorating Tai Chi lesson on the sun deck, just as the sun begins its gentle glow. Coffee and tea awaits the early bird. The day’s tour starts with a visit to Cai Be’s boisterous and colorful floating market on board a traditional sampan (a flat bottomed traditional Vietnamese wooden boat). During the languid ride, discover breathtaking landscapes along the river and quaff the rustic life on the Mekong Delta. The cruise will halt intermittently to allow us to observe how rice paste, rice cookies and coconut candies are made, along with the Longan fruit drying process. After touring the canals, we head to an ancient house surrounded by fruit orchards before heading to the Ship for lunch. In the afternoon, we cruise to Sa Dec.\nUpon arrival, we embark a traditional sampan to tour Binh Thanh Island and its manmade canals. Here, every villager is involved in the process of growing and processing water hyacinths into natural fibre floor mats and rattan baskets. We then board the Ship for a tranquil evening cruise towards Chau Doc. Including breakfast, lunch and dinner. The ship will moor midstream overnight near Chau Doc.\nOvernight on the Jahan Cruise.\nDAY 3: CHAU DOC - BORDER CROSSING (B/L/D)\nWelcome to the fish sanctuary of Vietnam. Begin your day with a hearty breakfast at the Dining Hall or in the quiet sanctuary of your balcony. Brace yourself for a short excursion in Chau Doc, a pleasant town near the Cambodian border with sizeable Chinese, Kinh and Khmer communities. We will also visit the colorful local market near an ancient temple. This is followed by a trip to the quay, where we will be feted to a boat excursion to the floating villages and rustic catfish farms where we will learn exciting facts about the breeding of Basa fish in the Mekong Delta. Then, it is back to the ship for a lunch.\nBy mid-afternoon, the cruise liner will make its way to the border for Phnom Penh, the capital city of Cambodia. It is an opportune time to relax on board, sharpen your culinary skills with our hands-on cooking classes or tour the ship with our impeccable Cruise Director. Or, better yet, do all three. “The Highway Blues” beckons as we float into a state of mild bliss caused by the hypnotic murmur of the ship’s engine. After hours of cruising, we will feel the rush of entering into a new country and to embrace new adventures.\nIncluding breakfast, lunch and dinner. Overnight midstream near Phnom Penh.\nNote: Due to high water in the Mekong Delta between July and December the following program applies: This morning, you will start your excursion on a local boat (sampan) and discover the border town of Tan Chau. Firstly, the journey takes you to the “evergreen” islands where you will be thrilled to learn the life of the locals.\nThe inhabitants of these islands live on houses poised on high stilts – a centuries-old tradition in this part of the Mekong Delta. We stop to visit a fish farm located in the Tan Chau canal – this farm consists of floating houses and the villagers make their living by cultivating fish. You continue by boat to Tan Chau pier, disembark here and stroll around this colorful local market with lots of regional products. You return to the cruise Ship after your market visit.\nOvernight on the Jahan Cruise.\nDAY 4: PHNOMPENH SIGHTSEEING (B/L/D)\nPhnom Penh was once the Paris of the East. Despite its rapid development, it has retained a lot of its rustic charm and elegance. After breakfast, we will be captivated by a comprehensive lecture on Cambodia’s modern history. This will be followed by a city tour of this dynamic, historical capital city of Cambodia which has managed to preserve huge slices of its French colonial allure. Sightseeing in the morning will include visiting the dazzling white and gold edifice of the Royal Palace and the Silver Pagoda, and witnessing exceptionally beautiful Khmer craft at the National Museum.\nAfter lunch at a local restaurant, we will visit the Genocide Museum and the Killing Field in town. Be prepared to be heart wrenched by this stark reminder of the genocide that transpired during Cambodia’s Khmer Rouge regime.\nWe go back to the Ship after our museum visit. Take a rest on your cabin balcony or the lounge and get prepared for an exciting evening event. The evening starts with a variety of cocktails served on the sun deck. Take a seat and enjoy the following Apsara dance performance. Experience the brilliance and richness of this culture, the soothing sounds of traditional music, the calming scent of incense sticks and a barbeque on the sun deck. It is indeed a truly marvellous experience, dining in the open with the stars and the moon as our companions.\nIncluding breakfast, lunch and dinner. Overnight in Phnom Penh. Overnight on the Jahan Cruise.\nDAY 5: PHNOMPENH - KAMPONG CHAM (B/L/D)\nWe will depart before the break of dawn from Phnom Penh with a stop at the little known Chong Koh silk weaving village. We will then return to the Ship and will cruise past the Mekong’s tranquil villages, exhilirating river life and sun-hardened fishermen on their shift boats.\nWe will tour the rural Angkor Ban village where time stood still. We are transported back a hundred years into an era unsullified by modernity. The houses here are truly rustic, made by hand and without any concrete. But most of all, we feel overwhelmed by the unbridled warmth of the villagers.\nTravelers can take part in the daily activities of the villagers to experience village life first hand. Then, we are back to the Ship to cruise towards Kampong Cham. Overnight on the Jahan Cruise.\nDAY 6: KAMPONG CHAM - WAT HANCHEY (B/L/D)\nWe will wake up in the small hours of the morning, truly energized to embark on a land journey to the pre-Angkorian temple of Wat Hanchey. Travelers either take a motor-taxi or climb 291 steps to the top. Located at the top of a hill overlooking the river, it offers one of the most breath-taking views in Cambodia. Built in the 8th century, this ancient structure, together with a bigger and newer addition underscores the superb architecture of the Chenla Empire which predates the mighty Angkor temple complex. We even get to engage in light banter with some of the monks in the area. We will return to the Ship to witness a blessing ceremony performed by orange-clad local monks.\nWhen the tide allows, we will enjoy a surprise sojourn in our zodiacs (inflatable boats) to the nearby bank to swim in the teacoloured Mekong River, an experience that will, no doubt, dominate dinner conversations in the weeks to come. We will then cruise towards Kampong Cham to visit Wat Nokor, a wat built within the ruins of an ancient temple which holds a timeless story. On the way back to the Ship, we will visit an orphanage. Along the way, we will have to maneuver a 2-kilometer rickety bamboo bridge. Our sense of balance lies in the balance! Overnight cruise to Kampong Chhnang.\nDAY 7: KAMPONG CHAM - KAMPONG CHHNANG - TONLE SAP (B/L/D)\nBefore the sun peeks over the horizon, the cruiser sets sail towards Kampong Chhnang on the Tonle River – an amazingly narrow river – which glides through leafy swaths of the Southeast Asian jungle. We are greeted by cacophonous children who waive enthusiastically early in the morning when they bring their livestock for a bath in the meandering Tonle River. Then we approach Kampong Chhnang, Cambodia’s “waterworld” where everything is on stilts balanced on water rich soil– it’s an awesome sight whilst practising Tai Chi on the deck.\nHere we take a motorboat excursion to the landing, followed by a short bus trip to see the Khmer-styled pottery at the Aundaung Russey village. Then, it is back to the river for an adventurous motorboat excursion to the wetlands, floating houses and fish farms in the region.\nWe then return to the Ship for lunch whilst cruising the narrow and meandering Tonle River. Your floating sanctuary will then cruise gently, in the afternoon, along the narrow and meandering Tonle River. In the evening, a sumptuous spread will be served. And for the big surprise – bring along your dance shoes and learn to jiggle the Cambodian way with our staff doubling up as dance instructors. (If you can’t dance, do not fret. Neither can our staff) Including breakfast, lunch and dinner. Overnight in Kampong Chhnang.\nDAY 8: TONLE SAP - SIEM REAP (B)\nThis great lake dominates Cambodia and is the largest freshwater lake in Asia with abundant birdlife. This lake is so wide that one can not see its shores from the middle of the lake.\nEnjoy the stillness of the water and the quiet tranquility on the way to Siem Reap. The water level varies a great deal according to the time of the year and the amount of rainfall recorded. Thus, minor changes to the itinerary may occur during different seasons.\nOur Heritage Line cruise liner comes to an end as we reach this city of the fabled Angkor temple complex. We bid farewell to the cruise as our journey comes to a close. But the memories and the friendships made on board linger on for a lot longer. We will cross the Tonle Lake at dawn and at 9.00h, we will disembark at the Siem Reap Port for onward coach transfer to the drop off point in town.\n(Per person in US$)\n(Per person in US$)\n|Superor State Room\n|Deluxe State Room\n|Signature Suites Room"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:35bf0032-9c76-449c-a4d3-b568cc68a6db>","<urn:uuid:f2bd092f-672e-477c-97d4-71b6f58e2834>"],"error":null}
{"question":"What are the main safety features of both self-retracting lifelines (SRLs) and wind energy harnesses for fall protection?","answer":"Both SRLs and wind energy harnesses offer crucial safety features. SRLs have an energy-absorbing brake that activates during a fall, reducing fall arrest forces and distances to safe levels. They can be made of galvanized/stainless steel cable or nylon webbing, with lifelines ranging from 6 to 175 feet. Wind energy harnesses feature built-in radioholsters, replaceable lumbar protectors, protective hip pads, and Tech-Lite Aluminum D-rings for security and comfort. They also include Force 2 Shockwave 100% Tie-Off Lanyards with elastic webbing and 1600 kg hooks for additional safety.","context":["- OIL & GAS\nFall protection training is crucial for any worker who performs work at height. Without the proper training, workers may not realize the severe consequences of a fall - serious injury or even death. Training consists of both classroom learning on topics such as industry standards, as well as hands-on training about how to use fall protection equipment, among other topics.\nIt is the company’s obligation to make sure all workers are educated in how to properly use equipment, including devices that are sometimes overlooked, such as a self-retracting lifeline (SRL).\nSRLs are becoming more common on job sites because they reduce fall distance and fall clearance requirements. The devices have also become smaller, lighter and more economical, making them an increasingly popular choice over shock- absorbing lanyards. Based on the growing popularity of SRLs, it is critical that fall protection training courses teach workers how to use these devices correctly.\nBackground on SRLs\nA self-retracting lifeline is a fall arrest device incorporating a lifeline that extends and retracts as a worker moves away from and toward the device. In the event of a fall, an energy-absorbing brake activates, stopping the fall while reducing fall arrest forces and fall distances to safe levels.\nThe lifeline is made of either galvanized or stainless steel cable or nylon webbing. SRLs are typically used when a great deal of mobility is needed on a job site. The amount of mobility is limited only by the length of the SRL’s lifeline, which can range from six feet to 175 feet or greater. Self-retracting lifelines are used extensively in the oil and gas industry, but the devices are also common in construction, general industry, transportation, utilities and wind energy.\nSeveral types of SRLs are common on job sites; selecting the right type depends upon the work environment and the type of work to be performed.\nFor example, sealed SRLs completely seal all dynamic components such as the motor spring and brake system inside the housing to protect the mechanisms from oil, grease, dirt and saltwater, making them ideal for use in dirty or corrosive environments.\nCertain activities such as welding require the SRL’s lifeline to be made of steel material or Kevlar, versus nylon, to protect it from heat and flames. Leading edge SRLs are available with lifelines that are resistant to damage from sharp, abrasive leading edges found on many construction sites.\nCompared to shock-absorbing lanyards, SRLs reduce the fall distance required for deceleration and fall arrest, making them preferred equipment for sites with low clearances or where work hazards cannot be bypassed or isolated.\nMany fall protection training courses touch only briefly on SRLs without fully explaining what users need to know. Training for authorized persons should include training on how to safely use a self-retracting lifeline.\nSRL training tips\nMany self-retracting lifeline users are unaware of how to use SRLs on varying surfaces, and also how to inspect, maintain and store the device. It is vital for any training program to include the following considerations:\nSRLs on walking surfaces/leading edges: Not all SRLs are designed for walking surfaces/leading edges; some are designed for ladder climbing and other purposes.\nTrainees should be taught that the device they are using must be appropriate for the surface and task. SRLs used on walking surfaces should have a reinforced lifeline with an energy absorber at the end nearest to the user. If one isn’t built in, a separate device should be attached inline between the dorsal D-ring of the user’s harness and the snap hook of the lifeline. This energy absorber is in addition to the energy absorber built into the device, and is necessary for situations in which the user goes over a sharp edge. In this situation, the cable may not slide, potentially damaging the cable and exerting high arresting forces on the user. The energy absorber on the user’s back prevents this.\nAlso, when using an SRL on a walking surface, the SRL should be positioned on its spine, rather than flat. This ensures that the cable or webbing coils properly. There are special brackets made for this purpose.\nSRLs and lateral movement: Trainees should be taught to mount the SRL to a central anchorage point directly overhead when moving side-to-side with a self-retracting lifeline, and to limit their movement away from the anchorage point. The further the worker travels from the anchorage point, the more it can counteract the benefits of the SRL in that a fall may result in a swing fall - a pendulum-like motion resulting from a fall that occurs in a position located horizontally away from the anchorage.\nUsers should refer to clearance charts that are provided with SRLs, and be able to assess the job site hazards to determine how far they may safely move from the anchorage point.\nSRLs on horizontal lifelines: A self-retracting lifeline significantly reduces the amount of fall distance when used with a horizontal lifeline. Horizontal lifelines are commonly used in the construction industry and in other situations when there are no overhead anchorage points.\nLike lateral movement, there is an increased danger of a swing fall when using an SRL with a horizontal lifeline, particularly if the SRL is not fitted with a device that helps it track along the horizontal lifeline. Again, users should be taught to apply the clearance charts provided with SRLs and be able to assess the hazards to determine safe movement from the anchorage point.\nSRL maintenance and storage: Maintaining self-retracting lifelines and storing them appropriately greatly extends the products’ longevity. Trainees should be taught how to inspect the SRL prior to use. This involves examining the device, both visually and manually, and making sure it engages correctly by performing a lock test (pulling the cable out fast enough to lock the system).\nMost self-retracting lifelines on the market today have impact indicators that show if a unit has been involved in a fall. If the SRL has been impacted, it should be taken out of service and sent to an authorized service center for repair. In the past, SRLs had to be sent to the manufacturer for recertification on an annual basis, but this is no longer the case.\nIf workers properly care for SRLs, they should last until they either fail an inspection or are impacted in a fall. Trainees should also be taught how to properly store SRLs, in a cool, dry place where they will be protected from physical damage and corrosive elements.\nOn the horizon\nSelf-retracting lifeline technology continues to evolve and improve, and training must take these new features into account. There are now SRLs on the market with built-in rescue capabilities that can automatically lower the worker to the ground or next level in the event of a fall. In addition, the newest SRLs feature field-serviceable lifelines.\nProper fall protection training is crucial for all workers working at height, and an integral component of that training is how to select, inspect, use, maintain and store fall protection equipment. After receiving initial fall protection training, workers should participate in regular refresher training sessions, particularly when new equipment is introduced to the job site. By mandating fall protection training as part of a comprehensive fall protection program, a company can help protect against the dire consequences of a fall.","The top of a wind turbine is no place to second-guess your fall protection equipment. Climbing hundreds of feet in to the air and working with little more than the horizon as your surroundings, you need reliable equipment that provides superior, all-day comfort. Trust the DBI-SALA and Protecta brands of fall protection equipment. After all, when nothing separates you from the ground 300 feet below, the only thing that should be on your mind is the task at hand.\nExoFit NEX Wind Energy Harnesses\n• New ExoFit NEX™ Wind Energy Harnesses, Lanyards and Accessories meet CE/ OSHA/ ANSI/ CSA requirements for global compliance.\n• ExoFit NEX™ Wind Energy Harnesses feature built-in radioholsters and replaceable lumbar protectors.\n• Protective hip pad prevents wear while climbing and ascending the tower.\n• Tech-Lite™ Aluminum D-rings offer a higher level of security and comfort.\n• Force 2™ Shockwave™ 100% Tie-Off Lanyards feature elastic webbing, 1600 kg hooks on leg ends and Tech-Lite™ rescue D-rings.\nFORCE2 Shockwave Wind Energy Lanyard\nThe FORCE2™ Shockwave Wind Energy Lanyard is a 100% tie-off lanyard with elastic webbing, 3,600 lb (1,632 kg) hooks on leg ends, Tech-Lite™ rescue D-rings & a nylon shock pack with a PVC resist cover to protect against abrasion while climbing and descending.\nOSHA/ANSI Compliant - North America\nCE EN355 Compliant - Europe, Middle East, Africa.\n* Suspension Trauma Safety Strap helps prevent the effects of suspension trauma after a fall\n* Allows worker who is suspended to stand up in their harness to relieve pressure\n* A safety accessory that can be attached to most harness brands/styles right in the field\n* Installation to your harness is fast and efficient - just choke off to strap and it's ready\n* Extremely compact and lightweight design stays out of the worker's way\n* Deployment and operation is fool-proof, simply unzip, and hook straps together\n* Continuous loop design allows for either one or two-foot suspension relief without pinching\nWhat is Suspension Trauma?\nThe Suspension Trauma Safety Strap was designed to help a worker overcome the potential negative health impacts of suspension trauma (otherwise known as Orthostatic Intolerance). This condition can occur when a person remains suspended at height (after a fall) for even short periods of time. Suspension in a harness may cause blood to pool in the veins of the legs which can result in unconsciousness; if not rescued promptly, serious injury or death may occur. OSHA requires that employers provide for prompt rescue of employees in the event of a fall. The Suspension Trauma Safety Strap is designed to prevent suspension trauma while a worker is awaiting rescue.\nSafety Strap Helps Prevent Suspension Trauma.\nThe Suspension Trauma Safety Strap allows the worker, who is suspended, to stand up in their harness and to relieve the pressure being applied to the arteries and veins around the top of the legs. The continuous loop design allows both sides of the harness to relieve the pressure being applied to the legs. The strap accommodates either having one foot or both feet in the loop at a time - it will relieve the pressure to both sides with just one foot in allowing for added movement of the legs. The strap allows for increased comfort, balance and improved circulation in the legs while suspended and waiting for rescue.\nFast and Easy Installation, Operation.\nThe two packs are compact, lightweight and easily installed to most harness sizes, styles and brands. To attach, simply choke the pack around the harness web at the hip. After a fall, the packs can quickly be unzipped, deploying the web loop and hook straps. These are easily connected and adjusted to provide a strap for the foot to conveniently step into. This allows the worker to stand in their harness and relieve the pressure on the legs while waiting for rescue."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:77d9e1c7-59f2-483b-8800-7f05d8545aae>","<urn:uuid:615a2577-d548-4eee-a4dd-bca31d559d31>"],"error":null}
{"question":"What are the main responsibilities of EMTs during search operations, and what are the salary ranges for EMTs in different regions of Mississippi?","answer":"During search operations, EMTs support police and fire departments, working under an Incident Command System for accountability. They operate from a command post that establishes search teams, defines search areas, and tracks personnel in the field. They may work alongside search dogs and must be prepared for various scenarios, from finding intoxicated individuals to potential victims. As for salaries in Mississippi, they vary by region: Gulfport-Biloxi pays $13.05-$28.93 hourly, Memphis area offers $12.98-$29.98, Northwest Mississippi ranges from $8.58-$35.77, and Jackson provides $10.00-$26.02 hourly.","context":["EMS drill stresses greater accountability\nJEMS volunteers completed a search and rescue drill at Fort Wetherill. Captain and field trainer Christopher Reilly led the drill, incorporating the emergency staff's newly adopted Incident Command System. The purpose of the drills, held four times a year, is to train EMS personnel how to support the police and fire departments effectively, Reilly explained.\nICS is a standardized on-scene, all-hazard incident management concept, according to the Occupational Safety and Health Organization. ICS was developed over 30 years ago as a solution to communication and organization problems that stymied the management of rapidly moving wildfi res. Since then, ICS has become a required practice for all hazardous materials responses nationally and is used for emergency operations in most states.\nJEMS Chief Rick Hodges assigned a safety officer and scribe to assist at the command post set up in the parking lot. He displayed a unified command system structure on the side of one of the three emergency engines at the scene. With about 20 volunteers gathered around the post, Hodges shared directions for the mock search of a missing person. \"If a real call comes in, we need to know who is assigned to what,\" Hodges told the group.\nScribe Linda Maclean began filling in the diagram with help from safety officer Nancy Bye, who is also a JEMS captain. They both asked the training offi cer about logistics. \"This is the first time we've seen this system,\" Bye said as she sketched the search area on a white board.\nThe volunteers waited and listened patiently while the chief and the trainer coordinated the search plan. \"It can be annoying to move slow with people milling around, waiting for the command post. But this is important,\" Reilly emphasized. \"Without the command post, it's a free for all, and no one knows where anyone else is.\" He held up a board with staff identification tags hooked on it. \"This is how we account for everyone in the field.\"\nThe command center established search teams, the area to be covered, and a list of supplies each team should carry. Hodges emphasized to the search groups that the missing man could be drunk in the bushes, a suicide or homicide victim, or miles away. \"He could have hitchhiked home,\" Hodges said, \"Hopefully that's the way a search ends, happy.\"\nSearch and rescue dogs from South County and Providence County joined the emergency drills, adding more perspective to the mock situation. North Kingstown firefighter Michael LeClair explained the precision of the animals' work, and the canines wagged their tails furiously in anticipation of a hunt. \"I'm just along for the walk,\" LeClair smiled. \"They work hard for their reward.\" The working dog's prize could be anything from a tennis ball to a stuffed toy. \"Sometimes, finding the person is reward enough.\"\nFinding the victim was certainly reward enough for the human volunteers. With dusk setting in, volunteers were happy when they heard over the EMS radio that the missing person was located.\nThe local EMS is always looking for good volunteers who want to help in the community, Hodges said. About 50 people are active in the emergency service, and about one-fifth of them are non-residents. \"It sounds like a lot, but we get tight during weekdays,\" he admitted. Many of the volunteers have full-time work off the island and are not available when a call comes in.\nJEMS' greatest need is for drivers, an easier place to start since medical training is not required. \"You just need to be a good driver,\" Hodges said. \"We teach you how to drive the emergency vehicles.\"\nFor information about free training to become a Jamestown emergency medical responder, send an e-mail to jems@ jamestownambulance. necoxmail, or call 423-7276.","If you are interested in the steps you will need to take to become an EMT in Mississippi, we have you covered with our latest resource below! All emergency medical technicians are regulated by the Mississippi State Department of Health and the steps laid out for you below will get you on track for certification!\nWhat does a Mississippi EMT do?\nAlthough the broad function of EMTs and Paramedics is to attend to the sick and seriously injured, there is much more to their duties. Here you’ll see some more-comprehensive descriptions of the EMT’s role.\nWhat is the process to become an EMT in Mississippi?\nEMTs and Paramedics in Mississippi will need to complete a post-secondary training course. Mississippi mandates that all EMTs and Paramedics in the state be professionally certified. Although every state could have differing requirements for EMT certification, the five steps below are very common.\nStep 1: Earn your high school diploma or G.E.D.\nWhen looking for an Emergency Medical Technician certification class, you can expect to also find that all programs require students to hold a G.E.D. or diploma before registration. So, regardless of your professional goals, make sure that you complete this step.\nStep 2: Graduate from an accredited EMT school in MS\nIf you wish to be eligible to test for certification, you must first complete a MS Dept. of Health accredited EMT program. Students must have finished the training in the previous 24 months and provide verification of successful program completion on the National Registry website (more info below). Junior colleges, trade schools, hospitals and universities all feature Emergency Medical Technician training programs. You can also take them at fire and police training academies.\nMississippi Department of Health Approved EMT Programs in MS\nBelow are all the EMT programs in Mississippi that are approved by the Mississippi Department of Health.\nMeridian Community College 910 Hwy 19 N Meridian MS 39307-5801 601-483-8241\nCoahoma Community College 3240 Friars Point Rd Clarksdale MS 38614 662-627-2571\nNorthwest Mississippi Community College 4975 Hwy 51 N Senatobia MS 38668-1714 662-562-3200\nEast Mississippi Community College 1512 Kemper Street Scooba MS 39358-0158 662-476-5000\nEast Central Community College 15738 Highway 15 Decatur MS 39327-0129 601-635-2111\nItawamba Community College 602 W Hill Street Fulton MS 38843-1099 601-862-8000\nJones County Junior College 900 South Court Street Ellisville MS 39437 601-477-4000\nHolmes Community College #1 Hill St Goodman MS 39079 662-472-2312\nHinds Community College 608 Hinds Boulevard Raymond MS 39154 601-857-5261\nCopiah-Lincoln Community College 1001 Co-Lin Lane Wesson MS 39191 601-643-5101\nMississippi Gulf Coast Community College 51 Main Street Perkinston MS 39573 866-735-1122\nMississippi Delta Community College PO Box 668 Moorhead MS 38761 662-246-6503\nJackson County EMS District 2204 Old Mobile Highway Pascagoula MS 39567 228-627-8727\nGulf Coast EMS District 2020 Intraplex Parkway Hattiesburg MS 39401 601-554-4947\nSouthwest Community College College Drive Summit MS 39666 601-276-3876\nSoutheast Trauma Care Region 207 South 28th Avenue Hattiesburg MS 39401 601-264-0342\nHow long is EMT School in MS?\nDepending on the particular training program in Mississippi, EMT training may often be finished in between two and six months.\nStep 3: Have a current CPR-BLS certificate for “Healthcare Provider” or equivalent\nStudents should have a CPR-BLS credential for “Healthcare Providers” for Emergency Medical Technician candidacy. The American Heart Association sets the standards for all Basic Life Support (BLS) and CPR programs. On the list of largest, and most-prominent providers of certification training is the American Red Cross.\nStep 4: Challenge the National Registry’s EMT Cognitive and Psychomotor exams\nThe Emergency Medical Technicians certification examination is given in two sections: the National Registry Cognitive (knowledge) and Psychomotor (skills) examinations. The Cognitive Exam is administered in an online, multiple-choice style, while the Psychomotor Exam is offered at a physical venue under the observation of an evaluator. We discuss the two test sections in depth below.\nStep 5: Have your name included on the National EMT Registry\nThe last step in becoming an Emergency Medical Technician is getting your name placed on the National EMT Registry within 30 days of finishing your coursework. A fee of $80 has to be paid so that you can get your name included on the registry. After becoming listed, you’ll be officially acknowledged as an Emergency Medical Technician.\nWhat is the median EMT salary in Mississippi?\nThe median pay for EMTs in Mississippi, along with extra regional statistics for Mississippi, is detailed in the following table. All data is from the website of bls.gov.\n|Gulfport-Biloxi, MS MSA||Hourly||$13.05||$20.87||$28.93|\n|Memphis, TN-MS-AR MSA||Hourly||$12.98||$18.84||$29.98|\n|Northeast Mississippi BOS||Hourly||$10.96||$17.77||$24.43|\n|Southwest Mississippi BOS||Hourly||$9.76||$16.20||$25.72|\n|Northwest Mississippi BOS||Hourly||$8.58||$15.04||$35.77|\n|Jackson, MS MSA||Hourly||$10.00||$14.88||$26.02|\n|Southeast Mississippi BOS||Hourly||$8.85||$14.50||$27.74|\nWhat is the job outlook for Emergency Medical Technicians and Paramedics in Mississippi?\nWith the population increasing and the Baby Boomers hitting retirement age in such large numbers, the healthcare industry in general is experiencing record growth in Mississippi. The Bureau of Labor Statistics projects a 18% rise in EMT and Paramedic jobs also over the subsequent decade."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:9032c976-e2b1-45f5-9532-9d6144f4eaf3>","<urn:uuid:d87868d1-8b5d-450b-80d0-f448635b3294>"],"error":null}
{"question":"How do modern K-12 schools address both social equity and digital safety concerns in their educational programs?","answer":"Modern K-12 schools are working to address both social equity and digital safety through comprehensive approaches. From an equity perspective, schools are implementing decolonial frameworks and critical methodologies to transform classrooms and liberate learners, examining connections between education and contemporary world issues. Meanwhile, to ensure digital safety, schools are implementing extensive cybersecurity measures to protect student data and systems. These include staff cybersecurity training, regular data backups, phishing defense mechanisms, and mobile device management. The government is supporting these dual efforts through initiatives like the $200 million FCC pilot program to strengthen cyber defenses in K-12 schools, while also maintaining focus on social justice and transformation in education policy.","context":["Meredith Madden's research focuses on decolonial studies in education and social inequalities in education.\nYou will tailor your own education coursework with the support of faculty, and your studies will be fully integrated with Hamilton's innovative liberal arts curriculum. Local schools will become your classroom because you will put in at least 75 hours of fieldwork there.\nAbout the Minor\nOne of the most revealing indicators of a college’s educational quality is the number of educators it produces: It's no coincidence that education consistently ranks as one of the top fields in which Hamilton graduates begin their careers. Many teach in private schools or work in volunteer teaching corps; others advance to graduate study in education.\nHamilton has just broadened my options entirely. I came in thinking law school and that’s it. And taking education courses says: ‘No, you can adapt your skills to any field that you’d like to.’\nJose Vazquez ’15 — Education studies minor\nA member of Hamilton’s Education Studies Program Committee will help each education minor determine a course of study that fits his or her interests. When they’re in the field, students will observe teachers, tutor and coach, study with administrators and policymakers, plan enrichment programs, work in resource rooms and more.\nCareers After Hamilton\n- Teacher, Temple Israel Hebrew School\n- School Social Worker, Bloomfield School District\n- President/CEO, Planned Parenthood\n- Learning Disabilities Teacher, Lincoln-Sudbury High School\nMethods of Tutoring English to Speakers of Other Languages 201FS\nPrepares students to perform as ESOL tutors by providing discussion of the practical approaches, methods and techniques tutors use in classroom settings. Using a communicative curricula that emphasizes function over form, this course addresses language teaching methods, interactive strategies for integrated learning for non-native speakers or English language learners and limited English proficient students. Discussion of the concept of culture helps tutors recognize the influence of culture on patterns of thinking and behaving, and language acquisition.View All Courses\nEducation, Teaching and Social Change 215S\nAnalysis of teaching as an act of social justice in response to fundamental societal problems embedded in educational institutions. Through the lenses of critical theory, pedagogy, and policy, this course examines the praxis of teaching and education policy to explore critical methodologies for transforming the classroom and liberating learners. Using a decolonial framework, connections between the classroom and contemporary world issues are studied so that students can address, respond to, and actively participate in engaged citizenship for the good of education and the greater society. Writing-intensive.View All Courses\nTechnology in Education: Issues and Opportunities 250\nWhat is the difference between learning from technology and learning with technology? This course explores the role of technology in learning and critically analyzes the cognitive, social, political, and logistical aspects of education technology in the K-12 public school setting. Students will research and develop a learning model incorporating technology in a proposal for a specific grade range in a public school system of the future. Hands-on experiences critically assessing technology in constructivist based learning are required. Oral Presentations.View All Courses\nAnthropology of Education 318\nExamines the school as a site for the reconstruction of cultural difference. Special attention paid to links between schooling and the nation, to connections between schooling and modernity, and to themes such as discipline, value, gender, language and labor. Examples from Bolivia, Tanzania, India and the United States, among other nation-states. Concludes with a consideration of globalization, specifically the rise in neoliberal approaches in the governance of school systems.View All Courses\nDesign, Development, and Delivery of Instruction 333\nStudy of theoretical and practical approaches to the design, development, delivery, and assessment of learner-centered instruction. Topics include planning and organizing instructional messages, adapting to learner styles, using Socratic discourse, integrating instructional technologies, and identifying classroom teacher prerogatives. Experiential sessions and videotaping. Oral Presentations.View All Courses\nEducation Practicum 370S\nApplied field experience in a K-12 functional area, including classroom instruction, guidance counseling or school administration. Mentored activities with education professionals. Semester-long placements directed toward analysis and evaluation of educational theories in practice. Oral Presentations.View All Courses\nThe Goal: A Holistic Education For Every Student","“Just as we expect everyone in a school system to plan and prepare for physical risks, we must now also ensure everyone helps plan and prepare for digital risks in our schools and classrooms.” –– Miguel Cardona, Secretary of Education\nCyberattacks against K-12 schools progressively put our children at risk. In September 2022, the Los Angeles Unified School District, the second-largest school system in the country, fell victim to ransomware on its IT systems. Hackers locked up files and demanded a ransom payment. Further investigation showed they posted roughly 2000 student assessment records on the dark web, including their driver’s license numbers, Social Security numbers, and medical and mental health records.\nIn a separate incident in March 2023, Minneapolis Public Schools declined to pay a ransom of $1 million, resulting in the online dumping of over 300,000 files. These files contained sensitive information about students’ sexual assaults, mental health crises, child abuse inquiries, and even campus physical security details.\nThese troubling incidents highlight the importance of enhancing cybersecurity measures in K-12 schools to ensure their students’ and staff’s safety and security.\nCyberattacks Against K-12 Schools on the Rise\nCyberattacks are happening more often to businesses, hospitals, and government agencies. Unfortunately, the education sector isn’t exempt. In this digital age, schools are at risk of more than falling victim to ransomware. They’re also the targets of phishing attacks, data breaches, website and social media account takeovers, Zoom hackings, and more.\nDuring the 2022-2023 academic year, cyberattacks targeted at least eight K-12 school districts. As a result, four schools had to cancel classes or shut down completely. According to a 2022 report, learning time lost after a cyberattack varied from three days to three weeks. Unfortunately, the actual financial cost of these attacks is higher than we think. Schools must pay for incident responders, legal services, forensic analysts, technical controls, network remediation, backup restoration, and more, including higher premiums for cyber insurance.\nIn 2022, some school districts suffered financial losses ranging from $50,000 to $1 million due to cyberattacks. However, in 2021, Baltimore County Public Schools faced a ransomware attack that resulted in over $8.1 million in recovery costs, serving as a prime example of a costly cyberattack.\nIt is essential to acknowledge the vulnerability of schools and its associated risks. Nowadays, children are more tech-savvy than ever and have easy access to the internet. This makes it easier for them to learn how to bypass schools’ technical controls like firewalls and proxy servers. Additionally, they are less likely to practice good cyber hygiene, so they may not be cautious regarding their sensitive information or accessing various websites and apps. And it’s not just the students; at K-12 schools, staff are prime targets for hackers due to their greater access to resources. They often lack proper training and awareness regarding cybersecurity best practices, leaving them vulnerable.\nAs we work to ensure that schools’ physical infrastructure is safe and secure, their digital infrastructures and students’ personal data require the same care and attention. With more schools using technology for remote, hybrid, and in-class learning, and rising concerns over how artificial intelligence (AI) will enable more frequent cyberattacks, cybersecurity in education is a top priority.\nBack to School Safely: Cybersecurity Summit for K-12 Schools\nSince assuming office, the Biden Administration has focused on improving the country’s cybersecurity. It has dedicated resources to support corporations, small businesses, individuals, and educational institutions to achieve this goal. In July, Federal Communications Commission (FCC) Chairwoman Jessica Rosenworcel proposed a pilot program that would provide up to $200 million over three years to bolster cyber defenses in K-12 schools and libraries. This program would collaborate with other federal agencies.\nAt the first-ever Back to School Safely: Cybersecurity Summit for K-12 Schools, First Lady Jill Biden, Secretary of Education Miguel Cardona, Secretary of Homeland Alejandro Mayorkas, educators, school administrators, and education technology (EdTech) providers came together to discuss efforts to strengthen security in K-12 schools.\nThey announced strategies to enhance K-12 school cybersecurity, including creating a Government Coordinating Council (GCC) to improve collaboration between the government and school districts. The council will organize training activities, suggest policies, and share cybersecurity best practices. In a written statement, Education Secretary Miguel Cardona said, “Just as we expect everyone in a school system to plan and prepare for physical risks, we must now also ensure everyone helps plan and prepare for digital risks in our schools and classrooms.”\nThe GCC will be crucial in the Department of Education’s plan to safeguard schools and districts from cybersecurity threats. By promoting consistent and active cooperation between the government and education sectors, this initiative will serve as a vital starting point. Additionally, it will aid in preparing for, responding to, and recovering from cyberattacks against K-12 schools.\nGovernment Resources to Enhance School Cybersecurity\nDuring the summit, it was announced that this school year, K-12 schools will receive customized cybersecurity assessments and training from the federal Cybersecurity and Infrastructure Security Agency (CISA). The Federal Bureau of Investigation and the National Guard Bureau will also provide updated resource guides to assist state governments and education officials in reporting cybersecurity incidents and accessing the federal government’s cyber defense capabilities.\nThe Department of Education and CISA have collaborated to release the K-12 Digital Infrastructure Brief: Defensible & Resilient. This is the second in a series of guidance documents designed to help educational leaders establish and maintain essential digital infrastructure for learning.\nCISA has committed to customized assessments, conducting exercises, and providing cybersecurity training to 300 new K-12 entities for the upcoming school year. It intends to complete 12 cyber drills for K-12 students this year. Moreover, the FBI and National Guard Bureau will present updated resource manuals to state administration and education authorities, assisting in reporting cybersecurity incidents and using federal cyber defense resources.\nTech Companies Step up to the Plate\nSchool districts often face a tough challenge in enhancing their digital defenses while managing their limited budgets and resources. Will Knehr, Senior Manager of Information Security and Data Privacy at I-PRO Americas Inc., knows this all too well. He says,\n“One of the biggest problems I get from educators is that they know that some of these resources exist, but they don’t have the budget, people, or time to use them. Getting the word out about these free and low-cost resources is important.” –– Will Knehr, Senior Manager of Information Security and Data Privacy at I-PRO Americas Inc. & member of the PASS Advisory Council and Technical Committee\nIt’s good to know that not only government agencies are working hard to fight against cybersecurity threats. Many technology companies have announced free, low-cost resources to help school districts keep their students and staff safe. These initiatives provide schools with more comprehensive cybersecurity options, regardless of budget limitations.\n- Amazon Web Services (AWS) launched a $20 million K-12 cyber grant program to help schools implement cloud-based cybersecurity solutions. They’ll help K-12 IT staff upskill and reskill through the AWS Skill Builder digital learning center at no cost. AWS will also offer no-cost cyber incident response assistance and Well-Architected Framework security reviews to all education technology companies that provide mission-critical applications to the K-12 community.\n- Through Project Cybersafe Schools, Cloudflare will offer free Zero Trust cybersecurity solutions to public school districts under 2,500 students, giving small school districts faster, safer internet browsing and email security to minimize cyber threats.\n- Cloud-based K-12 software provider PowerSchool joined the pledge to provide new free and subsidized “security as a service” courses, training, tools, and resources to all U.S. schools and districts.\n- Google released an updated “K-12 Cybersecurity Guidebook” for schools on steps that education systems can take to ensure their Google hardware and software applications’ security.\n- Learning platform company D2L has committed to collaborating with trusted third parties to provide access to new cybersecurity courses. They will also extend their information security review for their core integration partners and pursue additional third-party validation for D2L compliance with security standards.\nIn collaboration with government initiatives, these companies’ programs are valuable in safeguarding schools from outside threats by securing their cyber infrastructures.\nProtecting Schools’ Digital Infrastructures\nPrioritizing the safety of our school communities, including our students and staff, is essential. Collaboration and training are key to achieving this goal. That’s why everyone involved must be well-informed about the best practices of using technology, from hiring competent IT staff to educating staff and students about potential cyber threats and the correct deployment of best practices.\nOur PASS School Safety and Security Guidelines for K-12 Schools provide a helpful starting point for districts looking to strengthen their cybersecurity and network infrastructure. Here are a few top priorities that every school should integrate into their cybersecurity programs:\n- Staff Cybersecurity Training: Equip staff with essential cybersecurity knowledge and best practices to bolster their ability to recognize and address potential threats.\n- Regular Data Backups: Safeguard your critical data by routinely backing up core servers and storing these backups off-site. This strategy ensures swift data recovery in the event of an attack.\n- Phishing Defense: Given that “phishing” remains the primary vehicle for ransomware attacks, it’s crucial to implement effective filtering, testing, and simulation training to fortify your employees against such threats.\n- Multi-Factor Authentication (MFA) and Strong Passwords: Elevate security by enforcing multi-factor authentication or robust password requirements for access to teacher and education networks.\n- Mobile Device Management (MDM) or Mobile Application Management (MAM): Manage remote devices and applications through policies, enhancing control over security in the mobile sphere.\n- Software Patching and Vulnerability Remediation: Stay on top of software patches and vulnerability fixes to ensure that security equipment software and firmware are up to date, reducing the risk of exploitation.\n- Network Monitoring with Security Information and Event Management (SIEM): Implement SIEM systems for early detection and swift response to potential malware threats, strengthening your network’s security posture.\n- Pre-Approved Applications for School-Issued Electronic Devices: Guarantee that only authorized applications are installed and used on school-issued electronic devices, maintaining control over the digital environment.\nBy incorporating these priorities into your cybersecurity strategy, your school can create a more resilient and secure network infrastructure, better protecting against cyber threats.\nLearn about best practices for creating safer schools by downloading and reviewing our School Safety and Security Guidelines. Also, download SIA’s Guide to School Security Funding for more information on resources to enhance school security.\nThe Partner Alliance for Safer Schools (PASS) is a nonprofit 501(c)(3) bringing together expertise from the education, public safety, and industry communities to develop and support a coordinated approach to making effective and appropriate decisions with respect to safety and security investments. You can download the complete PASS Guidelines or check out our PASS Safety and Security Checklist for quick start tips. These resources—as well as whitepapers on various topics including barricade devices, lockdown drills, and more—are available at no cost.\n- Access Control\n- Classroom Door Locks\n- Crime Prevention Through Environmental Design\n- Duress Alarm\n- In the News\n- Lockdown Drills\n- New Technologies\n- PASS Advisory Council\n- PASS Board of Directors\n- PASS Recommendations\n- Safe School Week\n- Safety & Security Guidelines\n- School Safety & Security\n- Security Best Practices\n- Video Surveillance\nReady to get the Guidelines?\nThe most comprehensive information available on best practices specifically for securing school facilities, vetted extensively by experts across the education, public safety and industry sectors."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:849f7f4b-be83-48dc-98ae-41a0a4730cca>","<urn:uuid:1640a775-4556-4fa9-81f5-b3fddaa9e422>"],"error":null}
{"question":"What are the impacts of client-induced interference in wireless networks, and how do secure communication modules protect against network vulnerabilities?","answer":"Client-induced co-channel interference (CI-CCI) occurs when clients join AP cells together, even when the APs themselves are separated enough to avoid interference. When a client connects near the edge of one AP's cell with higher transmission power, it can overlap with another AP's coverage, causing affected clients to be unable to transmit/receive during the first client's transmission. This leads to reduced throughput through increased contention and retransmissions. As for security protection, secure communication modules implement a Chain of Trust architecture that verifies every bit of code running on the module, uses encrypted file systems to protect sensitive information, and implements firewalls to isolate components. The modules prevent unauthorized software loading, protect stored credentials, and maintain security through regular updates to address vulnerabilities.","context":["In the previous post, I discussed AP Induced Co-Channel Interference (CCI) and what tools we have available on the infrastructure side to combat it. In this post, I will discuss what Client Induced CCI (CI-CCI) is and what tools we have available to reduce its effect on network performance.\nClient Induced CCI, is where there are two APs that share the channel, but are separated enough (using some of the tools in the last post) that the APs themselves do not cause CCI between each other, but where there are clients which join the AP cells together. This is similar to the “Hidden AP” problem described by GT Hill.\nHow do clients join AP cells together?\nAs you can see in the above illustration, the APs are both using channel 36 and the outer rings of the APs coverage are the AP Rx sensitivity limits for 6Mbps (the data rate for which the PLCP preamble and header are sent on 5GHz). These APs are not causing CCI between themselves.\nNow suppose a client (Client1) connects to AP1 near the edge of the AP1’s cell. The image above shows the client device coverage pattern when connected to AP1. As you can see the client device coverage pattern now overlaps with some of AP2’s coverage pattern. The amount of channel overlap, will depend on how much “effective distance” there is between the two cells, and the Tx power of the client. In the image above, the AP Tx power has been designed to match mobile devices Tx power, however, the laptop uses higher Tx power than a mobile device. This causes an increased channel overlap between the two cells. This one client is “joining” the two channel 36 cells together, but the APs still cannot hear each other.\nLet’s add some clients to AP2. In the above image, the clients that are coloured orange are inside the coverage pattern of the Client1 on AP1. This will cause those clients to not be able to transmit or receive whilst Client1 is transmitting. In this way Client1 has joined AP1 and AP2 cells together, which could bring the total throughput of the combined cells down, through increased contention and retransmissions.\nSince AP2 does not hear Client1, AP2 can transmit at the same time as Client1, which could cause frame corruption if the intended receiver is one of the clients covered by the overlapping coverage area (clients coloured orange).\nWhat can be done to reduce the effects of CI-CCI?\nIn the last post we discussed the tools available for the infrastructure and you could use those tools to increase the “effective distance” between same channel APs to reduce the effects of the clients on CCI. But none of those tools are regularly available on client devices and so what can you do from the client perspective? Some clients have the ability to adjust Tx output power in their drivers, but in today’s life of mobile devices, most of these devices don’t have that sort of granular control. Let’s discuss what we can do across the board to try and address the CI-CCI issue:\n- Modifying data rates\nRTS/CTS works when a client prefixes its frame transmission with the RTS/CTS exchange. Thereby notifying clients in its cell that the client is about to transmit. More details about RTS/CTS can be read here on the excellent site from Rasika Nayanajith. Client configuration control of RTS/CTS is rare, although Apple’s iOS makes use of it. Will RTS/CTS fix the issues with CI-CCI? Let’s look at its effectiveness.\nIn GT Hill’s video, he mentions that when the traffic is downstream to the client (the AP is initiating the RTS/CTS) that the CTS response from the client is able to notify AP2 of the pending transmission, but with the greater separation of cells in our example this does not happen since Client1’s coverage doesn’t reach AP2. Which means that RTS/CTS for downstream traffic may not help too much.\nWith a greater percentage of the traffic on networks these days being upstream traffic, we also need to look at the impact of using RTS/CTS when the client needs to transmit a frame (upstream traffic).\nIn the images above, the RTS is sent by Client1 to AP1 (1). The RTS is only heard by those devices in Client1’s coverage area (including AP1). AP1 then sends out the CTS (2) and the CTS is only heard by the clients in the coverage area for AP1, as there is no overlap in the AP coverage areas. The only AP2 clients that will know to be idle would be the clients in the overlapping coverage area of AP2 and Client1 (light blue area) due to the RTS being received. An AP2 transmission to one of the orange coloured clients could result in a corrupt frame if Client1 is transmitting at the same time.\nYou can see that RTS/CTS will only have limited impact, and that impact is dependent on how much client induced cell overlap there is.\nModifying data rates\nBesides being a way to reduce overhead on the network caused by management and control traffic, data rates also affect client associations. If a client is unable to demodulate the minimum basic rate (MBR) they are unable to associate to the AP. So if you raised the MBR, the client needs to remain closer to the AP in order to associate. In the image above, the MBR has be raised to 24Mbps on AP1 (shown by the inner circle on AP1). You can see that in order for the client to associate it has to be within the 24Mbps coverage area. Client1’s coverage area is still the same, but now the client is further from AP2 and its clients, and so is not joining the two cells together. Thereby eliminating CI-CCI. This can be difficult to attain complete separation of the clients and APs, and it is likely that some CI-CCI still exists. It is also worth noting that modifying the data rates does not have any effect on AP Induced CCI due to the PLCP preamble and header being sent at the band specific lowest data rates (1Mbps on 2.4GHz, 6Mbps on 5GHz).\n802.11h includes a feature called Transmit Power Control (TPC). TPC is intended to reduce interference from WLANs to satellite services by reducing the radio transmit power WLAN devices use. TPC also can be used to manage the power consumption of client devices and the range between APs and clients. When clients associate/reassociate to an AP, the client provides the AP with its minimum and maximum transmit power capability for the current channel using a Power Capability information element.\nTPC allows the AP to control the transmit power of its clients by sending a management frame containing a Power Constraint information element (IE). The contents of this IE (along with the Country IE) provide supported clients with the local maximum transmit power level at which they can transmit.\nFrom the 802.11 Standard: “The local maximum transmit power for a channel is thus defined as the maximum transmit power level specified for the channel in the Country element minus the local power constraint specified for the channel (from the MIB) in the Power Constraint element.”\nAn AP may also use the minimum and maximum transmit power capability of associated clients as an input into the algorithm used to determine the local transmit power constraint for any BSS it maintains.\nBy using the Power Constraint IE (as you can see in the above images), it is possible for the infrastructure to reduce the effects of CI-CCI since the client is not transmitting at a power level greater than what the network is design for.\n802.11h only applies to DFS-compliant devices in the 5GHz band and by no means all 802.11 devices are 802.11h-capable, and not all infrastructure vendors allow the configuration of the Power Constraint IE. This means that support may be limited in your environment.\nWith 802.11k, an AP can advertise a transmit power constraint lower than the regulatory limit in the same way as 802.11h. The 802.11k Transmit Power Control and link reports also allows a client to discover how its signal is heard by the access point, including a link margin figure, so it can reduce transmit power levels if appropriate. 802.11k is dual band, unlike 802.11h, and is able to control the client transmit power level on both 2.4GHz and 5GHz bands.\nThe issue with 802.11k is client support – not all devices support this amendment.\nAs you can see CCI is a complex issue that is not only caused by APs but by clients as well. Eliminating CCI is very difficult in 5GHz and near impossible in 2.4GHz. There are many tools available in WLAN infrastructure to assist in reducing the effects of both types of CCI, all of which have pros and cons which need to weighed up as to whether of not you should employ them in your network.\nTPC: IEEE 802.11™-2012, Section 10.8\nPower Constraint IE: IEEE 802.11™-2012, Section 18.104.22.168","Securing Your Medical Device Starts with a Secure Communications Module\nIn today’s technological landscape, securing your device is an increasingly complex goal that requires more comprehensive approaches.\nChain Of Trust – Verified At Every Layer\nLaird Connectivity’s Chain of Trust architecture, in combination with secure production provisioning, secures software images running on the 60 SOM. For customers utilizing the 60 SOM as the wireless communications module, this architecture protects from attacks using vulnerabilities exploited on the module to further attack the hosted platform. For devices utilizing the 60 SOM as the host for the main application, the Chain of Trust provides a mechanism to provision and update software with signed images, preventing unauthorized or rogue software from loading into the device.\nThe Chain of Trust is based on a Secure Boot process that begins with an embedded Hardware Root of Trust. The Secure Boot process enables the Root of Trust to be chained to all executable code for the 60 SOM enabling verification of every bit of code running on the module. Secure Boot also controls the available boot sources. Insecure boot paths such as serial or USB ports are disabled permanently during the production process.\nDuring the production provisioning process, a secure symmetric key is programmed into the hardware. The first external boot code that is executed by the 60 SOM is stored in the onboard flash. This boot code is encrypted with the same symmetric key during provisioning thus ensuring the contents remain secret and can be verified as trustworthy. The device will not boot unless the code and data loaded from the flash memory matches the unique pre-determined hash generated during production. This proprietary and secure implementation protects against modifications to the 60 SOM, preventing the loading of insecure images or the enabling of external boot processes.\nSecurity can be increased further by enabling each individual device to be programed with a unique encryption key. If one device is ever compromised, that key cannot be used to attack multiple devices. At a minimum, we anticipate providing unique keys on a per customer or customer device type basis.\nThe Laird Connectivity Chain of Trust architecture includes an Encrypted File System on the 60 SOM to store confidential information such as network credentials, passwords, or other configuration details deemed sensitive by the end user or hospital where the unit is deployed. This protects any stored credentials or configuration details from exposure to hackers and intruders.\nDesigning and building secure products is only the beginning. Software and image updates are required over the life of the product to improve performance, add features, and fix security vulnerabilities. Over time, unpatched connected devices become one of the largest potential attack surfaces given that their security vulnerabilities become well-known and easily exploited. The reason is simple – an attacker can scan a network to find devices with old firmware, then simply take advantage of well-known, well-documented vulnerabilities without having to first do the hard work of discovering how to attack the device. This type of attack is so simple that it can be fully automated and deployed by amateur hackers.\nBy working in partnership with our customers, it is possible to ensure all devices are up to date. This may include Laird Connectivity managing the update process to connected devices in the field via a web service, as well as making updates available to our customers and end users where a physical update of the device may be required.\nOver time, unpatched connected devices become one of the largest potential attack surfaces given that their security vulnerabilities become well-known and easily exploited.\nFirewall – Isolating Subcomponents For Greater Security\nCompartmentalization is used commonly for physical security within buildings with the use of separate, secure rooms. If a person enters the building or room, they are separated from the main building or secure area via locked doors until building security clears them. Laird Connectivity’s 60 SOM leverages its Firewall in the same way, with the use of barriers implemented at the hardware level to enforce isolation between the software components of the system, preventing security vulnerabilities or breaches in one software component proliferating to other areas of the system. The use of compartmentalization in a system design increases the strength in depth of security within the design. The use of the 60 SOM to provide secure and robust communications immediately delivers additional protection within the product, protecting any mission critical applications running within the host processor of the product from attacks launched against the outwardly facing 60 SOM communications module.\nAdditional compartmentalization techniques within the 60 SOM Secure Communications Module utilize the hardware memory management unit of the processor and Linux system processes to further enhance security.\nTo aid customer products to implement the Firewall within their designs using the 60 SOM secure communications module, Laird Connectivity provides a secure communications link to the host via a software library/API. This allows the product host processor to control, configure, and communicate data through the 60 SOM secure communications module using a cryptographically secured communications channel. If the 60 SOM loses connection to the host via this secure connection for whatever reason, it will disable wireless access to the network.\nA malicious attack on the system can be intercepted by the 60 SOM, which functions as a basic firewall for the host system.\nThe host system communicates with the 60 SOM only over a secured connection via Laird Connectivity’s DCAL library, restricting access from outside actors.\nThe isolation allows the host device to remain safe in the event of attacks on the 60 SOM and allows mission-critical operations to take place unrestricted.\nLaird Connectivity continually monitors software packages utilizing Laird Connectivity’s Linux distribution process, ensuring that new vulnerabilities are analyzed and addressed as soon as they are identified. This is achieved through an automated set of tools which monitors the publicly available CVE database, alerting Laird Connectivity when new vulnerabilities are announced. Laird Connectivity is committed to working with 60 SOM customers on a process to alert them of new vulnerabilities and informing them of steps to mitigate their impact within the installed base.\nLaird Connectivity utilizes the 60 SOM and other aspects of Laird Connectivity’s technologies within our stand-alone products such as gateways and communication dongles. Laird Connectivity is actively performing penetration testing of these units to check for exploitable vulnerabilities.This will be followed by action plans and design activities to further harden the devices against exploits. Information and expertise captured through this process can be applied to the 60 SOM and confidentially shared with integration customers as needed. Laird Connectivity actively works with customers to support penetration testing of complete units that leverage our secure communication modules.\nWi-Fi Certification – Enterprise Level Of Security\nWi-Fi CERTIFIED™ is an internationally-recognized seal of approval for products indicating that they have met industry-agreed-upon standards for interoperability, security, and a range of application specific protocols. Whether deploying a new infrastructure or integrating new equipment into an existing infrastructure, using Wi-Fi CERTIFIED products ensures interoperability of Wi-Fi products from multiple vendors. Fewer network problems and support calls are often additional advantages of using Wi-Fi CERTIFIED products. Laird Connectivity is pursuing Wi-Fi certification on the 60 SOM series for 802.11ac Wave-2. The solution will also be WPA\\2 certified and offer a software roadmap to WPA3 certification. In terms of authentication types, Laird Connectivity goes beyond most vendors by supporting enhanced authentication methods.\nAuthentication Support - Laird Connectivity’s Enhanced EAP Supplicant\nLaird Connectivity provides an enhanced EAP Supplicant within our embedded Linux package. The Laird Connectivity supplicant manages the connection state machine and supports the standard supplicant and security role. The Laird Connectivity supplicant supports additional EAP types versus those found in most open source supplicants. Laird Connectivity has gone through extensive testing and optimization of the supplicant including modifications to tune and optimize supplicant behavior, ensuring the best performance in the field. An example would be taking advantage of Laird Connectivity’s signature fast scan and roam capabilities for the Enterprise. Support for Cisco Centralized Key Management (CCKM) is also included for greater interoperability and performance on Cisco’s network architecture.\nFIPS 140-2 Turbo - On-Board Cryptographic Engine\nWithin the Laird Connectivity 60 SOM, we are implementing FIPS 140-2 Turbo functionality, which allows you to satisfy federal security requirements for encrypting data in motion and rest without sacrificing enterprise network performance. The 60 SOM’s hardware acceleration will be directly NIST certified for FIPS 140-2 Level 1. The 60 SOM certification facilitates end products meeting the FIPS requirements without the need for the end product to go through the certification process directly. The solution will contain capabilities for power-on self-testing and encryption key management. As an added security benefit, the module will support full encryption of onboard network parameters and other information to maintain security of the local network. The hardware accelerator will be accessible via an API interface, allowing its use within the application for things such as encrypting data at rest. This dedicated hardware accelerator allows excellence in cryptographic generation without compromising the rest of the device’s resources and without impacting your application performance.\nLaird Connectivity’s Secure Communications Modules\nHealthcare providers and executives around the globe know it is imperative that patients’ health information is accurate, accessible and captured in near real-time. Connecting smart medical devices to the network to provide data interchange with EMR and like hospital systems is transforming hospital operations; increasing efficiency and accuracy of data capture. However, to minimize the exposure of patient and other critical data, this must be done in a secure environment.\nLaird Connectivity begins the process of securing the medical device with a secure communications module with security built in from production and maintained through the update process. Laird Connectivity’s Chain of Trust architecture is designed with multiple layers of verification, signing, encryption, and isolation to ensure the device only runs trusted software and isolating the host application from intrusion attempts. This, in combination with Laird Connectivity’s dedication to identifying and fixing vulnerabilities proactively, is the strength that the 60 SOM presents – a comprehensive, multi-level approach that ensures your device is secured in the event of a malicious attack.\nFor more on the Laird Connectivity 60 SOM module and the Chain of Trust architecture, please contact us."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:9698d1e9-ef3e-4509-a600-e4c24c2b10a0>","<urn:uuid:53bafeb0-6c5d-45ae-bd8e-f368bee4dd2f>"],"error":null}
{"question":"What are the architectural characteristics of mid-century modern homes, and how do they support environmental sustainability?","answer":"Mid-century modern homes were modest and calm structures, made of wood and brick, designed with careful attention to light and ventilation. They featured ingenious planning with efficient intercommunication between rooms and creative use of space, even on narrow sections. From a sustainability perspective, these homes incorporated passive environmental features like tree-lined streets for cooling, shutters for temperature control, and strategic window placement for natural ventilation. Their durable construction methods and use of local materials contributed to long-term sustainability, contrasting with modern buildings that may only last decades. The traditional planning of these homes also supported walkability, reducing car dependency and promoting better air quality.","context":["Jeremy Hansen’s introduction to Modern: New Zealand Homes from 1938 to 1977 quotes an enthusiast of the modernist movement from 1943: “There is a striking new feature in our national life which when war-time activities cease will manifest itself with increasing force: it is a strong desire for better and truer types of homes – homes designed with charm and artistic fitness and a realisation of how deeply they are capable of affecting the well-being of everybody and the joy of life to all.”\nThis was a time in our national life when we more concerned with living than with property.\nThe young moderns, recent graduates of the Architecture School at Auckland University, were concerned with the home more than any other building type. They saw a landscape of dull brick-and-tiles, decayed bungalows and rotting villas. They saw that these were in styles imported from overseas, from Britain and America, that these houses were assembled from mass-produced parts by builders, with little consideration for local conditions. They longed for an architecture that was both modern and of New Zealand, simple and straightforward like the homes of the pioneers, before prosperity and decadence took hold.\nThe modernist enthusiast was a contributor to Home and Building, a forerunner to Home, of which Jeremy Hansen is the current editor. Until the 21st century, the magazine was published “under the auspices of the New Zealand Institute of Architects”. It was a propaganda title, really, one designed to persuade New Zealanders to commission NZIA members to build their new homes.\nHansen and I are talking in a cafe in Auckland University’s Kate Edger Centre. The building is certainly modern – glass and steel, with sliding doors and a high-tech finish; machine-like. But the buildings we are discussing in Modern are nothing like that. They are modest and calm, made of wood and sometimes brick, and often with less glass than one might expect. They hardly could be called machines for living in. They are a different kind of modern. The building in which we talk was built after post-modernism; it is modern but self-consciously so: it knows modernism to be one stage in the history of architecture, one that came to an end but then was risen from the dead when PoMo had run out of things to say and poses to strike. The buildings we discuss were built when we were modern, when there was no possibility of being anything else. Their modernism is innocent and sincere.\nThe years after the war were an age, before the Oil Crisis and the European butter mountain, when many New Zealanders could afford an architect. Hansen is rather pleased that some of the houses in his book were built for “potters and teachers”. These are not “hilltop mansions visited for three weeks a year by their millionaire owners” but homes designed for everyday living. Several of them are still the homes of their original owners, and among these are homes designed and indwelled by their architects.\nSo who are these people and what do they want? Part of the fascination of this kind of book is to see in the photographs how people dwell and the stuff they have accumulated over the years of dwelling. Bill Alington, the architect of the Met Office in Wellington, sits in the dining room of the home he designed, a small home with many paintings and many chairs. His wife, Margaret, sits with her laptop at a built-in desk in one of the bedrooms. There is a kind of precision to living in houses of this kind. Hansen says they mould their owners to them. There is none of the boundless opportunity of the glass boxes on beaches of the one percent, with their acres of empty space and their multiplication of features. There is no pizza oven, no barbecue pit and the only dining table is in the dining room. These are people who somehow manage without a blackboard in the kitchen for writing inspirational messages and reminders of activities (Hansen mentions a woman who was selling her house and so had her friends write notes about imaginary events on the kitchen blackboard to make her life seem more interesting). These are people seemingly able to live as themselves, rather than as display.\nThese people seem to be those the English writer Michael Frayn once called “herbivores”: politically liberal, cultured and gentle. Their way of living, back in the day, was one they wanted everyone to be able to enjoy. They subscribed to the Listener and to Landfall, and hoped everyone might one day do the same. There are few of them left and few of their homes survive. One home in the book, Peter Middleton’s Lomas House, was saved from the demolition desired by its new owners by an enthusiast who offered to buy the house and move it from Hamilton to the shore of Lake Rotorua.\nSome have been altered, Hansen says, with “appropriate care and sensitivity”, although this writer remains to be convinced. The Clifton Hill House in Sumner, designed by Ernest Kalnins in 1965, has been given a fashionable stone garage and a stockade fence. It stands as a commentary on our times: the original house is open and full of light, welcoming and looking out to sea; the external additions on the landward side are like a fortress. Ours is a more frightened society than theirs.\nBut Hansen’s book shows other ways of being at home in the modern world.\n“This house works for me because it makes me feel like I’m living in the ’50s and in a cartoon world, which is what I want,” says the owner of Vladimir Cacala’s Tapper House in Kohimarama, Auckland, who works in television and drives a Chrysler Imperial LeBaron. While the herbivores acquired or made works of art and craft over decades, he has filled his house with bric-a-brac: dusky Tahitian maidens painted on velvet, coloured glassware and that coffee table you threw out years ago. The original owners would have cried into their chilli con carne.\nThere are no empty houses in this book. In his introduction Hansen reveals that, when a largely unfurnished house was to be photographed, “stylists” were hired to provide furnishings, so as to “re-create a sense of how these homes might have appeared in their heydays.” This was not a good idea. The modernist tradition, followed by Home and Building, was to photograph houses empty, often at night with all the lights blazing. The house style of Home is to show the house inhabited by its occupants. The styled recreations are in an eerie place in-between, of rooms sparsely furnished with exemplary pieces of modern design and also, incongruously, furniture from the lower end of the market. Had Ingmar Bergman directed a situation comedy in the ’50s (At Home with the Existentialists) the set might have been similar.\nBut at least without the distraction of the chattels of the owners, we can see the architecture. When Nikolaus Pevsner, the great architectural historian, came to New Zealand in 1958, he gave a radio talk for the NZBS and his script was published in the Listener. He observed, “now – what appears to me most praiseworthy in these new houses is the planning, ingenious ways of placing and surrounding the staircase, and ingenious intercommunication of rooms on one or on several levels and also at least in one case an ingenious placing of the whole house across a narrow section so as to get the maximum benefit out of it.” This ingenious planning is evident throughout the book. New Zealand architects, many of them immigrants from Europe, knew how to create space and solve problems.\nAnd this book, which was made possible because Hansen and his predecessor had commissioned photosets of mid-century houses for the magazine, provides plans. Most are the architects’ own, bits of history in themselves. Books of this type seldom include plans, but they show that the editor takes the reader seriously. Plans allow the reader to put the house together from the photographs, to see what goes where, to walk around, to be free from the tyranny of the image. This book also includes some supporting illustrations, like William Sutton’s painting of St Sebastian, tied to a tree and wearing nothing but his undershorts; Sutton’s model was the designer of his home, Tom Taylor. The accompanying text tells us that artist and designer remained friends.\nMost of the writers are architectural historians who will be familiar to readers of recent architectural books; some though are the owners, who contribute their own histories of dwelling. Many of the houses will be unfamiliar; Hansen’s choices are often outside the canon of New Zealand architecture. And many are in places not noted for their architecture.\nWhen Nikolaus Pevsner came here he was asked whether he could see a New Zealandishness in our architecture. He found that in modern houses, “built honestly of local materials for a client who – like all New Zealand – is neither rich nor poor but comfortably off; and built within that client’s money.” A lot has changed since then; New Zealand is no longer comfortably off and now most of the houses featured in magazines are built less than honestly of synthetic materials for clients who are very rich and probably not very honest. Many of the modernist houses of the 1950s have been demolished and replaced by what Hansen calls “bloated, forgettable dwellings”; much that is built today is full of “flabby, poorly planned spaces that ignore basic modernist precepts such as the careful management of light and ventilation”. Hansen is too polite to say so, but many of the homes of the rich are simply vulgar.\nThis book then, provides some nostalgia for a kinder and more sensible age. It also gives some hope for the future. Not only might houses of this type be appreciated and preserved by their owners and by the local bodies, but perhaps some of those modernist precepts, of honesty and planning, might be observed again in new houses. But even if they are ignored, this book broadens our understanding of the modernist house in New Zealand, showing the work of architects who are little known and the culture of an age that has not been forgotten but has been remembered badly.\nModern: New Zealand Homes from 1938 to 1977, Random House, $75.\nPaul Litterick is a PhD candidate in the School of Architecture and Planning at Auckland University, who has written a thesis about architectural writing in New Zealand.","Written by Emily Udy, Preservation Manager\nAt the heart of preservation is the desire to maintain the defining places of our community so that we can understand our place in the timeline of society. When we are engaged in preservation we are deciding what we want to survive into the next century, which is an awesome responsibility; one that belongs to us all.\nAs we look from past to future it is imperative to address how preservation supports sustainability and resiliency. These three goals are intrinsically tied together, and in fact, the Preservation movement is indebted to the Sustainability movement for their efforts to preserve our natural environment. Salem, like any city settled on the seashore, has a history tied to the water, and as a result many of our historic resources will face natural demolition as flood levels change and average daily tides move higher.\nA Historic House is a Passive House\nLong before there was “preservation” or “sustainability” construction trends and materials evolved to meet the climate needs of their area. In New England many character giving features are not style preference but passive ways to keep a home comfortable. Historic buildings are oriented on streets taking advantage of prevailing winds. The small rooms in historic houses were easy to warm by closing the doors and keeping in the heat from the fire. Double hung windows allow air flow and stairways from basement to attic let heat vent upwards in the summer. Exterior and interior shutters on moderate sized windows each play a different role in protection from wind damage or keeping heat or cold in or out. Brick walls or thick plaster stay comfortable for longer when the temperatures change. The tree lined streets we associate with many historic neighborhoods add considerable cooling during summer months. Obviously, in modern times we can take advantage of these historic characteristics to keep our homes comfortable with less energy use.\nBeyond historic construction methods, the preservation profession has long been contributing to environmental conservation whether it was recognized or not. Donovan Rypkema, a leading preservation economist, pointed out in a lecture that he delivered in Salem in 2015 that saving a 2200 sf house from demolition is equivalent to cutting the lifetime plastic bag use of 440 people.\nRypkema also highlighted a study that showed that to rehab a historic house used 47 tons of material energy (that includes energy used to extract, create, ship and discard the materials). To build a new home on a clear site used 182 tons (3.5 times more) and to demo the old house, haul it to a landfill and rebuild with LEED gold architecture standards used 351 tons – more than 7 times more energy than would rehabbing a house of the same size. As stated by Stephanie Meeks, President of the National Trust – it simply does not make sense to recycle cans and newspapers to save energy and not recycle buildings.\nIn Salem, so far we have only a few “tear-downs” a year, even in unprotected historic neighborhoods, but we must be vigilant; as the desirability of Salem’s neighborhoods increase so will pressure to build new. It is especially important to carefully evaluate the ideas that “it takes more money to rehab than to build new,” or that “new buildings are more energy efficient than old ones.” In many cases these are falsehoods that become even more outlandish when you consider the social and environmental costs of demolition. Preservation ensures that we aren’t throwing away our past and our future.\nA Traditional Neighborhood is a Walkable Neighborhood\nIn Salem’s case, there are many areas in the heart of our city where demolition happened decades ago. There is much of traditional neighborhood development style that today’s developers can utilize, including methods that increase sustainability. Thoughtful re-development of urban parcels allows new construction to take advantage of a city that has been walkable for centuries. Walkability promotes individual health and well-being; community-wide it means less car trips, higher use of public transportation, less use of fossil fuels, lower parking needs, fewer fields of asphalt, smaller heat islands, better air quality and lower consumption of automobiles in general.\nIf large-scale new construction in our city used a traditional style of city planning we would see fewer developments that were cut off from neighborhoods and rendered residents dependent on their cars. Traditional neighborhood construction will also extend the character and quality of life found in our historic neighborhoods to new city residents. It must be noted that proximity is not the same as walkability, specific design steps need to be taken to make leaving the car at home the default option.\nBuild New to Last\nThere is one final consideration about sustainability and new construction in our historic city that references the cliché “they don’t build things like they used to.” While material and labor supplies may be different than then were in the past, it is hard not to notice that in a city with centuries of quality construction some new buildings are built to last only a few decades.\nInstitutions like the city government and community advocacy organizations must expect the best from our developers. Most importantly, the members of our community must demand high quality, lasting design and construction. Historic Salem’s intention when commenting on new construction is to ask that what is being built now will be worth preserving in 100 years, at a minimum.\nAll of our efforts to cut plastic bag usage now will be for naught if we end up tearing down buildings that have outlived their usefulness before the builders have even retired.\nI’m going to conclude with this statement from the National Park Service:\nHistoric preservation is inherently a sustainable practice. A commonly quoted phrase, “the greenest building is the one that’s already built,” succinctly expresses the relationship between preservation and sustainability. The repair and retrofitting of existing and historic buildings is considered by many to be the ultimate recycling project…\nNational Park Service page on Sustainability, that I quoted above.\nThe National Trust and their Preservation Green Lab have information from rehabbing historic windows to reusing industrial cities.\nFollow us on Instagram!"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:50c8df15-8ade-43d8-a5ed-71ba22f9f004>","<urn:uuid:791bcfc6-eb51-4ff2-bcfc-41e4e672d6e1>"],"error":null}
{"question":"How do contemporary architectural spaces and Hindu temples differ in their approaches to transforming human experience?","answer":"Contemporary architecture shapes human experience primarily through structural and functional aspects, with architects focusing on aesthetics and humanist aspects, sometimes at the expense of technical considerations. In contrast, Hindu temples are specifically designed to transform human energy and consciousness through carefully planned architectural elements like domes and pyramids that reshape space. These temples use specific architectural features, ritual spaces, and design elements to facilitate the transformation of individual perception into universal perception, helping devotees transcend ordinary consciousness and connect with the divine. The entire temple structure is conceived as a tool for spiritual transformation, incorporating complex energy forms and elements intertwined with deities, trees, plants, colors, and forms.","context":["Building first evolved out of the dynamics between needs (shelter, security, worship, etc.) and means (available building materials and attendant skills). As human cultures developed and knowledge began to be formalized through oral traditions and practices, building became a craft, and “architecture” is the name given to the most highly formalized and respected versions of that craft.\nIt is widely assumed that architectural success was the product of a process of trial and error, with progressively less trial and more replication as the results of the process proved increasingly satisfactory. What is termed vernacular architecture continues to be produced in many parts of the world. Indeed, vernacular buildings make up most of the built world that people experience every day. Early human settlements were mostly rural. Due to a surplus in production the economy began to expand resulting in urbanization thus creating urban areas which grew and evolved very rapidly in some cases, such as that of Anatolia and Mohenjo Daro of the Indus Valley Civilization in modern-day Pakistan.\n“A house in Japan is considered differently from one in Europe. It is more transient, sits more lightly on the ground”\nThey grew out of harsh conditions the devastation of war, privations of the aftermath, recurrent earthquakes, shortage of land but then often chose to add voluntary challenges of their own. There are houses where the rooms are separated by courtyards, such that you have to expose yourself to the weather to pass from one to another, or where unusable voids are inserted into already cramped locations, or where normal expectations of privacy, comfort, cosiness, domesticity, beauty and shelter are, with careful deliberation but for reasons not completely explained, challenged.\nThen there are the full-sized constructions, the hut-on-stilts and the white boxes, one the work of the idiosyncratic reviver of traditional craft, the other a recreation of the house of 2005 by the Pritzker prize winner. The latter is conceived as a series of small pavilions connected by courts, recreated here with books, music, films and personal objects recalling the life of its owner. Lighting rises and falls in imitation of the cycle of day and night, accelerated to 60 minutes. When it is darker you can see films projected against the exterior of the building, which is something he likes to do. It is as immersive as the other material is restrained, creating an alternate version of the original rather than a perfect simulacrum.\nThe architecture of different parts of Asia developed along different lines from that of Europe; Buddhist, Hindu and Sikh architecture each having different characteristics. Buddhist architecture, in particular, showed great regional diversity. Hindu temple architecture, which developed around the 3rd century BCE, is governed by concepts laid down in the Shastras, and is concerned with expressing the macrocosm and the microcosm. In many Asian countries, pantheistic religion led to architectural forms that were designed specifically to enhance the natural landscape.\nWith the emerging knowledge in scientific fields and the rise of new materials and technology, architecture and engineering began to separate, and the architect began to concentrate on aesthetics and the humanist aspects, often at the expense of technical aspects of building design. To satisfy the contemporary ethos a building should be constructed in a manner which is environmentally friendly in terms of the production of its materials, its impact upon the natural and built environment of its surrounding area and the demands that it makes upon non-sustainable power sources for heating, cooling, water and waste management and lighting.\nIn the late 20th century a new concept was added to those included in the compass of both structure and function, the consideration of sustainability, hence sustainable architecture.\nWe shape our buildings; thereafter they shape us.\nLandscape architecture is the design of outdoor public areas, landmarks, and structures to achieve environmental, social-behavioral, or aesthetic outcomes. It involves the systematic investigation of existing social, ecological, and soil conditions and processes in the landscape, and the design of interventions that will produce the desired outcome.\nA system architecture can comprise system components that will work together to implement the overall system. There have been efforts to formalize languages to describe system architecture, collectively these are called architecture description languages (ADLs).\nBusiness architecture is defined as “a blueprint of the enterprise that provides a common understanding of the organization and is used to align strategic objectives and tactical demands.” People who develop and maintain business architecture are known as business architects.\nTo restrict the meaning of (architectural) formalism to art for art’s sake is not only reactionary; it can also be a purposeless quest for perfection or originality which degrades form into a mere instrumentality”. Among the philosophies that have influenced modern architects and their approach to building design are rationalism, empiricism, structuralism, poststructuralism, and phenomenology.\nThere was also the rise of the “gentleman architect” who usually dealt with wealthy clients and concentrated predominantly on visual qualities derived usually from historical prototypes, typified by the many country houses of Great Britain that were created in the Neo Gothic or Scottish Baronial styles.\nForests were the first temples of God and in forests men grasped their first idea of architecture.\nSince the 1980s, as the complexity of buildings began to increase (in terms of structural systems, services, energy and technologies), the field of architecture became multi-disciplinary with specializations for each project type, technological expertise or project delivery methods. In addition, there has been an increased separation of the ‘design’ architect from the ‘project’ architect who ensures that the project meets the required standards and deals with matters of liability.\nThe preparatory processes for the design of any large building have become increasingly complicated, and require preliminary studies of such matters as durability, sustainability, quality, money, and compliance with local laws.\nWe need houses as we need clothes, architecture stimulates fashion. It is like hunger and thirst you need them both.\nA large structure can no longer be the design of one person but must be the work of many. Modernism and Postmodernism have been criticised by some members of the architectural profession who feel that successful architecture is not a personal, philosophical, or aesthetic pursuit by individualists; rather it has to consider everyday needs of people and use technology to create liveable environments, with the design process being informed by studies of behavioral, environmental, and social sciences. Cognitive architecture can refer to a theory about the structure of the human mind. One of the main goals of a cognitive architecture is to summarize the various results of cognitive psychology in a comprehensive computer model.\nThere has been an acceleration in the number of buildings which seek to meet green building sustainable design principles.\nEnvironmental sustainability has become a mainstream issue, with profound effect on the architectural profession. Many developers, those who support the financing of buildings, have become educated to encourage the facilitation of environmentally sustainable design, rather than solutions based primarily on immediate cost. Major examples of this can be found in passive solar building design, greener roof designs, biodegradable materials, and more attention to a structure’s energy usage.\nEnterprise architecture applies architecture principles and practices to guide organizations through the business, information, process, and technology changes necessary to execute their strategies. These practices utilize the various aspects of an enterprise to identify, motivate, and achieve these changes.” Practitioners of enterprise architecture, enterprise architects, are responsible for performing the analysis of business structure and processes and are often called upon to draw conclusions from the information collected to address the goals of enterprise architecture: effectiveness, efficiency, agility, and durability.","From Hindupedia, the Hindu Encyclopedia\nTemples are places of focus for all aspects of life - religious, cultural, educational and social. A temple allows visitors to transcend the world of man and to connect with God.\nA temple is a place where you can approach God and realize divine knowledge. Temples are constructed to help aspirants in their journey to enlightenment and liberation. The principles of design and construction and the forms of architecture and decoration were all designed with this principle in mind. All of these are described in detail in the Agama and the Vastu shastra texts. These texts describe how architecture and decorations should be created to help focus human energy on the Divine. Vastu Shastra has a section explicitly devoted to the design of temples.\nTemple are designed to dissolve the boundaries between man and the divine. A temple is not considered to simply be the abode of God, but also is God. God and therefore by implication the whole universe is identified with the temple's design and actual fabric. The ground plan, site location (and its relation to shade and water), its vertical elevation relating to mountains, etc are all important aspects to a temple.\nThe Puranas state that \"The gods always play where groves are near rivers, mountains and springs\". Sacred sites are therefore usually associated with water, shade and lakes are often considered to be sacred and certain lakes have healing and purifying powers. Certain rivers, such as the Ganga, have descended from the heavens and their sacred waters are needed in the temple tank.\nCaves are places of great sanctity. Most of the earliest surviving shrines are rock cut caves. In later temples the garbagriha was designed to resemble a cave and as such was small and dark and the surfaces of the walls were unadorned and massive. The garbagriha is a place that encourages meditation which is possible only in solitude. Approaching the shrine is a movement from open spaces to a confined small space; from light to darkness, from a profusion of visual form and decoration to the visual simplicity of the cave. From this sanctuary the implied movement is vertical, to the symbolic mountain peak directly above the image of the god. This movement upwards is linked to the idea of enlightenment which is identified with the crowning finial of the temple - the amalaka or sikara.\nThe concept of spirituality in the system of sacred architecture in India is something that goes beyond the mere static relations between inert objects and space as found in other architectural traditions. The relationship of objects with one another and space in India's sacred architecture extends to include higher entities said to be in charge of various aspects of universal affairs, all of whom carry out their work in accordance with the will of God.\nMost ancient stone temples were the result of royal patronage and built to benefit of the whole community, they were expressions of the devotion and piety of the ruler and his people.\nThe temples were maintained through donations from royal patrons and private individuals. They were given money, gold, silver, livestock and income from grants of land which sometimes included whole villages.\nHow temples shape and transform energy\nThe purpose of all rituals is directed towards reshaping of human psyche, transformation of individual perception into universal perception, and radical changes in personal thoughts, desires, and ambitions. The space or sky is reshaped in temple architecture through domes, pyramids, various shapes and forms to provide maximum rhythmic response to achieve the desired results.\nAll religious rituals have a definite aim - transformation of the lower energy formats in the human being into the energy form of the outer spaces, or the sky, or the universal being that is immensity. A suitable medium is provided by the sky as shaped by the domes and pyramids. A deity in the temple is a medium to absorb all the individual desires suitably transformed by rituals. The Deep Mala (fire pillar) in the line of the deity's vision in the outer space of the temple serves as the bridge linking the inner vessel of collective desires represented by the deity with outer space or sky that is universal immensity. This fire pillar has a characteristic shape, which points towards the sky. A divine fire, which is the purifying factor in the temples, is lit using ghee made from cow's milk. Through this fire circulates a rhythmic ascending energy form. Deity's vision is normally aligned to the North or the East directions, which are the sources of Jaivik Urja or positive energies.\nTemple architecture represents the concept of evolution and radical changes. The complex energy forms and finer elements are intertwined with deities, trees, plants, colors, shapes and forms in the temple architecture. Different deities in the temple represent body, mind, intellect and the sub-components. These deities are then linked to the cosmos by associating them with specific directions. This philosophy establishes a chain of relationships between micro level elements and the macro level existence. The mandalas available in temples are essentially charts of existence, transformations, and energetic.\nTemple Architecture and Pranayam\nPavan or wind is the bridge for the mind to ascend to the sky. The holistic concept of evolution is defined in terms of the medium - the wind. Wind represents both, the Sound and the sky. Therefore, primordial sounds are the keys to reinforcing the bond between the mind and the sky. Controlling the wind element at individual level is called pranayam. We can say that the temple architecture provides a natural stage of pranayam, not with any definite individual efforts, but through various forms, shapes, rituals, and sounds. These parameters establish a unique path for correlating the wind and the sky. Domes and pyramids in the temple transform the sound into the mandalas. The echo of this rhythmic primordial sound takes the wind to the sky. The Gurutatva is described as 'Akhand Mandalakaram'. This means that the rhythmic mandalas created by the echo of primordial sounds activate the gurutatva in the human mind.\nIn other words, temple architecture creates a space for holistic atmosphere of natural Pranayam suitable for any individual. Echo of primordial sounds enters the limitless finer circles beyond the audible range and helps the mind to transcend maya to reach the Absolute.\nTemple Design Manuals\nThe Agamas and the Vastu Shastra texts are the scriptural authorities on temple architecture. They give precise details and formulas prescribing how to design, carve and assemble a temple. The resulting structure and its relationship with its surroundings create a subtle, sublime atmosphere in which ceremonies performed by priests easily lift the veil between this world and the world of the Gods and Devas so their blessings can pour forth to gathered devotees.\nIn fact, temple architecture is a specialized subject in Vaastu Shastra. Right from selection of site, to defining the dimensions of the structure, to placement of water source or pond and deep mala, to determining the exact form and proportions of the idol is described in great detail.\nThe temple should be built at a suitable place, like a Tirtha. The ideal location is a a beautiful place where rivers flow, on the banks of a lake or by the seashore; on hill tops, mountain slopes, or in a hidden valley. The site of the temple may be selected in a forest, a grove, or in a beautiful garden. Temples should also be built in villages, towns and cities or on an island, surrounded by water.\nThe temple itself should always face east since that is the most auspicious direction. From the east appears the rising sun, the destroyer of darkness and the giver of life.\nSignificance of Prakrima\nWhen in a temple, it is customary to circumambulate the deity.\nWhen a temple is established and life is infused into the deity through a proper pran pratistha ceremony, divinity enters the deity. This divinity is in the form of magnetic waves starting from the central point of the base of the deity and spreads around in a circle. The vibrations are the strongest near the deity and gradually weaken as the circle becomes larger. The positive vibrations influence a person walking around the deity.\nThe magnetic field moves in a clockwise direction, therefore, it is essential that one walks around the deity clockwise. By moving along the magnetic field of the deity one can benefit from the positive vibrations one receives. These vibrations are a blessing that strengthen the worshipper and protect him/her from all kinds of problems and calamities. After completing prayers and offerings, it is therefore customary to walk around the deity.\nThe longer one walks around the deity, the greater the benefit from the vibrations. It is customary to walk 5 to 11 times around a deity.\nReligious texts direct that when going around the deity of Shankar one should not cross the line where the offering of milk and water flows. For this reason, one takes only half a round around Shankar. One returns and then does the other half because the vibrations around Lord Shankar move both clockwise and anti-clockwise.\nWhen one walks anti-clockwise, the divine vibrations that move clockwise counter the individual's personal vibrations, gradually destroying them. Therefore, the anti-clockwise movement is prohibited. The harmful effects vary according to the Deity being circumambulated.\nThere are four kinds of rituals conducted in a temple: nitya (daily), naimittika (occasional), kamya (optional) and prayaschitta (expiation).\nMore about Temples\nTemples throughout India\nTemples of Andra Pradesh\nTemples of Karnataka\nTemples of West bengal\n- Kalighat Temple\n- Dakshineshwar Temple\nTemples of Gujarat\n- Somnath Temple\n- Dwarkadhish Temple\n- Dakor Temple\nTemples of Jammu & Kashmir\n- Vaishno devi Temple\nTemples of Tamil Nadu\n- Thiru Aayarpaadi Sri Kari Krishna Perumal\n- Pallikondeswara Swamy Surutapalli Devasthanam\n- Vasishteswaraswamy Temple\n- Sri Naganathaswamy Temple Thirunageshwaram\n- Sri Balasubramanya Swamy Temple Ayikudi\n- Sri Dharbarenyeswarar Temple Thirunallaru\n- Sri Swedaranyeswarar Temple Thiruvengadu\n- Sri Kalyanavaradharaja Perumal Temple Paruthiyur\n- Karungaali Sri Chinthaamaneesarar\n- Thiruverkaadu Sri Karumaari Amman\n- Thiruverkaadu Sri Vedhapureeswarar\n- Kettavarampalayam Sri Ramar\n- Madhuraanthakam Eri Kaatha Sri Raamar\n- Vallakkottai Sri Subramanyar\n- Thiruninravur Sri Hridhayaaleeswarar\n- Thiruninravur Sri Bhakthavatsala Perumal\n- Thirumazhisai Sri Othaandeswarar\n- Thirusoolam Sri Thirusoolanaadhar\n- Singaperumal Koil Sri Ugra Narasimhar\n- Chettippunyam Sri Varadharaja Perumal\n- Thirukkachur Sri Kachabeswarar\n- Thirukkachur Sri Oushadheeswarar\n- Hanumanthapuram Sri Agora Veerabadhrar\n- Chenganmaal Sri Chenganmaaleeswarar\n- Thiru Idaichuram Sri Gnanapureeswarar\n- Sembakkam Sri Jambugeswarar\n- Cherappanancheri Sri Veemeeswarar\n- Thirumalai Vaiyaavoor Sri Prasanna Venkatesa Perumaal\n- Ezhuchur Sri Nallinakkeeswarar\n- Ariyathurai Sri Varamoortheeswarar\n- Thiruvidandhai Sri Nithya Kalyaana Perumaal\n- Kolappaakkam Sri Agatheeswarar\n- Somangalam Sri Somanaadheeswarar\n- Gerugambaakkam Sri Neelakandeswarar\n- Porur Sri Ramanaadheswarar\n- Kunrathur Sri Naageswarar\n- Maangaadu Sri Velleeswarar\n- Poondhamalli Sri Vaidheeswarar\n- Kovur Sri Sundhareswarar\n- Maangaadu Sri Vaikunda Perumaal\n- Nandhambakkam Sri Kothandaramar\n- Pozhichalur Sri Agatheeswarar\n- Aathur Sri Muktheeswarar\n- Koyambedu Sri Kurungaaleeswarar\n- Semmancheri Sri Srinivasa Perumaal\n- Vyasarpadi Sri Raveeswarar\n- Pon Vilaindha Kalathur Sri Munkudumeeswarar\n- Ponpadhar Koodam Sri Chathurbuja Kothandaraamar\n- Manimangalam Sri Dharmeswarar\n- Manimangalam Sri Kailaasanaadhar\n- Vallam Sri Vedhaantheeswarar and Sri Giri Varadharaaja Perumaal\n- Kaaladi Sri Aadhi Sankarar"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:0b735351-7abc-4bfe-af17-da037875f93b>","<urn:uuid:86e43fe2-f969-42a4-9f5b-1261888b7da2>"],"error":null}