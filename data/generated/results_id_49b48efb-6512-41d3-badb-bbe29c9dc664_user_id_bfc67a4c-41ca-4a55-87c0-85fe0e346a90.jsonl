{"question":"What are the key differences between color perception in fish eyes versus humans with color vision deficiency?","answer":"Fish and humans with color vision deficiency have distinct differences in their color perception systems. Fish have retinal cones that can detect different ranges of color, and most fish, like humans, have red, green and blue sensing cones, though the range of colors sensed can vary. Some fish species can even perceive ultraviolet light and have UV-visible patterns on their bodies. In contrast, humans with color vision deficiency typically lack either red or green cones, making them unable to distinguish between these two colors. Complete color vision deficiency (monochromatic vision) in humans, where none of the sets of color cones works, is very rare. Most color vision deficiencies in humans are inherited, typically passed to male children through their mothers from affected grandfathers.","context":["It is actually a myth that bass are colorblind. Fish, including the bass see through their eyes and are also able to detect color. The eyes of fish are also usually rounder than most mammals due to the refractive index of the water; focus is achieved by moving the lens in and out so as not to distort things. The bass fish’s eyesight is one of its most important senses as it helps them locate food. They are considered “sight feeders” and use their other senses as well as support for their sense of sight.\nFish eyes are designed with a cornea, an iris, pupil, lens and a retina. The process of actually turning light into images all begins at the retina. This parabolic shaped surface located at the back of the eye is where photons of light are received and then transformed into electrical impulses for interpretation by the brain. The retina contains rods and cones (the photosensitive receptors that accomplish the task of receiving light and transforming it into impulses to direct it through nerve fibers leading to the brain).\nOne prominent characteristic of the fish eye from the outside is its bulbous nature. The outer layer of the eye known as the cornea is a dome shaped transparent layer that is the first to receive light. With the terrestrial vertebrate eye, light travels through the air and hits the cornea. Because the air and cornea differ in density however, the light is then refracted meaning it is bent and directed into the opening known as the pupil. Water and cornea are of about equal densities, so there is often little refraction with the cornea of a fish eye.\nBecause these creatures inhabit the water, that absorbs, scatters and bends light to a degree that makes seeing under water difficult and also different than seeing in air, fish have evolved out of necessity and are designed with specific anatomical characteristics in the eye that help them see underwater. In water colors behave differently and are not easily differentiated as they are when seen in the air. Therefore, fish eyes to varying degrees are equipped with retinal cones that can detect different ranges of color.\nIt should be noted that there are two types of photo receptors on the retina of the eye. The first are rods which are sensitive to light in general, and the second are cones which are sensitive to colored light. The ratio of rods and cones varies according to a fish’s habitat. There are also different types of cones located within a fish retina and each is sensitive to a different range of color. Most fish, similar to humans have red, green and blue sensing cones however the range of each color sensed can vary. Additionally some species of fish have been found to perceive ultraviolet light and also have patterns on their bodies that can only be seen under UV light. It is presumed that this is for attracting the attention of conspecifics.","vision(redirected from solid vision)\nAlso found in: Dictionary, Thesaurus, Legal, Financial, Encyclopedia.\nLike a camera lens, the lens of the eye reverses images as it focuses them. The images on the retina are upside down and they are “flipped over” in the visual center. In a psychology experiment, a number of volunteers wore glasses that inverted everything. After 8 days, their visual centers adjusted to this new situation, and when they took off the glasses, the world looked upside down until their brain centers readjusted.\nThe retina is made up of millions of tiny nerve cells that contain specialized chemicals that are sensitive to light. There are two varieties of these nerve cells, rods and cones. Between them they cover the full range of the eye's adaptation to light. The cones are sensitive in bright light, and the rods in dim light. At twilight, as the light fades, the cones stop operating and the rods go into action. The momentary blindness experienced on going from bright to dim light, or from dim to bright, is the pause needed for the other set of nerve cells to take over.\nThe rods are spread toward the edges of the retina, so that vision in dim light is general but not very sharp or clear. The cones are clustered thickly in the center of the retina, in the fovea centralis. When the eyes are turned and focused on the object to be seen the image is brought to the central area of the retina. In very dim light, on the other hand, an object is seen more clearly if it is not looked at directly, because then its image falls on an area where the rods are thicker.\nColor vision deficiency (popularly called “color blindness”) is the result of a disorder of one or more sets of cones. The great majority of people with some degree of deficiency lack either red or green cones, and cannot distinguish between those two colors. Complete color vision deficiency (monochromatic vision), in which none of the sets of color cones works, is very rare. Most deficiencies of color vision are inherited, usually by male children through their mothers from a grandfather with the condition.\nStereoscopic vision works best on nearby objects. As the distance increases, the difference between the left-eyed and the right-eyed views becomes less, and the brain must depend on other factors to determine distance. Among these are the relative size of the object, its color and clearness, and the receding lines of perspective. These factors may fool the eye; for example, in clear mountain air distant objects may seem to be very close. This is because their sharpness and color are not dulled by the atmosphere as much as they would be in more familiar settings.\nPatients who are in bed following surgery or for therapeutic rest should have articles on their bedside table arranged in the same way all of the time so that they can be found easily. If only one eye is affected, articles should be placed within reach on the unaffected side and persons communicating with the patient also should stand on that side. If peripheral vision is limited, objects and persons should be positioned in the patient's line of vision.\nSome patients, especially the elderly, may experience increased sensitivity to glare. Wearing sunglasses outdoors, adjusting the window blinds to deflect the sun, and using indirect lighting can help avoid discomfort. This does not mean that the patient should be in a darkened room. For most, increased illumination makes it easier to see. It is the glare that impairs their vision.\nWhenever it is necessary to do something for the visually impaired person, explain beforehand what will be done. This helps reduce confusion and establishes trust in the caregiver. (For patient care, see also blindness.)\nPatients with impaired vision may also benefit from such low-vision aids as convex or magnifying lenses that are hand held or mounted on a stand or clipped to the eyeglasses. Adjustable lamps, large-print reading matter, reading stands, writing guides and lined paper, and felt-tipped pens can facilitate reading and writing and improve the quality of life of a person with limited vision.\nCategories of nursing diagnoses associated with impaired vision include Anxiety, Ineffective Coping Patterns, Fear of Total Blindness, Impaired Home Maintenance Management, Potential for Physical Injury, Impaired Physical Mobility, Self-Care Deficit, and Self-Imposed Social Isolation.\nSee also: sight.\nSee also: sight\nachromatic vision See achromatopsia.\nalternating vision See contact lens.\nambient vision Vision mediated primarily by the peripheral retina and involved in spatial orientation and recognition of motion. See focal vision.\nanomalous trichromatic vision See anomalous trichromatism.\nbinocular vision (BV) Condition in which both eyes contribute towards producing a percept which may or may not be fused into a single impression. See sensory fusion; monoblepsia; critical period; bar reading test; FRIEND test; hole in the hand test; Worth's four dot test; Worth's classification of binocular vision; binocular vision single zone of clear.\nbinocular single vision See single binocular vision.\nblue vision See chromatopsia.\nblurred vision Vision characterized by poor visual acuity or in which the edges of objects are indistinct. It may be due to uncorrected or poorly corrected ametropia or presbyopia, anomalies of the ocular media (e.g. cataract, corneal opacity, haemorrhage in the vitreous), amblyopia, excess lacrimation, spasm of accommodation, optic neuritis, angle-closure glaucoma, diabetes, multiple sclerosis, migraine, etc.\ncentral vision Vision of objects formed on the foveola or the macula. See sensory fusion.\nchromatic vision See colour vision.\ncolour vision (CV) Vision in which the colour sense is experienced. Syn. chromatic vision. See Hering's of colour vision theory; Young-Helmholtz theory.\ndaylight vision See photopic vision.\ndefective colour vision See defective colour vision.\ndeuteranomalous vision See deuteranomaly.\ndichromatic vision See dichromatism.\ndistance vision (DV) Vision of objects situated either at infinity or more usually at some 5 or 6 m. See Snellen chart; near vision.\ndiurnal vision See photopic vision.\ndouble v . See diplopia.\neccentric v . See eccentric fixation; peripheral vision.\nentoptic v . See entoptic image.\nextrafoveal vision See peripheral vision.\nfield of vision See visual field.\nfocal vision Vision mediated by, primarily, the macular area of the retina and involved in the examination and identification of objects. See ambient vision.\ngreen v . See chromatopsia.\ngun barrel v . See tunnel vision.\nhaploscopic vision Vision as obtained by looking in a haploscope.\nindirect vision See peripheral vision.\nindustrial vision The branch of optometry concerned with vision and perception by the individual at work, the evaluation of visual performance in a given occupation, the prescribing of protective ocular devices and the determination of the optimum environment (e.g. illumination) to accomplish a visual task efficiently.\nintermediate vision Vision of objects situated beyond 40 cm from the eye but closer than, say, 1.5 m. See distance vision; near vision.\nisland of vision A description of the visual field as a three-dimensional hill surrounded by a sea of darkness. Stimuli that fall within the island can be seen, whereas stimuli that fall outside the island cannot be seen. The height of the island represents the sensitivity of the eye, with the highest acuity at the top of the hill corresponding to foveal vision and declining progressively towards the periphery (when the eye is light-adapted). See visual field.\nlow vision Vision impairment even after correction by conventional lenses, resulting from either congenital anomalies or ocular diseases such as cataract, glaucoma, age-related macular degeneration, pathological myopia, trachoma, onchocerciasis, etc. The correction and rehabilitation of patients with low vision is achieved by special aids called low vision aids (LVA) such as a telescopic lens, and appropriate counselling (e.g. about illumination and reading distance). The criteria that the health authorities normally use to classify a person as having partial sight take into consideration not only the corrected visual acuity but also the extent of visual field loss (generally less than 20º). Syn. partial sight; subnormal vision.The World Health Organization (WHO) defines low vision as visual acuity less than 6/8 (20/60) and equal to or better than 3/60 (10/200) in the better eye with best correction. See low vision aids; blindness; bracketing; Bailey-Lovie chart; contrast sensitivity chart; clipover; deaf-blind; halogen lamp; cross-cylinder lens; telescopic lens; apparent magnification; relative distance magnification; relative size magnification; magnifier; Kestenbaum's rule; magnifying spectacles; pinhole spectacles; galilean telescope; Pepper test; typoscope; eccentric viewing.\nmesopic vision Vision at intermediate levels between photopic and scotopic vision, and corresponding to luminances ranging from about 10−3 to 10 cd/m2. Syn. twilight vision.\nmonochromatic vision Synonym of monochro-matism. See monochromat.\nmonocular vision Vision of one eye only.\nmultiple vision See polyopia.\nnear vision (NV) Vision of objects situated 25-50 cm from either the eye, or more commonly the spectacle plane. See Jaeger test types; distance vision.\nnight vision; nocturnal vision See scotopic vision.\npanoramic vision Vision of some animals whose eyes are located laterally so that the two visual fields overlap only slightly or are adjacent, thus providing vision over a much larger region of the environment than if the two lines of sight were aimed in the same direction.\nperipheral vision Vision resulting from stimulation of the retina outside the fovea or macula. Syn. eccentric vision; extrafoveal vision; indirect vision. See sensory fusion; central vision.\nphotopic vision Vision at high levels of luminance (above 10 cd/m2) and resulting from the functioning of the cones. Syn. daylight vision; diurnal vision. See duplicity theory; differential threshold.\nprotanomalous vision See protanomaly.\nred vision See chromatopsia.\nvision science The scientific study of how the visual system contributes to an understanding of the environment by processing and interpreting the light stimulation to the eye. Various disciplines contribute to vision science including anatomy, biology, optics, physiology and psychology.\nscotopic vision Vision at low levels of luminance, below about 10−3 cd/m2 and resulting from the functioning of the rods. Syn. night vision; nocturnal vision; scotopia. See duplicity theory.\nvision screener An instrument used to measure various visual functions rapidly and inexpensively. There are various models, but most are modified stereoscopes with an internally illuminated set of targets and an optical system or variable target positioning to simulate either a near or far testing distance. Most of these instruments measure visual acuity, heterophoria, fusion, stereopsis, colour vision and visual field. See photorefraction.\nsimultaneous vision See contact lens.\nsingle binocular vision (SBV) Condition in which both eyes contribute towards producing a single fused percept. See sensory fusion.\nspatial vision See depth perception.\nstereoscopic vision See stereopsis.\nsubnormal vision See low vision.\ntelescopic vision See tunnel vision.\nvision therapy; vision training See visual training.\ntritanomalous vision See tritanomaly.\ntunnel vision Vision limited to the central part of the visual field as though one were looking through a hollow tube. It may be a symptom of hysteria, malingering, the final stage of either open-angle glaucoma or retinitis pigmentosa, etc. Syn. gun barrel vision; telescopic vision. See hysterical amblyopia; visual expander field.\ntwilight vision See mesopic vision.\nWorth's classification of binocular vision For the purpose of visual rehabilitation, binocular vision is often classified into three grades: (1) simultaneous binocular vision (first-degree fusion or superimposition); (2) fusion (sensory fusion or second-degree fusion or flat fusion); (3) stereopsis (third-degree fusion). See sensory fusion; superimposition.\nyellow vision See xanthopsia.\nn 2. the category of sight in which an image is focused on an area of the retina other than the macula. Also called\nPatient discussion about vision\nQ. What age does eye sight stabilizes? I was just wondering at what age does your eye sight usually level off and stop getting worse? Any ideas much appreciated!\nIt is also the age the eye is fully grown.\nDon't worry it will not get much worse maybe about -0,75.\nQ. What can you do to make your eye sight better? My eye sight isn’t that great. Is there anyway to improve it, like eating certain foods or drinking certain drinks?\nQ. My vision is blurry and I see zigzag lines, what is it from? Every now and again, usually after sitting a few hours in front of the computer or not sleeping enough at night, I start getting blurry vision. I see zigzag lines in front of my eyes and it can take sometimes an hour to go away. What is this from?"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:0db9897c-7722-46cc-b885-94a0308c9c12>","<urn:uuid:65736685-2ca7-4c96-8064-eaf78cd23dae>"],"error":null}
{"question":"在高海拔和低海拔地区，manometer压力计的测量结果会有什么不同？How do manometer readings differ between high altitude and low altitude locations?","answer":"Manometer readings will differ between altitudes because air density varies with elevation. At sea level, the weight of air above compresses the air around us, making it denser and creating higher pressure. At high altitudes, the air is less dense than at lower altitudes, resulting in lower pressure measurements. This is why total pressure and static pressure measurements using manometers will show lower values at higher elevations compared to sea level.","context":["Actually, a manometer can measure both of these things.Because static pressure in an immobile (static) fluid is homogeneous in all directions, pressure measurements in an immovable (static) fluid are not reliant on the direction of the fluid flow itself.Pitot-static tubes, for example, are used in airplanes to detect airspeed and are used to estimate altitude.To view the complete response, please click here.\nThe manometer is attached to the duct as illustrated on the right side of Figure 11 in order to measure total pressure in the duct. Total pressure is measured using this gauge, which is the force of static pressure plus the force of velocity pressure.\nWhat is a manometer used to measure?\nIn order to measure the pressure of liquids or gases, a manometer is utilized. This sort of pressure measurement tool is often used to measure either relative pressure or absolute pressure, depending on the application. The term ″relative pressure″ refers to the difference between the external air pressure and the atmospheric pressure.\nWhat is static pressure in water manometer?\nStatic pressure is measured in inches of water column (″WC″) and is defined as the amount of pressure required to displace one inch of water in a water manometer under standard conditions.\nWhat is the principle of dynamic pressure in a manometer?\nManometers are pressure measuring devices that work on the idea of dynamic pressure to measure the difference in pressure between two points. An explanation: Manometers are pressure measuring devices that employ the principle of pressure owing to static fluid (i.e. column height) to measure the pressure difference between two points on the same line.\nHow do you measure air pressure with a manometer?\nThe Manometer is used to measure the pressure in a vessel. Pressure is defined as a force per unit area, and the most precise approach to measure low air pressure is to balance a column of liquid of known weight against it, and then measure the height of the liquid column that has been balanced against the low air pressure column.\nDoes a manometer measure static pressure or total pressure?\nA manometer is a pressure gauge that may be used to measure absolute pressure directly. The manometer in Figure 5 monitors the pressure in a sealed leg above a mercury column as a function of the pressure at the absolute zero position. The traditional mercury barometer, which is used to measure atmospheric pressure, is the most frequent type of this manometer to find.\nHow do you measure static pressure?\nThe static pressure is measured with the use of a manometer equipped with pressure probes. Early manometers measured system pressure by measuring the height of a column of water. The water was physically lifted by the air pressure, which was measured in inches, which is why static pressure is still stated in inches to this day.\nWhat does an manometer measure?\nA manometer is a pressure measuring equipment that is used to measure and display pressure. Analog manometers and digital manometers are the two types of manometers available.\nCan static pressure be greater than pressure?\nBecause Total Pressure is the sum of static and dynamic pressure, the static pressure cannot be greater than the total pressure under normal circumstances.\nWhat device measures dynamic pressure?\nPressure meters, pressure gauges, and vacuum gauges are all terms used to describe instruments that are used to measure and show pressure in an integral unit. A excellent example is a manometer, which uses the surface area and weight of a column of liquid to both measure and signal pressure in a certain space of time.\nDoes a barometer measure static pressure?\nDepending on whether the manometer is configured as a barometer, some manufacturers refer to static pressure as being the vapor pressure of mercury in an empty container. Basic to this is absolute zero pressure, which serves as a reference for measuring barometric pressure in the absence of any external pressure.\nWhat is a manometer HVAC?\nManometers are instruments that measure the pressure exerted in a fluid column; the fluid can be either liquid or gaseous in nature. In reality, in the HVAC industry, we are most often concerned with gauging gas pressure. There are several distinct varieties of manometers, each of which may be used for a particular measuring purpose.\nWhat is static and dynamic pressure?\nThe pressure that exists at a certain place in a fluid is referred to as the’static pressure.’ The’stagnation pressure’ is the pressure that a fluid would reach if it were allowed to come to a complete stop without losing any mechanical energy. The difference between the two is what is known as ‘dynamic pressure. ‘\nHow does a manometer measure the pressure of a sample of gas?\nManometers are devices that measure air pressure by employing a container with a tube that is open at one or both ends, forming a ‘U’ shape. During the operation of a closed manometer, a sample of gas is introduced into one end of the instrument, which is then sealed. Afterwards, a fluid with a known density is poured into the opposite end of the tube.\nWhat is the difference between pressure gauge and manometer?\nBetween a manometer and a pressure gauge, there are no significant differences. Pressure gauge is an umbrella phrase that refers to any and all pressure measurement instruments, whereas a manometer is one of the devices that fall under the umbrella term.\nWhat is an aneroid manometer?\nIt is a mechanical gauge that has a circular dial and a needle that revolves in order to display pressure in millimeters of mercury (mmHg). Internally, a bellows-and-spring system detects and communicates the blood pressure measurement to a dial on the front.\nWhy is a manometer inverted?\nIn order to measure pressure differences in liquids, an inverted U-tube manometer is employed. When the liquid level in the manometer is low, the space above the liquid is filled with air, which may be entered or ejected by the tap on the top in order to raise or lower the level of the liquid.\nWhat is manometer and how does it work?\nA manometer is a measuring instrument that monitors the pressure of a fluid or gas in a confined area, such as in a boiler, using a pressure gauge. A manometer is used to measure the pressure in relation to the surrounding atmosphere.\nHow does a manometer measure vacuum pressure?\nWhen a vacuum is supplied to one leg, the liquid in that leg rises while the liquid in the other leg sinks. The quantity of vacuum is indicated by the difference in height, denoted by the letter ‘h,’ which is the total of the readings above and below zero. Manometers are instruments that use this concept to measure their output.","can vary at any particular point on the Earth depending on the density of the air Air Pressure can vary at any particular point on the Earth depending on the density of the air Density = mass / volume Density = mass / volume\nWhere is air pressure higher, up in the mountains or down in the valley?\nElc&feature=related Air Pressure and Altitude\nThis bottle was photographed at 3600m (left) then again at sea level (right) We all live underneath a huge ocean of air that is several miles deep: the atmosphere. The pressure on our bodies is about the same as ten meters of sea water pressing down on us all the time. At sea level, because air is compressible, the weight of all that air above us compresses the air around us, making it denser.\nDensity Density = Mass / Volume Warm air is less dense than cool air. Warm air rises. Cool air sinks. Air at high altitudes is less dense than air at lower altitudes.\nFactors that affect Air Pressure Temperature Water Vapor Elevation\nTEMPERATURE AND AIR PRESSURE HEAT Molecules move faster Move apart, become fewer and weigh less LESS AIR PRESSURE\nHIGH TEMPERATURE, LOW AIR PRESSURE LOW TEMPERATURE, HIGH AIR PRESSURE\nAMOUNT OF WATER VAPOR consists of air and water molecules\nMore water vapor means less air molecules (more water molecules) LOW AIR PRESSURE DRY AIR = HIGH AIR PRESSURE\nAir Pressure & Weather\nHigh pressure generally means fair weather Air mass in upper atmosphere Layer of Air Warm, moist air cannot rise No clouds\nLow pressure generally means cloudy, rainy weather Air masses move apart Warm air rises, clouds form\nWeather map Low pressure system: Increased cloudiness, winds, temperatures, and chance of precipitation. High Pressure System: Indicates clear, calm conditions with reduced chance of precipitation. Drier air usually results in a greater range of high and low temperatures.\nMeasuring Air Pressure Air Pressure is measured by an instrument called Barometer Types of Barometer Mercury Barometer Aneroid\nAir pressure increases, column of mercury rises Air pressure decreases, column of mercury drops\nHigh Pressure: Rising or steady - Continued fair Slowing falling - Fair Rapidly falling - Cloudy, Warmer Medium pressure: Rising or steady - Same as present Slowing falling - Little change Rapidly falling - Precipitation likely Low Pressure: Rising or steady - Clearing, cooler Slowing falling - Precipitation Rapid falling - Storm\nFactors Affecting Air Pressure FACTORIncrease/DecreaseAir Pressure Density Temperature Water Vapor Altitude\nAt the top of a mountain you drank a bottle of water, sealed it, but imploded on your way down. Why? How does air pressure affect scuba diving? A rising barometer indicates a spell of cool dry weather. A series of hot, humid days is preceded by a falling barometer. A southern, coastal areas tend to have lower air pressure than an inland area farther north. Rapidly dropping temperatures are accompanied by a rising barometer. You are planning a Picnic and check the barometer, which is falling. Why should you cancel the picnic? Why would a serious athlete decide to train at a high altitude? You are hiking Mount Everest and find it hard to breathe at a high altitude. How does a hot air balloon work?"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:760cc8c8-0c2f-4468-beba-b76a45ebc28e>","<urn:uuid:093f8ee1-8783-49be-afc5-26eed433c767>"],"error":null}
{"question":"How do the approaches to breaking down barriers between 'high' and 'low' art compare between Keith Haring's subway works and the Un-hidden Romania street art project?","answer":"Both initiatives aimed to democratize art but took different approaches. Keith Haring directly challenged the high/low art divide by creating his chalk drawings in New York subway stations, using vacant advertising spaces as canvases for his stylized figures. The Un-hidden Romania project takes a more institutional approach, deliberately activating unusual or forgotten spaces through street art installations and creating guided routes and maps to make urban art accessible to both locals and tourists, featuring over 120 independent artistic interventions across Romanian cities.","context":["Un-hidden street art in Romania BOOK\nThe first edition of the Un-hidden Street Art in Romania book is here to help you discover independent art in public spaces.\nA5 book, softcover\na limited edition of 500 prints\nUn-hidden street art in Romania book in numbers:\n100 local and international artists\n200 places to visit\n350 photos with murals in Romania\nUN-HIDDEN STREET ART IN ROMANIA (printed edition, 2019)\nAuthors CRISTINA POPA & ANDREI RACOVIȚAN\n© Bucharest, Romania, Save or Cancel, 2019\nPhotography © feeder.ro / Save or Cancel by Petre Fall, Alex I, Anita Jambor, Dan Alexandru, Ana Moca Grama\nPhotography © Erps, Coon One, Pass, Mădălina Dobraca, Alexandra Pandrea, Alexandra Costea, Kseleqoqynqyshy, Kaps crew, Kitra, Andreea Sasaran, Obie Platon, J.Ace, Sibiu Street Art Festival, Robert Obert, Sinboy, Reptilianul\nSee the Un-hidden street art book files here: Brașov; Chișinău, Moldova; Cluj\nUpdate GIVEAWAY 2020: Answer our survey about the feeder.ro/shop and you have a chance to win a copy of the Un-hidden Street Art in Romania book. Find out how to enter the giveaway.\nWe invite you to explore new cities added on the Un-hidden Romania 🇷🇴 street art map and discover the recent works made by local and international artists:\nAito Kitazaki (JP)\nFlorenciei Duran [Colectivo Licuado] (UY)\nGraffete [Sandy Balasoiu, Irina Mocanu]\niZZY iZVNE (MD)\nJohn Dot S\nLivi Po murals & Lettering\nMy Cute Creatures aka Creaturi drăguțe\nMurivale Mureşan Vasile\nNicolas Alfalfa (VE)\nSandu Milea Lucian\nSweet Damage Crew [Recis, Lost Optics, Cage, Pandele, Boeme1]\nWanda Hutira aka Que Nais\nWaone – Interesni Kazki (UA)\nAbout the Un-hidden Street Art in Romania editorial project\nUn-hidden Street Art in Romania aims to humanise and promote the knowledge of cities in Romania and Moldova, proposing their exploration through independent urban art, with the help of an exceptional publication, available online and in print, bilingual, for locals and tourists. The book presents over 120 independent artistic interventions and information about their authors.\nThe cultural interventions Un-hidden Bucharest 2017 and 2018, co-financed by AFCN, activate unusual or forgotten spaces, such as the Omnia Hall, Lente, Marconi Cinema, the Faculty of Sociology and Social Assistance, Dizainăr, Cina building – M60 Space, next to remarkable artists – Pisica Pătrată, Sorina Vazelina, Primitiv Print, Skinny Bunny, Paul Dunca, VJ VLC, Livi Po, Kseleqoqynqyshy, iZZY iZVNE (Moldova), John Dot S, Maria Bălan, producing a guided route and a print & online map with 22 of independent artistic interventions in the public space.\nUn-hidden Street Art in Romania is a remarkable publication through which locals and tourists alike will discover new dimensions of the city through the effervescent alternative culture in development.\nThe Un-hidden Street Art in Romania editorial project is organised by Save or Cancel team, composed of Cristina Popa (random) and Andrei Racovițan (ubic), through feeder.ro and is co-funded by AFCN, thematic area: book. The program does not necessarily represent the position of the National Cultural Fund Administration. AFCN is not responsible for the content of the application or the way the program results can be used. These are entirely the responsibility of the beneficiary of the funding.\nPartners: CNDB, ZEPPELIN, IGLOO, The INSTITUTE, DIZAINAR, SISAF, AMURAL\nAbout Un-hidden Romania multi-annual program\nPisica Pătrată – Totem @ Lente\nSkinny Bunny – Painting @ Omnia Hall (CNDB)\nVJ VLC – Video mapping @ Cinema Marconi\nSorina Vazelina și Primitiv Print – Mural & screen printing @ Lente\nPaul Dunca – Guided tour (1)\niZZY iZVNE – Mural @ Faculty of Sociology and Social Assistance\nPisica Pătrată – Painting @ Cina building – M60 Space\nLivi Po – Mural @ Dizainăr\nKseleqoqynqyshy – Mural @ Faculty of Sociology and Social Assistance\nJohn Dot S – Stencil workshop @ Dizainăr\nMaria Bălan – Linocut workshop @ Cina building – M60 Space\nJohn Dot S – Mural @ Omnia Hall (CNDB)\nOpen Doors – Travel Outside the box – Guided tour (2)\nPaul Dunca – Guided tour (3)\nUn-hidden Bucharest – Street art map print version\nKseleqoqynqyshy– Mural @ Faculty of Sociology and Social Assistance\nUn-hidden Bucharest – Street art exhibition @ Romanian Design Week with Pisica Pătrată, Kseleqoqynqyshy, John Dot S, Maria Bălan\nOpen Doors – Travel Outside the box – Guided tour @ Cartierul Creativ (4, 5)\nAito Kitazaki – Mural @ Lente\nOpen Doors – Travel Outside the box – Guided tour (6, 7)\nOnline art gallery – Online store @ feeder.ro/shop\nUn-hidden Romania – Street art map online version @ feeder.ro\nUn-hidden Romania – Street art exhibition x Acuarela @ Noaptea Albă a Galeriilor with Pisica Pătrată, Livi Po, Kseleqoqynqyshy, John Dot S, Maria Bălan & Skinny Bunny\nMaria Bălan – Linocut workshop x DIPLOMA @ Cina building\nKseleqoqynqyshy – Mural @ Lente\nAbout Save or Cancel\nSince 2008, Save or Cancel is a medium of communication and propagation of the arts and culture, promoting and facilitating their role in contemporary society.\nThe self-initiated multidisciplinary programs of Save or Cancel aim to identify sustainable and adaptable opportunities for (re)valorisation of the existence through architectural, cultural and editorial projects.\nVisit the project’s page Un-hidden Romania to find out more about past, current and future activities.","Jean-Michel Basquiat - \"Hollywood Africans\"\n“HIGH CULTURE” VS. “LOW CULTURE”\nThe 1980s: Street Art Enters The Gallery\nJean-Michel Basquiat - \"Florence\"\nWhile street art’s gallery and festival visibility exploded relatively recently, its legacy as a fine art movement stretches back several decades. The most common type of street art we are used to seeing today – essentially urban graffiti-inspired art – started to be seen in galleries across New York City starting in the early 1980s. Two of the most important artists in this trend are Jean-Michel Basquiat and Keith Haring.\nBasquiat started as a street artist in the late 1970s, spray painting graffiti on buildings in Lower Manhattan under the name SAMO. He was discovered by Andy Warhol, who he later collaborated with, and broke through as a solo artist in 1981. His highly individualistic and raw style garnered him acclaim from art scenes around the world, representing the first notable entry of street art influences into fine art.\nKeith Haring - \"Tuttomondo\"\nKeith Haring first gained attention through his chalk drawings in New York subways. He would use vacant advertising backboards in the stations as his canvas, drawing highly stylized outlines of human and animal figures. Again, his friendship with Andy Warhol was crucial to his international recognition. Haring broke through internationally through his mural work, painting walls in Melbourne, Rio de Janeiro, Amsterdam, Paris, and Berlin from 1984 to 1986. Throughout his career, he sought to break down the barriers between “high” and “low” art.\nWhile artists like Basquiat and Haring are well-known in the art world, they aren’t necessarily household names, and their careers were cut short by tragic deaths – neither artist made it past 1990 alive. While these artists were instrumental in bringing street art influences to fine art in the 1980s, the 1990s represented somewhat of a gap in the story of high-profile gallery street art, with most street art remaining in the streets.\nThe 2000s: Street Art Makes A Comeback\nStreet art as we know it today is perhaps most influenced by a handful of artists that emerged on the international art gallery scene in the early 21st century. Two of the most important names that shaped this period are Ron English and Banksy.\nThe Godfather Of Contemporary Street Art\nThe popular street art of the last two decades has been heavily influenced by a mash-up of corporate symbols, graphic design, graffiti and social commentary. Ron English, a trained contemporary artist who now works in oil paintings, started what we might consider to be today’s street art aesthetic back in the 1980s. His influence grew into a global mainstream art movement with artists like Banksy during the mid to late 2000s.\nEnglish’s work is unmistakable – think of corporate mascots like Mickey Mouse and Ronald McDonald warped into subversive characters, or billboards altered to twist their message into something more sinister. Through his work, English continued the tradition of breaking down or blurring the lines of high and low culture, mixing fast-food imagery with political figures and movie stars. This type of street art, often referred to as “culture jamming” is as foundational to contemporary street art as traditional New-York-style graffiti.\nThe Mystery Artist\nBanksy is perhaps the most well-known street artist in the world, yet his identity still remains somewhat of a mystery. Banksy began as a graffiti artist in Bristol, UK in the mid-1990s, and started exhibiting pieces in 2002. His style is directly connected to the culture jamming of Ron English, fusing and juxtaposing imagery to provoke a conversation or simply make a statement.\nEarly on in his career, Banksy began exhibiting paintings that were subversive versions of classic works by great artists of the past, such as is Monet’s Water Lily Pond, which he adapted to include urban litter in the landscape. More than perhaps any street artist, Banksy’s work has featured an aggressive tension between high and low art, often taunting the fine art world with stunts such as self-shredding canvases while selling his work for large sums of money.\nBanksy is the quintessential street artist turned mainstream fine artist, and his story is the template that many contemporary artists seek to replicate for their own careers. Many street artists today hide their identity while using social media, public spaces and art installations to make statements and engage audiences.\nThroughout its often-difficult relationship with the established fine arts world, street art has emerged as a global, mainstream phenomenon that brings to light tensions between “high” and “low” culture, marginalized and dominant groups, and public versus private spaces. Thanks to the work of a few seminal and groundbreaking artists, contemporary street art has gone well beyond its beginnings as simple graffiti tags and is now a legitimate and respected artform around the world."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:dd398601-3596-43f8-abe2-194309d1008f>","<urn:uuid:55c75ea9-b908-4954-b24d-701eace7f399>"],"error":null}
{"question":"How do heating/cooling costs compare to appliance energy consumption in homes, and what strategies are recommended to reduce both?","answer":"Heating and cooling accounts for approximately 56% of home energy consumption, while appliances typically account for 20%. To reduce heating/cooling costs, homeowners should seal cracks and openings, seal heat ducts (which can reduce energy bills by 20%), maintain moderate temperatures in unused rooms, and use fans before air conditioning in summer. For appliances, choosing Energy Star labeled products is recommended as they use significantly less energy while maintaining comparable quality to similar products. While these Energy Star products may cost more initially, the energy bill savings typically offset the price difference.","context":["What Can You Do to Reduce Pollution and Conserve Energy?\nEnergy conservation and pollution reduction go hand-in-hand, as the power plants that generate your electricity often release arsenic, mercury, other metals and acid gases that threaten human health and the environment (see References 1, page 3). Putting into practice a few simple, convenient ways to conserve energy will also help you to trim your utility bills.\nChoose Eco-Friendly Appliances\nThe U.S. Environmental Protection Agency and Department of Energy established the Energy Star program to label products, such as appliances and electronics, that utilize significantly less energy, while offering comparable quality to similar products. If the product costs more than non-Energy Star products, savings on your energy bill typically make up for the price difference. Purchasing appliances with the Energy Star label will help you to cut your energy costs and the pollution that accompanies high energy use. (See References 2)\nCut Heating and Cooling Expenditures\nSave on your home's energy usage for heating and cooling. Begin by sealing cracks and openings that allow air in and out of the house. These drafts can exert the same drain on your home's energy efficiency as leaving a window open all winter. Sealing heat ducts can shave an additional 20 percent off your energy bill. Keep the thermostat set at moderate temperatures in unused rooms, focusing your heating and cooling expenditures on the rooms you use the most. In the summer, before you switch on the air conditioning, use ceiling fans, window fans or open windows to keep you cool while using less energy. (See References 4)\nSwitch to Renewal Energy Sources\nIncreasingly, homeowners have the opportunity to purchase all or part of their electricity from a supplier that uses green or renewal energy sources, such as solar or wind. Your local utility company and the EPA's Green Power Locator can provide more information about opportunities in your area. (See References 5)\nReduce, Reuse, Recycle\nWhat you do with your household wastes also determines the amount of pollution and energy use your household creates. When you purchase a product with minimal disposable packaging or reuse an item instead of buying new, you save the energy it would have taken to make that packaging or that new item. Recycling also saves energy and reduces pollution. The EPA reports that, in 2009, the 178 million metric tons of carbon emissions avoided by recycling was equivalent to removing almost 33 million cars from the road that year (see References 3, page 17). Recycling also diminishes the need for harvesting or mining virgin materials, activities that can damage the environment and cause pollution. (See References 6)\n- U.S. Environmental Protection Agency; Reducing Toxic Pollution from Power Plants: EPA's Proposed Mercury and Air Toxics Standards; March 2011\n- Energy Star: How a Product Earns the Energy Star Label\n- U.S. Environmental Protection Agency; Municipal Solid Waste in the United States: 2009 Facts and Figures; December 2010\n- U.S. Environmental Protection Agency: Reducing Energy Use\n- U.S. Environmental Protection Agency: Buying Green Power\n- U.S. Environmental Protection Agency: Communicating the Benefits of Recycling","When designing a new home or remodeling an existing one, providing maximum energy efficiency requires careful planning, research, and attention to detail. Taking the “whole-house systems” approach ensures that you have properly considered all areas affecting energy efficiency, and addresses the interaction between you, your building site, climate, as well as other components of your home. The components of this approach, the ‘Big 6,’ include:\nAppliances and home electronics\nHeating and cooling\nWindows, doors, and skylights\nAppliances and Home Electronics\nAppliances typically account for 20% of a home’s energy consumption. Appliances include washers and dryers, computers, dishwashers, home audio equipment, refrigerator and freezer, air conditioners, televisions, DVD players/TV receivers, and water heaters. To ensure that your appliances provide maximum efficiency, verify that they are ENERGY STAR-qualified. The U.S. Department of Energy publishes guidelines for selecting ENERGY STAR-qualified appliances. Click on the image below for a link to these and other products.\nProper insulation offers one of the biggest energy saving benefits. Additionally, proper insulation provides added comfort and noise reduction, improving the overall livability of your home. Proper insulation involves a good understanding as to where to insulate, and which types and quantities of insulation to use for your particular climate and type of heating/cooling system you plan to install. Check with the U.S. Department of Energy website for insulation guidelines.\nArtificial lighting accounts for approximately 10% of a home’s energy consumption. Using new artificial lighting technologies (i.e., replacing inefficient incandescent light bulbs) can reduce lighting costs by 50%-70%. Maximizing the use of natural daylight in your home via windows and skylights will further reduce the need for artificial lighting.\nHeating and Cooling\nHeating and cooling typically account for approximately 56% of the energy consumption in your home, making it the largest energy expense. Whether you are upgrading/replacing an existing system or selecting a new system, make sure that you address supporting equipment such as thermostats and air ducts, which provide additional energy savings opportunities. Work with your builder or remodeler to consider the wide variety of technologies available for heating and cooling your home.\nWater heating can account for 14%-25% of your home’s energy use. There are many energy savings water heating strategies from which to choose for your home or pool. These strategies include how hot water is used, insulation techniques, and of course, employing energy saving equipment. The U.S. Department of Energy website contains information regarding selecting water heating equipment and employing water heating energy savings strategies.\nWindows, Doors, and Skylights\nWindows, doors, and skylights can gain and lose heat in several ways: 1) direct conduction through the glass or glazing, frame, and/or door, 2) radiation of heat into a house (typically from the sun), and out of a house from room-temperature objects, such as people, furniture, and interior walls, and 3) air leakage through and around windows, doors, and skylights. When selecting new or replacing existing doors and windows, use their energy performance ratings to tell you their potential for gaining and losing heat, as well as transmitting sunlight into your home. Atrium Windows and Doors is an example of an Energy rated company. Click on the picture below for more information.\nIn conclusion, builders and designers who use this ‘whole-house systems’ approach recognize that the features of one component in the house can greatly affect other components. This ultimately affects the overall energy efficiency of the house. The U.S. Department of Energy website contains vital information on proper implementation of these 6 energy savings techniques.\n1. “The whole-house systems approach”, U.S. Department of Energy"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:c80be826-de01-440e-bb02-f77110ff0c6a>","<urn:uuid:99fd0c6f-d22b-426b-a4bc-623512098edb>"],"error":null}
{"question":"What types of rescue equipment and operations are CareFlight nurses trained to handle in the Northern Territory's Top End?","answer":"CareFlight nurses are trained in day and night land based winching operations, difficult access locations, vessel transfers, survival training and Search and Rescue operations. They use various rescue apparatus including rescue baskets, harnesses and winch stretchers.","context":["Sheridan Bell 1, Paul Campbell2.\n1 CareFlight NT Operations, 12 Lancaster Drive, Darwin International Airport, MARRARA, NT, 0812.\n2 CareFlight NT Operations, 12 Lancaster Drive, Darwin International Airport, MARRARA, NT, 0812.\nThe advent of a new aeromedical model and service in 2010 to the top end of the Northern Territory introduced nurses from a Critical Care background into pre-hospital trauma care and helicopter down the wire winching operations.\nCareFlight embraced this opportunity to enhance the capability of the traditional civilian Flight Nurse role to incorporate a service that could access injured and sick patients in difficult access environments within the Top End on the Northern Territory.\nThe implementation of this service has been both unique and exciting for the flight nursing profession within CareFlight, and is paving the way for future Flight Nursing opportunities Australia wide. This is also a major asset for the people of the Northern Territory who are benefitting from the rapid response of critically care trained medical personnel to these difficulty remote locations.\nUp until 2011, there were no health care professionals within civilian emergency services in the Top End of the Northern territory who were trained in airborne winching operations. Since this time, we have gained training and experience in day and night land based winching operations, difficult access locations, vessel transfers, survival training and Search and Rescue operation with a vast array of rescue apparatus including rescue baskets, harnesses and winch stretchers.\nAdvancing this service in the last 12 months has incorporated the rotational Retrieval Doctors being winch trained for their period of secondment to the aeromedical service. Furthering the level of care the patients receive at the scene of injury or illness.\nSince the commencement of this service, multiple missions have been attended both on land and off shore. To explain this further, a winching mission to famous Gunlum Falls in Kakadu National Park will be explained.\nThis non contractual service that is partially funded and supported by the NT Government could not exist without the kind and generous fundraising and donations of the people of the NT and Australia wide. With big supports like Territory Insurance Office (TIO) and Darwin Mining Club (DMC), leading the way in operation support.\nAccess and Delivery of Emergency Care in Austere, Rural and Remote Areas is something we do in the Northern Territory day in day out. We love what we do, we love where we do it, we love who we do it with and most of all we love who we do it for- the people and visitors of the NT. With these advancements in the Flight Nurses professional role and capability, the future I only looking increasingly advanced for our excitingly unique and specialized profession.\nPaul Campbell and Sheridan Bell are Critical Care Flight Nurse’s working within the CareFlight Aero-medical Contract for the Top End of the Northern Territory of Australia. Both Paul and Sheridan are Down the Wire qualified Nurses, and are passionate about their chosen profession as Flight Nurses and paving the way for future operations and advancements. With multiple years of experience, Paul and Sheridan are part of an elite small group of nurses that on a daily basis work within difficult access locations across the NT, accessing areas by fixed wing and helicopter air frames, providing rapid response emergency critical care. Both love their profession and are proud to call The Top End their home."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:3e7b5b0d-a950-4248-8fed-610c7edc60c0>"],"error":null}
{"question":"Compare data flow process: Git developer workflow vs Azure real-time pipeline","answer":"A Git developer workflow involves developers making local changes through branch/commit/merge, publishing to public repos via push commands, and other developers fetching/merging those changes - creating an asynchronous flow. The Azure real-time pipeline works differently: data streams continuously from sources through Azure Event Hubs, gets transformed by Stream Analytics using SQL queries, and is delivered to Cosmos DB with minimal latency - forming a continuous, automated flow. Git's process is manual and coordination-based while Azure's pipeline is automated and real-time.","context":["Julius B. Lucks/Projects/Python Articles/Using Git\n- 1 Introduction\n- 2 Repository setup\n- 3 Project Workflow\n- 4 Git Shortcuts\n- 5 References\nThis document aims to describe how to set git repositories in a distributed mode of development. As such, it draws heavily on , with extra notes for project-specific configuration. Specifically, we want to create a repository structure/flow cycle modeled after , where there are two developers, each with a personal (private) repo, and a public (shared) repo.\nFor more information about working with git, including viewing logs, diffs, making patches. etc. - see .\nDistributed SCM Structure with 2 developers\nUnlike centralized SCM (cvs,svn) in distributed SCM, there are more than one repository, each no more special than another. In centralized SCM, there is one master repository which people sync with, ofter only permitting write access to special users. Below we will outline how to work with a distributed system shared by two developers, each with a local and public repo\nyou (Developer A): local repo, public repo them (Developer B): local repo, public repo\nThe general practice is that the developers work with their local repositories, much like working with a locally checkout out copy in cvs. When changes are worth reporting, they sync this local repository with their public repository. The public repository is accessible by the other developer which then updates their local repository with changes from your public repository.\nThe main difference is that you are dealing with whole repositories, not just checked out copies. This lets you have, locally, all the power of committing, branching, merging, etc., without having to be in communication with a central server. This is not only extremely beneficial when developing without an internet connection, it also allows you to try out code and make progress without conflicting with others. You can also perform many incremental commits as we will see later, without worrying about corrupting the main repository.\nThe distributed nature of the repositories also means that the repo is in many places causing increased speed and automatic data redundancy.\nDistributed SCM Project Flow with 2 developers\nBelow we describe the steps necessary to setup and interact with this repo structure. A typical project flow consists of you making changes, pushing (updating) them to your public repo, the other developer pulling those changes from your public repo, and merging them into their local repo, according to the following diagram :\nyou push your local repo ----------------------> your public repo ^ | | | | you pull | they pull | | | | | they push V their public repo <------------------- their local repo\nWe also discuss why this particular setup makes a lot of sense.\nEach developer is assumed to have\n- git installed on a local development machine\n- access to a machine accessible via http and ssh with git installed\nBelow we outline the steps necessary to setup the configuration of repos noted in 1.1 above.\nSetup the first local git repo\nAt this point, Developer B is not involved. Developer A follows the standard git repo setup on their machine :\n$ mkdir dev_a $ cd dev_a $ git init\nNow either files are imported or created into dev_a, and the initial commit is created\n$ #add some files $ git add . # Add the new files to the git index $ git commit # Commit the changes to the index to the object database # Specify a commmit message (usually \"initial import\" at # this point)\nIf you execute\n$ git branch\nYou should see only one branch (* master - the star indicating that you are in the master branch). Even though there is minimal content in the repo, we have enough to set up Developer A's public repo.\nSetup the public version of the first repo\nNow we need to make a public version of this repo to put on a web-accessible server. Outside of the dev_a repo, make a cloned, bare repo  (a bare repo is just the contents of the .git directory, with none of the project files checked out):\n$ git clone --bare dev_a/ dev_a.git $ touch dev_a.git/git-daemon-export-ok\nNow package this up and put in on the server. We will use tar and scp to do this. To make things easier, now and for development, put your id_rsa.pub public key inside of .ssh/authorized_keys on the server so you don't have to type your ssh password each time you login. Also make sure that 'PermitUserEnvironment yes' is enabled in /etc/sshd_config, and you have the correct PATH variable set in .ssh/environment (with PATH=$PATH:path/to/git) so that git will be found when ssh is used. Once this is setup\n$ tar czvf repo.tar.gz dev_a.git/ $ scp repo.tar.gz firstname.lastname@example.org:\nNow put the repo in a web-accessible location\n$ ssh email@example.com $ cd ~/public_html #(~/Sites on a Mac) $ mkdir git $ cd git $ tar xzvf ~/repo.tar.gz\nYou should now see the repo dev_a.git unpacked. Now tell git about it\n$ cd dev_a.git $ git --bare update-server-info $ chmod a+x hooks/post-update\nYou are now ready to begin using the public repo.\nDeveloper B clones the public repo for their local repo\nNow Developer B enters the story. We want Developer B to clone the public repository of Developer A initially so they are on the same page. Developer B should then execute\n$ git clone http://server.domain/~developer_a/git/dev_a.git dev_b $ cd dev_b $ ls\nDeveloper b should now see the full repo. Doing\n$ git branch\nShould show one branch (* master) as before.\nDeveloper B creates a public repo\nDeveloper B creates a public repo in the exact same way as in 2.2. We should now have the repo structure as depicted in 1.1.\nTo carry out the workflow depicted in 1.2., we will need a few more git commands. To start out, we will play around with our local git repo, assuming we are just a solo developer. From there it will be a small step to having a the workflow between Developer A and Developer B.\nUsing Git as a Solo Developer\nGit makes creating and merging branches really simple, so the recommended practice is to create a new local branch for any work you want to do. To see the branches we have, we do\n$ git branch * master\nThere is only one line of output indicating that we only have the single (default) branch, master. The * beside it indicates that we are in the master branch. Let's create a branch called add-new-feature (remember this is a local branch, so we don't need to worry about other people understanding the branch names, or branch name conflicts. No one can see them but us!)\n$ git branch add-new-feature $ git branch * master add-new-feature\nWe now see another branch, but we are still in the master branch. To move into the other branch we do\n$ git checkout add-new-feature $ git branch master * add-new-feature\nWe can now work in this local branch. Let's say we want to add a file called TODO.txt. We first create the file, then add it to git's index using git add\n$ echo 'Feature TODO file' > TODO.txt $ git add TODO.txt\nTo commit this change, we do\n$ git commit\nAnd add a commit message . Suppose we now want to add a line to TODO.txt:\n$ echo 'Write documentation' >> TODO.txt\nWe need to tell git about this by using git add again.\n$ git add TODO.txt\nUnlike cvs or svn, the add command is used to add ANY changes, not just new files. This is because git does not track files, it tracks content. By using git add, we are telling git we have added content. Now we need to commit this content in the same way\n$ git commit\nIf this seems like a lot of commands, there are many shortcuts which we will go over later. Now suppose we are done with our new feature, and it works, so we are ready to merge it into our master branch. Once all changes are committed, we can switch to the master branch, and merge in our changes from the add-new-feature branch\n$ git checkout master $ git merge add-new-feature\nIf all went well, there will be no conflicts. If there were conflics, we can resolve them and commit - see . We can be done with the add-new-feature branch, so we delete it with\n$ git branch -d add-new-feature $ git branch * master\nBranches are so easy and convenient that we can create a bunch of them for all the aspects we are working on - sub projects, documentation, etc. In this way, git's fine-grained control over commits and branches allows us to keep relevant work in a series of commits with many things going on at once.\nCommunicating your changes with Developer B with git push\nSuppose we want Developer B to see this new feature. We need to push our new master branch into our public repo. We can do this with the git push command\n$ git push ssh://server.domain/~developer_a/git/dev_a.git master:master\nThe url after git push gives the location of our public repo. The last part indicates that we want to push changes in our master branch, onto the master branch of our public repo.\nNow Developer B needs a way to get those changes.\nGetting changes from Developer B with git fetch\nThis step is very similar to that in 3.1., except instead of creating a branch from the HEAD of our master branch, we will create a branch from the public repo of Developer B\n$ git fetch http://devb_machine.domain/~developer_b/git/dev_b.git master:dev_b_changes $ git branch * master dev_b_changes\nThe last part of git fetch tells us to create a branch called dev_b_changes from the master branch of Developer b's public git repo. We can then checkout dev_b_changes in the same way to see what they have been up to, or we can merge the changes with\n$ git merge dev_b_changes\nOnce we are done, we can remove this new branch with git branch -d\nThe 2 Developer Development Cycle\nThe development cycle outlined in 1.2. then consists of\n- making local changes with git branch, checkout, add, commit, merge\n- publishing changes to the public repo with git push\n- fetching changes from Developer B with git fetch\n- merging these changes with git merge\nMulti-Developer Distributed SCM\nAdding another developer just means that developer then has local and public repos. When changes are ready to be fetched, Developer C just notifies Developer's A and B that they should check out the updates. Alternatively, a developer can periodically check out what the other two are doing with repeated git fetch calls. This can be done by greating branches for the two other devolopers. Suppose we are Developer A\n$ git fetch http://dev_b_repo_path master:dev_b_master $ git fetch http://dev_c_repo_path master:dev_r_master\nAfter sometime, to check for updates, Developer A can do\n$ git fetch --all\nWhich will update these branches. We can then git checkout and git diff to see what has been going on. Of course you don't need to keep track of what everyone is doing. You can make git work like svn or cvs by having some acknowledged master copy with only certain developers having write (push) access. The cool thing about git is that in principle no repo is more special than any other - it is up to the community to decide which repo to trust. This could mean that someone is considered the project maintainer in which case their repo might be the one to trust. In this case, the maintainer pulls from developers when they want to submit changes to the main line of code.\nThe distributed structure of git allows teams to work on different aspects of the project without interfering and coordinating with eachother. For example, one team might be bug fixing for a release, while others are experimenting with new features. Each team pushes and pulls within eachother so that they don't have to worry about conflicts. The maintainer can decide when larger project coordination should happen.\nIt is recommended that you use the long form of the commmands as stated above until you understand what git is doing, and how it is different from an SCM system you may be used to. Once you are comfortable, you can use the command short forms below to save some typing.","Build real-time data pipelines with Azure Event Hub, Stream Analytics and Cosmos DB\nAzure Cosmos DB is a low-latency, highly available and globally distributed NoSQL database, that Microsoft recommends as a database platform for high-velocity data from IoT devices. However, real-time data ingestion requires services like Azure Event Hubs, Azure IoT Hubs to serve as a gateway for streaming data. How do you build a real-time data ingestion pipeline from Azure Event Hubs into Azure Cosmos DB, with minimal coding?\nOverview of Azure Cosmos DB, Azure Event Hubs and Azure Stream Analytics\nAzure Cosmos DB is Microsoft’s managed NoSQL service, which supports non-tabular data models, like JSON, key-value, graph, and column-family type documents. It provides globally distributed, scalable, and low-latency data service that can serve high-concurrency and fast applications. Azure Cosmos DB guarantees low-latency read and writes, which allows building flexible near-real-time applications, based on the network of connected databases around the world (you can read more about Azure Cosmos DB here).\nAzure Event Hubs is a big data streaming platform and event ingestion service. It’s one of the recommended services to receive, transform and redirect fast-arriving streaming data (see here for more details). Although Event Hubs can direct its output into few Azure services, Azure Cosmos DB is not among its outputs. Therefore, some intermediate service needs to be built, to serve as a bridge between these two services, and Azure Stream Analytics can be used in this role. What makes Azure Stream Analytics attractive as the integration component for the streaming data, is its powerful SQL-based query engine and its ability to write into multiple Azure services, including Azure Cosmos DB, Azure Synapse Analytics, Azure SQL DB, storage, etc. (see this article for more details).\nIn this tip, we will build a solution that ingests the Twitter feeds from Twitter API’s into Azure Event Hubs, and then deliver them into Azure Cosmos DB, using Azure Stream Analytics.\nCreate a data ingestion pipeline into Azure Event Hubs\nI’ll use the TwitterClientCore application, described in this Microsoft article to ingest data into Azure Event Hubs. This application listens to the Twitter feeds related to a configurable list of keywords, like Microsoft, Azure, Skype, etc.\nPlease create the Azure Cosmos DB, Event Hubs, Azure Stream Analytics and Twitter accounts and follow the steps in this Microsoft article, to configure, compile and start the TwitterClientCore application.\nOnce the application started, it will receive the feeds and display the number of events sent to the Event Hubs, as follows:\nAt this point, we will be able to observe incoming traffic on Event Hubs’ dashboards:\nConfigure Azure Cosmos DB container\nOpen your Azure Cosmos DB account and add a container, using the Add Container button:\nProvide the database and container names and partitioning field. Notice that the partitioning field should include the name of the existing column from the data source (I have selected the lang column, which is part of Twitter feed payloads). Select the Analytical store option, which would allow us to analyze the data from Synapse Analytics in future (see Explore Azure Cosmos Databases with Azure Synapse Analytics for more info on this option). Here is the screenshot with the container settings:\nConfigure Azure Stream Analytics\nThe Azure Stream Analytics job requires an input, an output, and a SQL query to transform the data.\nLet us start by creating an Event Hub input. Open the Azure Stream Analytics account, navigate to the Inputs tab, and add a new Event Hub input:\nProvide the input name (twitter-eh in my example), select Event Hub namespace/name and the policy with the Send privileges. Ensure that the GZip option is selected, as the compression method. Here is the screenshot:\nNext, navigate to the Outputs tab, add the Cosmos DB output:\nSelect the Cosmos DB account name, database and provide the container name we created earlier, as follows:\nNext, navigate to the Query tab, add the following query (replace the input/output names, to match your endpoint names):\nSELECT * INTO [cosmosdb-output] FROM [twitter-eh] PARTITION BY [lang]\nNotice that we have included a partitioning column, as the Cosmos DB container requires this parameter. The Input preview table in the center of the screen, will show a sample data from the Event Hub:\nTest the query and save it, using the buttons, illustrated in Figure 9.\nNext, navigate to the Overview page, and start the job:\nThe job status will be shown, under the Start button. It will take a few minutes for the job to move from the Starting stage to the Running stage and after that, you will be able to see the execution stats in the Monitoring pane charts, as follows:\nNow we can check the data in Cosmos DB container. Let us navigate to the Data Explorer tab on the Cosmos DB account page, expand the database/container names and review some of the recently uploaded items:\n- Read: Welcome to Azure Cosmos DB\n- Read: Quickstart: Create a Stream Analytics job by using the Azure portal\n- Read: Azure Cosmos DB output from Azure Stream Analytics\nAbout the author\nView all my tips\nArticle Last Updated: 2021-03-23"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:b160b10a-d661-4469-b723-abe1b5be9784>","<urn:uuid:e459b088-6753-4916-a33f-56e5f244c883>"],"error":null}
{"question":"Could you explain what General Average is in marine cargo insurance and why it matters for shippers?","answer":"General Average is a unique concept in marine insurance where all cargo owners must share in the loss if some cargo is voluntarily sacrificed to save the voyage (like jettisoning containers to stabilize a ship in a severe storm). Even if your cargo wasn't lost, you're responsible for sharing the costs. Marine insurance can protect against this situation and help avoid additional expenses in retrieving your cargo.","context":["10 Common Questions About Importing Your Goods via Ocean Freight\nHere are the 10 most common questions we hear (and answers) about shipping via ocean:\n1. How long does it take to receive my cargo?\nMany factors can influence shipping times: for example, the origin and destination of the shipment, or if you’re shipping a less than container load (LCL), etc. Generally, however, if you are importing goods from one of the main ports in China to Los Angeles or Long Beach, you’re looking at approximately two weeks on the water.\nHowever, don’t expect your goods to be in your hands within 14 days. Once your goods arrive at port, they must be unloaded from the vessel, become available for pickup and then be delivered to your door (that is assuming they already cleared customs).\nShipping LCL adds additional time to the overall transit. This is due to the extra steps of consolidating your shipment with cargo from other shippers at the origin and then deconsolidating it at the destination. These steps occur at a specialized warehouse called a container freight station (CFS).\n2. Why do rates change?\nThere are a few reasons the shipping rates of a freight forwarder can change:\nGRI: This stands for General Rate Increase, which, simply put, is when ocean carriers raise rates in a particular trade lane. While GRIs can take place throughout the year, you can typically count on one happening on May 1st which marks the beginning of a new 12-month “contract season” between shippers and carriers.\nSupply/Demand: Ocean shipping rates increase during certain times of the year, especially when demand for space is high. One of these times is referred to as the “peak season.” The traditional peak season roughly spans July through October and coincides with the rush of cargo ahead of the holiday shopping season. Since retailers are looking to have products on shelves for Black Friday, shipping rates will tend to increase in the preceding months.\nThe Lunar New Year (Chinese New Year) holiday is another period that is impacted by supply and demand. Many countries in Asia shutter factories for two weeks or longer to celebrate. As a result, the weeks prior to the shutdown see a surge in cargo and rates. The Lunar New Year start date varies each year but occurs between late January and February.\nCost of Oil: Just like the cost of gas for our cars, a shipper’s rate may increase or decrease due to the cost oil. Changes are reflected in the bunker adjustment factor (BAF). Bunker fuel is the term for the fuel oil used in vessels.\nThe best way to stay on top of rate fluctuations is to work with a reliable international freight forwarder, who can keep you informed on market conditions.\n3. Do I need insurance?\nShort answer: It’s always recommended.\nLong answer: While you don’t need marine cargo insurance and aren’t required to buy it, having it is usually a good investment. Just as with home or car insurance, you’re protecting yourself from potential damages or losses that may occur. Specifically, marine cargo insurance can cover damage, loss, theft, non-delivery, etc., while your goods are in transit.\nOne thing that makes marine insurance different than your home or car insurance is a concept called “General Average.” A general average scenario occurs when some cargo is voluntarily sacrificed in an effort to save the voyage. This could come in the form of jettisoning some containers to stabilize the ship in a severe storm. General Average states that all cargo owners are responsible and will share in the loss (even if your cargo wasn’t actually lost). Marine insurance can protect you against a general average situation and avoid the additional expenses with retrieving your cargo.\n4. Wouldn’t it be easier to allow my supplier to handle the shipment?\nYes and no. While it may be “easier” by giving your supplier the responsibility, there are many disadvantages. One issue is compliance. For example, who will be filing your Importer Security Filing (ISF)? Ultimately, the ISF importer will be responsible for any mistakes or liquidated damage penalties for noncompliance.\nAnother issue is control. If your supplier is handling their shipments, that means they’re working with their freight forwarder to handle the transit process. That freight forwarder may not provide you with the ability to track your shipments, or even give you updates on your cargo. Communication problems with overseas forwarders can make scheduling deliveries to your warehouse difficult to coordinate and lead to unnecessary demurrage charges.\nWhat if you have more than one supplier? Keeping track of each different shipment can be a logistical nightmare in that case too.\nAt first, letting the supplier handle your shipment may sound appealing. However, as your business grows, it starts to become more of a burden than an asset. That’s why experts recommend that you control the shipping process by using your own international freight forwarder. They can help uncover more efficient ways of shipping such as building consolidations from multiple suppliers and offer tools to track every step of the shipping process.\nWant to learn more? Check out the other pros and cons of letting your supplier handle shipments here.\n5. Should I ship via air or ocean?\nA better question would be: How soon do you need your cargo? If you need it as soon as possible, air freight is a far faster shipping option than ocean freight. However, that speed comes with a cost — shipping rates for air freight are significantly higher. In most all cases, ocean freight will be the most cost-effective mode.\nThere are exceptions, of course: If your cargo is less than 100 pounds, shipping via air is often more cost-effective. Also, if your goods are perishable or sensitive (e.g., flowers or medicine), air freight is often the best option. Additionally, high-value merchandise may be better suited for air freight because of concerns over damage, theft, or the time value of money.\nEnvironmental impact may be another factor to consider. The carbon footprint of shipping via air freight is massive compared to ocean.\nBefore you start shipping via air, find out everything your freight forwarder wants you to know about air freight.\n6. How many pallets fit in a container?\nDepending on the pallet size, there are roughly 9 to 11 pallet spaces in a 20’ container and roughly 21 to 25 pallet spaces in a 40’ container.\nTo learn more about fitting pallets in a container, check out our chart on container dimensions.\n7. Is there a weight limit?\nYes. There are different weight limits depending on how you are shipping. For example, ocean freight usually has less weight restrictions than domestic freight. That’s because trucks can only carry so much weight; not to mention, state and federal laws govern how much a truck can carry on U.S. roads. The weight restrictions for trucks can range from 38,000 pounds to 44,000 pounds, depending on container size and other state restrictions. Tri-axle chassis are used for heavy loads.\n8. Why do I need to fill out a power of attorney (POA)?\nThere are two main reasons a POA for import shipments is required. If you hire a customs broker, they need to have the authority to conduct Customs business on your behalf.\nAlso, you will need a POA if your customs broker or international freight forwarder submits your Importer Security Filing (ISF) to U.S. Customs. The ISF must be submitted 24 hours before being loaded onto a U.S.-bound vessel, which gives time for customs to screen your cargo for any safety and security concerns. If the ISF doesn’t get to customs on time and accurately, you can face fees and penalties.\n9. When should I ship FCL vs LCL?\nThe larger your shipment is, the more likely you’ll want to ship full container load (FCL) to help reduce landed cost, potential handling damage, and receive your cargo faster.\nIf your shipment is less than 15 cubic meters (CBM), it will be more cost effective to move it as less than container load (LCL) cargo.\nYour international freight forwarder can help analyze market rates for your shipment to determine the breakeven point for shipping less than container load (LCL) or FCL.\n10. Why are the commercial invoice and packing list important?\nThey’re required by customs. Breaking it down, however, these two items are important for different reasons:\nCommercial invoice: Like other types of invoices, the commercial invoice describes the transaction happening between the exporter (your supplier) and importer (you). It lists your goods and the price you paid your supplier. Details on the commercial invoice will be used to determine the duties and taxes applicable to your shipment.\nPacking list: At first glance, the packing list may look similar to the commercial invoice. However, where the commercial invoice focuses on item prices, the packing list focuses on the physical count and breakdown of the related shipment. For example, a packing list would include the size, weight, and count of individual boxes/cartons matching a corresponding commercial invoice. Therefore, the packing list can be used in insurance claims to identify losses or by Customs when inspecting cargo or by your warehouse to reconcile what was expected vs. what was actually received.\nDo you have more questions about importing your goods via ocean? Contact us today to get answers to your ocean freight questions."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:fdcf810c-a017-4b36-9722-6c2a877d1c62>"],"error":null}
{"question":"从技术角度来看，蓝纹奶酪的发酵过程和城市环境中的生态系统服务有什么共同点？🤔","answer":"Both processes involve controlled environmental conditions and biological interactions that benefit humans. In blue cheese production, specific temperature and moisture conditions in caves allow beneficial molds (Penicillium) and bacteria to develop over 3-6 months, creating distinctive flavors through internal development from the inside outward. Similarly, in urban environments, the presence of natural elements creates controlled biological interactions that provide measurable benefits. Research shows these green spaces offer essential ecosystem services, improving mental functioning, reducing fatigue, and decreasing aggressive behavior, much like how the controlled environment of cheese caves produces beneficial outcomes.","context":["How Blue Cheese Is Made and Why Is It Blue?\nHow Blue Cheese Is Made and Why Is It Blue? Blue cheeses are all made from cow’s milk except from the famous French Roquefort cheese, which comes from ewes’ milk. The blue in blue cheeses is essentially blue mould. But it is extremely good for you. What happens is that an organism similar to penicillin is added to the milk or curd used to make ordinary white cheese. The mold, during three to six months of ripening, grows either in small, irregular natural openings in the cheese or in machine-made perforations, depending upon the type of cheese.\nThe small amounts of mold reproduce or spread to give the typical blue streaks in the white cheese, and, once ripened, the distinctive flavor. Blue cheeses are usually heavily salted to add to the flavor. The smell of this food is due both to the mold and to types of bacteria encouraged to grow on the cheese: for example, the bacterium Brevibacterium linens is responsible for the smell of many blue cheeses, as well as foot odor and other human body odors.\nBlue cheese is a general classification of cheeses that have had cultures of the mold Penicillium added so that the final product is spotted or veined throughout with blue, or blue-grey mold and carries a distinct smell, either from that or various specially cultivated bacteria. Some blue cheeses are injected with spores before the curds form, and others have spores mixed in with the curds after they form. They are typically aged in a temperature-controlled environment such as a cave. Blue cheese can be eaten by itself or can be spread, crumbled or melted into or over foods.\nThis cheese is believed to have been discovered by accident when cheeses were stored in natural temperatures and moisture-controlled caves, which happened to be favorable environments for many varieties of harmless mold. It was moist in the cave so the mold would form. According to legend, Roquefort was discovered when a youth, eating a lunch of bread and ewes’ milk cheese, abandoned his meal in a nearby cave after seeing a beautiful girl in the distance. When he returned months later, the mold (Penicillium roqueforti) had transformed his cheese into Roquefort.\nIt is often claimed Roquefort was praised by Pliny the Elder in AD 79. However, in the text, Pliny speaks of a cheese from Gaul, without mention of origin or even specifying that it was blue. This story was promoted by the ‘Société des Caves. Today this natural mold is refined and used for almost all blue cheeses. The mold culture is simply added to the cheese milk. For the cheese to turn blue, oxygen must be inserted into the cheese through thin needles or skewers. The blue mold then matures inside the air tunnels, developing flavor as it ages. Most mold cheeses take three to six months to mature. In blue cheese, this happens uniquely from the inside outward.\nGorgonzola is one of the oldest known blue cheeses, having been created around 879 AD, though it is said that it did not actually contain blue veins until around the 11th century. Stilton is a relatively new addition becoming popular sometime in the early 1700s. To fill the demand for Roquefort-style cheeses that were prohibitive due to either cost or politics, many varieties of blue cheese originated subsequently, such as the 20th century Danabluand Cambozola.","We are missing an important value of the urban forest, intangible social services\nAs managers and stewards of the state’s urban forests we often not acknowledging the important social values. We have a responsibility to not only provide technical assistance in the conservation and restoration of the urban forest, but to better represent and promote all of its values. During the past decade we have made significant progress in using a variety of models to document the role that our urban forests play in providing clean air and water, reducing stormwater flows, reducing energy demand and sequestering and storing green house gases. However, there is a growing body of research that suggests that we are missing this equally important set of values (Vejre et al. 2010).\nThere is a deep and abiding connection between people and plants. It can be traced back to the earliest periods of human existence. Today’s sprawling metropolitan areas can leave residents detached from nature. This can lead to feelings of stress and alienation, both of which are pervasive in our contemporary urban society.\nWhat the research says might surprise you\nFor instance, work and study often require long periods of directed attention that lead to fatigue. Fatigue can result in feelings of anxiety, irritability and an inability to concentrate. Rachel and Stephen Kaplan have conducted research on the restorative properties of encounters with nature in urban areas. They found that such interaction counteracts these symptoms of fatigue. They also conducted a survey of office workers with and without a view of a natural setting from their office window to determine rates of illness and worker satisfaction. Those workers without the nature view reported 23% more illness. Workers with the nature view indicated higher levels of job satisfaction, were less frustrated, more patient, and felt more enthusiastic (Kaplan 1989).\nThe physical environment has a well-documented impact on human aggression. Crowding, noise and high temperatures all contribute to levels of violent behavior. Some researchers believe that these environmental conditions lead to inattentive, irritable and impulsive behaviors that are all associated with violence. Contact with nature has been demonstrated to reduce the level of these behaviors and the incidence of aggression and violence in cities (Kuo and Sullivan, 2001). These same researchers found that green settings relieve the symptoms of Attention Deficit Hyperactivity Disorder in children. Children were shown to more successfully refresh their ability to pay attention by playing outdoors in green spaces.\nWhere to find more information\nIn light of these studies, and others far to numerous to summarize here, the USDA Forest Service – Urban and Community Forestry Program in partnership with the University of Washington have developed a web site, Green Cities: Good Health, which provides an overview of the scientific evidence of human health and well-being benefits provided by urban forestry and urban greening. The web site provides access to the nearly 40 years of research documenting how the experience of nature is profoundly important to human functioning, health and well-being.\nWhile civic leaders may intuitively accept that urban nature is important for public health, this web site presents supporting scientific evidence, confirming intuitions and expanding our knowledge. This science-based evidence can have a significant impact on public policy decisions regarding urban forestry, just as the science-based evidence that urban forest play a direct role in urban air and water management has had over the past decade. Given the consistent expansion of metropolitan areas in our state every bit of nearby nature has the potential to benefit hundreds to thousands of people daily. When we speak of ‘green infrastructure’ and its backbone – the urban forest – we should no longer be limiting our conversation to the bio-physical values but also to the social and physiological benefits that they provide to the vast majority of people living in our nation today.\nGreen Cities: Good Health University of Washington/U.S. Forest Service\nKaplan, R., & Kaplan, S. (1989). The experience of nature: A psychological perspective. New York, NY, US: Cambridge University Press.\nKuo, F.E., and W.C. Sullivan. 2001. Aggression and Violence in the Inner City: Effects of Environment Via Mental Fatigue. Environment and Behavior 33(4):543-571.\nVerje, H., F.S. Jensen, and B.J. Thorsen. 2010. Demonstrating the importance of intangible ecosystem services from peri-urban landscapes. Ecological Complexity 7(3):338-348."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:064a9887-d995-4132-ace4-7b2cc0e53725>","<urn:uuid:85d2466a-f23e-429d-8371-95f1cd718bac>"],"error":null}
{"question":"Could you analyze the defensive strategies employed at both Vicksburg and New Orleans during the Civil War? Compare and contrast their effectiveness.","answer":"At Vicksburg, the Confederate defenses relied on natural advantages combined with military fortification. The city sat on a 200-foot bluff with heavy fortifications including trenches, forts, redans, and artillery positions. The marshy approaches were further secured by Confederate snipers and saboteurs. In contrast, New Orleans' defense strategy, designed by Beauregard, focused on the Mississippi River as the city's vulnerable point. It relied on Forts St. Philip and Jackson, a river chain barrier, fire rafts, and gunboats. However, New Orleans' defenses proved inadequate due to lack of proper armament, unfinished ironclad ships, and insufficient troops. While Vicksburg withstood multiple Union attempts and required a lengthy siege before falling, New Orleans fell relatively quickly to Farragut's fleet, which managed to pass the forts with minimal casualties (fewer than 200 Union losses) in just 25-30 minutes.","context":["The Background of Vicksburg Campaign:\nIn Vicksburg Campaign, Vicksburg, sitting on the Mississippi River, was the “Gibraltar of the Confederacy.” Lose it, and the Confederacy was cut in twain. Hold it, and the South had access to the grain and men of the lower South and the West. Or, in Lincoln’s words: “We may take all the northern ports of the Confederacy and they can still defy us at Vicksburg. It means hog and hominy without limit, fresh troops from all the states of the far South, and a country where they can raise the staple without interference.”\nWith the Federals working their way down from the north, and already occupying the river’s southern outlet at New Orleans, a siege of Vicksburg was almost inevitable. But taking Vicksburg was an imposing challenge. Its big guns kept the river clear of Union blue, and the city’s marshy landward approaches were tough-sledding at best for an attacker, and were made even more perilous by Confederate snipers and saboteurs.\nIn 1862, Admiral David Farragut tried repeatedly to capture Vicksburg along the Mississippi, but the big guns chased him back. General Grant tried the landward approaches and had no better luck. With General Sherman defeated at Chickasaw Bluffs (in December 1862) and Confederate cavalry ravaging Grant’s communications and supply lines, the Federals were forced to withdraw. But in 1863, Grant came again. His plan this time was to skirt the big guns through elaborate engineering works— building a canal, diverting the river—all of which came to naught.\nBut in March 1863, Grant decided on a bold maneuver. He would march his men down the Louisiana side of the river, cross it well south of Vicksburg and then swing round and attack—marching all the time through enemy territory. Rear Admiral David Porter, meanwhile, would reinforce his gunboats and, in a perilous gamble, run them by the fortress at night. On 16 April and 22 April 1863, Porter made his two daring passages under the guns of Vicksburg with spectacular success. The air was lit with fire and flame, but his losses were minimal. The focus now shifted to Grant.\nThe Vicksburg Campaign:\nGrant’s plan was to isolate Vicksburg, marching first to Jackson, Mississippi, cutting off the citadel’s line of retreat and source of supply, and then invest the rebel fortress. Starting with 50,000 troops (a number that would grow to 77,000), divided into five corps, Grant faced 30,000 Confederates who were strung out defending too many points with too few men.\nOf the major battles of this campaign, four deserve attention. The Battle of Port Gibson (1 May 1863), conducted one day after Grant landed his men across the Mississippi River at Bruinsburg, led to the Federals achieving a foothold twenty-five miles south of Vicksburg. The Confederates had mounted a gallant defense, but had been outnumbered three to one. At the battle of Raymond (12 May), near Jackson, the bluecoats again outnumbered the Confederates three to one (and in artillery seven to one) and again the Confederates withdrew but only after a stiff fight. At the Battle of Jackson (14 May), Grant marched into Mississippi’s capital city, which Confederate general Joseph E. Johnston had decided to abandon, and destroyed its railways and industry.\nThe climactic battle before the siege of Vicksburg Campaign was at Champion Hill (16 May) twenty miles east of the city, where 32,000 Federals, under General Grant, collided with 22,000 Confederates, under General John C. Pemberton. In a hard fought contest, a Confederate counterattack nearly reached Grant’s headquarters. Saving the day for the Federals was General Marcellus Crocker (leader of “Crocker’s Greyhounds”) who threw two brigades of bluecoats against the rebels and saved the Federal line. The Confederates retreated to Vicksburg.\nGrant first tried to take Vicksburg by storm. But he underestimated the strength and stubbornness of the Confederate defenders. After five days of knocking Federal troops against the Confederate wall, Grant decided to shell and starve the Confederates instead, while continuing offensive probes and calling for reinforcements. The shelling of Vicksburg was so intense that its citizens dug a system of caves in which to live.\nBy July Confederate General Pemberton realized the game was up. He had been waiting for reinforcements from General Joseph E. Johnston, who despite having orders and reinforcements to save the city, would take no such risk. Pemberton wanted to fight his way out, but his subordinate generals thought this quixotic, given how ill fed and badly outnumbered the men were. Pemberton conceded this and on 4 July 1863, nearly 30,000 Confederates—and the city of Vicksburg—were surrendered to U.S. Grant. The 4th of July became a day of mourning, rather than celebration, in Vicksburg.\nWhat You Need to Know:\nLosing Vicksburg meant the loss of the Confederate West. The Mississippi River belonged entirely to the Federals, who captured the last remaining Confederate strongpoint, the besieged Port Hudson, Louisiana, on 9 July.\nIn the next two episodes Scott and James will discuss the Siege of Vicksburg. In the summer of 1863, Grant’s Army of the Tennessee came to Vicksburg, located on a high bluff converged on Vicksburg, a Mississippi town on the same river. Union occupation of the town was critical to control of the strategic river. If it fell then the Confederacy would completely lose access to critical supply lines in Texas and Mexico.\nGrant’s six-week campaign began in June. His army came to Vicksburg, which was defended by Confederate General John C. Pemberton’s men, who built a series of trenches, forts, redans, and artillery lunettes surrounding the city. Grant’s army surrounded Pemberton and outnumbered him two to one. Trapped for six weeks, the residents of Vicksburg were forced to dig caves and eat rats to survive. But, due to Pemberton’s diligence and resourceful mind, they continued to trust his command despite dire circumstances.\nThe West (other than Vicksburg), May 1862 – January 1863\n- Halleck and Grant\n- After the capture of Corinth on May 30, Halleck and the Union army in the west did little.\n- In July of 1862, Halleck was promoted to General-in-Chief of all U. S. armies and transferred to Washington.\n- Before leaving, Halleck dispersed his army, sending part under Buell to Chattanooga, part under Sherman to Memphis, and part to Arkansas. The rest remained near Corinth under Gen. William Rosecrans. Grant was in overall command in the “Far West”\n- Braxton Bragg was put in command of all western Confederate forces, replacing Beauregard.\n- Bragg shifted 35,000 men by rail from Corinth to Chattanooga, TN.\n- In September and October 1862, Confederate forces attacked the Federals at Iuka and Corinth, but both times, Grant and his army fought them off.\n- Two Confederate armies, one under Edmund Kirby Smith and one under Bragg, left Chattanooga in August 1862 and marched into Kentucky.\n- Union general Don Carlos Buell marched from Nashville to stop the rebels.\n- The two armies met at Perryville, KY on October 8.\n- There were about 4200 Union and 3400 Confederate casualties.\n- The battle was inconclusive, but since the Confederates retreated back into East Tennessee, the battle was a strategic Union victory.\n- This was the last time the Confederates mounted a major operation to take KY.\n- Buell did not vigorously pursue Bragg’s army. Lincoln fired him and replaced him with William Rosecrans.\n- Murfreesboro / Stones River\n- In December 1862, Bragg’s army was camped near Murfreesboro, TN, southeast of Nashville.\n- On the 26th, Rosecrans and his army marched from Nashville to challenge Bragg.\n- By the 30th, the two armies were camped only about 700 yards from each other. (Story about the battling bands).\n- Bragg attacked on the 31st, but the Federals held firm.\n- Little happened on January 1. Bragg attacked again on the 2nd, but the Federals again drove the rebels back. Bragg then retreated back to Chattanooga.\n- The Federals lost about 13,000 troops, and the Confederates lost 12,000. This is the bloodiest battle of the war in terms of the percentage of casualties (about 30%)\n- Rosecrans did not pursue Bragg, but nevertheless, the Union victory lifted northern morale.\n- (Fun fact) Soon after the battle Rosecrans received a new chief of staff, a young Brigadier General named James A. Garfield.\n- By June 1862, the Union controlled all of the Mississippi except for a 200-mile stretch between Vicksburg and Port Hudson, LA.\n- Vicksburg (“The Gibraltar of the Mississippi”) was on a 200-foot bluff, was heavily fortified (4 miles of batteries!), and was seen as the key to controlling the Mississippi.\n- First Attempt to Take Vicksburg (June – July 1862)\n- Flag Officer David Farragut sent a message to the military governor of Vicksburg, calling for the city’s surrender\n- The governor replied “Mississippians don’t know, and refuse to learn, how to surrender…If Commodore Farragut…can teach them, let [him] come and try.”\n- Farragut ordered the Union flotillas at Memphis and New Orleans (which had a combined total of 220 guns) to attack the Vicksburg defenses.\n- The attack was inconclusive. Farragut realized that the Navy alone could not take the city. The city could only be taken by an attack from the rear (the land side) combined with a naval bombardment.\n- The city was defended by 10,000 entrenched Confederate troops under Earl Van Dorn.\n- Farragut requested 3000 Union soldiers from New Orleans. They (along with 1500 contrabands) tried to dig a canal that would leave the fortress isolated.\n- This effort failed, and hundreds of the soldiers, contrabands, and sailors died of disease.\n- The Union gave up trying to take Vicksburg…for now.\n- Second Attempt of Vicksburg Campaign\n- In December 1862, Grant made a new plan to take Vicksburg. Grant would march down from Tennessee with an army and attack the city from the east. He hoped to lure most of the small army defending the city (now commanded by John C. Pemberton) and attack it.\n- Meanwhile William T. Sherman would take another force and attack the lightly-defended city from the North.\n- As Grant marched southward, his army’s supply line was cut by Confederate cavalry under Nathan Bedford Forrest and Earl Van Dorn (now commanding a cavalry unit). This forced Grant to return to Tennessee.\n- As Grant and his army were marching back to TN, they noticed that the countryside was rich with food and other supplies. He could have lived off the land.\n- Meanwhile, Sherman’s force was attacked and defeated at the Battle of Chickasaw Bayou (just outside Vicksburg) on Dec. 29\n- Grant abandoned his efforts. During the winter and early spring of 1863, he ordered that several canals be cut. He also considered using the Yazoo River. None of these efforts worked.\n- Many Northerners called on Lincoln to replace Grant. Lincoln refused, saying “I can’t spare this man. He fights!”\nWould you like to learn the complete history of the Civil War? Click here for our podcast series Key Battles of the Civil War\nCite This Article\"Vicksburg Campaign (March 29 to to July 4, 1863)\" History on the Net\n© 2000-2020, Salem Media.\nJuly 8, 2020 <https://www.historyonthenet.com/vicksburg-campaign-civil-war-1863>\nMore Citation Information.","A Queen Falls\nThe Surrender of New Orleans was a Critical Moment in the Civil War.\nOne-hundred-fifty years ago, April 1862, New Orleans fell to Union forces one year into the Civil War. The impact of this loss proved critical. At the time, England and France, who relied on southern cotton and northern wheat, were debating mediating between the belligerents. President Lincoln sought to have Europe remain neutral; President Jefferson Davis urged Europe to recognize the Confederacy as an independent nation.\nMajor Pierre G.T. Beauregard, a second lieutenant in the Army Engineers, had drained New Orleans and constructed the Custom House. A Creole to the core, he immediately resigned his commission and joined the Confederacy when war became a reality. He considered the Mississippi River the city’s most vulnerable point and formulated a plan of defense based upon an attack from below.\nBeauregard realized that the two forts south of New Orleans, Fort St. Philip and Fort Jackson, could protect the city, but only if the trees were cut from the batture to permit a clear line of fire, and only if the forts were properly armed with long guns. In addition, he ordered that a strong, lighted chain barrier to be stretched across the river to inhibit any attempts to sail upstream and that “fire rafts” be strategically placed along the river’s bank to be set alight and floated downriver in the event of an attack. Gunboats would add additional firepower.\nBeauregard noted that the city must be able to repel an attack for about 30 minutes and do massive damage in that short time because they could only hold the enemy at bay for that long.\nOn April 12, 1861, the same P.G.T. Beauregard had ordered his gunners to fire on Fort Sumter, thus opening the Civil War. In response, President Lincoln blockaded southern ports. New Orleans commerce crashed from $550 million per year to a mere $51 million per year within months.\nNearly all of the troops, arms and ammunition available to New Orleans had been ordered to Virginia. Cannons lacked shells and carriages. The ironclad Louisiana stood unfinished, lacking a driveshaft, guns and some armor. The ironclad Manasses likewise remained uncompleted in dry dock.\nWorse still, Confederate leadership mistakenly believed that New Orleans could only be taken by an attack from upriver, which required passing the heavily defended Vicksburg, Miss., – an impossible task.\nIn December of 1861 Union forces occupied Ship Island, the landing point for the British during the Battle of New Orleans. New Orleans residents shouted: “Chalmette’s glories will be repeated!”\nRichmond persisted in the assertion that an attack would only come from upriver.\nSoon shortages of food and basic supplies plagued New Orleans. The city established free markets for the hungry, which serviced 723 families on the first day. By March 7, 1862, the families needing assistance climbed to 1,862. By March 25, 1,921 families depended upon charity for sustenance.\nGovernor Thomas O. Moore appointed General Mansfield Lovell, formerly of Massachusetts, to defend the city, but he lacked arms and men and his command was limited to land troops. He had no authority over the unfinished gunboats Louisiana or Manasses, which belonged to private enterprises; he could not command the river fleet, nor could he give orders to the fire rafts.\nNews reports from Washington indicated that a major invasion force was headed for the Gulf of Mexico. An alarmed Lovell responded by sending a message to his superiors in Richmond: “The forts can be passed, we are disorganized, and have no general officer to command and direct.”\nTheir response: “The fleet is not headed for New Orleans and fears of our people are without cause.” Confederate leaders steadfastly refused to alter their belief that the Union attack must come from upriver.\nOn September 19, 1862, the Union warship USS Water Witch crossed the bar and took possession of Head of the Passes. On Oct. 10, four additional ships joined the Water Witch, thus closing the river to all commerce.\nFor a brief moment Confederate forces has cause for celebration. On what is called “The Night of the Turtle,” Confederate seamen took the ironclad steam ram Manassas downriver. In total darkness, the Manassas surprised the Union ships and rammed one. Captain Pope, who commanded the group of ships, ordered a full retreat, abandoning the passes and embarrassing the Union.\nHowever, the celebration proved short-lived; the collision had dismounted one of the Manassas’ two steam engines and the ship had to limp back to port for repairs.\nWhile this action unfolded, Union generals were making plans.\nNaval Officer David Porter planned to construct a new type of naval vessel, a mortar boat, for bombarding the forts so as to open the gate for a naval invasion from the mouth of the river.\nRunning a Gauntlet\nGeneral John Barnard, Chief Engineer of the Army of the Potomac, who – along with Beauregard – had been in charge of rebuilding the forts in the 1840s, calculated that ships steaming upriver would run a gauntlet of fire for 3.5 miles. For 2 miles they would face the fire of 100 to 125 guns, and 50 to 100 guns for 1.5 miles. That, of course, assumed that these forts were properly equipped and manned. It would take only 25 to 30 minutes to pass the forts. It must be done at night.\nLincoln approved Porter’s mortar boats, and also Porter’s recommendation that David Farragut, Porter’s foster brother, be made fleet commander.\nThe Washington Star, a northern newspaper, disclosed details of the Union plan of attack. General Lovell received a copy. Meanwhile, Confederate commanders in Richmond still refused to accept reality.\nAs for the forts, Lovell later reported: “I found matters generally so deficient and incomplete that I was unwilling to commit their condition to writing for fear of their falling into the wrong hands.”\nFurther complicating matters, the River Defense Fleet’s gunboats left New Orleans to support the defense of Island No. 10, which occupied a double bend of the Mississippi River near New Madrid, Mo., blocking the Union’s advance. Maintaining this position was a critical concern for the South.\nLovell’s problems mounted. The Mississippi River ran at an exceptionally high flood tide during spring 1862. High water and debris pressed against the blocking chain. It gave way. Although repaired, it still lacked the strength needed to repel a concerted attack by a determined enemy.\nShip Island became the staging area for the fleet and General Benjamin “The Beast” Butler’s 15,000 troops. With all details complete, Farragut hoisted anchor and crossed the sandbars at the mouth of the Mississippi River.\nOn April 18 of that year, Porter took his position below the forts, around a bend in the river to avoid coming under fire from the Confederate forts, and began shelling the forts. The massive volleys continued for several days, but Farragut was growing impatient. He ordered a midshipman to climb a mast and signal which mortar shells fell into the forts and which landed short or long. There were more “outs” than “ins.” The forts weren’t being reduced.\nFarragut camouflaged his ships with river mud, saplings and paint. He reinforced bulkheads with anchor chains and engines with sandbags. He sanded the decks to provide traction against water and blood.\nOn April 23, 1862, Farragut ordered the attack. At 2 a.m. on the 24th, he ordered two lanterns hoisted from the mizzenmast of his flagship Hartford, signaling his ships to advance at 3:30 a.m. The Union ships approached in three divisions: the first led by Captain Theodurus Bailey, the center under Admiral Farragut and the last under Captain H.H. Bell.\nAt 3:30 a.m., Confederate Sergeant Herman in the water battery below Fort Jackson reported to Captain William Robertson that he detected “… several black, shapeless masses barely distinguishable from the surrounding darkness moving silently but steadily up the river.” At 3:40 a.m. the guns of both forts Jackson and St. Philip opened fire.\nA fierce battle ensued involving competing cannon fire from the forts and Farragut’s ships. Confederate gunboats added to the melee with occasional efforts to ram Union ships by the hastily repaired Confederate ironclad Manassas. The few fire rafts that were lighted had little effect.\nThe Confederate gunboat Louisiana, upon which so much depended, remained tied to the shoreline above the fort.\nDavid Porter of the mortar boat flotilla stated: “I shake a little now when I think how near we came to being defeated. One day’s more delay and the game would have been blocked on us. They would have put the Louisiana in the only narrow channel where ships had to pass, and she would have sunk everything that came by.”\nRather than have these dangerous ironclads fall into Union hands, both would be set on fire and sunk.\nDaylight on April 24 found the Union fleet above the forts and on their way to New Orleans. Fourteen of Farragut’s ships had passed the enemy. One, the Varuna, lay disabled alongside the bank; three others sustained damage and turned downstream to the protection of Porter’s gunboats. Commander Alden wrote in his log: “Victory! The American flag floats over everything on the Mississippi River this morning.”\nThe batteries at Chalmette and the West Bank were hopelessly outgunned by the massive broadsides the Union fleet delivered. The West Bank battery possessed nine guns; Chalmette had only five. The Confederates abandoned their position after running out of ammunition.\nNew Orleans had no defenses. No batteries had been erected and only Lovell’s army remained. Lovell sent a telegram to Richmond: “The enemy has passed the forts. It is too late to send any guns here; they had better go to Vicksburg.”\nTotal casualties for the entire invasion amounted to fewer than 50 Confederates fewer than 200 for the Union.\nFarragut’s fleet found the river littered with cotton bales, burning ships, sugar casks and other debris. Wharves and warehouses were on fire. Curiously, sailors witnessed some civilians running along the levees waving either white flags of surrender or American flags. Others played “Dixie” and cursed the offending Union navy.\nWatching in Disbelief\nAt 2 p.m. Farragut’s fleet arrived in New Orleans under a heavy rain. Thousands of soaked New Orleanians stood on the levees and watched in disbelief as the USS Mississippi struck up the “Star-Spangled Banner.”\nForts Jackson and St. Philip now lacked communication with New Orleans. On April 27, 1862, Captain David Porter sent Confederate commander Duncan a request to surrender. Duncan refused, taking notice that he had no orders from the city to give up his position.\nSoon, events took on a momentum of their own. Duncan and Higgins noted a change in attitude among the men at Fort Jackson, ending in a small mutiny. “They seized the guards, turned the guns around and started spiking the guns.” Some immediately left the fort while others refused to obey orders. Duncan realized that holding out was impossible because “… there was no longer any fight in the men remaining behind … they were completely demoralized.”\nOn April 28, Duncan contacted Porter and arranged a surrender of the forts. Duncan and Higgins boarded the Union ship USS Harriet Lane and conceded the forts to the Union.\nBack in New Orleans, capitulation was under way as well. Under the pelting rain, Farragut ordered Captain Theodorus Bailey and Lieutenant George Perkins ashore protected by a white flag to demand the city’s surrender. They rowed to the wharf at Laurel Street. According to Bailey: “No one received us, although the whole city was watching our movements, and the levee was crowded in spite of the heavy rain-storm. Among the crowd were many women and children, and the women were shaking rebel flags and being rude and noisy.”\nAuthor George Washington Cable recalled years later that the actions of these two Federal officers was: “… one of the bravest deeds I ever saw. [they] walked abreast, unguarded and alone, looking not to the right or left, never frowning, never flinching, while the mob screamed in their ears, shook cocked pistols in their faces, cursed and crowded, and gnashed upon them. So through the gates of death those two men walked to City Hall to demand the town’s surrender.”\nDespite vitriolic abuse, Bailey presented David Farragut’s Unconditional Surrender document that called for hoisting the American flag over the Custom House, the Mint and the Post Office, and removal of the State Flag over the City Hall.\nMayor Monroe met with the City Council. They hated to surrender, but now lacked any means of defending the city. Bailey and Perkins were then secretly escorted back to their ships in a covered coach with a Confederate escort as locals pounded and kicked the mayor’s office door.\nFarragut ordered a troop of Marines to hoist the “Stars and Stripes” on the flagstaff at the U.S. Mint on Esplanade Avenue as well as the at Custom House. A citizen named William Mumford tore down the flag at the Mint.\nIn the meantime, discussions with the city council continued as anti-Union mobs ran through the streets. Finally, on April 28, Farragut informed Mayor Monroe that he had 48 hours to remove women and children before the fleet opened fire.\nOn April 29, New Orleans surrendered under occupation of only 250 Marines and two howitzers. On May 1 General Butler had Mumford hung for tearing down the American flag on the U.S. Mint.\nSouthern morale suffered a serious blow. Mary Boykin Chestnut confided in her diary: “New Orleans is gone, and with it the Confederacy! Are we not cut in two? The Mississippi ruins us if it is lost.”\nThe fall of New Orleans irreparably damaged the southern cause in the Civil War militarily, politically and diplomatically. This next month marks the 150th anniversary of this critical historic event. The tide had turned April 29, 1862, when the queen fell.\nRon Chapman is a professor of history at Nunez Community College."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:fb4c0953-5152-46dc-889b-a3be0f7ed9af>","<urn:uuid:7a85c207-e5fd-4612-b6ed-83b3c1b3bf82>"],"error":null}
{"question":"What is the main difference between ancient and modern science in terms of their goals?","answer":"Ancient science sought knowledge of what things are, with the goal of contemplation as an end in itself satisfying to the knower. In contrast, modern science seeks knowledge of how things work, with the purpose of using that knowledge as a means for the relief and comfort of all humanity, both knowers and non-knowers alike.","context":["Leon R. Kass (born February 12, 1939) is an American physician, scientist, educator, and public intellectual, best known as proponent of liberal education via the \"Great Books,\" as an opponent of human cloning and euthanasia, as a critic of certain areas of technological progress and embryo research, and for his controversial tenure as chairman of the President's Council on Bioethics from 2001 to 2005.\nThe Problem of Technology (1993)\n- Included in Technology in the Western Political Tradition (1993)\n- Ancient science had sought knowledge of what things are, to be contemplated as an end in itself satisfying to the knower. In contrast, modern science seeks knowledge of how things work, to be used as a means for the relief and comfort of all humanity, knowers and non-knowers alike.\n- p. 7\n- Even the modern word \"concept\" means \"a grasping together,\" implying that the mind itself, in its act of knowing, functions like the intervening hand (in contrast to its ancient counterpart, “idea,” “that which can be beheld,” which implies that the mind functions like the receiving eye). And modern science rejects, as meaningless or useless, questions that cannot be answered by the application of method. Science becomes not the representation and demonstration of truth, but an art: the art of finding the truth—or, rather, that portion of truth that lends itself to be artfully found.\n- p. 8\n- The truths modern science finds—even about human beings—are value-neutral, in no way restraining, and indeed perfectly adapted for, technical application. In short, as Hans Jonas has put it, modern science contains manipulability at its theoretical core—and this remains true even for those great scientists who are themselves motivated by the desire for truth and who have no interest in that mastery over nature to which their discoveries nonetheless contribute and for which science is largely esteemed by the rest of us and mightily supported by the modern state.\n- p. 8\n- Could technology, understood as the disposition and activity of mastery, turn out to be a stumbling block in the path of the master himself?\n- p. 9\nForbidding Science: Some Beginning Reflections (2006)\n- Francis Bacon, in his New Atlantis, could charge his elite scientists in Salomon’s House with practicing self-censorship to avoid publicizing dangerous knowledge. Here is how the Father of Salomon’s House describes their practice: “We have consultations, which of the inventions and experiences which we have discovered shall be published, and which not: and take all a oath of secrecy for the concealing of those which we think fit to keep secret: though some of those we do reveal sometime to the State, and some not.” Bacon, the first prophet of the new relation between science and society and of the “conquest of nature for the relief of man’s estate,” knew better than we that knowledge is dangerous, that publication is a public and politically relevant act, and that self-censorship on the part of scientists is necessary and desirable. The passage is also remarkable for its wonderful ambiguity regarding whether scientists or the State has ultimate authority over dangerous knowledge\n- Many of our fellow citizens do not share the blind faith in the simple beneficence of all technological innovation. And because they do not share the corporealist, morally neutral, and in some cases atheistic world-view that they attribute (fairly or not) to science and scientists, they are reluctant to surrender the power of decision to the very people who they think are creating the problem.\nLooking for an Honest Man (2009)\n- Grappling with real-life concerns — from cloning to courtship, from living authentically to dying with dignity — has made me a better reader. Reciprocally, reading in a wisdom-seeking spirit has helped me greatly in my worldly grapplings. Not being held to the usual dues expected of a licensed humanist — professing specialized knowledge or publishing learned papers — I have been able to wander freely and most profitably in all the humanistic fields. I have come to believe that looking honestly for the human being, following the path wherever it leads, may itself be an integral part of finding it. A real question, graced by a long life to pursue it among the great books, has been an unadulterated blessing.\n- Fifty years ago, when Europeans and Americans still distinguished high culture from popular culture, and when classical learning was still highly esteemed in colleges and universities, C. P. Snow delivered his famous Rede Lecture at Cambridge University, \"The Two Cultures and the Scientific Revolution.\" Snow did more than warn of the growing split between the old culture of the humanities and the rising culture of science. He took Britain's literary aristocracy to task for its dangerous dismissal of scientific and technological progress, which Snow believed offered the solutions to the world's deepest problems. In a vitriolic response to Snow, the literary critic F. R. Leavis defended the primacy of the humanities for a civilizing education, insisting that science must not be allowed to operate outside of the moral norms that a first-rate humanistic education alone could provide.\n- In contrast to 50 years ago, few licensed humanists today embrace any view of the humanities that could in fact justify making them the centerpiece of a college curriculum.\n- Diogenes … refuses to be taken in by complacent popular belief that we already know human goodness from our daily experience, or by confident professorial claims that we can capture the mystery of our humanity in definitions. But mocking or not, and perhaps speaking better than he knew, Diogenes gave elegantly simple expression to the humanist quest for self-knowledge: I seek the human being — my human being, your human being, our humanity. In fact, the embellished version of Diogenes' question comes to the same thing: To seek an honest man is, at once, to seek a human being worthy of the name, an honest-to-goodness exemplar of the idea of humanity, a truthful and truth-speaking embodiment of the animal having the power of articulate speech.\n- [Medical] science was indeed powerful, but its self-understanding left much to be desired. It knew the human parts in ever-finer detail, but it concerned itself little with the human whole. … The art of healing does not inquire into what health is, or how to get and keep it: The word \"health\" does not occur in the index of the leading textbooks of medicine. To judge from the way we measure medical progress, largely in terms of mortality statistics and defeats of deadly diseases, one gets the unsettling impression that the tacit goal of medicine is not health but rather bodily immortality, with every death today regarded as a tragedy that future medical research will prevent.\n- According to Lewis, the dehumanization threatened by the mastery of nature has, at its deepest cause, less the emerging biotechnologies that might directly denature bodies and flatten souls, and more the underlying value-neutral, soulless, and heartless accounts that science proffers of living nature and of man. By expunging from its account of life any notion of soul, aspiration, and purpose, and by setting itself against the evidence of our lived experience, modern biology ultimately undermines our self-understanding as creatures of freedom and dignity, as well as our inherited teachings regarding how to live — teachings linked to philosophical anthropologies that science has now seemingly dethroned.\n- I turned to [Aristotle's] De Anima (On Soul), expecting to get help with understanding the difference between a living human being and its corpse, relevant for the difficult task of determining whether some persons on a respirator are alive or dead. I discovered to my amazement that Aristotle has almost no interest in the difference between the living and the dead. Instead, one learns most about life and soul not, as we moderns might suspect, from the boundary conditions when an organism comes into being or passes away, but rather when the organism is at its peak, its capacious body actively at work in energetic relation to—that is, in \"souling\"—the world: in the activities of sensing, imagining, desiring, moving, and thinking. Even more surprising, in place of our dualistic ideas of soul as either a \"ghost in the machine,\" invoked by some in order to save the notion of free will, or as a separate immortal entity that departs the body at the time of death, invoked by others to address the disturbing fact of apparent personal extinction, Aristotle offers a powerful and still defensible holistic idea of soul as the empowered and empowering \"form of a naturally organic body.\" \"Soul\" names the unified powers of aliveness, awareness, action, and appetite that living beings all manifest.\nThis is not mysticism or superstition, but biological fact, albeit one that, against current prejudice, recognizes the difference between mere material and its empowering form. Consider, for example, the eye. The eye's power of sight, though it \"resides in\" and is inseparable from material, is not itself material. Its light-absorbing chemicals do not see the light they absorb. Like any organ, the eye has extension, takes up space, can be touched and grasped by the hand. But neither the power of the eye — sight — nor sight's activity — seeing — is extended, touchable, corporeal. Sight and seeing are powers and activities of soul, relying on the underlying materials but not reducible to them. Moreover, sight and seeing are not knowable through our objectified science, but only through lived experience. A blind neuroscientist could give precise quantitative details regarding electrical discharges in the eye produced by the stimulus of light, and a blind craftsman could with instruction fashion a good material model of the eye; but sight and seeing can be known only by one who sees.\n- For most Americans, ethical matters are usually discussed either in utilitarian terms of weighing competing goods or balancing benefits and harms, looking to the greatest good for the greatest number, or in moralist terms of rules, rights and duties, \"thou shalts\" and \"thou shalt nots.\" Our public ethical discourse is largely negative and \"other-directed\": We focus on condemning and avoiding misconduct by, or on correcting and preventing injustice to, other people, not on elevating or improving ourselves. How liberating and encouraging, then, to encounter an ethics focused on the question, \"How to live?\" and that situates what we call the moral life in the larger context of human flourishing. How eye-opening are arguments that suggest that happiness is not a state of passive feeling but a life of fulfilling activity, and especially of the unimpeded and excellent activity of our specifically human powers—of acting and making, of thinking and learning, of loving and befriending. How illuminating it is to see the ethical life discussed not in terms of benefits and harms or rules of right and wrong, but in terms of character, and to understand that good character, formed through habituation, is more than holding right opinions or having \"good values,\" but is a binding up of heart and mind that both frees us from enslaving passions and frees us for fine and beautiful deeds. How encouraging it is to read an account of human life—the only such account in our philosophical tradition—that speaks at length and profoundly about friendship, culminating in the claim that the most fulfilling form of friendship is the sharing of speeches and thoughts.\n- Perhaps the most remarkable feature of Aristotle's teaching concerns the goals of ethical conduct. Unlike the moralists, Aristotle does not say that morality is a thing of absolute worth or that the virtuous person acts in order to adhere to a moral rule or universalizable maxim. And unlike the utilitarians, he does not say morality is good because it contributes to civic peace or to private gain and reputation. Instead, Aristotle says over and over again that the ethically excellent human being acts for the sake of the noble, for the sake of the beautiful.\nThe human being of fine character seeks to display his own fineness in word and in deed, to show the harmony of his soul in action and the rightness of his choice in the doing of graceful and gracious deeds. The beauty of his action has less to do with the cause that his action will serve or the additional benefits that will accrue to himself or another — though there usually will be such benefits. It has, rather, everything to do with showing forth in action the beautiful soul at work, exactly as a fine dancer dances for the sake of dancing finely. As the ballerina both exploits and resists the downward pull of gravity to rise freely and gracefully above it, so the person of ethical virtue exploits and elevates the necessities of our embodied existence to act freely and gracefully above them. Fine conduct is the beautiful and intrinsically fulfilling being-at-work of the harmonious or excellent soul.\n- With his attractive picture of human flourishing, Aristotle offers lasting refuge against the seas of moral relativism. Taking us on a tour of the museum of the virtues — from courage and moderation, through liberality, magnificence, greatness of soul, ambition, and gentleness, to the social virtues of friendliness, truthfulness, and wit — and displaying each of their portraits as a mean between two corresponding vices, Aristotle gives us direct and immediate experience in seeing the humanly beautiful. Anyone who cannot see that courage is more beautiful than cowardice or rashness, or that liberality is more beautiful than miserliness or prodigality, suffers, one might say, from the moral equivalent of color-blindness.\n- To act nobly, a noble heart is not enough. It needs help from a sharp mind. Though the beginnings of ethical virtue lie in habituation, starting in our youth, and though the core of moral virtue is the right-shaping of our loves and hates, by means of praise and blame, reward and punishment, the perfection of character finally requires a certain perfection of the mind.\n- Prudence is … more than mere shrewdness. If not tied down to the noble and just ends that one has been habituated to love, the soul's native power of cleverness can lead to the utmost knavery.\n- I have discovered in the Hebrew Bible teachings of righteousness, humaneness, and human dignity—at the source of my parents' teachings of mentschlichkeit—undreamt of in my prior philosophizing. In the idea that human beings are equally God-like, equally created in the image of the divine, I have seen the core principle of a humanistic and democratic politics, respectful of each and every human being, and a necessary correction to the uninstructed human penchant for worshiping brute nature or venerating mighty or clever men. In the Sabbath injunction to desist regularly from work and the flux of getting and spending, I have discovered an invitation to each human being, no matter how lowly, to step outside of time, in imitatio Dei, to contemplate the beauty of the world and to feel gratitude for its—and our—existence. In the injunction to honor your father and your mother, I have seen the foundation of a dignified family life, for each of us the nursery of our humanization and the first vehicle of cultural transmission. I have satisfied myself that there is no conflict between the Bible, rightly read, and modern science, and that the account of creation in the first chapter of Genesis offers \"not words of information but words of appreciation,\" as Abraham Joshua Heschel put it: \"not a description of how the world came into being but a song about the glory of the world's having come into being\"—the recognition of which glory, I would add, is ample proof of the text's claim that we human beings stand highest among the creatures. And thanks to my Biblical studies, I have been moved to new attitudes of gratitude, awe, and attention. For just as the world as created is a world summoned into existence under command, so to be a human being in that world—to be a mentsch—is to live in search of our summons. It is to recognize that we are here not by choice or on account of merit, but as an undeserved gift from powers not at our disposal. It is to feel the need to justify that gift, to make something out of our indebtedness for the opportunity of existence. It is to stand in the world not only in awe of its and our existence but under an obligation to answer a call to a worthy life, a life that does honor to the special powers and possibilities—the divine-likeness—with which our otherwise animal existence has been, no thanks to us, endowed."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:dca8b37c-bd70-4f1b-b524-3c26d0f16c0a>"],"error":null}
{"question":"¿Cuáles son los requisitos específicos de laboratorio para la función hepática en ambos ensayos clínicos? What are the specific laboratory requirements for liver function in both clinical trials?","answer":"Both trials have specific liver function requirements. The B-cell lymphoma trial requires total bilirubin ≤ 2.5 times institutional upper limit and AST/ALT ≤ 2.5 times institutional upper limit. The HBV trial has stricter requirements, with AST (SGOT) and ALT (SGPT) both needing to be <1.25 x ULN, and direct bilirubin ≤1.0 x ULN. Additionally, the HBV trial requires evidence of limited or no fibrosis (F0-F2) by liver biopsy or non-invasive alternative, and an Alpha-Fetoprotein level ≤100.","context":["RATIONALE: Vaccines made from mouse DNA may help the body build an effective immune response\nto kill cancer cells.\nPURPOSE: This phase I trial is studying the side effects and best dose of mouse DNA vaccine\nin treating patients with recurrent B-cell lymphoma.\n- To evaluate the safety and feasibility of intramuscular DNA vaccination with a plasmid\nDNA vector expressing the mouse extracellular domain of CD20, namely pINGmminiCD20.\nDoses of pING-mminiCD20 will be escalated by group to determine the optimal biological\n- To evaluate antibody and T-cell responses to CD20 after vaccination.\n- To observe patients for evidence of any antitumor response generated after vaccination.\nOUTLINE: The entire immunization schedule comprises five injections administered every three\nweeks (for a total of approximately four and one half months). After the second injection,\nblood will be drawn for assessment of antibody and T-cell responses. After the fifth and\nfinal injection, blood will again be drawn for assessment of antibody and T-cell responses.\n- Patients must have a functional immune system as determined by the following tests:\n- Serum proteins immunoelectrophoresis (serum IgG levels ≥ 0.5 g/dL are required)\n- No evidence of anergy as shown by positive skin test with tetanus toxoid, mumps or\n- Circulating T-cells as measured by flow cytometry (serum CD4+ and CD8+ T-cell counts\n≥ 250 and 150 cells/μL, respectively) Patients must have histologically proven (and\nconfirmed at MSKCC) B-cell lymphoma of any histology, excluding Burkitt's lymphoma,\nLymphoblastic lymphoma (due to their aggressiveness and low likelihood of response to\n- CD20 surface expression must be confirmed by immunohistochemical staining or flow.\n- Measurable disease is not a pre-requisite for enrollment in the study. However, if a\npatient does have measurable disease as evidenced by imaging studies, these have to\nbe done within eight weeks of starting treatment.\n- Patients must have a Karnofsky performance status ≥ 70%.\n- Patients with evidence of active disease, progression of disease or relapsed disease\nfollowing one or more prior regimens of chemotherapy, immunotherapy or radiation\ntherapy (including autologous stem cell transplants), not requiring immediate\ncytoreductive chemotherapy. All treatment must be completed at least four weeks prior\nto administration of the first vaccination, except immunotherapy and\nradioimmunotherapy, which must be completed at least 90 days prior to receiving the\nfirst vaccination. Active disease includes patients with minor or partial responses\nafter therapy as evidenced by FDG-avid disease or biopsy.\n- Age ≥ 18.\n- Adequate contraception during study enrollment.\n- Avoidance of breast-feeding their infants during the study enrollment.\n- Patients must have adequate organ and marrow function as defined below:\n- Absolute Neutrophil Count ≥ 1,000/uL\n- Platelets ≥ 75,000/uL\n- Total bilirubin ≤ 2.5 times institutional upper limit\n- AST/ALT ≤ 2.5 times institutional upper limit\n- Creatinine ≤ 2 mg/dL\n- PT/PTT ≤ 1.5 times institutional upper limit\n- Patients must have no signs of congestive heart failure according to the New York\nHeart Failure Guidelines Class III/IV.\n- Patients who have had chemotherapy or radiation therapy within 4 weeks prior to\nentering the study.\n- Patients who have undergone an allogeneic stem cell transplant at any time.\n- Patients who have not recovered from adverse events due to agents administered more\nthan 4 weeks earlier.\n- Patients who have received immunotherapy (i.e. rituximab) or radioimmuno therapy\n(i.e. tositumomab or ibritumomab) within the past 90 days.\nPatients who display signs of anergy as indicated by skin testing.\n- Patients with Burkitt's lymphoma and Lymphoblastic Lymphoma.\n- Patients who have been previously immunized with any type of DNA vaccine.\n- Patients who have positive anti-DS-DNA antibodies.\n- Patients with life expectancy less than 3 months from the time of enrollment.\n- Patients with serious underlying medical conditions, active infections requiring the\nuse of antimicrobial drugs or active bleeding.\n- Patients with active Hepatitis C (HC) or Hepatitis B (HB) infection, the latter\ndefined as a positive test for HBsAg or measurable viral load. In patients who are\nHBsAg negative but HBsAg positive (regardless of HBsAb status), a HB viral load will\nbe performed and if positive the subject will be excluded. If the subject is HBsAg\nnegative, HBcAb positive (regardless of HBsAb status) but with negative HBV viral\nload, the subject may be included but must undergo HBV DNA PCR testing at least every\ntwo months from the start of treatment during the routine study visits for as long as\nthe subject remains on study. Prophylactic antiviral therapy, in addition to the\nmonitoring described above, may be initiated at the discretion of the investigator.\nPatients with documented HIV infection or other immunodeficiency disorder or on chronic\n- Patients with autoimmune diseases such as but not limited to rheumatoid arthritis,\nSjogren disease, ulcerative colitis, autoimmune hepatitis.\n- Pregnant or nursing women. Women of child-bearing age will be tested for qualitative\nβ-HCG within 2 weeks of immunization.\n- Patients receiving other investigational drug.","Safety and Immunotherapeutic Activity of Cemiplimab in Participants With HBV on Suppressive Antiviral Therapy\na study on Hepatitis B\nThe purpose of this study is to evaluate the safety and immunotherapeutic activity of cemiplimab in participants with hepatitis B virus (HBV) on suppressive antiviral therapy.\nSafety and Immunotherapeutic Activity of Cemiplimab in Participants With HBV on Suppressive Antiviral Therapy: A Phase I/II Ascending Multiple Dose Study\nThe study consists of up to three cemiplimab dose cohorts (n=10 participants each). The cohorts will open sequentially, based on the safety of the previous cohort. Cohort 1 will open first to examine the lowest dose. When Cohort 1 participants have completed study week 18 visit and there are no safety concerns, Cohort 2 will open for enrollment. A similar assessment will be conducted with Cohort 2 to make a decision on opening Cohort 3. In each cohort, participants enter the study 6 weeks prior to initiation of treatment. The 6-week lead-in period is followed by a 6-week treatment period where two infusions of cemiplimab are administered 6 weeks apart, at study weeks 6 and 12. The total study duration per participant is 90 weeks, including 78 weeks of follow-up after the treatment period. Study visit schedule includes visits at entry, and weeks 6, 7, 8, 10, 12, 13, 14, 16, 17, 18, 22, 24, 30, 36, 54, 72 and 90. Evaluations include: a medical and medication history; assessment of HBV antiviral therapy adherence; physical exam; blood, urine, and fecal collection; rectal swab; liver biopsy and fine needle aspiration; and optional leukapheresis.\nHepatitis B Virus Hepatitis B Chronic hepatitis B Functional cure Immunotherapy Checkpoint inhibitor therapy Hepatitis Cemiplimab\nYou can join if…\nOpen to people ages 18-70\n- Chronic HBV infection (defined as hepatitis B surface antigen [HBsAg] positive).\n- Receiving treatment at the time of study entry and for ≥12 months prior to study entry with HBV-active nucleos(t)ides, with tenofovir- or entecavir-containing therapy: tenofovir disoproxil fumarate (TDF), tenofovir alafenamide (TAF), TDF/emtricitabine (FTC), TAF/FTC, or entecavir.\n- Ability and willingness of participant to provide informed consent.\n- Ability and willingness of participant to continue HBV antiviral therapy throughout the study.\n- Weight ≥40 kg and <200 kg.\n- Evidence of limited or no evidence of fibrosis (F0-F2) by liver biopsy or non-invasive alternative, as defined in the study protocol.\n- The following laboratory values obtained within 42 days prior to study entry:\n- HBV DNA level <20 IU/mL with prior documented pre-treatment elevation of HBV DNA, with or without a liver biopsy confirming chronic active hepatitis B\n- Documentation of hepatitis B e antigen (HBeAg) status (positive or negative)\n- Hemoglobin ≥14.0 g/dL for male, ≥12.0 g/dL for female participants\nAbsolute neutrophil count (ANC) >1500/mm3\n- International normalized ratio (INR) ≤1.1\n- Albumin ≥3.5 g/dL\n- Creatinine Cl ≥60 mL/min, as calculated by the Cockcroft-Gault equation\n- NOTE: A calculator for the Cockcroft-Gault equation is available on the DMC website at www.fstrf.org.\n- Aspartate aminotransferase (AST) serum glutamic:oxaloacetic transaminase (SGOT) <1.25 x ULN\n- ALT serum glutamic:pyruvic transaminase (SGPT) <1.25 x ULN\n- Direct bilirubin ≤1.0 x ULN\n- AM cortisol >10 mcg/dL and <ULN\n- NOTE A: Female participants on estrogen-containing oral contraception or other exogenous estrogen treatment may repeat the AM cortisol as part of screening to determine eligibility. AM cortisol should be drawn prior to 12 noon or per local lab requirements.\n- NOTE B: Participants with a low cortisol level that was drawn after 10:00 AM may repeat the AM cortisol as part of screening to determine eligibility.\n- Normal creatinine phosphokinase (CPK) level\n- Thyroid stimulating hormone (TSH) and free thyroxine (T4) level within normal limits\n- Fasting blood glucose <126 mg/dL\n- Interferon-gamma release assay (IGRA) for tuberculosis (TB) with negative results within 90 days prior to study entry, OR prior positive TB IGRA or positive purified protein derivative (PPD) skin test with documented evidence of completed prophylaxis treatment.\n- HCV antibody negative result within one year prior to study entry; or if the participant is HCV antibody positive, an unquantifiable HCV RNA result (< lower limit of quantification [LLOQ], either target detected, or target not detected) within 42 days prior to study entry.\n- Karnofsky performance score ≥90 within 42 days prior to entry.\n- For female participants of reproductive potential, a negative urine or serum pregnancy test at screening, and again within 48 hours prior to study entry.\n- All participants must agree not to participate in a conception process (e.g., active attempt to become pregnant or to impregnate, sperm donation, oocyte donation, in vitro fertilization).\n- When participating in sexual activity that could lead to pregnancy, participants must agree to use at least two reliable forms of contraceptive simultaneously, over the time period of 36 weeks following study entry (to include time period up to 6 months after last infusion). Such methods include:\n- Condoms (male or female) with or without a spermicidal agent\n- Diaphragm or cervical cap with spermicide\n- Intrauterine device (IUD)\n- Tubal ligation\n- Hormone-based contraceptive\n- NOTE: Providers and participants should be advised that not all contraceptive choices listed above can prevent HBV transmission. Study participants who are sexually active with HBV negative or unknown HBV serostatus partners should be advised that they need to consider effective strategies to reduce the risk of HBV transmission and meet the requirement for effective contraception during their participation in the study. Study participants should discuss contraceptive choices and HBV risk-reduction methods with their health care provider.\n- Participants who are not of reproductive potential (women who have been post-menopausal for at least 24 consecutive months or have undergone hysterectomy and/or bilateral oophorectomy or salpingectomy or men who have documented azoospermia) are eligible without requiring the use of contraceptives. Acceptable documentation of menopause or sterilization is specified below.\n- Written or oral documentation communicated by clinician or clinician's staff of one of the following:\n- Physician report/letter\n- Operative report or other source documentation in the patient record (a laboratory report of azoospermia is required to document successful vasectomy)\n- Discharge summary\n- Follicle stimulating hormone-release factor (FSH) measurement elevated into the menopausal range as established by the reporting laboratory\n- Intention to comply with the dosing instructions for study drug administration and ability to complete the study schedule of assessments.\nYou CAN'T join if...\n- Any malignancy within the 5 years prior to study entry or current malignancy requiring cytotoxic therapy.\n- NOTE: A history of non-melanoma skin cancer (e.g., basal cell carcinoma or squamous cell skin cancer) at any time is not exclusionary.\n- Current chronic, acute, or recurrent bacterial, fungal, or viral (other than HBV) infections that are serious, in the opinion of the site investigator, and that required systemic therapy within 30 days prior to study entry.\n- Prior history of or active autoimmune disorders including but not limited to inflammatory bowel diseases, scleroderma, severe psoriasis, myocarditis, uveitis, pneumonitis, systemic lupus erythematosus, rheumatoid arthritis, optic neuritis, myasthenia gravis, adrenal insufficiency, hypothyroidism and/or hyperthyroidism, autoimmune thyroiditis, hypophysitis, multiple sclerosis, or sarcoidosis.\n- NOTE: For questions related to the definition of autoimmune disorders, sites should contact the team per the study protocol.\n- Any known acquired or congenital immune deficiency.\n- History of chronic obstructive pulmonary disease (COPD).\n- History of significant pulmonary conditions.\n- Unstable asthma (e.g., sudden acute attacks occurring without an obvious trigger) or asthma requiring:\n- Daily steroid or long-acting beta-agonist prevention\n- Hospitalization in the 2 years prior to entry\n- A history of chronic congestive heart failure or other significant cardiac condition.\n- Any active clinically significant medical condition that, in the opinion of the site investigator, would place the participant at increased risk.\n- History of pneumonitis within the last 5 years prior to study entry.\n- Retinopathy or uveitis within 180 days prior to study entry.\n- Any acute or chronic psychiatric diagnoses that, in the opinion of the investigator, make the participant ineligible for participation.\n- Any vaccination within 30 days prior to entry.\n- NOTE: Individuals who require vaccination must delay screening for the study until 30 days after receiving the last injection.\n- Human immunodeficiency virus (HIV) infection.\n- Evidence of current (within 1 year prior to entry) Hepatitis delta virus (HDV) infection (HDV antibody positive).\n- Acute or serious illness, in the opinion of the site investigator, requiring systemic treatment and/or hospitalization within 30 days prior to study entry.\n- Alpha-Fetoprotein (AFP) >100 within 42 days prior to study entry in the absence of imaging within the prior 6 months prior to study entry to exclude hepatocellular carcinoma (HCC).\n- Any known bleeding disorder (i.e., hemophilia).\n- Receipt of investigational drug or device within 6 months prior to study entry.\n- History of treatment with a phosphoinositide 3-kinase inhibitor, including idelalisib.\n- History of checkpoint inhibitor treatment including anti-programmed death-1 (anti-PD-1), anti-programmed death-ligand 1 (anti-PD-L1) or anti-cytotoxic T-lymphocyte antigen 4 (anti-CTLA-4) antibodies.\n- History of immunoglobulin IgG therapy.\n- Receipt of interferon (IFN) therapy within 12 months prior to study entry.\n- Use of immunomodulators (e.g., interleukins, cyclosporine), systemic cytotoxic chemotherapy, or corticosteroid therapy.\n- NOTE A: Participants receiving topical corticosteroids will not be excluded.\n- NOTE B: Participants receiving inhaled corticosteroids will be excluded.\n- Intent to use immunomodulators (e.g., IL-2, IL-12, interferon [IFNs], or TNF modifiers) or corticosteroids (other than topical steroids) during the course of the study.\n- Current HCV antiviral therapy or receipt of HCV treatment in the 6 months prior to study entry.\n- Use of anticoagulants within the 30 days prior to study entry.\n- Prior treatment with other immune modulating agents that was associated with toxicity that resulted in discontinuation of the immune-modulating agent.\n- Current use or intent to use biotin ≥5 mg/day, including within dietary supplements during the study.\n- NOTE: Please see the study protocol for a list of other names used for biotin that should be looked for on the labels of dietary supplements.\n- Positive thyroid peroxidase (TPO) antibody result within 42 days prior to study entry.\n- Positive glutamic acid decarboxylase antibody (GAD65 Ab) result within 42 days prior to study entry.\n- Positive islet cell antibody result within 42 days prior to study entry.\n- Positive antinuclear antibody (ANA) ≥1:80 within 42 days prior to study entry.\n- Anti-smooth muscle antibody >1:80 within 42 days prior to study entry.\n- Immunoglobulin G (IgG) ≥1.2 x ULN within 42 days prior to study entry.\n- Known allergy/sensitivity or any hypersensitivity to components of cemiplimab (anti-PD-1) or its formulation or previous mAb treatments.\n- Active drug or alcohol use or dependence that, in the opinion of the site investigator, would interfere with adherence to study requirements.\n- Breastfeeding or pregnancy.\n- A male participant with a pregnant female sexual partner.\n- For participants in the optional leukapheresis (LA) component, prior history of difficulty establishing venous access or current contraindication for LA, in the opinion of the site investigator and based on pre-LA assessments listed in the study protocol.\n- NOTE: Participants unwilling or unable to complete LA are still eligible for enrollment into the main study.\n- Participants with a history of solid organ transplant.\n- Ucsf Hiv/Aids Crs\naccepting new patients\nSan Francisco California 94110 United States\n- UCLA CARE Center CRS\naccepting new patients\nLos Angeles California 90035 United States\n- accepting new patients\n- Start Date\n- Completion Date\n- National Institute of Allergy and Infectious Diseases (NIAID)\n- Phase 1/2\n- Study Type\n- Last Updated\nPlease contact me about this study\nWe will not share your information with anyone other than the team in charge of this study. Submitting your contact information does not obligate you to participate in research.\nThe study team should get back to you in a few business days.\nYou will also receive an email with next steps. Check your junk/spam folder if needed.\nIf you do not hear from the study team, please call 888-689-8273 and tell them you’re interested in study number NCT04046107."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:4491a094-d3b4-460c-b9c8-894a9e48180e>","<urn:uuid:b1ee49d7-f85c-4bc3-aad3-c97028c31d82>"],"error":null}
{"question":"How do consumer-focused solutions for reducing food waste differ between US households and Japanese retail settings?","answer":"US household solutions focus on behavioral changes and personal responsibility, encouraging actions like meal planning, shopping from existing inventory, proper food storage, and understanding food date labels for home management. In contrast, Japan's retail approach is more technology-driven and incentive-based, implementing the EcoBuy program where consumers can earn points (worth about 20% of purchase value) for buying items near their best-before dates, using a smartphone app to track purchases and receive notifications about expiration dates. The Japanese system also includes features like recipe suggestions and actively involves retailers in the waste reduction process.","context":["Updated: Jun 5, 2022\nFood production and distribution requires a tremendous amount of land, water, energy, and labor, so when food is wasted — not eaten, roughly speaking — all those inputs also go to waste. Unfortunately, about one-third of food produced in the U.S. is never eaten, and more than a quarter of that waste occurs in homes. The environmental impacts are significant. According to the EPA’s 2018 Wasted Food Report (PDF), 66% of residential food waste was landfilled, 30% was sent to sewer/wastewater treatment or incinerated, and only 3% was composted. Reducing food waste is one of the most impactful and straightforward ways to conserve energy and resources, reduce greenhouse gas emissions, and save money. Here are some strategies to help:\nShop your fridge/pantry, plan out your meals, and make a list of ingredients with quantities before you go to the grocery store — then stick to the plan! This helps you to better use what you have and avoid purchasing what you don’t need.\nTake note of food waste after parties and communal meals, then adjust accordingly for future get-togethers by serving less food per person and offering extras to guests on their way out the door. Sharing is caring!\nWhen dining out, only order as much as you’ll eat. Don’t be afraid to ask about portion sizes and side dishes, split meals with your tablemates, and take any leftovers to-go.\nStorage and Prep\nClean, prepare, and cook perishable items soon after bringing them home, then store them in your fridge or freezer for later use. Take special care with fruits and veggies (PDF). This can be a great time-saver in addition to reducing waste throughout the week.\nUse clear containers to store your food so you can see how much of each item you have, and to remind you to eat it. Add labels and other notes to remind you what’s what. Don’t hesitate to put food in the freezer if you don’t expect to eat it within a few days.\nKeep items with the shortest shelf-life at the front of your fridge and push long-lasting items towards the back, so you’re more likely to see and use food before it goes to waste. To do this properly, it’s crucial to understand food product dating:\nA \"Best if Used By\" date indicates to consumers when a product will have the best flavor or quality. It’s not a purchase or safety date.\nA \"Sell-By\" date tells grocery stores when they must remove a product from sale, for inventory management purposes. It’s not a safety date.\nA “Use-By\" date is the last date recommended for use of the product while at peak quality, as determined by the product’s manufacturer. It’s not a safety date.\nBe resourceful with your food! When you have too much of an item, try a new recipe that features it. When you have leftovers, think of them as “ingredients” to be incorporated into new meals.\nIf food can’t be eaten in time for whatever reason, consider a couple strategies that prevent it from going into a landfill:\nDonate safe, untouched food to a food bank, food pantry, or food rescue organization. Food insecurity is high in Chicago — your donations will be appreciated! Here are some local options:\nUse leftovers and food scraps to make compost! This organic matter enriches soil, reduces greenhouse gas emissions, and reduces the need for chemical fertilizers. If you aren’t able to do the composting yourself, consider leveraging an established residential composting service that operates in our neighborhood:","Shopping app tested to reduce food loss in Japan\nThe Tokyo metropolitan government introduced last Friday a new model project in which it awards points to consumers who buy foods close to their best-before dates and consume-by dates in order to decrease the amount of food loss. NTT Docomo Inc., with a governmental subsidy for the model project, developed a smartphone app that can be used until the end of February, and will evaluate the project’s effect.\nThe project, called “EcoBuy,” started at a Mini Piago supermarket in Chuo Ward, Tokyo. Stickers with such phrases as “Item for EcoBuy — its best-before date must be within the following period of days” are put on the store shelves for 30 designated food items including daily dishes, sliced raw fish and milk.\nThe best-before date represents the period of time during which foods can be best enjoyed. The consume-by date is labeled on perishable foods.A homemaker visiting the store said, “I’d like to buy the items if it can help reduce food loss.” Terushi Ito, the president of 99ICHIBA Co., which runs the store, said, “If we can reduce the volume of food waste, the labor cost for disposing of it will also be decreased.”\nTo join the project, consumers using the app take pictures of best-before dates and consume-by dates of the food items as well as the relevant receipt, then send the pictures to the designated center. After the center confirms the purchase, the consumer will receive points equivalent to about 20 percent of the purchase amount.\nThe Tokyo metropolitan government will pay NTT Docomo up to ¥15 million for the project. Shops bear the cost of allowing customers to apply the points to later purchases. The app has functions of showing the customers latest information about the food items for the model project, alerting them that the best-before dates and consume-by dates of the items they purchased are approaching, and providing recipes for the items. “We seek to reduce food waste not only at shops but also at home so as to cut food loss in society as a whole,” an NTT Docomo official said. “We would like to confirm how much we can cut the amount of food loss and the project’s profitability through the experiment,” said an official at the government’s bureau of environment.\n6 million tons of food wasted\nAn estimated total of 6.21 million tons of food is wasted a year in Japan, prompting measures to be taken by food makers and retailers, according to the Agriculture, Forestry and Fisheries Ministry. The figure is about twice the amount of food the United Nations provides worldwide. It is also equivalent to a bowl of rice being discarded by every person every day.\nVarious measures are being taken to reduce the amount of food that is wasted. In May, the ministry and other organizations requested the food industry relax the practice of not delivering to retailers food that is close to its best-before date. In Kyoto, retailers extended the sales periods of food until close to their best-before and consume-by dates in a pilot program conducted in November and December.\nSome companies in the food distribution industry and food manufacturers have deleted the “day” and only indicate the month and year in best-before dates in an effort to reduce the amount of food that has no quality problems but remains unsold and has to be discarded. The move is gradually spreading.\nSubscribe to INQUIRER PLUS to get access to The Philippine Daily Inquirer & other 70+ titles, share up to 5 gadgets, listen to the news, download as early as 4am & share articles on social media. Call 896 6000."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:7e0aaf2f-f3c4-4728-9e0a-4255c1362d73>","<urn:uuid:32aa1683-92e5-48bc-aaf8-f0d25d5fb5c2>"],"error":null}
{"question":"How do proper footwear choices and court safety measures work together to prevent injuries in racket sports?","answer":"In terms of footwear, tennis players specifically need shoes that provide stability on both the inside and outside of the foot, with ample flexibility in the sole beneath the ball of the foot - running shoes are not an adequate substitute. Regarding court safety, players must be vigilant about potential hazards: avoiding water bottles on the court that could spill and cause slipping, being careful of sweaty patches on the floor when changing ends, and making sure not to leave balls under or near feet where they could cause tripping. These combined measures help prevent court-related injuries.","context":["Activities Sports & Athletics Avoiding Table Tennis Injuries Share PINTEREST Email Print Westend61/Getty Images Sports & Athletics Table Tennis Basics Playing & Coaching Gear Baseball Basketball Bicycling Billiards Bodybuilding Bowling Boxing Car Racing Cheerleading Cricket Extreme Sports Football Golf Gymnastics Ice Hockey Martial Arts Professional Wrestling Skateboarding Skating Paintball Soccer Swimming & Diving Tennis Track & Field Volleyball Other Activities Learn More By Greg Letts Greg Letts is a world-ranked table tennis player and an Australian Level 1 table tennis coach. He wrote the eBook, \"How to Win at Table Tennis.\" our editorial process Greg Letts Updated April 21, 2019 Although many members of the general public may view ping-pong as an unathletic pastime, serious ping-pong players know better. Table tennis is just as much a sport as any other (and even more so than some). An inescapable part of ping-pong being a serious sport is the potential for injury when you take to the table. Remember, it's hard to improve when you are forced to be a spectator through injury, and insult is added when you are sidelined due to an event that could easily have been avoided. So here's a list of safety tips that will help keep you injury-free and out on the table. Safety Tips for Table Tennis Make sure you get a proper warm up before starting play. Warming up involves 5-15 minutes of light physical activity to elevate your heart rate and breathing rate, increase your blood flow and make your muscles warmer. Serious athletes may require even more activity to fully get ready for the exertion ahead. Then follow up with some gentle stretching to finish the warm-up. For more information on warming up and stretching I would suggest reading the following two articles:Try not to umpire just before you are about to play since it's easy to cool down and get stiff again. If you must umpire, make sure you wear plenty of layers to keep warm and ask the tournament director for time to get an adequate warm-up before playing your next game.When using more than one ball (for drills etc), don't leave balls under or near your feet, where you might easily stand on them and trip over.If you are assisting in a multiple ball drill by picking up the balls that are rolling around the court, be careful to stay clear of the players doing the drill. Get too close and you might end up getting hit by a bat, or have one of the players tripping over you!Don't walk onto or through other players' courts until you make eye contact with the players involved. A player with his back to you may not even know you are there, and may suddenly move backward during a point, causing a collision with the potential to injure both of you.When playing doubles, keep aware of where your partner is, so that you don't accidentally hit him with your bat, or run into each other without expecting it. Believe me, sooner or later you will run into each other, so at least be prepared so that you can soften the impact.Don't jump or hurdle the barriers between courts. It's very easy to accidentally catch your foot when doing so, and end up tripping over. Plus you often can't see whether any balls are on the other side of the barrier. You don't want to successfully jump the barrier just to land on a ball and fall down!Don't take your water bottle onto the court. It's not legal, to begin with, and it's very easy to spill water without noticing it. There's nothing more guaranteed to cause an injury than suddenly slipping on a patch of water when you are off-balance and reaching for the ball!Speaking of slipping—when changing ends, take a quick look to see that your opponent hasn't left drops of sweat all over the floor. Slipping on a sweaty patch on the floor is another easy way to injure yourself.Be careful around tables—make sure that roller tables have their brakes on, and check all tables to make sure that their bracing struts are in the proper position. Also, be wary about sitting or leaning on tables, since there are many tables that will fold inwards if weight is placed on the table near the net.Some tables are easier to set up than others, and some tables really require two people to set up safely. It's easier than you think to accidentally pull a heavy table over on yourself when trying to set it up. Get a friend to help when necessary.Don't use a cracked blade—you never know when it might finally snap and have the paddle head go flying away, possibly to hit someone else!On the same theme—don't ever throw your bat around. While some players will attempt to minimize the danger by throwing it into a courtside barrier to vent their frustrations, it is all too easy to miss the barrier completely in your agitation and end up with your racket flying wildly across the hall endangering others.When you are on the court, show proper courtesy and decorum at all times. Goofing around while others are trying to play matches is a perfect recipe for being in the wrong place at the wrong time without realizing it—and that's how serious injuries can happen.Listen to your body. If you are playing and you get a sharp pain, don't ignore it! As you get older you will no doubt get used to playing with niggling injuries due to wear and tear on your body, but the best way to turn a small injury into a large one is to ignore your body's warning signs and keep on playing when new pain flares up.For those times when you do get injured despite all your best precautions, knowing some basic first aid can help minimize the injury and get you back out on the table as soon as possible. Here's some advice about treating sprains and strains to get you started.","Maximize Your Game\nAre you doing all you can to prevent injuries and stay at peak athletic performance?\nMay is National Physical Fitness and Sports Month. The national holiday is an annual observance, designated in 1983 by the President's Council on Fitness, to promote healthy lifestyles among all Americans.\nWe sat down and talked to Angie Spencer, PT, MSPT, CMPT, CAFS, CCI, owner of Champions Recovery Room and Physical Therapy in Urbandale. Spencer is a physical therapist with nearly 20 years of experience in an outpatient orthopedic physical therapy setting.\n“Preventing tennis injuries is important to keeping our tennis athletes healthy and being able to continue their training routine. Many things can be done off the court, reducing the risk of injury,” Spencer said.\n“Stretching and flexibility is important to every tennis player’s performance, as well as good core stability and strength of the upper and lower body. Flexibility and core strength are often overlooked. Doing a proper warm up, including some light stretching before and after every practice or match, while doing another stretching routine, will help keep players’ flexible and agile for the sport of tennis. Stretches to help prevent tennis elbow and shoulder injury should be made a priority, but also remembering your legs need to be stretched as well can help reduce knee and ankle injuries.”\nTraining off the court is something not all players make a priority.\n“Many tennis players believe the exercise and skills they are practicing on the court are sufficient to reach their top performance. When a player does not train ‘off the court,’ tennis players cannot become well-rounded, strong athletes, which could result in major injuries during a game.”\nThe right shoe is also important to prevent injuries.\n“Many players avoid buying tennis shoes and use running shoes as an alternative. Tennis players need a shoe that supports the foot during the quick side-to-side movements or shifts in weight associated with the game. Buy a shoe that provides stability on the inside and outside of the foot and has ample flexibility in the sole beneath the ball of the foot.”\nFollowing simple tips will help keep you injury-free as you step onto the court. If you do experience pain or injury while playing, a knowledgeable physical therapist can provide you with a rehabilitation program to get you back in the game as soon as possible.\nWhile tennis is an exhilarating game for players and fans alike, many do not realize the extent of injuries that can be caused by playing tennis. Tennis injuries are extremely common, whether you play recreationally, in a weekly league or professionally. Learning how to identify, prevent and heal tennis injuries is crucial if you want your game to be safe and successful.\nAthletic recovery is just as important as your training because it helps reduce muscle soreness caused by a strenuous workout, which can sometimes cause nagging aches and pains that turn into something more involved like an injury from repetitive stress.\nPhysical therapists can help with preventative training, athletic recovery, and injury screening and treatment. Many physical therapists, such as those at Champions Recovery Room and Physical Therapy, can also do fittings for custom foot orthotics, perform manipulation of the spine and joints with specialized manual therapy techniques, ACL and hip rehabilitation, work with repetitive stress injuries and sports rehabilitation.\nCelebrate this month (and every month) by keeping physically fit. Children should spend 60 minutes each day doing some type of physical activity and adults should spend 30 minutes daily keeping physically fit. Look for opportunities to be physically active in the workplace, at school and at home."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:9d0620bd-1674-4eda-bb68-a694f97c498e>","<urn:uuid:075cfa12-c268-4c7f-967a-0ea4650a4a62>"],"error":null}
{"question":"How do usability testing approaches differ between the prototyping stage in UX design and the Design Sprint methodology?","answer":"In UX design's prototyping stage, testing relies on methods like hallway testing with random people, remote unmoderated testing using tools like UserTesting.com, moderated testing with specialized equipment, and expert reviews by industry leaders. According to Nielsen Norman Group, 80% of usability issues can be detected with just 5 test users. The Google Design Sprint, on the other hand, takes a more focused approach with testing occurring specifically on Day 5, involving 6-20 real users testing a prototype that was created on Day 4. The Sprint method offers a more concentrated and time-boxed testing phase compared to the various testing options in traditional UX design.","context":["User Experience (UX) Design in Software Product Development: How to Create Exceptional Product Value\nHave you ever left a website frustrated, not being able to find the needed information about the company? Have you ever uninstalled an app, a potentially useful app, that later turned out to be a real pain to use? Sadly, you’re not the only one to encounter this problem. The internet is flooded with examples of poor user experience.\nThen you decide to build your own product. And, as we both know, you are not like most of the product owners out there: You really care about your customers and want to provide an outstanding user experience. But how do you do that? Reading a couple of articles on UX Magazine or UX Design Weekly won’t make you a UX designer. Furthermore, designers and developers do not necessarily have what it takes to create an outstanding user experience. The best way to go about this task is to go with a proper UX design process.\nThe Science Behind the UX Design Process\nJoel Marsh, a well-known O’Reilly author and professional designer, in his book “UX for Beginners” states: “UX is a lot more than buttons and wireframes. The stuff that seems obvious is only the tip of the iceberg, and the stuff that matters most is completely invisible.” In fact, design is only one of the 5 main ingredients of UX: psychology, usability, design, copywriting, analysis.\nBeing closely related to the broader business strategy, the process of UX design has a significant amount of “science” behind it. It typically involves research, testing, and analysis. Similar to a common scientific study, UX design typically follows the standard process:\nGoogle Design Sprint is a slightly different approach to the process. It is “a five-day process for answering critical business questions through design, prototyping, and testing ideas with customers”. The Sprint addresses the main issues in the UX process:\nDay 1: Unpack – define the problem and choose the primary focus area for the sprint.\nDay 2: Sketch – outline a solution.\nDay 3: Decide – choose the idea(s) that you will take to the next phase and create a hypothesis.\nDay 4: Prototype – make a prototype.\nDay 5: Test – let the real users (usually 6-20 people) get involved and test the prototype.\nWe at AltexSoft have adopted a similar approach, which is fully customized based on the specifics of your project. A typical UX product development process contains the following stages.\nAt this stage, a basic validation of your idea can be conducted through the following types of testing:\n- Hallway (Corridor) Testing – Simple ad hoc test conducted with randomly selected people (for example, a bypasser in an office corridor or a random visitor at a venue) aimed at getting quick user feedback;At this stage, a basic validation of your idea can be conducted through the following types of testing:\n- Remote Usability Testing (Unmoderated) – The process is usually conducted using specific tools, such as UserTesting.com, Userlytics or UserBob, which allow you to publish your prototype and have actual people review and test it remotely for a small fee.\n- Moderated Testing – It is similar to the previous one, but conducted “manually”. This option is somewhat costly and requires an actual interviewer conduct the testing and special eye-tracking and recording equipment to document the process and results.\n- Expert Review – As follows from the title, this type of usability testing relies on an independent expert opinion. This could be specialized agencies or proven industry leaders, rather than ordinary users.\nAccording to the Nielsen Norman Group, up to 80 percent of usability issues are usually detected by only 5 test users. At the prototyping stage, there is no need for large focus groups and significant investments into the process. However, this early testing proves extremely valuable in the long run. Being able to detect and fix the possible issues early, saves time and efforts. Making a simple change in a wireframe takes about an hour. The same change in the UI design might take up to one day. And it might take even more time and efforts to implement this change after launch.\nAs we mostly follow the agile approach in software product development, the above-described iterative process is usually recurring. Iteration by iteration, we discover, analyze, prototype, test and implement new functionality, one feature at a time. Therefore, the process is never complete.\nMaintaining and Evolving the Product After Launch\nThe importance of a proper UX design process is undisputed. However, even after the product is launched, there is still a room for improvement. The technology is evolving fast, so are the user expectations.\nIn order to stay competitive and profitable, you need to keep up with these challenges. A great, and cost-effective, way to improve your product after its launch is to invest in professional UX consulting.\nWhile some basic usability testing is usually conducted at the UX design stage, with the live product and its actual users you now have a pool of valuable data to turn to your advantage. The main activities in this regard are:\n- Testing (A/B testing or split testing, which employs several options of the same page or interface element in order to find out which one performs best)\n- UX Analysis\n- Interface Audits\n- Conversion Rate Optimization (CRO)\nAt this stage, professional UX consultants gather users’ feedback, measure KPIs, and provide actionable insights and recommendations as to how you can improve your product interface and its conversion. The process is usually tailored to fit your business objectives and requires special skills and tools.\nThe Business Case for UX Design: Proving the ROI\nA conventional wisdom is that every dollar spent on UX can bring up to $100 in revenue. That’s pretty impressive! But how is that possible? There are 4 areas your company may benefit from:\n- Better UX = Increased conversion and revenue\nAccording to the Customer Experience Optimization Report by eConsultancy.com, 94 percent of respondents have seen higher conversion rates as a result of their commitment to customer experience.\nThe famous case of a “$300 Million Button” is a great example of how simple UX improvements can make a huge difference in business. By simply removing the obstacles in the checkout process (making the registration optional), the company was able to grow conversion by 45 percent. This resulted in additional $15 million revenue within a month and total $300 million of annual revenue.\n- Better UX = Lower development and ownership costs\nIn his bestselling book “Software Engineering: A Practitioner’s Approach”, Robert Pressman describes a situation when a proper UX process can help you save money in the long run. He states, that if an issue costs $1 to be solved at the design stage, it would cost $10 to be solved during the development and $100 – after the product’s release.\nA research shows that at least 50 percent of the development time and effort are spent on changes and fixes that are avoidable. For a team of four charging $50 per hour, that makes $4000 in waste every week and over $200,000 annually.\n- Better UX = Increased user retention and brand loyalty\nThe numbers don’t lie: The study finds that 23 percent of users uninstall an app after only one use. Furthermore, an impressive 62 percent will use it less than 11 times. While UX might not be the only problem that pushes your users away, it is one of the things you can and should fix.\nRegardless of the product or a service you are offering, your solution has to be easy and pleasant to use in order to make the users stay. Be it an eCommerce website or a productivity app – if it is frustrating or does not perform well, most of the users will abandon it and pick another one, with better user experience. With an average online purchase value of $75.9, losing 100 potential buyers might cost you $7,590 in revenue.\n- Better UX = Higher efficiency with less support\nThis is especially true for enterprise products and corporate applications. Simple and easy to use tools require less training, minimize the risk of error, and improve the efficiency of your business operations. Improving the operational efficiency of a team of 10 by 10 percent could bring up to 40 additional hours and $1,000 in savings per week (with an average hourly rate of $25 per employee).\nIn case of McAfee Inc., the company was able to cut the support effort (and the corresponding cost) by 90 percent, simply by providing a clear and user-oriented design. Supposing that a support call costs $10 to process, reducing the number of such calls from 100 to 10 means up to $900 in savings.\nScott Jenson, a UX professional with 30+ years of experience working at Apple, Symbian, and Google, wrote: “A good UX isn’t hard, it’s just not prioritized”. Indeed, many executives prefer focusing on the immediate value instead of laying a foundation for the future success. Despite its proven ROI, setting up an efficient UX process requires substantial investment.\nIn order to optimize your UX strategy and increase the value of your efforts, McKinsey suggests asking yourself the following questions first of all:\n- Do you have a senior design leader with real authority?\n- Are you continuously reviewing your metrics?\n- Are designers working with the right people in the organization?\n- Do you really understand what motivates your customers?\n- How can you speed up your processes?\nWhile the user expectations and needs are constantly growing, setting the bar high for new products, having a good UX strategy is an important aspect of the business. User-centric companies, such as Apple, Coca-Cola, IBM, Intuit, Nike, Starbucks, Target, and Walt Disney, are setting the pace in almost every industry. That said, proper UX design has a major impact on the business ROI and plays a significant role in the software product development process in general, contributing to the creation of a more consistent, relevant and overall higher quality software product.","At the Google Ventures Design Studio, we have a five-day process for taking a product or feature from design through prototyping and testing. We call it a product design sprint. This is the fifth in a series of seven posts on running your own design sprint.\nAt this point in a design sprint, you’ve got a lot of ideas down on paper. You’ve explored the problem, generated a ton of solutions, and looked around at how other companies are solving similar problems.\nIt’s awesome to have a lot of ideas. It’s a great feeling. But I’ve got bad news: You can’t build and test everything. And even if you could, it wouldn’t be very useful, because you’d have too much information to sift through. So you’ve got tough decisions to make: Which solutions will you pursue and which will you put on ice?\nToday we’ll look at how to decide which solutions to flesh out, and how you’ll fit them together into something you can rapidly test with users to learn what’s working and what isn’t.\nThe decision-making process is hard, and this is one place where working as a group can become a liability. Companies and teams have a natural way that they make decisions—but in a sprint, the group effect can cause decision-makers to behave more democratically than they do in real life. Once the sprint is over and that rosy democratic feeling wears off, you can be left with something that doesn’t have true support from the deciders.\nTo combat this effect, the facilitator often has to draw out the decision-maker to give their honest, true opinion. You’d be surprised at how often this reluctant decision-maker is the CEO. No way, really? Yes way. In the sprint, people are out of their comfort zones, and even CEOs can begin to behave in non-standard ways.\nOne method is giving \"super votes\" to the deciders during design critiques, which you did in day 2. But most of the time, there are no special techniques. You just have to be blunt.\nAs facilitator, you should be upfront with the team if you sense you’ve got a case of groupthink. Let everyone know that you need more assertive participation from the deciders. Additionally, you should have the words \"make the call, Sally\" on the tip of your tongue throughout day 3. (I’m assuming your CEO’s name is Sally.) Don’t worry about being a sycophant. If you aren’t conscientious about bringing the decider in now, you’ll have a problem later.\nThe first thing I like to do in this phase of a sprint is comb through storyboards from the previous day looking for conflicts. A conflict is a place where there are two or more different approaches to solving the same problem. Conflicting approaches are super helpful, because they illuminate the choices for your product.\nFor example, let’s say you’re designing your homepage, where you explain your product to potential customers. Maybe one person’s storyboard uses a video, and another uses diagrams on a long scrolling page, and a third uses a single image of the product. Great, you’ve found a conflict! Every time you find one, write it down. I like to put the topic and solutions on sticky notes, like this:\nEach conflict is like a little gold mine. In business-as-usual design, designers often end up picking one approach and going straight to high resolution. When I was working on products in-house, I’d often get so caught up in that one solution that I wouldn’t even have time to think about how else it could be done.\nSo one of the best things about the product design sprint is that it allows you to map out those decision points, and perhaps even to explore a few conflicting ideas in parallel, instead of immediately committing to a safe choice.\nYou have two basic options for what kind of user study you’re going to run at the end of your sprint. You can prototype several different approaches and test them against one another (the \"battle royale\") or you can go with a single prototype (the \"best shot\").\nThe advantage of the \"best shot\" approach is that you can put a lot more work into that one prototype, or just get it done faster. If you’re testing only one solution, the user study is less complex, and it gives you more time to see what the users say about your competitors’ products (or just interview users, which is always surprisingly valuable for teams).\nThe \"battle royale\" works well for newer spaces where there really aren’t many conventions, and you need to figure which one is going to work best for the user. The disadvantage is that it takes more time, and your testers may run out of patience before you get all of the information you’d like to have from them. You may have to bring in more participants and run more studies.\nOn the upside, the results of a \"battle royale\" can be very surprising. When working with startups, I’ve often seen a dark-horse design turn out to be the strongest in user studies. When that happens, we thank our lucky stars we didn’t \"best shot\" it, or we never would have known.\nYou may also do some kind of hybrid. Occasionally, if you choose the \"best shot\" approach, you’ll get into testing and find that something’s really not working in your prototype, and you need to go back and have a \"battle royale\" over that specific feature.\nSo how do you know which to pick? Start with a gut check: If everyone is excited about one option, you may be ready for a \"best shot.\" But if it feels more like you’re sitting there and scratching your heads about what to do next—or else you want to throttle each other because those fools just won’t agree with you—well, you may need a \"battle royale.\"\nWhat else should you test in your user study? Listing out your underlying assumptions is a good way to revisit the big picture, especially when you’ve been heads down in a sprint for a few days.\nSome of those assumptions might be about the users (example: \"Users are willing to upload a profile photo\"), some about the business (\"the designer-with-glasses-and-beard market is large enough to support our product\"), some about technology (\"we can automatically cluster profile photos by beard shape\"), and maybe even some about messaging (\"people will still find beard jokes to be amusing, even for the third time in a single paragraph\").\nI can tell you that last assumption is false right now, but for most others, you’re going to need some kind of research. Guess what? You can test a bunch of them by showing a prototype to users.\nFor example, if you have a big assumption that users will be comfortable sharing private data in your product, you may want to pick the most aggressive sharing defaults you can think of to prototype. When you show it to users, you’ll find out pretty quickly whether your assumption was correct.\nTry to come up with a way to test all your assumptions, either in the user study or in some other parallel task that can start right away (e.g., ask the engineers to spend a few hours hacking at that beard-clustering algorithm). If you can’t test every assumption now, keep a list for next time. Untested assumptions are like takeout containers in your fridge: If you leave them for very long, things get nasty.\nOK, you’ve picked which conflicts to explore and you’ve decided which assumptions to test. Congratulations—you’re ready to script your prototype.\nNow we’re going to make a storyboard that shows exactly how the user will step through your prototype, click by click. This storyboard will become the spec for building the prototype. This is an activity that the group does together—it’s actually the last group step before you break for prototyping.\nStart by drawing a big grid on the whiteboard—each cell should be about as large as two sheets of copy paper, and for most sprints, you’ll cover one or two whole whiteboards with your grid. The idea is to draw a comic book that tells a story starting when the user opens the prototype and ending when they complete all necessary tasks.\nIn each comic book frame, you’ll draw a single action—whether it’s a pointer clicking on a button, text being entered, or a stick-figure user doing something in real life. You don’t have to worry about layout or design in great detail, but you do have to think through every action that takes place in the story.\nDrawing the storyboard is hard work, and you’ll want to facilitate carefully. Get one person to draw, but don’t make them figure everything out on their own. The group should be engaged and discussing what happens next and giving that brave soul holding the whiteboard marker as much help as possible.\nWhen you begin drawing, imagine you’re framing the prototype for your user study participant. How will they get to your product? What will they be trying to do when they get there? That’ll help you figure out whether the first frame of your comic book is an email or a Google search or an advertisement or the App Store or whatever—and hopefully the story will flow easily from there, following the outline you laid out in day 1.\nAs you storyboard, there will be lots of small decisions to make that didn’t come up earlier in the day. That’s expected, since you’re working at a finer level of detail now. The facilitator has to work hard here to not let people be too nice. You don’t want design by committee. If there’s a good argument going, don’t try to find middle ground or make people agree. Help the team place a bet on one of the opposing solutions and keep the other in your back pocket if it fails. Call on the CEO to make a tough call when needed. If both solutions are viable, you may want to opt for a \"battle royale\"—just don’t use it as an excuse to avoid decisions.\nWhen you’re finished with the user story, take a moment to pat yourselves on the back and eat some chocolate because it’s probably been a pretty epic task. You’ve given form to everything you want your user study participant to experience, and you’re ready to turn that story into a higher-resolution mockup.\nIn the next post, we’ll move on to prototyping. It’s time for the Fellowship of the Sprint to break up, at least temporarily, as everybody puts on their headphones and cranks out a crafty imitation of a real product. Stay tuned.\n[ILLUSTRATION: Doodles via Shutterstock]"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:1a66cc60-6ff4-471b-b2cc-c2eba3ee20c0>","<urn:uuid:fded4265-ebe8-416a-8518-65d5838f11af>"],"error":null}
{"question":"How do historical vs modern medical ethics differ on abortion?","answer":"Historical medical ethics, as represented in the original Hippocratic Oath, explicitly prohibited abortion, stating 'I will not give a woman an abortive remedy.' This prohibition has been maintained in modern versions, as seen in the 2009 Hippocratic Oath which states 'I will not help a woman obtain an abortion.' However, modern secular bioethics, which relies on principlism and emphasizes patient autonomy, has moved away from this absolute prohibition, leading to conflicts between religiously observant health professionals who maintain the traditional view and those who support abortion access based on patient choice and preference satisfaction.","context":["What Is the Purpose of Bioethics? - An Excerpt from the Dictionary of Christianity and Science\nOur new resource engaging the intersection between science and faith, The Dictionary of Christianity of Science, gives you access to key terms, theories, individuals, debates, and much more to help you engage in these important discussions. In today's excerpt, we give you a sampling of an article about bioethics by the moral philosopher Francis J. Beckwith.\nBIOETHICS. Bioethics is an interdisciplinary endeavor primarily involving the study of moral issues in health care and the life sciences for the purpose of providing ethical guidance for practitioners in clinical and research settings.\nWork in bioethics can be found in a variety of academic and professional fields, including medicine, philosophy, biology, theology, psychology, and law. The reason for this is that so many of the questions in bioethics overlap several fields of inquiry. For example, the question of whether a physician may assist in a patient’s suicide is not merely a medical or biological question, even though a physician is a medical doctor trained in the biological sciences and may use certain medicines to carry out his patient’s wishes. Rather, it is primarily a moral question about the rightness or wrongness of intentionally willing another’s death, even when the one being killed is requesting it. Thus, answering this moral question requires the conceptual tools of philosophy and/or theology.\nOther disciplines also play a part in the making of such judgments. For example, whether a patient is competent to choose a particular course of treatment requires the insights of psychology (or psychiatry), and whether there are governmental statutes or regulations on what a physician may or may not do to her or his patient requires the assistance of legal counsel. However, because bioethics is primarily concerned with answering moral questions — rather than medical, legal, or biological ones — all of the answers, even when they are inconsistent with each other, either employ the categories of philosophy and/or theology or they presuppose those categories in one way or another. This can be seen in the debates about some of the most contested questions in bioethics.\nBeginning of Human Life\nWhat one thinks about the morality of abortion, human cloning, embryonic stem cell research, or reproductive technologies will often be determined by what one thinks about the nature of nascent human life and/or the proper function of our sexual powers. If, for example, one believes that a human embryo or fetus lacks full moral status because it cannot engage in certain types of mental activities (e.g., have a self-concept, desire a right to life, have a life plan; Tooley 1983), then practices such as abortion and embryonic stem cell research, which virtually always result in the death of prenatal human subjects, will not seem to be serious moral wrongs (or even wrongs at all). Of course, a different view of nascent human life, one consistent with Christian anthropology, entails that these acts are gravely immoral (George and Tollefson 2011). The latter position, unlike the former, connects a human being’s full moral status to his nature (what he is) rather than to the maturation of those powers that flow from his nature (what he does).\nAlternatives to ordinary human reproduction (e.g., cloning, in vitro fertilization, surrogate motherhood, artificial insemination) often raise additional questions, though they are no less philosophical or theological than the question of the prenatal human being’s full moral status. For example, is it morally right (and/or consistent with God’s plan for marriage) to bring children into being apart from the marital act and in ways that seem more like manufacturing than begetting? Christians offer differing answers to this type of inquiry because they take contrary positions on the moral permissibility of extramarital reproductive technologies. Some argue that none of them are licit (Austriaco 2012), while others maintain that some are not immoral (Rae 1996).\nEnd of Human Life\nBioethical decisions at life’s end primarily involve answering questions about what constitutes appropriate treatment, the withdrawing or withholding of it, and proper administration of palliative care. For virtually all Christian bioethicists (Austriaco 2012; Keown 2002), a physician may not intentionally kill her patient. However, that does not mean that one is obligated to keep a patient alive at all costs. A physician may act in a way that advances her patient’s good by relieving substantial burdens even if she knows that such action will shorten whatever time remains in the patient’s life. So, for example, a physician may increase her patient’s intake of morphine in order to better manage his pain, even though the physician knows that it will likely hasten death.\nThere are, however, some secular bioethicists who maintain that because a patient’s autonomy and understanding of what is in his best interests are paramount in assessing a physician’s responsibility to her ailing patient, there are cases in which physician-assisted suicide is justified if the patient is rational, fully informed, and freely consents (Quill 1991; Smith 2012). This is not to say that Christian bioethicists deny that patient autonomy should play any role in bioethical decision-making. Rather, they argue that patient autonomy cannot be exercised in a way that requires the physician to cooperate with immoral ends (Austriaco 2012; Keown 2002).\nConscience Protection and Professional Responsibility\nAs secular bioethics increasingly becomes the dominant way that medicine understands its moral obligations, conscientious objection among religiously observant health professionals will likely increase. This is because secular bioethics relies heavily on a school of thought known as principlism (Beauchamp and Childress 2013). It maintains that health professionals should assess the morality of their clinical judgments on the basis of four principles — autonomy, nonmaleficence, benevolence, and justice — while at the same time excluding from their judgments contested metaphysical beliefs about the nature of the human person that are usually tightly tethered to religious traditions. Thus a patient’s good is determined almost exclusively on what he chooses to believe is in his interests and what fulfills his preference satisfaction.\nUnder a medical establishment shaped by principlism, there will be religiously observant health professionals who will decline for reasons of conscience to participate in, or refer a patient to physicians who are willing to provide certain procedures (e.g., abortion, euthanasia, sex reassignment surgery) that the patient believes are necessary for his wellbeing as he understands it. Some argue that such health professionals, with limited exceptions, should be viewed as acting in an unethical manner (Dickens 2009). Others, however, argue they should be accorded strong conscience protection, since the procedures are not contested for medical reasons, but rather for reasons having to do with deep and differing philosophical and theological positions for which the architects of modern liberal societies had promised tolerance (Kaczor 2012).\nFrancis J. Beckwith is Professor of Philosophy and Church-State Studies, and Co-Director of the Program in Philosophical Studies of Religion in the Institute for Studies of Religion, at Baylor University in Waco, Texas.\nREFERENCES AND RECOMMENDED READING\nAustriaco, Nicanor Pier Giorgio. 2012. Biomedicine and Beatitude: An Introduction to Catholic Bioethics. Washington, DC: Catholic University of America Press.\nBeauchamp, Tom L., and James F. Childress. 2013. Principles of Biomedical Ethics. 7th ed. Oxford: Oxford University Press.\nDickens, Bernard M. 2009. “Legal Protection and Limits of Conscientious Objection: When Conscientious Objection Becomes Unethical.” Medicine and Law 28:337 – 47.\nGeorge, Robert P., and Christopher Tollefsen. 2011. Embryo: A Defense of Human Life. 2nd ed. Princeton, NJ: Witherspoon Institute.\nKaczor, Christopher. 2012. “Conscientious Objection and Health Care: A Reply to Bernard Dickens.” Christian Bioethics 18:59 – 71.\nKeown, John. 2002. Euthanasia, Ethics and Public Policy. Cambridge: Cambridge University Press.\nQuill, Timothy. 1991. “Death and Dignity: A Case of Individualized Decision Making,” New England Journal of Medicine 324:691 – 94.\nRae, Scott B. 1996. Brave New Families: Biblical Ethics and Reproductive Technologies. Grand Rapids: Baker.\nRae, Scott B., and Paul Cox. 1999. Bioethics: A Christian Approach in a Pluralistic Age. Grand Rapids: Eerdmans.\nSmith, Stephen S. 2012. End-of-Life Decisions in Medical Care: Principles and Policies for Regulating the Dying Process. Cambridge: Cambridge University Press.\nTooley, Michael. 1983. Abortion and Infanticide. Oxford: Oxford University Press.\nSign up complete.","Definition of Hippocratic Medicine\nHippocratic Medicine has six primary requirements:\n- Transcendence essential to medicine\n- Physician & patient accountable to a higher authority\nMedicine as a Moral Activity\n- Medicine is a moral activity\n- Physicians help patients decide what they ‘ought’ to do\nLife Not Death\n- Physicians promise not to intentionally kill or do harm\n- Complete separation of killing and healing in society\n- Covenantal relationship between physician and patient\n- Professional relationship throughout illness until death\n- Informed by medical judgment, conscience and faith\n- Preserved by freedom to refuse harmful treatment\n- Moral consensus amongst like-minded practitioners\nHistory of the Oath of Hippocrates\nThe Hippocratic Oath was probably not penned by Hippocrates himself but, most likely, by a small collection of his students in the decades following his death. Nonetheless, the Oath named after him almost certainly represents the philosophy espoused by Hippocrates and by which he practiced medicine in his day. For the first time, this physicians’ Oath codified an ethical standard for the art, which, first of all, transcends the vicissitudes of societal law and, further, which specified a professional dedication to the sanctity of life and a trust-based relationship between doctors and their patients.\nFor 25 centuries, the Oath stood unamended and uncontested as the template for medical practice. Anthropologist Margaret Mead wrote:\nFor the first time in our tradition there was a complete separation between killing and curing. Throughout the primitive world, the doctor and the sorcerer tended to be the same person. He with power to kill had power to cure, including specially the undoing of his own killing activities. … With the Greeks, the distinction was made clear. One profession, the followers of Asclepius, were to be dedicated completely to life under all circumstances, regardless of rank, age, or intellect – the life of a slave, the life of the Emperor, the life of a foreign man, the life of a defective child. … [T]his is a priceless possession which we cannot afford to tarnish, but society always is attempting to make the physician into a killer – to kill the defective child at birth, to leave the sleeping pills beside the bed of the cancer patient. … [I]t is the duty of society to protect the physician from such requests. (Marker, 1991)\nThe importance of the immutability of this code of ethics cannot be overstated. The dangers of societal expectations, or even mandates, to violate that professional code are not merely hypothetical. After the second World War, it once again became necessary to codify professional conduct for physicians, but this time, in international law. Following the Nuremberg Trials and the “Subsequent Nuremburg Trials” of 1946-47 for Nazi doctors accused of crimes involving horrific examples of human experimentation, the Declaration of Geneva was adopted in 1948 by the General Assembly of the World Medical Association in Geneva. This declaration was a modern restatement of the tenets of the Hippocratic Oath which would forever prohibit the physician-perpetrated atrocities to humanity exposed among at least 20 Nazi doctors. These same atrocities were defended by the accused physicians based on their legality and by the mitigating circumstance that the doctors were operating under lawful orders of their superiors. The tribunal found the doctors guilty on the basis of violating a “higher law” than that of a nation. If such a professional code is not immutable, then these doctors were convicted unjustly. Clearly, by today’s standards, they would have been acquitted.\nSadly, this declaration has not been treated as immutable either, but has undergone a series of politically-correct amendments that considerably weaken it. For instance, the sanctity of human life has been obscured in the current form of the Declaration of Geneva. The following bracketed portion of the original provision was completely deleted in 2005:\nI will maintain the utmost respect for human life, [from the time of its conception, even under threat, I will not use my medical knowledge contrary to the laws of humanity].\nDespite clear examples of the wisdom of- and necessity for the Oath, some of its basic tenets have come under fire in recent decades.\nVersions of the Hippocratic Oath\nIt should be instructive to compare this translation of the original Hippocratic Oath from the 4th Century BC, with the subsequent 1995 and 2009 revisions.\nOriginal Oath of Hippocrates\nI swear by Apollo, Physician and Aesclepius, Hygeia and Panacea and all the gods and goddesses, making them my witness, that I will fulfill according to my ability and judgement, this oath and this covenant:\nTo hold him who has taught me this art as equal to my parents and to live in partnership to him, and if he is in need of money to give him a share of mine, and to regard his offspring as equal to my brother in male lineage and to teach them this art – if they deserve to learn it – without fee and covenant; to give a share of precepts and oral instruction and all the other learning to my sons and to the sons of him who has instructed me and to pupils who have signed the covenant and taken an oath according to the medical law, but to no one else.\nI will apply dietetic measures for the benefit of the sick according to my ability and judgement; I will keep them from harm and injustice.\nI will neither give a deadly drug to anybody if asked for it, nor will I make a suggestion to this effect. Similarly, I will not give a woman an abortive remedy. In purity and holiness I will guard my life and my art.\nI will not use the knife, not even from sufferers from stone, but will withdraw in favour of such men as are engaged in this work.\nWhatever house I visit, I will come for the benefit of the sick, remaining free of all intentional injustice, of all mischief and in particular of sexual relations with both female and male persons, be they free or slaves.\nWhatever I may see or hear in the course of the treatment or even outside the treatment in regard to the life of men, which on no account one must spread abroad, I will keep to myself holding such things shameful to be spoken about.\nIf I fulfill this oath and do not violate it, may it be granted to me to enjoy life and art, being honoured with fame among all men for all time to come. If I transgress it and swear falsely, may the opposite of all this be my lot.\nTranslated by J. Chadwick and W.N. Mann 1950\nHippocratic Oath | 1995\nI SWEAR in the presence of the Almighty and before my family, my teachers and my peers that according to my ability and judgment I will keep this Oath and Stipulation:\nTO RECKON all who have taught me this art equally dear to me as my parents and in the same spirit and dedication to impart a knowledge of the art of medicine to others. I will continue with diligence to keep abreast of advances in medicine. I will treat without exception all who seek my ministrations, so long as the treatment of others is not compromised thereby, and I will seek the counsel of particularly skilled physicians where indicated for the benefit of my patient.\nI WILL FOLLOW that method of treatment which according to my ability and judgment, I consider for the benefit of my patient and abstain from whatever is harmful or mischievous. I will neither prescribe nor administer a lethal dose of medicine to any patient even if asked nor counsel any such thing nor perform art or omission with direct intent deliberately to end a human life. I will maintain the utmost respect for every human life from fertilization to natural death and reject abortion that deliberately takes a unique human life.\nWITH PURITY, HOLINESS AND BENEFICENCE I will pass my life and practice my art. Except for the prudent correction of an imminent danger, I will neither treat any patient nor carry out any research on any human being without the valid informed consent of the subject or the appropriate legal protector thereof, understanding that research must have as its purpose the furtherance of the health of that individual. Into whatever patient setting I enter, I will go for the benefit of the sick and will abstain from every voluntary act of mischief or corruption and further from the seduction of any patient.\nWHATEVER IN CONNECTION with my professional practice or not in connection with it I may see or hear in the lives of my patients which ought not be spoken abroad I will not divulge, reckoning that all such should be kept secret.\nWHILE I CONTINUE to keep this Oath unviolated may it be granted to me to enjoy life and the practice of the art and science of medicine with the blessing of the Almighty and respected by my peers and society, but should I trespass and violate this Oath, may the reverse be my lot.\nAdapted and endorsed by 35 inter-faith ethicists and physicians.\nCopyright, 1995, Value of Life Committee, Inc., P.O. Box 35279; Brighton, MA 02135.\nHippocratic Oath | 2009\nIn the presence of the Almighty, I promise to keep this Oath to the best of my ability and judgment. Those who have taught me the art of medicine I will respect, and will seek to faithfully impart my knowledge to those who also accept this covenant, and to whom I am a mentor.\nI will always seek the healing and comfort of those who are sick according to my ability and medical judgment, protecting them from harm and injustice.\nI will not help a patient commit suicide; neither will I help a woman obtain an abortion.\nIn purity and holiness, I will guard my professional moral integrity.\nWhen indicated, I will seek the counsel of those with appropriate special skills for the benefit of my patient.\nI will always act for the benefit of the sick, treating them with respect and dignity, and avoiding all sexual involvement with my patients.\nWhatever I may see or hear about my patients, I will hold in strict confidence.\nMay I be found faithful to these promises and so enjoy life and the practice of the art of medicine at all times."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:250f0dd6-4e3e-43f7-8032-ecd6dfc2da58>","<urn:uuid:7d835de7-fc38-48fc-8917-1f5eff7c1bd5>"],"error":null}
{"question":"Could you compare the hardness of quartz found in Kentucky sandstones with the hardness of a typical kitchen knife?","answer":"Quartz, which is abundant in Kentucky sandstones, has a hardness of 7.0 on the Mohs scale, while a small knife has a hardness of 5.5. This means that quartz is significantly harder than a kitchen knife and would be able to scratch it, not the other way around.","context":["Silicates are arrangements of the elements silicon and oxygen with a wide variety of other elements. The most common silicates are quartz and feldspars. Quartz is abundant in Kentucky, and some of the feldspars occur as very fine grains in sandstones in Kentucky.\nCrystal system: orthorhombic. Cleavage: tabular. Color: clear to white, with yellow or red iron staining common. Hardness: 4.5-5. Luster: vitreous. Specific gravity: 3.4.\nCalamine is an older name for the rare mineral hemimorphite. It forms in oxidized zinc deposits. Hemimorphite is commonly associated with smithsonite and cerussite, and occurs in clusters of radiating, acicular crystals. A massive variety shows worm-like shapes. Hemimorphite occurs in zinc-bearing vein deposits in the Western Kentucky Fluorspar District in Crittenden and Livingston Counties.\nCrystal system: monoclinic and triclinic. Cleavage: good at 90 degree angle. Hardness: 6.\nThe feldspars are an important rock-forming group of minerals, but their occurrence in Kentucky is limited to very small detrital fragments of sandstones and cement that are only visible with microscopes.\nCrystal system: isometric. Color: various colors. Hardness: 6.5-7.5. Luster: vitreous. Specific gravity: 3.5-4.3. Uses: semi-precious stone, industrial abrasive saws, polishing tools.\nThe garnet group of silicates has a diverse chemical composition consisting of calcium, aluminum, magnesium, and chromium silicates. Some minerals included in the garnet class are pyrope (red to black), almandine (red), grossularite (green yellow), and uvarovite (green). It is translucent to transparent. Pyrope and almandine occur abundantly in ultramafic dikes in Elliott County, and many can be obtained by panning in the alluvial sediments near the dikes. Garnets can also be found in some glacial erratics in northern Kentucky and in metamorphic rocks in the Appalachian Mountains in Virginia and North Carolina.\nCrystal system: monoclinic. Cleavage: cleaves into sheets; flexible when bent. Color: dark gray to black (biotite), white to light brown (muscovite). Hardness: 2.5 (biotite)-3.0 (muscovite). Specific gravity: 2.7 (muscovite)-3.0 (biotite).\nMuscovite and biotite generally occur in igneous and metamorphic rocks, but in Kentucky they are commonly found as detrital sediment in sandstones, shales, and clays. Mica minerals are commonly mistaken for gold because of their golden color and cleavage, which causes light to be reflected easily. The magnesium-rich phlogopite has similar physical characteristics as biotite and muscovite, and is found in kimberlite dikes in Elliott County.\nCrystal system: orthorhombic. Fracture: conchoidal. Color: green. Hardness: 6.5-7. Luster: glassy. Specific gravity: 3.3-4.3.\nOlivine is a common igneous rock-forming mineral and occurs in both basaltic rocks and the kimberlite dike in Elliott County. Transparent gem-quality varieties are known as peridots. Olivine alters readily to serpentine.\nFracture: uneven. Color: black to dark brown. Hardness: 5.5. Luster: metallic. Specific gravity: 4.\nPerovskite can occur as cubic crystals and massive or reniform (kidney-shaped) masses. It is found in ultramafic rocks and in the peridotite dikes in eastern and western Kentucky.\nCrystal system: hexagonal. Fracture: conchoidal. Color: colorless or white, but may be tinted various colors (e.g., purple, amethyst). Hardness: 7.0. Streak: colorless. Luster: glassy. Specific gravity: 2.65. Uses: jewelry; prehistoric arrowheads, knives (flint); gravel (chert).\nQuartz is the hardest, most resistant mineral found in abundance in Kentucky. It is the main constituent in sandstones and geodes, and also occurs as vein quartz. Crystals usually consist of six-sided hexagonal prisms capped by pyramids on one or both ends. Quartz crystals are found in geodes that occur in several different rock types, particularly limestone. In south-central Kentucky, valleys and stream beds downslope from the Warsaw-Salem Formation are filled with geodes, some containing amethyst (another variety of quartz).\nSeveral cryptocrystalline (microscopic crystals) varieties of quartz occur in Kentucky. They are commonly recognized on the basis of their fibrous texture and granularity. The fibrous varieties include chalcedony, agate, onyx, and jasper, and granular varieties include chert and flint.\nAgate has delicate and varying shades of color arranged in layers. In the typical occurrence the bands are irregular, curved, or in concentric patterns. Agate is used as an ornamental material or in semi-precious jewelry. The color banding is usually related to chemical impurities; for example, iron gives a red or orange color and manganese or calcium give black or blue colors.\nFor the past decade, beautiful specimens of red, black, yellow, and gray banded agate have been discovered in Estill, Jackson, Powell, Madison, and Rockcastle Counties. These Kentucky agates are derived from the Renfro-Borden Formation of Early Mississippian age and can be collected along some river drainages where the Borden is exposed to weathering. Many of these agates are displayed at local rock shows. To see more pictures of Kentucky agates see the Kentucky Agate section of this web site.\nFlint is dark brown to black and breaks with a conchoidal fracture into fragments with sharp cutting edges. It is found in limestones or in soils derived from limestones.\nChert (and Jasper)\nChert and flint are cryptocrystalline varieties of quartz. Chert is usually gray to white; flint is dark brown to black. Chert and flint are very hard and break with a splintery fracture. Chert is usually associated with dolostone and limestone and occurs as lenses, irregular layers, and nodules, but some rock units are composed almost entirely of chert. Chert abounds in the upper St. Louis Limestone in south-central Kentucky between Glasgow and Somerset. The Boyle Dolomite of Silurian age also contains abundant gray, blue, and black chert. Any roadcuts or active or abandoned quarries in this region in the St. Louis Limestone or Boyle Dolomite would contain numerous chert nodules. Chert and flint were the most common silicate minerals used by early Native Americans for making arrowheads. Because chert and flint have a conchoidal fracture, they are easily shaped into arrowheads.\nJasper is an impure variety of quartz that has been colored some shade of red by iron oxide inclusions. The name \"Jasper\" may also be used for some siliceous agate material that has replaced organic material in petrified wood. It is used as an ornamental stone and in jewelry.\nOpal is an amorphous, massive silicate that exhibits a conchoidal fracture and has a characteristic play of colors caused by its water content. Opal is not common in Kentucky, but may occur in some siliceous fossils, particularly in the black shale, and in microscopic amounts in cherts and chalcedony. Opal is not a stable mineral, and in geologic time alters to other silicate minerals.\nMany fossils found in Kentucky are silicified. This means the original material of the fossil has been replaced with quartz. These small brachiopod fossils from Lexington are silicified. Click on the image to see the quartz inside. When the brachiopods are broken open you can see the white to clear quartz inside. In some cases the quartz is massive, in others crystalline.\nCrystal system: monoclinic. Color: green. Hardness: 3-5. Luster: greasy, wax-like. Specific gravity: 2.5. Uses: asbestos.\nSerpentine occurs in both a platy and a fibrous variety. The most common variety is chrysotile, which is the chief source of asbestos. In Kentucky, serpentine occurs in peridotite dikes.\nClay minerals are a subgroup of silicates that comprise the various claystones, such as ball clay, flint clay, and fuller's earth.\nIllite is the constituent of many shales and is an intermediate clay between montmorillonite and muscovite. It has more potassium than montmorillonite, but is not expandable or absorptive. It is structurally similar to chlorite, but chemically different.\nCrystal system: amorphous. Color: green. Streak: colorless or greenish, but lighter than the grains themselves. Luster: earthy to dull. Specific gravity: 2.3. Tenacity: brittle. Uses: fertilizer, soil conditioner.\nGlauconite, a variety of illite, occurs disseminated in shales, sandstones, and limestones, and is commonly associated with phosphate pebbles and iron sulfides. The Floyds Knob Bed of the Borden Formation (Mississippian) is a glauconitic siltstone that crops out in a semicircle around the Outer Blue Grass.\nCrystal system: monoclinic. Hardness: 2-2.5. Luster: earthy. Specific gravity: 2.6.\nKaolinite is the chief component of ball clay and flint clay. It has a sheet structure, and is therefore not as absorbent as montmorillonite.\nHalloysite is a hydrated variety of kaolinite (crystal system: amorphous; fracture: conchoidal; color: white, yellowish-white, gray, to green; hardness: 1.5; streak: white; luster: earthy to pearly; specific gravity: 2.1; tenacity: brittle) with little or no plasticity. Halloysite has a distinctive tube-like structural appearance.\nCrystal system: monoclinic. Hardness: 1-1.5. Specific gravity: 2.5.\nMontmorillonite is very fine grained, and visible only with powerful microscopes. It is the main mineral in bentonite and fuller's earth. Montmorillonite is called an expanding clay because the arrangement of its crystal lattice allows frequent and extensive substitution of additional minerals; actual composition may vary depending on iron, magnesium, zinc, aluminum, and silicate ratios.\nVermiculite is a related montmorillonite clay mineral that has the absorbent but not the expandable characteristics of typical montmorillonite.","Moh’s Scale of Hardness\nThere are two methods to measure the hardness of materials: scratch hardness and static load indentation hardness. Scratch hardness, also known as Mohs hardness, is a relative hardness and is rather rough.\nIt uses ten natural minerals as standards. The hardness order does not represent the absolute size of a particular mineral’s hardness, but indicates that a mineral of higher hardness order can scratch a mineral of lower order. The hardness of other minerals is determined by comparison with these standard minerals.\nThe unit of Mohs hardness is kilogram-force per square centimeter (kgf/cm²), denoted as [Pa]. It’s a standard for expressing a mineral’s hardness, first proposed in 1824 by German mineralogist Frederich Mohs. The hardness is represented by the depth of the scratch made on the surface of the tested mineral using the scratch method with a pyramid-shaped diamond needle.\nThe hardness scale is as follows: talc 1 (softest), gypsum 2, calcite 3, fluorite 4, apatite 5, orthoclase (also known as feldspar or periclase) 6, quartz 7, topaz 8, corundum 9, diamond 10 (hardest). Mohs hardness is also used to express the hardness of other solid materials.\nFor a more specific method: one would scratch the mineral to be tested against the standard hardness on the Mohs hardness scale to determine the hardness of the tested mineral.\nFor example, if a mineral can scratch calcite and be scratched by fluorite, then the hardness of that mineral is between 3 and 4. Alternatively, one can use a fingernail (hardness 2-2.5), a coin (hardness 3.5), or a small knife (hardness 5.5) to scratch the mineral in order to broadly determine its hardness.\n|Representative Mineral Names||Common Uses||Hardness Scale|\n|Talc, Graphite||Talc is the softest known mineral, commonly used in the form of talc powder.||1|\n|Skin, Natural Arsenic||1.5|\n|Nails, Amber, Ivory||2.5|\n|Gold, Silver, Aluminum||2.5~3|\n|Calcite, Copper, Pearls||Calcite can be used as carving material and industrial raw material.||3|\n|Fluorite (also known as Fluorspar)||Carving, Metallurgy, Building Materials||4|\n|Phosphorite||Phosphorus is an important component of biological cells; it is used as raw material in feed, fertilizer, and chemical production.||5|\n|Glass, Stainless Steel||5.5|\n|Orthoclase, Tanzanite, Pure Titanium||6|\n|Teeth (outer layer of crown)||The main component is hydroxyapatite.||6~7|\n|Soft Jade – Xinjiang Hetian Jade||6~6.5|\n|Pyrite||It is used as raw material for the production of sulfuric acid; gold refining; and can also be used in medicinal purposes.||6.5|\n|Hard Jade – Burmese Jadeite and Jade||6.5~7|\n|Quartz Glass, Amethyst||7|\n|Electric Stone, Zircon||7.5|\n|Quartz||According to the old hardness scale, quartz is rated as 7.||8|\n|Topaz, Chromium, Tungsten Steel||On the old hardness scale, topaz is rated as 8.||9|\n|Moissanite||Synthetic gems are 2.5 times brighter than diamonds and cost 1/10th of the price.||9.5|\n|Corundum||Corundum is rated as 9 on the old hardness scale. Natural gems such as rubies and sapphires are now considered types of corundum, as is the hardness of synthetic sapphire crystals.||12|\n|Diamond||Diamonds are rated as 10 on the old hardness scale, making them the hardest natural gem on earth.||15|\nWhat is Mohs Hardness?\nMohs Hardness is a standard that indicates the hardness of minerals, first proposed in 1824 by German mineralogist Friedrich Mohs. This standard is established by using a pyramid-shaped diamond drill to scratch the surface of a mineral, with the depth of the scratch indicating the hardness.\nThe hardness of a mineral refers to its ability to resist certain external mechanical forces such as scratching, indentation, or grinding. In mineralogy, the hardness often referred to is Mohs hardness, which is the scratch hardness compared to the Mohs hardness scale.\nThe Mohs hardness scale is based on ten minerals of different hardness, divided into ten levels from low to high: 1. Talc; 2. Gypsum; 3. Calcite; 4. Fluorite; 5. Apatite; 6. Orthoclase; 7. Quartz; 8. Topaz; 9. Corundum; 10. Diamond.\nIn use, standard minerals are scratched against minerals of unknown hardness. If the mineral can be scratched by apatite but not by fluorite, its hardness is determined to be between 4 and 5.\nThis method was established and named by German mineralogy professor Friedrich Mohs (1773-1839). However, accurate measurement of mineral hardness still requires a microhardness tester or hardness tester. Mineral hardness is also one of the physical properties of minerals. Minerals with high hardness have been widely used in industrial technology.\nDiamonds, corundum, and other minerals are not only used in industry, but also become precious gemstones. As gemstones, they usually have a high hardness.\nFor example, the hardness of opal is 5.5-6.5, quartz is 6.5-7, sphalerite is 7.5-8. Tsavorite is 8.5, and the hardness of sapphires and rubies is 9, second only to diamonds. People choose high-hardness minerals as gemstones, probably because they are wear-resistant, symbolizing their timeless value!\nAccording to needs, people have also developed a gem hardness scale to identify the mineral hardness of gemstones, from the softest to the hardest minerals: talc, gypsum, calcite, fluorite, apatite, zircon, corundum, silicon carbide, boron carbide, diamond, etc.\nWhen there is no standard hardness mineral, the simplest way to measure hardness is with a fingernail or a small knife. The hardness of a fingernail is 2.5, a copper coin is 3, and glass and a small knife are both 5. Those above 6 are almost all gemstone-like minerals."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:5db8f0b8-513b-4b28-96e0-3f01d87b3672>","<urn:uuid:6f5dff6d-ddef-43d0-937c-cbff7f05f328>"],"error":null}
{"question":"What are the key differences between project-based and problem-based learning approaches, and how do they complement writing instruction strategies?","answer":"Project-based learning focuses on investigating authentic questions over extended periods, while problem-based learning organizes content around problems rather than hierarchical topics, promoting autonomous learning and self-assessment. In terms of writing instruction, these approaches integrate well with strategies like student-generated writing, where students can produce closing papers, journals, and reports. The combination allows students to develop writing skills through meaningful tasks while solving real-world problems, supported by various roles (researcher, scribe, content editor, proofreader) and incorporating feedback through writing portfolios.","context":["The teacher can ask students to produce a one-minute closing paper on an index card at the end of each lesson in which they pose a genuine question about the topic studied that day, identify the key point from the content materials reviewed, summarize a discussion, or develop a question that might be used for a class test.\nWhen this happens, students discover the power of writing as a means of shaping ideas and clarifying meanings rather than as a way of correcting errors or fulfilling a class requirement.\nCurrently, spelling, planning, and revising are the areas we know most about tailoring writing instruction to meet the needs of students with LD. Opportunities to engage in meaningful writing were plentiful, as children not only responded in writing to the material they read, but kept a journal, generated personal -experience stories, and wrote reports that they shared with each other.\nImplementing a whole language program in a special education class. Teachers in 2nd and 3rd grade indicated that Arthur would hurry through writing assignments, doing little or no planning in advance, and writing quickly, taking short pauses to think about the spelling of a word or what to say next.\nWriting portfolios are a valuable tool for providing students with feedback regarding how well they incorporate various traits in their writing. Research on written composition: Reading Teacher, 51, Journal of School Psychology, 29, This student-generated information should be organized into categories either by the teacher or by the students with teacher guidance e.\nPerformance comparisons and impact on spelling. This program was implemented with 1st through 4th grade students with special needs in resource room classrooms.\nStudents are assigned to a group and given roles researcher, scribe, content editor, proofreader, and reporter for writing a brief summary that predicts the content of a lesson or unit text based on key vocabulary provided by the teacher.\nOften do not develop writing goals and subgoals or flexibly alter them to meet audience, task, and personal demands; Fail to balance performance goals, which relate to documenting performance and achieving success, and mastery goals, which relate to acquiring competence; Exhibit maladaptive attributions by attributing academic success to external and uncontrollable factors such as task ease or teacher assistance, but academic failure to internal yet uncontrollable factors such as limited aptitude; Have negative self efficacy competency beliefs; Lack persistence; and Feel helpless and poorly motivated due to repeated failure.\nTechnological tools can also provide support for planning and revising through the use of outlining and semantic mapping software, multimedia applications, and prompting programs.\nHandwriting and spelling instruction for students with learning disabilities: Any way you look at it, National Novel Writing Month is a time dedicated to students producing something truly great and original.\nSteve Graham, Karen R. Journal of Reading, 30, Literacy learning in whole language classrooms: A number of resources are available to help teachers do this e. Here is a quick breakdown of some posts you might like: Yet, for students with disabilities who tend to develop or exhibit chronic and pernicious writing difficulties, even this type of instruction may be inadequate.\nAn analysis of errors and strategies in the expository writing of learning disabled students.\nFor all of these, the teacher should first model how to use the strategy, then give students an opportunity to cooperatively apply the strategy while producing group papers, and finally let students practice using the strategy while writing individual papers.\nIn addition, text production processes can be supported or even circumvented in some instances by using spell checkers, word prediction programs, grammar and style checkers, and speech synthesis.\nA prompt sheet identifies the strategy steps and can be copied for each student or reproduced for a poster display. A third similarity between Arthur and other students with LD can be revealed by returning to our friend Snoopy once again.Second Sight: An Editor's Talks on Writing, Revising, and Publishing Books for Children and Young Adults [Cheryl B.\nKlein] on ultimedescente.com *FREE* shipping on qualifying offers. Whether you dream of writing a book for children or young adults, or you want to take a finished manuscript to the next level. I have been on hiatus from doing out-of-state teacher trainings recently for two reasons: 1) I'm writng a book on teaching writing, and 2) I'm preparing to retire from the classroom at the end of the school year.\nLearn about Purdue University's College of Liberal Arts, a college focused on strengthening the Undergraduate Experience, enhancing Graduate Education, and promoting Faculty Excellence. Here are some classroom activities to help you celebrate National Novel Writing Month.\nThis bar-code number lets you verify that you're getting exactly the right version or edition of a book. The digit and digit formats both work. A note for teachers: These lessons are posted so that you may borrow ideas from them, but our intention in providing this resource is not to give teachers a word-for-word script to follow.\nPlease, use this lesson's big ideas but adapt everything else. And adapt it recklessly; that's how you become an authentic writing teacher.Download","BY LAURA RIGOLOSI & SHERRISH HOLLOMAN\nProblems. There’s no shortage of them these days — the pandemic has spurred countless challenges and intense despair; there are too many to list. Teaching during the pandemic has been a challenge in and of itself, as we are always looking for ways for students to be engaged with curricula and drive their learning, and that’s hard to do whether we’re teaching in person or remotely.\nIf you think back to your college or grad school days, you may recall the constructivist thinkers, such as Jean Piaget, who believed that students learn best when they construct their own learning. Problem- and project-based learning offers teachers an opportunity to do just that — instead of telling students the answers, you can create a learning environment in which students learn through discovery, thinking, tinkering, reflecting, and developing answers on their own.\nWe may already be familiar with ways that we can bring this type of learning to in-person classrooms, but it can also be delivered to students who are learning in remote or blended environments.\nProblem-based learning vs. project-based learning\nProject-based learning is situated in real-life learning. The Buck Institute for Education defines project-based learning as a “teaching method in which students gain knowledge and skills by working for an extended period of time to investigate and respond to an authentic, engaging, and complex question, problem or challenge.” If you ever walk into a classroom and see students working on a project with an exciting buzz in the room, chances are, their teachers have designed a project-based learning task.\nIn our own lives, we know that when working on a project, we often discover a problem we didn’t realize we had — but once it surfaces, it demands a solution. (Remember those early pandemic days when we were acclimating to teaching remotely, but also trying to solve the problem of having no dedicated teaching space at home?) In teaching, this idea rings true, too. As we are learning more about a topic, we may discover a problem alongside our students, and this is the breeding ground for an exciting new project. This is the foundation of problem-based learning.\nProblem-based learning also offers students real-life learning opportunities, as well as the chance “to think creatively and bring their knowledge to bear in unique ways” (2020 Schunk, p. 64). Problem-based learning can look differently depending on the content and grade level, but often includes group discussions that allow for multiple perspectives on a topic, a simulated situation that involves role playing, or group work that includes both collaborative work and time to complete tasks individually.\nProblem-based learning promotes autonomous learning, self-assessment skills, planning time, project work, and oral and written expression skills. According to a July 2020 article from the Hechinger Report, problem-based learning has gained tremendous momentum, because it allows students to work more freely and at their own pace — a key advantage when learning remotely. In problem-based learning, the content and skills are organized around problems, rather than as a hierarchical list of topics. It’s also inherently learner-centered because the learner actively creates their own knowledge as they attempt to solve the problem.\nPutting the “Problem” into Practice\nAs former English teachers, we both understand the challenge of putting new professional learning into practice. For teachers who need a refresher on how to design a problem-based learning experience for their students, Problem Based Learning: Six Steps to Design, Implement and Assess breaks down the steps to move PBL into practice as follows:\nTo help put these problem-based steps into perspective, we can look to our recent work with partners from a high school in the South Bronx. The chemistry team there decided to use an anti-racist lens while addressing a problem that was very real to their students — fireworks. During the summer of 2020, there was a record number of firework incidents in New York City. According to an article in the New York Times, the city received over 1,700 fireworks complaints in the first half of June alone. Our partners used this problem as an opportunity for students to research fireworks from multiple lenses, and imagine how they might present their findings and recommendations to local officials. After all, shouldn’t New York Governor Andrew Cuomo hear from high school students in the Bronx about the effects fireworks have on their communities?\nHere’s what the framework might look like in this example:\nFrom here, we can imagine the possibilities for this framework, considering how students might address the underlying problem from different perspectives and content areas:\nTeaching and learning throughout a global pandemic has presented more than its share of challenges. Out of necessity, tremendous innovation has taken place with the use of technology, pedagogy, and curriculum. With problem-based learning, we can continue this innovation in our classrooms, offering our students opportunities to solve real world problems, demonstrate critical thinking, and collaborate with their peers. We would love to hear what problem-based learning tasks you are designing for your classrooms!"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:b74eefd6-179f-40d4-a7e3-e404550b1cc3>","<urn:uuid:2285de50-3939-41b9-a10b-f1d64c1e528a>"],"error":null}
{"question":"What are the key differences between mediation in divorce cases versus special education disputes?","answer":"In both contexts, mediation is voluntary and involves a neutral third party, but there are key differences. In special education disputes, mediation must be conducted by a qualified and impartial mediator trained in effective mediation techniques, and if successful, results in a legally binding settlement agreement that requires a new IEP meeting. In divorce mediation, sessions typically run for 1.5 hours over 4-6 sessions, using a single attorney mediator who helps negotiate a win/win outcome for the family without separate lawyers, resulting in a settlement agreement that must be submitted to court for final divorce approval.","context":["When a dispute arises between a parent and the school in an Individualized Education Program (IEP) meeting there are a few methods that can be utilized to work out the disagreement. Most School Districts will have at least one Informal Dispute Resolution (IDR) system in place that can be employed to work out the dispute. IDR will look different in every school district but most likely it will involve a meeting or phone call with a District employee who was not at the original IEP meeting discussing the disagreement and trying to come to a successful resolution. IDR is not mandatory and can be skipped if the parents want to exercise their rights under the Individuals with Disabilities Education Act (IDEA) procedural safeguards which could include Mediation or Due Process.\nA mediation is a meeting facilitated by a mediator used to find a peaceful settlement of the disagreement prior to starting costly litigation. The Requirements are:\n- Mediation is voluntary for both parties.\n- Mediation may not be used to delay or deny a parent’s right to a due process hearing or to deny other rights guaranteed under the IDEA.\n- Mediation must be conducted by a qualified and impartial mediator trained in effective mediation techniques.\n- Mediation can be on its own (informal mediation) or part of a due process complaint (formal mediation).\nIf a resolution is reached at the mediation, a legally binding settlement agreement will be signed by all parties involved. Once a settlement agreement is executed a new IEP meeting must be called to implement the services outlined in that agreement. If a resolution is not reached the next step would be a Due Process Hearing.\nDue Process Hearing\nA Due Process Hearing is typically held by the state department of education and presided over by an impartial hearing officer. The Due Process complaint form must outline the complaint and the proposed resolution. The other party has the right to respond and can file a Notice of Insufficiency (NOI) if they feel the complaint does not have enough information to proceed. Since the School District will involve an attorney it is recommended that you consult with an experienced Special Education Attorney before filing for due process.\nIf the Parents file for due process they have the burden of proof and a resolution session must be held between the School District and the Parents prior to the hearing. This resolution session can be waived if both parties agree and a formal mediation can be held instead. If a settlement is not worked out in either the resolution session or formal mediation then the case would proceed to a hearing. The hearing is similar to a court presiding and will include opening statements, presentation of evidence and cross examination.\nHearing Officer Decisions are final unless appealed to either State or Federal courts. You can’t file in State or Federal Court until after going through due process. Hearing decisions shall be made on substantive grounds unless procedural violations impeded the child’s right to a free appropriate public education, significantly impeded the parents opportunity to participate in the decision making process or caused a deprivation of educational benefits.\nWhat are Substantive Issues?\nSubstantive issues result in the denial of a free appropriate public education when the School District:\n- Does not address the child’s unique needs;\n- When the IEP does not provide some education benefits;\n- When the IEP is not followed; and\n- When the placement is not in the least restrictive environment.\nSome Due Process remedies might include:\n- Reimbursement – Such as, private therapies, private schools or Non-Public Schools paid out of pocket by the parents.\n- Compensatory Education – Receiving services from the school district that should have been provided over the past 2 years such as additional therapy hours.\n- Additional Services or Therapies both in school and out of school paid for by the District.\n- Different Placement including potentially a Non-Public School placement.\n- No punitive damages – parents can only get back money they spent.\n- Parents can get reimbursement of legal fees if they win the case. Expert witness fees are not reimbursable at this time.\nDue process complaints have a two year Statute of Limitations meaning the parents or school has up to 2 years to file. It is also important to note that if your child already has an IEP and you disagree with any part of it, the School District must maintain the current educational placement pending any proceedings. This clause most commonly referred to as a “Stay Put” means there can be no reduction of services while the disagreement is being worked out.","Divorce Mediation Frequently Asked Questions\nWhat is Divorce Mediation?\nMediation is the process of using a third party attorney to help negotiate and compromise a win/win outcome for all.\nWhy Choose Divorce Mediation?\nWhen using mediation, the goal is to help all parties and the family as a whole win. When using litigation, each attorney is looking for a win/lose outcome. The attorneys fight for their client’s rights whereas for the mediator the family is the client, resulting in a win/win. When using the litigation approach, the impact is a more costly and time consuming process with a tremendous negative emotional impact on the parents and the children.\nHow is mediation different from collaborative divorce?\nMediation is the negotiating of a divorce settlement or other dispute such as parenting issues using a single attorney mediator. The attorney mediator is trained to help the parties work together to develop solutions to all of the issues without the involvement of separate lawyers. This saves the cost of separate lawyers for both parties. Collaborative divorce involves each party having their own attorney for those who want to have a separate attorney.\nDoes it make sense to have children at mediation sessions?\nChildren should not be at the mediation sessions with the parents. If there are issues with the children, the divorce coach, who is a trained mental health professional, can work with the family to address these specific needs and concerns. Remember they are the victims of the separation and divorce and allowing them to have a voice in the process can make a huge difference in their lives because they usually feel most things are out of their control.\nChildren do not want to pick one parent over the other and they feel uncomfortable about talking about this, is it really good to get them involved?\nHaving the child discuss their feelings in a safe and secure environment, without fear of retribution, can make a difference in helping them move past any emotional issues. It helps them be a part of the solution and feel better about their future because now they feel more in control. This should be done wi\nHow long does divorce mediation take and how long are the sessions?\nEach session generally runs for an hour and a half and on average the entire process takes between 4 to 6 sessions.\nHow much does mediation cost?\nOur fees are reasonable and at a fraction of the cost of an attorney. We charge hourly and get paid at the beginning of each session .\nHow do I get started?\nWe offer a free phone consultation to help you determine if mediation is right for you. Contact us for more information or to schedule a consultation.\nIs it possible to get an unwilling party to try mediation?\nThere are various emotional stages that each party has to deal with internally before accepting their situation. Time can improve the situation or involving the party in our free consultation may help them understand the benefits of working in a cooperative way.\nCan I use an attorney during Divorce Mediation?\nYou always have the option of using a separate attorney during the process. However, because of the adversarial nature of divorce attorneys, it is possible for them to easily undo all the work completed during mediation. We do recommend that you use independent counsel to review the final agreement. We can recommend one for you at the end of the process to review the agreement before signing.\nAt the end of mediation, will we be divorced?\nYou will receive a settlement agreement which will be submitted to the court. Once signed and accepted by the court, you will be legally divorced.\nDo I need to bring anything to the first mediation session?\nWe will provide you with a form that will ask general questions regarding financials and family information.\nCollaborative Divorce Frequently Asked Question\nWhy would I use Collaborative Divorce instead of Mediation?\nIn some cases the parties cannot get along well enough to participate in a Divorce Mediation. If that is the case, there is another more peaceful way to accomplish a fair divorce. In Collaborative Divorce, the parties each have an attorney who is protecting them. While collaborative attorneys are trained in meditation techniques, it is very rare that clients in mediation have their advocates in the room with their mediators. The mediator does not represent either party, but the parties are encouraged to seek a review of their final agreement from their independent attorneys. In the collaborative process, clients attend a series of meetings accompanied by their attorneys; by their coaches; and/or by any other member of the collaborative team. The goal of the process is to create a global settlement in writing that meets the needs of the children and of the parents; a resolution acceptable to both spouses.\nWhat is collaborative practice and what is the collaborative team?\nCollaborative practice is a model for dispute resolution in which separated and divorcing couples, each represented by independent, specially trained attorneys, creatively reach agreement on all relevant issues without going to court and without threatening to do so. Collaborative divorce is a team approach often involving mental health professionals (\"divorce coaches\" and \"child specialists\"), and financial specialists all of whom agree to use neutral appraisers rather than appraisers favored by one party or the other. A primary goal of the collaborative model is to avoid the acrimony and trauma of subjecting the spouses and their children to the terrible stress and expense that comes with a divorce that is litigated in the courts.\nDo collaborative attorneys still zealously advocate for their clients?\nAbsolutely. Any good attorney knows that a negotiated settlement must work for both parties. Indeed, both parties must feel that they have “gotten” something (even if not everything) they needed. The role of the collaborative professional is to evoke the true needs and understand each party’s deepest concerns, helping a couple to develop resolutions that fall within a “range of reasonably acceptable options.” Our goal as attorneys is to “zealously” advocate. We can zealously advocate without being adversarial. Zealous advocacy does not mean the other spouse leaves the process with nothing, but rather that each spouse feels satisfied with his or her result.\nWhat kind of information will be disclosed?\nWith the exception of depositions under oath (which is not completely taboo), ALL documents and information typically exchanged in the traditional court based process is produced voluntarily. The parties will have to sign Financial Affidavits swearing under oath as to their income and assets, as well as liabilities. Because the Court may not accept your agreement without a sworn Financial Affidavit, it has to be done in the collaborative divorce process as well (and in a mediation). You will generally bring with you all of your financial statements such as bank accounts, investment accounts, mortgages, insurance, tax returns and other documents showing your income and expenses. Businesses, licenses, degrees, pensions, real estate and all other property is valued by neutral appraisers, specifically trained in the collaborative model. It is not uncommon for “ranges” in values to be established so that clients themselves can customize appropriate resolutions. The key to the collaborative process is that both parties agree to accurately and honestly disclose all assets, income and liabilities.\nWhy would I choose the collaborative divorce over mediation?\nSome people, for various reasons, are uncomfortable negotiating against his or her spouse without help of an attorney by his or her side. In the collaborative process, that help is right there; in the room; with the client every step of the way. While a mediator may provide some legal information, he or she does not advise either client. In a collaborative four way settlement meeting , it is common for both clients and for both attorneys to brainstorm as many options as possible. Once all of the options for each of the issues have been discussed, parties are typically in a better position to engage in a productive “give and take” in reaching an agreement, acceptable to both.\nHow is collaborative divorce different from a conventional divorce?\nThere is a big difference between a settlement that is negotiated during the conventional litigation process and one that takes place in the collaborative divorce process that prohibits court involvement or even the threat of court. Most conventional family law matters settle at the last minute before or during the trial. By that time, a great deal of money has been spent and emotional damage caused. The process is, for the most part driven by mutual coercion and fear which is why so many settlements occur just prior to trial. The settlements are often reached under conditions of considerable tension and anxiety and as a result are not done in consideration of important factors and do not provide for the proper support for the two parties. If the court makes a decision in your case, you have little control over what happens to you, your children and your property and often is not an ideal outcome for either party.\nIn contrast, the collaborative process is geared from the very beginning to make it possible for creative, respectful collective problem solving to occur. It is often quicker, often less costly, more individualized, less stressful, and almost invariably more satisfying. At its best, it is a process driven by mutual understanding and willingness to cooperate.\nWhat guarantees are there that all assets will be disclosed in the collaborative process?\nWhile there are no guarantees about anything in a mediation or the collaborative process, both require the parties to honestly disclose all assets, income and liabilities. Because each party has to submit a sworn affidavit to the court before their agreement will be accepted, a party intending to lie and hide assets or income during the divorce will also have to swear under oath to the court that all such information has been fully disclosed. If it is later found that a party did not disclose an asset or income, the court can re-open the case on the basis of fraud and can order the fraudulent party to pay the innocent party's legal fees as well as make a distribution order with respect to the assets that were not disclosed.\nCollaborative practice may not be for everyone. It is not always cheaper; it is not always quicker. The ultimate agreement however, is almost always better. Divorcing couples learn new ways to communicate in a manner that allows them to be better parents, better role models and often friends. The result is happier, more well-grounded children in healthier reconstructed families.\nIs a collaborative divorce cheaper than a traditional divorce? Is it faster?\nWhile there are no guarantees, the collaborative divorce process avoids the often wasteful and unnecessary hours of motions being filed, objections to the motions, hearings on the motions, nasty letters between attorneys, waiting in the halls of the courthouse while many other cases are called, and filing of appeals. Since the clients determine the frequency and timing of meetings, the pace is set by them and not by a judge handling hundreds of other matters. Obviously, cases with fewer issues take less time and cost less to complete. Even more complex cases, which will typically take more time and cost more money, are handled more efficiently than the traditional, court based model.\nDoesn't the collaborative divorce cost more because of the \"coaches\" and \"financial experts\" that are used?\nWhile there can be some expense for the coaches and financial experts, not every case needs these people involved in their case. What is beneficial about the collaborative process is that the parties with the guidance of their attorney can decide what resources to involve. The coaches are not necessary for each case, but when there is tremendous tension, they are expert at resolving disputes so the parties can move forward on the legal issues. Likewise, the financial experts are very good at moving quickly through the financial information and determining the value of assets such as pension plans, stock options, and investments. They are also very good at valuing businesses, or finding the right person to value the business. If this was done in a litigation environment in court, each party would hire their own expert to value the assets and there are usually extensive conflicts between the experts and the attorneys. The financial expert and coaches are only involved in the process to the extent necessary and are not \"on the clock\" for the whole process.\nFor whom is collaborative divorce the proper process choice?\nA collaborative divorce works best for:\nCollaborative practice is NOT a process for people destined to destroy their families and those seeking to punish and/or obtain revenge.\nA Peaceful Resolution"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:b065c7bc-2a42-48f4-9a1c-49f2afae53cf>","<urn:uuid:1f70228e-b049-4d71-9127-0d78e774719b>"],"error":null}
{"question":"What are the key similarities and differences between modern sports radio imaging and traditional sports radio talk formats from the 1960s?","answer":"While sports radio began in 1964 with Bill Mazer's show on WNBC focusing primarily on discussion and call-in formats, modern sports radio imaging has evolved to incorporate complex production elements like music integration, sound design, and emotional storytelling. Today's producers like Elisabeth Hart use sophisticated tools such as ProTools and plugins like RX7 to create highly produced content, while still maintaining the core sports discussion and analysis that characterized early sports radio. The format has moved beyond just talk to include elaborate imaging production for season openers, tributes, and promotional content that requires extensive research and collaboration with programming teams.","context":["Sports radio (or sports talk radio) is a radio format devoted entirely to discussion and broadcasting of sporting events. A widespread programming genre that has a narrow audience appeal, sports radio is characterized by an often-boisterous on-air style and extensive debate and analysis by both hosts and callers. Many sports talk stations also carry play-by-play of local sports teams as part of their regular programming. Hosted by Bill Mazer, the first sports talk radio show in history launched in March 1964 on New York's WNBC (AM).\nSoon after WNBC launched its program, in 1965 Seton Hall University's radio station, WSOU, started Hall Line, a call-in sports radio talk show that focuses on Seton Hall basketball. Having celebrated its 50th anniversary on air during the 2015–2016 season, Hall Line, which broadcasts to central and northern New Jersey as well as all five boroughs of New York, is the oldest and longest running sports talk call-in show in the NY-NJ Metropolitan area, and is believed to be the oldest in the nation.\nEnterprise Radio Network became the first national all-sports network, operating out of Avon, Connecticut, from New Year's Day 1981 through late September of that year before going out of business. ER had two channels, one for talk and a second for updates and play-by-play. ER's talk lineup included current New York Yankees voice John Sterling, New York Mets radio host Ed Coleman and former big-league pitcher Bill Denehy.\nSports talk is available in local, network and syndicated forms, is available in multiple languages, and is carried in multiple forms on both major North American satellite radio networks. In the United States, most sports talk-formatted radio stations air syndicated programming from ESPN Radio, SB Nation Radio, Sports Byline USA, Fox Sports Radio, CBS Sports Radio, or NBC Sports Radio, while in the Spanish language, ESPN Deportes Radio is the largest current network. In contrast, Canadian sports talk stations may carry a national brand (such as TSN Radio or Sportsnet Radio) but carry mostly local programming, with American-based shows filling in gaps. Compared to other formats, sports radio is not as popular on Internet radio; as a live and interactive format, it does not lend itself well to voice-tracking or automation (thus raising the cost and required labor to keep a station running 24/7), and most sports leagues place their radio broadcasts behind a paywall or provide their broadcasts directly to the consumer, though sports podcasting is a popular alternative to address this problem.\nAs with most other radio formats, sports radio uses dayparting. ESPN Radio, for instance, insists that its affiliates carry Mike and Mike in the Morning during morning drive time to provide as much national clearance as possible; in contrast, it carries less prominent programming in the afternoon drive to accommodate local sports talk, as well as in the evening (for its first two decades, rolling score updates aired under the banner of GameNight) to allow stations to break away for local sporting events. Somewhat unusually for radio, the late-night and overnight hosts have more prominence on a sports talk network, due to a near-complete lack of local preemption; Sports Byline USA, for instance, only operates overnights.\nSports radio stations typically depend on drawing an audience that fits advertiser-friendly key demographics, particularly young men with the disposable income to invest in sports fandom, since the format does not have the broad appeal to reach a critical mass in the general public. Prominent sports radio stations typically get their greatest listenership from live play-by-play of local major professional sports league or college sports franchises; less prominent stations (especially on the AM dial) may not have this option because of poorer (or for daytime-only stations, non-existent) nighttime signals and smaller budgets for rights fees.\nSports talk stations\n- RSN Racing & Sport - Melbourne, Victoria\n- 1116 SEN - Melbourne, Victoria\n- 91.3 SportFM - Fremantle, Western Australia\n- Sky Sports Radio - Sydney\n- ABC Grandstand - national, via DAB+ digital radio\n|CFAC||Sportsnet 960||Rogers Communications||Calgary||Alberta|\n|CFGO||TSN Radio 1200||Bell Media||Ottawa||Ontario|\n|CFRN||TSN Radio 1260||Bell Media||Edmonton||Alberta|\n|CFTE||TSN Radio 1410||Bell Media||Vancouver||British Columbia|\n|CISL||Sportsnet 650||Rogers Communications||Vancouver||British Columbia|\n|CKGM||TSN Radio 690||Bell Media||Montreal||Quebec|\n|CKST||TSN Radio 1040||Bell Media||Vancouver||British Columbia|\n|CJCL||Sportsnet 590||Rogers Communications||Toronto||Ontario|\n|CHUM||TSN Radio 1050||Bell Media||Toronto||Ontario|\n|CFRW||TSN Radio 1290||Bell Media||Winnipeg||Manitoba|\nIn 2009, Detroit's \"97.1 The Ticket\" WXYT-FM, thanks to the surprising time slot dominance of shows like Valenti and Foster, in addition to holding the play-by-play rights for the Detroit Tigers, Detroit Red Wings, Detroit Lions and the Detroit Pistons, became the USA's only sports talk radio station to be the highest rated station in their market, according to Portable People Meter rankings. The station relocated to the FM dial in October 2007 after existing on the AM dial for seven years prior, replacing a Free FM \"hot talk\" station, WKRK. This ratings success has led to WXYT-FM billing itself as the country's best sports station.\nWXYT-FM's recent influence has led to CBS Radio installing sports radio stations on the FM dial in Dallas (105.3 The Fan), Boston (98.5 The Sports Hub), Pittsburgh (93.7 The Fan), Washington, DC (106.7 The Fan), Baltimore (105.7 The Fan) and Cleveland (92.3 The Fan), in addition to simulcasting Philadelphia's heritage 610 WIP onto the former WYSP. Other non-CBS stations have also migrated to the FM dial, most notably Clear Channel's KFAN in Minneapolis, Greater Media's WPEN in Pennsylvania and Dispatch Media's WBNS-FM in Columbus, just to name a few.\nMore recently, a number large number of listeners have begun to make internet radio the primary medium of choice. Major market leaders such as ESPN, NBC, CBS, Yahoo, and Fox have invested in the creation of their own proprietary sports apps for streaming radio distribution on Apple, Android, and Windows devices. Other independent market leaders such as Sports Radio America choose to stream their content through non proprietary apps including Live 365 and Tune In.\nSortable lists of commercial sports talk radio stations:\n- Sirius XM 80 - ESPN Radio\n- Sirius XM 81 - ESPN Xtra\n- Sirius XM 82 - Mad Dog Sports Radio\n- Sirius XM 84 - ESPNU Radio\n- XM 86 - Sirius XM NBA Radio\n- XM 87 - Sirius XM Fantasy Sports Radio\n- Sirius XM 88 - Sirius XM NFL Radio\n- XM 89 - MLB Network Radio\n- Sirius XM 90 - Sirius XM NASCAR Radio\n- Sirius XM 91 - SiriusXM NHL Network Radio\n- XM 92 - SiriusXM PGA Tour Radio\n- Sirius XM 93 - Sirius XM Rush\n- Sirius XM 157 - ESPN Deportes Radio (in Spanish)\n- Sirius 207 - Sirius XM NBA Radio\n- Sirius 208 - SiriusXM PGA Tour Radio\n- Sirius 209 - MLB Network Radio\n- Sirius 210 - Sirius XM Fantasy Sports Radio\nInternet sports radio\n- RIVAL Radio 809 - sports programming all day long, with a little something special at night.\n- Sports Radio America - home to the next generation of independent sports radio talk show producers from around America.\n- Victory Lane Radio - racing on Monday nights and Football Frenzy on Wednesday nights.\n- Bradesco Esportes FM - Brazil's first sports radio network which covers mainly soccer, olympics, rugby among other sports. It is jointly owned by Grupo Bandeirantes de Comunicação and Grupo Bel.\n- BBC Radio 5 Live - national Public service broadcasting non-commercial network; combination of news/talk/sport format\n- Five Live Sports Extra - supplementary station to the above; exclusive live sports format\n- talkSPORT - national commercial network; features sports talk and live sports coverage\n- CBS Sports Radio\n- Champions Soccer Radio Network\n- ESPN Deportes Radio\n- ESPN Radio\n- Fox Sports Radio\n- Fútbol de Primera\n- Motor Racing Network\n- NBC Sports Radio\n- Performance Racing Network\n- SB Nation Radio (formerly the One On One Sports Radio Network, Sporting News Radio Network and Yahoo! Sports Radio)\n- Sports Byline USA\n- Sports Radio America\n- Enterprise Radio Network (United States)\n- KRAE (Cheyenne's ESPN sports radio) (USA, Wyoming) - contract ended 2013, now oldies radio\n- Réseau Sport Radio Network via Digital Radio Oceane (New Caledonia/Wallis & Futuna)\n- Sports Fan Radio Network (United States)\n- The Team (Canada)\nNotable syndicated programs\n- Don Cherry's Grapeline\n- Prime Time Sports\n- 2 Live Stews\n- The Ben Maller Show\n- Boomer and Carton\n- The Dan Le Batard Show with Stugotz\n- The Dan Patrick Show\n- The Doug Gottlieb Show\n- From the Press Box to Press Row\n- The Herd with Colin Cowherd\n- J. T. the Brick\n- The Jim Rome Show\n- The Michael Kay Show\n- Mike and Mike in the Morning\n- Mike and the Mad Dog\n- Mike's On\n- The Ryen Russillo Show\n- The Tony Kornheiser Show\n- The Toucher and Rich Show","Sports History in the making – Sports Imaging reloaded or…\nI was stunned by Elisabeth’s Kobe tribute on soundlcoud and reached out. We chatted for a while and thought I needed to share her learnings, findings and awesome way of marrying plug and play to songs with you. Enter Elisabeth!\n1. Can you give me a bit background on yourself, your career, achievements.. I am sure a lot of the readers will know you or of you, but it puts a lot of the below in context I assume.\nWell, my name is Elisabeth Hart, and I graduated from the Radio Broadcasting program at Fanshawe College too many years ago. (Not THAT many years ago…) I started my full time radio career at 1240 CJCS in Stratford, Ontario, and a few years and the launch of 1 new station (107.7 Mix FM) later, I moved onto what at the time was CHUM Peterborough (Country 105/1420 CKPT). 9 years, 2 management changes and 1 station flip later (1420 became Energy 99.7), I moved on up to Rogers Radio Toronto. I was hired as a Commercial Producer for 98.1 CHFI, KiSS 92.5, Sportsnet 590 the FAN and 680 News. During my time there, my role has evolved, and I am currently the Imaging Producer for Sportsnet 590 the FAN. I still do some Commercial Production, as well as assisting with Imaging for the Country stations that Rogers has across Canada, and I’ve done Imaging for the News, AC and CHR stations as well. I also do a lot of voiceover work for commercials and various other projects within Rogers, so all of that keeps me hopping!\n2. How is it to work for some of the biggest radio and sports brands in Canada? How do the task differ between the different stations / brands? What is the stylistic approaches and how difficult is it to change hats?\nIt’s great to get to work with some of the best brands in Canada – but they don’t get to BE the best brands in Canada without the best people. It really is about having the best team around you to help you and support you. If I need help with something I’m working on, I know that I can go to any of my co-workers and ask for their help, and they’ll offer their advice. It’s then up to me to decide whether that advice works stylistically with what I’m working on or not. We’re not doing heart surgery here where the patient lives or dies by whether we cut the promo the right away – everything is open to interpretation and is subjective. But knowing that you can get different feedback from really talented people is always a help.\nAs far as changing hats, for me it’s important to focus. As much as I may need to multi-task in the course of a day, if I’m sitting there working on a Sports promo while I’m thinking about what I’m going to do with the Country splitters I need to work on, then there’s a chance that the Sports promo might accidentally end up with a Country feel to it. And for me, that’s not the right move. Plan things out, set aside the time to work on and focus on that one project for that format before you move on. And if an idea comes to me for something different than what I’m working on, I’ll write it down so that I remember it for later, rather than abandoning what I’m in the middle of to see that idea through. There will always be times where fires come up and something needs your immediate attention, but as much as you can, focus on one project at a time.\n3. How do your days look like? Is there a blueprint? A routine ?\nMy days are usually pretty different, which I like. There are, of course, always some projects that need to get completed every day, and I have to balance what commercial projects I’m working on that need my attention as well, but there’s not necessarily a blueprint to each day. I usually meet a couple of times a week with the Assistant Program Director of the FAN to talk about what’s happening that we might need to promote, any shows coming up that need our attention, any new seasons starting, etc. I usually try and get some time in a couple of times a week as well just to scour the internet and see what’s out there that other people are doing that’s cool that I can take inspiration from.\n4. What is your baby? Most fun project?\nFor me, every year my favourite project is the Season Opener for the Toronto Blue Jays baseball season. I’ve been a Blue Jays fan ever since I was young (wait, does that mean I’m not young anymore? DAMMIT!!), and to this day I still even hold a part time job through the summer working at the Jays games. As a very passionate fan, and someone who listened to the game broadcasts growing up, to be able to produce the big season opener piece that kicks off the season across the network, is just amazing. There’s NHL teams in 7 different markets across Canada, and while the whole country rallied behind the Raptors when they won the NBA Championship title, there’s still nothing quite like the way Canada supports the Blue Jays, no matter HOW good or bad the team is doing. So being able to hype people up for another baseball season all across the country like that is pretty special to me.\n5. How important is institutional knowledge for a format like sports?\nTo be honest, it’s not the most important thing. Again, it comes down to the people you have around you, and also your willingness to research. The guy who was doing Imaging for Sportsnet before me is an incredibly talented producer named Anthony Conte. He’s now imaging 98.1 CHFI, and is the AC Imaging Lead for Rogers Radio. He was doing an amazing job with 590, to the point that I was intimidated when I took over the job – I wasn’t sure I’d be able to keep up his standard! But he isn’t a big sports guy. He was able to do such a great job because he would research – and because the other people – the assistant PD, the writer, the jocks, the show producers, etc. would all help out too. Even with myself, I’m not a huge basketball fan. So when it came to producing that Kobe Bryant tribute that you heard, I had to do a lot of research. I knew of Kobe Bryant in the same way everyone did, but not in the same way a basketball fan knew him. So I Google’d him, I watched Youtube videos, I asked my Assistant PD what the *most* important thing for Kobe was, and we had people pulling audio clips from different sources and sharing them with me. While it may have been a piece that “I” produced, there were a bunch of different people involved in it. Can you do great Imaging without having an incredible knowledge of sports? 100%. As long as you’re willing to put in the time and effort to research what you don’t know.\n6. What DAW do you use?\nProTools 12 for PC.\n7. What are your favorite plugins?\nWell, I was recently introduced to a plug-in called RX7, and I’m really excited about it. It’s an audio repair plug-in that can create near-studio quality instrumentals and acapellas from just about any song. It’s way better than the Vocal Remove feature in Adobe, or any other commonly-used attempt to create instrumentals. Previously, I was always limited to using songs where I could find a really good quality instrumental version, so I’m excited to get to play around with this plug-in, and expand my horizons!\n8. What are new learnings? Ideas you work on? Inspirations?\nAn idea I’m working on right now is the Season Opener piece for the 2020 Blue Jays season, but it’s actually an idea I’ve been working on since October. I heard a song that I liked and thought would be cool to use in something, so I found an instrumental version, threw it into ProTools, started putting some play by play with it, and thought hey – this could really work to kick off next season! So I’ve been working on it as I have time, and even though it’s almost done, I still go back to it from time to time just to listen to it with fresh ears, and make sure it all still works. I get my inspiration a lot from commercial music – even if it’s not using that exact track in my material, because you can’t use artist songs in EVERYTHING – it’s how can I get that same emotion? I’ll go on Youtube and search for fan videos – there’s no end of hype videos that fans have made for their favourite sports teams, and they’ve often used great hype music in the background. It not only gives me ideas of what music to use, but also helps me connect with what sort of emotions those fans have tied to their favourite teams.\n9. Any new tools you discovered lately?\nThe tool I’ve been using quite a bit is actually nothing related to ProTools or production at all – it’s actually an app on my iPhone. I’ve found Shazam has been extremely helpful. If I’m watching those hype videos on Youtube and I don’t recognize the song, I can just Shazam it. I’ve even been sitting at home watching TV and heard a commercial with a song that had a cool beat to it that I didn’t know, so I just Shazam it. A few months ago, I was watching a hockey game and just before puck drop, they were playing a piece of music in the arena that I wanted to use in something. I wasn’t able to Shazam it at the time, and it took me about three days to FINALLY find out what the song was (I’d gone through Spotify playlists, Youtube, all looking for songs played at hockey games and come up empty.) Finally, I was able to find a broadcast of that game, find where that play happened, and there was enough of the song coming through that Shazam was able to pick it up. (It was Tsunami by DVBBS, but where it kicks in at 1:17, in case you’re wondering!)\n10. Your favorite piece of imaging / production ever?\nMy favourite piece ever is probably a piece that Chris Pottage, my boss, produced in 2015 back when the Blue Jays were doing really well and made it into the post season.He managed to weave together Eminem’s “One Shot” with the Beatles “Come Together” WITH the Ok Blue Jays theme song, along with play by play calls in an absolutely incredible way that really captured the excitement of the Blue Jays run, and hearing that piece takes me right back to those sold out crowds and the energy in the stadium. It was electric, and his promo was just as electric.\nBut if you mean my favourite piece of MY Production – that’d probably be the 2019 Season Opener for the Blue Jays – that season was all about the team re-setting, and all of these young prospects finally coming up to the big leagues, and I just love how I was able to find a song that not only captured that theme, but worked so well with the play by play, and I was super happy with how it turned out. The opener for the 2020 season may actually be even better, but it won’t air until the end of March, so I can’t share it just yet. And if you’re wondering if I’m biased that all of my favorite projects are based around the Blue Jays – absolutely I am. I still try and connect to the emotions for other sports for the listeners, but they don’t connect emotionally the same way for ME. There’s a difference between “Hey, that’s a really cool piece of Production”, and “Oh, I love the memories and feelings that that piece of Production connects me to.”\n11. What would be your career advice for a youngster your twenty year old self trying to enter the radio biz?\nIt’s not too late for law school.\nBut if you really MUST try to enter the radio biz – then stay passionate. Even if you find yourself working with a format that you don’t really like, find a way to discover passion within it. You spend so much of your life working, there’s no sense in spending all that time doing something you don’t enjoy, so always find a way to find the fun within a job. And when you DO get stressed out, always remember…we’re not dealing with life-or-death situations here. We get to PLAY Radio for a living. And that’s a pretty fun thing to do!\nCheck out Elisabeth’s soundcloud for more of her work:"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:1261c90a-61a1-4aa5-af2e-9d3994261ad3>","<urn:uuid:d4e20ee9-919f-4425-a58e-00e51f51fcda>"],"error":null}
{"question":"What anti-tower argument did political science student Angela Day focus on in her protest against the KRKO radio towers?","answer":"Angela Day focused on the health issue, arguing that unlike with cigarettes where people could choose to quit when health risks were discovered, people didn't have a choice about exposure to the towers' potential health risks.","context":["Washington eco-saboteurs topple towers\nMonkey-wrenching of radio station doesn't last\nFall is usually a tranquil season in this rural suburb north of Seattle, where small farms grow flowers, nursery trees and other niche crops along the meandering Snohomish River. At harvest time, U-pick pumpkins swell on the vine, sunflowers bow their heavy heads and enterprising farmers cut cornfield mazes, urging tourists to \"Come get lost.\"\nThat bucolic spell was broken last year on Sept. 4 around 3 a.m., when one or more people sneaked up to a 40-acre site near the river, climbed a fence, fired up a large excavator that was equipped with a steel claw and aimed it at the dominant architecture: four giant broadcasting towers for KRKO, a sports-talk radio station.\nNeighbors heard the grinding of metal on metal. One grabbed a shotgun and chased a fleeing man, who vanished into the darkness. Another reported seeing three people fleeing. News helicopters awoke more locals, and a flurry of excited phone calls spread the word that the main tower -- 349 feet tall -- and an adjacent 199-footer had been knocked down.\nNear the crumpled steel latticework, someone left a banner with a hand-drawn heart, the scrawled words: \"Wassup? Sno. Cty.?\" and the signature \"ELF\" -- acronym for the radical Earth Liberation Front.\nBarbara Bailey at first thought the toppling was some kind of miracle. Her fifth-generation 400-acre farm is just across the river from the site, and she had opposed the construction of the towers. But when she learned it was sabotage, her feelings were complicated. She thought: \"It was not right. It was a crime ... but still it was a good feeling. After all the writing letters, lobbying politicians, raising money to pay lawyers, it was like somebody had decided the system wasn't working, and took matters in their own hands.\"\nThe crime remains unsolved. Lee Bennett, president of Citizens to Preserve the Upper Snohomish River Valley, says he has no clue who was behind the pre-dawn raid. \"It had to be someone who knew the machine was here, how to operate it, and how to safely punch an electrified tower over without being electrocuted.\"\nBennett's group has waged a legal battle against the towers for a decade, arguing that they pose a threat to wildlife, scenery and human health.\nBut KRKO's owners aren't backing down. The Skotdal family, which made its fortune in real estate development and does its radio business as S-R Broadcasting, began rebuilding the two damaged towers in August. They expect KRKO to be broadcasting its powerful 50,000-watt signal on 1380 AM once again by January 2011. They are also preparing the site for new towers for a proposed second 50,000-watt station, 1520 AM.\n\"You couldn't put (the towers) in a worse possible place -- right between two beautiful county parks,\" says Bob Heirman, who spent decades helping restore those parks, one of which is named after him. \"It's like putting a rendering plant next to your local high school.\"\nThe continuing conflict seems to highlight two notorious Northwestern character traits -- a strong green streak and a long stubborn streak. And the lack of arrests shows the of difficulty of catching the monkey-wrenchers who keep popping up in the region.\nBennett, a retired professor of geophysics and oceanography, traces the conflict to 2000, when a sign appeared in the middle of a 40-acre parcel of undeveloped farmland announcing that KRKO planned to build towers there. Opponents quickly rallied to fight the station's permit applications.\nThe original plans filed with the Federal Communications Commission called for up to eight antennas, some more than 400 feet tall, with more than 180 guy wires. KRKO's owners picked the site because it was the right size, had excellent ground conductivity, low population density and seemed easy to develop. Towers would boost the station's 5,000-watt signal tenfold.\nKRKO has earned a loyal audience by broadcasting Fox Sports Radio, along with local high school games, hockey and announcements about church fund-raisers and other community events. It also serves as the Emergency Alert System station for Snohomish County, and supporters argued that a more powerful signal would reach a wider audience -- critical during earthquakes, floods and other life-threatening events, says Ted Buehner, a National Weather Service meteorologist in Seattle.\nThe owners scaled down the towers' size and number, eliminated guy wires and cut down on hazard lighting after people raised concerns about birds colliding with the structures. (Many studies show that bird mortality rates vary depending on the design of towers.) The area is a major flyway for ducks and trumpeter swans, and Jan van Niel, one of the leaders of the Pilchuck Audubon Society, says the birds retreat to a lake in the Bob Heirman Wildlife Park, which is off-limits to dogs and hunting. \"The towers are between the county park refuge and the agricultural fields where they feed, so most birds pass that way.\"\nThe area is also prized by anglers, who pack the banks of the Snohomish River fishing for migrating salmon and steelhead trout. Some of them also didn't want like the towers there.\nIn the prolonged legal battle, three independent hearing examiners denied key county permits. One examiner said the towers would be \"stark, garish, angular and incompatible\" with the landscape. Another studied dozens of reports on the health impacts of radio-frequency radiation, including several that tied AM transmitters to increased cases of childhood leukemia. In her 64-page decision, she acknowledged that research had yet to prove conclusive links, but said it was important to take precautionary measures \"without having to wait until the reality and seriousness of the risks become fully apparent.\"\nProtester Angela Day, a political science doctoral student at the University of Washington, focused on the health issue. \"When initial studies suggested cigarettes might be bad for you, you could choose to either keep smoking or quit,\" she says. \"With the towers, we didn't have the choice. I was uncomfortable with that.\"\nThe station, which hired its own scientific experts to testify, says on its website: \"We absolutely deny that there is credible evidence to support the contention that someone can get cancer from living near an AM radio antenna.\"\nThat was also the opinion of a slim majority of the Snohomish County Council members. They eventually approved the four KRKO towers and two additional towers for the new station, saying that there was no hard evidence of health impacts and dismissing other concerns.\nTower backers describe the protests as \"NIMBYism.\" \"They've used up every excuse they can, and now, the bottom line is, these people are saying we don't like having towers here screwing up our view,\" says Steve Burling, an electronics technician and ham radio operator.\nBut once the first towers began broadcasting in February 2009, many neighbors heard KRKO coming through their corded phone lines -- even after the station provided them with special filters. \"They sent a little thing to clip on the phone, but it didn't do much good. All you can hear is the sports radio going loud and clear,\" says Agnes Elkins, whose house overlooks the site from an upscale hillside development. Locals also complained that the KRKO signal interferes with and sometimes wipes out other AM stations.\nMark Craven, an agri-tourism entrepreneur who runs nearby Craven Farm, says the radio signal has disrupted his walkie-talkies and other machines: \"Last year we had to buy new Visa machines. The old ones would cut off mid-transaction.\"\nCraven, who has a clear view of the towers, thinks they detract from the pastoral ambiance of his place. He draws about 50,000 visitors annually with weddings, antique shows, hay rides, baby farm animal petting, and pumpkin picking and flinging. For two years, the pumpkin slingshot's target was a five-foot radio tower built out of rebar with a flashing red light on top. It mimicked KRKO's tallest tower, which was topped in blinking red lights. The model \"got totally trashed,\" says Craven.\nThe Gigantic 2009 monkey-wrenching occurred shortly after the tower opponents lost another costly round in their legal battle. Seattle FBI special agent Fred Gutt, with the agency's typical taciturnity, says the strike might not even be an act of eco-sabotage. \"Just because one group claimed responsibility doesn't mean that they are responsible. We're not even sure if environmental concerns are the motivation for this activity.\"\nKRKO president and general manager Andrew Skotdal, who has kept the station on the air with a weaker signal, suggested early on that the estimated $2 million vandalism might have been done by locals. \"I was furious when I heard him being interviewed on the radio,\" says Craven. \"To suggest we might have had something to do with it was just ridiculous.\"\nBut anyone could have hung up that \"Wassup?\" ELF banner. ELF is an enigma; it reportedly has no command structure, no leaders, no meetings, no mailing list. Its small cells are unknown to each other -- a model that \"eco-terrorist\" researcher Gary Perlstein traces back to Mao Zedong's guerilla warfare in China from the 1920s to the 1940s. \"An ELF cell can actually be one single person,\" says Perlstein, professor emeritus of criminology and criminal justice at Oregon's Portland State University. \"The way ELF operates -- and I'm talking about a group that doesn't really exist as a group -- as long as they believe that someone's action is in their interest, they will claim it.\"\nELF's self-appointed spokesman in Washington, D.C., quickly issued a press release claiming that the group had toppled the towers but acknowledged he hadn't heard directly from anyone involved. To date, says Leslie James Pickering, an ELFer who wrote a book about the group, \"The only claim of responsibility I am aware of was the note left at the scene.\"\nThe unsuccessful legal battle against the towers became a \"textbook case for the necessity of direct action,\" says Pickering. \"When writing letters and protesting for a cause hasn't proven successful -- and it most often isn't -- it does not mean that the cause is unworthy, only that KRKO is unresponsive to public interest when it conflicts with their economic interest.\"\nThe FBI considers eco-saboteurs and animal-rights extremists a serious threat. The underground radicals have been linked to more than 2,000 crimes since 1979, a sizable portion of them in the Northwest, a corner of the country that tends to draw tree-, mammal- and fish-huggers who relentlessly recycle, demand cage-free eggs, and bring an almost religious fervor to their love of nature. \"You can look at it as a good thing or a bad thing, but we're probably more accepting of these types of organizations than elsewhere in the country,\" says Perlstein. \"Here, nature comes first. Animals are put on pedestals.\" That sentiment resonates in Ernest Callenbach's landmark 1975 novel Ecotopia, which called for the Northwest to secede from the U.S. and form a separate country dedicated to environmentalism.\nThe Seattle Times reported that between 1996 and 2001, 18 people were indicted on charges of ELF-related arson and sabotage in the Western U.S. Targets included a slaughterhouse, a timber-company headquarters and several buildings at Colorado's Vail ski resort. ELF also claimed responsibility for a 2001 fire at the University of Washington's Center for Urban Horticulture that destroyed decades of research as well as structures. Three women from ELF were sentenced to prison and ordered to pay nearly $6 million in restitution in that case. One of them, Chelsea Dawn Gerlach, was also involved in the Vail arson and the toppling of an Oregon power-line tower in 1999, prosecutors said.\nThe Snohomish Valley area, in particular, has been a hotbed of ELF activity. In 2004, ELF took credit for burning down two homes under construction. Flammable liquids were also found at other housing developments. In 2008, arsonists destroyed three massive luxury homes and damaged a fourth on the \"Street of Dreams,\" a high-efficiency development. They left a sign that said \"Built Green? Nope black!\" and signed it \"ELF.\"\nMore recently, following last year's attack on the towers, someone claiming to be from ELF spray-painted graffiti addressed to the Master Builders Association and KRKO on a nearby abandoned warehouse, warning: \"If you continue to risk killing children, mother earth and her creations, all your holdings are targets.\"\nThe FBI says it is \"casting a wide net\" in its investigation of the tower toppling. Agents have actively questioned many members of the Citizens group, including Toshenika Rosford, a flower wholesaler who owns a farm adjoining the KRKO site. \"I told them to look for drag marks -- because whoever did it must have huge balls,\" says Rosford. \"I was shocked someone had the gall to do it ... somebody really put their life on the line.\"\nNow, 14 months after the attack, tower opponents have their hopes pinned on the Federal Communications Commission. The agency has yet to approve either a permanent operating license for KRKO or a license application for the proposed 1520 AM station. Opponents have jammed the agency with objections in an attempt to sway the process. They took heart in June when the FCC requested additional information from the owners about the new station's impacts. The FCC followed up in mid-October with a letter calling the owners' response incomplete and saying that if the additional information was not received within 30 days, the application for the new station would be dismissed.\nAt this point, the Citizens group has exhausted its war chest. The group still owes a sizable chunk in legal fees, and is holding ice cream socials, spaghetti dinners, wine tastings and barn sales to raise money. But members believe their fight is important, with implications beyond the local community. Rural land is being developed all over the country, they say, and many small farms struggle to retain their unique character.\nIn their most recent fight, they pushed an amendment to the county shoreline management program that would prevent any future AM radio towers from being built in places like the valley floodplain along the Snohomish River. On Oct. 13, the county approved the amendment.\nIt was a sweet moment of victory in the uphill legal battle. And as Bennett says, \"We're not done yet.\"\nThis story was funded with reader donations to the High Country News Research Fund"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:91fa98ea-ae78-402f-ab5a-9e7d3a927f9d>"],"error":null}
{"question":"How do the visualization methods differ between Blender's fluid preview system and Maya's material rendering pipeline?","answer":"In Blender's fluid preview system, the visualization process involves a 'bake' button that calculates the fluid simulation progressively, with a progress bar showing the calculation status. The fluid can then have materials applied and be rendered normally, with options for subdivision surface smoothing. In Maya's material pipeline, visualization is integrated with a comprehensive rendering system that includes material creation, texture application, and specific renderers (software renderer, mental ray, and Arnold). Maya's system explicitly incorporates depth of field effects and focuses on material properties like color, texture, and reflectivity as part of the visualization process.","context":["TUTORIAL AS QUOTED FROM SOURCE\nFluid simulation is, well, fluid simulation. Make a fluid of any viscosity flow around other objects. In this tutorial, I'll show you how to make fluid flowing around multiple\nobjects for a very cool effect. Here's what it might look like: Fluid Simulation: Blue Goop\nModeling of the Scene\nHere's how we will start. First, press “ Z” to make the wire frame view appear. You can now see inside other objects. Press Numpad 1 to switch into front view. Now, Select the cube with RMB (right-mouse button). Press “S” to scale it. Make it about 10 grid spaces wide.\nYour 3D cursor (the red and white target) should already be in the center, but if for some reason it's not just click in the center of your cube with LMB. Now, Press Spacebar to bring up the add menu. Select Add>Mesh>Cube. A cube will appear in the center. Now, hit “S” again to scale it to about 6 grid units wide. Hit “S” again to scale, but this time press “Z” after starting to scale. You have just constrained the scaling to the Z axis, and a line should appear going up and down. You can now only scale in this direction; scale until your cube is about 1 grid unit wide.\nNow for the next stair. Press “A” to deselect everything. Now, hit “B” to activate box-select. Drag the box over the top four vertices. They will turn yellow, indicating that they are currently active. Now, press “E”, which will extrude this top face. We don't want to extrude up at all, so just press RMB. It appears nothing happened. But really an identical face was created in the same location of the one we extruded from. We can now scale this in with “S”. Make around 4 grid units wide. Now, we can extrude again with “E”. Bring the new face up about 1 grid unit. Repeat this two more times.\nOkay! The modeling job is almost done. Press Tab to go out of Edit Mode and switch into Object Mode. Your model should turn pink. Press “G” to move it and bring it to the bottom of the large cube. We will now add our fluid. Press spacebar then Add>Mesh>Icosphere. Just leave the subdivision at two for now. Move it to near the top left-hand corner. Scale it out to a good amount. Then press “S” then “X” to scale it only in one direction to make it slightly oblong.\nNow that you've got your scene set up, it's time for the fun part! Select the outer cube, we will use this as our domain. Click on the object button (looks like 3 arrows) at the divider near the bottom. Then, select the physics button (looks like a flaming arrow.) Select the “Fluid Simulation” and follow the settings described in the picture below.\nI set the resolution at 25 for rendering because this is only a test. Once you figure out your final positioning, you can render at higher resolutions.\nSelect the icosphere, (RMB) go into the fluid buttons and set it as a fluid. Change it's initial velocity to 0.10 in the X direction. Lastly, select the stair model and set it as an obstacle. This will cause the fluid to flow around it.\nGreat! Select the domain again and press the “Bake” button. This will start your simulation. The bar at the top of the screen will tell you how much fluid has already been calculated.\nOnce you make sure everything is correct, you can set the render resolution higher, I find 100 is a good number. If you have a good computer you might want to try 128. Beware though, this is about 400 MB!\nThis tutorial is pretty much finished. But remember once you finish baking you can add materials, lights, and render just as you would normally. Fluids are also available to apply subsurf to, making them nice and smooth. (This does take a quite a bit more render time though.) In my final simulation, I added pillars surrounding the stairs, making an interesting effect when the fluid flowed around them. Good Luck!","Shadows add realism and depth to a rendered scene. Learn how to create and manage shadows in Maya. In this video, George shows how to create shadows using Maya’s software renderer. He demonstrates the difference between raytraced and depth mapped shadows as well as methods to adjust each shadow type.\n- [Voiceover] When we work with lights,…we also have to be concerned with shadows.…Now in the real world,…every light casts a shadow,…but in Maya,…we have the option of casting a shadow or not.…And this can be very powerful,…because we have lights that don't cast shadows.…But let's take a look at how to control shadows in Maya.…So here I have a simple scene,…and I have one light in the scene,…and as you can see,…it's casting a shadow.…So let's go over to the attribute editor…and see what we have.…\nSo I've got a spotlight,…and if we scroll down,…you'll see we have a shadows roll out.…Now the first thing we can do…is we can affect the shadow color.…So I can make that shadow lighter or darker.…So if you don't want black shadows,…you can fix that right here.…And then we have two major types of shadows.…We have depth map and ray trace.…So you can pick one or the other.…So let's start off with the top one,…which is called depth map shadows.…\nAnd if I click on use depth map shadows,…it will highlight these values.…Now the first one is a resolution,…\nIt starts with the basics of selecting and manipulating objects and organizing scenes, as you learn the interface and explore Maya's features. Author George Maestri then takes you through polygonal modeling, creating and refining meshes, sculpting, and NURBS modeling. Once you understand modeling, George will show how to create and apply materials to surfaces—adding color, texture, and reflectivity. He'll then integrate cameras, lighting, and depth-of-field effects into the rendering process, using the built-in software renderer, mental ray, and the new Arnold for Maya renderer. Last but not least, he'll show how to add movement and life to your work with Maya's animation tools.\n- Getting familiar with the Maya interface\n- Configuring viewports and workspaces\n- Selecting and manipulating objects\n- Creating hierarchies and layers in scenes\n- Creating polygonal objects\n- Modeling and refining polygonal meshes\n- Working with subdivision surfaces\n- Sculpting a basic landscape\n- NURBs modeling\n- Projecting curves on surfaces\n- Creating and applying materials and textures\n- Adding lights and cameras to a scene\n- Adding depth of field and motion blur\n- Animating in Maya\nSkill Level Beginner\nMaya 2016 Extension 2 New Featureswith George Maestri1h 24m Intermediate\nRigging Mechanical Objects in Mayawith George Maestri1h 27m Intermediate\nModeling a Cartoon Character in Mayawith George Maestri3h 6m Intermediate\n1. The Maya Interface\n2. Select and Manipulate Objects\n3. Organize Maya Scenes\n4. Create Polygonal Models\n5. Model Polygonal Meshes\n6. Refine Polygonal Meshes\n7. Sculpt Meshes\nSculpt a basic landscape4m 51s\n8. NURBS Modeling Techniques\n9. Refine NURBS Models\n10. Create Materials\n11. Apply Materials and Textures\n12. Render in Maya\n13. Animate in Maya\n14. Render in Arnold\n- Mark as unwatched\n- Mark all as unwatched\nAre you sure you want to mark all the videos in this course as unwatched?\nThis will not affect your course history, your reports, or your certificates of completion for this course.Cancel\nTake notes with your new membership!\nType in the entry box, then click Enter to save your note.\n1:30Press on any video thumbnail to jump immediately to the timecode shown.\nNotes are saved with you account but can also be exported as plain text, MS Word, PDF, Google Doc, or Evernote."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:f314c60f-6fe5-43ec-ae80-e3f0d2f4072a>","<urn:uuid:9a6cd646-2306-4c89-915b-137166b2b1ce>"],"error":null}
{"question":"What role does water quality play in both coffee processing and coral reef health?","answer":"Water quality is crucial in both systems. In coffee processing, water is essential for milling, with newer eco-friendly methods recycling water and utilizing bacteria that actually improve the washing process. For coral reefs, water quality significantly impacts their survival, particularly regarding nutrient levels affected by fertilizer run-off from farms. Poor water quality can aggravate heat stress in corals during bleaching events, while proper water management in coffee processing can reduce environmental impact and conserve resources. Both systems demonstrate how water quality management at the local level can have significant environmental benefits.","context":["Every year, the trade show at the SCAA annual conference includes at least a few vendors selling the latest and greatest technology to filter, purify, ionize or otherwise ensure the quality of the water you put in your coffee. We are reminded that water is the principal ingredient in the coffee, after all. But you rarely hear anything at SCAA about the countless millions of gallons of water that are used to mill your coffee at origin. Increasingly, smallholder farmers are turning toward “semi-dry” wet mills like the one pictured here that dramatically reduce the amount of water needed for milling, leaving the balance for families to drink, cook, wash and farm with. As it turns out, the best water may be the water that doesn’t go into your coffee.\nIn standard wet milling processes, copious amounts of water are used to propel freshly picked coffee cherries into depulpers and through the channels that lead them to the tanks where they will ferment. After the beans are fermeted for just the right amount of time — a contested issue of no small consequence that merits another post on another day — more water is used to wash them. (Where that water goes and what it does to local water tables across the coffeelands is another issue of serious consequence worthy of its own post later, but the focus here is on the volume of water used in the wet milling process.)\nI have seen several estimates of the amount of water used during this process from different countries, and the most common figure seems to be about 1200 liters of water for every 100 pounds of fresh cherries. Those 100 pounds of cherries will produce about 16 pounds of export-ready coffee. These 16 pounds of export-ready coffee will lose about 20 percent of their volume in the roasting process, meaning that those 1200 liters of water will have been necessary to get just over 12 pounds of roasted coffee to market. That comes out to almost 100 liters for the last pound of fresh-roasted coffee you bought.\nIncreasingly, farmers — like those at Santa Anita de la Unión in Guatemala — are using a “semi-dry” wet milling process in which the cherries are propelled manually into a floating tank with a fixed amount of water that separates the quakers from the good beans before being passed to the depulper. This measure alone reduces considerably the amount of water needed for wet milling. But it is often combined with another measure — recycling the water from the depulping process for washing the beans after fermentation — that reduces water use even further. (As I have been led to understand, the water actually picks up bacteria in the depulping process that improve the quality of the washing process…Still looking for the research behind that and will update this post after I get a chance to review it.)\nThe most common estimate I have heard of the amount of water used in this process is 200 liters for every 100 pounds of cherries. That works out to less than 17 liters per pound of roasted coffee — a very favorable comparison indeed to the 100 liters used in traditional processes. What happens to the 1000 liters of water not used in “ecological” wet milling? It means more water for drinking. It means more water irrigation of coffee nurseries and other non-coffee crops that require it. It means more water for domestic uses, like cooking and cleaning. And it likely means significant conservation of limited water resources — no small achievement in the era of climate change.","Cecilia D’Angelo, a molecular coral biology lecturer at the University of Southampton. Jörg Wiedenmann, Elena Bollati & Cecilia D’Angelo/University of Southampton, Palawan colourful bleaching image by Ryan Goehrung/University of Washington\nBleaching events used to be few and far between, but they now occur nearly every year. After the coral is exposed, it often breaks down and dies, altering the ecosystem for the diverse array of life that relies on it. Corals stand little chance of bouncing back from these events — but a new study suggests they have an unusual survival method: taking on a vibrant neon color.\nWhen bleaching events occur, extended heat spikes cause corals to turn a ghostly white, often leading to their death. Even slight increases in annual ocean temperatures can wreak havoc on this relationship, expelling the algae from the coral’s tissue and exposing its white skeleton. These corals can still undergo some of their normal functions for a short period of time as they hope their algae come back — whereas drastic changes in ocean temperature almost always lead to coral death. Reports of colorful bleaching during the most recent mass bleaching event in the Great Barrier Reef in March and April gave scientists hope that patches of the system have a chance to recover. “Bleaching is not always a death sentence for corals, the coral animal can still be alive,” said Dr. They found that colorful bleaching events occur when corals produce “what is effectively a sunscreen layer” on their surface to protect against harmful rays and create a glowing display that researchers believe encourages algae to return. The Ocean Agency describes the process as a “chilling, beautiful and heartbreaking” final cry for help as the coral attempts to grab the algae’s attention. “Our research shows colorful bleaching involves a self-regulating mechanism, a so-called optical feedback loop, which involves both partners of the symbiosis,” lead researcher Professor Jörg Wiedenmann of the University of Southampton said in a press release. Climate Change\nDying coral reefs turn vibrant neon in apparent survival effort\nScientists say climate change is turning coastal Antarctica green\nStudy: Climate change makes a Dust Bowl heat wave more likely\nAir pollution is already spiking in China with virus lockdown lifted\nIndia’s carbon emissions fall for the first time in four decades\nMore in Climate Change\nResearchers at the University of Southampton’s Coral Reef Laboratory studied 15 colorful bleaching events worldwide between 2010 and 2019 — including one in the Great Barrier Reef, the world’s largest coral reef system — and recreated those ocean temperatures in a lab. But “colorful bleaching” has the opposite effect: the dying corals gain more pigment, and glow in shades of bright pink, purple and orange. Scientists first spotted the mysterious neon coral a decade ago, but they had been unable to figure out why it occurred. As the recovering algal population starts taking up the light for their photosynthesis again, the light levels inside the coral will drop and the coral cells will lower the production of the colorful pigments to their normal level.”\nColorful bleaching Acropora corals in the Phillipines. Dying coral reefs turn vibrant neon colors in apparent last-ditch effort to survive\nBy Sophie Lewis\nMay 22, 2020 / 4:48 PM\n/ CBS News\nScientists use mini-satellites to save coral reefs\nFor years, coral reefs around the world have been devastated by mass bleaching events as the oceans continue to warm due to climate change. “If the stress event is mild enough, corals can re-establish the symbiosis with their algal partner.”\nThe internal changes that allow colorful bleaching to occur. In 2017 alone, nearly half the corals on the Great Barrier Reef died — and experts say we are running out of time to save them. “Unfortunately, recent episodes of global bleaching caused by unusually warm water have resulted in high coral mortality, leaving the world’s coral reefs struggling for survival,” D’Angelo said. Scientists emphasized that while colorful bleaching is a good sign, only a significant reduction of greenhouse gases globally — in addition to improvement in local water quality — can save coral reefs beyond this century.\n“Now that we know that nutrient levels can affect colorful bleaching too, we can more easily pinpoint cases where heat stress might have been aggravated by poor water quality,” researchers said. Ryan Goehrung, University of Washington\nCoral reefs support more species per unit area than any other marine environment, including about 4,000 species of fish, 800 species of hard corals and potentially millions of other undiscovered species, according to the National Oceanic and Atmospheric Administration. Colorful bleaching Acropora corals in New Caledonia.\nRichard Vever/The Ocean Agency/XL Catlin\nCoral animals symbiotically coexist with tiny algae, providing them with shelter, nutrients and carbon dioxide in exchange for their photosynthetic powers. Together, these actions can secure a future for coral reefs.”\nFirst published on May 22, 2020 / 4:48 PM “The resulting sunscreen layer will subsequently promote the return of the symbionts. This study, published Thursday in the journal Current Biology, suggests the corals change color as a last-ditch effort to survive. “This can be managed locally, whereas the ocean heat waves caused by climate change will need global leadership. Disruptions to coral reefs have far-reaching implications for ocean ecosystems.\nIt’s not just warming oceans that cause colorful bleaching. Researchers say changes in nutrient levels within coral reefs due to fertilizer run-off from farms also lead to bleaching events — a problem that can be fixed at the local level. Experts believe only coral that has faced mild or brief disturbances, rather than extreme mass bleaching events, can attempt to save itself using this process."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:1c19b2f6-b9f0-4d0f-9edc-729eb4b2471f>","<urn:uuid:369f1095-9589-4d83-9007-35418b336092>"],"error":null}
{"question":"Could you explain what letterpress printing is and how this traditional method works?","answer":"Letterpress printing is a form of relief printing where the raised surface of letters or images is inked and then pressed into paper. The process involves typesetting words by hand, letter by letter, and then printing them on cast iron presses, some of which are over 100 years old.","context":["Funding boost for family-owned Glasgow print business as it endeavours to preserve printing heritage\nA FAMILY-owned printing business in Glasgow has received a “sizeable” funding boost from The Watson Foundation in an effort to keep alive traditional methods of printing.\nGovan-based Glasgow Press has embarked on an initiative to bring often-forgotten methods of printing to a new audience with a series of workshops and learning opportunities with a strong community focus.\nNow their efforts to engage the local community and beyond in learning about printing have attracted funding from the Foundation.\nThe donation from the philanthropic organisation The Watson Foundation which he set up following his retirement will, according to Glasgow Press, “turbo charge” its efforts to preserve and celebrate traditional printing.\nFor many years Glasgow Press has been collecting printing presses along with wood and metal type which would otherwise have been discarded as printing technology moved on.\nThe business has a firm emphasis on “letterpress printing” which is a form of relief printing. The raised surface of letters or images is inked and then pressed into paper. Words can be typeset by hand, letter by letter, and then printed on cast iron presses which in some cases are over 100 years old.\nDan Clark, who runs Glasgow Press with his family, said: “Letterpress printing is on the red list for endangered crafts and we’re keen to try and keep it alive.\n“There aren’t any college courses for anyone who wants to be a letterpress printer as far as we know, so by trying to make things more accessible, by getting people in for courses or introducing children into this method of printing, we are trying to keep this trade going a bit longer.”\nDan’s father, also Dan, now a remarkable 88 and still working part-time in the business – started off as a message boy at a printworks in the 1940s before opening the family business in 1960 on completing his apprenticeship.\nDan Clark junior said “We are immensely grateful to John Watson and The Watson Foundation for this generous and substantial funding. We will be sure to put the money to good use engaging with the local community and more widely given that we share John Watson’s long-held social justice credentials as well as his passion for printing\n“The boom in digital printing has eclipsed letterpress printing and we are proud to be making the case for the letterpress process. We are sure that engaging with the community in our Govan home and beyond will capture the public’s imagination and spark renewed interest in printing.”\nGlasgow Press has now set up a “community interest company” called “letterspace” to promote the work.\nThis is not the first time the Foundation has supported traditional printing and its role in Scottish life.\nIn 2018 the Foundation, working with the Scottish Printing Archival Trust, produced booklets entitled the Glasgow Print Trail and the Edinburgh Print Trail discovering why printing was so important to both cities.\nJohn Watson OBE explained: “Preserving traditional printing methods for me is a labour of love.\n“Most people associated with the printing industry generally agree that there have been more fundamental changes in the last 50 years than in the preceding centuries since Johannes Gutenberg invented printing from movable type in 1450.\n“It is worth remembering that printing was a mainstay of Glasgow’s industry – third only in importance to heaving engineering and shipbuilding.\n“The pace of change has been unrelenting and with technology and new processes, ways of communicating the printed word have changed out of all recognition. Sadly, the industry has, too, and many of the companies have fallen by the wayside.\n“This is why it is vital to ensure traditional print methods such as letterpress printing is not forgotten and Glasgow Press are leading the way with their community outreach initiative. I’m pleased to be supporting it and wish them well.”"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:41e5a4f9-3959-4b9c-bee9-7c8283579eca>"],"error":null}
{"question":"How do H.A. Brewing and Wolf Hills differ in their approaches to using wild/unique yeast in their beers?","answer":"H.A. Brewing experiments with capturing wild yeast from bees on their homestead, introducing it to sugars to grow and then using it for fermentation. In contrast, Wolf Hills uses specific strains of yeast for different beers, such as German ale yeast for their Kolsch and a special Oktoberfest yeast strain for their Marzen, but doesn't experiment with wild yeast capture.","context":["Crafting beer in the boonies: Brewery uses bees to brew\nEditor’s Note: This is the first in a series of stories on Montana’s breweries and distilleries.\nEUREKA – A hobby became a passion became a business for brewer Chris Neill of H.A. Brewing Co.\nH.A., which stems from Homestead Ales (already trademarked by an Ohio brewery), began in Neill’s cabinet show on his homestead, where he raises sheep, horses, chickens and pigs.\n“Chris raises much of his own food. He’s really passionate about what he puts in his body so he started making beer,” said Karl Kassler, a partner in the brewery. “He thought this would be a nice hobby business and it just took off.”\nNeill has put aside his carpentry tools to brew full time.\nThe brewery is 8 miles southeast of Eureka on the road to the Ten Lakes Scenic Area. It’s a bit of a drive – about 45 minutes from Whitefish – but a beautiful one.\n“We have great community support, and it doesn’t seem to make any difference it’s out in the boonies,” Kassler said. “We get patrons regularly from the Flathead Valley and as far away as Fernie (B.C., Canada). People like to drink our beer, have a pizza and kick back.”\nOn some fine summer nights, the brewery serves 400 pints of beer, meaning at least 120 people came. In the summer, the beer garden sees families with lawn chairs and toys and friends sharing picnic tables. A recent addition will alleviate winter crowding.\nJax Pizza has a food truck parked at the brewery making wood-fired oven pizzas, wings and salad.\n“It’s a family-friendly atmosphere,” Kassler said.\nH.A.’s most popular beer is the Grave Creek IPA, which won a gold medal from the Montana Brewers Association when the brewery had only been open two months. The light, low alcohol-by-volume Pioneer Pale Ale is another favorite. Among those who favor dark beer, like Kassler, the Black Irish and Boulder Creek Stout are meeting success.\n“I feel Chris’s porter is an exceptional Montana-made porter, but I’m biased,” Kassler said.\nThe brewery has nine beers on tap, with five flagship and four specialty beers.\n“The local and regional customers are really broadening their beer horizons by drinking more of the Belgian farmhouse ales, which are open-cask fermented,” Kassler said.\nOne of Neill’s most interesting experiments has been using wild yeast. He captured a bee on the homestead, put it with sugars and then the natural, wild yeast starts to grow. That’s introduced to the liquid to start fermentation.\n“All yeasts have very specific characteristics in how they convert sugar to ethanol. There are specific strains brewers use,” Kassler said. “When you use the wild yeast from, say, a bee you’re picking a particular yeast. You could ferment from yeast from the human body, but that would be gross. You choose something romantic everybody loves, like a bee.”\nIf you can’t make it to Eureka, be advised that 21 places in the Flathead Valley serve signature H.A. beers. The Stein Haus in Great Falls has a rotation of H.A.’s Grave Creek IPA, Black Irish and Pioneer Pale Ale. The Bottle Shop & Tasting Room in Helena carries Grave Creek.\nReach Tribune Staff Writer Kristen Inbody at email@example.com. Follow her on Twitter at @GFTrib_KInbody.\nBREWERY: H.A. Brewing Co.\nLOCATION: 2525 Grave Creek Road, 8 miles southeast of Eureka\nTOP BEER: Grave Creek IPA\nHOURS: 3 p.m. to 9 p.m. (serving pints until 8 p.m.), Wednesday-Sunday\nNOTE: No flights offered but samples available, pizza served","The following are descriptions of popular and past beers brewed at Wolf Hills.\nThis is not a complete list; all current tap selections can be found listed on the main page.\nYear Round Offerings\nWhite Blaze Honey Cream Ale\nAn approachable and very drinkable lower ABV session beer, Honey Cream Ale has a malt backbone of Pale malt with Honey malt, Caramel 40, and flaked corn. 10 pounds of wildflower honey are added to the end of the boil to enhance the sweet character. Hopped with traditional Czech Saaz and fermented with an English ale yeast, White Blaze is a crowd favorite and great for people just discovering craft beer.\nSRM 5 IBU 18 ABV 4.6%\nTroopers Alley IPA\nOur new and improved flagship IPA features a blast of dry hops to enhance the hop aroma and flavor characteristic of newer American IPAs. An old-school malt bill features a bit of Caramel 40 for light sweetness and color and a bit of Vienna malt for malt complexity. We hop Trooper’s with traditional American citrus hops Cascade and Centennial at the end of boil and in the dry hop.\nSRM 6.5 IBU 50 ABV 5.9%\nCreeper Trail Amber Ale\nThough slightly hoppier this is a traditional American Amber Ale featuring a complex malt bill. In addition to the pale base malts we add Caramel 60 for color, caramalt for head retention, and a wee bit of chocolate malt for complexity and color. Hopped with Willamette for an earthy balance.\nSRM 13 IBU 24 ABV 5.6%\nWolf’s Den Double IPA\nWhen we opened the brewery in 2009 you couldn’t find Double IPAs within 100 miles. So we made one. A simple malt bill of Pale malt with some Golden Naked Oats for body lets the hops shine through. A blend of Citra, Amarillo, Centennial, and Cascade provide a blast of hop flavor and aroma – mostly from dry hopping over 1 pound per barrel. We often sneak El Dorado, Azeca, Mosaic, and other new hops to keep you on your toes!\nSRM 5 IBU 65 ABV 8%\nFor Science! IPA\nA lighter and more variable IPA than the Wolf’s Den this is a sweeter evolving IPA providing a bit more mouthfeel and body to balance the hop load. We also minimize the early hop additions to keep the bitterness lower than was typical for early IPAs. We often experiment with each batch changing the hop varietals to both learn about and enjoy new hop varieties and old favorites. It’s all for science, right? Typical varieties include Amarillo, Azeca, Citra, El Dorado, and Mosaic but we are always up for something new.\nSRM 4.8 IBU 28 ABV 6.5%\nThis type of ancient German sour beer comes from a time when all beer was sour because we didn’t understand sanitation and the process of fermentation. To achieve the sourness we add lactobacillus planarum bacteria to this beer prior to boiling or ‘kettle sour’. The bacteria ‘sour’ the wort and bring the pH from about 4.9 to 3.3 providing a tart smoothness. After the wort is soured, we boil as usual killing the bacteria and sterilizing the wort. We add coriander for spice and salt for unique mouthfeel before fermenting with regular ale yeast. After fermentation we add a variety of pureed fruit. Typically we add 84 pounds of blackberry puree but will often half this with raspberries for more tart flavors. We have added mango and look forward to making Gose with various fruits in the future.\nSRM purple? IBU 4.5 ABV 5-6% (varies)\nFightin’ Parsons Pale Ale\nNamed after the man who would preach from the pulpit with a rifle by his side to protect early Abingdonians from village raids. This is our take on a traditional American Pale Ale showcasing hop flavor and aroma. Like Troopers, we add a bit of Vienna to an otherwise stark malt bill to get out of the way of the hops. Heavily dry hopped with Amarillo, Cascade, and Centennial. A lower ABV version of and IPA so you can have several.\nSRM 4 IBU 33 ABV 4.6%\nMostly Sunny New England Style/Hazy IPA\nA softer, less bitter, fruitier version of a traditional American IPA. Hazy IPAs are characterized by hopping the beer while it’s fermenting so the yeast can interact with hops to produce novel flavors of citrus juice. Mostly Sunny is our evolving version of this newly emerging style of IPA. We add a heavy charge of new hop varieties during fermentation followed by another charge of the same hops once fermentation is finished. We push 4 pounds of hops per barrel after the kettle. Hazy IPAs are far less bitter because very few hops are added to the kettle where heat can create bitterness from the hops. The haze is caused by the reactants from the yeast and hops together and not actual yeast suspended in the beer. The softness of the mouthfeel is due to the malted wheat and Golden Naked Oats in the grain bill.\nSRM 3 IBU 18 ABV 5.5-6.5%\nOur take on the most common beer style available anywhere. A clean, crisp heavily drinkable beer characterized primarily by using lager yeast that ferments cold and slow. Flatpicker is simple and features pilsner malt, German Tradition hops, and lager yeast. Because the beer is so simple there is little room for error. We hope you enjoy our version of this worldwide beer style.\nSRM 3 IBU 30 ABV 4.5%\nTraditional German style ale made like a lager. Ale yeast and faster fermentation creates slightly more fruity flavors with the same crisp lager finish. German hops balance with a mild bitterness for a refreshing and balanced beer. Pilsner malts, German Tradition hops, and German Ale yeast\nSRM 3 IBU 20 ABV 4.4%\nBlackstrap Pecan Porter\nIn the style of ‘dessert’ or ‘pastry’ stout this is a sweet porter enhanced by the finest pecan extract we could find. Porters are the kitchen sink of beer s and at one time was the dominant beer style around the globe. Our porter features Honey malt, Caramel 40, Chocolate malt, and Black Patent for color. Northern Brewer hops add an earthy spiciness to balance the sweet. Blackstrap molasses is also added during the boil for flavor and fermentability, raising the ABV.\nSRM 27 IBU 39 ABV 5.9%\nThis is our collaboration beer made with cold brewed Zazzy’s coffee made by Lincoln Road Cold Coffee. Oatmeal in the mash plus coffee in the finish. Sounds like a good breakfast to me. Chocolate and Pale chocolate malts dominate the grain bill for color and flavor. A touch of Roasted Barley enhances the astringency and dryness of the finish – like a dark black coffee. At times we will add lactose sugar (milk sugar) to the boil for those who prefer cream in their coffee. Northern Brewer and Willamette hops.\nSRM 37 IBU 35 ABV 5-6.5%\nAppalachian Pale Lager\nUnlike the pilsner, which is a traditional European style, our Pale lager is an American version of a dry, crisp, refreshing thirst quencher. Pislner malt and lager yeast dominate, with german hops to balance. Less bitter than the Flatpicker we often dry hop this beer with an American (citrusy) hop variety for an added punch.\nSRM 2.8 IBU 10 ABV 4-4.3%\nOur traditional, celebratory Marzen style lager features more complex malts than a pilsner. German Pilsner malt, Vienna, Munich, and Best Red provide a bready sweetness balanced by German Tradition hops. A special strain of Oktoberfest yeast is used to ferment this beer for several weeks longer than usual to allow the complex malt flavors to express themselves.\nSRM 8 IBU 21 ABV 6%\nBlack’s Fort India Brown Ale\nA hybrid style between a caramelly sweet brown ale and an IPA and based on a popular homebrew recipe this beer offers a high degree of complexity. Five different malts including wheat, caramel 40, and chocolate provide a solid malt sweetness while a mix of earthy and citrus hops blend for balance. Sweet and hoppy this beer is also dry hopped with centennial for a grapefruit like citrus flavor. Unusual and atypical but we hope you enjoy.\nSRM 24 IBU 51 ABV 7.5%\nSpecialties and Rarities\nWolf Hills has three wine barrels from Abingdon Vineyard and four whiskey barrels from Davis Valley Distilleries in Rural Retreat. We always have something aging in them and release it on tap when it’s ready. Here are a few we have enjoyed in the past.\nNaya 9th Anniversary Belgian Style Ale\nNaya was fermented with a traditional Trappist yeast that provides a clove-like spiciness and fruit character to the finished beer. A mix of wild yeasts and bacteria was added when the beer was racked to a whiskey barrel. During a four month aging period the beer developed a slight sourness and robust oak flavor from the barrel. Hints of whiskey. We are currently re-using this barrel for an Oud Bruin.\nSRM 16 IBU 15 ABV 7%\nBelgian Tripel (with Brettanomyces)\nBoozy Belgian tripel aged in a wine barrel for a year and dosed with Brettanomyces, a yeast that ferments beer further than traditional saccharomyces beer yeast. Brett has dried this beer out to a terminal gravity of near zero. Brett also adds some fruity and a few funky flavors. The barrel contributes to the dryness with a bit of woody character. The original beer contained Belgian candi sugar which attributes to the sweetness, though the beer is still dry. Before Brett this beer was almost too sweet. The brett has rounded this out to a nice drinkability. Some may remember because this was last year’s 8th anniversary beer. I stole some and put it in a barrel. Some may compare it to the world famous Duvel made in a similar way, though that would be a huge compliment and a bit of a stretch.\nSRM 6 IBU 21 ABV 12%\n$6/13 oz pour\nDire Wolf Russian Imperial Stout\nExtremely decadent, silky sweet, and boozy. The Dire Wolf is our take on a big stout and aged in whiskey barrels for several months. Dark, roasty malts including roasted barley and chocolate. Heavily hopped with traditional East Kent Golding hops and American Ale yeast. Two ages will be released through winter 2018-2019 with the second aged longer for more whiskey and oak character.\nSRM 72 IBU 60 ABV 9-11%\nSRM: Standard Reference Method. A simplified approximation of beer lightness or darkness. Pilsners are low SRM (3-2) and stouts very high. Past about 40 most beers look black. SRM is a measure of light penetration and really isn’t an indicator of color but provides an estimate of light to dark.\nIBU: International Bitterness Units. A measure of hop bitterness usually coming from boil or kettle hop additions. Heat removes bitterness from hops and is traditionally used to balance malt sweetness. Nowadays we add hops at various parts of the brewing process to feature flavors and aromas that do not add to bitterness very much. IBU beyond 100 is said to be beyond the threshold of human perception, but high IBU beers are typically more bitter. Below about 30 beers are likely to be sweet and beyond 50 or 60 one can detect a very apparent bitterness. Also am imperfect measure but provides some estimate of bitterness in beer.\nABV: Alcohol by Volume. Craft beers are typically higher in alcohol than traditional mass-produced beers. Often standard beers are around 4%. Gage yourself accordingly.\nMalt: Malted Barley. Dried and/or roasted to varying degrees to impart a wide variety of flavors from bread, honey, chocolate, and caramel.\nHop: Flowers from the female hop vine. Provide bitterness when boiled and incredible aroma and flavor when added when beer is cool. Interesting fact, the hops plants’ only direct relative is cannabis.\nDry hop: Hops added after fermentation when beer is typically finished and ready to carbonate and keg. Imparts flavor and aroma rather than bitterness, which comes from kettle hops added during the boil."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:478f7f0f-fbdd-46d3-b045-e41776037355>","<urn:uuid:d1e4b593-c3e9-42b6-ba21-76904f2cee1c>"],"error":null}
{"question":"What are the health risks of indoor air quality, and how do green roofs help address these concerns?","answer":"Indoor air quality poses numerous health risks, including symptoms like rhinitis, nasal congestion, cough, wheezing, headaches, fatigue, and cognitive impairment. These issues often stem from various contaminants such as mold, dust mites, carbon monoxide, formaldehyde, and volatile organic compounds. Green roofs help address these concerns by improving air quality through their vegetation. Additionally, they provide better insulation and reduced power consumption, which can help maintain healthier indoor environments. The vegetation on green roofs creates a biodiverse environment that contributes to overall air quality improvement in urban areas.","context":["Without a doubt, the quality of air indoors is worse than outdoors. Sick Building Syndrome (SBS) is a term used to describe situations where the occupants experience acute health and comfort effects that appear to be linked to the time spent in the building, but no specific illness or cause can be readily identified. A 1984 World Health Organization Committee report suggested that up to 30% of new and remodeled buildings worldwide may be the subject of excessive complaints related to Indoor Air Quality (IAQ). Cited causes, or contributing factors, of SBS include inadequate ventilation, chemical contaminants from indoor and outdoor sources, as well as, biological contaminants.\nIAQ Health Signs & Symptoms\nYou may be surprised to learn that there are a number of health related issues associated with SBS and Indoor Air Quality (IAQ). Some often mimic common cold and flu symptoms making recognition difficult. They include: rhinitis, nasal congestion, epostaxis, pharyngitis, cough, wheezing, worsening asthma, severe lung disease, dyspnea, conjuctival irritation, headaches or dizziness, lethargy, fatigue, malaise, nausea, vomiting, anorexia, cognitive impairment, personality change, rashes, fever, chills, tachycardia, retinal hemorrhage, myalgia, hearing loss.\nContaminants & Where They Come From\n- Mould / mildew / fungus -- 85% of all homes have mould and 10% to 35% have serious mould contamination. Prolonged exposure can cause anyone to develop an allergy. Sources may include dead plant material, animals, humans, soil, air, dung, and, food.\n- Dust Mites -- live in mattresses, pillows, carpets, fabric-covered furniture, bed covers, clothes, and, stuffed toys.\n- Pests -- droppings or body parts of pests such as cockroaches or rodents can be asthma triggers.\n- Pollen -- a fine, powder-like material consisting of pollen grains that is produced by the anthers of seed plants.\n- Animals -- dander, minute scales from hair, feathers, skin flakes, urine and saliva.\n- Infectious agents (bacteria or viruses).\n- Carbon Monoxide (CO) -- where ever fossil fuels are burnt such as chimneys and furnaces, back-rafting from furnaces, gas water heaters, wood stoves and fireplaces, gas stoves, automobile exhaust from attached garages, environmental tobacco smoke.\n- Carbon Dioxide (CO2) -- In occupied areas, the concentration of carbon dioxide (CO2), a product of human respiration, is used as an indicator of inadequate ventilation. If levels are high, constant irritability and complaints from building occupants is expected.\n- Nitrogen Dioxide (NO2) -- kerosene heaters, unvented gas stoves & heaters, combustion sources, environmental tobacco smoke.\n- Sulphur Dioxide (SO2) -- combustion sources (such as coal, petroleum, kerosene, propane and oil).\n- Formaldehyde (HCHO) -- pressed wood products (hardwood paneling, particle board, fibreboard) and furniture made with these pressed wood products, Urea Formaldehyde Foam Insulation (UFFI), combustion sources, environmental tobacco smoke, durable press drapes, other textiles, coated paper products, cosmetics, and glues.\n- Radon (Rn) -- Sources include the earth and rock beneath a building, well water, building materials. Symptoms are not readily apparent with short-term exposure.\n- Volatile Organic Compounds (VOC's) -- includes paints, paint strippers, other solvents, wood preservatives, aerosol sprays, cleaners, disinfectants, moth repellents, air fresheners, stored fuels, automotive products, hobby supplies, dry-cleaned clothes.\n- Pesticides -- insecticides, termiticides, disinfectants, lawn and garden products","Sustainability Series: Green Roofs\nArticle information and share options\nWith increased urbanization, green roofs are becoming a major trend. They offer many advantages to building owners and occupiers, the general public, and the environment: better insulation, reduced power consumption, stormwater retention, improved air quality, and a biodiverse environment offering aesthetic diversity. However, green roofs may introduce property considerations and concerns that owners and occupiers should be aware of when reviewing their property and business risks.\nGreen roofs comprise specific vegetation designed and installed on a series of components, including moisture retention material, a drainage system, a root barrier, and a protective layer for both the insulation and underlying roof surface.\nProperty Risk Considerations\nLet's take a closer look at a few features that will likely be of interest to commercial property insurers when evaluating the building's overall risk profile.\nDownload our guide\n1. Potential for water leakage damage\nThe irrigation pipework may be susceptible to freezing. Leaks in waterproofing membranes due to root growth or temperature fluctuations may expose the underlying roof structure to damage or corrosion. Drainage systems may become blocked by soil and vegetation. These risks are particularly relevant in the construction phase.\nLoss Prevention Tips: Penetrations in the roof should be minimized. It is important to include leak detection systems below the waterproofing membrane. Size gutters to accommodate both rainfall and irrigation runoff and maintain them regularly, with specific inspection points included. Consider the impact of roof gradient on the density of growth media and its propensity to shift or slide during heavy rain events.\n2. Fire load\nGreen roofs and their supporting components (e.g., vegetation and waterproofing membrane) typically add combustible loading to a roof and increase the potential for ignition from exposing fires or other ignition sources. In some cases, the design of a green roof may encompass space for recreational activities, introducing ignition sources such as lighting, electrical installations, barbecues/grills, and smoking.\nLoss Prevention Tips: Limit the overall fire risk by maintaining adequate moisture content of vegetation, regular removal of dead vegetation, and careful design and placement of noncombustible fire breaks. Include manual firefighting equipment such as fire hydrants, fire hose reels, and portable fire extinguishers, along with access provisions for the fire brigade. Potential ignition from electrical equipment, smoking, or other sources should be identified and carefully managed.\n3. Susceptibility to collapse\nGreen roofs introduce live loads associated with landscaping, precipitation, induced saturation due to irrigation, and periodic replacement of new soil and growth of vegetation.\nLoss Prevention Tips: The load carrying ability of concrete roofs versus all other types (e.g., long span steel) should be considered over the roof lifecycle. Some roofs may be susceptible to deformation over time, affecting the operational efficiency of certain components that originally functioned properly. Future changes may affect the structural integrity and should undergo formal review. Concrete roofs are generally more resilient than other roofs to changes that increase live loading.\n4. Damage from natural hazards\nGreen roofs are susceptible to natural hazard perils such as seismic forces and wind uplift pressures. Vegetation or potentially the entire roof system may be damaged, requiring replacement.\nLoss Prevention Tip: The system, including any moisture/root barriers, should be properly secured to structural elements (growth media should not be relied upon). Green roofs should not generally be installed in areas with elevated wind exposures such as coastal areas subject to hurricanes.\nGreen Roofs provide many environmental benefits, especially in urban locations where vegetation may be scarce. However, if a green roof is to be installed, ensure that the considerations outlined above are incorporated in the design, installation, and future maintenance requirements to mitigate the potential for loss."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:6c11557f-30f7-405e-b2c4-efef00a8f16d>","<urn:uuid:adefe97e-66fa-495a-b3a2-8e24a422fce1>"],"error":null}
{"question":"What safety precautions should be taken with tarantulas versus probiotics for those with health concerns?","answer":"For tarantulas, while they are generally not dangerous, people should handle them with care, avoid sudden movements, and maintain a safe distance since some species can deliver more severe bites. It's crucial to know the specific species being handled. For probiotics, safety concerns primarily affect people with compromised immune systems or serious illnesses, who should avoid taking them due to potential infection risks. The World Gastroenterology Organisation specifically recommends against probiotics for these individuals. For healthy individuals, probiotics are generally safe with only minor side effects like gas being reported in some cases.","context":["Tarantulas, with their intimidating appearance and reputation, often evoke fear and curiosity in humans. The question that lingers in the minds of many is whether these creatures pose a threat to our safety. In this article, we delve into the truth behind tarantula aggression towards humans and the risks of tarantula bites.\nContrary to popular belief, tarantulas are generally not inclined to attack humans. These fascinating creatures are solitary by nature and prefer to be left alone. While their large size and hairy bodies may seem intimidating, tarantulas are usually docile and non-aggressive towards humans.\n- Tarantulas are generally not aggressive towards humans and prefer to be left alone.\n- They typically only bite when severely provoked or when mistaken a finger for prey.\n- Tarantulas serve as important pest control providers in the ecosystem.\n- Their venom is usually comparable to a bee sting and not strong enough to pose a danger to humans.\n- While rare, some tarantula species can deliver bites with more severe symptoms.\nWill Tarantulas Attack Humans?\nTarantula Behavior and Habitat\nUnderstanding tarantula behavior is essential for individuals seeking to coexist with these fascinating creatures. Although their appearance may be intimidating, tarantulas are generally not aggressive towards humans. They are solitary creatures that prefer to be left alone, and their bites are typically a defense mechanism when severely provoked or mistaken a finger for prey. will tarantulas attack humans\nTarantulas are ambush hunters and rely on their venom to subdue their prey. While their venom is not strong enough to pose a danger to humans, a bite can be comparable to a bee sting. It is important to note that tarantulas do not spin webs like other spiders. Instead, they create silk structures for molting and mating, which they carefully construct and maintain.\nTarantulas are found on every continent except Antarctica and have adapted to various habitats. They have different species-specific adaptations, such as urticating hairs, which they can release as a defense mechanism. These tiny barbed hairs irritate the skin and mucous membranes of potential predators, providing an effective deterrent.\nTarantula Behavior and Habitat\n|Tarantulas are generally not aggressive towards humans\n|Tarantulas are found on every continent except Antarctica\n|Tarantulas bite only when severely provoked or mistaken a finger for prey\n|They adapt to various habitats and have species-specific adaptations\n|Tarantulas are solitary creatures that prefer to be left alone\n|They create silk structures for molting and mating\n|They rely on their venom to subdue their prey\n|They have urticating hairs as a defense mechanism\nBy understanding tarantula behavior and taking necessary precautions when encountering them, individuals can safely appreciate these unique creatures in their natural habitats.\nTarantula Anatomy and Size\nTarantulas, with their unique physical features, are fascinating creatures to observe. Let’s delve into their anatomy and explore the variations in size among different species. human safety and tarantulas\nAnatomy of a Tarantula\nTarantulas have a distinct body structure consisting of eight legs, a cephalothorax, and an abdomen. These arachnids also possess sensory appendages called pedipalps, located near their fangs, which aid in capturing and manipulating prey. do tarantulas pose a threat to humans\nOne of the most striking features of tarantulas is their impressive size. While there is considerable variation, with some species being smaller, several tarantulas boast an impressive leg span that can reach as large as a dinner plate. It’s truly awe-inspiring to witness these magnificent creatures up close!\nComparative Size of Tarantulas\n|Average Leg Span\n|Goliath Birdeater (Theraphosa blondi)\n|Pinktoe (Avicularia avicularia)\n|Chilean Rose (Grammostola rosea)\n|Curly Hair (Brachypelma albopilosum)\nThe table above provides a glimpse into the comparative size of various tarantula species. From the massive Goliath Birdeater to the smaller Pinktoe, each species showcases its unique dimensions and characteristics. tarantula behavior around people\nUnderstanding the anatomy and size of tarantulas is crucial in appreciating their beauty and marveling at the wonders of nature. Let’s continue our journey to unravel the secrets of these captivating creatures in the following sections.\nTarantula Mating and Reproduction\nTarantulas have a unique mating process that involves a complex courtship dance. The male tarantula must approach the female cautiously to avoid being mistaken for prey. He uses his front legs to tap on the ground, creating vibrations that attract the female’s attention. Once the female is aware of the male’s presence, she may raise her legs and abdomen, indicating her receptiveness to mating. understanding tarantula attacks\nDuring mating, the male transfers his sperm to the female using specialized structures called pedipalps. These appendages are located near his fangs and are used to deposit the sperm into the female’s reproductive organs. The sperm is then stored by the female for fertilization at a later time. how to minimize tarantula-human conflicts.\nAfter a successful mating, the female tarantula will lay eggs within a silk cocoon she creates. The number of eggs can range from 75 to 1,000, depending on the species. The female tarantula carefully guards her eggs until they hatch, ensuring their safety from predators and environmental threats.\n|Number of Eggs\n|75 to 1,000\n|Egg Incubation Period\n|Approximately 6 to 9 weeks\n|Egg Sac Care\n|Female tarantulas guard their egg sacs and may exhibit aggressive behavior if threatened.\nIt is important to note that male tarantulas have a significantly shorter lifespan compared to females. Male tarantulas usually die shortly after mating, while females can live for up to 20 years or longer. Female tarantulas also have the ability to molt multiple times throughout their lives, growing larger with each molt. These molting cycles allow them to repair damaged body parts and produce a new exoskeleton.\nOverall, the mating and reproduction process of tarantulas is a fascinating aspect of their biology. It showcases the intricate behavior and life cycle of these captivating creatures.\nTarantulas in the Wild\nTarantulas are fascinating creatures that can be found in various habitats around the world. They play a crucial role in the wild as efficient pest controllers, keeping populations of small rodents, amphibians, and reptiles in check. While most tarantulas prefer to live in warm and tropical regions, they can be found on every continent except Antarctica. South America is home to the highest diversity of tarantula species, with a wide range of adaptations to different environments.\nThese adaptable arachnids can be found in a variety of habitats, including rainforests, deserts, grasslands, and even mountains. Some species prefer to live in burrows, while others may inhabit trees, rocks, or leaf litter. Tarantulas are expert burrowers, creating complex underground tunnels that provide protection from predators and unfavorable weather conditions.\nIt is important to note that some tarantula species are currently threatened in the wild due to overcollection for the pet trade. These species are protected under the Convention on International Trade in Endangered Species of Wild Fauna and Flora (CITES) treaty, which restricts their trade and ensures their conservation. In order to preserve these amazing creatures and maintain the delicate balance of ecosystems, it is crucial to support responsible and sustainable practices in the pet trade industry.\nTable: Tarantula Habitat Examples\n|South American Rainforest\n|Cyriopagopus sp. “Blue”\n|Southeast Asian Rainforest\nTable Source: Compiled by the author\nTarantula Migration and Viewing Opportunities\nTarantulas are captivating creatures known for their distinctive behaviors, and one of the most remarkable events in their life cycle is their annual migration. This natural phenomenon occurs in certain regions, offering people the opportunity to witness the fascinating spectacle of tarantulas on the move.\nOne notable location for tarantula migration is La Junta in Colorado. Each year, during the month of September, male tarantulas embark on a journey in search of a mate. These male spiders roam the area, creating a mesmerizing sight as they traverse the landscape. This migration has attracted the attention of nature enthusiasts and curious onlookers, providing a unique opportunity to observe tarantulas in their natural habitat.\nThe hour before sunset is the ideal time to catch a glimpse of these migrating tarantulas. As the sun begins to set, the tarantulas become more active, increasing the chances of spotting them. It is crucial to respect these creatures’ space and observe them from a distance to avoid causing stress or disturbance.\nInteresting Facts about Tarantula Migration:>\n- Tarantulas migrate for the purpose of finding a mate, as males venture in search of receptive females.\n- During migration, tarantulas may travel several miles, crossing diverse terrain and landscapes.\n- These migrations are often triggered by environmental factors such as temperature and weather conditions.\n- Male tarantulas use their sensitive leg hairs to detect the pheromones emitted by female tarantulas in order to locate potential mates.\nWitnessing tarantula migration can be an awe-inspiring experience, allowing individuals to appreciate the natural wonders of these unique creatures. However, it is essential to remember that tarantulas are wild animals and should be observed with caution and respect for their habitat.\nRare Cases of Tarantula Bites\nIn general, tarantulas are not aggressive towards humans and their bites are rare. However, there have been some cases where tarantula bites have resulted in more severe symptoms. While these instances are uncommon, it is important to be aware of the potential risks and understand the symptoms associated with tarantula bites.\nThe symptoms of tarantula bites can vary depending on the individual’s sensitivity to the venom and the species of tarantula involved. Common symptoms include redness, swelling, and pain at the bite site. Some individuals may also experience muscle spasms, chest pains, sweating, and other systemic reactions.\nIt is crucial to seek medical attention if you have been bitten by a tarantula, especially if you are experiencing severe or prolonged symptoms. A healthcare professional will be able to provide the appropriate treatment and offer guidance on managing any potential complications. Remember to inform them of the Latin name of the spider if you know it, as this can assist in determining the severity of the bite.\nTarantula Safety and Keeping Pet Spiders\nWhen it comes to tarantula safety and keeping pet spiders, it’s important to be well-informed and take necessary precautions. While most tarantulas are harmless and not a cause for concern, there are some species that can be dangerous, such as the venomous Sydney funnel-web spider. Here are some key points to consider:\n- Handle tarantulas with care: Always approach them calmly and avoid sudden movements to prevent stress or aggression.\n- Know your spider’s species: Familiarize yourself with your tarantula’s Latin name to better understand its specific needs and behaviors.\n- Keep children and pets away: Tarantulas are delicate creatures and can be easily harmed if mishandled. Ensure they are kept in a secure enclosure away from curious hands and paws.\nKeeping Pet Spiders:\n- Provide an appropriate enclosure: Tarantulas need a suitable habitat that mimics their natural environment, including proper substrate, hiding spots, and sufficient ventilation.\n- Feed them a proper diet: Tarantulas are carnivorous and require a diet of live insects such as crickets, mealworms, or roaches. Make sure to provide varied and appropriately sized prey.\n- Maintain optimal temperature and humidity: Different species have specific temperature and humidity requirements, so it’s crucial to provide the right conditions for your pet spider’s well-being.\n“Tarantula safety and responsible pet ownership go hand in hand. By understanding and meeting the needs of these fascinating creatures, we can ensure their welfare while enjoying the unique experience of keeping a pet spider.”\nRemember, owning a tarantula is a responsibility that requires knowledge and commitment. If you are considering getting a pet tarantula, it’s essential to research and seek advice from experienced tarantula keepers. By taking the necessary precautions, you can safely and responsibly enjoy the captivating world of tarantulas.\nTarantulas are fascinating creatures that serve an important role in the ecosystem. Despite their intimidating appearance, they are generally not aggressive towards humans. While some species can deliver bites with more severe symptoms, tarantulas prefer to be left alone and will only bite when severely provoked or mistaken a finger for prey.\nUnderstanding tarantula behavior is key to coexisting with these unique creatures. They are ambush hunters and use venom to subdue their prey, but their venom is usually comparable to a bee sting and not a danger to humans. By taking necessary precautions when dealing with tarantulas, such as avoiding sudden movements and wearing protective gloves, individuals can safely appreciate their presence.\nTarantulas play an essential role in controlling populations of invertebrates and serving as natural pest control providers. They can be found on every continent except Antarctica, with most species residing in South America. However, some tarantula species are threatened due to overcollection for the pet trade, highlighting the need for their protection.\nIn conclusion, tarantulas are incredible creatures that deserve our respect and appreciation. By understanding their behavior and taking necessary precautions, we can coexist with tarantulas and marvel at their unique qualities. So, the next time you come across a tarantula, remember to observe from a safe distance and admire their important role in the ecosystem.\nWill tarantulas attack humans?\nTarantulas are generally not aggressive towards humans and prefer to be left alone. They only bite when severely provoked or mistaken a finger for prey.\nDo tarantulas pose a threat to humans?\nTarantulas are not a threat to humans. Their venom is usually comparable to a bee sting and not strong enough to cause harm.\nWhat do tarantulas eat?\nTarantulas are ambush hunters and prey on small rodents, amphibians, and reptiles. They play an important role in pest control.\nHow long do tarantulas live?\nFemale tarantulas can live up to 20 years, while males have a shorter lifespan. They molt multiple times over their lifetime.\nWhere are tarantulas found?\nTarantulas are found on every continent except Antarctica. Most species are found in South America.\nAre tarantulas dangerous to handle?\nTarantulas are not dangerous to handle if done with proper care and precautions. It is important to know the species and avoid unnecessary provocation.\nCan tarantulas bite?\nTarantulas can bite if severely provoked. While most bites are comparable to a bee sting, some species may cause more severe symptoms.\nAre tarantulas a protected species?\nSome tarantula species are threatened in the wild due to overcollection for the pet trade. They are protected under the CITES treaty.\nWhen is the best time to view tarantulas?\nThe ideal time to view tarantulas is in September, during the hour before sunset. Some regions, like La Junta in Colorado, are known for tarantula migrations.\nWhat should I do if I get bitten by a tarantula?\nWhile tarantula bites are rare, if you are bitten, it is important to seek medical attention. Symptoms can vary, and it is best to be evaluated by a healthcare professional.\nCan I keep a tarantula as a pet?\nYes, many people keep tarantulas as pets. However, it is important to research the species, their care requirements, and potential risks associated with keeping a pet spider.","Medically reviewed by Roxana Ehsani, MS\nPeople take probiotics for a variety of health concerns, especially for digestive conditions such as diarrhea, constipation, or irritable bowel syndrome (IBS). Probiotics may have an effect on the gut microbiome, which is made up of all the microorganisms that live in the digestive tract.\nWhat’s not clear is which types of probiotics are best for which conditions, what time of day to take probiotics, or whether probiotics need to be taken with food. This article will include a review of what is known about the best times to take probiotics.\nHow to Take Probiotics\nProbiotics come in several forms. You may consume probiotic-rich fermented foods, such as yogurt, kefir, kimchi, sauerkraut, kombucha, and miso. Or, you may take oral probiotic supplements in capsule or liquid forms. In addition to these are topical probiotics that are applied to the skin or the vagina.\nOne of the main issues with oral probiotics is how to keep them alive long enough to go through the acidic stomach environment and the small intestine to reach the colon, where they are thought to have the most effect.\nStarting a Probiotic\nIt’s important to consult with a healthcare provider before starting a probiotic. Probiotics are available in many different forms and strains, and it may be difficult to determine which type to try. A healthcare provider can help by recommending a specific probiotic.\nSome research has been done on what time of day probiotics should be taken and what types of food they should be taken with for the greatest effect, but a general agreement by the medical community has not been reached. However, there are some rules of thumb that can help you make choices about probiotics.\nOne study sought to determine if what you eat or drink with probiotics affects how they work. The experiment was not done on humans but instead was an in vitro (not using a live organism) study using a model of the digestive tract.\nThe strain in the study was Lactobacillus fermentum K73. Researchers found this particular strain lived longer when it was mixed with milk vs. water. This could mean that taking a probiotic with milk might be more helpful than taking it with water. But the authors couldn’t say for sure this made a significant difference in the makeup of the human gut microbiome.\nA 2017 study from Italy looked at a combination of Bifidobacterium longum and Lactobacillus rhamnosus, two types of bacteria found in the digestive system. The participants took the probiotics either 30 minutes before or 30 minutes after breakfast. Timing didn’t factor into the results, as both groups were shown to have an increase in “good” and a decrease in “bad” bacteria in their gut.\nPrebiotics are also important when taking probiotics. Prebiotics are nondigestible parts of plants that become food for the probiotics (and the existing microorganisms in the gut microbiome). Some food sources of prebiotics are nuts, legumes (beans, chickpeas, lentils, soybeans), onions, garlic, wheat products, artichokes, and chicory root.\nThe takeaway is that there's no one agreed-upon way to take a probiotic. It should be based on each individual's needs, how they structure their day, and if the probiotics are causing any side effects.\nWhen to Take Probiotics: Morning or Night?\nMost clinical trials on probiotics don’t include the time of day the supplements were taken, such as first thing in the morning or before bedtime. This could mean that researchers don't expect time of day to have much of an impact.\nThe best time to take a probiotic might be the time when it’s most convenient to take it. A probiotic that isn't taken because it is forgotten won't have any opportunity to work.\nTalk with a healthcare provider about how to take a probiotic. In addition, every probiotic will have its own instructions on how best to take it. Probiotics are made using various methods and with different ways of encapsulation, such as coatings on capsules to protect the contents from stomach acid or to allow timed release.\nThe product label should give details on when is the best time to take the probiotic, whether it should be taken with or without a meal or with milk or other foods, and if it's most effective when taken in the morning or at night.\nWhen Is the Best Time to Take Probiotics?\nThere’s not much evidence that shows the best time to take probiotics, so schedule it when it's most convenient for you to remember to take it.\nIf you experience symptoms after taking a probiotic, you can try changing the time of day you take the probiotic or taking the probiotic with a meal. If it’s recommended to take the probiotic with food, take it with the same meal each day (such as breakfast).\nCheck with a healthcare provider if it makes sense to take a probiotic at a different time of day from other medications. For example, they might recommend not taking it at the same time as an antibiotic, as that drug might kill the probiotic organisms.\nWhat might be most important is to take the probiotic consistently and to take it for the recommended amount of time.\nTips on Starting a Probiotic\nWhen starting a probiotic:\nAsk a healthcare provider for advice on which probiotic to take (such as a particular strain or brand).\nRead the label carefully to understand when and how to take it.\nTake the probiotic at the same time each day.\nAssess how you feel after you take it.\nConsider keeping a symptom diary to track progress.\nSigns Probiotics Are Working for You\nWhether a probiotic might help with symptoms will depend on a number of factors, such as the strains, formulation, dosage, and the condition being addressed. In general, one measure of whether a probiotic is working is that symptoms start to improve. However, you should keep in mind that the probiotic might not be the reason for the improvement.\nPeople take probiotics for several desired outcomes. Most of these have not been verified by research. Possible benefits you may notice include:\nMore regular bowel movements\nLess stomach pain/cramping\nClearer skin (fewer breakouts or improved eczema)\nImproved immunity (fewer colds or viral infections)\nPotential weight loss\nFewer vaginal infections\nSigns Probiotics Are Not Working for You\nThe evidence on probiotics is not clear-cut. It’s still unknown which strain (or strains) might be best for each person and for which health condition. Because probiotics usually are not covered by health insurance plans and can be expensive, consumers will want to know that what they're taking is worth the added expense.\nIf you are taking a probiotic supplement, check the label to see how long the manufacturer recommends taking it to experience benefits. See your healthcare provider if your symptoms continue or do not improve. If you have a health condition or concerning symptoms, do not take probiotics instead of seeing a healthcare provider.\nIf your healthcare provider has recommended a probiotic, ask your provider when you can expect to see results and what signs may indicate the probiotic is working. If there’s no noticeable change in that time period, ask whether you need to change the strain or discontinue taking probiotics.\nAny adverse effects that occur after starting a probiotic, such as a headache, bloating, or excess gas, are a reason to ask a healthcare provider about changing the probiotic or stopping it.\nProbiotics are largely thought to be safe. However, they are living organisms that can grow and colonize in ways that aren't intended. When using a probiotic, you may experience rare adverse events, including infections, especially if you are immunocompromised.\nFor that reason, it’s important for anyone with a health condition who wants to take probiotics to first ensure that it is safe to do so.\nWho Should Not Take Probiotics?\nProbiotics usually don’t cause any serious problems. Minor symptoms in some people include gas, as well as infections, though infections are uncommon.\nHowever, there are some people who might be at a greater risk of having a bad outcome. People who are very ill or who have a compromised immune system may not be good candidates for probiotics. The World Gastroenterology Organisation recommends against people with serious diseases or a compromised immune system taking probiotics.\nFor those who want to try probiotics, asking a healthcare provider will be the best way to get advice before exploring the option. The time of day probiotics are taken and if they should be taken with a meal will depend on the strain and on the manufacturer's recommendation.\nBenefits should be noticed within a few weeks. Probiotics are safe for most people, but those who have serious illnesses or are immunocompromised may be at a greater risk of complications."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:8c977c84-fcca-4ec8-a7a2-bb1ba41f8e5b>","<urn:uuid:718f93cf-4839-45d9-be23-dac1b5e325ed>"],"error":null}
{"question":"How do the voltage options compare between a Hammond X5 organ's power supply repair and modern guitar pedal power supplies?","answer":"The Hammond X5 organ's power supply operates with higher voltages, specifically dealing with +15V and -27V lines for its preamp and other circuits. In contrast, modern guitar pedal power supplies typically work with lower voltages - mainly 9V as the standard, with some units offering switchable options for 12V, 15V, and 18V to accommodate different pedal requirements. The Hammond's power supply also requires more complex voltage regulation and smoothing circuits compared to the straightforward isolated outputs of pedal power supplies.","context":["Well here is part 2 of the Hammond X5 organ repair. After fixing all the issues that were addressed in the first post ill first post the list of problems identified so far that haven’t yet been fixed.\n- Half the key wouldn’t work\n- Percussion on the pedals made awful noises if and when present\n- There didn’t seem to be and difference in note on the keys that worked\n- Sometimes it would make a god awful bang.\nWell the last problem on the list was the one I fixed first. As soon as I tried to check the voltages on the power supply it became blatantly obvious what was causing it. One of the wires, when touched, would reproduce the issue and seemed to move in the hold it was connected to inside the PCB.\nThe wire in question is the orange wire, which provides the +15 volts to the preamplifier board and is marked +B1. After removing and touching up the solder joints the banging had gone, but when going back to check the voltages -B12, that is supposed to be -27, was fluctuating like mad on my meter. I’ve seen this before and if I would have got my oscilloscope out to check I would guess there would be some severe ripple on -27 volts. When you see your meter fluctuating, it is this ripple that is confusing the multimeter and can sometimes be measured with the ac setting. I didn’t do this because it was a pretty certain bet that capacitor C10 100uf at 35v next to transistor Q3 in the top middle of the picture above was bad.\nHere is the schematic of the Hammond X5 Power supply.\nAs can be seen in the picture, this capacitor has the job of smoothing out the ripple on the -27 volt line. After this capacitor was replaced the meter read a steady -26.8 volts which is damn near -27. After a quick check of all the other voltages I was happy that I could move on and was hoping this had fixed a lot of the other problems and hadn’t created any.\nWell sadly that hopeful optimism was short lived as it only seemed to fix the banging noise. The circuits that the -27 volts fed must not really care about ripple or may even have local smoothing and even regulation that means that the ripple was a not an issue. Either way it is better that it’s fixed. Now I started looking for the reason why only half the keys worked.\nI first started pushing and pulling the connectors to see if the problem was just a bad connection but as soon as I pushed a connector on the amplifier and reverb drive board, sparks seemed to come from underneath. After pulling the board out this is what I found.\nNow ignore the bodge wire job, these are common in old equipment. I was more interested in the chip hanging on to the PCB for dear life. It was a 74LS93 4 bit binary counter. I was starting to think this could be the issue with the notes not changing and the keys not working as an organ such as the Hammond works by generating a tone and dividing it up to make each individual note, as far as I understand (I do stand to be corrected). This was leading me to think that the binary counter could be used in part of this division so I started to look for where it may have come from. Sadly this idea fell flat when not a single chip socket was found to be unpopulated, but this does tell me that somebody may have been here to fix this exact same issue before me and more than likely, caused further damage.\nThe only 74LS93 I could find was on the MDD generator board. This board is also where the -27 volts from the power supply go.\nThe two chips at the top of the board are MM5832 and MM5833. They are responsible for dividing the clock down into its respective notes. Looking at this part of the schematic it started to reveal that maybe the issue was with the missing keys.\nAs we can see the two chips to the right of the schematic (IC2 and IC3) are nicely labelled to show which note each output produces. As the note IC3 corresponded to the keys that wouldn’t play, I was pretty confident I was onto something. Before I cracked out the oscilloscope I decided that I would check IC1, the only 74LS93 I could find on the board that was the same as the chip found earlier in the repair. This is the response I got from my Wellon programmer that also doubles as a logic tester.\nTo no surprise, the one I found under the board gave a similar result. I was hoping that the two top level octave divider chip had survived and this was the only casualty. The thing that I kept thinking about was the problem with the -27 volt supply. I think that the problem with the supply had taken the chip out and then was replaced hence why I found one floating around under the PCB.\nI got my oscilloscope out and looked for an output from the MM5832 and MM5833 with signals looking good on the input. One was outputting a signal, but the other was stone cold dead. I looked at the schematic and decided that switching these two chips would alternate the keys that function (all be it out of tune). This test confirmed that the IC3 was dead.\nThis was a bit of a setback as these chips have been out of production now for quite some time. Luckily a bit of internet searching lead me to a website call FlatKeys owned by a guy called Chris Burrell who makes and sells logic replacements for these chips. He also deals with an assortment of other hard to replace chips for organs and sells them at a very reasonable price. The added bonus was with the FlatKeys FK50240 top octave synthesiser chip I could also add a transpose switch and an octave switch to give the X5 some new sounds that the original was incapable of (think of the start of Stargazer by Rainbow….well that was the only one I could do).\nI asked Chris if he could send me an example of it fitted to an X5 and almost immediately got an email back showing just that.\nThis gave me confidence that this was the right move and rang the owner of the X5 to see how he would like to proceed. This was greeted with excitement and he quickly ordered the parts and within a few days brought them round to the workshop for me to install.\nThe kit came with a small circuit board with the ribbon cables already attached in a black project box all screwed together for safe transport. Along with this there was a multi-position rotary switch that was already wired up to an octave switch. This was attached to a plug for an easy install and a transpose switch again connected to a plug. As this kit is made for various organs and keyboards no mounting holes have been drill so firstly I set about drilling and cutting the project box.\nHere you can see I’ve cut a slot for the ribbon cable and a cutaway for the switch plugs. It could have been a bit nicer but I didn’t have a file handy. I also at this point drilled some pilot holes to hold the box to the back of the organ.\nThis is with the box and board installed and switches plugged in ready to test before fitting the lid. Before I moved on and got to testing there was a little modification that needs to be made to the MDD board before the FK50240 will work in and X5\nThis is the install modification that Chris sent me that has to be made before moving on. As you can see R12 has to be replaced with a wire link and R13 removed. I could have just put a jumper over R12 and snipped R13 out to make life easy but I decided that I would do it proper and make it look good.\nThe photos I took after the modification were all blurry so in the picture above I’ve added two coloured arrows. The green arrow points to the resistor R12. This one needs replacing with a wire link or jumper. The yellow arrow points to R13 and that needs removing from circuit.\nAfter the MDD board has been modified all that is left to do is remove the MM5832 and MM5833 from their sockets and insert the respective plug from the FK50240 (these for me were handily labelled).\nHere is a picture of my install as you can see I pretty much copied the example given to me by FlatKeys. After this I hooked up the controls and gave it a quick test. At this point all the keys jumped back to life and I moved onto finding places for the new controls. Me not being well up on playing organ I decided it was best to contact the organs owner and asked for some direction.\nIll wrap it up here and finish this off hopefully with a part 3.","Building your first pedal board? Looking for clean, silent power for all of your effects pedals?\nThen you need a good pedal power supply.\nNo matter how many pedals you have in your chain or what level of musician you are, you’re going to need electricity fed to your effects pedals in order for them to operate properly.\nBut with all these terms like Voltage, Currency, and Isolation, how are you supposed to pick the right power supply for your needs and budget?\nThese are our top picks for the best pedal power supply in 2021.\nSnapshot: Best Pedal Power Supplies in 2021\n- Voodoo Lab Pedal Power 2 Plus – Best Overall\n- JOYO JP02 – Best Budget Option\n- Friedman Power Grid 10 – Best High End Option\n- Strymon Ojai – Best Isolated/Best Mini\n- JOYO JP05 – Best Rechargeable\nPicking a Great Pedal Power Supply That’s Right for Your Board\nWhen picking out the best pedalboard power supply, I kept in mind the following four essential pieces of information:\n- Voltage – Power requirement. Usually 9V, but ranges from 12V, 15V, 18V, even 24V. Minimum power must be met to power your pedal. NEVER add too much voltage – will damage your pedal.\n- Current – Flow of electrical charge, measured in milliamps (mA). Minimum current is a requirement and exceeding current minimum is ok.\n- Required Current – Rule of Thumb: add up mA requirements of all your pedals and double it. This is what your power supply needs to be rated.\n- Polarity – AC and DC. Most pedals are 9V, DC, Center-Negative\n- Isolation – means each pedal is individually powered, as opposed to daisy chain power. Isolated power is more reliable and reduces unwanted noise due to ground loops.\nThat Pedal Show refers to these requirements using the acronym VCPI. To learn more, check out their video on power supplies.\nMy list of the best pedalboard power supplies includes options that addressed the VCPI essentials in a safe and dynamic manner so that a wide array of pedals could be properly powered.\nMost notably, every option on this list features isolated power, as this is the best way to power your pedals.\nI also kept in mind that doing this can get expensive, so I’ve included some models that power for less, but not at the expense of quality. I also made sure to pick some smaller and rechargeable (battery powered) options for traveling/touring guitarists.\nThe Best Pedal Power Supply Options in 2021\n1. Voodoo Lab Pedal Power 2 Plus Review – Best Overall\n- A universal power supply for all battery-operated guitar pedal effects\nThere is no better place to start a list of the pedalboard power supplies than a tried and true staple of the industry. The Power Plus 2 from Voodoo Lab has been the go-to pedal power supply for professional guitarists for over a decade.\nThe Voodoo Lab Pedal Power 2 Plus is a compact power supply that is built like a tank and utilizes toroidal transformers for low noise. It requires a standard 120V power, so it can be used anywhere in the US. It has eight outputs, each with short circuit protection.\nEven if one of your pedals were to go down due to a short circuit, the rest would still be powered. The box comes with all the required connections, including a detachable AC chord.\nEach of the Power 2 Plus’s eight outputs are switchable between 9 and 12 volts, giving you greater flexibility for analog pedals. You will need a separate voltage-doubler if you want to power 18V pedals. Two of the outputs are designed to handle 250 mA of current with the included red cables.\nAll of the outputs are set for DC, Center Negative polarity and are 100% isolated.\n- Ultra Low Noise\n- Sturdy build ready for the road\n- Can Sag 4V to 9V on last two outputs\n- 250mA max current\n- Bulky, won’t fit under mini pedal boards.\n- Dimensions: 6 x 3.4 x 1.8”\n- Voltage: 9-12V Switchable\n- Current: 100/250mA (Rated 1100mA)\n- Isolated: Yes\n- Features: Sag, Low Noise, Highest Build quality,\nFinal Thoughts on the Voodoo Labs Pedal Power 2 Plus\nThough not the most versatile pedalboard power supply on the market, the Power 2 Plus is the perfect companion for analog pedal heavy boards. It has a solid construction that will last you a lifetime and is dead quiet. If you need other power requirements that this model doesn’t offer, I highly recommend looking at the rest of the Voodoo Labs power supply line.\n2. JOYO JP02 Review – Best Budget Option\nI get it – you want to spend your money on new pedals instead of a power supply. Thankfully, the JOYO JP02 is a highly rated and flexible power supply with a small price tag.\nThe JP02 from JOYO is the perfect pedalboard power supply for small pedal boards or those who are on a budget. Each of the ten outputs features independent short circuit prevention technology, so even if one pedal goes down the rest of your chain receives reliable and steady power.\nThe power supply has bright blue LED’s above each of the outputs, so you know that they are functioning. This is great for troubleshooting purposes. It features high performance, peripheral-simplified switching for an extremely low noise performance.\nThe first eight inputs are set for 9V, 100mA power that is ideal for overdrives and other analog pedals. The eight input is switchable up to 500mA and can power large Strymon or Line 6 digital modeling pedals. Output 9 is set for 12V and output 10 is set for 18V.\nThe supply can be powered by a DC18V input and is rated for 1.5 amps of current out. The entire supply is DC out and features all isolated power for each.\n- Easy to use and safe\n- Low noise\n- Low price point\n- Small build\n- Only one 500mA output\n- Dimensions: 5.75 x 2.56 x 1.38”\n- Voltage: 9/12/18V\n- Current: Rated 100/500 (Rated 1500mA)\n- Isolated: Yes\n- Features: LED’s\nFinal Thoughts on the JOYO JP02\nThis is a great option if you are purchasing your first power supply for your pedals, you have a small board, or if you are on a budget. It will power just about any analog pedal but may not be the best option if you have a wide array of high current digital pedals.\n3. Friedman Power Grid 10 Review (9V) – Best High End Option\n- Universal power from 100-240Vac meaning you can plug it in anywhere in the world\nOn the surface, the Power Grid 10 is like a Voodoo Lab Power 2 Plus on steroids. It has more inputs, more current potential, and also a larger price tag. In this case, the increase in price is very worth it once you realize what the Power Grid has to offer on the inside.\nThe Power Grid 10 is a lightweight power supply for your pedalboard built without traditional transformers in it. While it isn’t the smallest option around, it is built to fit perfectly under all Friedman pedal boards using just two screws.\nIf you aren’t using Friedman products, the Power Grid 10 can still be mounted on top of any board and is designed to be stable enough to have two standard sized pedals fit on top of it. It has universal power (100-240vac) so it can be plugged in anywhere around the world.\nThe box also comes with all the necessary cables and performs with zero hum, even with pedals on top of it trying to cause interference.\nWhile all ten outputs power 9V, the Power Grid 10 can power 18V pedals using the two slot cable included in the box. Because all then outputs allow for 350mA of current, this box can power just about any pedal on the market and many of them. As the front panel shows, all outputs are DC, Center Negative and are fully isolated for perfect power.\n- Universal Power (100-240vac)\n- 10 isolated, high current outputs\n- Zero hum\n- Dimensions: 4.5″ x 6.75″ x 1.75″\n- Voltage: 9V\n- Current: 350mA\n- Isolated: Yes\n- Features: Universal Power, Includes all cables, Riser with zero hum\nFinal Thoughts on the Friedman Power Grid 10\nFriedman makes some of the best pedalboard hardware around, including one of the best buffer pedals on the market. So it is no surprise that they will also make one of the best pedalboard power supply options ever. This will work for just about any pedal and it is the ultimate defense against noisy pedal boards.\n4. Strymon Ojai Pedal Power Supply Review – Best Mini/Best Isolated\nIf you have a small pedal board, real-estate is at a premium. You can’t afford to waste any space on your board for a pedalboard power supply, but you also need a power supply that can power the most demanding pedals. The Strymon Ojai (pronounced “Oh Hi”) is the most powerful supply in its class.\nThe Strymon Ojai is a small, powerful wonder of technology. Compared to all the other products on this list, the Ojai boasts big numbers in the currency department, while maintaining a manageable price point and a tiny footprint.\nIt is fully compatible with the rest of the Strymon power supply family using the 24V thru jack, so you can chain two or more Ojai’s together for larger pedalboards and it will remain low noise and high power. It also has LED’s to indicate which outputs are working.\nAll five of the outputs on the Strymon Ojai are 9V and a whopping 500mA, meaning that any pedal can be powered by this supply. It can even be used to power modelers like the Line 6 HX Stomp with the right connections. All of the outputs are DC, center negative and are double isolated. This makes this the best isolated pedal power supply on the market.\n- Incredibly small\n- High current\n- Low noise\n- Limited to 9V\n- Dimensions: 3.2″ x 2.3″ x 1.3”\n- Voltage: 9V\n- Current: 500mA\n- Isolated: Yes\n- Features: Expandable with 24V Thru, Dual Isolated outputs, LED’s\nFinal Thoughts on the Strymon Ojai\nFinding a powerful, reliable, and quiet power supply for your pedals was a challenge for a long time. Now, the Strymon Ojai has changed everything. This pedalboard power supply will fit under just about any pedalboard. While not the most dynamic in terms of voltage, it’s pretty crazy to think that power supplies twice as big as this can’t power as much as the Ojai.\n5. JOYO JP05 Review – Best Rechargeable Pedal Power Supply\n- With a built-in 7.4V/4400mAh rechargeable lithium battery pack, charging time is about 2.5hrs.\nSometimes a small pedalboard power supply isn’t good enough. Sometimes you need a small pedalboard that doesn’t need to plug into a wall to power all of your pedals. Whether you’re on tour or playing a small show outside, the JOYO JP05 is a high quality, battery powered supply that every traveling musician could benefit from using.\nThe JP05 steps things up a notch from the JP02 by incorporating a lithium battery to deliver its power. With 2.5 hours of charging time, you are rewarded with 10 hours of power time. If that isn’t enough, the JP05 is still capable of being powered through a DC 9V wall cable, and even if it comes unplugged the board continues to power your pedals through the battery.\nThe modern features don’t stop there. The JP05 also has a 5V USB output to power your phone or tablet. This will come in hand for all you working musicians out there that keep your charts on your phone. It is also small enough to fit under even the smallest of boards.\nSimilar to the JPO2, the JP05 has 4 DC 9V 100mA outputs, 3 DC 9V 500mA output, 1 DC 9V/12V/18V adjustable output, maximum output current is 100mA. All the outputs are DC, center negative, but the polarity can be reversed. Of course, all the outputs are fully isolated, making this a reliable, quiet, and extremely versatile power supply.\n- Low price point\n- Optional battery power\n- Wide range of current choices\n- USB output for phones and tablets\n- Only one jack for 12/18V\n- Dimensions: 7.83 x 6.77 x 2.76 inches\n- Voltage: 9/12/18V\n- Current: 100/500\n- Isolated: Yes\n- Features: USB output, LED, Rechargeable battery\nFinal Thoughts on the JOYO JP05\nOnce again, JOYO has created a game changing piece of gear at an unbeatable price. This might be the best pedalboard power supply for anyone looking to put together a small rig to take out on the road, or even for gigging musicians that need versatility when it comes to their power supply without breaking the bank. This is about as exciting as it gets when it comes to powering your pedals.\nFind The Right Pedalboard Supply For Your Needs\nDoing just a little bit of research about what kind of power your pedals need can go a long way in building your confidence around gear, as well as give you peace of mind knowing that your pedals are operating at their best.\nChoosing the best quality pedalboard power supply not only gives you a quieter signal, but it also protects your pedals and ensures that they will work year after year, not matter where you take them.\nI’ll be the first to admit that spending money on a power supply for your pedal board isn’t as exciting as buying a great tremolo pedal or a great delay pedal, but you will only be able to get the most out of your pedals if you give them the right power. Thankfully, doing this doesn’t have to break the bank.\nIt is my hope that this list has helped explain how power supplies work and how you can use that info to make an informed decision about what kind of pedal power supply you need.\nOnce your pedals are powered, all that’s left to do is play the guitar!\n- 6 Best Phaser Pedals In 2021 (Mini, Budget & High-End)\n- What Does A Wah Pedal Do?\n- What Does A Compressor Pedal Do & How Do They Work?\n- Guitar Effects Pedals Explained (How They Work & Our Favorites)\n- What Does A Chorus Pedal Do?\nDavis Wilton Bader is a professional guitarist/writer based out of St. Louis, MO. He plays in the bands Lumet and The Outskirts."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:8b48dd4c-0da0-41d6-bb60-e0015423b187>","<urn:uuid:1d13adc4-bb58-48ec-a280-e60f182c7799>"],"error":null}
{"question":"Which takes longer to mature: beets or cantaloupe?","answer":"Cantaloupe takes longer to mature. While beets can be harvested when they reach golf ball size and are sweetest when young, cantaloupes require 80-90 days to mature and take 10-20 days just to germinate.","context":["When to start: Spring.\nAt their best: Summer, autumn, and winter.\nTime to complete: 1 ½ hours.\nYou will need: Seed for root vegetables – such as carrots, radishes, beets, and celeriac.\nReliable and easy to grow, beets come in yellow, white, and striped varieties, as well as the traditional blood-red. Sow a new batch every few weeks and harvest the plump, sweet, earthy roots as you need them.\nSow seeds ¾ in (1.5 cm) deep in rows, directly in the soil, every two weeks from late spring. Thin the seedlings shortly after germination so plants are 6 in (15 cm) apart.\nBeets are sweetest and most succulent when young, and can be harvested when they reach the size of a golf ball. Leave some in the ground to grow larger, where they will last into winter. Pull them as required, although they will eventually become tougher and less tasty. When you harvest beets, cut leaves bleed and stain; instead, simply twist off the leafy tops.\nCarrots are the mainstay of the kitchen garden, but these root crops need free-draining soil to do well. They are also prone to a troublesome pest, carrot fly larvae, which you need to guard against.\nSowing in drills\nSow seeds ¾ in (1.5 cm) deep directly into the soil from mid-spring. As they grow you can thin them out and eat as baby carrots, leaving the others in the ground to mature. Try not to bruise their leaves when pulling them out because the smell attracts carrot fly (see Tips, below).\nCarrots are ready to harvest from midsummer. On sandy soils, you can simply pull them out of the ground, but on heavier soils use a fork, taking care not to damage the roots. Harvest young carrots to use immediately. Older ones can be stored in a cool place over winter.\nTips: Protecting covering\nThe carrot fly locates carrots by scent, and then lays eggs nearby. As these hatch, the larvae burrow into the necks of the roots, at the base of the foliage, often making the carrots inedible. To prevent this, cover the crop with a light, transparent mesh, such as garden fleece, dug into the soil at the bottom, or surround your crop with a solid barrier 30 in (75 cm) high, because the adults can only fly close to the ground.\nThe edible part of celeriac is actually the swollen base of the stem, not the root. Since this develops below soil level, it is commonly regarded as a root vegetable.\nSowing and planting\nIn early spring, sow seed in modules in a greenhouse or cold frame. Germination can be slow, and the plants need a long season to mature fully. Pot them on as they grow. When they are about 3 in (7 cm) tall, harden them off outside. Then plant them out into well-drained soil, at a distance of about 6 in (15 cm) apart.\nCeleriac can be harvested from late summer and throughout winter. As they do not store well once dug up, leave them in the soil until you need them. Cold weather improves the flavor of celeriac but can damage the plants. A mulch of straw applied before the first frosts will prevent this and keep your crop in good condition until you harvest it.\nRaised bed options\nElevating your crops in raised beds solves many problems. You can fill them with good-quality topsoil, free from weeds and stones, or use them for crops such as blueberries that like specific conditions—in this case, acid soil. They are also easier to tend, and can save your back from damage. Make your own or buy a kit, and choose beds made from natural lumber or woven willow, or lightweight man-made materials that last for many years.","Have Seeds will Garden (Planting Guide for a wide variety of vegetables)\nSo you want to start a garden but you don't know exactly how deep to plant the seeds or how far apart. Check out this list for tips and guidance. ~Happy Gardening!\nAsparagus: Soak seed overnight. Sow ½ inch deep. 2 inch apart. 60 degree soil. Let grow 1 yr. then transplant to permanent area. Harvest begins 3rd year.\nBeans/Peas: Sow 1 inch deep. 24 inches apart. For peas and pole beans provide a trellis. Peas and pole beans can be planted a bit closer at 6-10 inches apart. Full sun/well drained soil. Keep pea pods picked early and often to keep plants producing, same for bush beans- keep bean pods picked often so that plants keep producing blossoms.\nCabbage: Sow 1/8th inch deep as soon as soil can be worked. Full sun. Thin to 24 inch apart. Average 100 days to mature.\nCarrot: Sow seed ¼ inch deep. Thin to 6 inches apart. Full sun. About 65 days to harvest.\nCucumber: Plant 4 seeds per mound. ½ inch deep. 1 foot apart. Provide trellis. About 60 days to harvest.\nEggplant: Start indoors. Transplant to garden when 6-8 inch tall. Full sun/well drained soil. About 80 days to mature.\nEndive: Sow seed directly in garden as soon as soil is workable. Thin to 12 inch apart. 85 days to mature.\nLettuces/ Chards/Greens: Sow directly in garden. No deeper than ¼ inch. Thin to 12 inch apart. Full sun. Average 40-55 days to mature.\nTomatoes: Sow seed 1/4 inch deep. Start indoors 6-8 weeks before last frost date. Transplant to garden when 6-8 inches tall. Space plants 2-3 ft. apart. Full sun. Provide trellising or tomato cage. Full sun. Matures at about 70-80 days.\nPeppers: Sow seed no deeper than ¼ inch. Peppers need soil temp. of 70 degrees to germinate well. You can provide a heat mat underneath your seed tray to assist in germination. Germination takes 7-24 days. Transfer plants to garden when 6-8 inches tall. Full sun. 70-80 days to mature.\nRadish: Sow direct in garden ¼ inch deep. Full sun. Thin to 2 inches apart. Matures in 28 days.\nOkra: Plant seed ½ inch deep directly in garden. Thin to 18 inch apart when 2 inches tall. Harvest pods when 3 inches long. Keep pods picked to ensure good production. About 55-70 days to mature.\nPumpkins: plant 2 seeds per hill 1 inch deep. 4-6 ft. apart. Full sun. Keep free of weeds. Rich fertile soil. Average 95-110 days to mature.\nMelons: Watermelon / plant 2 seeds per hill 3 ft. apart. 1 inch deep. After danger of last frost. Full sun. Keep weed free. 75-80 days to mature.\nCantaloupe/ plant 2 seeds per hill 1 inch deep. 4 ft. apart. Full sun. 10-20 days to germinate. 80-90 days to mature.\nSquash: Summer squashes/ Sow 2 seeds per hill. 1 inch deep. 3 ft. apart. Full sun. 60-65 days to mature.\nWinter squashes/ Sow 2 seeds per hill. 1 inch deep. 4-6 ft. apart. After danger of last frost. 80-90 days to mature. Full sun.\nTurnip: Sow seed directly in garden as soon as soil is workable. ¼ inch deep. Full sun. 50 days to mature.\nHerbs: Sow seed direct in garden or pot at a depth of 1/8th inch. Full sun.\nI'm a photographer...and a country girl at heart.Born and raised in Florida, I have lived in many small/rural towns in Southern Florida. I love capturing the innocence and joy of childhood through photography.Along with photography I absolutely love gardening.\nContent /Writing copyright Tina Leavy/The Victory Gardener blog and The Urban Farmette blog and may not be reproduced without my written consent. Thank you for understanding. (images that have been used from Dk Images free clip art website have been duly noted..all other content/photos is/are copyright to the author of this blog)\nVisit our online Community Blog\n\"Small Town Living\"\nMy Victory Garden Etsy Shop\nI've been stalked/My Victory Garden Shop Item featured here."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:1dc98545-a564-4ec4-b6dd-c88fd88576e6>","<urn:uuid:0f75a53a-d09b-47fa-a79b-2697792db029>"],"error":null}
{"question":"Hey manufacturing pros! Can someone explain why machine simulation is super critical for mill-turn programming? Really need to understand this!","answer":"Machine simulation is crucial for mill-turn programming because it's much easier to have a machine crash on a mill-turn compared to 3-axis milling. The simulation allows users to visually represent machining processes on the computer without wasting valuable machine time proving programs. This helps prevent costly accidents and ensures optimal program verification before actual machining.","context":["Manufacturing is no island. Business is global. These last few years have definitely taught us that. With manufacturers facing fierce competition, producing a quality part in a shorter timeframe, all at a lesser cost is the promised-land we’re all trying to reach. So how do you get there? To some, it is like a remote island hidden somewhere across a turbulent ocean with only a broken paddle to help. To others, it is in plain sight—an achievable feat—albeit with some assistance from new technology.\nMulti-tasking is one technology that has been around for a while, but has only recently become the hot topic as more shops discover their uses. In fact, many companies who previously never thought about getting a multi-task machine have done so or are thinking about doing it for the gains in productivity they offer.\n“We wanted to reduce and/or eliminate fixturing as much as possible,” explains Tom Moore, co-founder of Single Source Precision Machining in Indiana, whose machine also comes with an optional indexing sub-spindle. “We also wanted to produce parts with better accuracy.”\nAdditionally, once a new technology has been acquired, new benefits are discovered. Phil Martin, of ETBO Tool and Die in ON reports, “We have been looking for a turning and a 5-axis solution for some time, but have been conservative in our purchases through the recent market collapse. Some of our tooling requirements are better suited to larger 5-axis machines and in the past we viewed a mill-turn as more of a production machine without the ability to address our larger requirements. With new opportunities for ETBO that are well suited to mill-turning it made us reflect on how many other applications we could apply mill-turn to.“\nAlthough any type of work can benefit, there are certainly target areas for multi-tasking: oil and energy, medical, small parts, etc. If you are doing this type of work, a multi-task machine is a valid consideration during your next budget meeting. Martin continues, “One of ETBO's focus areas is tooling for a diverse range of automotive metal stampings as well as thin metal stampings for medical and electrical markets. Through the downturn we have been more aggressive in marketing our ability to provide custom machining solutions for various industries including aerospace, medical as well as our more traditional automotive markets. With the synergies of these diverse markets […], a mill-turn machine was a perfect fit to supply our customers with the cost-effective quality solutions expected.”\nThe last few years have been challenging, and the parts have not gotten any easier. In fact, parts seem to be getting more and more challenging these days, perhaps due to the overwhelming use of 3-D solid model design systems allowing engineers to render, virtually test, assemble and visualize parts prior to manufacturing. Issues like design changes, aesthetics and manufacturability are all coming into play more than ever before, which combine to change processes downstream, including manufacturing. We now see lathe parts with features requiring moldmaking processes, and molds or parts where features can be more efficiently turned on a lathe or mill with turning capability. In order to keep up with challenging new geometries for parts while keeping costs low, multi-task machines are quickly becoming the go-to technology to reach that goal. Supporting this does offer some challenges.\nMulti-Tasking Software Challenges\nA machine like this could be considered disruptive technology— having the ability to significantly change downstream processes. One of the biggest mistakes a shop can make is not considering what might be affected and planning accordingly. In fact, many companies will look at new machine tool technology every year, and not much else to ensure maximum productivity gains.\nOne of the biggest challenges with a machine like this is the heightened complexity of NC programming and the importance of program verification. The concept of the machine is often misunderstood or under-utilized, and some shops struggle to make ends meet enough to justify the machine purchase or another machine. However, when the machine’s capabilities are finally realized, the machine quickly justifies itself. This is why a programming system is important for these machines.\nThere are a lot of things to think about, but here are six questions to ask when looking at your in-house programming software, or shopping for a new solution:\n1. Does it support all the options available to the machine?\n2. Does it support more complex machines/machining?\n3. Does it have the front-end capability to give me the tool control needed for this machine?\n4. Does it have a simulation capability that is accurate and can fully represent the machine?\n5. Does it have a post processor that is already proven for this machine tool?\n6. Does it program all my existing equipment at the shop?\nAs a manufacturing company adds new capabilities and technology, the shop shouldn’t get overburdened with implementing complex technology without the necessary supporting elements.\nWhether the multitask machine is a milling machine with turning capability or a multi-spindle, multi-turret lathe, the machines are getting more complex and solutions need to support these, and future machines. We are seeing machines with sub-spindles that index and move, four turrets, multiple gang-slides and even indexing B-axis milling heads on lathes now as common. Taking all these machines into account, a programming system is going to directly relate to the productivity you receive from your machines. In the case of ETBO, they looked for a CAM solution that has a simple interface while still allowing for detailed toolpath control.\nMartins explains, “With respect to mill-turn specifically, the solution we chose had a tight relationship with Mazak creating a robust post processor and good simulation of the machine model. There are two needs that stood out to ETBO for programming mill-turns. The first is machine simulation. It is much easier to have a machine crash on a mill-turn compared with 3-axis milling. The second is axis control. On a mill-turn there are often various strategies to accomplish an end product. We needed a programming system that provides robust toolpaths to utilize the axes that are applicable to the highest quality part and/or most cost-effective machining strategy.”\nThis programming system should support the tool control needed to generate a complete and optimized NC program up front. The program should offer the user a seamless interface for visually representing the machining processes on the computer, without having to waste valuable machine time proving programs. In this regard, the post processor should be proven, and available to be supplied with the programming solution. In the case of Single Source Precision Machining, a proven CNC-programming solution was supplied on-board the machine tool itself.\n“Our machine tool can do so much,” says Moore. “To date, our new programming system has matched our multi-task machine’s every move. Receiving a programming solution from the machine builder with a reliable post processor and simulations are a great value to us. In fact, we have just recently installed our second machine, so things are going good.”\nThe complexity of these machines will create a many-week, several-month process of proving out NC code if this is not the case. The cost of the machine being down all this time will add up to a significant difference in cost if the machine had been cutting parts. This monetary difference would also likely exceed the entire cost to implement a new programming system, so proper care should be taken in evaluating these things for your machining requirements today, and in the future."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:3f9220fd-1c33-4e1f-a6f4-9eabdd1a8049>"],"error":null}
{"question":"Compare the educational activities about bats with bedtime preparations in the stories - which involves more student participation?","answer":"The bat educational activities involve more student participation, featuring multiple interactive elements like brainstorming bat facts, reading words in unison, copying words into 'batty books,' writing words in alphabetical order, and receiving bat stickers as rewards. In contrast, the bedtime preparation activities are more passive, involving simply putting on a onesie, grabbing a stuffed animal, and listening to soothing bedtime verses.","context":["Beacon Lesson Plan Library\nDescriptionWe are going batty! In this lesson students begin with the word \"at\" then learn about bats and other things ending with \"at.\"\nObjectivesThe student uses beginning letters (onsets) and patterns (rhymes) as visual cues for decoding.\nThe student alphabetizes words according to the initial letter.\nMaterials-Pencils for each student\n-[Zipping, Zapping, Zooming Bats!] By Ann Earle. Illustrated by Henry Cole. Scott Foresman, May 1995. ISBN# 006445133X.\n-Overhead projector pens\n-Teacher-made poem (see associated file)\n-Dry Erase board\n-Dry Erase markers\n-Student Batty books (see Teacher Preparation)\nPreparations1. Gather materials. (See materials list)\n2.Construct \"batty\" books (one per student) by folding letter-sized, black construction paper in half horizontally, then folding first grade writing paper in half horizontally. Staple the papers together. Paste a batty picture on front of each book. Write students' names on batty books.\n1. Begin class by putting on the bat puppet and asking students to share facts about bats they already know.\n2. List bat facts they already know on the dry erase board.\n3. Read [Zipping, Zapping, Zooming Bats!] By Ann Earl.\n4. Discuss new bat facts learned from the book.\n5. Write the word bat on the board and ask if students know the beginning sound.\n6. Underline “at” in bat and ask students to sound out the word, then pronounce it.\n7. Ask if students can name words that rhyme with bat. List the words on the board and give a bat sticker to students who contribute a word to the list.\n8. Direct students to read, in unison, the list of \"at\" words.\n9. If students stay together and pronounce the words accurately, reward them with a special bat sticker for doing such a great job.\n10. Distribute batty books and direct students to copy the list of words from the board into their batty books.\n1. Use the bat puppet to introduce and read the poem about bats. (see attached file)\n2. Using the overhead projector, display the poem for students to read aloud with the teacher.\n3. Ask students to identify and read the “at” family words they find within the poem while the teacher underlines them.\n4. On a blank transparency, list the “at” family words.\n5. Discuss alphabetical order using the word list then ask students to suggest the first three words if they were alphabetizing the list. Congratulate students for correct answers.\n6. Distribute \"batty\" books and direct students to copy the bat poem in their books.\n7. Next, direct students to look at the “at” word list projected on the overhead and write the words in alphabetical order in their “batty” books.\n8. Collect students' books.\nAssessmentsIn this formative assessment, students brainstorm and suggest words ending in the syllable \"at\". The teacher writes student-suggested words in random order on the board. Students will have successfully completed the task if they are able to write the words alphabetically in their batty books with three or fewer errors. Students who do not meet the criteria will be assisted and given an opportunity to amend their word lists.\nWeb LinksThis site contains facts about bats and bat anatomy.\nAttached FilesThe “Batty” poem to use with the lesson for day two. File Extension: pdf\nReturn to the Beacon Lesson Plan Library.","Put on your onesie and grab your favorite stuffie. You’re just in time to cuddle up with a sweet and soothing new bedtime picture book!\nIn lyrical rhyming verse, Dreaming of You by Amy Ludwig VanDerwater and Aaron DeWitt (Boyds Mills Press, 2018) helps us imagine what some of our favorite animals might dream about at night.\nTonight may you dream sweet animal dreams.\nTonight may your dreams all run free.\nTonight may you dream of what animals dream.\nWhen they sleep, what do animals see?\nWhile kittens dream of lapping fresh milk, chipmunks dream of digging deep burrows, fishes of tasting new plants, horses of wild, windy rides, and bunnies of napping in thickets.\nVanDerwater includes ten different animals in all, featured in well crafted ballad quatrains with abcb end rhymes and the same repetitive word pattern in the first three lines — a perfect lullaby, calming and incantatory as it lulls the reader to slumberland:\nTurtles are dreaming of cool, muddy beds.\nTurtles are dreaming of learning to run.\nTurtles are dreaming of basking with you\non a rock in a river in hot summer sun.\nKids will love all the charming details and activities, while observing the animals in their natural habitats. Best part is discovering that all their animal friends are ultimately dreaming about them!\nAmy’s use of second person engages the reader from the very beginning, establishing an intimate tone that feels safe and comforting. Who would not love playing hide-and-go-seek with chipmunks or knowing that “Horses are dreaming of you telling secrets/into long ears as you nuzzle their faces”? So wonderful it makes you want to hug yourself. 🙂 Some of the verses are just sigh-worthy beautiful:\nRobins are dreaming of fine, twiggy nests.\nRobins are dreaming of waking up spring.\nRobins are dreaming of swooping with you,\npainting the sky with a song and a wing.\nAaron DeWitt’s mixed media illustrations take the reader full circle from starlit tucking-into-bed time through a happy sunrise the next morning. We first see a child’s footie pajama legs and a stuffed monkey being dragged to bed, where a pet dog and cat are blissfully snoozing. Through the window, we see a robin on her nest.\nWhen we turn the page, we’re outside, looking into the lit window from the animals’ vantage point. Kittens are up first in an old dairy barn, followed by various animal dreams in cool, muddy beds, deep burrows, underwater, on wide open spaces, in squishy bogs, thickets, and summer meadows, before the story takes us back home, with puppies “dreaming of long, waggy walks” and “listening to stories, cuddled up close in a quilt on your bed.”\nDeWitt skillfully captures the gentle playful spirit of the text, with no shortage of adorable animal antics to keep young ones enchanted and entertained. The level of detail is well suited to the target audience and the muted colors maintain a restful tone. I especially love the dozing bunny holding a carrot.\nBunnies are dreaming of napping in thickets.\nBunnies are dreaming of twitchety noses.\nBunnies are dreaming of you in a garden,\nsharing sweet carrots and nibbles of roses.\nDreaming of You is simply lovely, a welcome and refreshing take on the classic goodnight book. With its lilting cadences and deft rhymes, it’s a pleasure to read aloud, a loving gift of words for a child to hear right before he/she drifts off to sleep. Sweet Dreams!\nDREAMING OF YOU\nwritten by Amy Ludwig VanDerwater\nillustrated by Aaron DeWitt\npublished by Boyds Mills Press, March 2018\nPicture Book for ages 3-6, 32 pp.\n**Starred Review** from Kirkus\n♥️ Enjoy the official book trailer:\n📒 SPECIAL BOOK GIVEAWAY! 🐰\nThe publisher is generously donating a brand new copy of Dreaming of You for one lucky Alphabet Soup reader. For a chance to win, please leave a comment at this post no later than midnight (EDT) Wednesday, September 19, 2018. You may also enter by sending an email with DREAMING in the subject line to: readermail (at) jamakimrattigan (dot) com. Giveaway open to U.S. residents only, please. Good Luck!\nAnd guess who’s hosting the Roundup this week? None other than the beautiful and talented Amy Ludwig VanDerwater herself! Be sure to zip on over to The Poem Farm to check out the full menu of poetic goodness being served up in the blogosphere. Have a lovely weekend!\n*Interior spreads text copyright © 2018 Amy Ludwig VanDerwater, illustrations © 2018 Aaron DeWitt, published by Boyds Mills Press. All rights reserved.\n**This post contains Amazon Affiliate links. When you purchase something using a link on this site, Jama’s Alphabet Soup receives a small referral fee. Thank you for your support!\nCopyright © 2018 Jama Rattigan of Jama’s Alphabet Soup. All rights reserved."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:08c5c514-7b77-4e9c-8810-d02238c0750c>","<urn:uuid:490daca6-7f34-4679-ae05-d86e17354301>"],"error":null}
{"question":"How can organizations reduce manual interventions in their development pipeline? Looking for specific ways to automate and streamline processes.","answer":"Organizations can reduce manual interventions by implementing GitLab CI/CD to enable automation of manual tasks and steps. Manual work in testing, packaging, and configuring applications is error-prone, time-consuming, and doesn't scale well. Additionally, automated testing can be implemented including code quality and security testing (SAST, DAST). However, it's important to note that before automating, teams should improve their existing processes - automating an inefficient process will just produce bad results faster.","context":["If the goal is to deliver working features faster (aka reduce cycle time), then bottlenecks are things to be fixed to go faster.\nIn the Phoenix Project, they discover 'three ways' of DevOps, which are key to understanding the transformation:\nWhen you distill this down, the point is to find and remove the bottlenecks that inhibit the flow of value to customers (from Dev to Ops).\nFrom Mike Kravis on Forbes, where he writes about the key bottlenecks he sees in large organizations.\n|Bottleneck||Notes / How GitLab Helps|\n|1. Inconsistent environments Where dev, test and production environments are configured differently, causing errors, rework, and wasted effort troubleshooting.||GitLab CI/CD, Review Apps, docker support, and kubernetes integration helps to make each environment consistent.|\n|2. Manual intervention All the manual work that people do from testing to packaging to configuring applications takes time, is error prone and slows down delivery. Manual steps are simply not repeatable or scalable.||GitLab CI/CD enables automation to reduce manual tasks and steps|\n|3. SDLC maturity Many organizations still have waterfall processes or 'water-scrum-fall' agile processes, which limits their ability to accelerate delivery. **||GitLab supports and enables adoption of Agile practices of scrum, kanban, and SAFe|\n|4. Legacy change management process In many organizations, there are manual gates and reviews of changes to get approval like weekly \"change advisory boards\" based on ITIL practices tend to slow down and gate changes.||Organizations will need to understand how to evolve their change management processes. GitLab enables auditing, traceability, and approvals of changes. Also, Canary and Incremental deploys dramatically reduces the risk of deploying change.|\n|5. Lack of operational maturity In many situations, developers have limited access to application performance monitoring tools, so they lack insight into how their application is really working.||GitLab includes application monitoring (Prometheus to give developers feedback)|\n|6. Outdated testing practices Manual testing simply cannot scale and keep up with delivery at agile and devops velocity. The silos between dev and QA teams is also a common source of friction and bottlenecks.||GitLab CI enables automated testing including code quality, security testing (SAST, DAST, etc)|\n|7. Automating waste If team automates the existing process, they are probably automating an inefficient process. A best practice is to try to improve the process before automating it.||This is a cultural and organizational issue - if a person automates a wasteful process, then they are simply getting bad results faster. GitLab Auto Devops can help, by giving teams a working CI/CD pipeline, but ultimately the team must own their automation|\n|8. Competing or misaligned incentives and lack of shared ownership Often, silos have their own goals, objectives, and KPIs, if not well designed, often this creates situations where teams don't share the same common objectives.||Leadership must understand the impact of the goals and incentives they place on their teams. Are they encouraging an end to end (systems thinking) perspective or are they rewarding silos?|\n|9. Dependence on heroic efforts In the Phoenix Project, 'Brent' was the hero who was vital for every project. He was actually a bottleneck that slowed teams down because they depended on him rather than optimizing their processes.||This is a culture challenge, where firefighters get the accolades. Leaders need to evolve the culture to encourage fire prevention (which is boring).|\n|10. Governance as an afterthought Often, transformations do not account for how other processes and procedures need to change, such as portfolio planning, resource allocation, and approvals of changes.||Inherently a cultural challenge, but GitLab also helps with Code Owners, Merge Request Approvals, and traceability from Epics to Issue to Merge Request to application performance in production.|\n|11. Limited to no executive sponsorship If leaders view the agile and devops transformation as non-strategic, then they won't make the transformation a priority and ensure success.||Ultimately, long term success in a DevOps transformation is dependent on a cultural, process and tool transformation. Without executive sponsorship and support, transformations are at risk of stalling or being overtaken by initiatives that have executive sponsorship.|"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:515559a5-1ee8-4c27-bafa-bd5cf3b83d84>"],"error":null}
{"question":"Hi! Could you please explain how to use the MP3 encoder in Liquidsoap? I'm interested in both constant and variable bitrate encoding.","answer":"There are 3 types of MP3 encoding in Liquidsoap: CBR (Constant bitrate), VBR (Variable bitrate), and ABR (Average bitrate). For CBR, use %mp3.cbr with a fixed bitrate parameter. For VBR, use %mp3.vbr with a quality parameter ranging from 0 (highest) to 9 (worst). For ABR, use %mp3.abr with average, minimum, and maximum bitrate settings. All three types share common parameters like mono/stereo setting, samplerate (default 44100), internal_quality (0-9), and optional id3v2 tags support.","context":["Encoders are used to define formats into which raw sources should be encoded by an output. Syntax for encoder is:\n%encoder(parameters...) or, if you use default parameters,\nFormats determine the stream content\nIn most liquidsoap scripts, the encoding format determines what kind of data is streamed.\nThe type of an encoding format depends on its parameter. For example,\n%mp3 has type\n%mp3(mono) has type\nThe type of an output like\noutput.file is something like\n(...,format('a),...,source('a))->source('a). This means that your source will have to have the same type as your format.\nFor example if you write\nthen the playlist source will have to stream stereo audio. Thus it will reject mono and video files.\nLiquidsoap provides operators that can be used to convert sources into a format acceptable for a given encoder. For instance, the\nmean operator transforms any audio source into a mono source and the\naudio_to_stereo operator transforms any audio source into a stereo source.\nFormat variables (or lack of, rather..)\nYou can store an atomic format in a variable, it is a value like another:\nfmt = %mp3. However, an atomic format is an atomic constant despite its appearance. You cannot use a variable for one of its parameters: for example\nx = 44100 %vorbis(samplerate=x)\nis not allowed, you must write\nIf you really need to use variables in encoder, for instance if bitrate is given by a user’s configuration, you may alleviate that by generating a pre-defined list of possible encoders and include it on top of your script using the\nList of formats and their syntax\nAll parameters are optional, and the parenthesis are not needed when no parameter is passed. In the following default values are shown. As a special case, the keywords\nstereo can be used to indicate the number of channels (whether is is passed as an integer or a boolean).\nMp3 encoder comes in 3 flavors:\n%mp3.cbr: Constant bitrate encoding\n%mp3.vbr: Variable bitrate, quality-based encoding.\n%mp3.abr: Average bitrate based encoding.\nParameters common to each flavor are:\nmono=true/false: Encode stereo or mono data (default:\nstereo_mode: One of:\nsamplerate=44100: Encoded data samplerate (default:\ninternal_quality=2: Lame algorithms internal quality. A value between\n0being highest quality and\n9the worst (default:\nid3v2=true: Add an\nid3v2tag to encoded data (default:\nfalse). This option is only valid if liquidsoap has been compiled with taglib support.\nbitrate: Encoded data fixed bitrate\nquality: Quality of encoded data; ranges from\n0(highest quality) to\nbitrate: Average bitrate\nmin_bitrate: Minimum bitrate\nmax_bitrate: Maximum bitrate\nhard_min: Enforce minimal bitrate\n128kbps bitrate encoding:\n- Variable bitrate with quality\n6and samplerate of\n- Average bitrate with mean of\n128kbps, maximum bitrate\nOptionally, liquidsoap can insert a message within mp3 data. You can set its value using the\nmsg parameter. Setting it to\n\"\" disables this feature. This is its default value.\nShine is the fixed-point mp3 encoder. It is useful on architectures without a FPU, such as ARM. It is named\n%mp3.fxp and its parameters are:\n%wav(stereo=true, channels=2, samplesize=16, header=true, duration=10.)\nfalse, the encoder outputs raw PCM.\nduration is optional and is used to set the WAV length header.\nBecause Liquidsoap encodes a possibly infinite stream, there is no way to know in advance the duration of encoded data. Since WAV header has to be written first, by default its length is set to the maximum possible value. If you know the expected duration of the encoded data and you actually care about the WAV length header then you should use this parameter.\n%ffmpeg encoder is the latest addition to our collection. You need to have ffmpeg-av, ffmpeg-avfilter, ffmpeg-swscale and ffmpeg-swresample installed and up-to date to enable the encoder during liquidsoap’s build.\nThe encoder should support all the options for\nffmpeg’s muxers and encoders, including private configuration options. Configuration value are passed as key/values, with values being of types:\nfloat. If an option is not recognized (or: unused), it will raise an error during the instantiation of the encoder. Here are some configuration examples:\n- AAC encoding at\n%ffmpeg(format=\"mpegts\", %audio(codec=\"libfdk_aac\",samplerate=22050,b=\"32k\", afterburner=1,profile=\"aac_he_v2\"))\n- Mp3 encoding using\n- AC3 audio and H264 video encapsulated in a MPEG-TS stream\n%ffmpeg(format=\"mpegts\", %audio(codec=\"ac3\",channel_coupling=0), %video(codec=\"libx264\",b=\"2600k\", \"x264-params\"=\"scenecut=0:open_gop=0:min-keyint=150:keyint=150\", preset=\"ultrafast\"))\n- AC3 audio and H264 video encapsulated in a MPEG-TS stream using ffmpeg raw frames\n%ffmpeg(format=\"mpegts\", %audio.raw(codec=\"ac3\",channel_coupling=0), %video.raw(codec=\"libx264\",b=\"2600k\", \"x264-params\"=\"scenecut=0:open_gop=0:min-keyint=150:keyint=150\", preset=\"ultrafast\"))\n- Mp3 encoding using\nlibmp3lameand video copy\n%ffmpeg(format=\"mp3\", %audio(codec=\"libmp3lame\"), %video.copy)\nThe full syntax is as follows:\n%ffmpeg(format=<format>, # Audio section %audio(codec=<codec>,<option_name>=<option_value>,..), # Or: %audio.raw(codec=<codec>,<option_name>=<option_value>,..), # Or: %audio.copy, # Video section %video(codec=<codec>,<option_name>=<option_value>,..), # Or: %video.raw(codec=<codec>,<option_name>=<option_value>,..), # Or: %video.copy, # Generic options <option_name>=<option_value>,..)\n<format>is either a string value (e.g.\n\"mpegts\"), as returned by the\nffmpeg -formatscommand or\nnone. When set to\nnoneor simply no specified, the encoder will try to auto-detect it.\n<codec>is either a string value (e.g.\n\"libmp3lame\"), as returned by the\nffmpeg -codecscommand or\nnone. When set to\nnone, for audio,\nchannelsis set to\n0and, for either audio or video, the stream is assumed to have no such content.\n<option_name>can be any syntactically valid variable name or string. Strings are typically used when the option name is of the form:\n%audio(..)is for options specific to the audio codec. Unused options will raise an exception. Any option supported by\nffmpegcan be passed here. Streams encoded using\n%audioare using liquidsoap internal frame format and are fully handled on the liquidsoap side.\n%audioexcept that the audio data is kept as ffmpeg’s internal format. This can avoid data copy and is also the format required to use ffmpeg filters..\n%audio.copycopies data without decoding or encoding it. This is great to avoid using the CPU but, in this case, the data cannot be processed through operators that modify it such as\nsmart_cross. Also, all stream must agree on the same data format.\n%video(..)is for options specific to the video codec. Unused options will raise an exception. Any option supported by\nffmpegcan be passed here.\n%video.copyhave the same meaning as their\nGeneric options are passed to audio, video and format (container) setup. Unused options will raise an exception. Any option supported by\nffmpegcan be passed here.\n%ffmpeg encoder is the prime encoder for HLS output as it is the only one of our collection of encoder which can produce Mpeg-ts muxed data, which is required by most HLS clients.\nSome encoding formats, for instance\nmp4 require to rewing their stream and write a header after the fact, when encoding of the current track has finished. For historical reasons, such formats cannot be used with\noutput.file. To remedy that, we have introduced the\noutput.url operator. When using this operator, the encoder is fully in charge of the output file and can thus write headers after the fact. The\n%ffmpeg encoder is one such encoder that can be used with this operator.\nThe following formats can be put together in an Ogg container. The syntax for doing so is\n%ogg(x,y,z) but it is also possible to just write\n%vorbis(...), for example, instead of\nAll ogg encoders have a\nbytes_per_page parameter, which can be used to try to limit ogg logical pages size. For instance:\n# Try to limit vorbis pages size to 1024 bytes %vorbis(bytes_per_page=1024)\n# Variable bitrate %vorbis(samplerate=44100, channels=2, quality=0.3) % Average bitrate %vorbis.abr(samplerate=44100, channels=2, bitrate=128, max_bitrate=192, min_bitrate=64) # Constant bitrate %vorbis.cbr(samplerate=44100, channels=2, bitrate=128)\nQuality ranges from -0.2 to 1, but quality -0.2 is only available with the aotuv implementation of libvorbis.\nOpus is a lossy audio compression made especially suitable for interactive real-time applications over the Internet. Liquidsoap supports Opus data encapsulated into Ogg streams.\nThe encoder is named\n%opus and its parameters are as follows. Please refer to the Opus documentation for information about their meanings and values.\nvbr: one of\napplication: One of\ncomplexity: Integer value between\nmax_bandwidth: One of\nsamplerate: input samplerate. Must be one of:\nframe_size: encoding frame size, in milliseconds. Must be one of:\nbitrate: encoding bitrate, in\nkbps. Must be a value between\n512. You can also set it to\nchannels: currently, only\n2channels are allowed.\nstereo: equivalent to\nsignal: one of\n%theora(quality=40,width=640,height=480, picture_width=255,picture_height=255, picture_x=0, picture_y=0, aspect_numerator=1, aspect_denominator=1, keyframe_frequency=64, vp3_compatible=false, soft_target=false, buffer_delay=5, speed=0)\nYou can also pass\nbitrate=x explicitly instead of a quality. The default dimensions are liquidsoap’s default, from the settings\n%speex(stereo=false, samplerate=44100, quality=7, mode=wideband, # One of: wideband|narrowband|ultra-wideband frames_per_packet=1, complexity=5)\nYou can also control quality using\nThe flac encoding format comes in two flavors:\n%flacis the native flac format, useful for file output but not for streaming purpose\n%ogg(%flac,...)is the ogg/flac format, which can be used to broadcast data with icecast\nThe parameters are:\n%flac(samplerate=44100, channels=2, compression=5, bits_per_sample=16)\ncompression ranges from 0 to 8 and\nbits_per_sample should be one of:\n32. Please note that\n32 bits per sample is currently not supported by the underlying\nThis encoder can do both AAC and AAC+.\nIts syntax is:\n%fdkaac(channels=2, samplerate=44100, bandwidth=\"auto\", bitrate=64, afterburner=false, aot=\"mpeg2_he_aac_v2\", transmux=\"adts\", sbr_mode=false)\naot is one of:\nbandwidth is one of:\n\"auto\", any supported integer value.\ntransmux is one of:\nBitrate can be either constant by passing:\nbitrate=64 or variable:\nYou can consult the Hydrogenaudio knowledge base for more details on configuration values and meanings.\n%gstreamer encoder can be used to encode streams using the\ngstreamer multimedia framework. This encoder extends liquidsoap with all available GStreamer formats which includes most, if not all, formats available to your operating system.\nThe encoder’s parameters are as follows:\n%gstreamer(channels=2, audio=\"lamemp3enc\", has_video=true, video=\"x264enc\", muxer=\"mpegtsmux\", metadata=\"metadata\", log=5, pipeline=\"\")\nPlease refer to the Gstreamer encoder page for a detailed explanation of this encoder.\nFor a detailed presentation of external encoders, see this page.\n%external(channels=2,samplerate=44100,header=true, restart_on_crash=false, restart_on_metadata,restart_after_delay=30, process=\"progname\")\nOnly one of\nrestart_after_delay should be passed. The delay is specified in seconds. The encoding process is mandatory, and can also be passed directly as a string, without"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:3da24fcb-1eaf-4f8b-b65e-b1e62eb2b69c>"],"error":null}
{"question":"在探索文化研究中，Henry Morton Stanley 和 King Arthur 这两个历史人物如何被不同作品描绘？Can you compare their different portrayals in literature?","answer":"These historical figures are portrayed quite differently in literature. Henry Morton Stanley is presented in a more critical light, particularly in relation to 'exploration by warfare' as discussed in academic works. His portrayal is part of a broader examination of geographical exploration and empire. In contrast, King Arthur has been portrayed through multiple literary interpretations - from traditional views as an exemplar of medieval life and chivalric customs, to historical interpretations like in Rosemary Sutcliff's work showing him as 'an all-too-human' leader fighting Saxon armies, to fantastical portrayals involving witchcraft and magic. His character has been adapted across various genres, from T.H. White's humorous young adult works to Bernard Cornwell's darker, realistic war narratives.","context":["Geography Militant: Cultures of Exploration and Empire\nOctober 2000, Wiley-Blackwell\n1. Geographical Knowledge, Exploration and Empire.\n2. The Royal Geographical Society and the Empire of Science.\n3. Hints to Travellers: Observation in the Field.\n4. Missionary of Science: David Livingstone and the Exploration of Africa.\n5. Becoming an Explorer: The Martyrdom of Winwood Reade.\n6. Exploration by Warfare: Henry Morton Stanley and his Critics.\n7. Making Representations: From an African Exhibition to the High Court of Justice.\n8. Exploring Darkest England: Mapping the Heart of Empire.\n9. Geography Militant and its After-life.\n- New perspective on the history of geographical exploration\n(concerned with the relationships between culture, science and\n- Brings together a wealth of unpublished and published material\nin an engaging, well-illustrated and accessible book.\n- Interdisciplinary volume, that will appeal to historians,\ngeographers, and those in cultural studies, as well as a lay\n- Includes he latest scholarship on well-known explorers like\nStanley & Livingstone.\n- Author is well known in geography, imperial history, Victorian studies, cultural studies.\n\"Expoliting the divide 'twixt' science and the\nsensational and pointing to differing geographies of various\nperiods , this well wrought, closely knit book of nine illustrated\nchapters dwells on the age of exploration, colonization and the\nconcomitant rise of the British Empire and its institutions. A\nlisting of manuscripts consulted, extensive bibliography, and an\nindex complete this rigorous work.\" Choice\n\"...consistently thoughtful and lively; Felix Driver\nproduces a powerful sense of the complexity and strangeness of his\nmaterial.\" Times Literary Supplement.\n\"extremely wide ranging book which raises a multitude of\nissues\", Journal of European Studies.\n\" This book adds effectively to the traditional accounts of\nexploration known to so many of us\" International Journal of\n\"a lot of material, many interesting ideas and observations,\nsome fascinating juxtapositions, tantalizing suggestions, rich\nreferences, and polished prose ...\" Environment and\nA \"wonderful book [...] with Geography Militant Felix\nDriver has dined sumptuously at the Ritz-Carlton. To great\nadvantage, he has quite successfully mined many veins of knowledge\nfar bayond those disciplines where geographers normally toil. Each\nplace is revealed as pertinent and fascinating [...] This volume\ncontains so many meaty ideas, it is difficult [...] to give them\nthe attention they properly deserve. Suffice to say, Felix Driver's\nMilitant Geography is a tour de force. The research\nconducted to write this remarkable book is impeccable\"\nTerrae Incognitae, the journal of the Society for the History of Discoveries\n\"The range of material included in this book, only a portion of\nwhich can be covered here, is exceptional. Geography\nMilitant is a welcome contribution and will certainly spark a\nreconsideration of assumptions in a number of fields, including the\nhistory of science, cultural history and the history of\nimperialism.\" Susan Schulten, the History of Science\n\"this splendid book describes the culture of exploration and the\nmaking of he discipline of Britain in the 'militant' epoch. So many\nthemes and substantive descriptions tumble from these pages that\nsummary is difficult\" Christopher Lawrence, Medical\n[Driver contributes] to the ongoing project of reevaluating the history of Empire, demonstrating that the science of location and its graphic productions were far less stable and effective than postcolonial critics have claimed\" Robert D. Aguirre, Victorian Studies","Scott Farrell comments:\nI’m often asked for recommendations of books about chivalry and the epic adventures of King Arthur and his knights. While I enjoy talking about my favorite King Arthur books, when I saw that acclaimed librarian Nancy Pearl provided just such a list in her bibliophile’s compendium, Book Lust,» I knew I had to share her recommendations with Chivalry Today readers. (After all, when a librarian has her own action figure – complete with “amazing push-button shushing action,” — she definitely commands respect!) It’s the perfect summer reading list for chivalry fans.\nNote: See all the titles mentioned in this article at the Chivalry Today Bookshop»\nSummer Reading for the Round Table\nEver since Thomas Malory wrote his 14th century epic Le Morte d’Arthur,» the legend of King Arthur has been a powerful draw for readers of all ages, so it’s no surprise that writers, too, have found it to be a rich lode of inspiration and subject matter. Authors have taken a wide variety of approaches to the legend, from the traditional view of Arthur and his (Knights) of the Round Table as exemplars of medieval life and chivalric customs, to interpretations of the historical Arthur, to fantastical novels of witchcraft and white and black magic. In other words, there’s an Arthur for every age and taste.\nOnce you read Rosemary Sutcliff’s romantic and well-researched Sword at Sunset» (one of my very favorite novels), in which an all-too-human Arthur leads his fellow Britons in a fight to the death against the invading Saxon armies, knowing full well that a loss will mean the coming of the dark and the end of civilization, you’ll never be able to picture Arthur in any other way.\nT.H. White’s quartet of Arthurian novels, collective entitled The Once and Future King,» inspired the Broadway musical Camelot. The Sword in the Stone — aimed at young readers and filled with sly humor — opens the series. It introduces the young orphan Wart, who innocently pulls the famous sword Excalibur from the stone in a churchyard and becomes the High King. In The Queen of Air and Darkness, The Ill-Made Knight, and The Candle In The Wind, the tone grows darker, as White depicts a world in which even the most chivalrous knights and powerful wizards are unable to change their fates.\nThe Arthur legend is also the basis for other historical series, such as Jack Whyte’s Camulod Chronicles, including The Skystone» ; The Fort at River’s Bend» ; Uther» ; and others; Sharan Newman’s Guinevere» ; The Chessboard Quee» ; and others; and Rosalind Mile’s Guenevere: Queen of the Summer Country» ; and Bernard Conrwell’s darkly realistic series of men at war during the Dark Ages, The Warlord Chronicles, including The Winter King» ; Enemy of God» ; and Excalibur» .\nThe Mists of Avalon» by Marion Zimmer Bradley is one of the most enduringly popular novels about King Arthur. Bradley retells the legend from the viewpoint of the major female characters: Arthur’s mother Igraine and his half-sister Morgaine, his wife Gwenhwyfer, and the Lady of the Lake, Vivian. The central conflict here is religious — between the matriarchal Druidic beliefs and the more patriarchal, newly influential Christianity.\nAlthough Stephen R. Lawhead’s Pendragon Cycle was set in Camelot during the Middle Ages, his Avalon: The Return of King Arthur» posits a rebirth and return of Arthur in the modern world.\n©2007 Nancy Pearl\nSee all the titles mentioned in this article at the Chivalry Today Bookshop»\nAbout the Author: The New York Times calls Nancy Pearl “the talk of librarian circles.” Readers can’t get enough of her recommendations while bookstores and libraries offer standing room only whenever she visits. Since the release of the best-selling Book Lust,» in 2003 and the Librarian Action Figure modeled in her likeness, Nancy Pearl has become a rock star among readers and the tastemaker people turn to when deciding what to read next. In 2004, Pearl became the 50th winner of the Women’s National Book Association Award for her extraordinary contribution to the world of books."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:77b71839-a38d-44c1-b979-9b6ba003f899>","<urn:uuid:3ba05f93-beaf-4cd9-820b-3bd019d40a09>"],"error":null}
{"question":"How does Buena Vista's magnetite ore processing compare to typical beneficiation techniques for low-grade iron ores in terms of efficiency?","answer":"Buena Vista's magnetite ore is significantly more efficient to process compared to typical iron ores. Due to its intrusive origin, the magnetite at Buena Vista is coarser grained and hosted in softer rock than typical banded iron (BIF) deposits, making it easier to liberate during beneficiation. The ore easily upgrades to +45% Total Fe before grinding, resulting in lower plant time and energy consumption. In contrast, lower grade iron ores generally require more intensive beneficiation techniques including crushing, milling, gravity separation, heavy media separation, screening, and silica froth flotation to improve concentration and remove impurities.","context":["Buena Vista Magnetite Project\nBuena Vista is an advanced magnetite iron ore project. In excess of A$34 million has been expended on the Project over the past decade completing feasibility studies and permitting for the long term production of a +67.5 % Fe magnetite concentrate with no deleterious impurities.\nAll major development permits have already been secured.\nRequired technical work such as drilling, metallurgy, hydrogeology and plant design have already been completed.\nBuena Vista also provides a very favourable ore characteristic given its intrusive origin.\nIn this regard, the magnetite at Buena Vista is much coarser grained and the host rock softer, than typical banded iron (BIF) hosted magnetite deposits and consequently the magnetite is much more easily liberated during the beneficiation process.\nAs an example, the extensive metallurgical test work across run of mine material has demonstrated that Buena Vista ore will easily upgrade to +45% Total Fe before grinding meaning significant lower plant time and energy consumption as the ore is beneficiated.\nOn a comparative basis this provides significant capex and opex benefits compared to typical BIF hosted magnetite deposits.\nLOCATION AND HISTORY\nBuena Vista is located approximately 160km east-north-east of Reno in the mining friendly state of Nevada, United States.\nThe project was discovered in the 1890’s, and in the late 1950’s to early 1960’s around 900,000 tonnes of direct shipping magnetite ore with an estimated grade of 58% Fe was mined.\nIn the 1960’s US Steel Corporation acquired the project and carried out an extensive exploration program including 230 diamond drill holes and considerable metallurgical test work.\nThe project was refreshed in 2009 when Richmond Mining Limited, an ASX listed company acquired the project and commenced a detailed exploration program culminating in a definitive feasibility study in July 2011 and an updated study in 2013 for an expanded production rate.\nA key component of these studies was extensive investigation of the optimal logistics plan for development of Buena Vista. This included the negotiation of in-principle agreements with existing rail and port operators and the securing of all major mining permits.\nIn addition, detailed costings were completed on the trucking or slurry pipeline options to deliver the concentrate to the rail head located some 50 kilometres from mine site.\nThe Buena Vista mine site is ideally located with towns Fallon (20,000 population) and Lovelock (8,000 population) within close proximity to the mine site. This provides site personnel and their families the opportunity to reside in local communities with existing infrastructure and facilities.\nThe mine site is around 50kms from the Union Pacific rail line which connects with multiple export port options including Stockton, West Sacramento, Oakland, San Francisco and Richmond (Levin).\nGrid power is available within 40km of the deposits and sufficient water can be sourced from ground water aquifers located in the North Carson sink. The Nevada Department of Conservation and Natural Resources has already granted the required water rights for the life of the mine.\nThe mine is located in Churchill County in the State of Nevada which has a strong history of supporting mining developments and is easily accessed via the unsealed Pole Line road from Huxley or the sealed Coal Canyon road from Lovelock.","2018 8 22ensp;0183;ensp;The Lower grade sources of iron ore generally require beneficiation, using techniques like crushing, milling, gravity or heavy media separation, screening, and silica froth flotation to improve the concentration of the ore and remove impurities. The results, high quality fine ore powders, are known as fines.. Magnetite. Magnetite is magnetic,\n2015 6 30ensp;0183;ensp;The modular design and low capital cost of the plant offers opportunities for the exploitation of smaller high quality iron ore deposits. Its design also enables location flexibility ensuring the best point of the supply chain is accessed.\nFeldspar stone processing plant . Feldspar is the most common ore in the earth crust and it even appears on the moon and in the aerolite.\nIron Ore Mineralogy, Processing and Environmental Issues summarizes recent, key research on the characterization of iron ores, including important topics such as beneficiation (separation and refining), agglomeration (e.g., production of pellets or powders), blast furnace technology for smelting, and environmental issues relating to its\nThe journeys of iron ore. Whether its a long awaited trip to see family and friends, or the daily commute to work, every train journey begins with iron ore.\n2018 8 26ensp;0183;ensp;Cleveland Cliffs, Inc., formerly Cliffs Natural Resources, is a Cleveland, Ohio, business firm that specializes in the mining and beneficiation of iron ore.The firm is an independent company whose shares are traded on the New York Stock Exchange.Cleveland Cliffs' primary operations are organized and managed according to product category and geographic location U.S. iron ore\nWelcome to Mine Restoration Investments. The MRI Group is an environmental services company focusing on the abatement of environmental impacts of the mining industry, while delivering sustainable returns to its shareholders.\nUsha Martin is one of the largest manufacturers of wire ropes and today it is the largest specialty steel plant in India in long product segment. The companys vision is to be the global leader of the wire rope industry and the leading specialty steel manufacturer in India, by delivering customer delight, adopting modern technology and ensuring sustainable, inclusive growth for all of its\nhow to extract copper from copper ore process flow chart. how to extract copper from copper ore process flow chart. Copper mining The main stag, The flow chart shows you how copper is extracted from its ore and converted\nThis is when solid particles stick to one another, and while this is an undesirable feature in many powder and particle processing operations, it is essential for the successful sintering of iron ore fines, coke and fluxes into a suitable blast furnace feed.\nProcesses for Beneficiation of Iron Ores Iron is an abundant element in the earths crust averaging from 2 % to 3 % in sedimentary rocks to 8.5 % in basalt and gabbro.\nEssel Mining amp; Industries Limited (EMIL), established in 1950, is amongst the largest iron ore mining companies in the non captive private sector and the largest producer of Noble Ferro Alloys in India.\nAt Anglo American, we have large, high quality iron ore resources in South Africa and Brazil. In manganese we have a 40% share in both Samancor Holdings and Tasmanian Electro Metallurgical Company (TEMCO) in South Africa and Australia respectively.\nWe have a 73.9% interest in Sishen Iron Ore Company Proprietary Limited (SIOC), an entity which we manage. SIOC, in turn, owns the operating assets of the company.\neffect of titanium in iron ore asbl cab. EFfect of TiO2 content on Reduction of Iron Ore Agglomerates. The effect of titanium oxide on iron ore agglomerates is studied by the use of\n2011 4 10ensp;0183;ensp;In a previous article, I discussed the reasons behind the emerging iron ore bull market and the stocks that one could invest in to benefit from this macro trend\nFerro Chrome. Ferrochrome (FeCr) is an alloy of chromium and iron containing between 50% and 70% chromium. The Producer of stainless steel amp; tool steel are the largest consumer of ferrochrome and charge chrome.It is chromium that confers upon stainless steel its remarkable corrosion resistance.\nAustralias total production of iron ore for 2012 was 520 Mt with WA producing 505 Mt, or 97% of overall production. South Australia had a slight increase in its iron ore production from approximately 10 Mt in 2011 to 10.7 Mt in 2012, representing 2% of Australias total iron ore production.\nBeneficiation of Iron Ores. Iron ore is a mineral which is used after extraction and processing for the production of iron and steel. The main ores of iron usually contain Fe2O3 (70 % iron, hematite) or Fe3O4 (72 % iron. magnetite).\nthen reacts with the iron ore to form carbon dioxide and pure iron. Separating the iron from the slag 2 The melted iron sinks to the bottom of the furnace.\nNickel Ore Mining Process By Jone Doe 19 March 2017 . Nickel Ore can be divided into copper sulfide nickel ore and nickel oxide ore, flotation is the main beneficiation method when processing\nLONDON MINING DESTROYING SIERRA LEONES ECONOMY Open Letter to H.E President Dr. Ernest Bai Koroma . Dear Sir, LONDON MINING DESTROYING SIERRA LEONES ECONOMY\nSlurry Pumps from Multotec Multotec has expanded its product range to include slurry pumps and process water treatment solutions.\nIron Ore Crusher Price,Iron Ore Crusher Supplier In India. India is extraordinarily rich each in quantity and high quality of its iron ore Refined grain kind, high crushing zinc, tin ore, light weight aluminum ore Crushing equipment\nPhone : 0086 13953560679\nEmail : [email protected]\nWhatsApp : 0086 13953560679\n© 2017 Shandong Xinhai Mining Technology & Equipment Inc. sitemap"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:7a6ae4a0-132f-46db-83b8-d86974f34d00>","<urn:uuid:d7484211-1a3f-4891-a1d6-7da355475b2f>"],"error":null}
{"question":"Can you compare the age requirements for Representatives vs Senators according to Article 1? Looking for the specifics!","answer":"Representatives must be at least 25 years old and have been U.S. citizens for at least seven years, while Senators must be at least 30 years old and have been U.S. citizens for at least nine years. The higher age qualification for Senators was justified by Madison because the 'senatorial trust' required a 'greater extent of information and stability of character' than the House of Representatives.","context":["All the powers are vested in Congress (people’s representatives), which comprise the Senate and House of representatives. The bicameral structure of the Congress contains 435 members in the lower body and 100 members in the upper body, House of Representatives and Senate, respectively. The congressional districts or electoral districts are divisions of electing members of the House of Representatives. The population density in each state determines the number of electoral districts it is entitled to. Census in the U.S are decennial; hence the number of congressional districts is decided after every ten years in apportionment and delimitation processes. Congress is vested with powers by the Constitution to make legislation, declare war and override presidential vetoes.\nOn one hand, members of the House of Representatives are elected after every two years. The representation in the lower house is dependent on the population established in the United States Census. One representative is limited to substitute thirty thousand people. Each state, however, must have at least one congressional representative regardless of the population (Maggs, 2019). Article 1 requires a minimum age of 25 years, U.S citizenship for at least seven years, and residency to the state for qualification to be elected in the House of Representatives.\nOn the other hand, senators are elected for a six-year term with staggered elections. Every state has two senate representatives to achieve 100 senators for 50 states. A third of the states have to re-elect their senators every two years. Article 1 requires that members vying for senate meet a minimum age of 30 years, at least nine years of U.S citizenship, and inhabitancy to the respective state of the election (Gorgoshadze, 2018). Every two senators represent people of an individual state elected for six years.\nThe framers granted remarkable but restricted powers to Congress to make it a preeminent branch of government. The preamble lay down the Constitution’s foundation by highlighting the framers’ intention, such as achieving defense, justice, and ensuring domestic tranquility. The power of the Congress is enshrined in Article 1 of the Constitution with special executive powers. The preamble to the U.S constitution advocate for the theory of popular sovereignty and how government power flow from people. First, the preamble recognizes itself with the people of the U.S, signaling that power belongs to the people. At the start of the original draft of the preamble reads, “we the people,” indicating that most of the U.S people agreed upon the Constitution, and hence a majority of the people can amend it (Maggs, 2019). The entire people of the U.S are sovereign indicated in the preamble.\nThe framers also believe that our rights are naturally given by God or nature rather than by the government. Some of the rights are alienable, while others are not. By formation of the government made the U.S people surrender some of those rights to representatives. The people are sovereign, and the representatives are the people’s servants. Therefore, members of Congress and the president represent the will of the people they represent. According to Maggs (2019), the Constitution itself also didn’t declare for “we the people” when it was proposed until ratification by special conventions elected for that purpose. The people determine the supreme law, and people representatives are the people’s voice. The Enumerated and Implied powers entrenched to the congress grant exclusive capability to the Congress to perform its duties. Such authorities include overriding presidential vetoes and declaring war entrenched in Article 1, section 7, and section 8, respectively.\nThe president is granted with authority to veto legislation with powers to pass or block legislation by the Constitution. If the president fails to pass legislation into law, it is taken back to both houses for discussion. Congress is, however, granted powers to override the presidential veto if it achieves a two-thirds vote in both Houses. Since Congress represents the people and two-thirds in both houses represent the majority in the U.S, such a law should be passed to maintain the people’s sovereignty over the servants (president). Article 7 allows Congress to override a presidential veto if an act garners significant support (two-thirds votes) in both houses of representation, preventing the president from barring such an act.\nArticle 1, section 8, grants Congress authority to provide common defense by declaring war. Although the president is the commander in chief of the armed forces, Congress should decide on the declaration of war (Gorgoshadze, 2018). Domestic tranquility, defense, and declaring war are matters of sovereignty that belong to the people through representatives. Therefore, cooperation between the president and Congress is requisite in domestic tranquility and military affairs.\nThe U.S constitution gives the basis of the government on the idea that people have sovereign power and can alienate some of those powers to people (representatives) who decide on their behalf. The Congress is formed under the Constitution disintegrated into the House of Representatives and the Senate. Congress has 535 elected representatives in both houses acting on behalf of the general American population. The framers created the preamble that demonstrates the flow of power from the people to form the government. Article 1, section 7, and 8 allows Congress to override the president’s veto and declare war, respectively.\nGorgoshadze, M. (2018). The Essence and the Place of the Preamble in the Structure of the Normative-legal Act. Law Rev. Kyiv UL, 346.\nMaggs, G. E. (2019). A Guide and Index for Finding Evidence of the Original Meaning of the US Constitution in Early State Constitutions and Declarations of Rights. NCL Rev., 98, 779.","Table of contents:\n- Is Kroger a German name?\n- Is Bill a name?\n- What is Liam short for?\n- What is Billy short for female?\n- What is Bill stand for?\n- Is Bill American or British?\n- Will is short for?\n- Who can introduce a bill?\n- Can the president introduce a bill?\n- Can a citizen propose a bill?\n- Does Bill go to House or Senate first?\n- Can the president pass a law without congressional approval?\n- Who is the most senior US senator?\n- Who are the oldest senators currently serving?\n- What are three requirements to become a senator?\n- What degrees do senators have?\n- How many US Senators are up for election in 2022?\n- How much does a senator make a year?\n- How much does a congressman make a year?\n- Why are there higher qualifications for senators?\n- Why is the Senate term 6 years?\n- Do Both senators represent me?\nIs Kroger a German name?\nKröger or Kroeger is a German surname. Notable people with the surname include: Adolph Ernst Kroeger (1837–1882), American translator.\nIs Bill a name?\nBill is a masculine given name, generally a short form (hypocorism) of William. It can also be used as the adaptation into English of the popular Greek name Vasilis or Vasileios (Basil), especially amongst Greek immigrants in English-speaking countries, probably due to similarly in the sound.\nWhat is Liam short for?\nLiam is a short form of the Irish name \"Uilliam\" or the old Germanic name William.\nWhat is Billy short for female?\n▼ as a girls' name (also used less commonly as boys' name Billie) is pronounced BILL-ee. It is of Old English origin. Originally a nickname for William. Now a feminine name, a short form for Wilhelmina (Old German) \"will helmet, protection\".\nWhat is Bill stand for?\n|BILL||Bwrdd Iechyd Lleol (Welsh: Local Health Board)|\n|BILL||Beer Industry League of Louisiana|\nIs Bill American or British?\nBritish vs American Vocabulary\n|British English ↕||American English ↕|\n|bill (restaurant)||bill, check|\n|block of flats||apartment building|\nWill is short for?\nScottish and northern English : from the medieval personal name Will, a short form of William, or from some other medieval personal names with this first element, for example Wilbert or Willard.\nWho can introduce a bill?\nA bill can be introduced in either chamber of Congress by a senator or representative who sponsors it. Once a bill is introduced, it is assigned to a committee whose members will research, discuss, and make changes to the bill. The bill is then put before that chamber to be voted on.\nCan the president introduce a bill?\nAnyone can write it, but only members of Congress can introduce legislation. Some important bills are traditionally introduced at the request of the President, such as the annual federal budget. ... After being introduced, a bill is referred to the appropriate committee for review.\nCan a citizen propose a bill?\nAn idea for a bill may come from anybody, however only Members of Congress can introduce a bill in Congress. Bills can be introduced at any time the House is in session. There are four basic types of legislation: bills; joint resolutions; concurrent resolutions; and simple resolutions. A bill's type must be determined.\nDoes Bill go to House or Senate first?\nFirst, a representative sponsors a bill. The bill is then assigned to a committee for study. If released by the committee, the bill is put on a calendar to be voted on, debated or amended. If the bill passes by simple majority (218 of 435), the bill moves to the Senate.\nCan the president pass a law without congressional approval?\nThe president can issue rules, regulations, and instructions called executive orders, which have the binding force of law upon federal agencies but do not require approval of the United States Congress.\nWho is the most senior US senator?\nThe most senior senator, Patrick Leahy, did not reach the 40-year mark until Janu. From Novem, when Strom Thurmond reached the 40-year mark during the 104th Congress, until Daniel Inouye died on Decem, there was always at least one senator who had served for 40 years.\nWho are the oldest senators currently serving?\nOf those living, the longest-living senator is James L. Buckley. The oldest sitting senator is Dianne Feinstein (born 1933). The longest-lived senator in history is Cornelius Cole, who died at 102.\nWhat are three requirements to become a senator?\nThe Constitution sets three qualifications for service in the U.S. Senate: age (at least thirty years of age); U.S. citizenship (at least nine years); and residency in the state a senator represents at time of election.\nWhat degrees do senators have?\nThe Congressional Research Service notes that the vast majority of Members (95 percent) had an academic degree:\n- 168 Representatives and 57 Senators had a law degree. ...\n- 83 Representatives and 16 Senators earned a master's degree – often a Master of Business Administration (M.B.A.) – as their highest educational degree.\nHow many US Senators are up for election in 2022?\nThe 2022 United States Senate elections will be held on Novem, with 34 of the 100 seats in the Senate being contested in regular elections, the winners of which will serve six-year terms in the United States Congress from Janu, to Janu.\nHow much does a senator make a year?\nSenate Salaries (1789 to Present)\n|2017||$174,000 per annum|\n|2018||$174,000 per annum|\n|2019||$174,000 per annum|\n|2020||$174,000 per annum|\nHow much does a congressman make a year?\nSalaries of members of the United States Congress\n|Senators and House Representatives||$174,000|\n|Resident Commissioner from Puerto Rico||$174,000|\n|President pro tempore of the Senate||$193,400|\n|Majority leader and minority leader of the Senate||$193,400|\nWhy are there higher qualifications for senators?\n62, Madison justified the higher age requirement for senators. By its deliberative nature, the “senatorial trust,” called for a “greater extent of information and stability of character,”than would be needed in the more democratic House of Representatives.\nWhy is the Senate term 6 years?\nTo guarantee senators' independence from short-term political pressures, the framers designed a six-year Senate term, three times as long as that of popularly elected members of the House of Representatives. Madison reasoned that longer terms would provide stability.\nDo Both senators represent me?\nMembers of the U.S. House of Representatives each represent a portion of their state known as a Congressional District, which averages 700,000 people. Senators however, represent the entire state.\n- What is the most expensive item in Best Buy?\n- Is NF1 considered a disability?\n- What is the loss of muscle function in part of your body?\n- What is the first book ever written?\n- What is the verb form of decision?\n- Do I need a marriage license to get married in California?\n- What is Chula?\n- Do I need a lanyard for Universal?\n- Can snakes sense fear in humans?\n- What can I write in a sympathy card?\nYou will be interested\n- Do all blogs make money?\n- What are the most important aspect of human development?\n- Is width and height the same?\n- Does person centered therapy work?\n- Can I drink alcohol after fasting?\n- How can I calm my performance anxiety?\n- What happens when you read a lot?\n- Can someone see if you search them on Instagram?\n- Why am I jealous of my boyfriend's family?\n- Which side of the head is the brain located?"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:033fb69c-a8f9-41ca-b630-035da12bf65e>","<urn:uuid:3a884f55-e815-4e82-bda0-5ba6274b0c03>"],"error":null}
{"question":"What's the deal with that butterfly logo I keep seeing at grocery stores? 🤔 Is it actually important for making healthy food choices?","answer":"The Non-GMO Verified butterfly label doesn't tell you anything about the safety, quality, or nutritional content of food. It's simply a certification from the Non-GMO Project identifying products as Non-GMO. The label is primarily used as a marketing tactic that capitalizes on GMO fears, despite overwhelming research showing that GMOs pose no greater safety risk than conventional foods. In fact, over 2,000 studies and 20 years of GMO consumption have produced no evidence of health risks.","context":["I’m standing in aisle 5, scanning for my favourite pasta shape (rigatoni, for the record), when it catches my eye.\nWorld’s. Scariest. Butterfly.\nWhat. Is it doing here?\nNow I can’t help but notice it everywhere. In the produce section, in the frozen aisle, in every grocery aisle. And it’s multiplying…fast.\n*squinty angry eyes*\n…The Non-GMO Verified Butterfly\nIf you’ve stepped foot in any grocery store in the last couple of years, you’ve likely noticed the butterfly explosion too.\nIs it something you should look for?\nWill it help you make a meaningful decision about the food you’re buying?\nShould you be skeptical of food that isn’t butterfly-approved?\nShort answers…nope, no and nope.\nNow, let me give you the long-ish answer.\nWhat does the Non-GMO Verified Butterfly mean?\nThe label represents a certification from the Non-GMO Project which identifies food, personal care and cleaning products as being Non-GMO (This is not the same as GMO-free, which I’ll cover later). They claim to offer North America’s most trusted third-party verification. Today, companies have submitted upwards of 43000 products for verification.\nWho is the Non-GMO Project?\nThe Non-GMO project is self-described as a mission-driven nonprofit organization dedicated to building and protecting a non-GMO food supply.\nWhile they are a third party ‘independent’ verifier, it’s important to know who is behind it. The GMO Project’s board of directions (BOD) is made up entirely of CEOs, owners and high level employees of the organic or ‘natural’ food industry.\nThere is no one from a government agency or a scientific or educational institution. Not a single person with a science or health professional credential or background. There is a single farmer, a rancher, who manages a ‘natural’ beef business.\nThey say “When it comes to food labeling, third-party certifications are best because they ensure the claim is unbiased, rigorous and transparent”.\nI wouldn’t call a third party certifier with an entire BOD standing to directly benefit from use of the label unbiased or transparent. But let’s move on.\nHonestly, what IS a GMO?\nSpoiler: many people don’t know what the acronym stands for let alone what it actually means.\nTo clarify, GMO stands for genetically modified organism.\nThese days it seems to be a catch-all term for all that is evil and immoral when it comes to our food system. And while lots of people think they should avoid them, most aren’t sure why.\nWhile there is no agreed upon scientific definition, in agriculture it generally refers to a product that has been altered using genetic engineering. That means, the plant’s DNA, or genetic makeup has been altered in a specific way using technology rather than traditional plant breeding.\nGenetic engineering allows us to target a specific trait we know we want to change rather than relying on chance (luck!) that a beneficial trait might express itself as a result of mixing genes randomly, as with traditional breeding, over time.\nAll breeding methods, including simple cross-breeding, alter a plant’s DNA, so technically speaking all agricultural products are genetically modified. They have been since the dawn of agriculture, thousands of years ago. Genetic engineering is simply a more precise and expedient method.\nSome of the traits that have been developed using genetic engineering are drought tolerance, insect resistance and viral disease resistance. Things that really benefit food producers, and you in the end.\nGreat, what’s the big deal then?\nSome of the main concerns of those who oppose GMOs are the safety and long term health effects of consuming GMOs.\nThe thing is, we now have evidence from over 2,000 studies (and no, they are not all funded by industry) and over 20 years of GMO consumption by humans and animals which have produced no evidence that GMOs represent a health risk. As in, not one single case of an adverse health event…ever.\nBecause of that, every major mainstream health and regulatory body in the world agree that GMOs pose no greater safety risk than their conventional counterparts. These include Health Canada, The World Health Organization, The American Medical Association, The British Royal Society and about 200 others.\nIt’s pretty cut and dry. GMOs are as safe to eat as their non-GMO counterparts.\nSo, why is this label EVERYWHERE?\nPeople are more interested in where their food comes from than ever before and they’re looking for honesty and transparency in the food system.\nTHAT is awesome. We should be asking questions about our food. No one is asking us to blindly trust our food supply.\nOne of the big problems is most of us learn about our food through people who know very little about food. The folks who market it.\nSales of products labelled non-GMO are skyrocketing and today, they represent roughly $21 billion across North America, a 30% increase just in the last 2 years.\nYou better believe food marketers are hopping on that bandwagon. Paying for non-GMO verification has the potential to make a substantial impact on the bottom-line.\nIn other words, KA-CHING!\nWhat is the Non-GMO label telling me, then?\nWell, it is not used to better inform you as a consumer and it does not tell you anything about the safety, the quality or the nutritional content of your food.\nThey are used as a marketing tactic by an industry that understands today’s climate of fear around GMOs. Despite the overwhelming research around GMO safety, these labels work to scare consumers, and in turn demonize safe, nutritious food.\nConsumers will make choices and pay more for ‘peace of mind’; even if they don’t understand exactly what it is they’re paying for. I’m a parent myself, so I completely understand this mentality. I want the very best for my child. Peace of mind is valuable.\nAren’t false labelling claims illegal?\nWhile we do have regulations for food marketing and labelling in place through the Canadian Food Inspection Agency, this particular label seems to have slipped through cracks. It has been deemed acceptable since the label says Non-GMO rather than GMO-free.\nMy guess is the average consumer is not going to make a distinction there.\nThis type of label – one that identifies what’s not in a product, is called an absence claim. It works by having us believe that if it’s labelled non-GMO then GMOs must be undesirable.\nTechnically, if a product makes an absence claim, there must be a version of that product which contains the ingredient they are claiming is absent. Slapping a non-GMO label on something that would never contain GMO ingredients to begin with, is clearly misleading.\nYet again, the non-GMO label seems to get a free pass here, as there are a huge number of products carrying non-GMO labels when otherwise they wouldn’t be a source of GMOs.\nFor the record, the only GMO products approved for sale in Canada are:\nSoy, corn, canola, sugar beet, alfalfa, Hawaiian papaya, squash, cotton and more recently, salmon.\nSo while GMO ingredients are commonly found in processed or packaged food which often use soy, corn, canola, sugar beet and their derivatives as ingredients, nearly all fresh, whole foods are not genetically engineered. Despite what dear old butterfly might have you believe.\nAlright so what are the takeaways?\nDon’t get me wrong, food and agriculture is an extremely complex topic. While seemingly in-depth, this post glosses over many of the very legitimate questions and concerns that exist about our food system. That being said, I hope I’ve been able to take away a little bit of mystery and help you feel more confident about your food choices around this one specific topic.\n- Non-GMO Verified labels tell us nothing about the safety, quality and nutrition content of a food.\n- They are a marketing tool that leverages the power of fear to guide us toward making emotionally charged food purchases.\n- They are perpetuating false beliefs about the safety of GMOs and the amazing Canadian farmers and food producers that may use them.\n- This deliberate deception adds to consumer confusion by undermining some of our important and valuable labels and labelling laws (allergen labelling and nutrition claims for example).\nAre food labels and claims confusing to you? What kind of questions do you have about your food?"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:264c0c58-bae8-47c5-8c42-566c31e51f5e>"],"error":null}
{"question":"What are the parallels between Pope Francis's call for limits on freedom of expression and Putin's efforts to control information spaces, particularly in terms of their stated goals?","answer":"While operating in different spheres, both leaders advocate for restrictions on expression but with contrasting motivations. Pope Francis argues that freedom of expression should have limits to protect religious dignity and maintain social harmony, stating that one 'cannot make fun of the faith of others' and that freedom must serve the common good. In contrast, Putin's control of information spaces serves to break Western unity, invalidate NATO's collective defense, and weaken faith in Western governments. The Pope's position stems from a desire to promote respectful coexistence and prevent conflict, while Putin's approach aims to exploit information control as a tool for expanding Russian power and undermining Western institutions. The Pope seeks voluntary self-restraint for social harmony, while Putin employs active manipulation for strategic advantage.","context":["Pope Francis stirred controversy today in remarks on the papal plane from Colombo to Manila by telling journalists that it was “normal” for an insult to provoke a strong reaction, and that there are “limits” to freedom of expression, adding: “You cannot make fun of the faith of others”. Some commentators have interpreted what he said as giving succour to the notion that those who give gratuitous offense should expect retaliation.\nYet his words, printed in full below (H/T Gerry O’Connell at America magazine), make clear that he believes free speech cannot be understood as purely a question of license: its purpose is to build the common good of society. When it is used to insult and mock, it produces a reaction (the Pope jokingly used the example of a man punching someone who insults his mother). That does not justify, as he makes abundantly clear, a reaction of violence; but nor is it enough to declare that freedom of speech is unlimited (and in practice, in the law of most nations, it is not).\nHis statements can be summarized as follows:\n- Everyone has the right to practice their faith in freedom.\n- It is wrong to commit violence in the name of God and religion.\n- Freedom of expression is necessary to build the common good of society.\n- But it is important not to use this freedom to offend and insult.\n- When this happens, it is “normal” for those insulted to react.\n- Some things are sacred, and it is wrong to insult and mock religion, contrary to what many in post-Enlightenment societies believe.\nIn these statements, Francis has nailed a truth which has been lacking in the discussions following the appalling massacres in Paris, namely that freedom of expression is an inadequate basis for building society. Without respect for the beliefs of others and self-restraint in the exercise of freedom of expression, there can be no peace.\nWhat, then, are the limits that the law should impose? When does self-restraint turn into censorship? Francis does not enter these questions; he leaves that debate to others. His concern is to name some essential truths which should guide our discussion in the wake of Charlie Hebdo.\nPope Francis’s words on the papal plane:\nMaillard: Holy Father, yesterday at mass you spoke about religious freedom as a fundamental human right. But in the respect for the different religions, up to what point can one go in freedom of expression? That too is a fundamental human right.\nPope. Thanks for the question, it’s an intelligent one. I believe that both are fundamental human rights, religious liberty and liberty of expression. One cannot — but let’s think — you are French? Let’s go to Paris, let’s speak clearly. One cannot hide a truth: everyone has the right to practice one’s religion, one’s own religion without giving offense. Freely. That’s how we do it, we want everyone to do that. Second: One cannot offend, make war, kill in the name of one’s own religion, that is, in the name of God. To us, that which happens now, it stuns us. But let’s think about our own history: how many wars of religion have we had? You may think of the night of St. Bartholomew; how can this be understood? We too were sinners in this. But one cannot kill in the name of God. This is an aberration. To kill in the name of God is an aberration. I believe that this is the principal point in terms of religious liberty. One has freedom in this, but without imposing or killing in the name of religion.\nAs for freedom of expression: each one not only has the freedom, the right but also the obligation to say what one thinks to help the common good. The obligation! Let’s think, if a member of parliament or a senator doesn’t say what he thinks is the right path then he does not collaborate for the common good. Not only these, but many others too. We have the obligation to say openly, to have this liberty, but without giving offense, because it is true, one cannot react violently. But if Dr. Gasbarri (the papal trip organizer who was standing beside him), a great friend, says a bad word against my mother, then a punch awaits him. But it’s normal, it’s normal. One cannot provoke, one cannot insult other people’s faith, one cannot make fun of faith. Pope Benedict in a speech, I don’t remember exactly where, he spoke of this post-positivist mentality, of post-positivist metaphysics, that led to the belief that in the end religions, religious expressions, are a kind of subculture, which are tolerated but are of little value, are not on the Enlightenment culture. And this is part of the heritage of the Enlightenment. And so many people who speak badly about other religions, or religions [in general], they make fun of, let’s say toy with [make into toys] other people’s religions, these people provoke and there can occur what would happen to Dr. Gasbarri if he said something against my mother. That is, there is a limit. Every religion has dignity; every religion that respects life, human life, the human person. And I cannot make fun of it. This is a limit and I have taken this sense of limit to say that in freedom of expression there are limits, like that in regard to my mom. I don’t know if I have managed to answer the question.\nOn the plane back from Manila to Rome, Francis was asked to clarify his position.\nValentina Alazraki Crastich (Televisa): On the flight from Sri Lanka you used the image of the response that this poor man (Alberto Gasbarri, organizer of papal trips) might have merited if he insulted your mother. Your words were not well understood by everyone in the world and seemed to justify in some way the use of violence in the face of provocation. Could you explain a little better what you meant to say?\nPope: In theory we can say that a violent reaction in the face of an offense or a provocation, in theory yes, it is not a good thing, one shouldn’t do it. In theory we can say what the Gospel says, that we should turn the other cheek. In theory we can say that we have freedom of expression, and that’s important. But in theory we all agree. But we are human and there’s prudence which is a virtue of human coexistence. I cannot constantly insult, provoke a person continuously because I risk making him/her angry, and I risk receiving an unjust reaction, one that is not just. But that’s human. For this reason I say that freedom of expression must take account of the human reality and for this reason one must be prudent. It’s a way of saying that one must be educated, prudent. Prudence is the virtue that regulates our relations. I can go up to here, I can go up to there, and there, beyond that no. What I wanted to say is that in theory we all agree: there is freed of expression, a violent aggression is not good, it’s always bad. We all agree, but in practice let us stop a little because we are human and we risk to provoke others. For this reason freedom must be accompanied by prudence. That’s what I wanted to say.","Confronting the Russian Challenge\nInstitute for the Study of War (ISW) and Critical Threats Project (CTP) at the American Enterprise Institute\nRussia poses a significant threat to the United States and its allies for which the West is not ready. The West must act urgently to meet this threat without exaggerating it. Russia today does not have the military strength of the Soviet Union. It is a poor state with an economy roughly the size of Canada’s, a population less than half that of the U.S., and demographic trends indicating that it will lose strength over time. It is not a conventional military near-peer nor will it become so. Its unconventional warfare and information operations pose daunting but not insuperable challenges. The U.S. and its allies must develop a coherent global approach to meeting and transcending the Russian challenge.\nThe Russian Threat\nPresident Vladimir Putin has invaded two of his neighbors, Georgia and Ukraine, partly to stop them from aligning with NATO and the West. He has also illegally annexed territory from both those states. He has established a military base in the eastern Mediterranean that he uses to interfere with, shape, and restrict the operations of the U.S. and the anti-ISIS coalition. He has given cover to Bashar al Assad’s use of chemical weapons, and Russian agents have used military-grade chemical weapons in assassination attempts in Great Britain. Russia has threatened to use nuclear weapons, even in regional and local conflicts. And Moscow has interfered in elections and domestic political discourse in the U.S. and Europe.\nThe Russian threat’s effectiveness results mainly from the West’s weaknesses. NATO’s European members are not meeting their full commitments to the alliance to maintain the fighting power needed to deter and defeat the emerging challenge from Moscow. Increasing political polarization and the erosion of trust by Western peoples in their governments creates vulnerabilities that the Kremlin has adroitly exploited.\nMoscow’s success in manipulating Western perceptions of and reactions to its activities has fueled the development of an approach to warfare that the West finds difficult to understand, let alone counter. Shaping the information space is the primary effort to which Russian military operations, even conventional military operations, are frequently subordinated in this way of war. Russia obfuscates its activities and confuses the discussion so that many people throw up their hands and say simply, “Who knows if the Russians really did that? Who knows if it was legal?”—thus paralyzing the West’s responses.\nPutin is not simply an opportunistic predator. Putin and the major institutions of the Russian Federation have a program as coherent as that of any Western leader. Putin enunciates his objectives in major speeches, and his ministers generate detailed formal expositions of Russia’s military and diplomatic aims and its efforts and the methods and resources it uses to pursue them. These statements cohere with the actions of Russian officials and military units on the ground. The common perception that he is opportunistic arises from the way that the Kremlin sets conditions to achieve these objectives in advance. Putin closely monitors the domestic and international situation and decides to execute plans when and if conditions require and favor the Kremlin. The aims of Russian policy can be distilled into the following:\nPutin is an autocrat who seeks to retain control of his state and the succession. He seeks to keep his power circle content, maintain his own popularity, suppress domestic political opposition in the name of blocking a “color revolution” he falsely accuses the West of preparing, and expand the Russian economy.\nPutin has not fixed the economy, which remains corrupt, inefficient, and dependent on petrochemical and mineral exports. He has focused instead on ending the international sanctions regime to obtain the cash, expertise, and technology he needs. Information operations and hybrid warfare undertakings in Europe are heavily aimed at this objective.\nPutin’s foreign policy aims are clear: end American dominance and the “unipolar” world order, restore “multipolarity,” and reestablish Russia as a global power and broker. He identifies NATO as an adversary and a threat and seeks to negate it. He aims to break Western unity, establish Russian suzerainty over the former Soviet States, and regain a global footprint.\nPutin works to break Western unity by invalidating the collective defense provision of the North Atlantic Treaty (Article 5), weakening the European Union, and destroying the faith of Western societies in their governments.\nHe is reestablishing a global military footprint similar in extent the Soviet Union’s, but with different aims. He is neither advancing an ideology, nor establishing bases from which to project conventional military power on a large scale. He aims rather to constrain and shape America’s actions using small numbers of troops and agents along with advanced anti-air and anti-shipping systems.\nA sound U.S. grand strategic approach to Russia:\n- Aims to achieve core American national security objectives positively rather than to react defensively to Russian actions;\n- Holistically addresses all U.S. interests globally as they relate to Russia rather than considering them theater-by-theater;\n- Does not trade core American national security interests in one theater for those in another, or sacrifice one vital interest for another;\n- Achieves American objectives by means short of war if at all possible;\n- Deters nuclear war, the use of any nuclear weapons, and other Weapons of Mass Destruction (WMD);\n- Accepts the risk of conventional conflict with Russia while seeking to avoid it and to control escalation, while also ensuring that American forces will prevail at any escalation level;\n- Contests Russian information operations and hybrid warfare undertakings; and\n- Extends American protection and deterrence to U.S. allies in NATO and outside of NATO.\nSuch an approach involves four principal lines of effort.\nConstrain Putin’s Resources. Russia uses hybrid warfare approaches because of its relative poverty and inability to field large and modern military systems that could challenge the U.S. and NATO symmetrically. Lifting or reducing the current sanctions regime or otherwise facilitating Russia’s access to wealth and technology could give Putin the resources he needs to mount a much more significant conventional threat—an aim he had been pursuing in the early 2000s when high oil prices and no sanctions made it seem possible.\nDisrupt Hybrid Operations. Identifying, exposing, and disrupting hybrid operations is a feasible, if difficult, undertaking. New structures in the U.S. military, State Department, and possibly National Security Council Staff are likely needed to:\n- Coordinate efforts to identify and understand hybrid operations in preparation and underway;\n- Develop recommendations for action against hybrid operations that the U.S. government has identified but are not yet publicly known;\n- Respond to the unexpected third-party exposure of hybrid operations whether the U.S. government knew about the operations or not;\n- Identify in advance the specific campaign and strategic objectives that should be pursued when the U.S. government deliberately exposes a particular hybrid operation or when third parties expose hybrid operations of a certain type in a certain area;\n- Shape the U.S. government response, particularly in the information space, to drive the blowback effects of the exposure of a particular hybrid operation toward achieving those identified objectives; and\n- Learn lessons from past and current counter-hybrid operations undertakings, improve techniques, and prepare for future evolutions of Russian approaches in coordination with allies and partners.\nThe U.S. should also develop a counter-information operations approach that uses only truth against Russian narratives aimed at sowing discord within the West and at undermining the legitimacy of Western governments.\nDelegitimize Putin as a Mediator and Convener. Recognition as one of the poles of a multipolar world order is vital to Putin. It is part of the greatness he promises the Russian people in return for taking their liberty. Getting a “seat at the table” of Western-led endeavors is insufficient for him because he seeks to transform the international system fundamentally. He finds the very language of being offered a seat at the West’s table patronizing.\nHe has gained much more legitimacy as an international partner in Syria and Ukraine than his behavior warrants. He benefits from the continuous desire of Western leaders to believe that Moscow will help them out of their own problems if only it is approached in the right way.\nThe U.S. and its allies must instead recognize that Putin is a self-declared adversary who seeks to weaken, divide, and harm them—never to strengthen or help them. He has made clear in word and deed that his interests are antithetical to the West’s. The West should therefore stop treating him as a potential partner, but instead require him to demonstrate that he can and will act to advance rather than damage the West’s interests before engaging with him at high levels.\nThe West must not trade interests in one region for Putin’s help in another, even if there is reason to believe that he would actually be helpful. Those working on American policy in Syria and the Levant must recognize that the U.S. cannot afford to subordinate its global Russia policy to pursue limited interests, however important, within the Middle East. Recognizing Putin as a mediator or convener in Syria—to constrain Iran’s activities in the south of that country, for example—is too high a price tag to pay for undermining a coherent global approach to the Russian threat. Granting him credibility in that role there enhances his credibility in his self-proclaimed role as a mediator rather than belligerent in Ukraine. The tradeoff of interests is unacceptable.\nNor should the U.S. engage with Putin about Ukraine until he has committed publicly in word and deed to what should be the minimum non-negotiable Western demand—the recognition of the full sovereignty of all the former Soviet states, specifically including Ukraine, in their borders as of the dates of their admission as independent countries to the United Nations, and the formal renunciation (including the repealing of relevant Russian legislation) of any right to interfere in the internal affairs of those states.\nDefend NATO. The increased Russian threat requires increased efforts to defend NATO against both conventional and hybrid threats. All NATO members must meet their commitments to defense spending targets—and should be prepared to go beyond those commitments to field the forces necessary to defend themselves and other alliance members. The Russian base in Syria poses a threat to Western operations in the Middle East that are essential to protecting our own citizens and security against terrorist threats and Iran. Neither the U.S. nor NATO is postured to protect the Mediterranean or fight for access to the Middle East through the eastern Mediterranean. NATO must now prepare to field and deploy additional forces to ensure that it can win that fight.\nThe West should also remove as much ambiguity as possible from the NATO commitment to defend member states threatened by hybrid warfare. The 2018 Brussels Declaration affirming the alliance’s intention to defend member states attacked by hybrid warfare was a good start. The U.S. and other NATO states with stronger militaries should go further by declaring that they will come to the aid of a member state attacked by conventional or hybrid means regardless of whether Article 5 is formally activated, creating a pre-emptive coalition of the willing to deter Russian aggression.\nBilateral Negotiations. Recognizing that Russia is a self-defined adversary and threat does not preclude direct negotiations. The U.S. negotiated several arms control treaties with the Soviet Union and has negotiated with other self-defined enemies as well. It should retain open channels of communication and a willingness to work together with Russia on bilateral areas in which real and verifiable agreement is possible, even while refusing to grant legitimacy to Russian intervention in conflicts beyond its borders. Such areas could include strategic nuclear weapons, cyber operations, interference in elections, the Intermediate Nuclear Forces treaty, and other matters related to direct Russo-American tensions and concerns. There is little likelihood of any negotiation yielding fruit at this point, but there is no need to refuse to talk with Russia on these and similar issues in hopes of laying the groundwork for more successful discussions in the future.\nREAD THE FULL REPORT HERE."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:d4aff5bf-bca7-47b9-8879-7f46a92dd948>","<urn:uuid:e8df76d6-c645-4357-9010-d1b8c8557cc9>"],"error":null}
{"question":"What is the relationship between teat condition and mastitis resistance, and how does Mycoplasma infection impact this relationship?","answer":"Healthy teat skin serves as the first line of defense against pathogens, with dead cells filled with keratin and bacteriostatic fatty acids creating a hostile environment for bacteria. Poor teat conditions, including severe erosions, raw and ulcerated skin, or compromised teat ends prevent tight closure of the teat canal, enabling pathogens to enter the udder. In the case of Mycoplasma infections, these organisms are particularly dangerous because they can cause significant udder swelling and may infect all quarters simultaneously, leading to clinical episodes where infected quarters shed billions of organisms. Unlike other mastitis pathogens, Mycoplasma infections cannot be effectively treated, making maintaining good teat condition even more critical for prevention.","context":["A group of organisms, Mycoplasma, are capable of causing a serious mastitis condition. Mycoplasma organisms are not true bacteria. They are much smaller in size compared to most bacteria but are larger than viruses.\nMastitis is caused by many different species of bacteria including Staph aureus, Streptococcus agalactiae and E. coli. In addition, other non-bacterial microscopic organisms occasionally cause similar problems. Species of yeast, Prototheca algae and Nocardia, a species of mold common in soil, occasionally cause serious mastitis problems, usually on an isolated basis. Often such forms of mastitis are difficult or impossible to treat because they do not respond to available antibiotic therapies.\nAnother group of organisms, Mycoplasma, are also capable of causing a serious mastitis condition. Mycoplasma organisms are not true bacteria. They are much smaller in size compared to most bacteria but are larger than viruses.\nMycoplasma lack true cell walls and many of the internal systems common to bacteria. They act as cellular parasites, gleaning basic molecular compounds from the host cells. They cannot make many of these compounds themselves, since they lack the chemical machinery. Despite these characteristics, Mycoplasma are capable of causing serious infection and a variety of illnesses in animals and humans.\nSpecies of Mycoplasma\nThere are several species of Mycoplasma capable of causing cattle infections. They may cause pulmonary infections, middle ear infections, joint infections etc. Some, but not all species are also capable of causing mastitis. It is quite common for these other symptoms of Mycoplasma infectionj to be noted at the same time as mastitis problems are occurring.\nWhen attempting to identify and resolve Mycoplasma problems, it is always beneficial to understand the organism involved. Other considerations include; how to spot problems, how to treat infections, risks to the overall herd, and how to minimize these risks.\nSince Mycoplasma are unique organisms, it is especially important to determine if they are the cause of a mastitis problem. To date there is no effective tool for treating a Mycoplasma mastitis problem, so other control techniques have to be implemented.\nSpecies of Mycoplasma-causing mastitis\nThere are several Mycoplasma species that may be involved with mastitis, but certain ones are more of an issue than others. The following list identifies potentially infectious species and their basic characteristics.\n- Mycoplasma bovis - the most common cause of Mycoplasma mastitis. Estimated it is responsible for about 50% or more of the cases of mastitis caused by Mycoplasma.\n- Mycoplasma bovigenitalium - found in reproductive tract of animals, may contaminate the teats when uterine discharges occur.\n- Mycoplasma canadense - occasional cause of mastitis and quite resistant to heat treatment.\n- Mycoplasma californicum - may cause mastitis\n- Mycoplasma alkalescens - may cause mastitis\n- There are several other species of Mycoplasma that may cause disease but they are not considered to be a significant cause of mastitis.\nOther Diseases Associated with Mycoplasma Infection.\nCalf middle ear infections\nThis condition may be caused by Mycoplasma infections. Affected calves tend to have characteristic droopy ears and/or head tilt. It can be fatal and treatment is not always successful. One way calves may contract this problem is through consumption of Mycoplasma contaminated waste milk. Calves, especially in group housing, may suck or chew each other and this may allow organisms on the mouth region to get to the ear and eventually cause new infections. There may also be other means of spreading and contracting the disease.\nJoint swelling and lameness\nMycoplasma species can enter leg joints and infect the fluids therein causing swelling and lameness.\nRespiratory infections (pneumonia)\nMycoplasma is known to be a common cause of respiratory tract infections. Infected animals that cough may actually expel the organisms into the air in tiny water droplets. These may float on air currents and possibly contaminate other animals. This may be more commonplace with confinement housing, especially if ventilation is inadequate.\nMycoplasma is unique in that it may be possible for organisms to move from an infection site in one part of the body (i.e. respiratory tract) to the mammary gland by internal relocation. This however is not the primary cause of herd outbreaks of Mycoplasma mastitis.\nMycoplasma is spread from udders of infected cows to non-infected cows primarily by contaminated milking clusters and hands during milking.\nMinimizing these issues is critical in controlling Mycoplasma spread just as it is with other forms of contagious mastitis including Staph aureus and Strep ag.\nClean hands, clean clusters and post-milking teat dipping are critical control measures.\nMycoplasma mastitis has come to be a concern on many dairies. It has been recognized as a problem for many years in the larger dairies of California and Florida. In the 1970’s-80’s it produced serious difficulties for many large west-coast dairies and for a time was thought to be a large herd mastitis problem.\nNo longer. Most states have detected its presence and prevalence appears to be increasing in many areas. Herds buying cattle of unknown Mycoplasma status are vulnerable to the disease, especially if management is unfamiliar with the nature of this problem.\nHerds that are built or expanded rapidly by purchasing and assembling cattle from various locations are the most vulnerable. Generally, there is no previous history on many of these animals. As a result, a significant amount of pre-purchase testing is required to screen out infected animals.\nRelocation of large numbers of animals to new facilities results in stress which reduces disease resistance and increases susceptibility to disease producing agents, including Mycoplasma.\nOnce a cow develops a Mycoplasma clinical mastitis infection, most outcomes thereafter are negative. Cows often develop significant udder swelling as well as off-color and occasionally markedly different looking milk. They may actually develop infections in all quarters simultaneously, which is an unusual observation for mastitis. They may appear to recover spontaneously but generally remain carriers and will have occasional flare-ups and releases of large numbers of organisms.\nTreatment with infusion tubes and/or systemic treatment produces no improvement for a simple reason. The antibiotics available, predominantly penicillin based products, tend to interfere with cell wall development. Since Mycoplasma have no cell wall,such antibiotics are ineffective.\nBacteriology of milk from infected cows\nDuring clinical episodes, Mycoplasma infected quarters will shed huge quantities of organisms. Dr. Allen Britten, Udder Health Services, Inc., Bellngham, Washington has indicated that infected cows may shed billions of organisms into raw milk during clinical outbreaks. At other times they may shed very few. This pattern of shedding makes it difficult to always rely on bulk tank cultures as a way of determining if Mycoplasma is present. Nevertheless, routine bulk tank screening should be used as a monitoring tool to help determine if milk from Mycoplasma infected cows is entering the tank.\nMycoplasma impact on standard raw milk bacteria counts\nMycoplasma shedding will not impact the standard plate count (SPC), PI counts or the LPC, even if a lot of cows are shedding. These organisms require such unique conditions for growth that standard milk bacteria tests will not reveal them. Infected cows may have a significant somatic cell response, so the SCC will rise as a result of infections.\nMinimize animal purchases. Untested new animals entering a herd may be the source of Mycoplasma infection.\nLook at records from a herd being considered for purchase to determine if there has been a problem. If no such information exists, have a bulk tank milk sample taken and sent to a lab that has procedures in place to properly test for Mycoplasma. University veterinary diagnostic labs and certain private veterinary diagnostic labs are capable of doing this properly. The key is to work with a group familiar with testing and identifying Mycoplasma. Also, if Mycoplasma growth is found, ask for them to have it speciated to determine if the species is a likely cause of mastitis. Speciation requires specialized procedures that take additional time. Often the sample has to be sent to another facility equipped to do such [J3] work. Sequential bulk tank samples spanning several days would help increase the chances of detecting cows that may be shedding Mycoplasma organisms sporadically and at different concentrations.\nSick Pen Cows\nThere is always a concern about cattle in hospital facilities. It is a collecting point for all cows suffering a variety of ailments. Cows may go in with a sore foot, be exposed to mastitis while there, and emerge with a mastitis problem. When facilities allow, waste milk from the hospital string should be routinely sampled and evaluated for Mycoplasma and other, more standard, mastitis pathogens.\nAny clinical mastitis case should be sampled prior to treatment and the sample frozen. If there is no response to treatment then the sample(s) should be tested to determine which organisms are present. Keep in mind that Mycoplasma is a very contagious organism, so if there is a Mycoplasma concern, get the sample tested promptly rather than accumulate several before testing.\nMilking procedures and facilities are concerns in hospital facilities. If a separate milking facility exists, make certain that cows are milked in a manner that minimizes cow to cow transfer of potential pathogens. Milkers must wear surgical gloves and they need to sanitize hands and milking clusters between cows.\nCows infected with Mycoplasma mastitis should be identified, totally segregated from the main herd and culled immediately if they show clinical problems.\nIf they appear normal, they need to remain segregated until they are no longer productive or experience a clinical flare-up. Then they should be culled.\nCows can never be returned to the main herd even if they appear to have recovered because they may become chronic, occasional shedders.\nMastitis Treatment Products\nTreatment of mastitis cases should only involve products labeled for intramammary infusion. They should be factory produced, unopened products.\nOccasionally cows are treated with bottled medicines as additional supportive therapy. These materials often become contaminated, due to sloppy procedures, with Mycoplasma and become a source of new infections. This is also a problem with certain other organisms that can survive in medications including yeast, molds and Nocardia.\nProper infusion technique is critical when treating udder infections. Thoroughly clean the teat ends per the product label recommendations. Frequently a sterile gauze pad soaked with alcohol is used to effectively clean and sanitize the teat end. Infuse tube products using the partial cannula insertion method. Research has demonstrated that using a very short cannula to infuse product reduces steak canal keratin ream out and possible relocation of pathogens from the streak canal into the udder.\nWhat sanitizers are effective?\nMycoplasma organisms contain no special characteristics that make them resistant to sanitizers. Iodine, especially with an acid base, is highly effective against Mycoplasma organisms when used as a hard surface sanitizer.\nAutomated backflush systems are available. When linked to an acid based iodine solution, clusters can be sanitized between cows to limit cow to cow transmission of Mycoplasma organisms by the milking cluster. It is advisable to put maximum effort into identifying infected cows and eliminating them, rather than relying exclusively on a backflush system as a means of limiting Mycoplasma transfer.\nIodine teat dips, especially products providing high levels of free iodine, will effectively kill Mycoplasma organisms on contact. They need to be used pre- and post-milking to sanitize teats. Teat dipping is a vital part of all mastitis control programs, including Mycoplasma mastitis.\nWaste milk handling\nMilk from cows being held out of the tank for various reasons is often used as calf feed. However, it frequently carries significant health risks due to pathogens. It may be contaminated with various mastitis pathogens including Staph aureus and Mycoplasma species. Heat treating waste milk to kill pathogens is advisable using pasteurization units available today.\nMycoplasma species are susceptible to heat treatment, but they differ in terms of time and temperature requirements for lethality. The following table provides the needed time and temperature exposure for killing three of the more common species.\nTime needed to kill Mycoplasma pathogens (minutes)*\n*Butler et al. Journal of Dairy Science. October, 2000.\nIt is clear if M. canadense is involved, then more time will always be needed to kill it. At the highest temperature, 158 F, the time required is reduced to 3 minutes.\n- Several species of Mycoplasma are capable of causing bovine mastitis. Mycoplasma bovis is the most commonly identified species.\n- Purchased cows with unknown infection status are a common source of problems.\n- Mycoplasma species may cause ear, respiratory tract and joint infections.\n- Medications contaminated on-farm by improper handling procedures are a common source of problems.\n- Mycoplasma udder infections are unresponsive to mastitis treatment products.\n- Infected cows may shed huge numbers into raw milk during clinical periods.\n- Mycoplasma organisms in raw milk will not be picked up by normal standard plate count (SPC) or preliminary incubation (PI) procedures, even when present in large numbers.\n- Special growth procedures must be used to determine the presence of Mycoplasma in milk. Use labs familiar with these special requirements.\n- Routine bulk tank screening is helpful to determine if infected cows are present and shedding.\n- Infected cows must be identified and either segregated or culled to protect the rest of the herd. Segregated cows can never be allowed to re-enter the main herd.\n- Primary means of Mycoplasma spread is cow to cow by contaminated milking clusters and hands. Milking time hygiene is critical in limiting spread of Mycoplasma.\n- Iodine teat dips and sanitizers effectively kill Mycoplasma organisms.","Teat skin is the first line of defence against invading pathogens. The surface of the teat skin consists of dead cells filled with keratin. In addition, there are fatty acids present on the skin that are bacteriostatic. When intact, this provides a hostile environment for bacteria, thus preventing their growth. This bacteriostatic property can be removed and this is why teat sanitiser should be chosen carefully. The normally intact surface of the skin may also become compromised by cuts, cracks, chaps, bruising, lesions, etc. Bacteria can then multiply on the surface of the skin and become a reservoir for mastitis infections. This is particularly the case for organisms such as Streptococcus dysgalactiae and Staphylococcus aureus. Therefore, every udder care product from Lely consists of disinfection and care components to reduce the infection pressure (figure 2). Besides this, trials have shown that cows with dry and badly cracked teat skin are much slower milkers. They may need double the time on the unit to achieve the same level of yield, and of course, this increased time can lead to teat-end damage.\nThe teat canal is located between the teat cistern and the outside ending of the teat. It is lined with a skin-like epidermis that forms the keratin material that has antibacterial properties. At the level of the teat canal, the circular muscle is well developed, forming the teat sphincter, the function of which is to retain milk in the intervals between milking. It takes at least 20 minutes to 30 minutes for the teat to become fully closed. Hence, in order to protect the teat from bacterial contamination, the advice is given that animals should not be allowed to lie down for at least 30 minutes after milking. In robotic milking systems, in which this behaviour is hard to control, a barrier spray can give a good outcome. Although, in particular, cows prefer to drink after milking and during feeding. This behaviour is clearly seen in herds, where cows will go to drink after visiting the robot. Therefore, ensure an adequate and clean water supply.Figure 1. Anatomy of an udder quarter. The teat sphincter is very important in the first line of defence, because it is the gateway to the canal and the udder tissue.\nMany bacteria entering the teat between milkings become trapped by the layer of keratin and lipid lining in the teat canal. They are then flushed out at the start of the next milking by the first flow of milk, as this removes the superficial layers of keratin lining the teat canal. This is known as ‘the keratin flush’. It is very important to ensure that udder preparation and unit attachment are such that milk flows out of the teat when the cluster is applied, and that there are no reverse flow mechanisms that might lead to milk and infection being propelled back up into the udder. This is why cows with short teat canals (short vertical length) and those with a wide cross-section diameter are more susceptible to mastitis. Therefore, Lely provides a portfolio of different teat liners to match these teat sizes and farmers’ needs.Figure 2. Lely udder health products. The positioning is based on whether the product emphasises a disinfecting or a caring effect. All products are biocides and thus meet the requirements for disinfection.\nVisible teat health\nTeat-end scoring is nowadays a commonly pursued parameter in research. In the field, teat condition has been accepted as an indicator for quality of milking. Different settings of the milking machine, such as vacuum level, flow rate at cluster removal and pulsation rate, including previously mentioned causes, affect teat tissue. Changes can be observed such as teat swelling, teat flattening, colour changes, openness of the teat orifice, loss of keratin, vascular damage (haemorrhages), and teat-end callosity. Research has found that teat ends with severe erosions that were raw and ulcerated (broken skin) showed a higher prevalence of mastitis (Farnsworth, 1995).\nMicroscopic view of a teat end with a smooth and thin callosity ring and the corresponding view of the outside (De Man, 1998).\nMicroscopic view of a teat end with a severe rough callosity ring and the corresponding view of the outside (De Man, 1998).\nTight closure of the teat canal is not possible, thus enabling pathogens to enter the udder.\nThis image shows poor teat skin condition and poor teat-end condition. Poor teat skin can be caused by environmental factors, such as cold, wet or muddy conditions, or lime bedding material drawing moisture from the skin. Another possible issue is irritation by the disinfecting ingredient in the absence of sufficient emollients.\nPoor teat-end condition can be caused by poor milking performance, such as the wrong milking settings, overmilking or poor teat liner fit.\nThe teat-end condition is a very important defence against invading pathogens. Optimising skin condition is a preventive and effective way to limit cases of mastitis. In the next article, we will delve deeper into the cow-related and environmental pathogens that cause mastitis.\nManagement, T4C & InHerd, Cow health, Tips & Tricks\nWhy treatment plans?\nEnsuring that animal health issues are dealt with effectively and that the labour involved in the treatment is efficient is very important, particularly as herd sizes become larger. Farms with an automatic milking system have an extra tool they can use to increase the effectiveness of the treatment, thereby decrease the effort of the farmer has to make to achieve the best results.\nPreparation for dry period pays off\nDry period infections are a very important part of the epidemiology of environmental pathogens such as E. coli and S. uberis. These infections often remain subclinical throughout the dry period, but are then an important cause of clinical mastitis in the first few months of the subsequent lactation period. This article will give more insight and information about the different stages of the dry period and their relation to mastitis."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:8e8bf7c9-1b8f-40f4-86fb-ba83b13b4447>","<urn:uuid:8d30d356-45a8-4397-b28a-f2c501fc3124>"],"error":null}
{"question":"How does soil water conservation compare between no-tillage and conventional tillage systems in cotton production?","answer":"No-tillage systems generally conserve more soil water compared to conventional tillage. During drought periods, cotton under no-tillage maintains higher photosynthetic rates, especially in standard row conditions. This is because no-tillage systems enhance soil residue cover, improve water infiltration, and reduce evaporative soil water loss. The crop residue covering at least 30% of the soil surface breaks the impact of rain drops, helps wick moisture into the soil, and reduces runoff, leading to better soil water storage.","context":["|Delaney, D - AUBURN UNIVERSITY|\n|Terra, J - AUBURN UNIVERSITY|\nSubmitted to: Southern Conservation Tillage for Sustainable Agriculture Proceedings\nPublication Type: Proceedings\nPublication Acceptance Date: June 24, 2002\nPublication Date: June 24, 2002\nCitation: Prior, S.A., Reeves, D.W., Delaney, D.P., and Terra, J.F. 2002. Effects of conventional tillage and no-tillage on cotton gas exchange in standard and ultra-narrow row systems. In Santen, E. van (ed). Making conservation tillage conventional: Building a future on 25 years of Research, Proc. of 25th Southern Conservation Tillage Conference for Sustainable Agriculture. pp. 334-338. Special Report No.1. Alabama Agric. Expt. Stn., Auburn University, AL. Interpretive Summary: Crop production in the U.S. is often limited by available soil water. Adoption of no-tillage farm practices can lead to more crop residue on the soil surface which can enhance soil water storage. Our goal was to evaluate cotton gas exchange during reproductive growth as affected by tillage practice (conventional tillage vs. no-tillage) and row spacing (standard 40 in vs. ultra-narrow row). The benefits of no-tillage are most probable in years experiencing sporadic precipitation during reproductive growth. Reflective of good soil water status, no-tillage cotton maintains higher rates of photosynthesis. In years with frequent rainfall during reproductive growth, the benefits of no-tillage are less frequent. Compared to standard row cotton, the benefits of no-tillage are less pronounced in ultra-narrow cotton which had lower rates of photosynthesis due to faster canopy closure and greater plant-to-plant competition for soil resources. Adoption of no-tillage practices can help minimize detrimental impacts of drought stress on cotton grown in coarse textured soils during critical reproductive periods such as boll filling.\nTechnical Abstract: The availability of soil water to crops is a major limitation to crop production. Use of conservation tillage systems enhances soil residue cover, water infiltration and reduces evaporative soil water loss. Our objective was to measure cotton (Gossypium hirsutum L.) leaf level photosynthesis, stomatal conductance, and transpiration during reproductive growth under different row spacing and tillage conditions on a Norfolk loamy sand (Typic Kandiudults; FAO classification Luxic Ferralsols) in east-central AL. Gas exchange measurements occurred in the summer of 1999, 2000, and 2001. The study used a split-plot design replicated four times with row spacing (standard 40 inch row and ultra-narrow row) as main plots and tillage systems (conventional and no-tillage) as subplots. In 1999, standard row cotton under conventional tillage maintained higher photosynthetic rates during early reproductive growth when soil water was not limiting; the opposite pattern occurred latter during drought cycles. During drought periods, photosynthetic rates were higher in no-tillage systems especially under standard row conditions. In 2000 and 2001, the benefits of no-tillage were sporadic due to frequent rainfall events occurring throughout reproductive growth. In 2000, ultra-narrow row cotton consistently had lower photosynthesis rates compared to standard row cotton; lesser degrees of this occurred in 1999 and 2001. In all years, stomatal conductance and transpiration measurements generally mirrored those of photosynthesis. These results suggest that during periods of infrequent rainfall, high rates of photosynthesis can be maintained in no-tillage systems that conserved soil water needed during critical reproductive stages such as boll filling.","- Increase understanding of the benefits of conservation tillage.\n- Increase understanding and application of best management practices.\n- Key Points:\n- With conservation tillage, at least 30 percent of the soil surface is covered with crop residue after planting.\n- Maintaining residue on the soil surface increases water infiltration, reduces erosion, increases organic matter, reduces weed pressure, saves and reduces costs.\n- Best Management Practices with regard to soil compaction, fertilizer application, weed control, roller choppers, closing wheels, planting moisture, water, earthworms, stalk spreaders and narrow rows are essential to conservation tillage.\n- Assess your knowledge:\n- Define conservation tillage and list its benefits.\n- Explain how organic matter affects soil compaction with regard to conservation tillage.\n- Describe how tillage affects weed control.\n- Explain how conservation tillage reduces runoff.\n- Discuss the implications of the best management practices for conservation tillage on corn, sorghum, cotton and wheat.\nBecause of increased crop production costs, most farmers have to re-evaluate how they till and consider conservation tillage practices. With conservation tillage, at least 30 percent of the soil surface is covered with crop residue after planting. Maintaining residue on the soil surface increases water infiltration, reduces erosion, increases organic matter and reduces weed pressure. Economic advantages also result from having less labor, less fuel, fewer repairs and less maintenance, better field accessibility, lower capital investment and lower equipment horsepower requirements.\n- Fundamental Best Management Practices for Successful Conservation Tillage\nThe primary cause of compaction comes from heavy equipment traffic crushing air spaces out of moist soil. Top soils typically contain approximately 50 percent of pore space by volume. Pore space may be filled with water or air; so, when weight is applied to a moist soil, the soil aggregates are crushed, and some of the pore space is destroyed. Traffic patterns must be controlled, and proper tire pressure on equipment must be maintained. Generally, the potential for compaction increases as the percent of clay in the soil increases and as the organic matter content decreases. Reduced tillage leaves residue on the soil surface, which decreases the rate of decomposition and increases organic matter in the surface horizon.\nFertilizer placement and application\nSurface applications of fertilizer can result in nitrogen loss from volatilization and cause phosphorus and other immobile nutrients to accumulate near the soil surface. Nutrient deficiencies are likely to occur in no-till or stale seed beds.\nBecause placement and timing of phosphorus applications are important, thefollowing practices are recommended:\n- Phosphorus should be applied before or at planting to ensure that it is available early in the season.\n- In corn and sorghum production, it is important to apply a starter fertilizer or place all phosphorus fertilizer close to the developing seedling to prevent nutrient deficiencies.\n- Where a starter or a well-placed high-phosphate fertilizer is used, grain crops grow better and mature faster although yields may not be higher. This is also true if you use a pop-up, or seed placed fertilizer, that is applied directly to the seed.\n- While pop-ups have not helped cotton, they are more likely to increase yield and to establish stands quickly in grain crops. The amount of phosphorus in the pop-up should be subtracted from the total amount that is needed for the crop to prevent over-fertilization.\n- To slow stratification, phosphorus and other immobile nutrients should be banded 5 to 6 inches below the surface where possible. Placing the nutrient close to the planted row will also increase fertilizer efficiency.\nWeeds compete with the crop for moisture, fertilizer and light and can be greatly reduced if the soil is not tilled. It is easier and generally better to control weeds under no-till and reduced tillage systems. These are some other practices that help with weed control:\n- Use herbicides in the winter and during the growing season.\n- Applying transgenic technology, such as Roundup Ready® and LibertyLink® products, has made conservation tillage much easier.\n- A hooded sprayer is important for weed control in sorghum (particularly for grass control) and in cotton (for lay-by applications of herbicides).\n- Pre-emergence herbicides are still important. Weed control before planting prevents weeds from depleting valuable soil moisture and from creating a haven for insects.\nRoller choppers or rolling stalk choppers\nStalk choppers are found to be more effective in continuous cotton crops or where ridge-tillage is done farther north in Texas. The stalks are left standing all winter and spring to protect the soil against wind erosion, and are chopped in late winter or early spring when beds are remade. These choppers proved to be of no extra benefit in no-tillage in south Texas. They were ineffective in breaking surface compaction, but did a good job of chopping residue. Residue managers on the planter adequately removed un-chopped stalks at planting time.\nThe closing wheels or closing system\nUsing closing wheels or a closing system on the planter might mean the difference between a good stand and a poor stand. Because of varying conditions at planting, you should have several types of closing wheels. Schlagel Manufacturing wheels and closely spaced spiked closing wheels have been the most effective in tests with loose soil under most planting conditions.\nIt is important to break any side wall compaction caused by disc openers, to firm the seed in the bottom of the seed trench and to leave the surface slightly roughened to prevent crusting and baking. The seed must be firmed into moist soil and properly covered (as with conventional tillage) to achieve a good stand. Double disc planters tend to leave smooth, slick side walls that reduce root penetration.\nIf a small bed is made before the onset of winter, moisture should be more consistent at planting time. You can then use a bed to remove dry soil and will not need to plant “in a hole” to find moisture.\nMake sure the bed is not a high ridge, but rather only a low, rolling hump formed without burying residue. Meanwhile, keep the bed covered with as much residue as possible. Flat planting and “busting out” the dry soil on the surface to get to moisture will cause deep planting in a trench. It also will bury the seed if a heavy rain comes before stand establishment. Try to maintain as much residue on the surface as possible to increase water penetration.\nCovering the soil with residue rather than tilling it clean improves water infiltration. The impact of rain on base soil destroys small aggregates, or clods, causing the soil to seal over. Residue breaks the impact of rain drops, “wicks” or moves moisture into the soil, and reduces runoff.\nJust because a field is under conservation-tillage does not automatically mean you will have a large number of earthworms, which can do a tremendous amount of tillage. Their populations rise and fall with moisture, number of roots and amount of organic matter (their food source) in the soil. Water soaks into the soil through worm tunnels, which also helps soil gas exchanges.\nStalk spreaders are important for distributing the residue rather than pushing it into wind rows. This is particularly true for combines with larger headers, but less important for smaller combines.\nMaking rows 30 inches instead of 38 to 40 inches can help shade the soil faster (close the crop canopy faster) and reduce weed growth. In research around the state, sorghum yields have consistently been higher with narrow rows."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:15ba7afe-e860-4eb6-b290-e6fe951b1b50>","<urn:uuid:882033b2-936a-4d2c-853f-6d09bb811029>"],"error":null}
{"question":"Regarding hidden or invisible aspects, what common features are shared between ancient Egyptian tomb art and the mysterious elements of the Mona Lisa painting?","answer":"Both Egyptian tomb art and the Mona Lisa contain intentionally hidden or mysterious elements. Egyptian art was largely meant to be invisible to living eyes, being painted on tomb walls or buried within them. Similarly, the Mona Lisa contains several hidden or mysterious features: claimed hidden initials in her eyes (though disputed by the Louvre), an unsettling broken backdrop that doesn't properly align, and most notably her enigmatic smile that appears to change when viewed from different angles due to how peripheral vision processes the shadows around her mouth. Both forms of art thus incorporate elements that are either physically hidden or visually elusive to viewers.","context":["The exhibition Invisible, at the Hayward in London, presents what seems to be a quintessentially, even parodically, modern idea: the art of the invisible. What a laugh! Fortunately the show's subtitle, Art About the Unseen, 1957-2012, nails things down. Certainly, artists were making art about the unseen long before 1957; you could even say it is art's oldest theme. Here, then, is a brief history of such art from 30,000BC to AD1957.\nWhen the guide briefly switches off the lights, the cave of Cougnac, in France, is pitched into total darkness. You get a terrifying sense of what it was like for stone-age artists to negotiate the maze of crystal formations in this underground labyrinth in the Lot region, lighting their way with flickering flames. They painted animals on rock surfaces: lit in a smoky glow, these images would have been islands of visibility surrounded by vague shadows and inky darkness. Cave paintings existed at the very edge of visibility. Beyond the image lay a darkness pregnant with mystery – and surely that darkness was part of this art's magical meaning.\nJump to ancient Egypt and artists are even more preoccupied with invisible worlds. Gods with the heads of jackals and lions did not wander those deserts. Yet such invisible beings were given form by Egyptian sculptors and painters. In fact, Egyptian art itself was largely intended to be invisible – to living eyes at least. It was painted on the walls of tombs or buried in them.\nClearly, to do its religious and magical work, art did not have to be seen. Images placed high up on buildings – from the centaurs on the ancient Greek Parthenon to the gargoyles on medieval cathedrals – were hard for anyone on the ground to glimpse, yet they were believed to be powerful. A key moment in the emergence of the idea that art is to be looked at is a debate over where to place Michelangelo's David in Florence, Italy, in 1504: the statue was originally meant to go high up among the buttresses of Florence's cathedral, but it was instead placed at the heart of the city where everyone would see it properly.\nThe reason we find the idea of an \"invisible\" exhibition so bizarre is not because art has to be visible but because the secular idea of art that emerged in the Italian Renaissance (and is enshrined in galleries today) privileges the visual experience above all others. In the past, art that embodied ancestors or represented gods was often kept hidden away, too precious to be seen. Elements of this idea persist today. In Florence, again, a precious medieval painting of the Annunciation – created, it is said, by an angel guiding a monk's hand – is still kept in the church of Santissima Annunziata hidden behind an ornate screen. Only at times of prayer does a hidden mechanism raise the screen to reveal the painting to the faithful.\nArt of the invisible? It's a great idea – but not exactly new.","7 Mysteries of the Mona Lisa\nAs the most famous painting in the world, the Mona Lisa draws more than six million admirers to the Louvre each year. Just what is her peculiar power?\nMonda Lisa mystery #1: Who was Mona Lisa?\nOver the past century, it has been proposed that Mona Lisa was a noblewoman – Isabella d’Este, Marquise of Mantua, or Costanza d’Avalos, Duchess of Francavilla. Others have stared at that unsettling visage and seen the face of a man – Leonardo da Vinci himself, or the man who was for 20 years his assistant (and perhaps his lover), Gian Giacomo Caprotti. There is even a theory that the picture may have started out as a portrait from life but, over the years that Leonardo worked on it, evolved into an abstract vision of the feminine ideal.\nThese days, most experts agree that the Mona Lisa is a portrait of Lisa Gherardini del Giocondo, wife of a Florentine silk merchant named Francesco del Giocondo (hence the name by which she is known in Italy and France, La Gioconda, or La Joconde). When she sat for Leonardo da Vinci, in around 1503, she was about 24 years old. Her contrapposto pose – with the body angled away from the viewer, head turned forward – was widely admired and copied by Leonardo’s contemporaries. And his sfumato technique, where sharp edges are blurred to create an uncannily lifelike effect, was seen as a brilliant technical innovation, very unlike the slightly frozen human figures of earlier, lesser painters.\nMona Lisa mystery #2: The hidden initials\nIn 2010, Silvano Vinceti, chairman of Italy’s National Committee for Cultural Heritage, claimed to have discerned letters minutely painted on Mona Lisa’s eyes: L and V (Leonardo da Vinci’s initials) in the right eye, and perhaps C, E or B in the left. The Louvre responded that Vinceti’s letters were simply microscopic cracks in the paint.\nMona Lisa mystery #3: The broken backdrop\nThe distant, dreamlike vista behind Mona Lisa’s head seems to be higher on the right-hand side than on the left. It is hard to see how the landscape would join up. This is subliminally unsettling: Mona Lisa appears taller, more erect, when one’s gaze drifts to the left than when it is on the right.\nMona Lisa mystery #4: The bewitching smile\nIn 2000, scientists at Harvard University suggested a neurological explanation for Mona Lisa’s elusive smile. When a viewer looks at her eyes, the mouth is in peripheral vision, which sees in black and white. This accentuates the shadows at the corners of her mouth, making the smile seem broader. But the smile diminishes when you look straight at it. It is the variability of her smile, the fact that it changes when you look away from it, that makes her seem so alive, so mysterious.\nMona Lisa mystery #5: The unknown bridge\nThe Mona Lisa’s background landscape seems unreal, but the bridge might be one that Leonardo knew. It is usually said to be Ponte Buriano in Tuscany, but in 2011, a researcher claimed it depicts the Bobbio Bridge over the Trebbia, which was washed away in a flood in 1472.\nMona Lisa mystery #6: Da Vinci’s obsession\nLeonardo da Vinci worked on the painting for four years, and possibly at intervals after that. He always took it with him when he travelled, and he never signed or dated it. The picture went with him when, towards the end of his life, he moved to France.\nIt was sold to his last patron, King François I, and remained out of sight in the royal collection for almost 200 years. In 1799 Napoleon came across the painting and commandeered it for his bedroom. Only in 1804 did the Mona Lisa go on public display – in the newly founded Louvre Museum.\nAt that time, it was not seen as particularly interesting, but in the middle of the 19th century Leonardo’s stock as an artist slowly rose. He came to be seen as the equal of the two acknowledged Renaissance greats, Michelangelo and Raphael. This new interest in Leonardo as a painter drew attention to his few known works.\nMona Lisa mystery #7: Was Mona Lisa unwell?\nMona Lisa has often been scrutinised by medical experts. In 2010, an Italian doctor looked at the swelling around her eyes and diagnosed excess cholesterol in her diet. Other conditions ascribed to her include facial paralysis, deafness, even syphilis.\nMore happily, it has been suggested that the look of contentment on her face indicates she is pregnant. Dentists have also posited bruxism, compulsive grinding of the teeth; or that the line of her top lip suggests that her front teeth are missing – which, along with the faintest hint of a scar on her lip, raises the possibility that she was a victim of domestic violence.\nJungians have seen her as an accomplished representation of the anima, the female archetype that resides in each one of us. It seems that almost any condition can be read into that puzzling face.\nFrom Great Secrets of History © 2012. The Reader’s Digest Association, inc.\nThis article originally appeared on Reader’s Digest\nImages: Reader’s Digest"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:1ff332f0-c3af-4775-bb86-00652cc5b6b7>","<urn:uuid:396e3ecf-6577-4b32-b125-2aa6b07b7961>"],"error":null}
{"question":"What role did speeches play in shaping American democracy in the 20th century, and what essential speaking techniques were used to achieve this impact according to Churchill's framework?","answer":"Major speeches played a crucial role in American democracy - JFK's 1961 inaugural address represented optimism and featured memorable chiasmus like 'ask not what your country can do for you,' while MLK's 'I Have a Dream' speech was pivotal for civil rights. According to Churchill's analysis, these influential speeches succeeded by following key principles: using clear, simple language that precisely expressed meaning, building arguments in waves toward clear conclusions, employing effective analogies to help audiences connect with complex ideas, and utilizing dramatic imagery and symbolism (what Churchill called 'Wild Extravagance') to make emotional appeals that could become rallying cries for movements and shape national dialogue.","context":["“According to most studies, people’s number one fear is public speaking. Number two is death. Death is number two. Does that sound right? This means to the average person, if you go to a funeral, you’re better off in the casket than doing the eulogy.”\nThat great joke by comedian Jerry Seinfeld sums up how many of us feel about having to give a speech in public –scared to death of it.\nHowever, there are many people out there who seem like they were born to deliver public speeches. Their gift of elocution and the message contained within it was so strong that we’re still talking about their speeches today – more than 100 years later, in some cases.\nWhether you’re going to deliver a key note speech at a business conference, or a best-man/bridesmaid speech at a wedding, there are things here that we can all learn from. Here are eight of the most captivating speeches ever.\nMartin Luther King, Jr.’s ‘I have a dream’\nConsidered a defining moment of the civil rights movements, Martin Luther King, Jr. helped to shape modern America with the ‘I have a dream’ speech. It was delivered in front of 250,000 civil rights supporters at the Lincoln Memorial on August 28, 1963.\nThe most quoted part of the speech – which would then be applied as the name of the speech in its entirety – wasn’t even planned. King, Jr. went off-script after an audience member urged him to “Tell them about the dream, Martin.”\nWinston Churchill’s ‘Blood, toil, tears and sweat’\nChurchill’s first speech as Prime Minister came on May 13, 1940, during the first year of World War II. With a nervous nation and an unhappy conservative party listening in, Churchill aimed to settle an entire nation’s nerves, and with this speech he cemented himself as a great leader.\nHistorian Robert Rhodes James described the feeling in the room as “electrifying”, and it was a feeling Churchill would replicate in many future speeches to come.\nJohn F. Kennedy’s Inaugural Address\nHe might sound like Mayor Quimby from The Simpsons to younger generations, but JFK’s inauguration speech on January 20, 1961 will always be remembered; both as a sign of optimism in American history, and as the first inaugural address televised in colour.\nThe speech itself was written by both Kennedy and his writer Ted Sorensen, and its clever use of chiasmus had a long-lasting impact on listeners: “…ask not what your country can do for you, ask what you can do for your country.”\nEmmeline Pankhurst’s ‘Freedom or death’\nPankhurst was at the forefront of the British suffragette movement, helping women win the right to vote with powerful lines like: “We will put the enemy in the position where they will have to choose between giving us freedom or giving us death.”\nIt was an explanation as to why women in Britain had to turn to aggressive means to get results, and although the speech was actually delivered in Hartford, Connecticut, on a fundraising tour of the US, her words were heard loudly back home.\nAbraham Lincoln’s Gettysburg Address\nPresident Lincoln’s self-penned speech took place at the Soldier’s National Cemetery in Gettysburg, Pennsylvania, on November 19, 1863. It addressed the human equality proposition from the Declaration of Independence. The civil war, and slavery, was still in full force; how then, Lincoln argued, could all men be created equal?\nIt took just two minutes to be delivered, and was instantly recognised for its importance. Abraham Lincoln couldn’t have been more wrong when he said: “The world will little note, nor long remember what we say here.”\nMargaret Thatcher’s ‘The lady’s not for turning’\nSay what you will about her politics – the Iron Lady certainly had a way when it came to orating. Her speech at the Conservative party conference in Brighton on October 10, 1980 – one year after being elected the UK’s first and only female Prime Minister – is a great example.\nThatcher’s delivery of playwright Sir Ronald Millar’s words became a crucial part of political development in the 1980s. The speech was so captivating that she received a five-minute standing ovation afterwards.\nSusan B. Anthony’s ‘Women’s rights to the suffrage’\nSusan B. Anthony was arrested and fined $100 for casting a vote in the 1872 presidential election (an illegal act for women at the time). This led to her delivering the rousing ‘Women’s rights to the suffrage’ speech, and refusing to pay the fine.\nShe became known as the ‘Napoleon of the women’s rights movement’, and also fought tirelessly for civil rights. Her speech concludes: “Every discrimination against women in the constitution and laws of the several states is today null and void, as is every one against Negroes.”\nWinston Churchill’s ‘The few’\nConceived during preparation for an expected German invasion, Churchill’s second entry in this list aimed to inspire all Brits. At this point in the war, many thousands of RAF pilots and British civilians had already died or been wounded. ‘The few’ was given on August 20, 1940.\n“The gratitude of every home in our Island, in our Empire, and indeed throughout the world, except in the abodes of the guilty, goes out to the British airmen who, undaunted by odds, unwearied in their constant challenge and mortal danger, are turning the tide of the World War by their prowess and by their devotion. Never in the field of human conflict was so much owed by so many to so few.”\nSo, when you’re writing your speech and preparing your presentation, just think back to these eight examples for inspiration. You’ll have the room captivated from the off.","Sir Winston Leonard Spencer-Churchill, famed British Prime Minister during World War II, was not only a noted statesman, but also a gifted student of oration and history.\nChurchill wrote numerous pieces on history, the English language, and how to develop the skills necessary to develop a mastery of rhetoric. So gifted was Churchill that he was awarded the Nobel Prize for Literature in 1953:\n“…for his mastery of historical and biographical description as well as for brilliant oratory in defending exalted human values.”\nChurchill was a lifelong student of the study of rhetoric and the art of public speaking. He was so talented that phrases and imagery that he used last to this day.\nChurchill’s speech, “The Sinews of Peace,” in which he invokes the imagery of “an Iron Curtain” to describe the descent of Communism that was dividing Europe in the aftermath of World War II, is perhaps the most notable example of his mastery of rhetoric and effective communication.\nBut before Churchill was Prime Minister, before politics and war, Churchill wrote what he believed were the 5 principle elements of effective persuasive speaking. They were collected in an unpublished essay entitled: “The Scaffolding of Rhetoric” in 1897.\nChurchill pulled these elements from a study of classical Greek works to Shakespeare and Lincoln (who was himself a masterful student of rhetoric). These elements remain as true and as effective as ever. You need to master them if you want to find success as a public speaker.\nI. Correctness of Diction\nThere is no more important element in the technique of rhetoric than the continual employment of the best possible word. Whatever part of speech it is it must in each case absolutely express the full meaning of the speaker. It will leave no room for alternatives…\nThe unreflecting often imagine that the effects of oratory are produced by the use of long words. The error of this idea will appear from what has been written…All the speeches of great English rhetoricians–except when addressing highly cultured audiences–display an uniform preference for short, homely words of common usage–so long as such words can fully express their thoughts and feelings….\nSelection of language is of paramount importance. When speaking, or writing, make sure each word carefully selected to convey your exact meaning.\nChurchill notes too that flowery and verbose language actually detracts from attempts to directs communicate and persuade. The focus of communication should be what you are trying to convey – don’t detract from that by muddling the message with complicated prose.\nThe great influence of sound on the human brain is well known. The sentences of the orator when he appeals to his art become long, rolling and sonorous. The peculiar balance of the phrases produces a cadence which resembles blank verse rather than prose. It would be easy to multiply examples since nearly every famous peroration in the English language might be quoted.\nEffective public speaking has a tempo all its own. As Churchill notes, any famous text or quote that you know by heart likely flows along its own unique rhythm.\nLyricism in language is a lost art in most of the modern world. But mastery of the establishment of rhythm in your speech and writing will lead to easier digestion by its audience.\nIII. Accumulation of Argument\nThe climax of oratory is reached by a rapid succession of waves of sound and vivid pictures. The audience is delighted by the changing scenes presented to their imagination. Their ear is tickled by the rhythm of the language. The enthusiasm rises. A series of facts is brought forward all pointing in a common direction. The end appears in view before it is reached. The crowd anticipate the conclusion and the last words fall amid a thunder of assent.\nKnow where you are going. You have to be headed somewhere – and your audience should pick up on it. Effective communication builds like a crescendo in music – constantly growing and building upon itself, working its way to a grand finale.\nBy the time the finale is reached it should be palpable to the audience. They know it is coming and are eager for the experience.\nThe ambition of human beings to extend their knowledge favours the belief that the unknown is only an extension of the known: that the abstract and the concrete are ruled by similar principles: that the finite and the infinite are homogeneous. An apt analogy connects or appears to connect these distant spheres. It appeals to the everyday knowledge of the hearer and invites him to decide the problems that have baffled his powers of reason by the standard of the nursery and the heart.\nHelp people develop connections. A well developed analogy – “an Iron Curtain” – can be more powerful than thousands of words.\nThe most effective communicators are masters of analogy. A good analogy makes the foreign, familiar and the clouded, clear. A well tuned analogy may win over a more technically sound argument because while theory and logic should prevail, most people are prone to be swayed by emotional appeal instead.\nV. Wild Extravagance\nA tendency to wild extravagance of language–to extravagance so wild that reason recoils is evident in most perorations. The emotions of the speaker and the listeners are alike aroused and some expression must be found that will represent all they are feeling. This usually embodies in an extreme form the principles they are supporting. Thus Mr. Pitt wishing to eulogise the freedom possessed by Englishmen:\n“The poorest man may in his cottage bid defiance to all the forces of the Crown. It may be frail; its roof may shake: the wind may blow through it; the storms may enter, the rain may enter–but the King of England cannot enter! All his forces dare not cross the threshold of the ruined tenement.”\n…The effect of such extravagances on a political struggle is tremendous. They become the watchwords of parties and the creeds of nationalities. But upon the audience the effect is to reduce pressure as when a safety valve is opened. Their feelings are more than adequately expressed. Their enthusiasm has boiled over…\nMake an impact. Extending the power of analogies to appeal to an audience’s emotions, Churchill notes that such an appeal is best completed with a flourish of over-the-top imagery and outrageous symbolism.\nIn Churchill’s example, Mr. Pitt notes that so strong is the rule of law and freedom in England that despite the disparity in prestige and power between a poor man and the King – both have equal rights and liberty in the eyes of the law. Such an extreme comparison hammers in the point about the power of the rule of law far more powerfully than a treatise on the topic might.\nFollow these straightforward rules and you’re on the path to becoming an effective public speaker."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:243512ba-20a0-4ede-b23e-952fad0ac83f>","<urn:uuid:dec14a1c-ba4a-4b04-b4d0-54cea6187cdb>"],"error":null}
{"question":"Compare the wildlife found on Mount Olympus vs Denali National Park - list the key animals mentioned for each.","answer":"On Mount Olympus, the wildlife includes: chamois, deer, wildcats, stone martens, red foxes, squirrels, and 108 species of birds (especially birds of prey). In Denali National Park, the wildlife includes grizzly bears, wolves, caribou, and bighorn sheep.","context":["Mount Olympus - The Hiking Paradise at the Home of the Gods\nMount Olympus is one of the most popular hiking and mountaineering destinations in Europe. It offers an adventurous trip to the legendary home of the Greek gods to both beginners and experienced hikers. On their way up, travellers experience the region's diverse and breathtaking nature.\nGeography - Greece's highest mountain range\nWith a height of 2,918 metres, Mount Olympus is the highest mountain range in Greece. It consists of the Mytikas (2,918 m), Skolio (2,911 m), Stefani (2,909) and Skala (2,866 m). Altogether there are 52 peaks. Mount Olympus lies in the north of the country on the border between Macedonia and Thessaly. Towns and villages near Mount Olympus are Litochoro, Katerini, Leptokaria and Panteleimonas. Mount Olympus is only 20 kilometres away from the sea. The climate is mild to hot in summer along the coast, while it is harsh in the mountains, especially in winter.\nNature - A unique flora and fauna\nThe massif has special geological features as well as its own flora and fauna. It became a protected area in 1938 and was declared a biosphere reserve by the UNESCO in 1981. The higher rock strata are characterised by a particularly rich flora and fauna. They are home to 30 types of orchids, gentian, boxwood and Bosnian pines as well as a variety of rare and endangered animal species. Among the numerous mammal species in this region you find chamois, deer, wildcats, stone martens, red foxes and squirrels. There are about 108 bird species, especially birds of prey, most of which are protected. The rivers and lakes are populated by a number of reptiles and amphibians, which are characteristic of Greece, as well as countless insects such as butterflies and horseflies.\nCulture - Following the traces of the gods\nThe origin of the name of \"Olympus\" is not clear but this is not the case with the mountain's significance. Mount Olympus is the home of the gods in Greek mythology. The father of the gods, Zeus, lived on the summit, a light-flooded place, with eleven other gods in twelve heavenly palaces. The peak of Mytika was first climbed in 1913. A special sight is Greece's highest situated chapel on the peak of Profitis Ilias. At the edge of Mount Olympus, you see the beautiful village of Litochoro, which is characterised by its old town with scenic alleys and the church on the village square. Many hikes and mountaineering tours start here. Dion lies at the coast, at the foot of Mount Olympus. At the edge of the village you see an excavation site with finds from an ancient city from 5 and 4 BC. The city was once the cultural centre of ancient Macedonia. In the Archaeological Museum in Dion, travellers can marvel at finds such as mosaics and clay mobiles on three storeys. A special highlight are the four sitting philosophers of the House of Dionysos. About an hour's drive away, near Kondariotissa, you find the modern Monastery of Osios Ephraim. It was built in Byzantine style combined with modern architecture. Today it is one of the greatest nunneries in Greece.\nExperience - Enjoying Greek mentality and food\nVisitors of Mount Olympus enjoy a unique flora and fauna on their way up the mountain range. The Olympus is the favourite destination of many travellers of Greece who want to be active. A prerequisite is that you are in good physical condition. Visitors should definitely not underestimate the mountain or climb it without appropriate hiking gear. Besides a good state of health, you need sturdy shoes and warm clothes. If you follow this advice, you will experience a wonderful adventure. Visitors who do not dare to ascend the majestic mountain but want to explore the region's beautiful landscape can go on a tour through the forests in the lower areas of the Olympus National Park by car. Travellers who want to get to know authentic Greece should visit the mountain village of Litochoro. You experience the Greeks' typical culture, mentality and hospitality at first hand in its taverns, especially in the evening. The taverns serve dishes such as Greek salad, souvlaki and moussaka, which are characteristic of Macedonia. They are accompanied by a glass of Greek wine and ouzo.\nActivities - A paradise for adventurers\nMount Olympus is the best-known mountain range in the whole of Greece and is popular with hikers and mountaineers from all over the world. You can hike from hut to hut and stay overnight. There are several mountain cabins and shelters on the way to the peak, for example Stavros, Christos Kakalos and Josos Apostolidis. The best equipped hut is Spilios Agapitos. Most huts are opened from June to the end of September, which is why these months are also the best time for travelling around Mount Olympus. Most hiking trails start in the villages of Litochoro, Dion and Petra. The E4 European long distance path is highly recommended. It starts in Litochoro and runs through the Enipeas Gorge to the peak of Mount Olympus. Only experienced mountaineers should climb the highest peaks.\nYou can easily reach Mount Olympus by plane via the airport in Thessaloniki (SKG), which is only one hour away. From there, you can continue your journey by car on the well-developed roads or the motorway. In addition, travellers can arrive by train going from Thessaloniki to Athens. The nearest stations are located in Katerini and Leptokaria. The people in Greece speak Greek, of course, but you can also communicate in English.\nA trip to Mount Olympus means conquering spectacular hiking trails, experiencing Greek mythology and culture at first hand and being mesmerised by a rich flora and fauna.","What Is The Highest Mountain Peak In North America – Denali is the highest mountain in North America at 19,685 feet or 6,000 meters. The next three highest peaks exceed 5,500 m, while 21 peaks exceed 4,500 m. Of the 200 highest peaks in North America, 160 are in the United States and 30 are in Canada. Mexico has the most peaks on the continent with 11 of the next 200 peaks. Seven of the 200 highest peaks in North America are located on the border between the United States and Canada\nLocated in eastern Alaska, Mount Bona is the fifth highest mountain in the United States and the 10th highest on the continent. The mountain is an ice-covered stratovolcano and the country’s highest volcano, Mount Boa is the source of the Clotlan Glacier and other smaller glaciers It also supplies ice to the Russell Glacier system The first successful ascent of Mount Boa was in 1930\nWhat Is The Highest Mountain Peak In North America\nKing Peak, the ninth highest mountain in North America, was first climbed by several students from the University of Alaska in Yukon, Canada on June 6, 1952.\nHas Anyone Really Summited The World’s 14 Highest Mountains?\nIztachihutl, a 17,160-foot-tall dormant volcano, is located in Mexico. This mountain is located on the border of the Mexican state of Puebla, Mexico. The four peaks of Iztachihutl are snowy and on a clear day the mountain can be seen from Mexico City, which is 70 km away from the mountain. Iztachihut was first climbed to the top in 1889. But there is an assumption that local residents have climbed the mountain before.\nMount Lucania is the seventh highest mountain in North America This mountain is located in Yukon, Canada It is connected to Table Mountain by a long ridge Mount Lucania is 17,257 feet high and is named after the ship RMS Lucania. It is the third highest peak in Canada and the 15th highest peak on the continent.It was first climbed in 1937.\nMount Foraker is a 17,400-foot mountain located in Denali National Park in Alaska. The northern peak of the mountain was first climbed on August 6, 1934, and the southern peak was conquered four days later. The mountain was named after US Senator Joseph B. Foraker.\nPopocatepetl, an active volcano, is located in Mexico and has an elevation of 17,802 feet. It is the fifth highest peak in North America and the second highest peak in Mexico.The mountain is located about 70 kilometers from Mexico City and is often visible from the city.\nBlack Mountains (north Carolina)\nLocated on the border between the US state of Alaska and the Canadian Yukon Territory, Mount St. Elias is located about 40 kilometers southwest of Mount Logan, the second highest peak in both countries. On July 16, 1741, Mount St. Elijah was first discovered by the Russian Vitus Bering. The mountain rises to an elevation of 18,008 feet The mountain was first climbed on July 31, 1897 by Prince Luigi Amadeo di Savoia and his team.\nThe highest mountain in Mexico and the second highest volcanic peak in the world, Pico de Orizaba is the third highest peak in North America. The dormant stratovolcano rises 18,491 feet above sea level. Mount Pico de Orizaba, located in the eastern part of the Trans-Mexican Volcanic Belt, last erupted in the 19th century.\nLogan, the second highest peak in North America, at 19,551 feet The mountain is located in the Kluane National Park Reserve in the Yukon Territory of Canada and is the highest mountain in the country. Mount Logan is named after Canadian geologist William E. Logan. Logan and Hubbard Glaciers Begin at Mount Logan Due to tectonic movements, the mountains are in a state of growth and gain height over time.\nMount Denali is the highest peak on the North American continent The mountain rises to a height of 20,310 feet The mountain is famously the third highest peak in the world Mount Denali, the central feature of Denali National Park in Alaska, is also the third most isolated peak in the world. Denali was first climbed on June 7, 1913 by four climbers who completed the South Summit route. Currently, the most popular route to the summit of Denali is the West Buttress Route, first used by Bradford Washburn in 1951. The United States is home to a variety of spectacular landscapes, including some of the highest and tallest mountains in North America.\nAlaska Visit Denali National Park To See Stunning Array De Wildlife Y Gaze En Mt Mckinley La Highest Mountain Peak In North America With An Elevation De Pies Mount Mckinley Y Denali\nWhen it comes to mountains, no other state has peaks like Alaska Because of their remote location in the Last Frontier, some of these mountains are rarely visited, but they retain their awe-inspiring status as distinctive features of Alaskan geography.\nOf the 11 highest mountains in the United States, 10 are in Alaska, the last being Mount Whitney, which is part of the Sierra Nevada mountain range in California.\nDenali is the tallest mountain in the United States at 20,310 feet (6,190 meters). Denali is also the highest mountain in North America and the third highest of the Seven Summits (short for the highest mountain on each continent), after Mount Everest and Mount Aconcagua.\nLocated in the Alaska Range, Denali and the surrounding mountains were formed as a result of intense tectonic activity, and the height of the mountain is gradually increasing every year.\nThe Mount Mitchell Hike: The Highest Mountain In North Carolina\nThis feature was formed as a result of the convergence of the Pacific and North American tectonic plates, which caused the earth’s crust to rise in this area.\nAt about 18,000 feet (5,500 m) above Everest, Denali is one of the highest base climbs on Earth.\nDenali is the Athabaskan word koukon, meaning “high,” and the mountain has been of great importance to the Alaska Natives who have lived around it since ancient times.\nThe mountain is the centerpiece of Denali National Park and Preserve, a protected area of more than 5 million acres of wilderness that is home to a variety of wildlife, including grizzly bears, wolves, caribou, and bighorn sheep.\nThe Highest Mountains In Europe: The Top 10\nAdditionally, Denali National Park and Preserve is one of Alaska’s most visited destinations, attracting tourists from around the world during the summer months. Many of the park’s wildlife can be seen on the park’s unique trails\nMount St. Elias crosses the Alaska-Canada border in the St. Elias Range in northeastern Alaska. At 18,009 feet (5,489 m), it is the second highest mountain in the United States and Canada, after Mount Logan.\nMount St. Elias is part of the Mount St. Elias Range, part of the Pacific Coast Range that stretches from southeastern Alaska through the Yukon to northern British Columbia.\nMount St. Elias, Wrangel-St. Elias National Park and Preserve, the largest national park in the United States, is home to nine of the 16 highest peaks in the United States.\nUtah Mountain Ranges [maps & Recreation]\nThe mountain range is located near Glacier Bay, where there is a high density of glaciers and ice fields, and tectonic and volcanic activity, which has helped make the mountain range popular for current and past research.\nMount St. Elias, Lake Agassi, the confluence of the Libby Glacier and the Agassi Glacier. Photo: NPS/J. Frank is in the public domain.\nMount St. Elias saw its first recorded summit in 1897 by an Italian expedition. On July 31, 1897, an Italian team led by Prince Luigi Amedeo, Duke of Abruzzi, accompanied by American guides and climbers.\nMount Foraker is the third highest peak in the United States at 17,400 feet (5,304 m).\nDenali Claims To Be Tallest Mountain In World\nMount Foraker, located in the central Alaska range of Denali National Park and Preserve, is 14 miles from Denali, the highest peak in North America.\nMount Foraker also sits on a branch of the Kahiltna Glacier, the longest glacier in the Alaska Range, opposite Mount Denali and Mount Hunter.\nLook at the Cahiltana Glacier with the Crosson and Foraker Mountains on the right.In the center of the photo is the confluence of the main Cagiltana and the Northeast Branch, which is a larger opening in the lower left. Photo: NPS/Tucker Chenoweth, public domain\nThe first successful ascents of the North and South Peaks were made in 1934 by Charles Houston, T. Recorded by Graham Brown and Chichelle Waterston.\nThrough The Lens: Mount Denali, North America’s Highest Peak, In Photos\nMount Bona is a dormant stratovolcano located in the St. Elias Mountains. Mount Bona is located in the eastern part of the Alaska Range in Wrangel-St. Elias National Park and Preserve, the largest national park in the United States.\nAt 16,550 feet (5,\nWhat's the highest peak in north america, highest mountain peak in america, highest mountain peak in north america, the highest mountain peak in north america, what is the highest peak in north carolina, what is the highest peak in alaska, what is the highest peak in north america, what is the highest mountain peak, what is the world's highest mountain peak, highest peak in north america, what is the highest peak in europe, what is the highest mountain peak in south america\nHello my name is Sophia, Wellcome to my blog"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:028a151f-61bb-4980-b9ee-62583b64b8e0>","<urn:uuid:1a9b6acc-c490-4323-a136-1b38847d08e6>"],"error":null}
{"question":"How do computer ergonomics and general ergonomic safety guidelines compare in their approach to preventing wrist-related injuries?","answer":"Both sets of guidelines focus on proper wrist positioning to prevent injuries, but approach it from slightly different angles. Computer ergonomics specifically emphasizes wrist support for the keyboard, maintaining neutral wrist position while using the mouse, and positioning the keyboard so arms are parallel to the floor to prevent carpal tunnel syndrome. The general ergonomic safety guidelines address this as part of broader ergonomic hazards, identifying that improper work methods and workstation design can lead to musculoskeletal injuries like carpal tunnel syndrome and tennis elbow, suggesting adjustable and detachable keyboards as part of the solution.","context":["If you work at a computer all day, you know that it can start to take a toll on the body. The eyes strain, the wrists ache, and slouching can lead to poor posture and all the problems associated with that.\nWhat are computer ergonomics?\nComputer ergonomics seek to alleviate these stress conditions by focusing on a number of different things, including:\nWrist support for the keyboard\nKey board angle\nChair height and type\nLighting in the room\nPlacement of documents, phones, and other frequently used items\nEach of these things, and more, have a direct affect on your comfort and the overall stress to your eyes, joints, wrists, and other body parts as you spend prolonged periods of time in front of the computer. Computer ergonomics deals with the placement and set up of desks, computers and computer equipment, and lighting so that it minimizes stresses and risks.\nThe Rules for Computer Ergonomics\nThe following are some rules for computer ergonomics to help you maintain the safest positions while working at or using a computer:\nIf you are working at a computer, the way you sit and align your body will affect you. The ideal situation and set up would include:\nThe top of the monitor should be situated at or below eye level.\nHead and neck are balanced and in line with the torso, so you are sitting up straight in your chair and not hunched over the keyboard.\nThe elbows are supported and close to the body, with the wrists and hands in line with the forearms.\nLower back should be supported in an ergonomic chair or with a pillow. Stools should be avoided when sitting at a computer.\nThe feet should be flat on the floor.\nPlacement of Components\nWhere the components of your desk and computer are placed is also important to your overall health.\nComputer monitor. Ideally, your monitor should be situated so you don’t have to strain your neck and lean forward to look at it. You don’t want to have it too close or too far from you, as this can result in blurred vision. In addition, it should be placed in an area where there is sufficient lighting and reduced glare.\nKey board: The placement and support of the keyboard are also important, as continual typing puts you at increased risk for carpal tunnel syndrome. Your keyboard should be placed so that your arms are parallel with the floor.\nComputer mouse: The mouse should be placed so that it allows for a straight, neutral wrist position. You can use a mouse pad with a special wrist support so you can keep this neutral position.\nDesk: Your desk should have sufficient space for your computer, mouse, phone, documents, and anything else so you are not cramped or overcrowded. The desk should also be made so a chair can fit comfortably under it, and your knees are not hitting drawers or supportive structures for the desk.\nChair: Your chair should allow you to be level with the computer screen while at the same time providing sufficient support for your back.\nComputer ergonomics allow you to work with a computer more comfortably. The above rules will help you to follow computer ergonomic guidelines so you will have the least amount of stress on your eyes and joints as possible.","What is Ergonomics?\nThe term “ergonomics” can simply be defined as the study of work. It is the science of fitting jobs to the people who work in them. Adapting the job to fit the worker can help reduce ergonomic stress and eliminate many potential ergonomic disorders (e.g., carpel tunnel syndrome, trigger finger, tendonitis). Ergonomics focuses on the work environment and items such as the design and function of workstations, controls, displays, safety devices, tools and lighting to fit the employee’s physical requirements, capabilities and limitations to ensure his/her health and well being. It may include restructuring or changing workplace conditions to reduce stressors that cause musculoskeletal disorders (MSDs).\nWhat are ergonomic hazards?\nErgonomic hazards refer to workplace conditions that pose the risk of injury to the musculoskeletal system of the worker. Examples of musculoskeletal injuries include tennis elbow (an inflammation of a tendon in the elbow) and carpal tunnel syndrome (a condition affecting the hand and wrist). Ergonomic hazards include repetitive and forceful movements, vibration, temperature extremes, and awkward postures that arise from improper work methods and improperly designed workstations, tools, and equipment.\nWho do I contact if I want an ergonomic evaluation performed on my workspace?\nContact the Environmental Health and Safety at 773.702.9999 to request an ergonomic evaluation or send an email to firstname.lastname@example.org. Please note that a request from your Physician is required for Environmental Health and Safety to conduct an ergonomic evaluation of an employee's workstation.\nI am renovating my area and purchasing new furniture, can someone review the furniture?\nA workstation outfitted with the proper furniture and equipment can lead to a more comfortable and safer work environment. Ergonomic injuries occur at workstations due to reaching, bending, awkward postures and applying pressure or force. If workstations are designed properly, most ergonomic hazards can be reduced if not eliminated.\nPrior to purchasing furniture or workstation equipment due to renovations of existing space or new construction consult Environmental Health and Safety to have the specifications reviewed and to provide guidance.\nWhat are the basic guidelines for setting up a computer workstation correctly?\nWorkstations that include video display terminals (VDTs) should be ergonomically designed for both computer and non-computer work. VDT workstations should be adjustable so users can easily change their working postures and equipped with the following:\n- Adjustable and detachable keyboards;\n- Display screens that tilt up and down;\n- Brightness and contrast controls;\n- Flexible copy-holders that reduce the distance between the screen and source material; and\n- Proper lighting and anti-glare filters should be installed to prevent glare from the VDT screen. VDTs should be placed in the workspace in such a way as to minimize or diminish glare.\nWhat are the basic criteria for an ergonomically designed chair?\nWhen an employee spends six to eight hours in the chair, the height of the chair and the work surface are critical. The human body dimension that provides a starting point for determining correct chair height is the “popliteal” height. This is the height from the floor to the crease behind the knee. The chair height is correct when the entire sole of the foot can rest on the floor or a footrest and the back of the knee is slightly higher than the seat of the chair. This allows the blood to circulate freely in the legs and feet.\nArmrests: Armrests should be large enough to support most of the lower arms but small enough so they do not interfere with chair positioning. Armrests should support your lower arms and allow your upper arms to remain close to the torso and made of soft material and have rounded edges.\nBackrests: Backrests should support the entire back including the lower region. The seat and backrest of the chair should support comfortable postures that permit frequent variations in the sitting position. The backrest angle should be adjustable but lock into place or have a tension adjustment.\nSeats: Seat pans should be height adjustable and have a user adjustment for tilt. Note: “users adjustment for tilt” is defined as any method of activation the movement of the seat pan/backrest. This can be either through the use of manual devices (e.g. levers, knobs, adjustments) or by movement of the body/body weight. Seat pans should be padded and have a rounded, “waterfall” edge, wide enough to accommodate the majority of hip sizes.\nBase: Chairs should have a strong, five-legged base and casters that are appropriate for the type of flooring at the workstation."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:0b96d3cd-f377-4e26-b5e5-75a615afb60b>","<urn:uuid:2b799cdd-8569-4b19-92b7-19f7531c0915>"],"error":null}
{"question":"How different are dance education paths + what safety measures needed for injury prevention in professional training?","answer":"Dance education paths differ significantly in their focus: pre-professional programs emphasize performance and company training, conservatories offer intensive arts education with BFA degrees, and college programs provide broader liberal arts education with dance focus. To prevent injuries across these paths, several safety measures are essential: wearing properly fitted shoes (especially for pointe work), not attempting pointe before age 11-12, ensuring proper nutrition, gradually increasing training intensity, avoiding hard and uneven surfaces, alternating between different types of training, and stopping activity when experiencing pain. These precautions are crucial as ballet can cause various injuries including stress fractures, ankle sprains, and chronic foot conditions.","context":["Dance after High School: Pre-Professional, Conservatory or College Program?\nSerious dancers have choices to make to pursue their passion to a professional career. The differences in these three choices primarily reflect the amount of focused study in dance to the inclusion or exclusion of other subject areas.\nPre-professional programs (sometimes called trainee or apprentice program) at a professional ballet company school, for instance, will usually comprise a day beginning with ballet technique class(es) followed by rehearsals either with a second company or the main company in the corps or ensemble roles. Classes in partnering, variations, modern, jazz, choreography, etc. may also be included. Students are generally in their last few years of high school (and work to finish their high school courses) or just graduated from high school. Training and performance is emphasized and the opportunity to be seen by and perform with professionals leading to the possibility of being hired. It may be advisable to go through the college application process and then defer admittance to pursue this path for a year or two. It is very important to know the focus of the program/company in terms of repertoire and style and how the faculty teach technique class as study will generally be with only 2-3 teachers. There is generally a high level of competition to be accepted into these programs.\nConservatory programs offer BFA degrees (Bachelor of Fine Arts) and focus strongly on dance and related arts with relatively few required courses in academic, non-arts. These are generally self-contained institutions such at The Juilliard School or North Carolina School of the Arts where all the students art studying one art form very seriously with exposure to and study of the other art forms. There is often the opportunity for students to study composition and choreograph. The faculty is generally comprised of professionals who work/worked extensively in their field. There is generally a high level of competition to be accepted into the premiere programs.\nLiberal Arts College programs offer BA and sometimes BFA and MA/MFA degree programs in dance. The focus is on providing a liberal arts education through a required core curriculum of academics with a focus on dance. The first 2-3 semesters (depending on AP credits from high school) include these core academics alongside dance and arts requirements. While there are a few programs with distinct major programs in a Ballet and a Modern Dance Department, most colleges and universities have one Dance Department and offer training in both ballet and modern and other dance forms as well as related arts. Many programs offer related majors, minors or certifications such as in Arts Administration or Pilates; there is generally a wide array of elective courses within the department and performing arts to pursue a secondary interest. There is usually the possibility to study composition and choreograph as well as study pedagogy and student teach. There is the possibility at most programs to double major or minor in either a related art or an academic area although a double major is generally extremely demanding and often discouraged. Most but not all departments require an audition and the competitiveness to be accepted varies widely. The focus, training, faculty background and size, course offerings, performance opportunities, guest artists, and facilities also vary widely. It is very important to research and understand these factors at each department you are considering in relation to your goals.","Ballet dancing has been a popular art form for centuries, and for those who have never danced, it has tremendous appeal as either a hobby or a potential vocation.\nChildren—especially girls—are often fascinated by the apparently effortless grace with which dancers move, and by the colorful, intricate costumes they sometimes wear.\nBut ballet has a darker side, and the placid expressions worn by dancers often mask terrible pain caused by ballet injuries.\nThe careers of professional dancers are often short; before they turn 30, most of them retire from dancing, either to teach or to pursue some other, unrelated occupation.\nThe purpose of this article is not to discourage children or their parents from considering ballet lessons, or even to discourage anyone from pursuing ballet as a career; rather, our purpose here is to make the reader aware of the risks that ballet injuries can pose to their future well being.\nTypes of Ballet Injuries That Can Happen\nMany of the various injuries dancers can suffer are not severe, and pose no serious threat to the dancer’s future as a performer, or to his or her ability to participate in other sports or to walk with a normal gait.\nOther types of ballet injuries, however, can be cause for concern. As you read about the various types of ballet injuries, feel free to click on any of the related links to read more about them.\nInjuries and conditions that can be caused by ballet include:\n- Bunion or hallux valgus (sometimes at an unusually young age)\n- Ankle sprains (particularly later ankle sprains)\n- Stress fractures brought on by small, repetitive impacts over the course of time\n- Trigger toe\n- Shin splints\nDancer’s Fracture is the most common acute fracture suffered by dancers. It strikes the fifth metatarsal, the bone that lies along the outer edge of the foot. It usually happens when a dancer jumps and lands badly, coming down on an inverted (turned-in) foot. As the name “dancer’s fracture” suggests, this is a common ballet injury.\nSesamoiditis: When a dancer is on demi-pointe, her weight rests on the sesamoid bones, which lie just behind the big toe. Regular application of this kind of stress can cause gradual onset of sesamoiditis, and the dancer will experience pain when bending or straightening the big toe.\nPlantar fasciitis: This is an inflammation of the plantar fascia, a band of tissue running along the bottom of the foot from heel to toes. The injury is related to overuse.\nAnkle Impingement: Many variations of this condition exist, but the one that concerns us here is posterior ankle impingement syndrome, also known as os trigonum syndrome, and commonly referred to as “dancer’s heel.” This happens when soft tissue becomes trapped and pinched by bones, which causes painful spurs to form.\nAchilles Tendonitis: This condition can be brought on by dancing on a floor that is not properly “sprung,” or in other words is too hard. Overtraining can also cause Achilles tendonitis, especially hard training in a short time after a period of inactivity.\nCuboid Syndrome: Repetitive movement (such as one does when training) or sprains (another common ballet injury) can cause cuboid syndrome, a painful misalignment of the cuboid bone on the outer edge of the mid foot.\nMetatarsalgia: This condition generally manifests itself as pain in the ball of the foot, and in dancers it is often brought on by years of overwork and by forcing the foot into extreme positions, leading to instability in the joints of the toes.\nWhat Causes Ballet Injuries?\nThe particular causes of particular ballet injuries vary, of course, depending on the movement being performed. There are, however, certain techniques and types of footwear used in ballet that are particularly dangerous.\nPointe Technique: Dancing en pointe puts a great deal of stress on various parts of the foot and toes. Poor technique or poorly fitted shoes can make matters worse and increase the chances of injury.\nAlthough dancers who practice this technique wear shoes designed for it, dancing en pointe still causes friction between the toes and the shoes themselves, which can cause chaffing and blistering. En pointe dancing is the culprit in many cases of bunions, hammertoes, sesamoiditis, bursitis, trigger toe, and stress fractures.\nWhat Long-Term Complications Can Result from Ballet Injuries?\nThe long-term complications associated with ballet injuries are as varied as the injuries themselves, but the outlook for such injuries if they are not treated usually involves chronic pain and possibly antalgic gait (or in layman’s terms, a limp).\nHow You Can Prevent Ballet Injuries\nBallet injuries can be prevented by making sure that you are wearing properly fitted shoes, especially if you are dancing en pointe.\nThere are professionals you can see who know how to fit you properly for pointe shoes; ask your ballet instructor how to find one. Once you have your shoes, take proper care of them.\nDancers should not try dancing en pointe before the age of 11 or 12. By doing so, they can risk growth-plate injuries, and the bones of their feet may develop improperly.\nIndeed, no dancer should ever attempt a feat that he or she is not ready for. As in sports, proper training in dancing is crucial to injury prevention.\n- Ensure proper nutrition.\n- If you increase your training schedule, take it slowly; don’t suddenly go from once a week to every day.\n- If you spend most of one workout en pointe or demi-pointe, focus your next workout on something else.\n- Avoid dancing on hard surfaces.\n- Avoid dancing on uneven surfaces.\n- If you are in pain, STOP!"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:0ea36f52-eebe-41cd-bfad-f93c80eb623c>","<urn:uuid:4aea92b0-3025-42b6-847a-19af540fd1e8>"],"error":null}
{"question":"Can you compare the methods for finding new drug candidates between ligand deconstruction and chemical library screening, listing the key steps for each?","answer":"Both methods serve drug discovery but operate differently. Ligand deconstruction involves: 1) Starting with a known hit compound, 2) Breaking it down into component fragments, 3) Optimizing selected fragments, and 4) Developing improved chemical series. Chemical library screening involves: 1) Creating/maintaining collections of stored chemicals with associated data, 2) Screening compounds against drug targets, 3) Identifying and verifying initial hits, 4) Analyzing commonalities among active compounds, and 5) Optimizing the library by synthesizing similar compounds in the active chemical space.","context":["Ligand deconstruction is a strategy for early-stage drug discovery in which a known hit is dissected into component fragments and one or more of them is optimized. When successful, it can lead to new and improved chemical series. One such example was just published in J. Med. Chem. by Andreas Lingel and colleagues at Novartis.\nThe researchers were interested in finding inhibitors of the protein methyltransferase polychrome repressive complex 2 (PRC2). Although some drugs have entered the clinic against this anticancer target, all of these are competitive with the cofactor S-adenosylmethionine (SAM), and resistant mutants are already being detected. Thus, the team sought a molecule that would act through a different mechanism.\nPRC2 is actually a complex of four different proteins. The SET domain of the protein EZH2 contains the catalytic machinery, but a protein called EED stabilizes the protein complex and is necessary for activity. EED also recognizes trimethylated lysine residues on histone substrates, allosterically activating methyltransferase activity.\nA high-throughput biochemical screen identified compound 1, which has low micromolar activity and is noncompetitive with the SAM cofactor and substrate peptide. Subsequent NMR and crystallography experiments revealed that compound 1 binds to EED, with the tertiary amine binding in the same pocket that normally recognizes trimethyllysine. However, compound 1 is quite complex, with three stereocenters. Thus, the researchers sought to deconstruct it to something simpler. They began by chopping off two of the rings – an unconventional disconnection but one supported by crystallography, which revealed that the terminal rings were not closely associated with the protein.\nThe resulting fragment 2 was down more than an order of magnitude in potency but had improved ligand efficiency. Crystallography confirmed that it binds in a very similar fashion to compound 1. Initial SAR was conducted around the methoxybenzyl moiety, which is buried within the enzyme. Most changes were not tolerated, but compound 9 did show somewhat improved activity.\nNext, the researchers sought to optimize the positively charged portion of the molecule. Replacing the amine with a guanidine improved the affinity but at a cost to cell permeability. This led to a search for less conventional replacements, ultimately yielding the 2-aminoimidazole moiety in compound 16. Not only did this regain the activity of the initial molecule, it also shows good permeability and cell activity. Crystallography revealed that it too binds in a similar fashion to the original hit.\nThis is a nice example of fragment-assisted drug discovery (FADD), in which concepts from FBDD were used to simplify and optimize a hit from HTS. There is of course much more to do with this series, not the least of which is figuring out exactly how the molecules actually inhibit PRC2. Trimethyllysine-containing peptides that bind to EED normally activate the enzyme, yet the small molecules that bind to the same site somehow allosterically inhibit activity. Despite multiple crystal structures, the researchers frankly acknowledge that they were “not able to decipher the molecular basis for this phenomenon.” A number of conformational changes occur when EED binds to ligands, and perhaps these propagate through the protein complex. A picture may be worth 1000 words, but we may have to wait for the movie to learn the full story.","A chemical library or compound library is a collection of stored chemicals usually used ultimately in high-throughput screening or industrial manufacture. The chemical library can consist in simple terms of a series of stored chemicals. Each chemical has associated information stored in some kind of database with information such as the chemical structure, purity, quantity, and physiochemical characteristics of the compound.\nIn drug discovery high-throughput screening, it is desirable to screen a drug target against a selection of chemicals that try to take advantage of as much of the appropriate chemical space as possible. The chemical space of all possible chemical structures is extraordinarily large. Most stored chemical libraries do not typically have a fully represented or sampled chemical space mostly because of storage and cost concerns. However, since many molecular interactions cannot be predicted, the wider the chemical space that is sampled by the chemical library, the better the chance that high-throughput screening will find a \"hit\"—a chemical with an appropriate interaction in a biological model that might be developed into a drug.\nAn example of a chemical library in drug discovery would be a series of chemicals known to inhibit kinases, or in industrial processes, a series of catalysts known to polymerize resins.\nGeneration of chemical libraries\nChemical libraries are usually generated for a specific goal and larger chemical libraries could be made of several groups of smaller libraries stored in the same location. In the drug discovery process for instance, a wide range of organic chemicals are needed to test against models of disease in high-throughput screening. Therefore, most of the chemical synthesis needed to generate chemical libraries in drug discovery is based on organic chemistry. A company that is interested in screening for kinase inhibitors in cancer may limit their chemical libraries and synthesis to just those types of chemicals known to have affinity for ATP binding sites or allosteric sites.\nGenerally, however, most chemical libraries focus on large groups of varied organic chemical series where an organic chemist can make many variations on the same molecular scaffold or molecular backbone. Sometimes chemicals can be purchased from outside vendors as well and included into an internal chemical library.\nDepending upon their scope and design, chemical libraries can also be classified as diverse oriented, Drug-like, Lead-like, peptide-mimetic, Natural Product-like, Targeted against a specific family of biological targets such Kinases, GPCRs, Proteases, PPI etc. Among the compound libraries should be annotated the Fragment Compound Libraries, which are mainly used for Fragment Based Drug Discovery FBDD.\nDesign and optimization of chemical libraries\nChemical libraries are usually designed by chemists and chemoinformatics scientists and synthesized by organic chemistry and medicinal chemistry. The method of chemical library generation usually depends on the project and there are many factors to consider when using rational methods to select screening compounds. Typically, a range of chemicals is screened against a particular drug target or disease model, and the preliminary \"hits\", or chemicals that show the desired activity, are re-screened to verify their activity. Once they are qualified as a \"hit\" by their repeatability and activity, these particular chemicals are registered and analysed. Commonalities among the different chemical groups are studied as they are often reflective of a particular chemical subspace. Additional chemistry work may be needed to further optimize the chemical library in the active portion of the subspace. When it is needed, more synthesis is completed to extend out the chemical library in that particular subspace by generating more compounds that are very similar to the original hits. This new selection of compounds within this narrow range are further screened and then taken on to more sophisticated models for further validation in the Drug Discovery Hit to Lead process.\nStorage and management\nThe \"chemical space\" of all possible organic chemicals is large and increases exponentially with the size of the molecule. Most chemical libraries do not typically have a fully represented chemical space mostly because of storage and cost concerns.\nBecause of the expense and effort involved in chemical synthesis, the chemicals must be correctly stored and banked away for later use to prevent early degradation. Each chemical has a particular shelf life and storage requirement and in a good-sized chemical library, there is a timetable by which library chemicals are disposed of and replaced on a regular basis. Some chemicals are fairly unstable, radioactive, volatile or flammable and must be stored under careful conditions in accordance with safety standards such as OSHA.\nBecause a chemical library's individual entries can easily reach up into the millions of compounds, the management of even modest-sized chemical libraries can be a full-time endeavor. Compound management is one such field that attempts to manage and upkeep these chemical libraries as well as maximizing safety and effectiveness in their management.\n- Ian Yates. Compound Management comes of age. Drug Discovery World Spring 2003 p35-43\n- Archer JR. History, evolution, and trends in compound management for high-throughput screening. Assay Drug Dev Technol. 2004 Dec;2(6):675-81\n- Casey R. Designing Chemical Compound Libraries for Drug Discovery. Business Intelligence Network December 1, 2005.\n- GLARE - A free open source software for combinatorial library design.\n- Some examples of Chemical libraries for Drug Discovery\n- Huggins DJ, Venkitaraman AR, Spring DR (January 2011). \"Rational Methods for the Selection of Diverse Screening Compounds\". ACS Chem. Biol. 6 (3): 208–217. doi:10.1021/cb100420r. PMID 21261294."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:b621fcb1-a655-4b42-b509-cb34b3ab5b4f>","<urn:uuid:dc989ef3-eb59-43f9-861f-86c0b3f6f321>"],"error":null}
{"question":"What is the main purpose of a Runway Safety Area (RSA)?","answer":"A Runway Safety Area is a graded and clean area surrounding the runway that should be capable, under normal (dry) conditions, of supporting airplanes without causing structural damage to airplanes or injury to their occupants. Its purpose is to improve the safety of airplanes that undershoot, overrun, or veer off the runway.","context":["Below are the first 10 and last 10 pages of uncorrected machine-read text (when available) of this chapter, followed by the top 30 algorithmically extracted key phrases from the chapter as a whole.\nIntended to provide our own search engines and external engines with highly rich, chapter-representative searchable text on the opening pages of each chapter. Because it is UNCORRECTED material, please consider the following text as a useful but insufficient proxy for the authoritative book pages.\nDo not use for reproduction, copying, pasting, or reading; exclusively for search engines.\nOCR for page 2\n2 CHAPTER 1 Background Introduction airports have much higher tolerance to risk than others. Ide- ally the risk associated with specific airport and operation From 1995 to 2004, 71 percent of the world's jet aircraft factors should be modeled to assess the level of safety being accidents occurred during landing and takeoff and accounted provided by specific conditions of existing or planned for 41 percent of all onboard and third party fatalities (Boeing, RSA. Some intuitively important factors that would affect 2005). Landing overruns, landing undershoots, takeoff over- risk, such as various environmental and operational charac- runs, and crashes after takeoff are the major types of accidents teristics of the airport, are not considered yet. that occur during these phases of flight. Records show that In current risk assessment methods, factors that determine while most accidents occur within the boundaries of the run- safety cannot be analyzed independently; however, a rational, way strip, most fatalities occur near but off the airport area systematic identification of safety influencing factors and (Caves, 1996). their interrelationships has never been conducted. This situ- Currently, Federal Aviation Administration (FAA) stand- ation impedes the assessment of effects of safety improve- ards require runways to include a runway safety area (RSA)-- ment opportunities and, consequently, risk management. a graded and clean area surrounding the runway that \"should Moreover, most airfield design rules are mainly determined be capable, under normal (dry) conditions, of supporting air- by a set of airfield reference codes, which only take into ac- planes without causing structural damage to airplanes or injury count the design aircraft approach speed and the aircraft to their occupants\" (FAA, 1989). Its purpose is to improve dimensions (wingspan or tail height). The resulting protection the safety of airplanes that undershoot, overrun, or veer off the is segregated in widely differing groups that do not necessar- runway. ily reflect many of the actual risk exposure factors. The size of the RSA depends on the type and size of aircraft using the runway. RSA standard dimensions have increased Project Objectives over time. The predecessor to today's standard extended only 200 feet from the ends of the runway. Today, a standard RSA The original objective of this project was to collect histor- can be as large as 500 feet wide and extend 1,000 feet beyond ical information related to overrun and undershoot accidents each runway end. The standard dimensions have increased to and incidents to develop a comprehensive and organized address higher safety expectations of aviation users and database with editing and querying capabilities, containing accommodate current aircraft performance. critical parameters, including aircraft, airport, runway, oper- However, applying the new standards to existing airports can ation, and causal factor and consequence information that be problematic. Many runways do not meet current standards could assist the evaluation of runway safety areas. because they were constructed to an earlier standard. The prob- The research team extended the project objective to lem is compounded by the fact that the airports are increasingly include the development of risk models for overrun and un- constrained by nearby land development and other natural fea- dershoot events. The primary function of the risk models is tures, or they face costly and controversial land acquisition, or to support risk management actions for those events by in- a need for unfeasible wetlands filling projects. creasing the size of the RSA, removing obstacles, construction The runway safety area standards are prescriptive and its of arrestor beds or perhaps, where that is not possible, by the rigid nature results in \"averaged\" degrees of protection being introduction of procedural measures or limitations for oper- provided across broad ranges of risk levels, such that certain ations under high-risk conditions.\nOCR for page 2\n3 Three sets of models were developed in this study--landing basis for creating analysis software that can be used to overruns, landing overshoots, and takeoff overruns. Each set assess risks of aircraft overruns and undershoots. is comprised of three parts: probability of occurrence, loca- tion, and consequences. The models can improve the under- Applied to any specific airport, the analysis approach for standing of overrun and undershoot risks and help airport RSA risk assessment developed in this study will allow users operators manage these risks. to determine if the risk is relatively high or low and whether Based on the information described above, the goals for there is a need for risk management action. The safety bene- this research project were extended to include: fits provided by possible mitigation measures (e.g., increased size of RSA) can be evaluated using the same approach. 1. Development of a comprehensive database for aircraft In addition, three innovative techniques were incorporated overrun and undershoot accidents and incidents; to improve the development of risk models. One major im- 2. Determination of major factors affecting the risks of such provement in the modeling of accident occurrence is the use accidents and incidents; of normal operations (i.e., nonaccident and nonincident) 3. Description of how these factors affect operations and flight data. With normal operations data (NOD), the number associated risks, to improve understanding on how these of operations that experience the factor benignly, singly, and events may occur; in combination can be calculated, so risk ratios can be gener- 4. Development of risk models for probability, location, and ated and the importance of risk factors quantified. consequences for each type of accident: landing overruns The second improvement is the use of normalization tech- (LDOR); landing undershoots (LDUS); and takeoff over- niques to convert information to a standard nominal airport. runs (TOOR); Using such normalization procedure allows comparing acci- 5. Development of a practical approach to use these models dent and NOD data for different operation conditions, thus for assessing risks on existing RSA under estimated oper- creating a larger pool of relevant information. ation conditions; Finally, the models developed were integrated in a rational 6. Development of a list of relevant factors that should be probabilistic approach for risk assessment of RSA. Based on reported for aircraft overrun and undershoot accidents so historical information for flight operations and weather con- that availability of quality data can be improved for future ditions, and considering the configuration of the RSA and studies; and presence of obstacles located close to the runway, the proba- 7. Development of prototype software to evaluate risks bility distribution for accidents involving severe consequences under specific operation conditions that may serve as the may be estimated."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"}],"document_ids":["<urn:uuid:377d77a7-47ca-4c4a-b7fa-eb94028534b5>"],"error":null}
{"question":"Por favor, ¿podrías explicarme cómo funciona el GPS y qué elementos técnicos determinan su precisión? Please explain how GPS works and what technical elements determine its accuracy.","answer":"GPS operates through a network of satellites orbiting Earth that emit signals picked up by ground receivers. These signals contain precise timing and location data for accurate positioning. The technical elements determining accuracy include: 1) Satellite Geometry - the arrangement of satellites relative to the receiver affects signal strength and accuracy, with ideal geometry ensuring sufficient visible satellites, 2) Receiver Quality - the quality and sensitivity of the GPS receiver influences its ability to interpret weak signals and mitigate interference, and 3) Satellite Clock Accuracy - precise timekeeping among GPS satellites is crucial for accurate positioning, as any discrepancies can introduce errors in GPS calculations.","context":["Unraveling the Mysteries of GPS Signal: What Influences its Accuracy?\nIn an era where GPS (Global Positioning System) has become integral to our daily lives, understanding the factors that affect GPS signal accuracy is paramount. Whether you’re navigating unfamiliar roads, tracking fitness activities, or conducting precise scientific measurements, the reliability of GPS hinges on various environmental and technical elements. Delve into the nuances of GPS signal fluctuations to unlock the secrets behind its performance.\nThe Essence of GPS Signal\nBefore dissecting the intricacies of GPS signal reliability, let’s grasp the fundamentals. GPS operates through a network of satellites orbiting Earth, emitting signals picked up by receivers on the ground. These signals convey precise timing and location data, facilitating accurate positioning across the globe.\nThe stability and accuracy of GPS signals are subject to environmental conditions. Here are key environmental factors that can influence GPS signal strength and accuracy:\nRecommended: How Much Does A Silver Dollar Weigh\n- Atmospheric Interference: Atmospheric conditions such as ionospheric delays and signal multipath can distort GPS signals, affecting accuracy.\n- Terrain and Obstructions: Tall buildings, dense forests, and mountainous terrain can obstruct GPS signals, leading to signal loss or degradation.\n- Weather Conditions: Adverse weather like heavy rainfall, snow, or fog can attenuate GPS signals, reducing accuracy temporarily.\nBeyond environmental influences, technical aspects play a pivotal role in GPS signal reliability. Explore the technical determinants affecting GPS accuracy:\n- Satellite Geometry: The arrangement of satellites relative to the receiver impacts signal strength and accuracy. Ideal satellite geometry ensures a sufficient number of satellites visible to the receiver, enhancing accuracy.\n- Receiver Quality: The quality and sensitivity of the GPS receiver influence its ability to interpret weak signals and mitigate interference, directly impacting accuracy.\n- Satellite Clock Accuracy: Precise timekeeping among GPS satellites is crucial for accurate positioning. Any discrepancies in satellite clock accuracy can introduce errors in GPS calculations.\nDespite the challenges posed by environmental and technical factors, advancements in GPS technology and signal processing techniques have enabled the development of mitigation strategies:\nAlso Read: What Is Printfriendly\n- Augmentation Systems: Augmentation systems like WAAS (Wide Area Augmentation System) and DGPS (Differential GPS) enhance GPS accuracy by correcting signal errors caused by atmospheric disturbances and satellite clock inaccuracies.\n- Multi-Constellation Reception: Modern GPS receivers can leverage signals from multiple satellite constellations, including GPS, GLONASS, Galileo, and BeiDou, improving signal availability and accuracy.\nFAQ: Navigating Common Queries\nQ: Can GPS signals penetrate indoor environments?\nA: GPS signals struggle to penetrate dense structures like buildings, making indoor positioning less accurate compared to outdoor environments.\nQ: How does urbanization impact GPS signal reception?\nA: Urban areas with skyscrapers and high-density buildings often experience signal blockages and reflections, leading to decreased GPS accuracy.\nRecommended: Does Chemiosmosis Use Active Transport\nQ: Do smartphones use the same GPS technology as dedicated GPS devices?\nA: Yes, smartphones utilize GPS technology, often supplemented by other positioning methods like Wi-Fi and cellular triangulation for improved accuracy in urban canyons and indoor settings.\nThe reliability of GPS signals is contingent upon a myriad of factors, spanning environmental conditions, technical intricacies, and mitigating strategies. By comprehending the nuances of GPS signal fluctuations, users can navigate with confidence, harnessing the full potential of location-based services in their daily endeavors. As technology evolves and innovations emerge, the pursuit of precise positioning continues, driving the quest for ever-improving GPS accuracy.\nAlso Read: How To Change Your Cameo On Snapchat\nAlso Read: What Is The Leek Spin Song"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:e544ec2f-b8ac-42c0-9197-d8af024fde25>"],"error":null}
{"question":"What are the different types of waterproofing used for house foundations and how do they compare?","answer":"There are two main ways to waterproof a foundation: anti-corrosion and anti-filtration. Anti-filtration waterproofing requires high-class specialists and is only needed in places with high moisture levels. Anti-corrosion waterproofing, which is suitable for simple country houses, baths, and garages, can be either vertical or horizontal. Horizontal protection uses roofing felt folded several times and protects against capillary water penetration. Vertical protection is more complex but very effective and can be done using paint waterproofing with bitumen or roofing material.","context":["In order to provide strength and durability to the foundation of a house or any other building, it is necessary to install waterproofing. Proper foundation waterproofing is a guarantee of the reliability of the entire building. After all, if the foundation is destroyed, the structure itself will be damaged. In this article, we will tell you how to do the foundation waterproofing with your own hands, consider the methods, and also show photos and video instructions.\nThere are two ways to waterproof the foundation: anti-corrosion and anti-filtration .\nAnti-filtration waterproofing of the foundation is not done. Its arrangement requires high-class specialists who have experience in this field. However, in most cases such protection is not required. Anti-filtration system is needed only in places with high levels of moisture. For simple country houses, baths, garages suitable anti-corrosion waterproofing.\nAnti-corrosion waterproofing can be:\nVertical protection is more complex in execution, however, it is very effective.\nThe horizontal system for protecting the foundation from moisture is made of ordinary roofing felt folded several times. It is installed very simply and more common than all other types of waterproofing.\nHorizontal waterproofing serves to protect against capillary penetration of water and steam.\nThis foundation protection system is superimposed on its base. For waterproofing, you can use liquid bitumen. In the process of working with this material should only be remembered that it emits harmful fumes in a heated form. Therefore, should be protected.\nVertical protection can be:\nWhen applying vertical waterproofing, pay due attention to the handling of material joints, which are often vulnerable to moisture penetration.\nPaint waterproofing is made using bitumen, which is applied to the surface. In the vertical waterproofing can also be used roofing material.\nWaterproofing with liquid bitumen\nLiquid bitumen is an excellent material for waterproofing foundation. It:\nLiquid bitumen has disadvantages. The main one is that this material in a heated condition is very harmful.\nWork with bitumen should be done only with the use of protective equipment. In addition, bitumen requires regular heating, and it must be carefully monitored. This material quickly ignites and burns, exuding asphyxiating smoke and unpleasant odor.\nTo make waterproofing with bitumen, follow the instructions below.\nWaterproofing is not limited to the treatment of the foundation basement and basement. In order to protect the house from moisture, it is necessary to conduct high-quality drainage next to the house itself. For this it is necessary to equip one simple system.\nAfter arranging the drainage system, you should not worry that the basement will be flooded after the snow melts on the site. When arranging such a system, you must be careful.In the event of an error, water can start flowing directly to the foundation. In difficult cases for such work it is better to invite a specialist.\nAn additional measure of protection is an external drainage system. These are various trays and water collectors. This method was known in ancient times. It still works effectively. Before you make the waterproofing of the foundation of the house, it is necessary to consider the drainage area.\nWater is affected by the tape foundation:\nWhen arranging waterproofing, it is necessary to take into account all types of damage that moisture can cause in various ways.\nWaterproofing can be:\nFree-flow waterproofing is designed for seasonal melting of snow when water rises to the foundation. It is made as follows:\nAfter the arrangement of such a system, seasonal waters will no longer pose a danger to the strip foundations.\nThis type of insulation is made using various sprayed and coating agents. It protects the foundation constantly from any moisture. As a result of the work, a special membrane is created, which has water-repellent characteristics.\nThis type of foundation protection must completely fill all cracks and small cracks. One of the most effective ways to create capillary waterproofing is liquid bitumen. This method has already been described above.\nThe material that is used for waterproofing should have the following characteristics:\nFor more information about foundation waterproofing, see the following videos:"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"}],"document_ids":["<urn:uuid:104971a1-9491-445e-bc7c-6d913783674a>"],"error":null}
{"question":"What's the main idea behind Oates' Decentralization Theorem from 1972?","answer":"Oates' Decentralization Theorem states that fiscal responsibilities should be decentralized when there are no cost savings from centralization and no interjurisdictional externalities. This is based on the idea that local governments are closer to the people and have better knowledge of local preferences and cost conditions compared to central agencies.","context":["بازبینی \"قضیه ی تمرکز زدایی\" ــ درباره نقش اثرات جانبی\n|کد مقاله||سال انتشار||مقاله انگلیسی||ترجمه فارسی||تعداد کلمات|\n|3123||2008||7 صفحه PDF||سفارش دهید||3750 کلمه|\nPublisher : Elsevier - Science Direct (الزویر - ساینس دایرکت)\nJournal : Journal of Urban Economics, Volume 64, Issue 1, July 2008, Pages 116–122\nThe “Decentralization Theorem” [Oates, W.E., 1972. Fiscal Federalism. Harcourt Brace Jovanovich, New York] is central to the discussion of fiscal federalism. We revisit the role of consumption spillovers in evaluating the merits of (de)centralization. Unlike the general prediction, a higher degree of spillovers may reduce the difference in utility of centralization and decentralization. The non-monotonicity result relates to the difference in expenditures on public consumption. Provided decentralized choices yield higher levels of public expenditure, a rise in the amount of spillovers allows residents to enjoy larger gains in public consumption (and thereby utility) under decentralization relative to centralization.\nThe question of whether fiscal responsibilities should be assigned to a (de)centralized authority has long been debated in public economics. The discussion refers to Oates’ Decentralization Theorem (Oates, 1972) stating that in the absence of cost savings from centralization and interjurisdictional externalities, fiscal responsibilities should be decentralized. This argument implicitly assumes that the center is unresponsive to preference heterogeneity and thereby is only able to implement uniform policies. More specifically, “[. . .] individual local governments are presumably much closer to the people [. . .], they posses knowledge of both local preferencesand cost conditions that a central agency is unlikely to have” (Oates, 1999, p. 1123).1 If the geographical scope of a jurisdiction falls short of the spatial pattern of spending benefits, the optimal assignment of policy tasks is deduced by trading off the welfare costs of policy uniformity against the welfare gains from internalizing spillovers in policy-making.2 Consider a country consisting of two regions which differ in their preferences for local public goods, whichexhibit regional spillovers. In this setting, fiscal decentralization allows for a better matching of public good provision to local tastes, whereas under centralization uniform provision ignores local taste heterogeneity, but internalizes spillovers. The central question to be examined in this paper is how the difference in the utility of centralization and decentralization changes with respect to the level of consumption spillovers.3 Using quasi-linear, iso-elastic preferences, the welfare difference turns out to be non-monotone in the strength of spillovers. A larger amount of spillovers may reduce the welfare differential between centralization and decentralization. The rationale for this result is that decentralization may yield higher expenditures on public goods than centralization.\nنتیجه گیری انگلیسی\nThe paper provides a formal treatment of how relative welfare with (de)centralized policy relates to the strength of spillovers in public consumption. Most of the discussion on the costs and benefits of fiscal federalism rests on a welfare trade-off which is taken to be monotone in the primitive of the economy. In contrast to the presumption, the analysis points to a non-monotone trade-off. A marginally higher degree of spillovers may promote the well-being of constituents under decentralization compared with centralization. The analysis reveals that a non-monotonicity of W will only arise when decentralization is welfareenhancing. The finding may not extend to models of fiscal federalism which differ from the specification adopted in the paper. Suggestively, a non-monotone sign of W may arise in models in which decentralization generates distortions beyond the failure to internalize spillovers or in which centralization exhibits allocative advantages in addition to the internalization of spillovers. In these cases, theW-curve potentially shifts upward and multiple crossing points with the 0-line may exist. We leave a rigorous analysis of the reasoning to future research."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:ee761654-f620-446c-b1ac-a9fb34509225>"],"error":null}
{"question":"What are the contrasting approaches to responsibility and planning demonstrated by those who died without proper estate planning (like Prince and Picasso) versus the strategic military planning exhibited by Lt. Felipe Fernandez during the Battle of Tayug?","answer":"The cases of Prince and Picasso demonstrate severe consequences of poor planning - Prince's $300 million estate faced unnecessary taxes and false claims from alleged heirs, while Picasso's estate paid an extra $30 million in taxes due to lack of planning. In contrast, Lt. Fernandez showed meticulous tactical planning during the Battle of Tayug by carefully positioning his four light machineguns with proper spacing, ensuring no unprotected gaps between units, maintaining ammunition supplies, and adapting to changing battlefield conditions. His careful planning and execution helped hold a crucial defensive position against overwhelming Japanese forces, demonstrating how proper planning can have significant positive outcomes whether in civilian or military contexts.","context":["Some Cautionary Celebrity Tales; Why Careful Estate Planing Is Important\nHave An Estate Plan\nEven celebrities die without wills. Picasso’s unplanned estate paid an extra $30 million in taxes. Prince’s $300 million estate faced unnecessary significant taxes and disputes, with false claims by alleged heirs, including a convicted felon. James Dean died at 24 without a will so his assets passed to his estranged father. Steve McNair, the NFL quarterback murdered by a girlfriend, left a $20 million estate and no will. He died leaving a wife and their two children, and two children from a prior marriage, complicating his estate. Martin Luther King’s estate’s disputes continued for 50 years, again due to the lack of a will. Jimi Hendrix died with no will. His father inherited his $66 million estate. His father left it all to his own adopted daughter and nothing to Hendrix’s natural brother.\nPlan Early, Update Your Plan And Use Trusts\nRobin Williams created multiple trusts for his family, which also kept his assets private after death. Unlike Philip Seymour Hoffman, who had a will but disregarded advice of lawyers and advisors to create trusts because he did not want “trust fund kids”. This resulted in large estate taxes and full public disclosure of his estate plan. Paul Walker died at 40 with a will. But he had failed to update it and his wishes at 40 may have been very different than at 28. Heath Ledger also had an old will, made before his daughter’s birth, leaving everything to his parents and sister.\nPlan For Taxes\nJoe Robbie, owner of the Miami Dolphins and the stadium, expected his family business to remain in the family. Family feuds and an estate tax liability caused a sale of the team and stadium for $109 million to pay taxes of $43 million. Fifteen years later these assets sold for $1 billion. The Wrigley family sold the Cubs for $20.5 million in 1981 to pay taxes. The value 30 years later was $2 billion. Tom Clancy’s estate planning failed to consider taxes and his children’s share of $28.5 million paid all $12 million in taxes, while their step mother’s share of $60 million paid no taxes.\nAvoid Post Death Conflicts\nAudrey Hepburn left valuable memorabilia to be divided equally between her two sons with provisions for a charity. The sons could not agree and lawsuits followed. Princess Diana left a “letter of wishes” providing personal property to her godchildren. It was not binding and they received nothing. Careful drafting can eliminate or greatly reduce personal property and other disputes.\nJerry Garcia’s estate was $10 million. Ex-wife, Mountain Girl, sought $4.6 million based on a one paragraph divorce agreement which she drafted. His office manager sued for half of the royalties from Cherry Garcia ice cream. Kirk Kerkorian, billionaire casino owner, died at 98 with a $3 billion estate. His second wife (an 18 day marriage) had sought $320,000 a month support for “their” daughter Kira, who he learned had a different father. Although his will provided $8.5 million to Kira, she contested it. His third wife (of 441 days) Una challenged their pre-nuptial agreement instead seeking $1 billion (over $2 million per day of marriage). An ex-girlfriend sued his estate for $20 million in promised support which she drafted.\nAnticipate The Unexpected\nJackie Kennedy died in 1994. Her will provided that her estate ($50-$100 million) would pass either to her children or if they disclaimed to a trust for charity and her grandchildren. Her children did not disclaim and apparently chose to pay more estate taxes. While she purposely provided this control for them, she may have been surprised at the outcome. Jim Morrison died in 1971 with a will, leaving all to his girlfriend. She died in 1974 without a will so her parents, who hated Morrison, inherited everything.","Each time I reminisce about the Second World War, I often wonder how the little things that I did as a Platoon Leader influenced the final effort in winning that war. As a commander of a small unit, a platoon, I thought that my action was very minor but remembering the story about the ‘one horse shoe nail’ which could have caused the loss of a battle, I began to wonder if my little effort may have indeed contributed to the success of the entire operation. Such is my feeling about the battle of Tayug when the 26th Cavalry (PS) was tasked to defend the Agno River at all cost to prevent the Japanese in outflanking the Northern Luzon Division commanded by General Wainwright on December 25, 1941. The following is a narration of my participation in that battle.\nOn the 25th of December 1941 my Machine Gun Platoon and the remainder of Troop “E” under the command of 1st Lt. William P. Leisenring were positioned on the eastern bank of the Agno river, my platoon along the river bank, astride the road coming from Asingan leading to the town of Tayug. My orders were to deny the enemy from crossing the river and to hold my position at all cost until relieved. With fourteen men and four light machineguns it was a very big order but as a soldier I had to execute the order with what I have. I positioned my guns about three yards from the bank allowing ten yards interval between guns. On my left were four heavy machine guns and their supporting men from the 91st Division and on my right was a rifle squad lead by Corporal Jeremias dela Cruz, I told my men to dig in while I inspected both flanks of my platoon making sure that there were no unprotected gaps with units adjacent to mine.\nJust as the sun began to set, sending its red rays like the rafters of a huge lean-to emanating from the western skies, the Japanese advance snipers began firing their rifles. At first there were a few scattered shots then there were more and then a lot more that sounded afar off and because the rays of the sun were impending our sights I ordered my men to hold their fire.\nThere was a full moon that night. How ironic is it that on a night after the celebration of Jesus’ birth, that men are engaged in a deadly combat when they are supposed to fall on their knees and give thanks that the Son of God was born? The sky was unusually clear and the light of the moon was mellow reminding me of those nights in my younger days when my loving father and I sat on rice paddies and he relating to me local folklores such as the nymphs and fairies that peopled our valleys and woodlands; the battles between the Christians and the Moros of which the former always emerged victorious; these and other stories which were told in camp fires during the Filipino Spanish war when my father fought his own battles. I was absorbed in deep thought about my loving parents who were just four kilometers to the east but too far for me to hug them goodbye.\nSuddenly a mortar round fell a few yards in front of me which luckily landed in the depth below. This awakened me from my day dreaming and as I scanned the river bed I saw many Japanese, like a multitude of giant hermit crabs, creeping towards our positions. I alerted my men and gave the orders to fire at will. The gunners saw what I had seen and they expertly trained their fire at the creeping shadows. They fanned their fire right and left cutting down the advancing Japanese like a big scythe cutting a field of ‘cogon’ grass. The gunners tilted their guns to shot down those who were attempting to scale the twenty foot bank of the river. Suddenly all hell turned loose and the firing became so intense, like the celebration of a Chinese New Year, as the Japanese desperately made their utmost effort to dislodge us from our positions but my men held their own. An ammunition bearer who was out of a fox hole cried out, “God, oh God, come and help us. Help us please for I could hardly open my eyes because of the intense firing”. After a pause he continued, “Don’t send Jesus, he was just born.” I caught myself snickering but such was the reaction of one who is desperately clinging to his life. Suddenly I noticed that some of my guns and others on my left were no longer firing. I jumped and ran to check what was happening and found that some guns had ruptured cartridge and others had just run out of ammunitions. Having the only combination tool, I ran from gun to gun extracting the ruptured cartridge and ordered an ammunition carrier to replenish those that had run out. I ventured on my left to find that the gun positions had only two men left, the rest had withdrawn to the rear. I checked with the men manning the guns and found out that they had the same problems as my men. I told the men that they are now under my command and for them to hold their positions until further orders. I made the guns operational and had ammunitions brought to them During all of this time Lt Leisenring kept shouting at me to be careful and not to take unnecessary risk but I kept on doing what was to be done, sometimes unavoidably exposing myself to enemy fire. I lost tract of the time but at about 2:00 o’clock past midnight, there was silence. After a few minutes I ordered Pfc Jesus Gonzales and Pfc. Alberto Lazo to check what was happening below. On their return they reported that there were countless dead Japanese together with dead animals which the Japanese used as shields when they attempted to break through our lines.\nLt. Leisenring sent his orders to withdraw so I ordered my guns to withdraw with my four riflemen covering the withdrawal. The Lieutenant commended us for a job well done and recommended me for an award of a Silver Star Medal.\nWhat if the Japanese were able to break through my line, would there be a successful withdrawal of the entire USAFFE at the bridge in Layac junction? Would there be a Bataan campaign which upset the Japanese timetable in their conquest giving the United States ample time to prepare for an allout war? Would there be a Bataan Death March? Would there be a prison camp at O’Donell where my father-in-law died? I just wonder as I reminisce.\nSource: (Ret) CPT Felipe Fernandez, Silver Star awardee, response to a request by M.E. Embry who applied the highlights in the essay.\nCaptain Felipe Fernandez (U.S. Army, ret.), was born on August 13, 1916, in the small town of San Nicolas, in the province of Pangasinan, Philippines, son of Isidoro and Maria (Alimourong) Fernandez, the fourth of six children. He died on March 9, 2013 in Seaside, California at the age of 96. He lived a long, fruitful and healthy life until diagnosed with cancer in early 2013. He passed away peacefully, at home, surrounded by loving family."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"}],"document_ids":["<urn:uuid:bdbae407-29c3-490e-baa7-489b51d508e4>","<urn:uuid:8b409066-ee5d-44b8-bb88-b5976a6a1f0b>"],"error":null}