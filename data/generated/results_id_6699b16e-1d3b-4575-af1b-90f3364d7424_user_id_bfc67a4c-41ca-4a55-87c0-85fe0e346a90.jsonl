{"question":"What are the computational limitations of modeling biological systems mathematically, and what role might quantum biological processes play in overcoming natural system constraints?","answer":"Mathematical modeling of biological systems faces fundamental computational constraints due to combinatorial complexity - even simple molecular interactions can require dozens of equations, scaling up to over 10^30 equations for pathways like EGFR signaling. This forces modelers to make potentially problematic simplifying assumptions. Intriguingly, biological systems themselves may have evolved ways to handle complexity through quantum processes. For instance, photosynthesis achieves nearly 100% energy transfer efficiency potentially through quantum coherence, while some species use quantum effects for magnetic navigation through radical pair mechanisms in photoreceptors. These natural solutions suggest evolution may have developed methods for quantum manipulation that exceed our current technological capabilities.","context":["An \"Uncertainty Principle\" for traditional mathematical approaches to biological modeling\nIf you're a biological modeler, chances are there are two words that keep you up at night and that on occasion, might even have given you serious pause to question the wisdom of your choice of profession. Those two words are combinatorial complexity.\nFor anyone not entirely familiar with this concept, imagine one of the simplest possible biomolecular systems, with a kinase K that can bind to and phosphorylate a substrate S at either of two positions a and b, as shown in this first diagram. Even this very simple system can produce 13 possible molecular species: K unbound, S unbound in one of its 4 phosphorylation states, K bound at a with S in one of its 4 states, K bound at b with S in one of its 4 states.\nTaking the traditional biological modeling approach of using ordinary differential equations (ODEs), you would therefore have to write 13 rate equations to describe this system. So far so good.\nBut now let's add the phosphatase P that dephosphorylates the sites a and b on S. If we do a similar analysis of the possible molecular species for our new system, taking into account now, the possible bound and unbound states of PandK on S, we discover that the addition of this single agent P yields 21 new molecular species in addition to the 13 that we already had! Furthermore, since we are working with a model of interdependent ODEs, we will also need to rewrite our original set of rate equations.\nIf it takes all this work to describe what is almost the simplest imaginable kind of system, how many equations would we need for a real biological system? How many rate equations would we need to describe the canonical epidermal growth factor receptor (EGFR) signaling pathway for example?\nHold on to your hats ... drum roll ... somewhere north of 1030 equations.\nAll this said however, biological modelers have built models of complex cellular systems like the EGFR pathway, so how on earth have they done it? The answer is by simplifying the system, typically by either ignoring features that are presumed to minimally impact the system's behavior, or by aggregating features to create a less granular description of the system, again under the presumption that this will not significantly affect the model's behavior.\nThe danger inherent in such approaches is that they require a set of a priori hypotheses about what are and what are not, the important features of the system i.e. they require a decision about what aspects of the model will least affect its behavior before the model has ever been run.\nThe famous Uncertainty Principle that we all learned in high school physics states that it is impossible to simultaneously determine with any accuracy, both the position and the momentum of an electron. A recasting of this principle for traditional biological modeling might be\n\"Scope or resolution, but not both at the same time\"\nOne could argue that whereas the original physical principle is absolute, in the case of biological modeling the limitation is one of technology - \"If we had a big enough, fast enough computer ...\" etc. Perhaps, but when you compare the storage and processing time required to solve a system of 10^30 equations with the scale of our universe, the biological modeling version of the principle seems pretty darn absolute to me.\nDid I hear someone say \"quantum computer\"?\nJust let me know when they've built one that could address this problem and I will gladly publish an update :-)\n© The Digital Biologist","In a real sense, everything is quantum mechanical: matter and interactions are governed by the rules of quantum physics. True, we haven't figured out how gravity fits in yet, but the structure of atoms, nuclei, molecules, and solids—along with the characteristics of light—are best described by quantum mechanics. However, we don't need to take the unique features of quantum mechanics into consideration when modeling many systems, including most in chemistry or biology.\nNevertheless, it is possible life has evolved to exploit some quantum phenomena, including coherence and tunneling. In a new Nature Physics review article, Neill Lambert and colleagues examined the evidence in favor of quantum biological phenomena in photosynthesis, photoreception, magnetic sensation, and even our sense of smell. They conclude that the evidence is ambiguous: compared with other biochemical processes, any uniquely quantum influences appear small.\nFor something to be considered fully quantum, the researchers used the criterion that it could not be fully described using heuristic models. (The authors refer to such heuristic depictions as \"classical.\" Though that's common, it's also misleading: talking about chemical bonds, electron transport, and the like isn't exactly the language of classical, Newtonian physics.) By contrast, quantum mechanics requires the use of the quantum state, the physical and mathematical description of a system that encodes the probability of the outcomes of measurements.\nThese states may be superpositions: combinations of mutually exclusive measurement outcomes, such as perpendicular polarizations of light, or spin orientations of electrons in atoms. Quantum information and computing are based on the preparation, manipulation, and entanglement of quantum states, but these generally require special conditions, including far lower temperatures than organisms can withstand. The present paper concerned itself with examining if life can get around those problems, exploiting quantum physics for evolutionary gain.\nTracing back along the food web, most of the energy organisms need to survive originates with the Sun. Green plants, cyanobacteria (also known as blue-green algae), and other organisms capture photons and use them to drive the chemical reactions of metabolism. Photosynthesis is incredibly efficient: nearly 100 percent of the energy from the photons absorbed by the photosynthetic machinery is transferred to the chemical reaction center.\nA set of chemistry experiments that began in 2007 showed that this efficiency may be due to quantum mechanics. The data demonstrated coherence between the electrons in the various pigment molecules during the transfer process, meaning that the states of the electrons were coordinated and they acted as a single system, even though they resided in different atoms. The clearest results were obtained at 77 Kelvin, but follow-up experiments showed the coherence could also be present at room temperature. Additionally, complicated theoretical models of the energy transport process seem to support the hypothesis that coherence plays a role in photosynthesis.\nAt the present time, no one has observed quantum coherence in a living organism. Additionally, it's not clear whether the boost that should come from exploiting quantum states would be sufficient to explain the efficiency of photosynthesis. After all, it's one thing to observe this phenomenon under lab conditions, it's another to demonstrate it in a living organism, and yet another to show that quantum coherence is the reason for the remarkable efficiency. Finally, as always in evolution, it's important to remember that a particular feature may be adaptive but may have evolved for other reasons: perhaps efficient photosynthesis was the side effect of some other adaptation.\nHowever, if coherence can be shown to exist in these biological systems at room temperature, then it's worth asking why it's there and how it might provide an evolutionary advantage. One possible explanation is that each pigment molecule experiences a lot of \"noise,\" or random jiggling inside the cell. By exploiting quantum coherence, these random fluctuations can be canceled out during the electron transfer, meaning fewer photons are lost to photosynthesis.\nMagnetic navigation in birds\nMany migratory species use Earth's magnetic field for navigation, including a number of birds. Our understanding of how animals sense magnetic fields—magnetoreception—is still lacking some details, and not all species appear to use the same mechanisms. The authors of the review noted that European robins, along with a handful of other species, seem to navigate using photoreceptors: cells in the retina that measure light intensity. The behavior of these cells can be disrupted using magnetic fields, suggesting there may be a dual dependence on light and magnetism for navigation.\nThe photoreceptors contain radical pairs: two molecules bound to each other that have single electrons in their outer layers. When they are correlated, the spins of these electrons form a state similar to one used in quantum information theory and computing.\nIn the radical pair model of magnetic navigation, an incoming photon induces one of the electrons to undergo a transition between quantum states. Since it is correlated with the other electron, this induces a second transition, which sends a signal through the bird's nervous system. Since the spin states are sensitive to external magnetic fields (useful for navigation), they can also be disrupted by laboratory fields.\nEven though the radical pair concept would explain all the aspects of navigation in some species, there's a major problem with this model. As the review points out, coherence between the electrons' states would need to be sustained for a relatively long time—longer than any current lab experiments have achieved. The very advantage of the model may end up making it untenable, unless researchers can figure out how the photoreceptor cells stabilize the quantum states for extended periods.\nTunneling our senses\nA less finicky phenomenon based on quantum states is tunneling, the process by which a particle (or some more abstract quantum system) can pass through a barrier between states—a transition that would be forbidden in classical terms. Tunneling is used in many applications, including some types of diodes, where electrons pass from one side of a junction to another. Certain chemical reactions in biology appear to depend on the tunneling of electrons or protons over distances equivalent to dozens of atoms.\nExperiments indicate that tunneling plays a part in photosynthesis and certain enzyme interactions, including possibly the sense of smell. In that case, it's not simply the size and shape of the molecules we perceive as odorants that trigger a smell: it's also the transfer of an electron from the odorant to the receptor in the nose via tunneling.\nFinally, the authors discussed the possibility that photoreceptor cells used to sense light could depend on quantum coherence. In this scenario, light triggers a very rapid molecular change in a protein-linked molecule in the retina, which then induces a secondary change in a protein. The speed and efficiency of these processes hints at a possible underlying quantum effect, but experimental evidence to support this has not been obtained yet.\nAn intriguing aspect of all of these possibilities is that perhaps evolution has figured out a better way of performing tricky quantum manipulations than we have. In a sense, that's not surprising: life has had a long time to evolve photosynthesis, photoreception, and navigation, while our understanding of quantum mechanics just began in the 1920s and '30s.\nIt may also turn out that the phenomena described above don't really rely heavily on the quantum state after all, since evidence is sketchy at present. Nevertheless, the hints are there: we may be at the point where we can test if life has solved the problem of manipulating quantum states, meaning quantum biology could be a new field of study in this century."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:ce72db11-a441-482e-a056-61e666d3ea9a>","<urn:uuid:7e7a95d1-85bc-4d43-be97-bc9a84c7d516>"],"error":null}
{"question":"How can I safely explore backcountry snow terrain, and what are the key avalanche danger signs to watch out for? 🏂","answer":"For safe backcountry exploration, start with easy routes near the piste that are accessible by lift and run parallel to it. Using a guide is highly recommended as they know safe areas and can help you avoid unnecessary risks. As for avalanche dangers, watch for fracture lines, eroded snow, and shooting cracks on slopes. The most dangerous slope angles are between 25 and 60 degrees, with 38 degrees being the 'magic angle' for avalanches. Before venturing out, you must take avalanche rescue courses and carry essential safety gear (probes, shovel, avalanche beacon). Also check weather and avalanche reports, as factors like precipitation, wind, and temperature affect snowpack stability.","context":["The feeling of gliding through untracked powder is a world away from the freshly groomed runs of the resort, but it doesn’t come easy. As well as adjusting the way you ski or snowboard, you’ll also need to have your wits about you and plan accordingly. If this is your first foray into the backcountry, then this fistful of powder tips are going to be invaluable.\nBefore you go galavanting off into the unknown, it’s a good idea to plot some easy little routes not too far from the piste. This is a great introduction and training ground for riding powder. Ideally you want to look for sections of snow that are easily accessible by a lift and run somewhat parallel to the piste. Yes, these areas are likely to be tracked out rapidly, but it’s much easier to hike out back to the piste than if you head off away from the resort.\nUnlike your blue, red and black runs which finish at a lift station, your powder lines could take you anywhere, so it’s important to consult a map and have strong bearings of where you're heading. You’ll often find sections of powder between the meandering bends of the piste, which allow you to get used to the feeling of riding deeper snow, but in short bursts so that you can rest your untrained legs.\nRiding the piste, you have a sound surface for your equipment to plane on, but in the backcountry things are a little different. In this environment you need to ensure that the front tip of your skis or snowboard are raised out of the powder. Failing to do this means you will quickly loose speed and become buried. Speed is most certainly your friend here as this will help you to ‘stay afloat’ and glide across the surface, rather than getting bogged down. It can take a bit of getting used to, particularly as you need to keep your weight back and initiate turns from the rear.\nIt goes without saying that you should seek expert advice when going beyond the barriers into the backcountry. You can find training courses in the resort where you’ll learn how to use probes, shovels and transceivers properly, how to asses the snowpack and how to search for buried parties in the field.\nHowever, the very best way to get the most from the backcountry is to use the services of a guide. This remains relevant for all ability levels right through to your advanced rider. These guys know exactly where to find the good stuff and can save you a lot of valuable time and effort. They’ll also ensure you’re riding in safe areas, avoid unnecessary hikes out, and generally get you home in one piece. You’ll also glean a lot of valuable tips and mountain know-how from a good guide, while getting access to some of the best powder to be had.\nWe caught up with Derek Chandler of Marmalade Ski School, one of our official FLOA partners to talk about the diversity of the 3 Valleys and escaping the confines of normal skiing…\nMaximising your time in the snow is what it’s all about. Here’s five tried and tested ways to ride harder, stronger, and longer…","Avalanche Safety: The Do's and Don'ts\nVacant slopes, fresh pow, and untouched winter wilderness: If you're a skier, snowboarder, snowshoer, or climber, chances are your boots may be treading on avalanche-prone territory this winter season.\nAvalanches are a sobering reality. More than any other natural hazard in the backcountry, they bury dozens of outdoor recreationists every season. Last winter experienced a high number in avalanche-related deaths, in which 24 people died in the US: 8 were skiers, 7 were snowboarders, and the rest were snowshoers, climbers, and trekkers.\nLearning about the mountain's snowpack habits can be be a life-saver for you and other adventure aficionados. Based on a checklist published in mountaineering Bible Freedom of the Hills, along with some anecdotal wisdom from avalanche forecaster Brandon Schwartz at Sierra Avalanche Center, we encourage our readers to follow these essential DO's and DON'Ts, keeping in mind the four main elements that cause avalanches: terrain, snowpack, weather, and people. After all, what's more fun than keeping yourself and others from getting buried alive in several feet of snow?\nDO: Check whether or not the terrain can produce an avalanche. Look for signs in the slopes: fracture lines, eroded snow, or shooting cracks. Also, measure the mountain slope's angle -- you can use your ski poles or a compass to figure out the likelihood of an avalanche coming your way. The \"magic angle\" for an avalanche is 38 degrees, and avalanches frequently get rolling somewhere between the ball park of 25 and 60 degrees. Slope degrees change with every aspect, or direction, and configuration of the mountain, so practice doing this \"on the fly.\"\nDON'T: Underestimate the likelihood of an avalanche simply because the slope might be under 25 or over 60 degrees. Also, DON'T just guess at angles -- they change with various aspects of the mountain.\nDO: Determine the snowpack -- the density and distinct layers of snow -- and its stability. What happens in a deadly slab avalanche: a layer of the snowpack collapses on a weaker layer of hoar and slides down the mountain's ice crust. One way of doing this is by carving out a block of snow. If it slides with ease, then there's a greater chance a much larger layer could fracture with a distinctive wumph sound and cause a slab avalanche, like this one here (note the shooting cracks):\nDON'T: Depend solely on others' observations.\nDo: Before going out to bag your next summit, check the weather and avalanche reports. Precipitation, wind, and temperature all affect the snowpack. Example: If there were windy days on the slopes, then wind-swept snow likely accumulated on cornices and ridges -- called wind-loading -- creating a deadly chunk of snow waiting to break off. We recommend visiting National Weather Service and Avalanche.org -- backcountry guides swear by these sites before even lacing their boots.\nDON'T: Assume (there's that word again) that since the sun is out and the powder is pristine, that you aren't in danger due to yesterday's weather.\nDO: Please, leave the hubris at home, check the attitude at the car, and respect the mountain and her forces of nature. People trigger a whopping 92 percent of deadly avalanches, so be aware of your impact on the mountain for the sake of outdoor recreationists around you. Give yourself a mental inventory: What is my goal? What are the risks I'm facing? and most importantly, Am I prepared for the worst? Make sure you have the knowledge and training in avoiding and surviving an avalanche, and we highly recommend taking a course.\nDON'T: Partner with reckless morons -- they're out there. Also, DON'T climb, ski, snowboard, or attempt anything else beyond your physical or technical limits. And finally, DON'T go into avalanche-prone regions without having taken 1) an avalanche rescue course, 2) safety gear like probes, a shovel, avalanche beacon, and 3) a partner who has the same resources.\nPhoto Credit: iStockphoto/SebastianHamm; iStockPhoto/DarioEgidi\nJ. Scott Donahue is an editorial intern at Sierra. He was a freshman in Mr. Hancock's English class when he first read Jon Krakauer's Into Thin Air. Now, he's currently working on a graduate thesis composed of travel essays. Topics include substitute teaching kindergartners in Nepal, drinking rice beer with a Tibetan porter, and running a marathon from Everest Base Camp."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:632ff6cf-b7e0-4059-9e83-dce3264f480d>","<urn:uuid:86680bc6-95fa-4c26-b619-bea561a425f9>"],"error":null}
{"question":"What are the key differences between IPEF and RCEP in terms of their market access and economic incentives?","answer":"IPEF and RCEP differ significantly in their economic incentives. RCEP offers clear market access benefits, aiming to reduce 90% of existing tariffs over 20 years and harmonize non-tariff barriers. In contrast, IPEF notably lacks market access incentives, which is considered a significant weakness. Without promising greater U.S. market access, IPEF must rely on other benefits like supply chain resilience, trade facilitation, and climate action commitments to attract participants. This lack of market access incentives in IPEF has been identified as potentially limiting the United States' ability to secure its interests in the framework.","context":["On May 23, U.S. President Joe Biden formally launched his administration’s long-awaited Indo-Pacific Economic Framework (IPEF) during his State visit to Japan. Speaking in Tokyo during the formal launch ceremony, Biden described the IPEF as a platform to “work toward an economic vision that will deliver for all peoples” by “taking on some of the most acute challenges that drag down growth and by maximizing the potential of our strongest growth engines.”\nThe IPEF is the main economic pillar in the Administration’s broader Indo-Pacific Strategy. The Biden administration is clear that it does not intend to join or rejoin existing regional multilateral trade pacts—such as the Regional Comprehensive Economic Partnership and the Comprehensive and Progressive Agreement for Trans-Pacific Partnership—nor does it plan to forge new ones in the foreseeable future. This means that if the IPEF fails, the United States will return to a status-quo in which it will not be a party to any regional trade agreement in Asia. It is also important because, as President Biden emphasized during his speech, the Indo-Pacific collectively accounts for half of the world’s population and over 60 percent of the global GDP.\nA recent APCO Forum piece summarizes how the IPEF will generally be organized and executed. Other than a rebranding of the four individual pillars that will constitute the IPEF, the launch event did not reveal any significant changes in how the Administration intends to proceed with the initiative. Twelve countries—Australia, Brunei Darussalam, India, Indonesia, Japan, Republic of Korea, Malaysia, New Zealand, Philippines, Singapore, Thailand and Vietnam—were present for the launch. However, as experts from the Center for Strategic and International Studies (CSIS) noted in a recent piece, “it is not yet clear which pillars each country plans to participate in.” In a joint press conference held between President Biden and Japanese Prime Minister Fumio Kishida on May 23, Kishida captured the sentiment of many regional partners and allies when he said, “Japan welcomes the launch of the Indo-Pacific Economic Framework . . .. Having said so, Japan hopes to see the United States return to the TPP from a strategic perspective.”\nDetails—other than the countries that initially intend to engage with the IPEF—remain sparse. The same CSIS experts assess in their piece that negotiations with interested countries will begin in mid-Summer, with agreements likely being finalized just ahead of the November 2023 U.S.-hosted Asia-Pacific Economic Cooperation (APEC) Leaders’ Meeting.\nWhile the IPEF launch succeeded in convening a group of regional partners that represent 40 percent of world GDP, we may not see continued smooth sailing moving forward. Success for the United States in negotiating IPEF with its partners will hinge on their ability to convene productive future negotiations around the IPEF’s four pillars.\nAs negotiations commence, there are three factors which will indicate progress (or not) in the IPEF negotiation process.\nHuman rights and labor standard issues among member countries\nPresident Biden contends the IPEF will hold all member countries to high labor and environmental standards. That said, the Office of the United States Trade Representative (USTR)’s Labor Advisory Committee for Trade Negotiations and Trade Policy recently questioned Malaysia, India, Indonesia, the Philippines and Thailand’s participation in the IPEF citing poor records on labor rights, child labor and human rights. According to the Labor Advisory Committee, “given the scale and scope of worker rights abuses the IPEF must include strong and enforceable labor standards and corporate accountability measurers integrated throughout each of the framework’s modules or pillars.” Notably absent from the initial IPEF launch were Myanmar, Cambodia and Laos.\nThe IPEF’s lack of market access incentives\nWithout market access incentives, standards within the IPEF must provide clear indication that member countries will benefit from increased U.S. participation in the Indo-Pacific region. Necessary incentives ensuring lower barriers to access, detailed parameters around supply chain resilience, cooperative trade facilitation, climate action and a linked digital economy agenda—among other commitments—must come to fruition during negotiations to provide Indo-Pacific countries incentive to continue to take part in the IPEF. As CSIS noted, “without the promise of greater U.S. market access, a significant incentive for regional partners to agree to high U.S. standards goes away. A lack of enforcement mechanisms also limits the ability of the United States to secure its interests in the framework.”\nThe framework’s decentralized pillar program\nThe “pick and choose” four pillar program, in which IPEF member countries must adhere to the entirety of whichever pillars they choose to partake, adds further obstacles to the negotiations. The ability of countries to pick and choose also instills uncertainty in the trade community in the United States, which traditionally looks for binding and enforceable agreements. In the context of human rights and labor standards, this is concerning.\nIf the United States is able to keep interested parties at the table, the IPEF could be a benefit for U.S. businesses. Unified standards among member countries would establish a system that holds developing Indo-Pacific nations accountable to their commitments on labor standards, corruption, decarbonization standards and others. An Indo-Pacific that is economically tied to the United States provides greater opportunity for development, thus providing U.S. business greater opportunity to take part in each market.","A Historic Deal: the Signing of the Regional Comprehensive Economic Partnership\nThe major economies of Asia and Oceania entered the largest regional free trade agreement, shifting growth prospects (even more) to the East\nPublished by Marzia Moccia. .Free trade agreements Asia Conjuncture Oceania Foreign markets South-east Asia Global economic trends\nLog in to use the pretty print function and embed function.\nAren't you signed up yet? Log in!\nOn the international trade front, one of the most important news in recent days has been the signing of the largest free trade agreement in the world: the Regional Comprehensive Economic Partnership (RCEP). The agreement, signed last 15 November, establishes a trade area that includes about 30% of the world's GDP and about 2.2 billion consumers, with 10 ASEAN economies (Indonesia, Vietnam, Cambodia, Indonesia, Laos, Malaysia, Myanmar, Philippines, Singapore, Thailand and Brunei) and the main economies of the Asia-Pacific: China, Japan , South Korea, New Zealand, Australia.\nProposed for the first time in 2012, the agreement was signed after several years of negotiations, which initially involved also India. Over the past year, however, the Country left the negotiating table, concerned about the potential threat of China to the domestic industry.\nAlthough many details are yet to be disclosed, the RCEP is divided into 20 chapters, providing for a specific discipline on the reduction of tariff barriers - with the aim of reducing 90% of existing tariffs over 20 years - and introducing a harmonisation of non-tariff barriers (rules of origin and technical standards). The trade agreement also provides for specific rules on investment protection, e-commerce and services liberalisation. Unlike the FTAs concluded by several countries of the area with the European Union, the new RCEP is more focused on tariff abolition and trade facilitation, as it does not provide for a discipline on more modern issues, such as environmental and labour standards or a regulation for public procurement. According to several commentators, it was precisely this characteristic, less ambitious than the new FTA agreements, that represented a strength during negotiation phase, since the agreement includes countries with even profoundly diverse degrees of economic development.\nHowever, the reasons why the new trade agreement should be seen as an historic pact for the future of international economic relations go far beyond the simple commercial aspects. They relate mainly to four main geo-economic considerations:\n- The process of regionalisation of value chains within the so-called \"Asian bloc\"\n- The greater autonomy of China from Western value chains\n- The support of a multilateral approach to international trade\n- The competitive challenge for EU and the US\nThe process of regionalisation of value chains within the \"Asian bloc\"\nThe introduction of an integrated economic Asia-Oceania area involves a region where internal trade already accounts for almost 30% of total international flows: in 2019 intra-area commercial flows reached $2366 billion, as shown in the graph below.\nTrade with the other geographical regions is much lower and, in almost all cases, the value of exports is higher than imports, with a trade surplus of the RCEP area.\nMore than a third of internally traded goods is represented by intermediate goods, highlighting an already existing significant economic interdependence within the area. The most significant gains from the trade deal are expected in the medium term, deriving from the harmonisation of the rules of origin and the definition of common technical standards, currently subject to several disciplines and often dependent on previous bilateral agreements.\nThe trade deal will therefore help to strengthen the degree of integration existing between different countries of the area, expanding the basket of goods traded and encouraging local businesses to seek suppliers within the RCEP region.\nIt should also be stressed that the trade agreement involves countries with a very different degree of development, which therefore have complementary cost advantages. This evidence might also allow an efficient productive specialisation in the several districts.\nMajor autonomy from China\nFrom the point of view of geopolitical balances, the agreement marks a fundamental step in delineating the perspective influence of China in the region. In fact, China holds a position of absolute leadership in the market, with a share close to 30%. The growth of the Dragon Country in the area has also been uninterrupted and has affected all the main categories of goods, as shown in the graph below.\nWhile the trade agreement gives China an incentive to strengthen its already significant supply chain links, it also builds a large potential market for its consumer goods. The strategy is in line with the objective of strengthening the domestic industry announced by the country as part of the 14th Five-Year Plan, which will consolidate the gradual upgrading of China's exports of goods, especially in technology-intensive sectors.\nA signal in favour of multilateralism\nIn a context of growing protectionism, the agreement goes against the trend, strengthening the involvement of the countries in the area in supporting a multilateral bargaining mechanism. The signing of the RCEP has also speeded up economic integration in North-East Asia establishing a common economic future, after negotiations on a possible China-South Korea-South Japan agreement had been stalled for several years.\nThe competitive challenge for US and EU\nThe signing of the agreement poses significant geo-political challenges for the United States and the European Union, for which a significant reduction of influence in an area of strong growth could be expected, to the benefit of China. However, while the EU has been an active partner within the region in recent years, signing Economic Partnership Agreements with Japan, South Korea and Vietnam, and engaging in intensive negotiations with China, the same cannot be said for the US.\nAfter the withdrawal from the Trans-Pacific Partnership (TPP) by outgoing president Donald Trump in 2017, the definition of the possible role of the US in the economic future of the Pacific region will be left to the new President-elect Biden. One of the Obama administration's objectives during the TPP negotiations was to strengthen economic ties in the Pacific as a counterbalance to the rise of the Dragon country, through a \"new generation\" agreement that would also regulate labour and environmental standards.\nThe signature of the RCEP agreement could be an opportunity to resume the Atlantic dialogues and define a common strategy by the EU and the US."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"future"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:2ab51c06-4a83-4b23-8ef9-e866d7a3ea77>","<urn:uuid:906f6794-4652-4352-9ae2-997cd6e27723>"],"error":null}
{"question":"What are the legal protections for National Guard members' employment, and how does workplace stress affect accountants' work-life balance in terms of email habits?","answer":"National Guard members are protected by USERRA, but only for federal service, not state duty (though state laws may offer additional protections). Meanwhile, professional accountants face significant work-life balance challenges, with 38% checking their emails outside work every day, and 33% checking emails even while sick or on annual leave. This constant connection to work contributes to stress levels, with one-third of accountants experiencing stress daily.","context":["It is important to notice that National Guard members may carry out service under either federal or state authority, however solely Federal National Guard service is roofed by USERRA. Nonetheless, navy employees on energetic state duty may qualify for protection under broader state legal guidelines patterned after USERRA, or beneath state legal guidelines defending emergency reduction workers. No, the federal government recently despatched a stern reminder to all employers, particularly those concerned in providing healthcare, that they need to nonetheless comply with the protections contained within the Health Insurance Portability and Accountability Act (HIPAA) Privacy Rule during the COVID-19 coronavirus outbreak. Department of Health and Human Services (HHS) issued a reminder after the WHO declared a global health emergency. In reality, the Rule contains provisions which might be instantly relevant to the present circumstances.\nA distinction between main and secondary authorized rules, where a main rule governs conduct and a secondary rule allows the creation, alteration, or extinction of major rules. In the House, a Representative administers the oath to the Speaker, who then does the identical to the opposite members.\nUsed generically, the time period decide may discuss with all judicial officers, together with Supreme Court justices. Federal public defender organization As provided for within the Criminal Justice Act, a corporation established inside a federal judicial circuit to symbolize criminal defendants who cannot afford an sufficient protection.\nby Susan Nevelow Mart of the University of Colorado Law School tested if on-line legal case databases would return the identical related … Read More\nLegal Aid Services of Oklahoma Focusing on Holistic Family Advocacy\nThe Bachelor’s in Legal Studies program develops competencies in areas such as authorized analysis and writing, understanding of various areas of legislation, skilled ethics, criminal procedure, and more. Students put together to pursue both a variety of careers, including as paralegals or legal assistants, as well as future graduate research. With a handful of undergraduate and graduate packages, the Department of Legal Studies, Public Administration, and Sport Management on the University of West Florida offers college students with diverse choices to pursue academic and professional paths. As part of the College of Education and Professional Studies, the Department offers access to College resources for college students, as well as group engagement alternatives through outreach packages.\nLegal methods are subsequently the kind of factor that is apt forappraisal as just or unjust. It is unnecessary to ask whether or not a certain fugue is simply or to demand that it turn into so. The musical requirements of fugal excellence are pre-eminently inside—a great fugue is a good instance of its style; it ought to be melodic, fascinating, inventive and so forth.—and the further we get from these inside requirements of excellence the more diverse evaluative judgments about it become. While some formalists flirt with similar ideas about regulation, this seems inconsistent with law’s place amongst human practices.\nRather, the concept underlying the Charter was certainly one of state sovereignty constrained by international regulation and likewise deepened by a way of purpose, … Read More\nSome of the packages cowl how legal guidelines relate to politics and economics, while some of them emphasize authorized research and writing. The Burke School of Public Service and Education’s Bachelor of Science in Legal Studies diploma program prepares students to pursue a wide range of careers in regulation and related fields. Students engage in a balanced curriculum that promotes a comprehensive understanding of the legislation, authorized follow, and legal providers, and develops researching and writing abilities, in addition to ethics.\nWith this CRN we hope to collaboratively work toward finding progressive solutions to the problems that these workers face, on the identical time contributing to the scholarly community by filling a spot within the Law and Society community. Through the annual meetings of the Law and Society Association, the Citizenship and Immigration Collaborative Research Network supplies a forum by which scholars and practitioners who are thinking about these issues can manage discussions, share work, and exchange ideas. In the past, we now have met to check research interests in various national settings, and we now have organized panels and roundtables on citizenship and immigration. Interested colleagues and researchers can join the listserv by emailing Miranda Hallett at\nIdeal for college students, regulation school school, and authorized researchers, LegalTrac supplies indexing for more than 1,200 major legislation evaluations, authorized newspapers, specialty publications, Bar Association journals, and worldwide legal journals, including more than 200 titles in full text. The American Association of Law Libraries not solely endorses LegalTrac, its special advisory … Read More","Professional accountants stressed out by workplace pressures\nSurvey says stress affecting personal relationships, health and more\nTORONTO, May 13, 2019 – One in three professional accountants feel stressed on a daily basis, according to a new study by a wellbeing organization supported by the Institute of Chartered Accountants in England and Wales (ICAEW). The accounting profession is in the grip of a mental health crisis, with professional accountants grappling with workplace pressures and rising industry stress levels. Just two per cent of respondents surveyed reported no stress at all.\nThe Chartered Accountants’ Benevolent Association (CABA) is a charity that provides lifelong support largely to past and present members and their families, including emotional and physical wellbeing. It was established in 1886, six years after the formation of the ICAEW, in 1880. Whilst CABA works in partnership with the ICAEW, it is independent and all communication is confidential.\nNearly two-fifths (37%) of survey respondents said their job was the main cause of their stress, while a third (29%) cited the difficulty of trying to maintain a work-life blend. Driving this issue home, two-fifths (38%) check their emails outside work every day, and a third (33%) even check their emails while sick or on annual leave.\nKelly Freehan, service director, CABA, explains: “While a certain degree of pressure can help with motivation, if stress levels are excessive, we risk becoming less productive or burning out. With our research finding that many chartered accountants feel their workloads are so severe that they need to constantly check their emails outside work, it’s clear that firms should be actively encouraging their staff to maintain a healthier work-life blend.”\nMental health and wellbeing has been the focus of several articles in Canadian Accountant. Chartered professional accountants looking for ways to cut workplace mental health costs have been instrumental in the development of the National Standard of Canada for Psychological Health and Safety in the Workplace. We have also published articles on mindfulness and meditation for accountants.\nSome CPA organizations provide health and wellbeing resources to their members. CPA Alberta, for example, offers CPA Assist, Alberta’s CPA Assistance and Wellness Program.\nCABA’s new “mental wellbeing” campaign, however, is unique in its scope. Featuring a dedicated microsite, the campaign will provide support and advice to empower members of the ICAEW chartered accountant community to take greater care of their mental wellbeing.\nAccording to CABA, there are various pressures within the workplace itself, which many accountants are grappling with on a regular basis and are sure to contribute to the rising industry stress levels. The research found that the most commonly felt workplace frustrations include:\n- being overworked (41%)\n- office politics (33%)\n- feeling undervalued (29%)\n- failure to increase pay or rewards (29%)\n- having to attend too many meetings (28%)\nA fifth (21%) of respondents cited money as the main cause of stress, though this was of greater concern to younger and middle-aged respondents than it was to their older colleagues. A quarter (24%) of 18-34-year-olds and a third (32%) of 35-44-year-olds report money being their main source of stress. This was in comparison to just 1-in-10 (10%) 45-54-year olds and fewer than a fifth (17%) of those aged over 55.\nThe research actually found that younger and middle-aged chartered accountants are likely to feel more stressed overall than their older colleagues. More than two fifths of 18-34-year-olds (43%) and 35-44-year-olds (45%) report feeling stressed every day, compared with just 13% of 45-54-year olds and 15% of those aged over 55.\nSays Freehan: “It’s particularly concerning to see that so many young people within the industry are wrestling with stress, with our research showing that they are the most likely to take work home, stay late in the office and work on days off. Business leaders must provide tangible support that helps staff to form healthy working habits at the start of their careers, if we’re to avoid the risk of fewer young people seeking opportunities in accountancy.”\nWhatever the root cause, it’s clear that stress is having a major impact on accountants. Three-quarters (76%) claim that work has negatively affected them in the past 12 months, with a particular strain being placed on their social lives. Some of the most common effects include:\n- a close relationship being damaged (61%)\n- being unable to partake in hobbies (37%)\n- feeling unable to concentrate on non-work-related issues (28%)\n- being unable to see friends (27%)\n- putting on weight due to a lack of work-life blend (27%)\nColin Ellis is managing editor of Canadian Accountant. With files from CABA. Image from rawpixel.com."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:f82b64bc-0c9c-4fd5-99a6-a69436f682da>","<urn:uuid:1dfaadd0-6c2e-48d5-8ba2-6129688b4b58>"],"error":null}
{"question":"How do the scientific achievements of Heinrich Hertz compare to modern discussions of zero-point energy in terms of their experimental validation and acceptance by the physics community?","answer":"Heinrich Hertz's scientific achievements, particularly his discovery of radio waves, were rigorously validated through careful experimentation and provided concrete evidence confirming Maxwell's electromagnetic theory. He developed innovative experiments to measure wave patterns, wavelength, and frequency, demonstrating that these waves moved at the speed of light and exhibited properties like diffraction and polarization. In contrast, zero-point energy claims, particularly regarding free energy generation, lack experimental validation and misinterpret established physics. While the Casimir effect (related to zero-point energy) is scientifically proven, its actual implications show that any available energy would be extremely tiny or zero, and impossible to tap into as it represents the minimum possible energy state. The physics community accepts Hertz's work as foundational science, while viewing free energy claims based on zero-point energy as a misunderstanding of complex physics.","context":["Oct 22 2013\nFree energy is to physics what creationism is to evolutionary biology. Both offer a teaching moment when you try to explain why proponents are so horribly wrong.\nFree energy proponents have been abusing the laws of thermodynamics (come to think of it, so have creationists), and more recently quantum effects a la zero-point-energy. Now they are distorting a new principle of physics to justify their claims – the Casimir effect. Apparently this was a hot topic at the Breakthrough Energy Conference earlier this month.\nBefore I get into the specifics, I do want to address the general conspiratorial tones of the free-energy movement. I wonder if anyone influential in the free-energy subculture realizes that their conspiracy-mongering over free energy is perhaps the greatest barrier to their being taken seriously. There is also the fact that they get the science wrong, but if they think they are doing cutting edge science (rather than crank science), then convince us with science and ditch the conspiracy nonsense.\nHere is the opening paragraph from a recent blog pushing the Casimir effect as a source of free energy:\nWho is benefiting from suppressing scientific research? Whose power and wealth is threatened by access to clean and free energy? Who has the desire to create a system where so few have so much, and so many have so little?\nOK – you lost me right there. This is a naive child’s view of the world, where “the adults” form a monolithic inscrutable force controlling the world. When you actually become an adult you may realize that no one has total control. No one and no institution is that competent, powerful, and pervasive. It would take an obviously totalitarian state to exert that much control.\nIf free energy were real, someone would be making it happen. Ironically the very existence of the free-energy movement proves their own conspiracy theories wrong. If a company could produce a genuine free-energy machine, they would, and they would become the wealthiest company in the world. Further, free energy would improve everyone’s quality of life. No matter who you are, your life would become better with free energy.\nFree energy proponents, apparently, would rather believe the world is run by megalomaniacs who are simultaneously brilliant (in executing their conspiracy) and idiotic (in wanting to execute their conspiracy) rather than entertain the possibility that they have the science wrong.\nThe Casimir Effect\nScientific American has a good quick discussion of what the Casimir effect is. The Casimir effect is related to zero point energy, which refers to the fact that a perfect vacuum in space still contains energy in quantum fluctuations. This is sometimes referred to as the quantum foam, out of which virtual particles are created and destroyed.\nThis quantum vacuum energy exists as various wavelengths – in fact, infinite wavelengths. When you place two mirrors facing each other in a vacuum, some of these waves will fit in the space between and some will not. This creates a situation in which there is more energy in the vacuum outside the mirrors than between them, which in turn results in a tiny force attracting the two mirrors together.\nThis effect was predicted by Dutch physicist Hendrick Casimir in 1948, and later confirmed by experimentation. I must emphasize that this force is extremely tiny.\nHere is where the free-energy gurus, however, have their fun. Our current understanding of quantum effects predicts that there is an infinite amount of this zero-point energy in the vacuum. Imagine if we could somehow tap into that energy – infinite free energy. You can see why this is an exciting idea.\nThere are two problems with zero-point energy as a source of free energy, however. The first is that there almost certainly is not an infinite amount of zero-point energy in the vacuum. If there were, then relativity predicts that space-time would be infinitely curved in on itself by this energy, which it obviously isn’t. In another article for Scientific American, physicist Matt Visser explains:\nObservation indicates that in our universe the grand total vacuum energy is extremely small and quite possibly exactly zero. Many theorists suspect that the total vacuum energy is exactly zero.\nVisser also explains that the total quantum energy in the vacuum is a property of the universe known as the cosmological constant. He explains:\nFrom a particle physics point of view, however, these limits are extremely stringent: the cosmological constant is more than 10(-123) times smaller than one would naively estimate from particle physics equations. The cosmological constant could quite plausibly be exactly zero. (Physicists are still arguing on this point.) Even if the cosmological constant is not zero it is certainly small on a particle-physics scale, small on a human-engineering scale, and too tiny to be any plausible source of energy for human needs–not that we have any good ideas on how to accomplish large-scale manipulations of the cosmological constant anyway.\nThe total amount of zero-point energy therefore ranges from exceedingly tiny to precisely zero. It is clearly not infinite, and it is not even a usable amount.\nThe second problem is that it is impossible to tap into this zero-point energy as a source of usable energy. This is because, by definition, zero point energy is the minimum energy state. Again from SA:\n“The zero-point energy cannot be harnessed in the traditional sense. The idea of zero-point energy is that there is a finite, minimum amount of motion (more accurately, kinetic energy) in all matter, even at absolute zero. For example, chemical bonds continue to vibrate in predictable ways. But releasing the energy of this motion is impossible, because then the molecule would be left with less than the minimum amount that the laws of quantum physics require it to have.”\nSo, if zero point energy is a real thing rather than just a convenient mathematical artifact, the amount of energy it represents is too small to be of any use, and we could never tap into that small amount of energy anyway because by definition it is the lowest energy state that can exist.\nNone of this stops free energy enthusiasts from thinking that the Casimir effect proves that zero-point energy is real and a potential source of free energy. Again from the blog linked to above:\nThe implications of this are far reaching and have been written about extensively within theoretical physics by researchers all over the world. Today, we are beginning to see that these concepts are not just theoretical, but instead very practical and simply very suppressed.\nSuppressed? You can Google it and find all the information you could ever want. How is this information suppressed?\nTo summarize, it seems that zero-point energy gurus are only looking at a small slice of the physics. They see the equations for zero-point energy as it relates to electromagnetic fields, for example, and get all excited when the equations show infinite energy. However, physicists understand the full picture – when you look at all the stuff in the universe, these mathematical artifacts all average out to zero. Maybe there is a tiny residue of minimum energy left over, but you could never tap into it anyway.\nThere is no conspiracy – just a willful misunderstanding of complex physics.\n23 Responses to “Free Energy and the Casimir Effect”\nLeave a Reply\nYou must be logged in to post a comment.","German physicist Heinrich Hertz discovered radio waves, a milestone widely seen as confirmation of James Clerk Maxwell’s electromagnetic theory and which paved the way for numerous advances in communication technology. Born in Hamburg on February 22, 1857, Hertz was the eldest of five children. His mother was Elizabeth Pfefferkorn Hertz and his father was Gustav Hertz, a respected lawyer who would later become a legislator. In his youth Heinrich displayed an interest in building things, and as a teenager he constructed a spectroscope and a galvanometer that were so well designed that Hertz used them throughout his college years. Initially Hertz planned a career in engineering, but after a year of employment at the public works office in Frankfurt, a summer of classes at the Polytechnic in Dresden, a year of military service in Berlin, and a brief stint in the engineering department at the University of Munich, he finally decided to pursue the subject that most deeply interested him: science.\nThroughout his life Hertz read works on science and carried out experiments as a hobby. But once he decided that science was to be his career, he applied himself to these tasks with even greater enthusiasm. In the winter of 1877, he studied various scientific treatises, and the following spring, he gained some laboratory experience by working with Gustav von Jolly. Subsequently he enrolled at the University of Berlin, where he was privileged to study under the great German physicist Hermann von Helmholtz. With Helmholtz’s encouragement, Hertz resolved to compete for a research prize to be awarded to the student best able to determine whether or not electricity moved with inertia. Hertz began a series of experiments into the matter, and this mode of learning seemed to suit him. He confided in a letter sent to his family during that time, \"I cannot tell you how much more satisfaction it gives me to gain knowledge for myself and for others directly from nature, rather than merely learning from others and for myself alone.\"\nIn August of 1879, Hertz won the prize for his evidence demonstrating that electricity had no inertia. Another prize problem was soon proposed by Helmholtz, who wanted students to attempt to prove which of the theories of electromagnetic phenomena then circulating was correct. Interestingly, Hertz did not choose to compete for this prize, but years later would be the first person to successfully provide the kind of definitive evidence that Helmholtz sought. At the time, Hertz instead embarked on a study of induction produced by rotating spheres. His work in this area helped him earn his doctorate degree ahead of schedule, in 1880, magna cum laude.\nHertz’s first academic post was as lecturer of theoretical physics at the University of Kiel, but due to his dissatisfaction there he accepted a position at the Karlsruhe Polytechnic in 1885. It was at Karlsruhe, where he remained until he received an appointment as physics professor at the University of Bonn in 1889, that Hertz carried out his most important work. In 1886, Hertz began experimenting with sparks emitted across a gap in a short metal loop attached to an induction coil. He soon built a similar apparatus, but without the induction coil, to act as a detector. When the induction coil connected to the first loop (the transmitter) produced a high voltage discharge, a spark jumped across the gap, sending out a signal that Hertz detected as a weaker spark across the gap in the receiving apparatus, which he placed nearby. To determine the nature of the signals that he was able to transmit and receive, Hertz developed a number of innovative experiments.\nBy measuring side sparks that formed around the primary spark and varying the position of the detector, Hertz was able to determine that the signal exhibited a wave pattern, and to ascertain its wavelength. Then, by using a rotating mirror, he found the frequency of the invisible waves, which enabled him to calculate their velocity. Amazingly, the waves were moving at the speed of light. Thus, it appeared to Hertz that he had discovered a previously unknown form of electromagnetic radiation, and in the process confirmed James Clerk Maxwell’s theory of electromagnetism. To further prove that this was indeed the case, Hertz continued his experiments exploring the behavior of the invisible waves. He discovered that they traveled in straight lines and could be focused, diffracted, refracted and polarized. Hertz announced his initial discovery in late 1887 in his treatise \"On Electromagnetic Effects Produced by Electrical Disturbances in Insulators”, which he sent to the Berlin Academy. He later published additional details following the series of experiments he carried out in 1888. For a time the waves he discovered were commonly referred to as Hertzian waves, but today they are known as radio waves.\nIn addition to his radio wave breakthrough, Hertz is notable for the discovery of the photoelectric effect, which occurred while he was investigating electromagnetic waves. Because of some difficulty in detecting the small spark produced in his receiving apparatus, Hertz sometimes placed the receiver in a dark case. This, he found, affected the maximum length of the spark, which was smaller than when he did not use the case. With further research into the phenomenon, Hertz discovered that the spark produced was stronger if it was exposed to ultraviolet light. Though he did not attempt to explain this fact, others, including J.J. Thomson and Albert Einstein, would soon realize its importance. The phenomenon of electrons being released from a material when it absorbs radiant energy, which was the cause of the stronger sparks observed by Hertz when ultraviolet radiation was used, would come to be known as the photoelectric effect.\nAfter 1889, when Hertz was teaching at the University of Bonn, he studied electrical discharges in rarefied gases and spent a significant amount of time composing his Principles of Mechanics. Unfortunately, he never saw the work published due to his premature death associated with blood poisoning on New Year’s Day 1894. Only 37 years old at the time, Hertz also never lived to see the tremendous impact the discovery of radio waves would have on the world in the 20th century."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"future"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:add830ad-7caf-4c84-892a-f6879c61ee50>","<urn:uuid:9a7cc391-5e00-478b-bd29-65c9f202db33>"],"error":null}
{"question":"What's the load capacity difference between heavy duty vs light duty wheelbarrows?","answer":"Heavy duty (contractor grade) wheelbarrows have a larger capacity of six to ten cubic feet and cost between $200-500. They're built with sturdy construction including added steel supports and heavier gauge materials. Light duty (homeowner) models have a smaller capacity of four to six cubic feet, cost $60-250, and are lighter in weight. While heavy duty models require more strength to use when loaded, they withstand years of punishing use and are better for hard daily work.","context":["Caring for horses means spending a good bit of each day moving materials from one place to another. There’s feed to carry down the aisle and distribute to each horse, and hay to deliver to the stalls and paddocks. The manure needs to be taken out to the pile, and new bedding hauled in and distributed. Small wonder, then, that we depend so much on wheelbarrows, carts and wagons.\nAs simple and common as these tools are, they come in a variety of sizes, materials and styles. Most were designed for use at construction sites or in home gardens, and the best model for your horsekeeping needs may not always be obvious. In fact, at many horse barns you’ll find more than one wheelbarrow or cart, each dedicated to a different job.\nThe key to selecting the right wheelbarrow or cart for your situation is to match specific features to the jobs you do and the conditions under which you work. You well know what you need to accomplish each day around the barn. Here’s what’s available to assist you.\nThe difference between wheelbarrows, carts and wagons\nA wheelbarrow, that familiar form with one wheel set at the front of a curved carrier tray, has long been used for mucking stalls and carrying bags of feed and bedding. Its single front wheel makes it easy to negotiate tight turns through doorways and around obstacles. The rounded, sloped front of the carrier tray is ideal for dumping loads of loose material, such as manure or shavings, and it’s easy to clean with a hose.\nHowever, the shape of the tray can be clumsy for carrying flat or bulky items. It takes physical strength to lift and push a loaded wheelbarrow as well as a bit of dexterity to keep it upright. The single-wheel design can tip readily, especially when the wheelbarrow is pushed over uneven terrain. Some models have two wheels placed side by side at the front of the carrier; these provide more stability under heavy loads but require a larger turning radius.\nClick here to see what your stall-cleaning style says about your personality.\nA garden or utility cart has two distinguishing features: a relatively flat bed suited to carrying stackable items such as hay bales and bagged feeds, and two wheels–one on each end of an axle positioned under the carrier tray. Carts are also more stable and less prone to tipping than traditional wheelbarrows, and because the load is balanced over the axle, more of the weight is borne by the wheels, so it takes less strength to lift and move it. Many are designed to be pulled rather than pushed–making them easier for some people to manage. However, carts require a wider turning radius than a wheelbarrow and may be difficult to maneuver in close quarters.\nSome utility carts have plastic carrier trays with slightly curved sides; others have straight-sided, rectangular trays with flat beds made of metal or wood. Many carts also have release mechanisms for dumping their loads.\nA feed cart is designed for hauling grain and other processed feeds to farm animals. Most are plastic, with a single tublike container set upon wheels. Some have two wheels with supporting legs; others rest on three or four wheels. Several feed carts designed specifically for feeding horses have multiple compartments to hold different concentrates and/or supplements, so each horse can receive a customized portion as the cart is moved down the aisle.\nSome models, which are also called chore carts, have very deep, large-capacity tubs. These are designed for transporting a large amount of feed that will be dumped in one place, such as a trough for swine or cattle. A person using one to feed horses would have to reach all the way to the bottom to scoop out the last of the grain.\nA wagon is a four-wheeled vehicle, usually pulled by a single handle, that can carry much heavier loads, some as much as 2,200 pounds. Because all of the weight rests on two axles, no lifting at all is required to pull the load. Some also have removable sidewalls, so they can be converted into flatbeds for bulky items. However, having two axles spaced at the front and back of the load means that maneuverability in tight spaces is difficult.\nA dolly, or muck bucket cart, is a metal-framed cart, often with an adjustable handle, designed to hold one or two 70-quart buckets. This can be a lightweight, inexpensive alternative for someone with only one or two horses, but it probably doesn’t offer enough capacity for someone who has to muck and feed a full barn.\nHeavy vs. light duty wheelbarrows and carts\nWheelbarrows and carts are typically divided into two categories: those for homeowners, and those for contractors or industrial use.\nThe contractors grade models, called heavy duty, are designed to withstand daily use with larger, heavier loads. They have a sturdy construction, typically with added steel supports for the underside and a heavier gauge plastic or steel for the carrier tray. These models are built for a larger capacity (six to 10 cubic feet) and are more expensive (roughly $200 to $500). It takes more muscle to use them when they’re loaded, but these wheelbarrows will withstand years of punishing use and are the better choice for hard, daily work.\nThe homeowner models are intended for occasional use in landscaping or yard work. They are lighter in weight and usually have a smaller capacity (four to six cubic feet); they are also less expensive (roughly $60 to $250). These models might be suitable for use in a small barn, or they might be preferred by someone who has difficulty handling heavier loads, but they won’t last as long when put to rigorous use.\nMaterial options for wheelbarrows and carts\nPlastic is often preferred for wheelbarrows or carts used in horse barns because it is versatile, durable and lightweight; it is easy to clean with a hose; and it requires little maintenance. Even so, with heavy use over time, plastic carriers may develop scratches, which can trap residues, and they can crack and break.\nMetal trays, usually made of steel with some sort of protective coating, are stronger and more durable than plastic but they are also heavier and require more maintenance. These models are designed for jobs that require heat, such as mixing asphalt.\nA word about wheels\nWheelbarrows come with two types of wheels:\nPneumatic tires, which look like miniature automobile tires complete with inner tubes, can handle heavy loads and are well suited for use on soft or loose ground. The wider the tire, the more smoothly it rolls over gravel, mud or sandy soil with minimal damage to the ground surface. Most are three or four inches wide, but some wheelbarrows come with tires as wide as six inches. The air cushion in the inner tube also provides a smooth ride and minimizes bounce over uneven terrain or obstacles such as doorsills or curbs.\nPneumatic tires are, however, prone to flats. It’s a good idea to keep a replacement inner tube on hand for repairs and to store a bicycle pump nearby to maintain tire pressure.\nSolid or flat-free tires will never go flat. Some are molded from a single solid piece of hardened rubber; others look like pneumatic tires but are molded with tiny air pockets inside to cushion the load. These tires are designed for use at construction sites, where there may be screws, nails and other sharp debris scattered over the ground. This, of course, is not the case around the barn, but flat-free tires do save the maintenance issues associated with pneumatic tires. Solid tires can handle heavy loads, but they do tend to be harder and provide a rougher ride than the pneumatic tires.\nMany models of wheelbarrow can be used with either type of tire, which can be purchased separately.\nSome heavy-duty carts come with spoked wheels, which resemble those on bicycles. They have a wider diameter than other wheelbarrow tires, which makes them easier to pull over hard surfaces, such as barn floors or pavement, even when the cart is fully loaded.\nThese wheels are also easier to pull over very large obstacles, such as rocks or branches. However, spoked wheels cannot handle the heaviest loads, and because the tires are narrow, they are difficult to pull through, and will slice into, soft footing.\nTest driving wheelbarrows\nAs good as a particular wheelbarrow or cart may look, you won’t know if it “feels” right until you try it out. The height of the handles and the way the carrier balances over the wheels will make a big difference in how easily you can use it. Also consider the height of the bed, and how high you’d have to reach or how low you’d have to stoop to load and unload it.\nIf possible, try out the models you are interested in at the store. Pile some weight–bags of mulch, for example–into the carrier tray and see whether you’re strong enough to handle it when it’s loaded. Don’t forget to check how readily it turns, especially if you’re considering a two-wheeled model.\nChoosing the right wheelbarrow or cart for the task at hand can help you speed through your chores while avoiding an aching back and shoulders at the end of the day. And that will leave you more time to enjoy your horses.\nDon’t miss out! With the free weekly EQUUS newsletter, you’ll get the latest horse health information delivered right to your in basket! If you’re not already receiving the EQUUS newsletter, click here to sign up. It’s *free*!"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:b69de14f-7555-44f8-b00a-63117ecb172c>"],"error":null}
{"question":"Which is more comprehensive - the basic IPCC Guidelines coverage or Scope 3 emissions reporting? Need examples please!","answer":"Scope 3 emissions reporting is more comprehensive, as it covers 15 detailed categories throughout the entire value chain, accounting for 75-95% of a company's total emissions. While the IPCC Guidelines focus on seven specific greenhouse gases (CO2, CH4, N2O, HFCs, PFCs, SF6, and NF3) across economic sectors like energy, industrial sources, agriculture, land use, and waste, Scope 3 extends further to include elements like purchased goods and services, capital goods, business travel, employee commuting, investments, and end-of-life treatment of sold products.","context":["GHGMI is proud to launch the only comprehensive and instructionally rich courses to break down the IPCC Guidelines for Greenhouse Gas Inventories into something that learners can more easily digest. These guidelines provide the technical basis for every type of GHG reporting. Our courses are designed to equip learners with the knowledge to succeed in reporting under the Paris Agreement.\nWhat are the IPCC Guidelines?\nThe Intergovernmental Panel on Climate Change (IPCC) is the premier organization developing methods to estimate GHG emissions. Today, the 2006 IPCC Guidelines (and 2013 Wetlands supplement) contain the latest, scientifically robust and internationally accepted methods for estimating GHG inventories.\nThe IPCC Guidelines outline the preeminent methods for estimating seven GHGs (CO2, CH4, N2O, HFCs, PFCs, SF6 and NF3) across all sectors of the economy: energy (including transport), industrial sources, agriculture, land use change and forestry, and waste. These methods are spread across thousands of pages of documentation, written in dense, technical language.\n“Powerful training for UNFCCC and future NDC reporting.”\n– IPCC Guidelines Course-Series Instructor\nGHGMI’s IPCC Guidelines courses are a dynamic and resource-efficient means for ministry staff to develop the prerequisite knowledge for reporting under the UNFCCC and Paris Agreement.\nTo learn more about each individual course, follow these links:\n- 501 IPCC: Introduction and Cross-Cutting Issues (pre-requisite)\nFlexibility: Available online, these courses are accessible from anywhere in the world 24/7, and are applicable to a broad range of GHG MRV reporting levels, including national, sub-national, program, entity and product levels. Our courses are designed as effective teaching tools for individual learners or as part of a larger capacity-building engagement.\nEvidence-based: Our courses apply high-quality, pedagogy-rich instruction to actively teach learners this technically rigorous material. Lessons include visual graphics, sequential steps, and intuitive curriculum – making it easier for participants to learn. Each course also contains examples and interactive exercises using case studies from GHG inventories and software tools.\nLearner-driven: Our system for learner interaction creates a virtual university for carbon accounting experts. These courses are written in a modular fashion, so that learners with a specific sector interest can focus on that material specifically.\nPolicy relevant/politically essential: As the foundational technical guidance on GHG MRV, the IPCC guidelines are the basis for NDCs, NAMAs, BURs, REDD+ and all forms of GHG reporting.\nThe deployment and translation of the IPCC courses are two of the fast-start, scalable capacity building activities recommended in the activities annex of the Coalition for Paris Agreement Capacity Building.\nFor Project Managers: This series is a cost-effective “out-of-the-box” solution for institutions and organizations to use for MRV training. Going beyond discrete initiatives will help build sustained, long-term capacity. Innovative capacity building approaches allow the international community to more effectively tackle climate change. Hybrid learning that combines rigorous e-learning modules with in-person workshops is a highly effective capacity building approach, supported by pedagogical research. Contact us to discuss how this package can strengthen your trainings.\nFor Grantmakers: Our IPCC courses are a scalable and resource efficient means of capacity building. A scholarship fund for our IPCC courses can provide technical skills to underrepresented groups in the greenhouse gas MRV space. This support is particularly important for individuals from Least Developed Countries. Interested grantmakers and social entrepreneurs should contact us.","Scopes in the Greenhouse Gas Protocol and how do you tackle Scope 3 emissions\nThe European Union has increasingly put emphasis on sustainability, and simultaneously developed new directives and regulations in order to unite reporting on emissions. The European Union strives to align regulations to ensure comprehensive and comparable reports, taking into account not only the companies own emissions but also those of their respective value chains.\nBreaking down the Greenhouse Gas Protocol\nAccording to the most acknowledged emissions calculation method, called the Greenhouse Gas Protocol (GhG Protocol), companies should divide their emissions into three scopes. Calculations made in accordance with the GhG protocol can be viewed as an inventory of emissions, to understand which business activities have the most impact.\nScope 1: Covers direct emissions from the company, and include mobile and stationary fuel, emissions from industrial processes, and fugitive emissions. These emissions are related to buildings, vehicles and stationary engines that are operated by the company.\nScope 2: These are indirect emissions from purchasing energy generated outside the reporting company’s operations, such as electricity, heating, cooling and steam. These are areas that the company consumes and can control, but do not own and produce themselves.\nScope 3: This scope includes indirect emissions divided into 15 categories. Scope 3 is likely to cover 75-95% of the company’s total emissions. The 15 categories in scope 3 are intended to provide companies with a systematic framework to measure, manage, and reduce emissions across the value chain. The categories are divided to be mutually exclusive which will ensure that double counting of Scope 3 emissions are avoided for each company. Scope 3 emissions are associated with both upstream and downstream activities in the value chain.\nThese are the Scope 3 categories in order. Here are some examples of what activities are included in each category and should be counted as emissions in Scope 3 (and are not considered to be Scope 1 or 2 emissions).\n1. Purchases of Goods and Services:\n- Purchasing of office supplies, food for the canteen, goods to be sold to end-users or materials for production such as timber.\n- Purchasing of data storage, cleaning services, and other services.\n2. Capital Goods:\n- Purchase of equipment, machinery, buildings, facilities, and vehicles used to manufacture goods or provide a service.\n3. Fuel and Energy related activities:\n- Mining of coal\n- Refining of gasoline\n- Transmission and distribution of natural gas\n- Production of biofuels\n4. Upstream Transportation and Distribution:\n- All purchased transportation and distribution services.\n- Transportation of purchased goods from 1st tier suppliers.\n- Waste generated in day-to-day operations such as organic waste, general waste, plastic etc. Additionally, includes the emissions from treatment relating to if the waste is recycled, incinerated or goes into landfill.\n- Wastewater from operations.\n6. Business Travel:\n- Distance traveled by employees in different means of transportation such as flight, taxi, train etc. for business purposes.\n7. Employee Commute:\n- Distance traveled by employees between work and home by bike, public transport, car etc.\n8. Upstream Leased Assets:\n- Rented office spaces or leased company cars.\n9. Downstream Transportation and Distribution:\n- The commuting of customers to and from the reporting companies stores.\n- Last mile delivery purchased by customers.\n10. Processing of Sold Products:\n- Processing of glass (intermediate product) to make wine bottles (final product).\n11. Use of Sold Products:\n- Electricity consumption of sold electronics over their expected lifetime.\n- Fuel consumption of sold vehicles over their expected lifetime.\n12. End-of-life treatment of Sold Products:\n- The glass jar is recycled whereas the plastic label goes into incineration.\n13. Downstream Leased Assets:\n- Energy consumption in buildings rented to other companies.\n- Fuel consumption of vehicles leased to other companies.\n- The franchisor, the organization granting the license, should report the Scope 1 and Scope 2 emissions of franchisees.\n- If the company has invested 10% in a company, this company is responsible for 10% of that company’s emissions. Financial institutions additionally have to include emissions from commercial loans, mortgages, project financing etc.\nHow to tackle Scope 3:\nUnderstanding the most important emissions in scope 3:\nStart by going through each Scope 3 category in the GhG Protocol. Understand which are related to the company’s operations and write them down.\nMake a priority list:\nThe categories should be ranked, in order to understand their importance. It is preferred to rank the categories by making a rough estimation of their CO2 emissions, by using a spend based method or an estimation of amount/units bought and multiply these with some CO2 coefficients you can rank them according to percentage of the company’s CO2 emissions. However, if you are unfamiliar with this then the categories can also be evaluated on several other parameters to get an overview of their importance; such as the amount of units bought, the influence it has on operations, the risk it poses or simply the categories with the most spending. Once you know which are the biggest or most important categories, it can help the process to move forward by selecting which ones are also the easiest to start with. You want this step to result in a priority list that balances impact of emissions and feasibility of collecting the data.\nDecide on the methodology:\nEach category in Scope 3 might require different data collection methods. Which methodology you choose depends on availability of data as well as quality and accuracy desired. Generally, you will choose an activity based approach if you want higher quality and more accurate accounting, whereas spend based data is more of an estimation but also easier to handle and collect. The decision should be based on the time frame for collecting data and how many categories you want to cover.\nMake a roadmap:\nOnce you have a clear picture of your categories you have to decide if you want to focus on the biggest category first, or go for the low hanging fruits. The general recommendation would be to focus on one or a few categories at a time and make a roadmap for which categories you want to include for the next report. For banks it can make sense to only focus on category 15, the investments, as they can be 90% of the company emissions. However, if the biggest category requires extensive work and there is a limited time for collection of data, it can make sense to start with some smaller and easier categories. Although the bank’s smaller categories might only cover 10% of emissions for now, it will be a chance to learn how to approach different types of calculations for next year's reporting, and therefore be a good start as well. Trying to do everything at once might result in bad quality data or work overload.\nDo transparent reporting:\nIf you do not have all data needed, or there are any doubts in the data collected, then the best thing is to make notes and tell the truth. It is okay if you do spend based reporting, or if you can only collect data about some of your purchased products and services. However, you have to be honest in your reporting about this and let the readers know the scope and quality of the data collected.\nCorrect data management can help you to get started or refine your GhG accounting. Klappir has a lot of experience in collecting, analyzing and creating reports from high-frequency data on CO2 emissions."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:5046be44-ae5c-441a-a0aa-11fe0dd715be>","<urn:uuid:67567e6c-854e-45c1-b949-7cba0b278572>"],"error":null}
{"question":"How do saltwater chlorination levels compare to DE filter maintenance requirements in modern pool systems?","answer":"Saltwater pools maintain a chlorine level of 1 part per million in the pool water, which is the same as municipal drinking water, despite generating higher levels (10 ppm) at the filter system. For D.E. filter maintenance, the system requires monitoring pressure gauge readings and backwashing when pressure rises 5-10 LBS above normal operating pressure. After backwashing, new D.E. powder must be added through the skimmer. The D.E. filter requires cleaning at least twice per season (beginning and end) to inspect filter fingers, o-rings, and tube sheets, while the saltwater system continuously generates chlorine by splitting sodium chloride molecules, eliminating the need to handle chlorine tablets.","context":["Hayward Perflex D.E. Filters: Bumping Vs Back Washing\nThe Hayward Perflex D.E. Pool Filter operates in a different way then most D.E. pool filters. This type of filter cannot only be back washed but the filter also has an option called “regeneration” which allows you to get the most from your D.E. powder. The Perflex filter also has something called a “bump handle” that most D.E. pool filters do not have. This bump handle allows you to shake the D.E. from the pool filter fingers that are inside of the tank.\nThe Hayward Perflex filter has man options that may seem confusing at first, but once you read and learn how to use this type of pool filter you will have no problem keeping your swimming pool clear and blue. You will need a bag of D.E. powder before you can operate this type of pool filter, So if this is your first time working with this type of swimming pool filter, then the first thing you are going to need is a bag of D.E. powder that can be bought online or locally at any pool supply store.\nWhat Does Bumping The Handle Do?\nBumping is when you lift the bump handle on the top of the Perflex filter up and down. When you bump the handle you move the tube sheets inside of the filter up and down which causes the D.E. to loosen up and move around inside of the filter tank.\nWhat Is Back Washing The Filter Mean?\nBack Washing is the process of removing dirt, debris and also D.E. powder from the pool filters tank. This is done using the swimming pool filter systems pump. You first have to bump the handle on top of the filter a few times and then open a valve to back wash the filter. See below for a full lesson on how to properly back wash your Hayward Perflex D.E. Pool Filter.\nHow Does The Hayward Perflex D.E. Pool Filter Work?\nThere are a few main parts that you will always be working with when you operate your Hayward Perflex D.E. swimming pool filter. The main parts are the filter gauge, the bump handle, the back wash valve and you will also need to have a supply of D.E. filter powder. When a filter is brand new or just has been back washed there is no D.E. powder inside of the filter. Inside of the filter is something called “fingers”. These filter fingers are plastic tubes that are covered by mesh that allow the water to flow through them but not the D.E powder.\nWhen you add D.E. powder through the skimmers, the D.E. powder travels through the swimming pool plumbing, then enters the filter where the D.E. powder then coats the filter fingers. Once the filter fingers are coated with D.E. powder the pool water flows through the fingers and then the D.E. powder catches all the debris. Once all of the D.E. inside of the filter gets covered with dirt and debris, it will be time to regenerate or back wash the filter.\nWhat Is D.E. Filter “Regeneration”?\nRegeneration is a bit different than actually back washing the pool filter. When you regenerate you are basically “flipping” & “re-using” the existing D.E. inside of the pool filter. When you back wash you are removing the old D.E. powder from the pool filter and then replacing it with new. Regeneration does not require you to replace the D.E. powder because you are just “bumping” it around and its sticking back to the filter fingers inside of the tank.\nHow To Regenerate The D.E. Inside The Filter\nRegenerating your D.E filter will allow you to get a bit more filtering action from the D.E. inside of the filter tank. The first thing you will want to do is shut the filter system down. Once you have shut the filter system down you will want to bump the handle up and about 10 times and then you will want to turn the filter back on again. What the bump does is loosen all of the D.E inside of the filter, turning it over and then once you turn the filter back on the D.E. re sticks to the filter fingers. You will want to regenerate if you see the flow of your filter start to slow down. If you have regenerated your filter but you still have poor flow, then your best bet would be to just back wash the filter which removes all of the D.E. from the filter.\nHow Often Do I Back Wash My Perflex Filter?\nOn the side of your filter you will have something called a pressure gauge. This pressure gauge measure the amount of pressure that is inside of your pool filter tank. When your filter has just been back washed you should take a look at the number on that gauge and see what your “normal operating pressure” is. When the pressure on the gauge raises 5 – 10 LBS above the normal operating pressure the filter will have to be back washed.\nHow To Back Wash The D.E. Out Of The Hayward Perflex Filter\nWhen the pressure inside of your filter tank rises you will have to back wash the pool filter. To do so, the first step is to shut down the filter system. Once you have shut down the filter system you will want to bump the bump handle up and down slowly at first but in the end you will want to bump that handle 15 or so times. Once you have bumped the handle you will want to open up the valve that is located on the bottom of the filter tank. This is called the back wash valve and when you open it pool water and D.E. will start coming out. Once you have opened the valve you will want to turn the filter system back on for about two minutes to forcefully flush all of the D.E. out of the filter.\nNow that all of the D.E. powder has been flushed from inside of the filter tank, you are going to need to add more D.E. powder to the filter. You can do so by making sure that the filter system is turned on and running and then you will want to walk over to the pool skimmer and add the required amount of D.E. to the pool filter. You can do so by scooping the D.E. powder into the filter slowly and 1 LB at a time.\nWhen & How Do I Add More D.E. Powder?\nEvery time you back wash the Hayward Perflex filter you will need to re-add some D.E. powder to the filter. Once you have finished back washing you will want to turn your filter system back on. Once the filter system is back on you will want to look on the side of the filter tank and see how much D.E. you are supposed to add to the filter. Once you find out how much D.E. you need to add to the filter you will want to walk over to the skimmer and add that amount slowly inside the skimmer until it all has worked its way to the filter.\nCleaning, Storing & Maintaining Your Hayward Perflex Pool Filter\nThe best way to get the longest life from your pool filter is to properly take care of it. You should take your filter apart at least once in the beginning of the season and once at the end of the pool season. This way you can clean the filter fingers, inspect all of the o-rings, inspect the tube sheets and keep the inside of the filter is perfect shape.\nIf you have any questions, please feel free to ask them below.","Frequently Asked Questions\nWhat is Lothorian Pools?\nThe name Lothorian was originally inspired by J. R. R. Tolkien’s Lothlorien, the beautiful gardens in the fictional universe of Middle-Earth. Adapted, our definition of Lothorian is a state of mind, referring to a natural environment of peace and tranquility where family and friends gather to enjoy life.\nWhy saltwater in a swimming pool?\nA saltwater system creates a better, healthier swimming experience for most people. In a saltwater pool the water feels soft and your skin feels smooth and refreshed. Saltwater is healthier and safer for your hair, eyes, and skin. The amount of salt in a saltwater pool is negligible—3,000 parts per million—as compared to a tear (12,000 parts per million) or the ocean (33,000 parts per million).\nSaltwater pools are NOT chlorine free pools. A saltwater pool is simply one that utilizes a chlorine generator. Chlorine generators have been around for decades. Chlorine is generated at the filter system by splitting the sodium chloride molecule. With a salt generator it is not necessary to handle or buy chlorine tablets. The interesting fact about salt generation is that while chlorine levels at the generator are at a super-chlorination level, 10 parts per million, the chlorine recombines with the water over the length of the piping and the result is a chlorine level of 1 part per million in the pool. Municipal drinking water also has a chlorine level of 1 part per million. This residual chlorine present in the pool eliminates algae and waterline scum build-up.\nWhat is Winter Maintenance?\nWinter maintenance is the management of water chemistry to ensure a longer life for the pool’s interior finish and monitoring of water levels to protect the tile and coping from the effects of freezing and thawing. We will also survey the cover to ensure that it is safe and secure. This service provides monitoring for the filter system, winter plugs, and plumbing lines to ensure the stability of winterization.\nIs diving safe?\nYes, as long as it is done from a specified diving area into an appropriate dive envelope. We prefer to use a dive rock in our designs for two reasons: first, they look cool; and second, because there is no springy, bouncing action. A stable rock is a safer platform. Lothorian conforms to the American National Standards Institute (ANSI) safe diving envelope standards on all of our pools.\nWhy a Diatomaceous Earth filter?\nA diatomaceous earth (D.E.) filter is the most efficient filter on the market. Water is filtered through porous, microscopically small, sponge-like particles that are able to remove particles to 4-5 microns as compared to a sand filter that filters to 40-50 microns. Municipal drinking water is filtered to 5 microns. The D.E. filter also has a greater ease of maintenance, as compared to the labor intensive cleaning of cartridge filters.\nWhat is Diatomaceous Earth?\nD.E. is a natural mineral called a diatom, which is a type of hard-shelled algae. An abrasive, fine white powder, it’s chemical composition consists of 86% silica, 5% sodium, 3% magnesium and 2% iron.\nWhat is the seasonal cost of pool operation?\nPlease see the Maintenance Plans section of this website for details on our maintenance contracts.\nWhy use a PebbleTec product as the pool’s interior finish?\nThe lifespan of a PebbleTec® pool finish is 20+ years as compared to a traditional plaster finish at 7-10 years. PebbleTec also warrants their product for five years as compared to a one year guarantee with traditional plaster finishes. The reasons are evident in the content of each finish’s composition.\nA PebbleTec® finish consists of 50% cement and 50% silica (stone). PebbleTec products are consistent in their color and pebble distribution throughout, and the thickness of the finish is 3/8” to ½”. Because 90-95% of the surface is pebbles, it is much more resistant to acid and poor water chemistry. It is also more resistant to staining and in the unlikely event of the need for a repair to the finish, seeing the patch is far less likely. To learn more or to see completed projects in a specific color, visit www.pebbletec.com.\nTraditionally used, plaster is a combination of 40% cement and 60% calcium carbonate (crushed marble). On the Mohs Scale of Mineral Hardness, marble is a 3 and silica is a 7. (For comparison, a diamond is 10, a fingernail 2.5, and a hardened steel file is a 7.) When applied, plaster is troweled until the “cream” is brought to the surface. That cream consists of calcium carbonate and cement. The cream hardens and becomes a protective layer approximately the thickness of two to three sheets of paper. Because the protective layer is so thin, an acid bath or the effects of pool chemicals over time are damaging to plaster.\nWhat are the benefits of an automatic cover system?\nWe use only the Coverstar cover system for its superior quality and unparalleled customer service. There are four reasons for you to purchase a Coverstar automatic cover system: energy savings, environmental conservation, safety, and convenience.\nThe savings in monthly costs of operation are between 50-90%. You will save on consumption of electricity for heating the pool and running the filter systems, the addition of water and chemicals due to evaporation, and the life of all of the pool’s filter and maintenance components because the system is not running as frequently.\nUse of a Coverstar system is also good for the environment. Because the cover acts as a solar panel, you will be conserving gas used for operating the pool heater. Closing the cover will protect the pool against the effects of evaporation, so you will be conserving water and using 70% less pool chemicals. You’ll also be conserving energy and reducing pollution because the pump and filter will only have to run for a few hours a day.\nA closed pool cover is the best way to protect your family. Coverstar systems use only cover fabric which is tested and certified to meet the American Standard for Testing and Materials (ASTM) standard for safety pool covers. A keyed security box contains the switch that opens and closes the cover. You will have peace of mind knowing that no one is able to gain access to the switch without the key. This box must be located within sight of the pool to ensure that no one is in the pool while the cover is being closed.\nUsing a cover will allow you to extend the swimming season, since opening the pool earlier and closing much later in the season gives you the opportunity to take advantage of warm spring and fall afternoons. Keeping the cover closed also ensures a much cleaner pool, saving you the time and effort of extra skimming and vacuuming. To learn more about the benefits of a Coverstar automatic cover, visit www.coverstar.com."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:b4571dde-782c-4a58-912a-d35a4e021632>","<urn:uuid:3db44141-aebe-419e-be8c-fbd9d0a70b20>"],"error":null}
{"question":"What are the benefits of context-based learning 和 how does it support classroom accessibility for DHH learners?","answer":"Context-based learning through CLT offers several benefits: activities are anchored in real-life situations, making learning more meaningful and motivating for students; it encourages active participation and language manipulation; and it focuses on successful communication regardless of perfection. For DHH learners, this approach helps address classroom accessibility through three key areas: communication, language, and literacy development; classroom access to both auditory and visual information; and specialized pedagogical practices that support their preferred communication mode, whether spoken or signed, ensuring equitable access to the regular classroom curriculum.","context":["Teaching methods have evolved over the course of time. In the 1960s, teaching languages was all about behaviorism, which consisted in drills and patterns you would learn by heart and mechanically respond to. There also was the grammar and translation only approach, which taught about structures and vocabulary, but not how to communicate. For quite a few decades, teachers have been using the communicative approach, and here is why.\nThe Communicative Language Teaching (or CLT) focuses on the acquisition of four major skills:\n- Written Comprehension\n- Written Expression\n- Oral Comprehension\n- Oral Expression\nThe theory behind CLT is that in order to achieve communication, any student needs to work on these skills and hone them, as they are very closely intertwined in any given situation. For instance, a conversation with a neighbor is going to involve a mix of listening and speaking. Responding to an email will involve reading and writing.\nOn top of these 4 major skills can be added a secondary set of skills that will allow students to gain proficiency:\n- grammar skills: to improve your syntax and understanding of structures\n- semantic skills: to enlarge your vocabulary and go further in\n- pragmatic skills: to interact and interpret situations properly\n- methodological skills, to improve efficiency in learning\nThe benefits of this method have been proven by many studies, and here are some of the most important ones:\n- The activities done with a teacher using CLT are anchored in a context, giving meaning and logic to the activity. The skills then involved in this activity should be transferable from the classroom to real life. Knowing that they have a purpose in actual situations makes the students more motivated. They are no longer learning something just for a test, they are learning something for their personal enrichment.\n(Example: watching a scene of people ordering at a restaurant, studying the questions, the vocabulary, the behavior of the people involved, and then producing a dialogue between a customer and waiter so as to be able to do that for real one day)\n- This approach also revolves around the idea of deconstructing the language, manipulating it, producing and achieving. It has the benefit of making students more involved and active in their learning process. The students are no longer automats, they are builders of sense.\n- The focus of CLT is to achieve communication, getting a point across, even imperfectly. It does not matter if the message is flawed, as long as it is understood, the student has succeeded. The point of this approach is to reinforce students’ confidence in themselves and their abilities, and encourage them to continue.\n- CLT is a more complete approach than other methods as it does not focus on one aspect of language acquisition, therefore making each learner better trained and prepared for real life communication.\nMany countries in the world have trained their teachers to work using the communicative approach, especially in Europe through the implementation of the Common European Framework of Reference for Languages (CEFRL), and science has also confirmed the benefits of this approach. So now, it is up to you to develop the minds of your students!","The question of how best to teach learners who are deaf or hard of hearing (DHH) is perhaps the oldest topic in any area of education for children with diverse learning needs. Developments in a number of fields have accounted for more DHH learners achieving educational outcomes commensurate with their hearing-age peers than at any point in that long history. Efforts to further develop and implement effective educational practices with these learners continue, with an abundance of interventions proposed in the literature and in practice. Despite this, evidence for their efficacy remains limited. Such evidence as there is tends to be drawn from observations of professional practice and not always from the outcomes of high-quality research. This is not to say that a lack of research evidence for a particular educational practice means that it is necessarily ineffective or should not be used. Rather, it is to acknowledge the preeminence of quality research outcomes as the cornerstone of an evidence-base for educational practice with DHH learners while recognizing that contributions can come from two other sources: the expertise and experiences of professionals involved in the education of DHH learners in educational settings, and the views and preferences of DHH learners and their families about how the best educational outcomes can be achieved. The vast majority of DHH learners are educated in regular classrooms alongside their hearing peers, including a significant minority whose primary or preferred language is a signed language. Questions of how best to facilitate access to regular classrooms for those DHH learners are inextricably linked to issues in three areas: (a) communication, language, and literacy; (b) classroom access; and (c) pedagogical practices and other educational supports. The first area covers the unique set of challenges that relate to DHH learners acquiring a language (i.e., whether that be spoken or signed) and how best to support their ongoing development and use of their communication, language, and literacy skills in the classroom. The other two sets of issues, relate to the difficulties that are typically encountered by DHH learners in gaining access to the regular classroom curriculum through their preferred language and mode of communication (i.e., how best to access the auditory and visual environment of the classroom on an equitable basis with their hearing peers), and how best to support that access through instructional techniques and/or specialist support services. In all three areas there remains the challenge of assembling an evidence base for practice from quality research evidence.\nGreg Leigh and Kathryn Crowe\nCommunication is about working together to create shared meaning. It usually requires at least two people (one acting as the sender, and one or more acting as the receiver), uses a particular code (which may involve either conventional or unconventional signals), may take linguistic or nonlinguistic forms, and may occur through speech or other modes. In the classroom context, spoken language is typically the preferred mode of communication and the primary medium through which teaching and learning takes place. For learners with speech and langue disabilities, this is problematic. Communication does not develop in a vacuum. Cognitive and social routes are both important and therefore evidence-based practices (EBP) that impact on both need to be considered. In an attempt to delineate evidence-based strategies from assumptions or commonly accepted practices that have become “teaching folklore,” three aspects should be considered: (a) the best available research evidence that should be integrated with (b) professional expertise (using for example observation, tests, peer assessment, and practical performance) as well as (c) the learner’s and his/her family’s values. EBP thus recognizes that teaching and learning is individualized and ever-changing and therefore will involve uncertainties. Being aware of EBP enriches service delivery (in this case teaching practice) and enables teachers to support their learners to achieve high-quality educational outcomes. Research has shown that high expectations from teachers have a significant influence on the development of academic skills for children with speech and language disability. Teachers should therefore be empowered to understand how they can set up the environment in such a way that responsive, enjoyable interaction opportunities can be created that will enable learners to develop a sense power and control which are important building block for learning. They also need to understand the important role that they play in shaping behavior through the provision of consistent feedback on all communication behaviors and that communication entails both input (comprehension) and output (expression). Four teaching approaches that have some evidence base for learners with significant speech and language disabilities include: a) communication passports: this is a means through which idiosyncratic communication attempts can be captured and shared enabling everyone in the learner’s environment to provide consistent feedback on all communication attempts; b) visual schedules: a variety of symbols (ranging from objects symbols to graphic symbols) can be used to represent people, activities, or events to support communication. Visual schedules signal what is about to happen next and assists learners to predict the sequence of events, to make choices, and to manage challenging behavior; c) partner training: as communication involves more than one person, communication partner (in this case teachers) training is required in order to ensure responsivity; d) aided language stimulation: this classroom-based strategy attempts to provide a strong language comprehension foundation by combining spoken language with pointing to symbols, thereby providing learners with visual supplementation."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:255f2968-42d2-48d1-bfcf-fc49d2c043f3>","<urn:uuid:e9e5bb4b-9699-42e3-93fd-f28cee36a5c7>"],"error":null}
{"question":"How do vacuum furnaces compare to traditional atmosphere furnaces in terms of control systems and surface quality?","answer":"Vacuum furnaces have more sophisticated control systems than atmosphere furnaces, particularly for temperature control. They feature digital displays, touchscreens with graphics for operating parameters and alarms, and often include adaptive process control with real-time monitoring and data collection. In terms of surface quality, vacuum heat treatment produces superior results with no oxidation and decarburization, better surface quality, and smaller deformation of parts. It also provides degassing and degreasing benefits while being pollution-free, making it advantageous from both quality and environmental perspectives.","context":["Applications involving vacuum heat-treating are typically performed for one of the following reasons:\n- Processes that can be done in no other way than in vacuum;\n- Processes that can be done better in vacuum from a metallurgical standpoint;\n- Processes that can be done better in vacuum from an economic viewpoint;\n- Processes that can be done better in vacuum from a surface finish perspective.\nAll of the common (and several uncommon) heat treatment processes can be run in vacuum, from annealing and brazing to sintering and tempering. Many companies that currently outsource vacuum heat-treating ask themselves if they would be better served by setting up this capability in-house. Others who already have an in-house heat treat department wonder if switching to vacuum processing will offer them a competitive edge. This article will help address these questions.\nVacuum furnaces are typically characterized by their method of loading, horizontal or vertical, as well as if there is internal load movement, being classified as either batch or continuous (i.e. multi-chamber) types (Fig. 1 – 2). The various furnaces sizes, production capabilities, and feature configurations are almost endless and detailed extensively elsewhere , . Since most vacuum furnaces have a life expectancy of 40 – 50 years, decisions as to what to purchase, and from whom, become very important.\nThe features necessary for a particular vacuum furnace to run a specific heat treatment process are often determined by results one wishes to achieve, the type and geometry of the component parts to be run (Fig. 3 – 4), the productivity requirements, the physical size of the load, the pressure and temperature to be attained, and the medium (gas or oil) to be used for cooling the load. It is important to recognize that a furnace designed for one application use will in all likelihood be used for many other applications over the span of its lifetime. As such, having the greatest possible flexibility in features and options is highly desirable.\nFurnace Design & Construction\nThe main parts of a vacuum furnace are the:\n- Vacuum vessel;\n- Pumping system;\n- Heating chamber/Hot zone;\n- Quench/Cooling system;\n- Vacuum and Temperature Controls.\nVacuum furnace vessels can be grouped into so-called hot wall and cold wall designs, the latter being far and away the most common. A typical hot wall furnace has a retort that is commonly metallic or ceramic, dependent on the temperature. The heating system is usually located outside of the retort and consists of resistive heating elements or induction coils. Limitations of this retort-type furnace are the restricted dimensions of the heating zones and the restricted temperature range of the metallic retort, usually limited to 1100°C (2000°F) maximum. With cold wall furnaces, the vacuum vessel is cooled with a cooling medium (usually treated water) and is kept near ambient temperature during high-temperature operations.\nPumping systems are the heart of a vacuum system. Common to most vacuum systems are mechanical pumps, which have the ability to work against atmospheric backpressure and booster pumps used to improve the speed of pump down as well as the level of vacuum that can be reached. Diffusion pumps are a popular option to help reach extremely low vacuum levels while other types of pumps (turbomolecular pumps, cryo pumps) are used to reach ultra-low vacuum ranges.\nHot zone designs vary by type of insulation and materials of construction. In general, they can be classified as:\n- All metallic (radiation shields or shield pack)\n- Combination (inner metallic shield(s) separated or backed by ceramic or graphite insulation)\n- All graphite (board, fiber, carbon-carbon composite)\n- All ceramic fiber\nThousands of hours of operating service have confirmed that graphite insulation (felt or board) is suitable for almost all high vacuum applications, including brazing of advanced superalloys. When combined with a hot face of carbon-carbon composite, maximum hot zone life can be achieved, especially in high-pressure gas quenching and brazing furnaces.\nIt is important that the hot zone support structure be designed to prevent warpage of the internals since, for example,\ninsulation can crack or gaps can be created through which radiant energy can leak. The structure must be simple and allow a fastening system that avoids undo conductive heat losses while holding the assembly rigid. Hot zone superstructures can be as simple as steel expanded metal mesh or as complex as solid stainless steel enclosures, the latter having the advantage of no rusting and no subsequent outgassing. The critical factor is to help ensure proper temperature uniformity in the workload area and minimize heat loss to the shell.\nAnother important factor in hot zone design is thermal expansion and contraction, especially important in today’s high-pressure gas quench designs. The expansion rates and temperatures must be taken into careful consideration in the design stage to allow for proper clearances around element supports, nozzles, or restraint systems so that the insulation remains flat with minimal buckling or cracking.\nMost vacuum furnaces are electrically heated. Resistance heating elements are constructed from metal or graphite in a variety of styles. The following elements materials are commonplace: stainless steel alloys (300 series) for temperatures to 760°C (1400°F); nickel/chromium and iron-aluminum based alloys to temperatures of 900°C (1650°F); Inconel® and other nickel alloys for use to 1150°C (2100°F); silicon carbide (SiC) for operating temperatures to 1090°C (2000°F); molybdenum to achieve 1700°C (3100°F); graphite for use up to 2000°C (3630°F); tantalum, typically to 2400°C (4350°F) and tungsten to reach 2800°C (5075°F).\nThe choice of a heating-element material depends largely on operating temperature. For low-temperature operations such as aluminum brazing or vacuum tempering, inexpensive stainless steel or nickel-chromium alloys can be used for the heating elements. For higher-temperature general heat-treating applications such as hardening, or brazing, graphite or molybdenum are popular choices for element materials. Lightweight, curved graphite elements are becoming increasingly popular for vacuum furnaces. These elements have the advantage of lower thermal mass and have excellent structural integrity. For specialized heat-treating applications above 1480°C (2700°F), graphite or refractory metals (tantalum, tungsten) are popular choices. Still other processes such as low-pressure vacuum carburizing use graphite or silicon carbide elements.\nQuench (cooling) systems are typically either oil or (high pressure) gas. Many companies use oil quenching to achieve consistent and repeatable mechanical and metallurgical properties and predictable distortion patterns. The reason oil quenching is so popular is due to its excellent performance results and stability over a broad range of operating conditions. Oil quenching facilitates hardening of steel by controlling heat transfer during quenching, and it enhances wetting of steel during quenching to minimize the formation of undesirable thermal and transformational gradients which may lead to increased distortion and cracking.\nGas pressure quenching has seen a tremendous rise in popularity in recent years and its success is highly dependent on a number of factors including material, component geometry, loading, net to gross load ratio, gas parameters, and equipment design. Pressure ranges span sub-atmospheric, low, medium, high and ultra-high pressure (up to 25 bar in commercial practice) irrespective of the type of gas used.\nThere are two popular quench cooling loop designs available for vacuum furnaces, internal and external. While cooling performance criteria are very similar for both designs, each has some advantages and drawbacks. Furnaces with internal heat exchangers have the advantage of a compact design. The furnaces occupy slightly less floor space than those with a similarly sized external cooling loop. Its chief drawback is the close proximity of the quench blower motor (including drive shaft and bearings) to heat emanating from the furnace hot zone. If a failure of a motor component or heat exchanger does occur, it is often necessary to remove the entire hot zone to gain access for repairs. There is also the risk of damage to the furnace internals, and possibly the load if the heat exchanger should ever develop a water leak.\nIn the external quench loop design, the blower housing, heat exchanger housing and quench piping are located outside the vacuum chamber. The external quench loop design permits easier maintenance access to the blower and heat exchanger. The external loop design also minimizes the occurrence of quench blower failures by isolating sensitive components from exposure to heat from the vacuum chamber. Furthermore, any water leaks that might occur in the heat exchanger are confined within the separate heat exchanger housing.\nVacuum furnace process control systems (Fig. 5) are somewhat similar to atmosphere furnace control systems; however, they tend to be somewhat more sophisticated, especially from a temperature control standpoint. As they continue to evolve, even the most basic control systems often include easy-to-read digital displays and touchscreens with graphics to display operating parameters and alarms. Where more advanced data management applications justify the higher cost; the PC-based control system is a user-friendly and versatile tool. Perhaps their biggest advantage is the ability to access information that allows the user to analyze, adjust and download operating parameters from remote locations. These systems can be connected to local networks for multiple user access and even to the Internet via secure connections.\nMost of today’s temperature control systems involve “adaptive” process control. Depending on the machine or process, different variables exist that must be monitored, controlled and/or changed during the cycle. Sensors monitor a selected process or equipment parameter; send the gathered data back to a controller, which then compares it to a predetermined value or set point. Through calculations, a controller sends a signal back to the device to make the proper adjustments to obtain a controlled process. Programmable Logic Controllers (PLC’s), sensors, and computers make this all possible. In turn, data trending, real-time process monitoring, and data collection for permanent retention are commonplace. By analyzing this data, new cycles, containing modified variables, can give better results in less time.\nVacuum controls depend to a great extent on the type of vacuum system and the required operating vacuum level. For each operating range, different vacuum gauges are required, often in combination with one another, to accurately determine and/or control the vacuum level of the chamber at any given moment in time. The criteria for selecting a vacuum gauge are dependent on various conditions such as:\n- The vacuum range to be detected;\n- The gas composition (inert, reactive, corrosive);\n- Required accuracy and repeatability;\n- Environmental conditions.\nVacuum gauges are divided into three basic categories based on their working pressure. These include absolute pressure gauges; vacuum gauges useful down to around 0.001 mbar (1 micron); and vacuum gauges for use below 0.001 mbar (1 micron).\nThe purchase of a vacuum furnace involves a considerable capital investment. As a result, the question of buying a used furnace often at a lower cost than a new furnace is a fairly common one. Most used furnaces are sold on an as-is, where-is basis with no manufacturer’s warranty. One of the most important issues to assess is the condition of the vacuum chamber water jacket. In situations where the furnace cooling system has been connected to an untreated water supply, water jacket blockages from mineral build-up are all too common, and moving the furnace triggers untold problems. Ultrasonic testing of the shell is a good first step, as is looking for discolored or blistered paint, a sure sign of overheating.\nThe condition of the hot zone, heating elements, pumping system, and vacuum controls must also be considered. A thorough visual inspection is usually the first step in the process as is consultation from various suppliers of the component on the system. For example, when looking at the condition of the hot zone, distorted, discolored or crumbling insulation, warped or broken elements or hearth rails, missing element insulators, and other damaged hardware are all signs that restoration work is required. All-metal hot zones are often more expensive to repair.\nThe control system is another important consideration when evaluating used furnaces. Having State-of-the-Art controls is highly desired and with technology obsoleting itself at an extremely rapid pace, even 10-year-old control systems can be problematic.\nThe right vacuum furnace is the one that performs the intended application with the highest degree of uptime productivity and uncompromised quality. While an array of business decisions must be made to justify the expense, it is comforting to know that today’s vacuum furnaces offer the latest technological advances in furnace design (hot zone materials, control packages, and quenching systems) and offer many operating and performance improvements. It is often advisable to look to companies who have proven experience both as a heat treater and furnace manufacturer. This type of knowledge will shorten the learning curve.\n- Herring, Daniel H., Vacuum Heat Treatment, BNP Media Group II, 2012.\n- Herring, Daniel, Heat Treating Equipment: Types and Selection Criteria, white paper, 2010.\n- Herring, Daniel H., Saving Money by Maximizing Furnace Uptime Productivity, Industrial Heating, January 2013.\n- Important Considerations for Establishing a New Vacuum Heat Treating Capability, white paper, Vac Aero International.\n- Buying a Vacuum Furnace, New Versus Used, white paper, Vac Aero International.\n- Vacuum Furnace Quenching Systems: External versus Internal, white paper, Vac Aero International.","Wed, 20 Jun 2018 02:34:45 +0000\nApplication of vertical vacuum oil quenching furnace in heat treatment\nVacuum heat treatment technology has no oxidation and decarburization, degassing, degreasing, good surface quality and small excellent comprehensive mechanical properties, heat treatment parts deformation, long service life, pollution-free environment, high degree of automation, a series of prominent advantages and got rapid development. The research and application of vacuum heat treatment technology have been applied in many fields such as annealing, oil (gas) quenching, high-pressure gas quenching, carburizing, nitriding, metal seeping, tempering, sintering, brazing, coating and cleaning. With the continuous improvement of the quality requirements of heat treatment for aviation products, vacuum heat treatment technology has been widely used in the aviation industry. In recent years, more and more vertical vacuum oil quenching furnaces are used, but only two domestic manufacturers can produce this equipment. Compared with the foreign equipment, the price is only 1/3 ~ 1/4 of the imported equipment, and the cost performance is relatively high. Vertical vacuum oil quenching furnace is mainly used in long axis, long pole parts, such as quenching, compared with horizontal vacuum oil quenching furnace, its deformation greatly decreases, and has been successfully applied in aircraft landing gear, aerospace manufacturing enterprises of vertical shaft, missile shell quenching. Because the quenching transfer time of three-chamber vertical vacuum quenching furnace is relatively long, there are few studies on the application of thin-walled slender rods in China. Parts made in our company, there are a lot of main mechanical parts belong to the thin wall long pole parts, such as: shell, rocker arm, shoulder axis, etc., the effective thickness of 3 ~ 10 mm, length is 700 ~ 1600 mm, material for 30 crmnsia, 30 crmnsini2a, in the existing horizontal vacuum oil quenching furnace and ordinary box type resistance furnace quenching, or deformation is large, or oxidation decarburization is serious, unable to meet the technical requirements. Therefore, our company purchased zcl-408 vertical vacuum oil quenching furnace, which suspended the parts vertically for quenching such parts. This study discusses whether the hardening of thin-walled parts can meet the technical requirements of heat treatment in vertical vacuum oil quenching furnace.\nTest equipment, materials and methods\n1.1 test equipment.\n1.2 structure introduction zcl-408 vertical vacuum oil quenching furnace is a vertical three-chamber structure, one is a quenching oil groove, one is an intermediate transition chamber, and the top one is a heating chamber. It is composed of furnace body, furnace drum, heat insulation, elevator, feeding tank, oil tank, oil circulation mixing mechanism, vacuum system, air charging system, water cooling system and hydraulic system.\n1.3 test material test qualified materials into the factory all the reinspection, choose commonly used 30 crmnsia low alloy structural steel and low alloy high strength steel 30 crmnsini2a, processed into 3, 10 mm thickness hardness test specimens with (20 mm long and 20 mm) wide and 5 mm short rate tensile test bar. 1.4 test technical requirements and plans\n1.5 both the process test piece and the tensile test bar are bound in the effective heating zone by three layers: upper, middle and lower; 3 hardness test pieces are placed in each layer; and 2 tensile test bars are placed. Quenching with vertical vacuum oil quenching furnace, vacuum degree of work 1 ~ 10 pa, adopt B244 vacuum quenching oil quenching oil, the oil temperature heating to 50 ℃; The tempering is carried out by a jh-36 well return furnace. Stepwise warming is adopted for quenching, preheating first and then warming up to quenching temperature.\n1.6 mechanical properties require mechanical properties of two types of steel.\n2. Results and analysis\nThe samples 2.1 first 30 crmnsia test data to 900 ℃ quenching, processing results. Can see 3 mm thick and 10 mm thick thin-walled 30 crmnsia in heating zone in next three layers, which tie after 900 ℃ quenching hardness value acuity after 49 HRC, 10 mm in the middle of thin-walled package has the highest hardness can reach 51.5 HRC. All of them meet the requirements of mechanical properties of 30CrMnSiA (greater than or equal to 47HRC), indicating that the quenching transfer time of 40s in the vacuum state can meet the rigid requirements for quenching of thin wall parts made of 30CrMnSiA from 3 to 10mm. The samples were then compacted with tensile strength. Showed that 30 crmnsia sample in heating zone in next three layers, which samples at 500 ℃ tempering after binding the tensile strength of the overall reached 30 crmnsia mechanical performance requirements (1080 mpa) or higher. For the yield strength, the sample reached the maximum of 1025MPa in the lower layer, the upper layer was relatively uniform to 1000MPa, the middle layer had the minimum yield strength of 957MPa, and the three layers generally met the requirements of 30CrMnSiA mechanical performance (> 835MPa). The elongation reached a maximum of 20% in the middle layer, and was relatively uniform in the upper layer. The overall elongation also reached the requirement of 30CrMnSiA mechanical performance (10%). All three layers meet the requirement of 30CrMnSiA mechanical performance (45%). Hardness after tempering. As you can see, the sample after 500 ℃ tempering, 10 mm thin wall parts in the middle of a maximum 40.5 HRC hardness, and 3 mm thin wall parts of hardness in the lower reaches the minimum 37.5 HRC. The hardness of all three layers of samples reached the requirement of 30CrMnSiA (> 35HRC).\n2.2 due to the test data of 30 crmnsini2a 30 crmnsini2a test data after quenching and tempering hardness, tensile strength after 250 ℃ tempering. Can see: 30 crmnsini2a the tensile strength of the sample in the lower reach a maximum 1722 mpa, in the middle, the tensile strength of the minimum value of 1700 mpa, conform to the mechanics performance of 30 crmnsini2a p 1570 HRC), elongation and basically meet the mechanical property requirements of 30 crmnsini2a (8%), reduction of area in the middle reaches a maximum 68%, reach the minimum value in the lower 48%, and can meet the requirements of the reduction of area of 30 crmnsini2a (45%). 30 crmnsini2a samples, 3 mm, 10 mm thin wall parts after 250 ℃ tempering hardness values are meet the requirements of mechanical properties (35 HRC) or higher. All in all, from the hardness and strength after quenching and tempering hardness data after data can be seen that the vertical vacuum oil quenching furnace quenching can satisfy the demand of the thin-walled parts quenching, quenching hardness and strength in accordance with 30 crmnsia and 30 crmnsini2a technical requirements. After the test, we have been on the rocker arm, shoulder axis, such as shell thin wall parts was carried out in the vertical vacuum oil quenching furnace heat treatment, hardness, mechanical properties, deformation and surface brightness all met the technical requirements, has been successfully used in the product, use effect is good. HB/Z191 stipulates that the transfer time of quenching should generally not exceed 25s, while the vertical vacuum oil quenching furnace is divided into three Chambers with a large size, which is difficult to meet such requirements. We use of vertical vacuum oil quenching furnace quenching transfer time is 40 s (starting from the open interval of popular open time, all parts to sink groove end timing), all the parts began to leave the heating chamber parts in oil tank for only 20 s, in a vacuum heating speed is slower than the salt bath and controlled atmosphere furnace, so the temperature of the vacuum parts reduce speed is slower than under atmospheric conditions, therefore, the 40 s of the vertical vacuum oil quenching furnace quenching transfer time is can satisfy the requirements of the alloy steel vacuum heat treatment.\n3. Conclusion vertical vacuum oil quenching furnace can be used for long shaft rod parts heat treatment, can effectively solve the problem of quenching deformation of such parts, can be widely used, therefore, we through the three specimens, 10 mm thick, experimental study on heat treatment of the vertical vacuum oil quenching furnace quenching transfer time can satisfy my company alloy steel slender thin-walled bar heat treatment technical requirements."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:c548951a-f791-41a7-85a1-85a9a4fc7e5f>","<urn:uuid:b8caf9d4-db2c-4a5a-9557-88ede61082e3>"],"error":null}
{"question":"What are the ways urban renewal projects can lead to corruption in construction, and what technical challenges do buildings face with wall tie problems that require stabilization?","answer":"Urban renewal projects can lead to corruption through various schemes, as exemplified by T Dan Smith who received £156,000 from architect John Poulson for getting local councillors to accept Poulson's redevelopment schemes. This involved signing up councillors to company payrolls and pushing through developments for private gain. As for wall tie problems, buildings face significant technical challenges including cracks and bulging in masonry, bowing walls, and potential building collapse. These issues often arise from insufficient ties during construction, inadequate protection (pre-1945 ties weren't galvanized), and substandard galvanizing between 1964-1981. The problems require immediate remedial action including installation of new wall ties and potential additional masonry stabilization work like lateral restraint or steel reinforcement.","context":["Corruption and Construction\nThis article appeared in Ceylon Today on Wednesday August 20 2014.\nUrban renewal seems to be inseparable from corruption. T Dan Smith was once a local hero in Newcastle-upon-Tyne and then he was sentenced in 1974 to six years in prison for accepting bribes. Smith believed strongly in the need to clear Newcastle of slum housing and put a great deal of effort into regeneration plans.\nModernist planning was at its height in Britain during the 1960s, after the end of post-war austerity. Newcastle, as well as Manchester and Birmingham, was drastically transformed. It was a time of “clean sweep” planning, where the only constraints on redevelopment were economic. Conservation policy was restricted to the preservation of a limited number of major buildings and monuments. In his article, Alas Smith and Burns? Conservation in Newcastle upon Tyne city centre 1959–68, John Pendlebury of the School of Architecture at Newcastle University, wrote “though modernist rationalism was the driving force in the city’s re-planning, it co-existed with a conscious policy of conservation, born out of a picturesque design tradition.”\nNot everyone appreciated Smith’s efforts. Alec Glasgow wrote a contemporary folk song:\nWeep, Geordie, weep,\nAt the murder of your city.\nWeep, Geordie, weep\nFor the vandals have no pity.\nSmith’s name is usually spoken in negative terms regarding the destruction of historic and aesthetically pleasing buildings, which were replaced with a concrete jungle.\nSome called him Smith “Mr Newcastle” others called him “the mouth of the Tyne”. Another nickname was “one-coat Smith”. When he ran a painting and decorating firm, his painters were noted for their stingy use of materials. Despite this, the firm was granted more than half the contracts for painting council houses.\nWhile his evangelical zeal to make Newcastle a better place may have been genuine, Smith’s desire to make money was stronger and got mixed up with his political ambitions. Smith was appointed Chairman of Newcastle council’s Housing Committee in 1958 and was elected as Leader of the City Council in 1959. He created one of the country’s first free-standing Planning Departments and made it the most powerful department in the council. He strengthened his power by creating an inner Cabinet of his own supporters. When Harold Wilson became prime minister in 1964, Smith was confident that he would be invited to take a national ministerial post. However, Wilson had vague suspicions about Smith’s probity and did not call him.\nIn 1962, Smith set up a PR firm to support redevelopment of other urban centres in the northeast, and later nationwide. Through this, he established links with John Poulson, an architect with a reputation for rewarding those who put business his way. Smith eventually received £156,000 from Poulson for his work, which typically involved signing up local councillors on to the payroll of his companies and getting them to push their councils to accept Poulson’s redevelopment schemes. Poulson earned more than £1,000,000 through Smith.\nAnother of Poulson’s contacts was the then Shadow Commonwealth Secretary Reginald Maudling. In 1966, Maudling accepted an offer to be Chairman of one of Poulson’s companies for £5,000 per annum. Maudling’s son Martin, who had left Oxford University without taking a degree, went to work for another Poulson company. Poulson agreed to donate large sums of money to a charity patronised by Maudling’s wife. Maudling helped to bring pressure on the government of Malta to award a £1.5 million contract for the new Victoria Hospital on Gozo to Poulson. This had led to heavy losses to the Maltese government. A Parliamentary inquiry into Maudling’s conduct concluded that he had indulged in “conduct inconsistent with the standards which the House is entitled to expect from its members”.\nNo punishment was imposed but Maudling drank himself to death at the age of 61. The son, William Maudling, 42, who once lived in Downing Street with his family, threw himself from the 16th floor in 1999, his life ruined by heroin.\nSmith’s PR firm was also involved with Wandsworth Borough Council in pushing a redevelopment scheme. Smith’s Wandsworth council contact, Alderman Sidney Sporle, fell under police suspicion of corruption in the late 1960s. The police investigation led to Smith himself being charged with bribery in January 1970. He was acquitted at his trial in July 1971, but was forced to resign all his political offices. Smith was arrested again in October 1973 after Poulson’s 1972 bankruptcy hearings disclosed extensive bribery. He pleaded guilty in 1974 and was sentenced to six years’ imprisonment; despite his guilty plea, he continued to assert his innocence.\nAfter his death, Smith’s career was the inspiration for Austin Donohue, a character in Peter Flannery’s play, Our Friends in the North. The part was first played by Jim Broadbent in the Royal Shakespeare Company production, and then by Alun Armstrong (who once stayed at our friends’ guesthouse in Badulla) in the 1996 BBC television drama version.\nBack in the early 70s, I worked as a social security visitor in a poor district of Manchester. Some of the street names were already familiar to me from reading about the Moors Murderers. Brady and Hindley once trod those drab streets in Gorton and Ardwick. Things were changing in those days. The streets had been built as warrens of terraced back-to-back houses for the workers of the industrial revolution. Lives could be cramped and stunted but there was also a sense of community still celebrated by the popular teledrama Coronation Street, which started in the early 60s and is still running.\nManchester Corporation, like similar ruling bodies in other municipalities, probably had good intentions when they embarked on slum clearance and urban renewal. Some of the old houses were pretty grim with outside toilets and some had gas mantles rather than electric light.\nNew blocks sprang up quite quickly. These resembled something out of a movie about the French Foreign Legion. Local people called them Fort Ardwick and Fort Beswick. As well as disrupting the sense of community enjoyed in the old terraces these new blocks might have been designed to assist crime with their walkways in the sky.\nEven when they were brand new, these dwellings proved not fit for purpose. They were put up very quickly using prefabricated materials like a huge Lego kit. They were not as durable or well-designed as Lego.\nThe kind of concrete used caused condensation indoors so that the walls were dripping wet, causing respiratory problems in the elderly and in babies. Under floor heating was installed which could not be controlled by the tenants. Tenants were often baked to a frazzle and faced with huge fuel bills that they could not pay. A friend of mine lived in a council property in Hulme and found the place infested with cockroaches and beetles because the walls were built of straw.\nI visited Manchester eight years ago and the area once covered by Fort Beswick had neat little rows of houses all on ground level. Although there was more space and the houses looked in good condition, they did rather remind me of the old terraced houses that were demolished in the 1960s and 1970s.\nNot far from Beswick is the new home of Manchester City football club. The new stadium was built at a cost of GBP 110 million for the 2002 Commonwealth Games. The stadium is owned by the City Council and leased by the football club which, despite its previous lack of glamour, in 2008 became the richest club in the world after a takeover by an Arab consortium headed by Dr Al-Fahim, known as the Donald Trump of Abu Dhabi. A previous owner was former Thai prime minister Thaksin Shinawatra, known to Mancunians as “Frank Sinatra”.\nManchester City FC signed an agreement with the Council in March 2010 to allow a £1 billion redevelopment led by architect Rafael Vinoly of land around the stadium and possible stadium expansion. In a spooky link with T Dan Smith, Vinoly was hiredby Wandsworth Council in London to develop the area around Battersea Power Station. The proposed development is supposed to generate 15,000 jobs. The network of tall curved blocks of offices will block the view of Sir Gilbert Scott’s industrial masterpiece. The accommodation is not intended to attract local families but hordes of predatory bankers with no children but easy access to the City and huge bonuses.\nManchester City’s stadium was a part of the massive Eastlands redevelopment. According to the consultative regeneration framework document, 3,000 jobs were created in ten years. This is low considering that at least 2,000 jobs were axed to cut public spending. The much-lauded regeneration of East Manchester never lived up to the hype of galvanising growth and job-creation in one of the city’s most deprived areas. New jobs tended to be poorly paid ‘flexible’ jobs, servicing the consumption habits of middle classes. Only half of the hundreds of new jobs at supermarkets went to local residents. Save the Children found that 27 %per cent of children in Manchester were living in “severe poverty” – the worst record of any local authority in the country.\nOn my last visit to Manchester, the city centre was very different from the bleak place it was during the Thatcher years. The IRA did the city a favour by blowing up the ugly Arndale Centre and opening the way to better buildings. The new city centre reminded me of Seattle. There were luxury apartments and chic hotels. Even old churches and cotton mills had been converted into housing. Salford used to be grim but now it has luxury accommodation and an arts centre dedicated to LS Lowry. Somehow, it was still grim.\nAccording to the Eastlands document, 5,000 extra homes have been built in East Manchester. However, the Manchester-Salford Housing Market Renewal Pathfinder (whoever came up with that name!) between 2008 and 2009 demolished 2,200 more homes than it built at a cost of GBP 600 million. The project ground to halt, leaving a wasteland behind it.\nHow will Beswick, Bradford and Lower Openshaw compete with the new enterprise scheme at Manchester Airport (another arm of the council)? There are many empty office blocks in Manchester and but more will be built at the airport.\nSome people will have made a lot of money out of continually knocking British cities down and re-building them. Not many of those people will go to prison like T Dan smith did. In 1985, Smith wrote that “Thatcherism, in an odd sort of way, could reasonably be described as legalised Poulsonism. Contributions to Tory Party funds will be repaid by the handing over of public assets for private gain”.\nThatcherism and Poulsonism live on in all the British political parties.","CPD: Masonry stabilisation and reinforcement\nOf the nine million dwellings in the UK constructed with cavity walls, BRE research shows that two million suffer wall tie corrosion and are in need of remedial wall tie replacement. Richard Walker from repair and preservation specialist Peter Cox explores how to identify and determine the extent of the problem and outlines the repair and replacement process.\nWall ties are designed to help prevent the movement of a wall and improve its resistance to vertical loads and to horizontal pressures from weather-related forces, such as wind, cold and damp. In cavity walls, the wall ties must also prevent water transfer across the cavity and be flexible enough to allow differential movement between the two walls without undue stress to the mortar.\nTake the test online here\nGiven the important role they play in ensuring the structural integrity of a building, wall tie failure can cause significant problems, including:\n- Cracks and bulging in masonry and mortar\n- Cracks to render coats\n- Bowing walls\n- The lifting of roof tiles (or the ‘pagoda’ effect)\n- The peeling away of the outer leaf of bricks\n- Potential building collapse\nIdentifying wall tie problems\nHere in the UK, there are a number of reasons why wall tie failure is such a prevalent problem. These include:\n- Insufficient ties installed during construction\n- Insufficient protection – prior to the 1945 some ties were not galvanised\n- Substandard galvanising – the British Standard for weight of galvanising was reduced between 1964 and 1981\n- Chemical action – for example, where black ash mortar has been used\n- Location – for example, marine environments with driving rain and salt-laden atmospheres\nSigns of bulging, bowing and cracking in mortar joints can all be signs of wall tie failure. In particular, deformed masonry with regular horizontal cracking is likely to be suffering from wall tie failure generally and the entire structure may need to be treated. In addition, internal cracks and the lifting of roof edges can also indicate wall tie failure. Where such visual signs are evident, a specialist wall tie survey must be undertaken.\nPeter Cox was established in 1951 and today boasts over 60 years’ experience in providing a range of property services, including damp proofing, waterproofing, timber preservation and wall stabilisation.\nPeter Cox has built an enviable reputation as the market leader in the repair and preservation of all types of property, from private housing to historic and listed buildings.\nIt is an approved installer of the world-renowned Cintec systems and its branded AnchorbondVtek masonry anchors are used in heritage buildings across the UK and Ireland.\nThe first step in a repair programme is a survey to determine the extent of the problem. The positions of existing wall ties are plotted using a metal detector and then an endoscope is used to examine their condition in the cavity. Ties should be inspected in each wall and at different heights. Bricks should usually be removed for the physical examination of sample ties or if cavity wall insulation is present.\nDuring the inspection process, it is important that care is taken not to create debris that could cause cavity bridging, especially where there are insulated cavities, and also to minimise the disturbance to existing finishes.\nThe tables in the BRE Digest 401 should be referred to when estimating and classifying the two visible sections of the wall ties being inspected in situ; namely, the cavity spanning section and the section buried in the outer leaf. Table 2 specifies the corrosion level and a description of the condition, while Table 4 specifies the predicted life based on the current condition and recommended action.\nDepending on the observed current state of the wall ties, recommended actions range from inspections within 10, 5 or 2 years or the need to fit remedial ties.\nThe surveyor will also need to consider whether the problems can be resolved by using remedial wall ties or lateral restraint ties or whether additional work is required. It is particularly important to consider whether additional work is required when dealing with non-standard constructions.\nRemedial wall ties\nWhere there is total failure, red rust and erosion, or red rusting to parental metal it is vital to fit remedial wall ties immediately. It is also recommended to effect immediate repairs where there is terminal corrosion, zinc white rust or heavy to moderate corrosion to zinc or bitumen.\nA range of alternative wall tie designs is available – resin, mechanical or cementitious. The choice of tie will depend on the type and condition of the masonry. In most cases, the existing ties are isolated to prevent further corrosion and cracking of the mortar joints.\nFormed in austenitic stainless steel, replacement ties are available in different lengths to suit different cavity widths. In cavity wall repairs, designs should incorporate a centre drip to prevent transmission of water across the cavity.\nInstallation of remedial wall ties\nWhatever wall tie is used for remedial work, the number one rule to observe is to fit the remedial ties first, then deal with the old ties.\nWork should never be attempted the other way around.\nFor replacement wall tie solutions, at Peter Cox, we favour our DriveTie wall tie. This is because they provide an effective and cost-effective method of tying most masonry types including brick, block and timber frame.\nOnce the new remedial wall ties have been installed, the old ties can be dealt with. Typically, all ‘fishtail’ ties must be dealt with by isolating the ties within the mortar joint and enveloping them with a waterproof foam to prevent further corrosion and expansion because, if not, they rust and can continue to cause further cracking of the mortar joints. However, it is possible to leave narrow gauge ‘butterfly’ ties in situ because these are unlikely to cause additional damage as they corrode.\nAnchorbond Vtek masonry anchors\nThe anchor is comprised of a hollow stainless steel bar surrounded by a woven polyester fabric mesh sleeve into which a specially formulated cementitious grout is injected under pressure. The flexible sleeve expands and moulds itself into the spaces within the wall, providing a strong mechanical and chemical bond when cured. Various attachments can be welded to the anchorhead.\nBenefits of the AnchorbondVtek masonry anchors include:\n- Purpose-designed for each application\n- Versatile in use\n- Works well in weak substrates\n- Effective in poor quality materials and for bridging cavities\n- Invisible when installed\n- Fire resistant\n- Cementitious-based and, therefore, sympathetic to the original structure\n- Approved by heritage authorities\nThey can be used as replacement and supplementary wall ties, lateral restraints, for stabilising masonry (including solid, cavity, hollow pot and rubble filled), crack stitching, stitching anchors (for example, in arch consolidation), as stud anchors, in parapet wall strengthening, and retaining wall anchoring.\nWhere there has been extensive damage to masonry as a result of wall tie failure, additional masonry stabilisation work may be required. This is typically one or more of the following:\n- Lateral restraint\n- Steel reinforcement\n- Crack stitching\nSubject to the correct design and specification, installation of grout-inflated anchors, such as the AnchorbondVtek masonry anchors, will also serve to address these issues. In addition, grout inflated anchors can serve as secure fixing anchors, where required.\nFinally, some additional rebuilding of brickwork and / or repairs to external finishes may be required.\nGrout-inflated anchors offer a versatile and cost-effective solution for masonry stabilisation and strengthening in a variety of applications. Some typical applications are illustrated below.\nLateral Restraint - Wall to Floor Joists (not relevant on bridges)\n- 20 or 30mm diameter hole – to suit anchor length\n- Anchor expands to provide rigid support\n- Neoprene drip in cavity\n- Cavity and solid walls can be tied to the joists running parallel to the wall\n- Bespoke length to suit individual requirements\n- ‘Armlock’ fixed with stud anchor to joists at 90° to wall\nStitching Anchor for Rubble Filled Wall\n- Sock expansion into the soft friable core\n- Anchor body design to suit load - std 15 x 15, 20 x 20 or 30 x 30mm square hollow section\n- Length to suit\nExternal Wall Anchored to Internal Wall\n- External wall and Internal wall tying options\n- Anchor positioned at floor levels and hidden in the void below the floorboards\n- Length to suit individual situations – bespoke designs\n- Bottle screw to tension the anchor\n- Solid bar connector\n- Choice of 30mm and 60mm diameter anchors\n- Delaminated and poor quality masonry can be stitched together\n- 30mm anchor in 60mm diameter drilled holes\n- Anchors positioned to suit individual constructions\n- Stitching anchor at 90° to the normal\n- Length to suit individual situations\n- Bespoke design – anchors made to measure\n- Typically 30 x 30mm hollow square section steel anchor filled with grout to carry main load\nThe installation of grout-inflated anchors\nInstallation of grout-inflated anchors is typically a five-step process.\nDrilling: installation holes are created in the masonry using a wet diamond drilling process with extension drill bits added as required to achieve the required hole depth. The waste is removed in the form of cores.\nInserting anchor: for stabilisation work anchor lengths are typically 1m to 11m in length. Care needs to be taken not to puncture the polyester sock.\nInjecting grout: the cementitious grout is site mixed and then sieved before pouring into a pressure pot which operates at between 3 and 4.5 bar. The grout is forced into the sock around the anchor. This expands to fill the cavity drilled out, starting to harden after approximately one minute.\nAnchor sections: the grout is injected through the rod in the case of hollow section anchors but if solid single or multi bar sections are used, a separate injecting tube is inserted in the fabric sock. As illustrated here, threaded rods can be used to facilitate fixing attachments to the anchor head – for instance, for tie bar extensions and support brackets.\nSurface repair: drilling holes are made good so that the repair will be almost invisible – this is particularly important in the case of historic properties.\nWhere are grout-inflated anchors used?\nAt one of the finest medieval castles in England, our team installed a series of specially designed bespoke anchors into the Nevill tower which had been falling apart. During the process, we worked closely with English Heritage to ensure we met their exacting standards.\nManchester’s light railway system was first opened in 1992 and continues to undergo massive expansion. Our technicians installed high load fixings into existing buildings adjacent to routes. These fixings take the straining wires where the overhead cables are fixed.\nDysart Toll Booth\nThis medieval toll booth in Fife was comprehensively repaired in 2009. We repaired major cracks with six metre stainless steel rods drilled through the building in hidden locations.\nIn order to carry the new gantries which will hold the electric cables for electrification of the line – which create huge loads of up to 200kn – the viaduct walls had to be strengthened. Our technicians installed 6m long anchors, going in at a 45˚ angle, for the gantries over the two-mile stretch. In some places, it was necessary to drill through the viaduct horizontally to enable installation of the cantilever gantries.\nThe access bridge to this 200-year-old, Grade II listed building – now a golf course and country park – was cracking, so our technicians installed anchors to ensure the bridge was secure from further deterioration.\nThe signs of wall tie degradation or insufficiency, such as bowing walls or horizontal cracking, should be investigated immediately and a structural assessment carried out.\nAs a result of changing building standards, wall tie-related problems are extremely common in the UK, especially in older properties or homes built in the 1970s.\nBecause of the risk of possible masonry collapse, early intervention is vital and can halt serious damage occurring. Where more serious masonry problems exist or a structural solution is required, grout-inflated anchors are a well-proven and versatile solution. Their installation has very littlecosmetic impact on a building, making them an ideal solution for listed properties.\nRichard Walker is National Technical and Development Manager at Peter Cox\nTake the test online here"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:8cef6aa2-6f9d-4397-8d59-966ccdd2e189>","<urn:uuid:1a237fd7-ce6d-4317-a82d-b32fe0005582>"],"error":null}
{"question":"What were the key characteristics of Las Pachucas' fashion style in 1940s Los Angeles, and how did it challenge societal norms?","answer":"Las Pachucas developed a distinctive style that combined masculine and feminine elements. They wore zoot pants or skirt suits, complemented by hyper-feminine pieces including heavy makeup, tight shirts, and carefully styled bangs. This fashion choice was revolutionary as it appropriated male clothing while maintaining feminine elements, creating a groundbreaking gender-queering fashion statement. Their style challenged both white supremacy and conservative gender norms of the 1940s. During World War II, their fashion choices were seen as unpatriotic due to fabric rationing restrictions. The Pachuca look represented a form of rebellion against systemic race-based poverty and conservative gender norms, allowing these women to take agency over their sexuality and public presence, though they often faced criticism from both within and outside the Chicanx community.","context":["The zoot suit is perhaps the most powerful item of clothing in American history. It has sparked race riots and defined entire subcultures. Depending on who wore it and when, the zoot suit has represented things from class solidarity, protest against racism, anti-war efforts, and more. For Las Pachucas, young Chicanx women, in 1940s Los Angeles, the zoot suit was a statement of protest against white supremacy, a call for gender equality, and a sartorial choice.\nThe style grew out of Black dancehalls and balls in 1930s Harlem, where dancers found oversized suits with cuffed pants to be the most flattering outfit for revelry. Loose fabric could sway with the wearer’s body, but without a cuff at the ankle dancers might trip. With this in mind, local manufacturers designed oversized suits with a tapered pant. Soon, men all over Harlem wore the unique style: loose, ballooning trousers cropped tight at the ankle and a long jacket with padded shoulders, often accessorized with a long watch chain or chic matching hat.\nThe outfit was a symbol of personal wealth and achievement. Zoot suits required extra fabric for draping, so they were not cheap. In his autobiography, Malcolm X vividly describes how at age 15 he saved up to buy his first one: “Sky-blue pants 30 inches in the knee and angle narrowed down to 12 inches at the bottom, and a long coat that pinched my waist and flared out below my knees.”\nAfter gaining popularity in Harlem, the look spread to urban centers across the country where it became popular with many marginalized groups. It is now is perhaps best associated with X or with Chicanx labor organizer Cesar Chavez.\nIn Los Angeles, the look became a symbol of unapologetic Chicanx pride. For Pachucos, a subset of Mexican-American youth, the suit asserted pride in one’s ethnic and racial identity. White Angelinos, however, interpreted Chicanx pride as a threat to law and order. White supremacy shaped government systems, interpersonal interaction, and business behavior. The Pachuco look crystalized during the Mexican Repatriation; from 1929 to 1944 the U.S. government forcibly deported approximately two million people of Mexican descent.\nIn wearing something that asserted pride in non-white heritage, Los Pachucos threatened Los Angeles’ unjust racial order. Plus, the zoot suit’s emphasis on a man’s hips and waist challenged white America’s perception of masculinity, and white Angelinos often derided Pachucos for being feminine.\nOf course, the suits were not limited to men. Las Pachucas made their own unique political statement with zoot pants or skirt suits. Unapologetic Chicanx pride was just as threatening to white Angelinos coming from women, if not more so. Las Pachucas appropriated male clothing, which constituted a groundbreaking gender-queering fashion.\nAmerican perceptions of womanhood changed greatly in the early 1940s. The United States entered World War II in 1941, and quickly mobilized into a total war society. Rosie the Riveter needed women to go and fight for their country by working in factories; the War Production Board (WPB) rationed all types of goods. In March, 1942, the WPB declared a standard dress pattern for women to limit excess fabric. Though zoot skirt suits used less material than men’s pantsuits, the Pachuca staple was thus seen as explicitly unpatriotic.\nPlus, Pachucas often complimented their masculine pieces with hyper feminine ones. Heavy makeup, tight shirts, and impeccably coiffed bangs were Pachuca staples. With this suggestive look, Pachucas took agency over their own sexuality. They were one of many groups of women who have used menswear or provocative clothing to empower themselves, but they were met with derision from both within and outside the Chicanx community.\nIn Luis Valdez’ hit 1981 Zoot Suit, a film about young Pachucos and Pachucas in the wake of the 1942 Sleepy Lagoon murder, a mother yells at her daughter to scrub her face lest she “look like a Pachuca.” The same mother begs all her children to be careful wearing zoot suits—they know what police will think of them if they get into trouble—but only cautions her daughter about perceived sexual promiscuity.\nIn both Zoot Suit and in reality, Los Angeles media sensationalized the event, printing random photos of Pachucos and Pachucas who were completely unconnected to the murder. This press coverage confirmed many white Angelino’s deep-seated belief that young Chicanx men and women caused civil unrest and moral decay. Twelve young Mexican-American men were falsely convicted for the murder, and the media's popular hate speech lead to increased racism and hate crimes against the Chicanx community.\nIn response to the Sleepy Lagoon coverage, La Opinión, LA’s most popular Spanish-language daily, blamed Las Pachucas for white LA’s anger. There are also many historical reports of sexual violence against Pachucas from white individuals in positions of power (especially police).\nThe Sleepy Lagoon murder coverage spurred the 1943 Zoot Suit Riots, a week-long uproar during which white servicemen attacked anyone they could find wearing a zoot suit. Uniformed army men violently shredded clothing off of Pachucos’ and Pachucas’ bodies. Clothing had become a symbol of culture and survival. “I paid $75 for my outfit and nobody is going to take it off me,\" one Pachuca woman said.\nIn a society that denied them many conventional opportunities for wealth accumulation or self expression, this style of clothing constituted a rebellion against systemic race-based poverty and conservative gender norms. Their aesthetic was representative of the ways in which they chose to live their lives. Many would go out, sometimes with men, sometimes not, and just take up space and hang out in public.\n“I thought Pachucas were so cool,” Chicanx studies scholar Dr. Rosa-Linda Fregoso said. “They took over the street and taught me that it wasn't only a male space.” Pachucx faces and bodies populated the streets, theatres, and even police stations. In making themselves publicly visible, they demanded recognition. When sociologist Catherine Ramírez interviewed former Pachucas, she found that about half dressed for explicit political reasons, and another half wore what made them feel good.\nThe bold, powerful look still influences fashion today. In homage to icons like Cab Calloway, jazz legend Clive Davis often wears Zoot-inspired looks. Trendy It girl label Re/Done makes a series of ‘40s style pleated zoot jeans, and women wear oversize suits on the runway and the red carpet. For its Spring/Summer 2020 womenswear collectin, Balmain presented oversized zoot-influenced suits of many colors and fabrics. Jacquemus' Spring/Summer and Fall/Winter 2020 collections also included plenty of oversized, broad-shouldered jackets.\nIn these new contexts, the meaning of the Pachuca symbol has changed greatly. It is often worn not deliberately as an homage to Black or Chicanx culture, but rather as a general statement of female empowerment through menswear-inspired clothing. Still, even when stripped from its original powerful connotation, the Pachuca look continues to empower many. The Pachuca influence also gave way to new Chicanx youth culture trends, which continue to define the very ethos of perseverance for many in Los Angeles.\nIn the words of writer and artist Barbara Calderón-Douglass, the contemporary aesthetic shaped by the Pachuca look “embodies the remarkable strength and creative independence it takes to survive in a society where your social mobility has been thwarted by racism. The chola identity was conceived by a culture that dealt with gang warfare, violence, and poverty on top of conservative gender roles. The clothes these women wore were more than a fashion statement—they were signifiers of their struggle and hard-won identity.”"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:c125ecef-d01e-46e2-967e-16fccf14ab81>"],"error":null}
{"question":"What documents are required for registration at the Naviter Open paragliding competition in Slovenia?","answer":"For registration, pilots must present several documents: an FAI Sporting Licence issued by their local NAC, an FAI IPPI Card showing their skill level, their National flying licence, a signed application form, a signed waiver/release form, liability (3rd party) insurance documents, and proof of medical, rescue and repatriation insurance.","context":["Now in it's 7ᵗʰ year, pilots love the fun and friendly atmosphere, the in-depth daily briefings and the easy access to some of the best pilots in the world. You will find this to be the perfect springboard into competition flying, or maybe you will see it as the ideal environment in which to improve your XC skills.Expand page for more details\n- left before start of event\nRegistration will open on\n01/12 12:00 (Europe/Ljubljana)\nLive trackers supplied\nThe organiser will supply Live trackers to you, for safety and scoring\nThe region supports helicopter rescues\nMedic on takeoff\nThere will be medical rescue services at launch\nGood phone coverage\nThe region generally has good phone coverage\nWe will be monitoring radios for your safety whilst you fly\nLive trackers mandatory\nLive Tracking is MANDATORY in this event to score. The organiser will provide the trackers.\nThere will be vehicles supplied to take you to launch\nThere will be retrieve vehicles available to return pilots to HQ\nToilets on takeoff\nThere are toilets at, or near, the launch\nThe organisers will download the tracklogs at HQ for scoring\nThe organisers are using Airtribune for registration, tracking, blogging and to publish scores\nThere will be daily lunch packs for pilots\nMaps are supplied\nThe organisers will give you printed maps at registration\nThe organisers will give you an ID card with all the essential items listed\nThe organisers will have free WiFi available at HQ\nThere will usually be goal beers for winners\nThere will be a social evening with food\nThere will be an organised party during or after the event\nYou will receive a tshirt or other welcome gift\nCarbon offset / Plant a Tree\nThe organisers have commmitted to Carbon Offset the event and/or Plant a Tree for each competitor\nThe organisers support reducing the use of unnecessary plastic at the event. This includes NOT giving disposable water bottles\n- PRIZE DRAW GOODY BAGS\n- PRIZE DRAW NAVITER INSTRUMENT - OUDIE N\n- PRIZE DRAW NAVITER INSTRUMENT - OUDIE N+\n- TEAMS [beer prizes] |\n- Reynolds (u95kg) (for info only) |\n- OVERALL |\n- SPORTS |\n- STANDARD |\n- WOMENS |\nNow in it's 7ᵗʰ year, pilots love the fun and friendly atmosphere, the in-depth daily briefings and the easy access to some of the best pilots in the world. You will find this to be the perfect springboard into competition flying, or maybe you will see it as the ideal environment in which to improve your XC skills. Or simply to just enjoy flying in an international, friendly setting where the flying and social experience counts for much more than the results.\nNaviter are specialists for instruments, so expect there to be plenty of teaching on how you can improve your flying in this area. Indeed, Naviter are considered the leading instrument manufacturer for XC & Competition flying, and you will rarely see a top pilot without one.\nThe Naviter Open is a CAT 2 competition open to all Serial Class pilots (Pilots flying EN-A to EN-D wings). It is being held in Tolmin, Slovenia. The event is being supported by Naviter and and the Naviter Team.\nAt the end, we will be giving away lots of prizes through our famous 'names in a hat' system we have used each year. There will be several Naviter Oudie N's and more. But you have to be there to win.\nSlovenia is a beautiful venue and the flying is awesome. Set in the heart of the Julian Alps, you have it all. Grassy launches, amazing scenery and huge landing fields. Typical tasking is 40-80km flying in a pure Alpine environment. The Soca valley is very friendly, with land outs throughout in generally large, open grass fields. Typically we do not have strong valley winds. The backdrop of the Julian Alps is stunning, often still with Snow capped mountains in June. The camp site, where the event is based, makes the perfect venue for us to be hosted, with a bar, food and landing field all on hand. Otherwise, it is just a short drive to the town where you can find an array of hotels and accommodations to suit all budgets.\nWe look forward to flying with you here!\nSelection and entry fee\nThe event is a Serial Class only competition. That means no CCC gliders. But EN-D's and below are allowed. We want to have a fun week where everyone feels like they have a chance of scoring well, regardless of what they fly.\nFirst selection will be made on 15th December. Selection is based on an algorithm that considers many factors, including but not exclusive to, early registration, past attendance, the glider type you fly, WPRS and pilot experience. EN-D places will be awarded later than Sports places. Some places will be kept back for EN-D pilots at a later date. The organisers decision is final.\n♦ Payment will be made by Paypal. Details will be sent by email if your application to participate is accepted.\n♦ Only a deposit will be asked for at this time. The balance to be paid to the local organisers in June 23.\n♦ If you receive a payment offer you should pay immediately, otherwise we will offer your place to another pilot on the waiting list.\n♦ Do ensure you have read the cancellation policy before making any payment!\nThe payment will get you:\n♦ Daily transport to launch for all tasks\n♦ Retrieves daily\n♦ Daily lunch packs\n♦ Trackers, for safety and scoring\n♦ Typical tasking/scoring services\n♦ T-shirt & other souvenirs as we are able\n♦ Discounts for alternative activities in the event of bad weather\n♦ Other event benefits, as they are made available\n♦ There will be a few social evening with drinks organised, and as has become custom, an open, free bar on some evenings\nREQUIRED BY YOU\nAll Pilots must hold the following:\n♦ An FAI IPPI 5 licence\n♦ An FAI Sporting Licence (Don't confuse this with an IPPI card)\n♦ Medical/Travel insurance\n♦ Third party liability insurance (usually from your National Organisation)\n♦ A mobile phone\n♦ Recently repacked reserve parachute\n♦ Helmet, certified for paragliding use\n♦ Harness with certified back protection\n♦ 2M Radio capable of 143.900 Mhz FM\nIf you do not have ALL of the above you will NOT be permitted to participate.\nIn addition, you must supply a copy of a Certificate of Airworthiness valid in the 2 years prior to the event start & or a photo of the certification label in the wing (for wings less than 2 years old and with the manufacture date published in the wing, that is sufficient). You can upload this in Airtribune\nDocuments required at registration\nFAI Sporting Licence\nIssued by your local NAC on behalf of the FAI. You cannot normally compete in an FAI event without one. Contact your National Association (NAC)\nFAI IPPI Card\nThe FAI IPPI card is an International card that states your skill level. It is issued by your National Association and matches the level or your flying Licence\nThe organiser requires you to present your National flying licence.\nSigned Application form\nYou will need to sign an applicaton form at the venue. Usually the organiser will provide it on arrival.\nSigned Waiver/Release form\nYou will need to sign a waiver or release document to show you understand the risks. Usually the organiser will provide it on arrival.\nYou must provide the organiser with a copy of your liability (3rd party) insurance documents\nYou must provide the organiser with a copy of your medical, rescue and repatriation insurance. You can purchase Global Rescue cover from Airtribune\nPilots who want to cancel their registration should do it as soon as possible.\n♦ A pilot who cancels at least 45 days before the event start can ask for:\na. His or her entry fee to be used for the next event where he or she will be selected.\nb. A refund of the entry fee (an administrative fee of €60 will be deducted).\n♦ Cancellations received less than 45 days before the event start will lose the deposit and not be refunded. Nothing further will be due.\nCancellations must be informed by email and the date at which the email is received will be used to calculate your refund, if one is due.\n♦ Refunds are not immediate. We have one admin day per month.\n♦ Your deposit and/or payment is NOT transferable to another pilot. There is a strict selection process and you cannot bypass this by passing your payment to another pilot.\n♦ Please ensure you have adequate holiday travel insurance to cover any losses if you later find you are unable to participate.\nCANCELLATION BY US\nIn the event the organisers deem it necessary to cancel, relocate, or reschedule the competition you will be entitled to a partial refund:\n♦ Up to 45 days 45 days before the event start : 100% less a 30€ admin charge\n♦ Less than 45 days before the event start: 75% refund\n♦ Less than 7 days before the event start: A percentage of funds once any costs incurred by the local organiser have been deducted.\n♦ After the start of the competition any refunds will be at the sole discretion of the organisers.\nThe organisers will not be liable for any costs incurred by anyone attending, or intending to attend the event, or any consequential losses arising from the cancellation, relocation, or rescheduling of the event.\nPrize fund and scoring categories\nSports - All gliders EN-C and below\nLeisure - All gliders EN-B and below\nWomen - All females\nTeams - Top 3 score from a team of 5 persons\n(At least x3 Sports wings per team)\nPlease note CCC gliders are not allowed.\nSaturday 17 June\nSaturday 17 June\nUnofficial practice. Transport will be available at the camping and a task displayed. No retrieves or scoring. The transport cost this day is the standard parataxis charge and should be booked through the camping reception, as normal.\nSunday 18 June\n8.45am: “Mandatory Safety Briefing”\n1st task Sunday 18 June\nLast task Friday 23 June\n6-7 pm Trophies & Naviter Open Prize Draw\n08h30: Headquarters open\n09h00: Morning briefing @ HQ\n09h30: Transportation to take off\n10h30: Task and Safety Committee meetings\n11h15: Pilot briefing/Task briefing\n11h30: Take-off window opens (depending on the actual weather conditions)\n16h00: Scoring office opens\n19h00: Provisional results (depending on the last pilot being retrieved)\nThe daily schedule is subject to change. Any changes to the schedule will be posted on the whiteboard in HQ. Changes will be announced by the Meet Director at the Daily briefing\nAccommodation and Activities\nSlovenia has accommodation options to suit all budgets. The camping options are ideal because they are centrally located and within 2 minutes walk of the competition headquarters.\nCamper vans and Tents:\nThe Camping Gabrje campsite are the hosts for the Naviter Open. All the facilities you need including electricity, toilets & showers, restaurant & bar, Wifi, and the competition HQ are within 2 minutes walk of the camping pitches. For camping there is no need to pre-book, simply register at the front desk on arrival.\nFurther details of the campsite can be found at:\nHotels & Pensions:\nThere are several pensions and hotel options in and around Tolmin.\nThe Tolmin Tourist website: https://www.dolina-soce.com/en/\nLocal goings-on and services.\nIn: English, Italian, German, French & Slovene\nA large selections of hotels in the region available for booking.\nNOTE: Almost all hotels will require some form of transport to get to the HQ as they are minimum 1km from the camp site. We will try to provide morning transfers from the town centre to HQ though, on task days.\nHow to get here\nCamping and caravans\nTolmin is about a 14 hour drive from Calais/Dunkirk, about 6 hours from southern France and around 2 hours from Venice, Italy.\nThe nearest main international airport is Venice(VCN) (3 hours drive). Trevesio (3 hours), Trieste (45 mins) and Ljubljana (90 mins) airports represent attractive alternatives. Venice airport is serviced by many airlines.\nTrains from Venice to the Tolmin region (Most na Soci) cost about 40 Euro.\nTaxis from Venice to Tolmin cost about 300Euros.\nThere are a multitude of car hire companies operating out of all the airports. If hiring your car in Italy please make sure you have insurance and permission to cross the border into Slovenia. There is not normally any additional charge for this.\nTolmin, Gabrje Slovenia\nThe nearest international airports are Ljubljana (85 km away) and Trieste (70 km away) in Italy. From there you can catch a bus to Tolmin.\nYou can get to Tolmin by bus from most of the major cities in Slovenia.\nIf you travel from Ljubljana, you will find the timetable here:\nIf you travel from Nova Gorica, you will find the timetable here:\nThere is a train station at the nearby town of Most na Soci. It is about 10 minutes by taxi to Tolmin. Connecting trains are hard though as it is a small service. You can come from the north, through from Austria, with a change at Jesenice. You can also board at Nova Gorica in the south.\nThere is a Car Train service from the north than can turn a 1 hour car drive into a 35 minute ride through the beautiful valleys. But there are only 3 or 4 services a day, from Bohinska Bistrica.\nYou can get to Tolmin either from Kobarid, Nova Gorica or Ljubljana/Idrija. The Headquarters are located at Gabrje Campsite, which is ca. 4 km NW from Tolmin by the Soca river.\nNaviter Open 2022 Award Ceremony\n- Grant Oseland (154)\n- Mark Hayman (184)\n- Nick Lloyd (201)\n- Mathilde Mercier (232)\n- Valerie Ollier Dureault (120)\n- Ali Matthews (101)\n- Mark Hayman (184)\n- Matthijs Groeneveld (192)\n- Jernej Fidler (165)\n- Dennis Halamek (146)\n- Jernej Fidler (165)\n- Marijn van Rijsewijk (183)\nReynolds (u95kg) (for info only) |\n- Remko Bolt (207)\n- Marek Mastalerz (182)\n- Simon Holzmann (216)\nTEAMS [beer prizes] |\n- UKAF 1\n- UKAF 4\n1 UKAF 1 Nick LLOYD Andy MCDONALD Karl FORD Nicholas DOWDALL | 2 Cloudbusters Matthijs GROENEVELD Remko BOLT Marcin KRAWCZYK Ilona LODDER | 3 UKAF 4 Grant OSELAND Aaron HALL Andy BROWNTEAMS [beer prizes] | results\nPRIZE DRAW NAVITER INSTRUMENT - OUDIE N\n- Andrew GASKELL\nAndrew GASKELL won a brand new Oudie N instrument from Naviter Instruments, with his name being drawn in the lottery.\nPRIZE DRAW NAVITER INSTRUMENT - OUDIE N+\n- James ALLCOCK\nJames ALLCOCK won a brand new Oudie N+ instrument from Naviter Instruments, with his name being drawn in the lottery.\nPRIZE DRAW GOODY BAGS\n- Damian Presser\n- Michael Buck\nGoody bags with book about Hike and Fly by one of the pilots, Aimilios Apostolopoulos from Greece."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:953aca1b-544f-44f0-ae5f-eda122c99f90>"],"error":null}
{"question":"What are the key manufacturing capabilities of FDM 3D printing technology, and how is it being applied to revolutionize housing construction globally?","answer":"FDM 3D printing technology can create parts with different layer thicknesses (.007\" to .013\") and runs 24/7, allowing quick project turnaround in as little as 1 day. The parts can achieve 75% of the strength of molded parts and are used for prototypes, testing, and production. In construction, this technology has enabled remarkable achievements like printing a 250 square meter office complex in Dubai in just 17 days for $140,000. The construction applications show significant benefits, including 70% reduction in labor demand, 90% cost reduction, and 80% faster building times. Companies like ICON have demonstrated the ability to print houses in 24 hours for as low as $4,000, making it a promising solution for affordable housing globally.","context":["Fused Deposition Modeling (FDM)\nWhat is FDM?\nStratasys’ co-founder Scott Crump invented the Fused Deposition Modeling (FDM) technology and process. He patented FDM in 1989. FDM uses a thermoplastic filament that is heated then extruded layer by layer to create a 3D dimensional object.\nToday, FDM is the most widely used 3D printer because of the ease of use and that it runs engineering grade plastics. Over the last 10 years, FDM has become more widely known and affordable by providing desktop 3D FDM printers as well as industrial grade machines. Currently, there are hundreds of available 3D FDM printers in all different sizes, for different materials and applications.\nFDM runs 24/7 with connections to provide real-time status to our AM technicians. The ease of material changeouts allow flexibility to run all different materials. This flexibility, along with the minimal finishing required, allows the ability to turn projects around in as little as 1 day. The Stratasys Fortus machines are made for repeatable, industrial printing and provide the best parts and best finishes FDM can offer.\nMost FDM parts are used for fit and function prototypes, vacuum form templates, testing, jigs & fixtures and production parts. Engineers often choose the FDM printer because printed part strength can be roughly 75% a molded part of the same material (ex ABS).\nHow does FDM work?\nThe FDM printer builds a 3D model of a component from a 3D CAD file using a thermoplastic filament. That filament is then heated and extruded or deposited layer by layer until the part is complete. There is typically a heated nozzle for both support material and thermoplastic material. The machine reads the layer slice then deposits the support material and plastic materials as required. After the layer is complete, the build platform is lowered and the next layer is started. When the build is complete. The FDM parts are removed from the machine, removed from the platform and support material is removed often by hand or small tools.\nFDM Build Layers\n- .007” Build Layers – Best part finish, accuracy, detail, and strength but increases cost and build times*\n- .010” Build Layers – Best balance of finish, accuracy, detail, strength with cost and build times\n- .013” Build Layers – Best for quick low-cost parts but finish, accuracy, detail, and strength are not as great\n- Tough & Durable Prototype Parts\n- Real Thermoplastic Materials\n- End-Use Production Parts\n- Ease of Use\n- Test Parts Production Materials\n- No size limitation – Section and Bond Large Parts Together\nFDM Best Uses\n- Fit Checks\n- Material Testing\n- Master Patterns\n- Durability Testing\n- End Use Production\n- Heat Resistance\n- UL 94-Rated\n- Color Printing\nNot sure which additive process is best for your application?\nDownload our Best Additive Uses Chart (pdf).\nFDM Printer & Equipment\nLarge Industrial FDM Machines (Fortus 400 & 900)\nThese FDM printers have large build platforms from 16” x 14” x 16” (400mm x 350mm x 400mm) to 24” x 36” x 24” (900mm x 600mm x 900mm). These machines can build parts at different layers of .007”, .010” and 0.013” (125-330 micron) layers depending on the material chosen and requirements for parts. These are great for building large parts or loading up lots of smaller parts.\nDesktop 3D Printers\nThese 3d printers have smaller build platforms, roughly 11” x 7” x 6” (280mm x 175mm x 150mm). Most printers print in layers of 0.004”-0.010” (100-250 micron). Desktop printers are great for home projects, schools, and engineering offices. Parts are great for fit & function or gadgets.\nTTH offers (4) standard FDM materials based on the requirements of our customers and industries. Other materials are available for special requirements. Here is a list of standard materials offered:\n- BEST for STANDARD PROTOTYPING or ABS PRODUCTION PARTS\n- White ABS Material (available in other colors as requested)\n- Soluble Supports\n- Applications: Form, Fit and Function Prototyping, Master Patterns for Vacuum Castings, Testing, and Production\n- BEST for TOUGH & RIGID PROTOTYPES or PC PRODUCTION PARTS\n- White PC Material\n- Soluble Supports\n- Applications: Form, Fit and Function Prototyping, Jigs & Fixtures, Master Patterns for Vacuum Castings and Production\n- BEST for TOUGH & RIGID PRODUCTION MEDICAL COMPONENTS\n- White PC Material\n- Applications: Medical Device Components & Parts\n- BEST for HIGH HEAT PROTOTYPES & PRODUCTION PARTS\n- Ultem Material Colored Black or Tan\n- Tough, Durable & Rigid\n- Applications: Aerospace & Medical Parts, UL94V-O Rated Parts, High Heat Applications\nDon’t see the materials you want to use? It may be available with another additive process.\nDownload our Materials Availability Chart (pdf).\nFDM Finishes Available (*Standard*):\n- *Level 1 – Quick Finish = Support Removal Only*\n- Level 4 – Primer Finish = Sand/Remove/Fill All Build Lines & Primer\n- Level 5 – Painted Finish = Sand/Remove/Fill All Build Lines, Primer, and Paint/Mask/Texture as Needed (Show Quality Finish)\n- Metal Finish = Level 1 with Primer & Plated with Chrome, Nickel, Brushed Nickel, Gold, or Other\n- Minimum Wall Thickness & Feature Size\n- .007” Build = .028” (.71mm)\n- .010” Build = .040” (1.02mm)\n- .013” Build = .052” (1.32mm)\n- Tolerances = +/- 0.010” for first inch then +/- 0.002” per inch thereafter\n- Inserts = Preferred for threads. Install in Post-Processing with Heat Stake or Adhesives\n- Holes = Drill, Ream, and Tap (Print threads then chase or ream in finishing.","For the last week, Michael Torpak, a sculptor from Prague, has been overseeing his latest project.\nIt’s a new home that he hopes will be ready to launch on the Vltava River in early August.\nThe home itself took a few weeks to design. It’s shaped like a protozoa – a primitive sea living and free moving parasite – with one bedroom, a kitchen and a planted roof.\nWhat’s remarkable about the structure is that it was built in less than 48 hours, for a cost for a few thousands euros.\nTorpak worked with a 3D printing company to do the job. The mortar was printed layer by layer, and then the windows, panes, electricity and utilities were installed by a hand.\nIt’s a nice little project. And houses and huts like this are springing up cities across the world.\nIn Dubai, for example, a Chinese company called WinSun has printed a 250 square meter office complex using a printer the size of a five aside football field.\nThe building was finished in just 17 days and cost $140,000. It stands 31 feet tall with a traditional foundation made of precast concrete, and a structure that is reinforced with concrete and rebar.\nDubai plans to ‘print’ 25% of all ‘new build’ property by 2030. And there are serious benefits to this method of construction.\nFor one thing, it’s a cheap way to build. The Dubai government estimates that reaching the 25 percent 3D printed building goal will reduce labor demand in construction by 70 percent, cut costs by 90 percent, and reduce the time it takes to build by 80 percent.\nSecondly, this method of construction is quite friendly to the environment.\nBuilding and construction are responsible for 39% of all carbon emissions in the world, with operational emissions (from energy used to heat, cool and light buildings) accounting for 28%.\nBut by using materials and printing houses in a matter of days, there is serious scope for reducing the emissions.\nA group of architects in America have used mud to create prototype 3D-printed earth structures for example.\nItalian 3D-printing company WASP is even working on a 3D printing machine that can build houses from soil and agricultural waste.\nWill we build structures like these in the UK?\nI think we will. I can see how you might decide to build a structure of your own in your back garden, or a modest 3D cabin by a river or lake in the countryside.\nThe house below, for example, was 3D printed by a company called ICON in Texas.\nThe prototype for the 3D printed house cost around $10,000 in about 24 hours. But homes of about 55 to 75 square metres will soon be “printed” in under 24 hours for $4,000.\nYou can take a look around the inside of the house in this video.\nA Planet of Slums\nI think the biggest use of 3D printing, however, could be in supplying affordable housing in cities.\nIn the last few decades, we’ve seen a vast exodus in developing countries from the countryside in the city. Many of these people have ended up living in vast slums outside major cities with very poor housing and facilities.\nThe scale of this industrialisation utterly dwarfs that of the Victorian age, says Mike Davis in his book Planet of Slums.\nLondon in 1910 was seven times larger than it had been in 1800.\nBut today, Dhaka in Bangladesh, Kinshasa in the Congo, and Lagos in Nigeria, are each approximately 40 times larger than they were in 1950.\nIn many cities, 3D printing could provide a means to produce good quality housing for slum dwellers.\nIt would be relatively cheap for the government to undertake these projects, and it might help reduce poverty and the environmental impact of slum life.\nAs Peruvian economist Hernando de Soto has pointed out, these slums are often a thriving hotbed of small entrepreneurs’ selling vegetables, doing laundry, and even opening small bars and restaurants.\nIf these people were afforded decent housing and property rights, they might be able to borrow against their property to scale up their businesses.\nICON, who built the house in Texas, are currently 3D printing affordable housing in El Salvador and have plans to expand from there.\n3D Printing Farms\nIn fact, I see a big future for 3D printing.\nIn the next ten years, I can see printer farms springing up in most major cities.\nThese factories will have printers of all sizes and they will could supply most of the goods needed where they are needed—whether semiconductors, medical equipment, drugs, body parts, food, vehicles, robots or houses.\nClearly, the present way of manufacturing things is unsustainable.\nThe global supply chains involved in sourcing components from different parts of the world, cobbling them together into products and then shipping them to end markets is increasingly risky and costly.\nThis is where a cluster of fast evolving technologies comes in, comprising generative software design, 3D printing, robotics and new synthetic materials.\nIn fact, you can read more about the 3D Printing boom in our free Life After Covid report."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:f93837c3-24bf-462f-aef5-472752a1a05b>","<urn:uuid:f3395a34-dfed-4684-9133-2ec835e10b92>"],"error":null}
{"question":"How to join the Communist League in 1847? What was process for new members?","answer":"To join the Communist League, a candidate had to meet several requirements and follow this process: 1) They needed to have acknowledged means of subsistence, conduct themselves in a manly fashion, and never have committed a dishonorable action. 2) They could not belong to any political or national association. 3) They had to be unanimously admitted into a community. 4) Two League members would read the Rules to the candidate and ask five questions about their commitment to community of property principles, willingness to work for these principles, promise to maintain secrecy, and comply with League decisions. 5) Finally, they had to give their word of honor as guarantee to observe these commitments.","context":["Special pages :\nDraft Rules of the Communist League\n|Author(s)||Communist League (Marx-Engels)|\nFirst published: Gründungs dokumente des Bundes der Kommunisten (Juni bis September 1847), Hamburg, 1969;\nThis document is a draft of the Rules of the Communist League adopted at its First Congress in the beginning of June 1847 (see Note 69) and distributed among the circles and communities for discussion. It shows the reorganisation work done by the League of the Just leaders as agreed with Marx and Engels, who consented early in 1847 to join the League on the condition that it would he reorganised on a democratic basis and all elements of conspiracy and sectarianism in its structure and activity would be eliminated. Engels, who was present at the Congress, took a direct part in drawing up the Rules. The draft recorded the change in the League’s name, and it is referred to here as the Communist League for the first time. The new motto, “Working Men of All Countries, Unite!” was also used for the first time. The former leading body, the narrow People’s Chamber (Halle), was replaced by the supreme body — the Congress, composed of delegates from local circles; the executive organ was to be the Central Authority. The relations between all the League organisations were based on principles of democratism and centralism. At the same time a number of points in the draft showed that the reorganisation was not yet complete and that former traditions were still alive, namely: Art. 1 formulating the aims of the League; one of the points in Art. 3, making the sectarian stipulation that members were not to belong to any other political organisation; Art. 21, limiting the powers of the Congress by the right of the communities to accept or reject its decisions, etc. On the insistence of Marx and Engels these points were later deleted or altered. The Second Congress (November 29-December 8, 1847) adopted the Rules in an improve(] and more perfect form, which finally determined the structure of the Communist League according to the principles of scientific communism. This document was discovered, together with the “Draft of a Communist Confession of Faith” in 1968 among the papers of Joachim Friedrich Martens, a member of the Communist League in Hamburg.\nSECTION I THE LEAGUE[edit source]\nArt. 1. The League aims at the emancipation of humanity by spreading the theory of the community of property and its speediest possible practical introduction.\nArt. 2. The League is divided into communities and circles; at its head stands the Central Authority as the executive organ.\nArt. 3. Anyone who wishes to join the League is required:\na. to conduct himself in manly fashion;\nb. never to have committed a dishonourable action;\nc. to recognise the principles of the League;\nd. to have acknowledged means of subsistence;\ne. not to belong to any political or national association;\nf. to be unanimously admitted into a community, and\ng. to give his word of honour to work loyally and to observe secrecy.\nArt. 4. All League members are equal and brothers, and as such owe each other assistance in every situation.\nArt. 5. All members bear League names.\nSECTION II THE COMMUNITY[edit source]\nArt. 6. A community consists of at least three and at most twelve members. Increase above that number will be prevented by division.\nArt. 7. Every community elects a chairman and a deputy chairman. The chairman presides over meetings, the deputy chairman holds the funds, into which the contributions of the members are paid.\nArt. 8. The members of communities shall earnestly endeavour to increase the League by attracting capable men -and always seek to work in such a way that principles and not persons are taken as guide.\nArt. 9. Admission of new members is effected by the chairman of the community and the member who has introduced the applicant to the League.\nArt. 10. The communities do not know each other and bear distinctive names which they choose themselves.\nSECTION III THE CIRCLE[edit source]\nArt. 11. A circle comprises at least two and at most ten communities.\nArt. 12. The chairmen and deputy chairmen of the communities form the circle authority. They elect a president from among themselves.\nArt. 13. The circle authority is the executive organ for all the communities of the circle.\nArt. 14. Isolated communities must either join an already existing circle authority or form a new circle with other isolated communities.\nSECTION IV THE CENTRAL AUTHORITY[edit source]\nArt. 15. The Central Authority is the executive organ of the whole League.\nArt. 16. It consists of at least five members and is elected by the circle authority of the place where it is to have its seat.\nSECTION V THE CONGRESS[edit source]\nArt. 17. The Congress is the legislative authority of the League.\nArt. 18. Every circle sends one delegate.\nArt. 19. A Congress is held every year in the month of August. The Central Authority has the right in important cases to call an extraordinary congress.\nArt. 20. The Congress in office decides the place where the Central Authority is to have its seat for the current year.\nArt. 21. All legislative decisions of the Congress are submitted to the communities for acceptance or rejection.\nArt. 22. As the executive organ of the League the Central Authority is responsible to the Congress for its conduct of its office and therefore has a seat in it, but no deciding vote.\nSECTION VI GENERAL REGULATIONS[edit source]\nArt. 23. Anyone who acts dishonourably to the principles of the League is, according to the circumstances, (removed) either removed or expelled. Expulsion precludes re-admission.\nArt. 24. Members who commit offences are judged by the (supreme) circle authority, which also sees to the execution of the verdict.\nArt. 25. Every community must keep the strictest watch over those who have been removed or expelled; further, it must observe closely any suspect individuals in its locality and report at once to the circle authority anything they may do to the detriment of the League, whereupon the circle authority must take the necessary measures to safeguard the League.\nArt. 26. The communities and circle authorities and also the Central Authority shall meet at least once a fortnight.\nArt. 27. The communities pay weekly or monthly contributions, the amount of which is determined by the . respective circle authorities. These contributions will be used to spread the principles of the community of property and to pay for postage.\nArt. 28. The circle authorities must render account of expenditures and income to their communities every six months.\nArt. 29. The members of the circle authorities and of the Central Authority are elected for one year and must then either be confirmed anew in their office or replaced by others.\nArt. 30. The elections take place in the month of September. The electors can, moreover, recall their officers at any time should they not be satisfied with their conduct of their office.\nArt. 31. The circle authorities have to see to it that there is material in their communities for useful and necessary discussions. The Central Authority, on the other hand, must make it its duty to send to all circle authorities such questions whose discussion is important for our principle.\nArt. 32. Every circle authority and failing that the community, even every League member, must, if standing alone, maintain regular correspondence with the Central Authority or a circle authority.\nArt. 33. Every League member who wishes to change his residence must first inform his chairman.\nArt. 34. Every circle authority is free to take any measures which it considers advisable for the security of the circle and its efficient work. These measures must, however, not be contrary to the general Rules.\nArt. 35. All proposals for changes in the Rules must be sent to the Central Authority and submitted by it to the Congress for decision.\nSECTION VII ADMISSION[edit source]\nArt. 36. After the Rules have been read to him, the applicant is asked by the two League members mentioned in Art. 9 to reply to the following five questions. If he replies “Yes”, he is asked to give his word of honour, and is declared a League member.\nThese five questions are:\na. Are you convinced of the truth of the principles of the community of property?\nb. Do you think a strong League is necessary for the realisation of these principles as soon as possible, and do you wish to join such a League?\nc. Do you promise always to work by word and deed for spreading and the practical realisation of the principles of the community of property?\nd. Do you promise to observe secrecy about the existence and all affairs of the League?\ne. Do you promise to comply with the decisions of the League?\nThen give us on this your word of honour as guarantee!\nIn the name and by the order of the Congress\nHeide [Wilhelm Wolff]\nKarl Schill [Karl Schapper]\nLondon, June 9, 1847"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:666f7e83-dd47-4678-bafa-d5a09ca850ad>"],"error":null}
{"question":"What are the key differences between RNA and DNA probes in terms of hybridization strength and protocol requirements, and how does this compare to the optimization requirements in one-step versus two-step RT-qPCR?","answer":"RNA and DNA probes exhibit different hybridization characteristics and protocol requirements. DNA probes provide high sensitivity but don't hybridize as strongly to target mRNA molecules compared to RNA probes, and importantly, formaldehyde should not be used in post-hybridization washes with DNA probes. RNA probes should be 250-1,500 bases in length, with 800 bases showing optimal sensitivity and specificity. Similarly, optimization requirements differ between one-step and two-step RT-qPCR. One-step RT-qPCR requires compromised reaction conditions as both RT and PCR must occur together, affecting efficiency and yield. In contrast, two-step RT-qPCR allows for individual optimization of both the RT reaction and qPCR steps, typically producing higher yields and enabling better optimization for challenging sequences.","context":["For the best experience on the Abcam website please upgrade to a modern browser such as Google Chrome\nIn situ hybridization indicates the localization of gene expression in their cellular environment. A labeled RNA or DNA probe can be used to hybridize to a known target mRNA or DNA sequence within a sample. This labeled RNA or DNA probe can then be detected by using an antibody to detect the label on the probe. The probes can therefore be used to detect expression of a gene of interest and the location of the mRNA.\nYou can also access our most popular protocols straight from your phone with the Abcam app, which features protocols, scientific support and a suite of useful tools that are handy for any bench scientist. Learn more.\nPreserving RNA is made difficult due to presence of RNase enzyme. This enzyme is found on glassware, in reagents and on operators and their clothing. RNase quickly destroys any RNA in the cell or the RNA probe itself so users must ensure they use sterile techniques, gloves, and solutions to prevent RNase contamination of the probe or tissue RNA.\nGeneral sample storage when using frozen sections\nFor best results on older slides, do not store slides dry at room temperature. Instead, store in 100% ethanol at -20°C, or in a plastic box covered in saran wrap at -20°C or -80°C. Such storage preserves slides for several years.\nRNA probes should be 250–1,500 bases in length; probes of approximately 800 bases long exhibit the highest sensitivity and specificity. Transcription templates should allow for transcription of both probe (antisense strand) and negative control (sense strand) RNAs. Clone into a vector with opposable promoters to achieve this. Circular templates must be linearized before making a probe, though PCR templates can also be used.\nDNA probes provide high sensitivity for in situ hybridization. They do not hybridize as strongly to target mRNA molecules compared to RNA probes, so formaldehyde should not be used in the post hybridization washes.\nProbe specificity is important. If the exact nucleotide sequence of the mRNA or DNA in the cell is known, a precise complementary probe can be designed. If >5% of base pairs are not complementary, the probe will only loosely hybridize to the target sequence. This means the probe is more likely to be washed away during wash and detection steps and may not be correctly detected.\nThis protocol describes the use of DIG-labeled single-stranded RNA probes to detect expression of the gene of interest in paraffin-embedded sections.\nFor frozen sections, start at Step 2. If using formaldehyde-fixed paraffin-embedded sections, continue with this step.\nBefore proceeding, slides must be deparaffinized and rehydrated. Incomplete removal of paraffin can cause poor staining of the section.\nPlace the slides in a rack and perform the following washes:\nKeep the slides in the tap water until antigen retrieval. From this point onwards the slides cannot dry as this will cause non-specific antibody binding and high background staining.\n2) Antigen retrieval\nDigest with 20 µg/mL proteinase K in pre-warmed 50 mM Tris for 10–20 min at 37°C. Incubation time and proteinase K concentration may require optimization.\nWe recommend trying a proteinase K titration experiment to determine optimal conditions. Insufficient digestion will reduce hybridization signal and over-digestion will result in poor tissue morphology, making localization of the hybridization signal very difficult. Optimal proteinase K concentration will vary depending on the tissue type, length of fixation and size of tissue.\n3) Rinse slides 5x in distilled water.\n4) Immerse slides in ice-cold 20% (v/v) acetic acid for 20 sec. This permeabilizes the cells to allow access to the probe and the antibody.\n5) Dehydrate the slides by washing for approximately 1 min per wash in 70% ethanol, 95% ethanol and 100% ethanol, then air dry.\n6) Add 100 µL of hybridization solution to each slide.\n|Reagent||Final concentration||Amount to use per 1 mL of solution|\n|Denhardt's solution||5x||50 µL|\n|Dextran sulphate||10%||100 µL|\n|Herapin||20 U/µL||10 µL|\nSalt solution (20x)\nDenhardt’s solution (100x)\n7) Incubate the slides for 1 h in a humidified hybridization chamber at the desired hybridization temperature. Typical hybridization temperatures range between 55–62°C.\n8) Dilute the probes in hybridization solution in PCR tubes. Heat at 95°C for 2 min in a PCR block to denature the RNA or DNA probe. Chill on ice immediately to prevent reannealing.\n9) Drain off the hybridization solution. Add 50–100 μL of diluted probe per section, covering the entire sample. Incubate in the humidified hybridization chamber at 65°C overnight. Cover the sample with a cover slip to prevent evaporation.\nDuring this step, the RNA probe will hybridize to the corresponding mRNA, or the DNA probe will hybridize to the corresponding cellular DNA. Optimize the hybridization temperature depending on the sequence of the probe used, as well as the cell or tissue type. This temperature should be optimized for each tissue type analyzed.\nOptimal hybridization temperature for probes depends on the percentage of bases present in the target sequence. The concentration of cytosine and guanine in the sequence are important factors.\n10) Stringency washes\nSolution parameters such as temperature, salt and detergent concentration can be manipulated to remove non-specific interactions.\nTo prepare 1 L of 20x saline sodium citrate (SSC)\nWash away excess probe and hybridization buffer with higher temperatures (up to 65°C) for short periods of time. If left too long this can wash off too much of the hybridized probe RNA/DNA.\nThis step removes non-specific and/or repetitive DNA/RNA hybridization.\nFollow these guidelines to optimize temperature and salt concentration for stringency washes:\n11) Wash twice in MABT (maleic acid buffer containing Tween 20) for 30 min at room temperature. MABT is gentler than PBS and is more suitable for nucleic acid detection.\nTo prepare 5x MABT stock\nBring the pH to 7.5 by adding Tris base. About 100 g will be required. Bring final volume to 1 L.\n12) Dry the slides.\n13) Transfer to a humidified chamber and add 200 µL blocking buffer to each section (MABT + 2% BSA, milk or serum). Block for 1–2 h at room temperature.\n14) Drain off the blocking buffer. Add the anti-label antibody at the required dilution in blocking buffer. Check the antibody datasheet for a recommended concentration/dilution. Incubate for 1–2 h at room temperature.\n15) Wash slides 5x10 min with MABT at room temperature.\n16) Wash the slides 2x10 min each at room temperature with pre-staining buffer (100 mM Tris pH 9.5, 100 mM NaCl, 10 mM MgCl2).\n17) For fluorescence detection, proceed to Step 18. For other forms of detection, return the slides to the humidified chamber and follow manufacturer’s instructions for color development.\n18) Rinse slides in distilled water.\n19) Air dry the slides for 30 min. Wash in 100% ethanol and air dry thoroughly.\n20) Mount using DePeX mounting solution.","When performing real-time qPCR, one has to decide whether to use a one-step protocol that combines the RT reaction and PCR in one tube, or a two-step protocol where the RT reaction is performed separately from the PCR (Figure 1). Each method has its advantages and disadvantages that extend from the ease of logistics and cost of reaction to the resulting yield and sequence representation. One-step reactions are certainly easier to set up with less overall hands-on time, but don’t provide the flexibility and control possible with two-step reactions. One-step or two-step, which would you choose?\nFigure 1. Comparison of Workflows for One-Step and Two-Step RT-qPCR Procedures. In the one-step protocol, in a single tube RNA is converted to cDNA and all of the cDNA is immediately used for qPCR quantification. No discrete cDNA product is generated. In a two-step protocol, cDNA is produced in the RT step, and a portion of that cDNA is used to set up a subsequent qPCR reaction.\nOne-Step RT-qPCR: Convenience with Minimal Handling\nIn one-step RT-qPCR, reverse transcription and PCR occur in the same tube. Gene-specific primers are used for generating the cDNA and for subsequent amplification. Major advantages of one-step reactions include limited sample handling and reduced bench time, which helps to decrease chances for pipetting errors and cross contamination. This method is quick to set up and makes processing multiple RNA samples easy when you are amplifying only a few genes of interest. It is also ideal for situations where the same genes must be amplified repetitively, such as in high-throughput detection of a specific target using well-established reaction conditions.\nHowever, when using this protocol, you must take several concerns into account. For example, with one-step RT-qPCR, you are fully committed to the amount and quality of the RNA used in the reaction. With regards to RNA quality, it is important to remember that common contaminants such as fatty acids, buffer salts, phenol, or other components from sample isolation can compromise the efficiency of the RT reaction which is the most sensitive part of the procedure.\nIn addition, when using oligo(dT) or random hexamer primers with one-step RT-PCR, all of the RNA sample is converted to cDNA and all of the cDNA is used in the PCR. So if the copy number of your target sequence is high, signal may come up too early, or if there is not enough cDNA, you may not detect your target. If you want to adjust your template concentrations, or to amplify other genes at a later date using the one-step method, you must keep an aliquot of the original RNA sample and run the entire reaction again. Because of these factors, one-step reactions may require substantially more RNA in your initial samples if you are performing multiple amplifications. Variation between these different RT reactions can complicate assay interpretation significantly.\nFurthermore, the combined RT and PCR reagents must allow these reactions to proceed together in one tube. This prevents use of the most optimal reagents and conditions for each individual reaction. So while all of the cDNA from the RT reaction is used in the PCR in a one-step RT-qPCR—theoretically providing very sensitive detection—the compromised reaction conditions affect efficiency and, thus, yield.\nThe specialized systems that allow both reactions to occur together are usually supplied through commercial kits, which can be expensive. However, one-step RT-qPCR reactions are smaller, using less reagent(s) compared to the two-step method.\nFinally, one-step RT-qPCR requires careful evaluation to prevent primer dimer formation because the gene specific primers will be present not only during PCR cycling, but also during the lower temperature conditions of the RT reaction.\nTwo-Step RT-qPCR: Control and Flexibility\nIn two-step RT-qPCR, reverse transcription and PCR are performed as two separate reactions, with RNA converted to cDNA first, using a choice of random hexamers, oligo dT primers, or even gene-specific primers. Use of random hexamers and/or oligo dT primers provides a cDNA archive of all RNA species in the sample. Either a portion of the RT reaction is diluted into the PCR, or the RT reaction can be extracted and precipitated, allowing control over the amount of cDNA input. This flexibility is an asset when working with genes of variable abundance. Any residual cDNA is available for future amplification reactions of other genes, or even for other applications. Use of the cDNA from the same RT reaction for multiple PCRs makes it easier to compare results across experiments because reaction variability and failure is limited to the PCR step, which simplifies any required troubleshooting.\nTwo-step RT-qPCR allows for individual optimization of the RT reaction and qPCR for efficiency and maximum sequence representation. These protocols typically produce higher yields of cDNA during the RT than one-step procedures. Because of the ability to optimize both reactions, and the higher cDNA yields, two-step reactions can be more sensitive than one-step reactions. The ability to separately optimize reactions is also beneficial when performing PCR on challenging sequences.\nBeyond just having more control over the reactions and variability, the two-step method is particularly useful when the goal is to detect multiple messages from a single RNA sample or perform multiple PCR amplifications from a single cDNA sample.\nSo One-Step or Two?\nThe choice between one-step and two-step reactions comes down to the goal of your experiment. Use one-step RT-qPCR when you have a lot of sample, and/or a limited number of known targets that you may be repeatedly amplifying. It can be ideal for high-throughput detection applications that use an optimized reaction with well‑tested primers.\nUse two-step RT-qPCR when your sample is precious and/or you need to examine multiple targets or targets that may not be well characterized and may require you to go back to test new targets. Two-step RT-qPCR will also give you the control to optimize in difficult PCR situations and when target abundance is variable.\nChoosing Between One-Step and Two-Step RT-qPCR\n| ||Two -Step Protocol ||One-Step Protocol |\n|Primers used in RT || |\n- Oligo(dT) primers\n- Random hexamers\n- Gene-specific primers\n- A mix of these\n|Advantages || |\n- Choice of primers\n- Optimize reactions for maximum yield\n- Modulate amount of RT that goes into PCR—controlling for target abundance\n- Perform multiple PCR reactions on the same cDNA sample\n- Adjust for challenging PCR (e.g., GC-rich sequences)\n- Experiment with different RT and Taq enzymes\n- Quick setup and limited handling\n- Easy processing of multiple samples for repetitive tests, or high-throughput screening\n- Fewer pipetting steps, reducing potential errors\n- Eliminates possibility of contamination between the RT and qPCR steps\n|Considerations || |\n- Requires more setup, hands-on, and machine time\n- Additional pipetting increases the chances for experimental errors and contamination\n- Uses more reagents\n- Must “start over,” or save RNA aliquot and perform new RT to analyze new target or repeat amplifications\n- Reaction conditions are not optimal—efficiency & thus quantification are affected\n- Primer dimers a bigger potential problem\n|Best for: || |\n- amplifying multiple targets from a single RNA source\n- when you plan to reuse cDNA for additional amplifications\n- working with multiple RNA samples to amplify only a few targets\n- assays performed repeatedly"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:d8a4866d-1c33-4991-bd09-15c588691a9f>","<urn:uuid:649aadc1-3212-45d9-8895-1b2e76517911>"],"error":null}
{"question":"How do I access complimentary Wi-Fi and entertainment services on Viking Heimdal river cruises?","answer":"Viking Heimdal cruise deals include complimentary Wi-Fi internet access. The ship also offers live evening entertainment and daily lectures. These services, along with 24-hour coffee/tea and daily bottled water, are all included in the cruise fares.","context":["Viking Heimdal Review and Specifications\nSpecifications of Viking Heimdal\n|Builder||Neptun Werft GmbH (Rostock, Germany)|\n|Owner||Viking River Cruises|\n|Speed||15 kn / 28 kph / 17 mph|\n|Length (LOA)||135 m / 443 ft|\n|Beam (width)||29 m / 95 ft|\n|Gross Tonnage||5000 gt|\n|Passengers||95 - 190|\n|Decks with cabins||3|\n|Christened by||Susie Barrie|\nReview of Viking Heimdal\nThe Viking Heimdal boat cruises on cruises on Saone and Rhone rivers in France, departing from Avignon and Chalon-sur-Saone.\nViking Cruises Heimdal ship was designed by the legendary architects Yran & Storbraaten, standing behind Seabourn Yachts. This new river cruise boat offers state-of-the-art patented design, luxury and engineering. All Viking Longships offer guests more ways for personalizing their trip. Viking Heimdal longship offers two Explorer Suites with spectacular 270-degrees views and private wraparound verandas. The seven Veranda Suites have two full rooms, veranda off their living room and French balcony in bedroom. There are also 39 Veranda Staterooms, 25 Standard Staterooms and 22 French Balcony Staterooms. The Observation lounge and bar have floor-to-ceiling glass doors for panoramic views. The Aquavit indoor-outdoor terrace is located at the bow of ship, ideal spot for relax, as well as the Sun Deck with shaded sitting area and 360-degree views.\nViking Heimdal river cruise prices are per person and based on double occupancy. These are only indicative Viking cruise rates. They might be different when you book your Viking River Cruises France cruisetour deals, influenced by travel agency promotions, special offers and discounts, group travel booking or last-minute deals rates. On some Viking Heimdal France cruise tours, the line offers round-trip USA airfare discount deals (on select cabins or itineraries/dates).\nRiver Cruise Itineraries\nFollows the list of Viking Heimdal river cruise itineraries as themes, officially announced by the Viking Cruises line:\n- \"PORTRAITS OF SOUTHERN FRANCE\" are 7-day Rhone and Saone river cruises from Avignon to Chalon, and the reverse from Lyon/Chalon to Avignon. These deals include 6 complimentary land tours/excursions in the Provence and Burgundy (France wine regions).\n- Also included in Viking France river cruise fares are: bus transfers (Lyon to Chalon /embarkation, Avignon to Marseille /airport), scenic drive (Pommard-Volnay-Meursault), Beaune (cellar wine tasting), Tournon (red wines tasting), Chateauneuf-du-Pape (wine tasting).\nViking Heimdal wiki\n- Accident reports and news CruiseMinus.com\n- Cabin grades number: 8\n- Total number of cabins: 95 (of which 9 Suite, 22 French Balconies, 25 with a non-opening window).\n- Restaurants: Heimdal (dining room), Aquavit Terrace (al fresco).\n- Lounges & Bars: Observation, Heimdal Atrium (lobby), Library (Internet), Shop (gifts, souvenirs), top-deck (shaded relaxation area, Sky Bar, Herbal Garden, putting green, walking track, shaffleboard). The Heimdal longship doesn't have gym room, pool, jacuzzi, spa.\n- Viking Heimdal cruise deals are inclusive of Internet (Wi-Fi), all tours/excursions, ground transfers, live evening entertainment, daily lecturers, food and wine tastings, 24-hour coffee/tea, replenished daily bottled water.\n- Note: To/from Viking Heimdal boat transfers and airport meet&greet services are included in the fares only when airfares are purchased through the Viking cruise line.\n- Viking Heimdal is one of the latest generation Viking Cruises longships. This is a new (patented) Viking river cruise boat design allowing bigger accommodations without the need to shrink public areas size and to compromise the vessel's general comfort.\n- The other Viking cruise ships in France are the longships Buri, Delling, Forseti, Hermod, and also Neptune, Pride and Spirit.\nCabins Types/Categories (number, location), high to low\n- Suites - ES (x2, deck3), AA (x7, deck 3)\n- Balcony Staterooms - A (x20, deck3), B (x17, deck2)\n- French Balcony Staterooms (with floor to ceiling door-windows) - C (x5, deck3), D (x15, deck2)\n- RiverView Staterooms - E (x18, deck1), F (x7, deck1).\nNote: You can see the CruiseMapper's list of all river cruise ships and riverboats in the \"itinerary\" section of our River Cruises hub. All companies and their fleets are listed there."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:de64ab66-ad1c-4762-8a64-7fa27d9dbdec>"],"error":null}
{"question":"How do Turkish Delight and Sutlu Nuriye differ in their sugar-based preparation methods? 🤔","answer":"Turkish Delight and Sutlu Nuriye have distinct sugar preparation methods. For Turkish Delight, sugar is combined with 1.5 cups of water and lemon juice, then boiled until reaching 240 degrees Fahrenheit without stirring for 45-60 minutes. In contrast, for Sutlu Nuriye, sugar is dissolved in 2 cups of water while stirring constantly, then simmered for just 10 minutes before adding milk to create a lighter, milky syrup.","context":["Vegan Candy Recipes: The amount of candies that are not vegan is honestly shocking. A lot of brands use animal-based colorings or texture enhancers to create more easily marketable products despite the ingredients not being entirely necessary. Vegan candies often seem to fall into a routine of being chocolate-based and if you want something a little different, vegan recipes can be few and far between. Let’s take a look at three delicious non-chocolate vegan candy recipes that are perfect for gifting or just making for yourself!\n1. Vegan Gummy Bears Recipes\nVegan gummies are one of the most satisfying sweets. They’re jiggly and delicious, offering a ton of flavor in a tiny bite. These gummies do require a bear mold but you can opt to go for a different shape based on what you have on hand or even pour the mixture directly onto a sheet pan to later cut into squares for a low-effort version.\nThis recipe is derived from this one from The Edgy Veg.\n- 1 cup apple juice\n- 2 tbsp liquid sweetener\n- 2 tbsp agar agar powder\n- Whatever fruit flavoring you want to use (more on this below)\nHow To Make\n- Blend the fruits you want to use as a flavoring with the apple juice until smooth. Typically, the ratio would be one serving of the fruit with the apple juice. So, one whole peeled orange for one cup of apple juice. A handful of berries for one cup of apple juice. So on and so forth.\n- Strain the blended mixture so no pulp remains or very little\n- In a saucepan combine the liquid sweetener and agar agar with the fruit liquid.\n- Bring this to a simmer and stir constantly to prevent any lumps from forming\n- Remove from the heat and allow to cool to a workable temperature\n- Use a dropper or spoon to place the liquid into the molds, trying to be as neat and clean and possible\n- Place in the freezer until fully set\n2. Saltwater Taffy Recipes\nA classic carnival treat, saltwater taffy is perhaps one of the most versitale and easily flavored options out there! Plus, it is super fun to make!\nThis recipe was inspired by this one from Buzzfeed’s Tasty.\n- 1 cup sugar\n- 1 tbsp corn starch\n- 1 tbsp butter alternative\n- ⅔ cup light corn syrup\n- 1 tsp salt\n- ½ cup water\n- 1 tsp vanilla extract\n- ½ tsp flavoring of choice\n- Food coloring\nHow To Make\n- Place the sugar into a large pot. Add the corn starch and whisk well until well combined.\n- Add the butter alternative, corn syrup, salt, water, and vanilla to the pot and mix well\n- Heat the mixture, stirring often to prevent burning, until it reaches around 250 degrees Fahrenheit\n- At this point, you can add the food coloring and mix to combine\n- Pour the mix into a heat-safe, well-greased dish.\n- Let it cool for around ten minutes or until you can safely touch it\n- Begin stretching the taffy. Stretch it to a length of twelve inches then fold it back over itself. Repeat for ten to fifteen minutes.\n- Once it becomes difficult to pull, roll it into a log and cut it into pieces.\n- Wrap with wax paper and store. Enjoy!\n3. Turkish Delight Recipes\nTurkish delight is a classic treat that is sweet and floral due to the inclusion of rosewater in the recipe. It is also conveniently vegan!\nThis is adapted from this recipe from The Spruce Eats.\n- 4 cups sugar\n- 4 ½ cups water\n- 2 tsp lemon juice\n- 1 tsp cream of tartar\n- 1 ¼ cup corn starch\n- 1 ½ tbsp rose water\n- 1 cup powdered sugar\nHow To Make\n- Line a glass pan with aluminum foil or parchment paper and spray well with non-stick spray. Set aside.\n- In a saucepan, combine the sugar, 1 ½ cups of the water, and lemon juice over medium heat. Stir until the sugar dissolves and the mixture boils\n- Allow the mixture to continue to boil without stirring until it reaches 240 degrees Fahrenheit. This can take 45 to 60 minutes. Watch it closely to prevent burning or spillage\n- In a separate saucepan that is larger, combine the rest of the water with the cornstarch and cream of tartar.\n- Heat over medium heat, stirring often until it thickens.\n- Remove the sugar mixture from the heat once it is to temperature and slowly add it into the cornstarch mixture, stirring well.\n- Reduce to low heat and simmer for about an hour, stirring every ten minutes or so.\n- Remove from the heat and add the rosewater and any food coloring you may want\n- Pour the candy into the greased pan and let it set, uncovered, for twenty-four hours.\n- Remove the candy from the pan\n- Slice into cubes on a well-greased or powdered sugar-coated work station\n- Toss in powdered sugar to prevent sticking\nThese vegan candy recipes are delicious, vegan, and definitely worth giving a shot if you have some extra time and a hankering for sweet treats!","Sutlu Nuriye is a delicious Turkish dessert; similar to baklava but lighter with its milk based syrup. Crushed hazelnuts are used in the filling here and works wonderfully with the milky syrup, which gives Sutlu Nuriye a whitish look. Sutlu Nuriye is lighter, creamer than baklava and really easy to make at home, using filo pastry sheets. They have been a huge hit with the children, as well as adults in our home, great for entertaining.\nSutlu Nuriye believed to be created due to the supply shortage in 1980s. Rather than the expensive pistachios, a baklava producer used hazelnuts and flavored with milk for lighter syrup. The result has been today’s popular Sutlu Nuriye, a delicious, lighter version of the regular baklava.\nYou can prepare Sutlu Nuriye a day ahead of time and keep it in a cool place; always serve at room temperature. I hope you enjoy this soft, light, melt-in-the mouth Sutlu Nuriye, a variation of baklava in milky syrup. Turkish coffee or Turkish tea, cay aside complements Sutlu Nuriye very well.\nMy very best wishes to you all for the festive season. Many thanks for your company, recreating my Turkish recipes at your homes, your kind share and comments, I greatly appreciate it. It’s been a pleasure enjoying Turkish cuisine with you all and I look forward to sharing many more recipes in the New Year. I wish you all a happy, healthy new year in good company and delicious food.\n- 270 gr x 2 packs of filo pastry sheets (12 filo sheets in total; each sheet 480 mm x 255 mm each)\n- 200 gr/4 oz. /a little less than 1 cup unsalted butter, melted\n- 340 gr/ 12 oz. chopped/crushed hazelnuts\n- For the syrup:\n- 16 fl. oz. / 2 cups water\n- 12 fl oz. / 1 ½ cup whole milk\n- 270 gr/ 1⅓ cup sugar\n- Preheat oven to 180 C/ 350 F\n- Take out the fresh filo pastry sheets from the fridge and bring to room temperature 20 minutes prior using. To thaw frozen filo sheets, it is best to place them in the fridge the night before or bring it to room temperature 2 hours before using.\n- Grease the baking dish with the melted butter.\n- Place two filo pastry sheets to the baking dish (trim the sheets at the edges if necessary to fit into your baking dish) and brush with the melted butter.\n- Place 2 more filo pastry sheets and brush with the melted butter. Place another two sheets over them and brush with melted butter.\n- Crush the hazelnuts in a food processor, carefully pulsing a just few times or chop by hand (take care for the hazelnuts not go too small pieces or fine).\n- Spread the chopped hazelnuts evenly on the 6th sheet of buttered filo pastry.\n- Lay two more sheets of filo pastry and brush with melted butter. Repeat this 2 more times, buttering every two sheets, until you reach 12th sheet.\n- Brush the 12th sheet of filo pastry with butter and ease the sheets into the corners and trim the edges if necessary.\n- Then using a sharp knife, cut right through all the layers to form small square pieces. It should make about 30 pieces in total.\n- Bake the pastry in the preheated oven (180 C/ 350 F) for 25 minutes, until golden at top.\n- While the pastry is baking, prepare your syrup.\n- Put the sugar into a heavy pan, pour in water and bring to the boil, stirring all the time. Once the sugar is dissolved, lower the heat and simmer for 10 minutes.\n- Pour in the milk to the pan, give a good stir to the syrup and turn the heat off. Leave the pan aside to cool down; the syrup needs to be luke warm to pour over cooked filo pastry.\n- Once the filo pastry is cooked and golden at top, take out of the oven and leave it aside to cool down for 15 minutes.\n- Slowly pour in the luke warm milky syrup over cooled cooked filo pastry and let the pastry to soak the milky syrup for 35- 40 minutes.\n- Once milky syrup is absorbed by the pastry, take out the Sutlu Nuriye squares and serve at room temperature.\n- You can prepare Sutlu Nuriye a day ahead of time and keep in a cool place, covered."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:6979d3d3-2782-4768-9a12-bde46eadaf9e>","<urn:uuid:396743af-6359-4054-9598-772f8daa09aa>"],"error":null}
{"question":"What is loudspeaker baffle step diffraction, and how does it impact the transition from high to low frequencies in speaker systems?","answer":"Baffle step diffraction occurs when a speaker transitions from radiating into half space at high frequencies to radiating into full space at low frequencies. At high frequencies, the speaker only radiates forward (half space), while at lower frequencies it radiates both forward and backward (full space). This transition results in a gradual 6 dB reduction in output from high to low frequencies, typically occurring between 1000-2000 Hz and continuing down to about 200 Hz. The center frequency of this transition depends on the baffle dimensions - smaller baffles have higher transition frequencies. This effect occurs because when wavelengths become long enough, the cabinet becomes too small to act on those waves, leading to a roll-off in SPL at lower frequencies.","context":["Understanding Baffle Step and Diffraction\nBaffle step and diffraction are two very important concepts to understand in speaker design. Most people who are in to performance audio have probably heard the terms before, but if you don’t really understand what they are or how they affect a speaker, this article is for you.\nWe’re going to start with a look back at something you probably learned in an early physics class, and that is how two waves interact with each other in different waves. If you look below at figure 1, we are displaying a graphical representation of a sine wave with the x-axis represented as time and the y-axis represented as amplitude (or output level). When a wave moves, it moves in both positive and negative cycles across the x-axis with a peak and a trough defined by the initial output amplitude and a period, or cycle time, defined by the frequency being generated. Cycle time is the length of time to go from the zero position on the y-axis, through both the positive and negative peaks, and back to zero. In this instance, let’s assume this is a 1 kHz sine wave. This would mean we have a cycle time (T=1/f or in this case T=1/1000) of 1 millisecond. This means it takes one millisecond for a 1 kHz wave to complete a full period or cycle.\nNow let’s add a second 1 kHz wave of exactly the same amplitude to the first and play them together at the same time. In figure 2, this situation is displayed. The red wave shown is laying directly over the top of the blue wave of figure 1 and together, they create the new wave shown in magenta. In this instance, the two waves are equal in both period and amplitude which creates a second wave of the same period but with double the amplitude (magenta line). This is called constructive interference. Constructive interference is when two waves add to each other’s amplitude to create a higher amplitude than the original.\nBut what if we started the second 1 kHz wave later? Let’s start it 0.5 millisecond later so that the second wave is perfectly out of phase (180 degrees) from the first. This means that the second wave’s positive peak is occurring at the first wave’s negative and vice versa. This produces the flat magenta line in figure 3. The two sine waves have cancelled each other out through destructive interference and now have no measurable amplitude.\nNow let’s look at what happens if we start the second wave only slightly out of phase. Let’s start the second wave only 0.25 milliseconds later (90 degrees out of phase). In this instance, we get a mix of both constructive and destructive interference which creates a new wave that has slightly more output ins some areas and slightly less in others. You can see from the graph that the peak of the wave is higher than either the blue or the red because both waves have positive values at this point and add to a higher amplitude. But if you look where the magenta line crosses the x-axis, you can see that one of the waves has some positive value and the other has some negative value of an opposite amount, which is why the magenta line goes to zero.\nIf you'd like to play with this yourself, you can check out this handy tool.\nGetting to the Point\nYou might be wondering what this has to do with diffraction and baffle step. Both diffraction and baffle step are technically part of the same phenomenon, but when people refer to baffle step, they are generally referring to the region when a wavelength gets long enough that it is no longer interacting with the front of the box. When you put a woofer in a box and measure it, you will see a gradual roll of in the low frequencies, and this is baffle step loss.\nWe have to cover a little bit more theory to really explain what is going on, so just hang in there. Let’s look at the image below, which represents a standard tweeter on a standard square-edged baffle. When a tweeter emits sound, it emanates as a complex spherical waveform that tries to radiate in all directions at once.\nWhen a wave encounters a sudden transition, like the edge of a baffle, it can cause the wave to scatter in all directions at once. That occurs when the physical length of the wave is approximately the same distance as the distance to the edge of the baffle. This causes some of the wave to be reflected back into the forward direction and interact with the waves that were emitting from the tweeter itself. This causes additions and subtractions to the original wave like described above.\nAn object must be sufficiently acoustically large before full diffraction can occur. So what is sufficiently acoustically large? An object has to be greater than ½ of the wavelength at whatever frequency you are looking at to be considered sufficiently acoustically large. Wavelengths above this value see full diffraction effects. Below half the wavelength, the diffraction effects get progressively smaller until about one tenth the wavelength. At that point the object essentially becomes acoustically invisible and has no measurable impact on the diffraction signature of the wave.\nAs the wavelengths get longer in nature, or in other words get lower in frequency, the cabinet becomes too small to act on those waves, and you start to have a roll-off in the SPL at those frequencies. Depending on cabinet size and shape, this usually starts between 1000-2000 Hz and continues down to about 200 Hz.\nThat together is diffraction and baffle step. And that is what takes a nice looking, flat factory response in shown in gray below to the black line on the graph. In the black line, we now have a small peak at 1500 Hz and a roll-off below so that we are a full 6 dB down at 200 Hz.\n2Pi versus 4Pi\nNow the question might be, why don’t we see that from graphs from a manufacturer? When you see a measurement posted by a manufacturer, it is typically the infinite baffle, or 2Pi response of the driver, also known as half space response. When we measure a driver in a cabinet, it is generally the 4Pi response. The terms 2Pi and 4Pi are referring to a coordinate system using radians on an x/y axis. This can also be done in degrees. If you were to lay a circle centered on both the x-axis and y-axis and trace around it’s perimeter, you would have gone 360 degrees or 4Pi when you get back to your starting point. When we refer to 2Pi, we are referring to half that space, or 180 degrees of change.\nWe’ll look first at 4Pi. If you assume that you have a point source (or a source emitting sound that is infinitely small) in free space, it has nothing to act on it which could cause any form of diffraction because the wave is free to radiate in all directions at once. But if we place that point source on an infinitely large wall, you get something a bit different, and that is the 2Pi interaction. The large wall reflects acoustic energy, which wants to radiate in all directions. This contains the wave to one side of the wall (or 180 degrees). This effectively doubles the output in the forward direction and adds an additional 6 dB to the perceived response of the point source.\nFinally… the point\nWhen you change the wall to something the size of a typical baffle, you end up with something in between. At higher frequencies that see the baffle as sufficiently acoustically large, you end up with a 6 dB increase in output. At lower frequencies as you go through the transition region, you see a gradual reduction of output until the baffle becomes acoustically transparent and you are at a full 6 dB of reduction. And in that transition region at the edge of the baffle, you get that scattering effect described earlier which causes a series of peaks and dips. The severity of that is dictated by the size of the driver and the shape of the edge.\nSmaller drivers radiate higher frequencies better off axis, so more of the energy reaches the edges and cause diffraction artifacts in the form of peaks and dips in the on-axis response. Larger drivers see less of this because the high frequencies don’t radiate as strongly off axis, so there is not as much interference on-axis.\nI also want to mention is that there are several online calculators available to calculate a baffle step circuit for a speaker. In reality, good designers never use a separate baffle step circuit and the circuit used generally doesn’t even look similar to those online calculators. Good designers incorporate the compensation into the shaped response of the driver at the crossover with more typical crossover circuits, and you can see this in the crossovers we design if you look at the circuit as well.\nFinally, if you want to simulate baffle step and diffraction of a speaker, you don't even need to know all the math behind how it works. There are hand tools for free online like Jeff Bagby's Response Modeler, which allow you to simulate it by putting in your cabinet dimensions and loading in a driver's .frd file.","Technical Discussions of Audio and Loudspeaker Issues\nTopic No. 2\nLoudspeaker Diffraction Loss\nLoudspeaker enclosure \"diffraction loss\" occurs in the low frequency range of loudspeakers in enclosures that are located in the open, away from walls or other surfaces. The essence of it is this: At high frequencies the speaker is radiating into \"half space\" i.e. it is only radiating into the forward hemisphere. No significant energy is radiated to the rear of the speaker. At low frequencies the speaker is radiating into both the forward hemisphere and the rear hemisphere. That is, at low frequencies the speaker radiates into \"full space\". Because the \"energy density\" at low frequencies is reduced there is a loss of bass. In short, speaker systems designed for radiation into half space (mounted flush on an infinite plane) exhibit a loss of bass when implemented in typical speaker enclosures. Fortunately, this bass loss can be accurately modeled and subsequently compensated.\nMost loudspeaker modeling is performed based on the assumption of radiation into half space. A speaker radiating into half space plays 6 dB louder than the same speaker radiating into full space. This is the crux of the diffraction loss. A full range speaker finds itself radiating into half space at the upper frequencies but radiating into full space at lower frequencies. As a result, there is a gradual shift of -6dB from the highs to the lows. This is what is called the \"6 dB baffle step\" or the enclosures \"diffraction loss\". The center frequency of the transition is dependent on the dimensions of the baffle. The smaller the baffle the higher the transition frequency.\nThe shape of the diffraction loss frequency response curve depends on the size and shape of the enclosure. Olson has carefully documented the diffraction loss of enclosures of various shapes (see references below). All enclosure shapes exhibit a basic 6 dB transition (or \"step\") in the response with the bass ending up 6 dB below the treble. A spherical enclosure exhibits this transition clearly with a very smooth diffraction loss curve. In the curves below I have taken the liberty of extending the frequency range of Olson's original graphs from 100 Hz to 20 Hz at the low end and from 4kHz to 5 kHz at the high end. The low frequency response was extended to more clearly reveal the \"stepped\" nature of the response. I wanted it to be clear that the response levels off at the low end. Olson's own reproductions of the measured diffraction loss of a sphere by Muller, Black, and Davis tend to confirm that my extensions to the responses are correct.\nMore \"angular\" enclosures exhibit the underlying 6 dB step along with a series of response ripples that are dependent on the placement of the speaker with respect to the baffle edges. The worst case appears to be placing the driver at the center of a circular baffle so that it is the same distance from all diffracting edges.\nPlacing the driver on the baffle so that it is a different distance from each edge tends to minimize the response ripples and make the diffraction loss look more like the smooth loss of the sphere. Olson's rectangular enclosure is an improvement over the cube and the cylinder face but the driver is still equidistant from three edges. Other authors report further reduction in the ripples with careful driver placement and edge rounding.\nBecause the spherical diffraction loss is a common element for the diffraction of all enclosures and the response ripples are much more difficult to predict (and can be minimized anyway) it makes sense to approximate the diffraction loss of a loudspeaker as the diffraction loss of the equivalent sphere.\nOne simple electrical circuit which produces a 6 dB step reduction in the bass response is shown below.\nIf we let R1 = R2 = R then a 6 dB attenuation results at low frequencies. At higher frequencies (where C1 becomes a low impedance) the attenuator is effectively bypassed and the signal is passed without attenuation.\nIt can be shown that the 3 dB \"center\" frequency for the above network is given by:\nThe frequency response of diffraction modeling network typically looks like this:\nCareful inspection of Olson's spherical diffraction loss curve reveals a -3dB frequency of about 190 Hz for the 24\" sphere. Assuming that the 3 dB frequency is inversely proportional to the baffle diameter I have arrived at the following approximation for calculating the -3dB frequency as a function of baffle diameter.\nOnce the diffraction loss is known it is possible to design a simple electrical network that will exactly mirror the spherical diffraction loss and restore the lost bass to a speaker system. Loudspeaker designers have traditionally compensated for the diffraction loss by reducing the level of the tweeter and making other adjustments in the crossover. The method I propose is to design for half space but then do a precise mirror image compensation for the diffraction loss by way of an R-L network wired in series with the (impedance compensated) speaker. Alternately, the diffraction loss can be compensated at line level with a simple R-R-C network. Line level correction would reduce the requirement for the large inductors typically needed for a speaker level compensation network.\nA simple electrical network which produces a 6 dB step reduction in the treble response is shown below.\nHere R2 represents the loudspeaker load impedance. If we let R1 = R2 = R then a 6 dB attenuation results at high frequencies. At lower frequencies (where L1 becomes a low impedance) the attenuator is effectively bypassed and the signal is passed to the driver (R2 here) without attenuation.\nThe frequency response of 6 dB diffraction compensation network looks like this:\nTo design an RL network which will compensate for diffraction loss of a particular system we start by setting:\nNext, you can use my empirically derived equation to calculate the value of the inductor L1:\nI arrived at this equation for L1 by forcing the 3 dB frequency of the compensating network to match the 3 dB frequency for the diffraction loss of the baffle.\nThe resulting RL network should be wired in series with the speaker system it is compensating. The correction will be most accurate if the loudspeaker itself approximates a resistive load.\nWinSpeakerz models the diffraction loss of the enclosure as a simple spherical diffraction loss. Provided the driver is located \"irregularly\" on the baffle this gives very good approximation to the actual diffraction loss of the enclosure. The frequency of the transition is controlled by the \"Baffle Width\" parameter at the System Editor page 1. The response of the speaker can be viewed with or without the diffraction loss and the diffraction loss can also be viewed separately.\n25Jun99 A follow-up post on this topic:\nbeen researching the idea of adding a baffle step compensation circuit\nThe 6 dB loss is correct for a speaker enclosure in free space. When the enclosure is placed in a room it will encounter various effects due to the room (reverb, standing waves, boundary effect, cavity effect . . .)\nDiffraction loss and room effects are independent and completely different effects. The diffraction loss is nicely predictable whereas the effects of the room are highly variable, not only from room to room but also with speaker placement and room furnishing. This typically means that each listening environment will be unique and will require unique compensation.\nI suggest the 6 dB diffraction loss correction as a correction for the diffraction loss alone. I don't suggest that it will neutralize all the effects due to a unique listening environment. Others suggest you \"deal with diffraction\" in the crossover, usually by just lowering the tweeter level a bit.\nIn some situations 3 or 4 dB of diffraction loss correction may result in an overall response that is closer to neutral (flat). But the most correct way to compensate the room would be to do it separately from any diffraction loss correction. Room compensation might take the form of several notch filters tuned to the worst peaks resulting from room modes. Next you might want to tilt the treble up a smidge to compensate for reverberation that has significant treble loss. Dark room reverb will make the playback sound a little darker. Bright reverb . . . bright. Next, depending on the size of the room and speaker response, you might need to compensate for the cavity effect. In larger rooms cavity effect can be ignored but in vehicle cabins it is a major effect.\nDiffraction loss compensation is only part of the job of precisely compensating for the difference between a theoretical half-space acoustic load and what happens when we place an enclosure in a real world listening room. Reducing the degree of diffraction loss compensation MAY reduce the coloration from the room effects as these effects largely tend to \"boost the bass\" but such an adjustment is imprecise at best.\nIf we can systematically identify each source of color between our half space model and our particular listening room then we can then take steps to precisely neutralize the response in our own listening room. Spherical diffraction compensation is one effect we can correct with a high degree of precision. As we move toward a better understanding and modeling of our listening rooms I'm sure we will work out more practical and precise ways to compensate our rooms.\nComments and critique are welcome. :-)\nLoudspeaker Diffraction Technical References\nWright, J. R.\nRasmussen, Soren and Rasmussen, Karsten Bo\nGonzalez, Ralph E.\nPorter, James, and Geddes, Earl\nBews, R. M., and Hawksford, M. J.\nKral, Robert C.\nOlson, H. F.\nMuller, G.G., Black, R., and Davis, T. E.\nAudio Home Page | Catalog | Tech\nTopics | Audio Links | Book\nYour comments are welcome at firstname.lastname@example.org"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:fbb2937c-435c-47dd-988f-1c708536ee9e>","<urn:uuid:32abda9b-c1df-486a-979b-bd04dc4a1a40>"],"error":null}
{"question":"How does sand particle shape affect stability in turfgrass surfaces, and what role does water content play in surface strength?","answer":"Sand particle shape significantly impacts stability - rounded particles are least stable due to lower friction resistance, while angular particles provide the most stability but can be abrasive to turfgrass. As for water content's role, soil strength initially increases with moisture but then rapidly decreases as moisture content continues to rise. The relationship between soil strength and moisture depends on bulk density (how well packed the soil is) - higher bulk density increases strength but reduces porosity, which affects both drainage and air availability for turfgrass growth. A balance is needed between providing sufficient strength while maintaining adequate porosity for plant health.","context":["and then trying to walk on them. Being rounded and of similar size leads to an unstable footing. If the marbles were broken (angular) and of different sizes they would fit , or lock, together and produce a more stable surface. Sands complying with USGA specifications do not necessarily produce a stable surface.\nSand particle shape has also been included and discussed in the USGA specifications primarily because of the stability, or lack of, associated with shape. Rounded particles are the least stable and very angular particles are the most stable because of the differences in friction resistance associated with those shapes. Although, in\ntopdressing applications angular particles can be extremely abrasive to the turfgrass and\nphysiological stress. turfgrass manager is\nRounded particles are less abrasive but are to obtain the sand material with the optimum\nalso less stable. characteristics.\ncause considerable the struggles of the\nProducts have been developed and added to soil in an attempt to increase stability (Adams et. aI, 1989; Beard et. aI, 1988; Gibbs, 1990). Meshed products and many kinds of fibers have been added with what I think is limited effectiveness. Root systems (Reid et. aI, 1982) of the turfgrass plant (if there is one) behave in this way and act somewhat like reinforcing rod in concrete to increase strength. In fact, many types of fibers have been added to concrete to increase the strength with some success.\nThe objective of this research was to characterize commonly used sands and mixtures using routine soil mechanics procedures. Conventional laboratory testing was used to measure frictional resistance and geotechnical modeling was used to predict behavior of the sands in this study.\nMETHODS AND MATERIALS\nFour sands were selected based on their use and particle-size distributions. The sands varied from 2NS, a sand mixture specified by the Michigan Department of Transportation, to TDS 2150, a sand quarried from the dune deposits along Lake Michigan near Grand Haven, Michigan. The characterization data for these sands are\nincluded in Appendix A. To predict bearing capacity of these sand materials a computer algorithm ofTerzaghi's\nequation was used. The variables used to study the changes in bearing capacity were angle of internal friction, inclination, and water content (matric potential).\nComparing the sieve results in Appendix A with the USGA recommendations indicates the 2NS and Mortar sands do not fit the recommendations. Both sands do not have the required 60 % in the Medium and Coarse sand fractions while the 2NS also has more than the maximum of 10% above 1.0 mm in size. These two sands are\ntoo well-graded (wide distribution) to fit the USGA specifications. The other two sands selected, Michigan #40 and TDS 2150, fit within the USGA specifications. are uniformly graded and similar in particle-size distribution.\nFrom an engineering perspective, the 2NS and the mortar sand are generally found to be the better materials when considering them for use in construction. This because as the particle-sizes that make up a sand become more widely distributed the sand becomes more stable. Greater stability results from the smaller sized particles filling the voids of the larger sized particles. The problem that arises when considering well-graded sands for putting greens or athletic fields is the relationship between gradation and porosity. As the sand becomes more well-graded it becomes less porous reducing the hydraulic conductivity and slowing drainage.\nMichigan #40 and TDS 2150 sands theoretically do not have the problem of low porosity because they are poorly-graded samples that fall within the USGA specifications. But, these sands have lower strength and less stability because of the uniform distributions.\nThe difficulty in the design of the sand mixture arises because strength is inversely related to porosity and the greater the porosity, generally the better the environment for turf grass growth. The success of the turfgrass depends upon its ability to flourish in the sand mixture, while at the same time the stability of the sand mixture depends, in part, on a well established rootzone system ..\nWe have begun to quantifY the relationships between sand-size distribution, angularity, pore-size distribution, soil strength and compressibility. It is already known, at least qualitatively, the wider the distribution of sand, the greater the stability. With further research we hope to quantifY these relationships in the form of improved material specifications to give turf grass managers better tools to make sand selections.","1 What causes land drainage problems – the principles of water movement in soils\nWhilst the effect of a functioning land drainage scheme can be observed by all, a basic understanding of the physical principles which govern this process enables the observer to both assess the effectiveness of a land-drainage scheme and also to appreciate why inappropriately or poorly designed drainage schemes fail to work.\nFigure 1 A diagram depicting a microscopic view of soil\nThe key to all of the following discussion is to understand what makes up a rootzone. The key ingredients are:\n1.Mineral particles (sand,silt,clay)\n4. Organic matter\nConsider for example the soil rootzone represented in Figure 1, mineral and organic particles are locked together to form the solid fraction of the soil, whilst the pores (the spaces between the solid particles) are occupied by either soil water or soil air. The combination of solids, water and air is critical for not only plant growth, turfgrasses need all three components, but also surface strength.\n1.1 Sports surface strength and soil moisture content\nThe resistance of a football pitch to wear by players and the ability of a pitch to provide a stable platform for quality football is highly dependent upon the turfgrass and the soil in which it grows. The grass plant provides a cushioned surface, with natural lubrication to reduce skin abrasion – its root network also provides a core structural function. The soil not only provides an anchorage for the grass plant but it is also the key load bearing component. The two components combined (grass and soil), must be of sufficient strength to resist damage from repeated use – both from running and sliding, but not of excessive strength so as to cause impact or musculoskeletal damage.\nFigure 2 The effect of soil moisture content and bulk density on rootzone strength. As moisture content increases, surface strength initially increases but then rapidly decreases. As bulk density increases from Bulk density 1 to Bulk density 2, the rootzone strength increases\nThe strength of a turfgrass plant is a function of agronomic factors related to plant health and growing environment. The strength of a natural soil or sand rootzone is a question of engineering and is closely related to the bulk density of the soil (how well packed the soil is) and its moisture content (how wet the soil is). Typically, as bulk density increases – soil strength increases and as moisture content increases – soil strength decreases (see Figure 2).\nThe principal target for groundstaff is to prepare surfaces that have a sufficiently high bulk density so as to provide strength to resist wear. The problem is that as bulk density increases, porosity (the amount of pores in a soil) decreases and restricts the amount of air in the soil available for the turfgrass; a balance is required.\n1.2 Water movement and retention in soils\nAs is shown in Figure 1, water occupies the pore space in a soil; it also illustrates the range of pore sizes in a soil. Water is held in a pore by capillary action, effectively sucking water into the pore – this is the same way in which a sponge absorbs a liquid. Large pores create a low amount of suction; small pores create high amounts of suction – therefore the size of pores is critical in water movement in soils.\nFigure 3 Unsaturated (a) and saturated (b) rootzones. In the saturated condition all the pores are full of water\nAll the water in a soil is being pulled downwards by gravity, large pores will drain because there is not enough suction to hold the water in the pore against gravity, meanwhile small pores, where the suction is greater than the pull of gravity do not drain. This is why sandy soils, where the particles are large, and hence the pores are large, drain easily under gravity, but clay soils, where the particles and pores are very small will hardly drain under gravity and require land drainage schemes. In fact in clay soils, it is very difficult to pull water out – it must be pushed out, this happens when the soil is saturated (see Figure 3).\nThe same physical property also means that water is pulled upwards (and sideways) to occupy dry pores – a phenomenon known as capillary rise.\nSaturation in a soil is defined as when all the pores are full of water. Note that any water applied to the top of the soil in Figure 3b will displace water out of the bottom of the soil by pushing it out as a head of water builds (see Figure 4a). The rate at which the water moves through and out of the soil is known as the saturated hydraulic conductivity. In a coarse grained soil such as a sand rootzone, the hydraulic conductivity is high; in finer clay soils, the hydraulic conductivity is orders of magnitude lower.\nFigure 4 Flooding due to high water table; (a) water added to the top of the saturated soil displaces water from the freely draining base of the profile; (b) when the water table is high, water cannot flow out of the soil and accumulates at the surface – flooding the pitch\nIf there is no exit for the water below (e.g. the water table lies directly below the soil) then water will accumulate at the surface, water logging and eventually flooding the pitch (Figure 4b). This situation is known as a ground water drainage problem and can be solved by lowering the water table with piped drainage so that the soil has a greater storage capacity for rain water (Figure 5).\nSuch groundwater problems in sports surfaces are actually quite rare. The majority of waterlogging and flooding of football pitches is actually caused by another problem entirely – requiring a different solution.\n1.3 Infiltration and surface drainage problems\nFlow of water in and out of the soil in Figure 4a assumes that water can get into the top of the soil in the first place. The entry of water into the top of a soil is termed infiltration. Typical infiltration rates for different soils and rootzones used in sports surfaces are shown in Figure 6. The difference between a high sand content rootzone, such as a 70/30 mix, and a compacted clay is of several orders of magnitude.\nFigure 5 The effect on a groundwater table of the installation of piped drainage. Note that at the drain locality, the water table is lowered, increasing the capacity of the soil to store rainwater and reducing the frequency of flooding. Note also, how the water table rises in-between the drains – the height to which it rises is a function of the depth of drains and their spacing. If the drains are too shallow or too far apart, they can actually cause flooding – a common indication of poor drainage design\nThe infiltration rate (how quickly water enters the soil) is a function of many factors but is governed by the pore space in the soil – if the soil has an open structure at the surface water can flow into the soil easily; if the soil is capped or has small pores, the infiltration rate is reduced (Figure 7).\nLow infiltration rate in sports surfaces is caused by:\n· small pore sizes (eg. clay soils)\n· compaction (wheeled traffic)\n· capping (silts or sliding feet)\n· some types of organic matter, including thatch\n· or a combination of all of these.\nThe inability to get water into a sports surface is one of the most common land drainage issues and is known as a surface drainage problem.\nFigure 6 Typical infiltration rates for different soil media\nConsideration of the above factors shows why clay soils are so susceptible to this problem – they have very small pores and the surface is easily smeared – providing a water tight seal over the soil. Installation of groundwater drains such as in Figure 5 will not solve this problem, the water cannot even get into the soil, let alone exit through the drains. If waterlogged and flooded pitches are to be avoided then it is necessary to bypass this impermeable surface completely using a surface drainage (or bypass) system. A surface drainage system uses bands of higher hydraulic conductivity / infiltration rate material to allow precipitation to bypass the low conductivity soil, straight from the surface to a piped drainage system. Such a scheme is detailed in Figure 8; the low conductivity soil (such as a heavy clay) has a very low infiltration rate and low hydraulic conductivity.\nFigure 7 The effect of particle size on infiltration rate: (a) coarse particles with wide pores allows water to infiltrate easily; (b) smaller particles with smaller pores restrict or prevent infiltration\nIn this system, some of the water from precipitation will pass slowly into the low conductivity soil – providing water to the grass plant. The majority of water, however, will flow across the surface and through the topdressing layer, into the vertical sand slits and then down into the collector drain and out to an outfall – completely bypassing the slowly permeable soil.\nThe system requires two key components for it to be effective:\n1. A high hydraulic conductivity connection to the surface – if a vertical slit becomes capped with fine textured soil, water cannot flow into the system and it will be redundant. For this reason it is necessary to provide frequent topdressing and careful devoting of the surface.\n2. An adequate collector drain network and outfall – if the water is not taken away, the material will become saturated rapidly and the system will not function.\n1.4 Review of the key principles in selecting land drainage design\nFor the design of effective drainage in natural turf pitches the following points must be considered:\nFigure 8 A cross section and plan view of a sand slit system as an example of a surface or bypass drainage system for a low conductivity soil football pitch\n1. An investigation to determine whether the problem is caused by a water table (groundwater drainage) or a low infiltration rate (surface drainage).\n2. An investigation to determine the physical properties of the native soil.\n3. Determination of the correct depth and spacing for any piped drainage infrastructure.\n4. Calculation of the correct capacity for any infrastructure.\n5. Selection of hydraulically compatible and appropriate materials.\n6. Connection to free flowing outfall.\n7. Provision for hydraulic connection to the surface for bypass systems\nToo many land drainage schemes fail because these points have not been addressed. The system must be designed to solve the problem in the field. As discussed above, the most common drainage problem is that of surface drainage in fine textured (clay) soils.\n2 Sand Slit Drainage\nIf designed correctly and well maintained, sand slit drainage systems, such as the one illustrated in Figure 8, will reduce the risk of flooding and fixture cancellation.\nThe benefit of sand slit drainage systems A typical system of 1 m spaced sand slits (350 m deep) over a perpendicular lateral pipe network of 80 mm diameter drainage pipe (at a spacing of ~ 6 m and depth of 0.6 m) will provide sufficient drainage to prevent waterlogging in all but the most extreme conditions; with the proviso that such a system is regularly topdressed with a compatible sand. Over a period of years, prevention of revenue loss from fixture cancellation will offset the cost of installation and maintenance of a sand slit system. Less tangible benefits might include, improved reputation, the opportunity to stage higher profile fixtures – such as county tournaments etc, and improved training and youth development.\nThe sand slit system is highly effective and can be installed at a lower cost than a sand carpet system, where the complete pitch area is excavated and replaced with a purchased high sand content rootzone. The ongoing maintenance of sand slits is relatively cheaper and more straightforward than a sand carpet system too.\nInvestment in a sand slit and pipe drainage system that is regularly top-dressed will provide an effective land drainage scheme that will provide a direct, significant improvement to any facility where it is installed. As such, funding bodies can feel reassured that sand slit and pipe drainage will provide an effective return on investment (again, with the proviso that the facility in receipt has sufficient resources to both top up and topdress the system).\n3 Mole drainage\n3.1 The principles of mole drainage\nMole drainage is achieved by pulling a bullet-shaped plough through the soil to create a contiguous channel at depth. This channel provides a high hydraulic conductivity bypass conduit for water flow. The Cranfield University Mole Plough is shown in Figure 9. It is pulled through the soil at a depth of at least 400 mm, with a slight grade to encourage water flow.\nAs the plough is pulled through the soil, at approximately 1-2 m spacing, a vertical leg slot is formed, in addition a number of cracks are formed from the foot of the plough up to the surface as the soil is disturbed. These cracks form the principal bypass for water flow, connecting the surface to the mole channel (Figure 10). In agricultural, mole drainage surface heave is encouraged to help loosen the soil. In the drainage of sports surfaces, however, this is undesirable as surface disturbance upsets ball roll and can be a trip hazard.\nThere is a limited range of soil types that are suitable for mole drainage – essentially heavy, non-dispersive clay soils. The mole channel is cut by the foot, but formed by the expander. The expander forces displaced soil into the walls of the channel. In a sandy soil, the soil would simply collapse following the expander; in a clay soil, the soil is smeared and has sufficient cohesive strength to hold the channel open. As the channel dries it sets to form a hard walled channel, which can last anything from to 10 years.\n3.2 The benefit of mole drainage in sports fields\nThe benefit of this approach to drainage is cost – there are no materials required. The principal costs are labour, time, fuel and wearing of parts. There are reduced costs for drainage pipe and backfill material if a lateral collector system is required. As discussed above, however, the drainage scheme must be both low cost and drainage effective.\n3.3 The cost of mole drainage in sports fields\nA critical factor in the drainage of sports surfaces is the time taken to return to play following completion of works. With mole drainage there are three principal concerns:\n1. Surface disturbance\n2. Dangerous leg crack widths\n3. Durability and collapse of moles\nSurface disturbance is caused by incorrect mole plough design, technique and soil conditions. Soil conditions are also responsible for short-lived moles too. Dangerous leg crack widths are a function of a soil property that affects clay soils and any drainage scheme. Due to the complex mineralogy of clay soils they exhibit shrink and swell properties. As the clay particles get wet they expand, as they dry out they shrink. This property causes the leg slot crack caused by the installation of the mole to expand as the soil dries through the summer – this expansion can reach a critical width whereby the risk of ankle joint or tendon injury is too high and fixtures are cancelled until the soil becomes wet and swells, closing the crack.\nCareful consideration of the factors must be taken into account when installing mole drains. The critical requirements are the operating parameters – such as depth of installation, soil moisture content at installation and post installation management.\nThe article is an extract from the final report of a Football Foundation funded research project entitled The physical and financial benefits of mole drainage as an alternative to sand slitting in slowly permeable soils.\nThe research was conducted by:\nContact and principal author: Dr Iain James\nCranfield Centre for Sports Surfaces\nSchool of Applied Sciences, Building 42a\nBedford MK43 0AL\nTel. No: +44 (0)1234 750111 ext 2736\nManagement Systems Ltd\nContact: Dr Richard Earl\nUnit 1, Highfield Parc\nBedford MK43 7TA\nTel. No: +44 (0)1234 821750\nShould you have any further questions please do not hesitate to contact us"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:23ea6ba7-3b30-40bf-9ce7-3fa18253cc03>","<urn:uuid:67557ce9-d7ef-4963-8eb3-8f42118aa552>"],"error":null}
{"question":"What is the nutritional fiber content of this Cuban rice and black beans recipe?","answer":"This recipe contains 8 grams of fiber, which provides 33% of the recommended daily value.","context":["Editor's note: Chef, nutritionist, and cooking teacher Lourdes Castro shared this recipe from her cookbook, Latin Grilling. It's part of a festive Cuban party menu she created for Epicurious.\nThis dish gets its name from the wars between the dark-skinned Moors (moros) and the lightskinned Spaniards (cristianos) that occurred during the eighth century in Spain. It's a very popular Cuban dish, and it's perfect for outdoor entertaining, as it can be made well in advance and be served at room temperature.\n- Serves 8 to 10\n- 4 cloves garlic, peeled\n- 3 teaspoons salt\n- 1/4 pound bacon (about 6 strips), chopped\n- 2 tablespoons olive oil\n- 1 onion, finely chopped (about 1 cup)\n- 1 green pepper, seeded and finely chopped (about 3/4 cup)\n- 1 bay leaf\n- 1/4 teaspoon ground cumin\n- 1/2 teaspoon dried oregano\n- 1 1/2 cups long-grain white rice\n- 2 (15 1/2-ounce) cans black beans, not drained\n- 1 3/4 cups water\n- 1 tablespoon red wine vinegar\n- Mash the garlic and render the bacon fat\n- Put the garlic on a cutting board and sprinkle 1 teaspoon of salt over the cloves, let it sit for a few minutes, and mince it into a paste with a knife. Set aside.\n- Place the bacon and olive oil in a large pot and set it over medium-high heat. Sauté the bacon until it renders its fat and turns a golden brown color, about 6 minutes. Move the bacon around as it's cooking to prevent it from sticking to the bottom of the pot.\n- Sauté the vegetables and rice\n- Add the onion, green pepper, and garlic paste to the bacon and sauté until the vegetables are limp and translucent, about 5 minutes. Add the remaining 2 teaspoons of salt, the bay leaf, cumin, oregano, and rice and stir for 1 minute until well mixed and all the rice is coated in oil.\n- Add the beans, simmer, and serve\n- Add the beans and their liquid, along with the water and vinegar, to the pot. Cover and bring to a boil, then reduce to a simmer. Cook for 35 to 40 minutes, or until all the water has been absorbed by the rice. Allow the covered pot to sit off the heat for 5 minutes. Fluff the rice with a fork and serve.\nCanned black beans\nIf you do not want to use the liquid from the canned black beans, just add an extra 1/2 cup of water with the drained and rinsed black beans. Technique\nCooking the rice\nRice requires a specific amount of liquid to cook properly. Because onions and green peppers can contribute a considerable amount of liquid to a recipe, a volume measure for each is given. While the measurements are approximate, making sure the chopped vegetables are close to these amounts will ensure that the rice cooks properly. Advance preparation\nYou can prepare the recipe in its entirety the night before with very little effect on the taste and texture of the dish. However, you will want to warm the dish before serving. This can be done in the microwave or on the stovetop. Just sprinkle about 1/4 cup of water over the rice to make sure it does not dry out when reheated.\nReprinted with permission from Latin Grilling by Lourdes Castro, © 2011 Ten Speed Press, a division of Random House, Inc.\nA Miami native, Lourdes Castro has served as a personal chef and nutritionist for high-profile clients, such as Cristina Saralegui and professional athletes, and as an associate of the James Beard Foundation. A highly regarded Latin chef and cooking teacher, Castro is the author of Simply Mexican and Eat, Drink, Think in Spanish. She currently teaches food science at New York University and is the director of the Culinary Academy at the Biltmore Hotel in Coral Gables, Florida. Find out more at Lourdescastro.com\n- Carbohydrates51 g(17%)\n- Fat10 g(15%)\n- Protein11 g(23%)\n- Saturated Fat3 g(13%)\n- Sodium1122 mg(47%)\n- Polyunsaturated Fat1 g\n- Fiber8 g(33%)\n- Monounsaturated Fat5 g\n- Cholesterol9 mg(3%)"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:4de77b97-1d6b-42bb-8d10-e3889fa96888>"],"error":null}
{"question":"How does the treatment of legacy pollution sites compare between the 1970s environmental laws and the 2020 water protection changes?","answer":"The environmental regulations of the 1970s, particularly after removing ex post facto provisions, exposed companies operating on legacy sites to lawsuits for existing environmental degradation. This meant companies could be held accountable for historical contamination, as demonstrated in cases like the Woburn Toxic Trial. In contrast, the 2020 water protection changes create a system where many water bodies are completely excluded from protection, making it impossible to address legacy pollution in these waters. The new regulation prevents controlling discharges of water pollution at their source, making it difficult to identify pollution sources or clean up downstream waters. This represents a fundamental shift from holding companies accountable for past contamination to completely removing protection for many water bodies.","context":["Are Companies Responsible for Pollution by Corporate Predecessors?\nOne of the arguments posed by the defense in the Woburn Toxic Trial was whether Beatrice Foods, Inc. and W.R. Grace & Co. were liable for environmental damages, when the Aberjona River valley was historically home to industries known for their negative environmental impacts. The defendants also questioned being singled out by the plaintiffs because of their \"deep pockets\", while other industries near municipal wells G and H that were less able to pay damages were not included in the complaint.\nHistory of Manufacturing in Woburn\nWoburn was one of the first heavy manufacturing areas in the nascent United States. Shortly after the completion of the Merrimack Canal connecting the town with Boston, leather processing became a core business of the Woburn area (Tarr, 1987 (Acrobat (PDF) 3.5MB Jun18 09)). By the mid 1800s Woburn was one of the leading leather producing centers in the country. For more details, see Leather Tanning under Key Issues in the Trial.\nAs the leather industry expanded, production of the specialty chemicals needed by leather manufacturers concentrated in the Woburn area. By World War I, Woburn had a significant chemical industry located north of town in the Aberjona River valley. The industry, which began to support leather processing, soon began manufacturing stock chemicals and finished products such as pesticides, fertilizers, and plastics. A history of the Woburn chemical industry can be found at Chemical Companies in Woburn under Key Issues in the Trial.\nImpact of Environmental Regulations\nPrior to the 1960s there were minimal environmental regulations governing the manufacture and production of hazardous chemicals. The national awareness of the risks from improper handling of hazardous chemicals came to the forefront of of the media in the 1960s. A major hurdle in developing regulations involved identifying the responsible parties. A significant difference between the environmental regulations developed in the late 1970s and those placed enforced in the 1980s was removal of ex post facto provisions, which exposed companies operating on legacy sites to lawsuits for the environmental degradation that existed. A list of some of the landmark environment regulations enacted in the 1960s and 1970s includes:\n- Resource Conservation and Recovery Act - 1976\n- Clean Water Act - 1977\n- Comprehensive Environmental Response and Liability Act - 1980\n- Water Quality Act - 1987\nCorporate Responsibility and the Woburn Toxic Trial\n\"Making companies pay\" was a rallying cry from the plaintiffs' attorneys during the Woburn Toxic Trial. This mantra implied that Beatrice Foods and W.R. Grace, and by analogy any large corporation, who should be accountable for the environmental damage they created. At that time, following on the heels of the environmental damages at Love Canal, New York, and Times Beach, Missouri, there was overwhelming public sentiment agreeing that corporations must be responsible for their impacts to the environment. The defense demonstrated how widespread industrial contamination was in the Aberjona River watershed. From this perspective, the defense questioned why they should be responsible for what was considered normal prior to enactment of Resource Conservation and Recovery Act.\nCorporate Responsibility and a Mock Trial\nThe verdict in the Woburn Toxic Trial demonstrates the difficulty in connecting the practices of Beatrice Foods and W.R. Grace to the health problems of Woburn's citizens. Questioning the corporate responsibility companies operating facilities at the source locations of the TCE and PCE contamination can be used as an argument during a mock trial. A consideration would be focusing on the short time that Beatrice actually owned by Riley Tannery, and Beatrice's responsibility for the overall contamination on the Riley property (i.e. is it fair to a new corporate owner to absorb such a cost when its contribution may have been minimal).","By Kelly Foster, Waterkeeper Alliance\nReversing more than 40 years of progress and settled law, on January 23, 2020, the Environmental Protection Agency and the Army Corps announced a new regulation that removes clean water protections against discharges of pollution into rivers, lakes, streams, and wetlands across the country.\nWhile the Clean Water Act is focused on protecting drinking water, fisheries, recreation and other important uses of our waters, the new regulation is focused only on protecting large, commercially navigable waters, the territorial seas, and a small subset of the waters that feed into them.\nThe regulation is a definition of “waters of the United States” under the Clean Water Act, and it determines whether toxic chemicals, radioactive waste, sewage, and other pollutants can be dumped into rivers, streams, lakes, and wetlands. Waters that meet the definition of “waters of the United States” are protected. Waters that fall outside it aren’t. That means the pollution prohibitions and controls designed to protect people and water quality simply do not apply — allowing uncontrolled discharges directly into the water.\nBecause everyone understands that clean water is vital to public health and wildlife, and pollution moves downstream, the Clean Water Act’s regulatory definition of “waters of the United States” has long protected traditionally navigable waters, territorial seas, interstate waters, and intrastate rivers, streams, lakes, wetlands, canals and other waters against pollution and destruction.\nThe new EPA and Army Corps regulatory definition is an extreme departure from that, and the impact on our nation’s waters will be dramatic if it is not overturned in court.\nThe regulation is not based on science and it will not protect drinking water, fisheries or recreational uses of the nation’s waters as required by the Clean Water Act. It eliminates protections for these, and many other, categories of waters:\n- Interstate waters that flow across state boundaries (unless protected by another category)\n- Rivers, streams and canals that flow in response to rainfall\n- Rivers, streams and canals that flow year-round or seasonally but do not connect in a “typical year” to commercially navigable waters or the territorial seas via surface flow\n- Lakes that do not contribute surface flow to commercially navigable waters or the territorial seas via surface flow in a “typical year”\n- Wetlands that do not physically touch or become flooded by an otherwise protected water\nThe agencies claim the rule is being adopted to provide regulatory certainty and predictability for American farmers, landowners, and businesses — but it provides nothing remotely akin to that because it creates arbitrary, non-scientific categories of protected waters.\nAlthough there is extensive data available on the hydrology and connectivity of our nation’s waters, EPA and the Army Corps say that “there are significant limitations to the extent to which currently available data can be used to identify the scope of all or even a subset of jurisdictional waters” and that they are “in the early stages” of their effort to develop data necessary to do so. In other words, the agencies do not know which waters are included in the regulatory definition they devised, and they are just now starting to try to figure that out.\nBecause the Clean Water Act would no longer be controlling all discharges of water pollution at their source as intended, we will not know where the pollution is coming from and we won’t be able to clean up downstream waters.\nEven worse, the agencies say they “bear the burden of proof” for determining if a water is jurisdictional and, if they lack data to make that determination, they will treat waters as “non-jurisdictional.” In response to comments pointing out problems with their definition, the agencies describe the shortcomings of the available data and types of complex, case-by-case evaluations they “may” undertake to make a decision. In other words, the agencies’ definition doesn’t provide regulatory certainty for anyone, including the agencies themselves.\nDespite the agencies’ lack of knowledge regarding what their own regulation means for our nation’s waterways, it is possible to get a sense of how many waterways will lose protection. The loss is dramatic, particularly in the West. For example, there are millions of miles of rivers and streams across the country that could lose protection solely based on frequency of flow, including the ephemeral streams that make up roughly 90 percent or more of all surface water in the arid Southwest. And at least 51 percent of the nation’s wetlands could lose protection solely because they do not physically touch a protected river, stream or lake.\nThese are significant and devastating losses, and we know it will be far worse than that. For example, we applied the rule to 12 watersheds across the country and determined:\n- A 14,605 square-mile-area New Mexico’s Rio Grande Basin could lose protection. Additionally, roughly 90 percent of the rivers and streams in the basin could lose protection, including a stream that receives discharges from Los Alamos National Laboratory upstream from a drinking water intake for the City of Santa Fe.\n- Streams and rivers throughout the Meramec and Lower Missouri River watersheds, including 1,083 miles of streams that go subsurface and feed into rivers and springs, could lose protection.\n- All of the waters in a roughly 5,185 square mile area of the Snake River basin that include waters important for recreation, tourism and trout fishing like the Big Lost River, Little Lost River, and Medicine Lodge Creek could lose protection.\nUncontrolled pollution discharged into these waters not only harms the receiving waters directly, but also causes pollution and harm to downstream waters. This is true without regard to whether the water exists because of rainfall, goes subsurface, or doesn’t physically touch a downstream water. And because the Clean Water Act would no longer be controlling all discharges of water pollution at their source as intended, we will not know where the pollution is coming from and we won’t be able to clean up downstream waters.\nThe Clean Water Act requires the agencies to maintain and protect the chemical, physical, and biological integrity of the nation’s waters. The Agencies have instead adopted a regulation that is a giveaway to industrial polluters and will allow widespread pollution and destruction of the nation’s waters. This endangers all of us. Waterkeeper Alliance is committed to overturning this dangerous regulation, and restoring broad water quality protections for our country’s water resources."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:57564c68-4308-4282-a016-dafb7a76ca25>","<urn:uuid:75910c9e-a8ba-47dd-b94b-d4999f345c07>"],"error":null}
{"question":"Could you compare the destructive impacts of the 79 CE Vesuvius eruption and the 1980 Mount St. Helens eruption in terms of their reach and effects?","answer":"Both eruptions had significant but different impacts. The 79 CE Vesuvius eruption had an extremely wide reach, with ash traces found up to a thousand kilometers away, even reaching Greece. The Mount St. Helens 1980 eruption caused massive local destruction: destroying 250 homes, 47 bridges, 15 miles of railways, and 185 miles of highway, killing 57 people, and dramatically changing the mountain's structure by reducing its elevation from 9,677 feet to 8,365 feet through a massive debris avalanche.","context":["VESUVIUS | Revealing ancient mysteries of the 79 d.C. eruption\nA new multidisciplinary study has allowed better dating of the event and following the eruption effects up to a thousand kilometers distance\nThe event would have happened between 24 and 25 August of 79 d.C. This is the date that, based on current knowledge, the eruption of Pompeii occurred, according to the famous letter from Pliny the Younger to Tacitus. But the true date is another!\nAfter centuries of volcanic quiescence, the 79 d.C. eruption of Vesuvius destroyed part of the territory and surrounding towns. Almost two millennia after the eruption, an international team of researchers has analyzed again that event to get to a comprehensive state-of-the-art on the knowledge of the most famous eruption ever, starting from the exact date on which it occurred.\nThe integration among field studies, laboratory analyses, and historical fonts has allowed following all phases of the eruption over time, from the magma chamber to the ash deposition in areas extremely far from Vesuvius, with traces up to Greece.\nThe study “The 79 CE eruption of Vesuvius: a lesson from the past and the need of a multidisciplinary approach for developments in volcanology”, recently published on the prestigious journal ‘Earth-Science Reviews’, has been coordinated by Istituto Nazionale di Geofisica e Vulcanologia (INGV), in collaboration with Istituto di Geologia Ambientale e Geoingegneria at Consiglio Nazionale delle Ricerche (IGAG-CNR), Centro Interdipartimentale per lo Studio degli Effetti del Cambiamento Climatico (CIRSEC) and Dipartimento di Scienze della Terra at Università di Pisa, Laboratoire Magmas et Volcans at Clermont-Ferrand (LMV) in France, and School of Engineering and Physical Sciences (EPS) at the Heriot-Watt University of Edinburgh in UK. The study has been conducted in the framework of the project ‘Pianeta Dinamico’ funded by INGV.\nThe team of multidisciplinary researchers has collected and then critically analyzed the huge scientific production available for this eruption, by integrating it with new discoveries.\n“Our work examines with a wide and multidisciplinary approach different aspects of the 79 d.C. eruption, by integration of historical, stratigraphic, sedimentological, petrological, geophysical, paleoclimatic and modeling data, referring to the magmatic and eruptive processes of one among the most famous and devastating events occurred in the Neapolitan volcanic area’’, explains Mauro A. Di Vito, a volcanologist at INGV and coordinator of the study. ‘‘The work starts by dating the eruption, which would have occurred in the autumn and not on 24 August of 79 d.C. as previously thought, and continues by analyzing the volcanological data available in the proximity of the volcano, getting to thousand kilometers distance where traces of the eruption have been found in the form of fine ash’’.\n“Since the 13th century, the date of 24th August has been the subject of debate among historians, archaeologists and geologists because it is inconsistent with numerous pieces of evidence”, says Biagio Giaccio, researcher of the Igag-Cnr and co-author of the article. “Such as, for example, the finds in Pompeii of typically autumnal fruits or the heavy tunics worn by the inhabitants that were badly reconciled with the date of 24-25 August”, adds Giaccio. However, the definitive proof of the inaccuracy of the date only emerged a few years ago. “An inscription in charcoal on the wall of a building in Pompeii which translated quotes ‘The sixteenth day before the calends of November, he indulged in food in an immoderate way’, indicating that the eruption certainly occurred after 17 October”, continues Giaccio.\nThe date more accredited is therefore now that of 24-25 October.\nThe study has been integrated by a quantitative assessment of the impact of single phases of the eruption on those areas, including archaeological sites, close to the volcano.\n‘‘The spirit of our work has been that of understanding how a past event can represent a window to the future, by opening to new perspectives for the study of similar events that might occur again’’, continues Domenico Doronzo, volcanologist of INGV and co-author of the research. ‘‘This study will allow improving the applicability of forecasting models, from precursory phenomena to impact in eruptive and depositional processes. It will also allow contributing to reducing the vulnerability of those areas and infrastructures exposed to volcanic risk, not only close to the volcano but also – as the 79 d.C. event teaches – at a hundred kilometers distance from it’’.\n“In recent years, it has become increasingly important to understand the impact of eruptions on climate also to be able to study the origin and impact of some short climatic variations. However, we still do not know a lot – and with the appropriate resolution – of the climatic conditions at the time of the eruption of 79 d.C.” comments Gianni Zanchetta of the University of Pisa and co-author of the research.\n“In this work, we combined knowledge on regional climatic conditions at the time of the eruption to attempt a first synthesis” comments Monica Bini of the University of Pisa, co-author of the research. “Also, to direct future research on this aspect which still has many dark sides”.\nThe results of this study have received the appreciation of a world icon of volcanology like Raymond Cas, Prof. emeritus at the School of Earth Atmosphere and Environment of the Monash University (Australia) ‘‘The 79 AD eruption of Vesuvius volcano is one of the most iconic in the field of physical volcanology’’, states the Emeritus Professor Ray Cas. ‘‘Observations of the eruption as well as endless studies of the deposits and interpretation of the eruption processes underpin many of the concepts and understanding of explosive eruption processes in modern volcanology. A review of what is known about the eruption and its deposits is therefore very important to research volcanologists, and justifies a thorough and long review paper, such as this one. The authors are to be congratulated on the extremely comprehensive details extracted from the huge historical record and contemporary scientific literature on this iconic eruption’’.","Augustine volcano is one of the most active volcanoes in Cook Inlet, having\na symmetrical cone rising 1,254 meters above the sea level. This volcano has\nerupted in 1812, 1883, 1935, 1963-64, 1976 and 1986, and surprisingly, the\nintervals between these eruptions have reduced from 70 to 10 years. It's\nsummit comprises of several overlapping lava dome complexes formed during\nhistoric and prehistoric eruptions. Nearly 12 landslides took place at\nAugustine and the most recent slide that occurred was at the onset of the\n1883 eruption, when a part of the volcano's summit collapsed into the sea.\nWithin an hour, a tsunami nearly 9 meters high crashed ashore on the coast\nof the Kenai Peninsula, which was 80 kilometers away. As it was low tide,\nthere was no loss of life, only minor property damage was reported.\nLassen Peak is the southernmost active volcano in the Cascade Range. It is\npart of the Cascade Volcanic Arc which is an arc that stretches from\nnorthern California to southwestern British Columbia. Located in the Shasta\nCascade region of Northern California, Lassen rises 2,000 feet above the\nsurrounding terrain and has a volume of half a cubic mile, making it one of\nthe largest lava domes on Earth. It was created on the destroyed\nnortheastern flank of now gone Mount Tehama, a stratovolcano that was at\nleast 1,000 feet (300 m) higher than Lassen.\nLassen Peak has the distinction of being the only volcano in the Cascades\nother than Mount St. Helens to erupt during the 20th century. On May 22,\n1915, an explosive eruption at Lassen Peak devastated nearby areas and\nrained volcanic ash as far away as 200 miles to the east. This explosion was\nthe most powerful in a 1914–17 series of eruptions that were the last to\noccur in the Cascades before the 1980 eruption of Mount St. Helens in\nWashington. Lassen Volcanic National Park was created in Shasta County,\nCalifornia to preserve the devastated area and nearby volcanic geothermal\nDiller Canyon on Shastina near Weed, CaliforniaDuring the last 10,000 years\nShasta has erupted an average of every 800 years but in the past 4,500 years\nthe volcano has erupted an average of every 600 years. The last significant\neruption on Shasta may have occurred 200 years ago, as noted above.\nMount Shasta can release volcanic ash, pyroclastic flows or dacite and\nandesite lava. Its deposits can be detected under nearby small towns\ntotaling 20,000 in population. Shasta has an explosive, eruptive history.\nThere are fumaroles on the mountain, which show that Shasta is still alive.\nThe worst case scenario for an eruption is a large pyroclastic flow, such as\nwhat occurred in the 1980 eruption of Mount St. Helens. Since there is ice,\nsuch as Whitney Glacier and Mud Creek Glacier, lahars would also result. Ash\nwould probably blow inland, perhaps as far as eastern Nevada. There is a\nsmall chance that an eruption could also be bigger resulting in a collapse\nof the mountain, as happened when Mount Mazama in Oregon collapsed to form\nwhat is now called Crater Lake, but this is of much lower probability.\nBeing one of the world's largest active volcanoes, Mauna Loa rises more than\n9,000 meters from the seafloor, and is considered to be taller than Mount\nEverest. Since 1900, it has erupted 15 times, with eruptions lasting from\nless than a day to nearly 145 days. The most recent eruption of Maura Loa\nwas on March 25, 1984, that continued for 3 weeks. As a result, the lava\nflows were only 6.5 kms away from buildings in the city of Hilo. It erupts\nless frequently than Kilauea, but produces a larger quantity of lava in a\nshorter period of time.\nKilauea is the youngest volcano in the Hawaiian Islands. It's longest\nrift-zone eruption began on January 3, 1983 when a row of lava fountains\nbroke out from its east rift zone, about 17 kms from the summit caldera. The\nvolcano was named Kilauea which means 'spewing' or 'much spreading'; in\nreference to the lava flows when it erupts. The lava eruption from the\nKilauea volcano has covered approximately 75 square kilometers of forest and\ngrassland, destructed 179 homes, and added 120 hectares of new land to the\nisland. Of all the eruptions at Kilauea, a few historical eruptions were\ndangerously explosive, as fast-moving mixtures of ash and gas, called\npyroclastic surges, killed many people.\nMount Hood, called Wy'east by the Multnomah tribe, is a stratovolcano in the\nCascade Volcanic Arc of northern Oregon. It was formed by a subduction zone\nand rests in the Pacific Northwest region of the United States. It is\nlocated about 50 miles east-southeast of Portland, on the border between\nClackamas and Hood River counties.\nThe exact height assigned to Mount Hood's snow-covered peak has varied over\nits history. Modern sources point to three different heights: 11,249 feet\nbased on the 1991 U.S. National Geodetic Survey 11,240 feet based on a 1993\nscientific expedition and 11,239 feet of slightly older origin. The peak is\nhome to twelve glaciers. It is the highest mountain in Oregon and the\nfourth-highest in the Cascade Range. Mount Hood is considered the Oregon\nvolcano most likely to erupt, though based on its history, an explosive\neruption is unlikely. Still, the odds of an eruption in the next 30 years\nare estimated at between 3 and 7 percent, so the USGS characterizes it as\n\"potentially active\", but the mountain is informally considered dormant.\nMount Jefferson is a stratovolcano in the Cascade Volcanic Arc, part of the\nCascade Range, and is the second highest mountain in Oregon. Situated in the\nfar northeastern corner of Linn County on the Jefferson County line, about\n105 miles east of Corvallis, Mount Jefferson is in a rugged wilderness and\nis thus one of the hardest volcanoes to reach in the Cascades; though USFS\nRoad 1044 does come within 4 miles of the summit. The lower reaches of the\nmountain's north side also extend into southeastern Marion County, although\nits summit does not. Many people consider Jefferson's craggy, deeply\nglacially scarred appearance to be especially beautiful and photogenic\nMount Rainier is the largest volcano in the Cascade Range and is located in\nWashington state. Reaching a height of 4,392 m (14,410 ft), this dangerous\nvolcano is situated near the large urban areas of Tacoma and Seattle. The\nmain danger from an eruption of Mount Rainier would be the creation of\nlandslides or mudflows of volcanic debris that resemble wet concrete. This\nwould result in melting of the glaciers capping the mountain, causing floods\nthat would mix with loose volcanic ash and rock debris, destroying\neverything in its path.\nThe United States is well known for its active volcanoes that have the\npotential to wreak havoc on their immediate surroundings by releasing ash\nand toxic gases and causing mudslides. Nearly 80% of the US active volcanoes\nare in the Aleutian Islands and about 75% of the world’s volcanic eruptions\noccur along the Pacific Ring of Fire.\nMount St. Helens\nMount St. Helens is an active stratovolcano located in Skamania County,\nWashington, in the Pacific Northwest region of the United States. It is 96\nmiles south of Seattle and 50 miles northeast of Portland, Oregon. Mount St.\nHelens takes its English name from the British diplomat Lord St Helens, a\nfriend of explorer George Vancouver who made a survey of the area in the\nlate 18th century. The volcano is located in the Cascade Range and is part\nof the Cascade Volcanic Arc, a segment of the Pacific Ring of Fire that\nincludes over 160 active volcanoes. This volcano is well known for its ash\nexplosions and pyroclastic flows.\nMount St. Helens is most famous for its catastrophic eruption on May 18,\n1980, at 8:32 am PDT. which was the deadliest and most economically\ndestructive volcanic event in the history of the United States. Fifty-seven\npeople were killed; 250 homes, 47 bridges, 15 miles of railways, and 185\nmiles of highway were destroyed. The eruption caused a massive debris\navalanche, reducing the elevation of the mountain's summit from 9,677 feet\nto 8,365 feet and replacing it with a 1 mile wide horseshoe-shaped\ncrater. The debris avalanche was up to 0.7 cubic miles in volume. The\nMount St. Helens National Volcanic Monument was created to preserve the\nvolcano and allow for its aftermath to be scientifically studied.\nMount Baker is an active glaciated andesitic stratovolcano in the Cascade\nVolcanic Arc and the North Cascades of Washington State in the United\nStates. It is the second-most active volcano in the range after Mount Saint\nHelens. It is about 31 miles due east of the city of Bellingham, Whatcom\nCounty, making it the northernmost volcano in the Cascade Range but not the\nnorthernmost of the Cascade Volcanic Arc, which extends north into the Coast\nMountains. Mount Baker is the youngest volcano in the Mount Baker volcanic\nfield. While volcanism has persisted here for some 1.5 million years, the\ncurrent glaciated cone is likely no more than 140,000 years old, and\npossibly no older than 80-90,000 years. Older volcanic edifices have mostly\neroded away due to glaciation.\nThis listing of the\nTop Ten most active volcanos in the United States is constantly being revised as dormat volanos become active.\nSo, if you feel we are missing one, please email us and let us"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:e2fe2541-b7f9-4640-927b-f7ad90b328c8>","<urn:uuid:9e63b521-c0ac-42ed-b9c9-def39f2e97da>"],"error":null}
{"question":"What are the most effective ways to insulate windows for temperature control in a home? Do double-glazed windows make a significant difference?","answer":"Double-glazed windows are highly effective for temperature control. They can help keep your house cooler, and their efficiency can be verified through the Window Energy Rating Scheme (WERS) in Australia. For those who can't install double-glazed windows due to budget or heritage restrictions, alternative solutions include retrofitting windows with special films that act as an extra barrier. The choice of window frame also matters - timber and PVC frames provide good thermal protection, while modern aluminum frames now offer improved thermal performance. Additionally, window coverings play an important role - natural fiber curtains with blackout backings, solar block-out blinds, or exterior screens and blinds can effectively keep heat at bay.","context":["There’s so much we can do to harness natural cooling methods for the steamy summer months in this hot country, so how do we best adapt our homes – or build new ones – to prepare for those days when the mercury’s pushing 40?\nMuch depends on the local climate of course; some solutions that work for the hot and humid top end might not work for southern states.\nBut one thing’s for sure – the more we can maximise natural cooling, the more comfortable we’ll feel and the lower the bills will be in the long term.\nKeep the heat at bay by creating shading around the house, especially around west- and north-facing windows. Removable shades, such as the one pictured, mean that for those in temperate climates, you have the flexibility to remove the shade in winter.\nOr, for the same purpose – sun in winter, block-out in summer – plant deciduous vines on pergolas and create cool zones around the house. This will reduce the heat inside the home on hot summer days, but when the leaves drop in winter, light and warmth will be allowed inside.\nImprove your eaves\nEnsure your eaves are the right size and angled to keep the high summer sun at bay and allow winter sun through.\nLet it in/shut it out\nLearn to control temperatures by simply knowing when to open up your house for maximum air flow. On hot summer days, get into the habit of shutting up during the day – all doors, blinds, curtains – and then opening everything up in the evening to vent the house with evening breezes.\nIf renovating or building, carefully plan glazing for maximum air movement. Operable clerestory windows, especially placed on the southern side of the home, can be a wonderful way to create refreshing cross breezes – remember hot air rises.\nGo for glazing\nAnd of course, if the budget permits, double glazing will help to keep your house even cooler. Check the rating of any windows you buy – Australian windows are rated for thermal efficiency by the Window Energy Rating Scheme (WERS).\nIf your budget or heritage restrictions mean you can’t change older windows, consider alternatives to double glazing – older windows can be retrofitted with films that will act as an extra barrier against the elements, without ruining the look of your windows.\nAlso remember that the choice of frame will affect temperatures too. Timber and PVC frames can offer great thermal protection, but aluminium windows (often less expensive) are now available with improved thermal performance.\nTreat windows well\nHow you cover your windows will also play a part in keeping your house cool. Opt for natural fibres and consider full blackout backings on curtains, as well as a sheer layer for temperature control.\nIf blinds are more your thing than curtains, then choose carefully for full solar block-out.\nAlternatively, consider screens and blinds on the exterior of the windows. This can be a highly effective way to keep the heat at bay.\nThink thermal mass\nIf possible, create areas of thermal mass in your home for no-cost cooling (after it’s installed of course).\nThermal mass is any material – such as concrete or stone – that can absorb and store heat, taking a long time to heat up and to cool down. So in warm weather, thermal mass can help maintain a building’s cool.\nBrick walls and concrete slab floors are good examples of thermal mass.\nBe fan savvy\nThere are so many reasons to love ceiling fans – they now come in all shapes and sizes so they can look great, but mostly because they provide cooling at a fraction of the cost of other cooling methods.\nWhen we think of insulation we often thinking of keeping the warmth in and cold at bay, but of course it works the other way around too.\nGood insulation is one of the most effective ways we can keep our homes cool, and this isn’t something that only applies to new homes and renovations – older and existing homes can benefit from retrofitting with good insulation.\nCheck the insulation levels in your home and talk to a specialist about upgrading. For builders or renovators, ensure the best insulation from the start. It’s likely that any extra costs will be recouped in savings over time.\nInsulation isn’t just about what’s between the walls – it’s in our floors and ceilings too, and even the colour of your roof can affect how cool your house is. Opt for lighter colours to keep temperatures lower at home.\nBy Liz Durnan"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:0c05b300-f1ab-4675-be7f-ca372e31386d>"],"error":null}
{"question":"What is the stability argument for anarcho-capitalism compared to current political systems?","answer":"Under current systems, people in power (politicians, military officers, and police) are selected specifically for desiring and using power, making them more likely to seize control. In contrast, under anarcho-capitalism, protection agencies would be run by people selected for business efficiency and customer service, making power grabs less likely.","context":["Anarcho-capitalism is a political philosophy and school of anarchist thought that advocates the elimination of the state in favor of self-ownership, private property, and free markets. Anarcho-capitalists hold that, in the absence of statute (law by centralized decrees and legislation), society tends to contractually self-regulate and civilize through the discipline of the free market (in what its proponents describe as a voluntary society).\nI believe in individual freedom; that's my primary and complete commitment—individual liberty. That’s what it's all about. And that's what socialism was supposed to be about, or anarchism was supposed to be about, and tragically has been betrayed.\n- Although anarchism rests on liberal intellectual foundations, notably the distinction between state and society, the protean character of the doctrine makes it difficult to disinguish clearly different schools of anarchist thought. But one important distinction is between individualist anarchism and social anarchism. The former emphasizes individual liberty, the sovereignty of the individual, the importance of private property or possession, and the iniquity of all monopolies. It may be seen as liberalism taken to an extreme conclusion. 'Anarcho-capitalism' is a contemporary variant of this school.\n- Thomas Bottomore, Dictionary of Marxist Thought, Anarchism entry, p. 21 (1992).\n- Anarcho-capitalism, in my opinion, is a doctrinal system which, if ever implemented, would lead to forms of tyranny and oppression that have few counterparts in human history. There isn't the slightest possibility that its (in my view, horrendous) ideas would be implemented, because they would quickly destroy any society that made this colossal error. … I should add, however, that I find myself in substantial agreement with people who consider themselves anarcho-capitalists on a whole range of issues; and for some years, was able to write only in their journals. And I also admire their commitment to rationality — which is rare — though I do not think they see the consequences of the doctrines they espouse, or their profound moral failings.\n- Noam Chomsky in Z Net, Answers from Chomsky to eight questions on anarchism, 1996.\n- Perhaps the best way to see why anarcho-capitalism would be much more peaceful than our present system is by analogy. Consider our world as it would be if the cost of moving from one country to another were zero. Everyone lives in a housetrailer and speaks the same language. One day, the president of France announces that because of troubles with neighboring countries, new military taxes are being levied and conscription will begin shortly. The next morning the president of France finds himself ruling a peaceful but empty landscape, the population having been reduced to himself, three generals, and twenty-seven war correspondents.\n- David Friedman, Chapter: \"The Stability Problem\", The Machinery of Freedom: Guide to a Radical Capitalism (2nd ed.), p. 123.\n- We must ask, not whether an anarcho-capitalis society would be safe from a power grab by the men with the guns (safety is not an available option), but whether it would be safer than our society is from a comparable seizure of power by the men with the guns. I think the answer is yes. In our society, the men who must engineer such a coup are politicians, military officers, and policemen, men selected precisely for the characteristic of desiring power and being good at using it. They are men who already believe that they have a right to push other men around - that is their job. They are particularly well qualified for the job of seizing power. Under anarcho-capitalism the men in control of protection agencies are selected for their ability to run an efficient business and please their customers. It is always possible that some will turn out to be secret power freaks as well, but it is surely less likely than under our system where the corresponding jobs are labeled 'non-power freaks need not apply'.\n- David Friedman, Chapter: \"The Stability Problem\", The Machinery of Freedom: Guide to a Radical Capitalism (2nd ed.), p. 123–124.\n- Laissez-faire capitalism, or anarchocapitalism, is simply the economic form of the libertarian ethic. Laissez-faire capitalism encompasses the notion that men should exchange goods and services, without regulation, solely on the basis of value for value. It recognizes charity and communal enterprises as voluntary versions of this same ethic. Such a system would be straight barter, except for the widely felt need for a division of labor in which men, voluntarily, accept value tokens such as cash and credit. Economically, this system is anarchy, and proudly so.\n- [A]fter I got evicted from the Republican Party, I began reading considerably more of the works of American anarchists, thanks largely to Murray Rothbard … and I was just amazed.\n- The American Benjamin Tucker (1854-1939) believed that maximum individual liberty would be assured where the free market was not hindered or controlled by the State and monopolies. The affairs of society would be governed by myriad voluntary societies and cooperatives, by, as he aptly put it, “un-terrified” Jeffersonian democrats, who believed in the least government possible. Since World War II this tradition has been reborn and modified in the United States as anarcho-capitalism or libertarianism.\n- Individualist anarchism … stands in rigorous opposition to attacking the person or property of individuals. The philosophy revolves around the \"Sovereignty of the Individual\"—as an early champion, Josiah Warren, phrased it. … Largely due to the path breaking work of Murray Rothbard, 20th century individualist anarchism is no longer inherently suspicious of profit-making practices, such as charging interest. Indeed, it embraces the free market as the voluntary vehicle of economic exchange. But as individualist anarchism draws increasingly upon the work of Austrian economists such as Mises and Hayek, it draws increasingly farther away from left anarchism.\n- The philosophy of 'anarcho-capitalism' dreamed up by the 'libertarian' New Right, has nothing to do with Anarchism as known by the Anarchist movement proper. It is a lie . . . Patently unbridled capitalism . . . needs some force at its disposal to maintain class privileges, either from the State itself or from private armies. What they believe in is in fact a limited State -- that is, one in which the State has one function, to protect the ruling class, does not interfere with exploitation, and comes as cheap as possible for the ruling class. The idea also serves another purpose . . . a moral justification for bourgeois consciences in avoiding taxes without feeling guilty about it.\n- Albert Meltzer, Anarchism: Arguments For and Against (1981), p. 50.\n- I'm a, what, an anarcho-capitalist socialist… I don't know… I'm kinda a moderate, I think I'm moderate.\n- Krist Novoselic, interviewed by Nick Gillespie, \"Nirvana's Krist Novoselic on Punk, Politics, & Why He Dumped the Dems\", ReasonTV (19 June 2014), 11:33–11:42.\n- Capitalism is the fullest expression of anarchism, and anarchism is the fullest expression of capitalism.\n- I define anarchist society as one where there is no legal possibility for coercive aggression against the person or property of any individual. Anarchists oppose the State because it has its very being in such aggression, namely, the expropriation of private property through taxation, the coercive exclusion of other providers of defense service from its territory, and all of the other depredations and coercions that are built upon these twin foci of invasions of individual rights."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:f1ef3581-39d6-4daa-96b0-3a60877d2cf5>"],"error":null}
{"question":"How does modern placemaking address sustainability goals, and what limitations have been observed in implementing sustainable practices across different projects?","answer":"Modern placemaking addresses sustainability goals by incorporating it as a core philosophy in development, focusing on creating sustainable, affordable spaces that maximize benefits for everyone. This is reflected in the obligations under Section 106 of the Town and Country Planning Act, which requires developers to mitigate their impact on surrounding neighborhoods. However, implementation has faced limitations, as revealed in design excellence programs. While individual projects like Thaden School have successfully emphasized sustainable lifestyles in their campus and curriculum, and TheatreSquared has incorporated locally sourced materials and energy efficiencies, regional development patterns have often limited progress outside project boundaries. This suggests that while sustainability can be achieved at the project level, broader regional patterns can constrain the overall environmental impact.","context":["What is “placemaking” and how does it affect you?\nPlacemaking is becoming an integral part of commercial property planning, thanks largely to Section 106 of the Town and Country Planning Act (1990) and the community infrastructure levy. It should be an important consideration when developing property – particularly if it’s for residential use.\nHere’s a guide outlining what placemaking is and how it affects your property development plans.\nWhat is placemaking?\nPlacemaking is considered a modern concept, though it’s been around since the 1960s. Originally coined by writers William H Whyte and Jane Jacobs, it refers to the act of developing cities in order to benefit the people living within them. It puts that benefit first.\nThese days, placemaking encapsulates many different philosophies and processes. It’s not just about creating spaces that are people-friendly, with amenities, public spaces and schools. It’s about sustainability, affordability and maximising the benefits for everyone – developers and homebuyers alike.\nSection 106 and Your Responsibilities as a Developer\nUnder Section 106 of the Town and Country Planning Act, developers are obliged to mitigate against the impact of their development – specifically in terms of the surrounding neighbourhood. For example, a newly developed residential property may require additional infrastructure (like a connecting road), or local amenities (e.g. a larger healthcare centre to cope with additional residents).\nYou’re only required to undertake this if the changes are necessary as a direct result of your development. A specialist from the local council will review your development plans and provide an assessment of your exact responsibilities.\nThe community infrastructure levy is a development tax (set at a standard rate) which is given to the local area to reinvest as they see fit.\nThe Advantages of Placemaking\nAt a glance, placemaking may not seem to offer much benefit to developers. However, experts testify that it can improve ROI, particularly when adopted in the early stages of development.\nFor example, placemaking encourages developers to view the area as a whole, rather than zoning in on specific components. By adopting a wider view, they can avoid common errors, such as building in a heavily congested street (without addressing the issue by changing the infrastructure).\nBy developing from a people-centred perspective, developers are also likely to make their residential properties more desirable. There are several incidences of this in action – the well-known Poundbury development in Dorchester (Dorset) is a classic example. In Poundbury, new build prices are 29% higher than other comparable areas, due to its excellent public infrastructure and resident-centred approach.\nThe Demand for Quality Housing\nBuying trends in the UK show that quality housing is a top priority for many homebuyers, resulting in faster sales and higher sales values for property developers. Additionally, when impact on community is prioritised, this has a positive effect on the area and the people living within it. It’s more likely that the property will sell quickly (benefitting the developer), and it’s also more likely that the public sector will invest into it too – which again, improves the overall community.\nPlacemaking is especially important in the 21st century. In 1950, just a third of the world’s population lived in cities. 50 years later, this has risen to a half, and it’s anticipated the figure will rise to two-thirds by 2050. The need to create appealing, liveable space for communities has never been higher – and by taking placemaking into account, developers are recognising this growing need, and their integral role in the process.","NWA Design Excellence Program Five-Year Impact\nBuilding Regional Capacity\n- Building Regional Capacity\n- Strengthening Public Life\n- Celebrating Local Culture and Place\n- Elevating Standards of Sustainability and Resilience\n- Highlighting Lessons Learned\n- Addressing Regional Limitations\nSince 2015, the Northwest Arkansas Design Excellence Program has supported 15 projects across the region’s five largest downtowns. The Walton Family Foundation recently collaborated with Gehl, an urban design firm, to create a framework – Uncovering the Impact of the Design Excellence Program – that measures the impact of completed projects and helps position the program, and the region, for a future of design excellence.\nOver the past five years, the Design Excellence Program built regional capacity by helping transform downtown spaces, improve quality of life and attract and retain new residents.\nCompleted projects are helping enhance downtown transformations across the region. TheatreSquared has become an anchor space for the future Cultural Arts Corridor in downtown Fayetteville, while the Rogers Historical Museum is bringing visitors to local businesses. At the same time, Thaden School has become an asset that attracts families to the region.\nProjects have created new places and opportunities for people to spend quality time in public – moving or staying, alone and with others.\nTo attract a more diverse audience, TheatreSquared created varying price points for tickets. The Rogers Historical Museum has increased programmable space and created new local events. At Thaden School, facilities like the pump track are now open for community use, while the Quilt of Parks design will create a variety of spaces that will welcome people of all ages and backgrounds.\nSeveral Design Excellence Program projects have incorporated elements of the “Arkansan identity” or historical spaces to develop authentic yet world-class designs.\nIn a nod to local heritage, projects have leveraged recycled materials, historical features and architectural styles as inspirations for their designs.\nCompleted Design Excellence Program projects have weaved sustainability into the fabric of their design and operations.\nDesign Excellence Program projects like Thaden School emphasize sustainable lifestyles throughout its campus and curriculum, while TheatreSquared demonstrates these values with locally sourced materials and energy efficiencies. The design for Luther George Park will minimize environmental impact and be adaptable to use in all seasons.\nIn addition to its successes, the Gehl report also identifies challenges to consider for future design and programming.\nThe report revealed the need to listen to user needs more comprehensively, integrate that feedback into designs and, where possible, more intentionally engage traditionally underrepresented communities. Additionally, it identified connectivity issues, which could be addressed through more comprehensive planning.\nThe report identified three primary factors that may limit future program impact.\nIn spite of site-specific mobility enhancements, inadequate walking, biking and transit infrastructure surrounding the projects have affected the user experience and foot traffic, particularly among households without cars. While these projects have emphasized sustainability in their construction, operation and programming, regional development patterns have sometimes limited progress outside their site boundaries. Finally, even though many projects are near areas with above average diversity, nearly all face challenges in attracting these types of audiences, signaling the need for more intentional engagement with underrepresented communities."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:ef364d39-91c0-46e0-a3a2-325f018db8cc>","<urn:uuid:929a4ccc-e5b5-4acc-87af-dc59b47bbbe5>"],"error":null}
{"question":"What was the first official fire alarm box invention, and what are its current commercial applications?","answer":"The first official fire alarm box was invented by William F. Channing in the mid-17th century, using telegraphy and electromagnetism. Initially, it was a complex device with wires and levers that required careful operation to send telegraph signals to fire stations. John Nelson Gamewell later simplified it to the universal 'pull-down lever' design. Today, commercial applications include advanced fire alarm control panels with sophisticated smoke sensing technology, integrated networks combining fire detection with security systems, central command centers, and various peripheral devices including intelligent sensors and gas detectors.","context":["Have you ever stopped to wonder where fire alarm systems started, or how they started? If you have, nice! If not, well we’re here to walk you through the interesting history of fire alarms from way back when to the ones you know today.\nIt All Starts with Community\nWith the creation of organized communities, precautionary solutions had to be created to protect those communities. Fire alarm systems, at their most basic level, were created along with civilization and the pursuit of staying safe, alive and well.\nAnd the demand for fire alarm systems remains to be universally understood: providing people with a clear warning of fire will always be imperative to protecting the livelihood of both life and infrastructure around the world.\nFrom Then to Now: The Evolution of the Fire Alarm System\nIt was the Ancient Greeks and Romans who set the first fire alarm system standards: the simple method of applying water to fire, quickly, to extinguish it! Specifically, their creation was a fire-extinguisher and pump, a prototype to the fire alarms we think of today.\nOkay, maybe not exactly what we think of today – it’d probably be safe to assume modern fire alarms are much more precise. Buckets were the equivalent to street-side fire hydrants – but hey – we must give credit where it’s due, right?\nObviously, technology has advanced greatly – but the premise and necessity remain the same: a system had to be created to specifically warn others of a fire and call to action those who could put it out. With this, more and more measures were put into place with the implementation of specific fire-fighting tools: the fire-fighters, fire “trucks”, fire equipment and fire stations.\nBell Towers: The First Fire Alarm System\nBy the 16th century, it was quickly understood within most civilizations a water supply must be kept close by, quickly accessible in case of a fire. Eventually, larger communities and cities needed better ways warn others from further distances. Here’s where bell towers came into play as the first fire alarm system.\nTraditionally, bell towers were meant as a public service, to track time. Bells were also rung for Church, and town gatherings, but the difference was in the way they sounded the bells. You could say fire began to have a certain ring to it.\nIt was in America, in 1828, that the Pennsylvania State House rebuilt their Independence Hall a new steeple, hung a new bell, and employed an official fire watchman. A man named Franklin Peale came up with specific bell-ringing signals: ringing the bell at certain intervals alerted folks of the general location where the fire was, and men would quickly head in that direction, water in tow.\nThis was quickly adopted in other big cities, too. Bell codes worked, to a degree, but the unpredictability of fire itself called for higher, more precise ways of locating and distinguishing fires before they reached a detrimental level of destruction. The fire alarm system has come a long way since the bell tower.\nThe Official Invention of the Fire Alarm System Box\nAmerica in the mid-17th century was an exciting era of life-changing (and saving) inventions. Following Samuel Morse’s invention of the telegraph in 1837, and its commercial usage starting in 1844, it was scholar of electromagnetism, Mr. William F. Channing, of Boston, MA, who invented the first official fire alarm system.\nChanning, and his assistant Mr. Moses Farmer, both believed that the use of telegraphy and electromagnetism could be used for different types of communication and location – specifically for emergency situations. The end goal was to create a quick, reliable fire alarm that could be used to alert firefighters the near-exact location of a fire as it started.\nOriginally, the fire alarm system was a bit confusing. It was a large, hefty contraption with several wires and levers, containing a telegraphic key with a metal handle. In 1852, when a fire was detected, someone would crank the handle, relaying the fire box alarm number to the nearest fire station location. Doing this also sounded the literal alarm, which would ring at the location and at the nearest fire station.\nOften, however, if the crank was turned too quickly, the telegraph signal couldn’t be sent out. The relationship between the electric current that would pulse out this fire alarm signal and the release of the bell-clappers (the alarm itself) was very touchy and had to be done precisely. As one may assume, in a state of emergency, your average Joe may be a little scared and not necessarily aware that he should make sure to crank the handle correctly.\nFor a busy city, this invention was a monumental moment toward the improvement of local and city-wide safety procedures, and it was quickly patented. After refining their product, patent for Channing and Farmer’s “Electromagnetic Fire Alarm Telegraph” was issued on May 19th, 1857.\nModern Fire Alarm Systems\nIn March of 1855, Channing set out his proposal describing the victories and merits of his invention at the Smithsonian Institution lecture in Boston, Massachusetts. He described his invention as “a higher system of municipal organization than any which has heretofore been proposed or adopted.” However efficient, his product just wasn’t catching on, and he was going into debt.\nJohn Nelson Gamewell, a postmaster and telegraph operator from Camden, South Carolina, attended the same lecture Channing did, and recognized the potential of the invention. He bought the rights to the fire alarm box Channing and Farmer created. Gamewell adapted the fire box to become more easily handled, and it sold to over 500 cities. Instead of all the intricate levers and pulleys arranged on Channing’s product, the now universally-recognized “pull-down lever” was adapted.\nThink of the fire alarms we have today – the small red box containing a handle beneath the glass that is to be pulled in case of a fire, often setting off sprinklers and sending out a signal to the fire station(s) close by to come and put out the fire. The original design wasn’t all that different, but there were too many things that could go wrong. Gamewell’s adaptations made all the difference and gave way to the fire alarm systems we have today.\nSo there you have it – the fire alarm system from ancient times through modern day. It’s quite interesting to see how powerful innovations that will benefit people the world round have evolved throughout history. Will there be additional fire alarm innovations beyond what we have in this modern age? As technology and the Internet of Things continue to race on, we would set a friendly wager on changes down the line. I mean, we have moved from paper fire inspections to awesome fire inspection apps!","Protection of human life while safeguarding buildings, assets and inventories is the principal function of a commercial alarm system. Recognizing that fire, smoke and gas leaks are continual threats, industry leaders have made significant strides in developing new detection and warning devices. Fire detection and early warning are essential elements of any commercial building.\nThe types and sophistication of alarm systems depend on the building configuration, type of business and potential threats of fire. A commercial alarm system might range from simple smoke detection to advanced fire and gas detection plus fire suppression systems and central monitoring.\nCommercial alarm systems fall into two categories, fire and security. In many instances, these two functions can be highly integrated and may be monitored from a central control point. Certain features may overlap. For example, closed circuit TV monitors and motion detection sensors may be important for either function to determine the status of individuals and any developing conditions within the building.\nFire Detection and Alarm Systems\nChicago’s high-rise buildings, hospitals, universities, offices and apartments require advanced fire and smoke detection systems to protect the lives of the occupants. Quick response and evacuation plans must be clearly designed for a wide range of scenarios.\nTypes of Commercial Alarm Systems\nModern fire alarms systems consist of sets of complementary devices designed to provide early warning to occupants to evacuate a building until any danger is suppressed or removed. Detectable threats may include smoke, fire, carbon monoxide or other dangerous gases.\n- Alarms may be activated by smoke detection or heat detection.\n- Alerts are usually designed with alarms sounding with bells or horns, plus highly visible strobe lights and loud speaker voices to direct evacuations.\n- Alarms can be automatically triggered by sensors or manually activated from an individual alarm location or central control point.\n- Alarms soundings may be set at different frequencies and volumes, depending on the radius to be covered.\nNOTIFIER Commercial Alarm System\nNOTIFIER™️ by Honeywell, represented by High Rise Security Systems (HRSS) in Burr Ridge, produces the most advanced fire and smoke detection and control systems available. Their highly sophisticated systems include a range of detection, alarm, voice-alert, and central and remote control features that can be fully integrated to allow for monitoring of conditions throughout the building or complex. Some products include:\n- Fire Alarm Control Panels are equipped with advanced smoke sensing technology and voice evacuation systems.\n- NOTIFIER Integrated networks combine all of the fire detection systems and technologies with existing security systems, closed-circuit TV, and card access capability for a complete system.\n- NOTIFIER Command Center allows for central oversight and command during an emergency.\n- Peripheral Devices and Accessories include intelligent sensors and control points as well as remote annunciators, auxiliary power supplies and additional notification devices.\n- Gas Detectors may be fixed, stand alone or portable and are capable of detecting dangerous gases that may be present.\nHRSS Professional Support for NOTIFIER\nWhether designing a new building or retrofitting an older one, a complete fire detection system is a key design feature. Architects and engineers collaborate early with professional fire safety technicians to ensure the proper commercial alarm systems are in place and that all NFPA and local code requirements are met. Proper placement of sensors, alarms and fire suppression equipment is critical for the safety of the occupants and building content.\nThe professionals at High Rise Security Systems in Burr Ridge are recognized experts in fire safety warning devices. As sales and service representative of state-of-the-art NOTIFIER™️ by Honeywell fire detection and alarm systems, HRSS technicians are trained and certified to install and maintain your commercial alarm system."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:b0b90424-36e9-49d4-b55f-c94ffd9dd872>","<urn:uuid:e18891b1-7ea8-4822-8a59-b01c39eddabd>"],"error":null}
{"question":"¿Cómo se comparan los system effects en sistemas de seguridad industriales con los human error mechanisms? Me interesa entender las diferencias y similitudes en estos conceptos tan importantes! 🤔","answer":"System effects in industrial safety and human error mechanisms share important connections but have distinct characteristics. System effects arise from the organization and configuration of the system itself, like accidents resulting from tightly linked features where variation in one component affects others unexpectedly. Examples include flash crashes in stock markets or rising error frequencies at shift ends. Human error mechanisms, on the other hand, are part of the ordinary spectrum of human behavior and can be categorized in various ways like exogenous vs endogenous errors, or perceptual vs cognitive vs organizational levels. Importantly, what's labeled as 'human error' is actually done in hindsight, as these mechanisms are the same ones that drive normal human performance. Both concepts contribute to understanding accidents and safety, but system effects focus on organizational/structural causes while human error examines individual performance variability.","context":["Quite a few posts here have focused on the question of emergence in social ontology, the idea that there are causal processes and powers at work at the level of social entities that do not correspond to similar properties at the individual level. Here I want to raise a related question, the notion that an important aspect of the workings of the social world derives from “system effects” of the organizations and institutions through which social life transpires. A system accident or effect is one that derives importantly from the organization and configuration of the system itself, rather than the specific properties of the units.\nWhat are some examples of system effects? Consider these phenomena:\n- Flash crashes in stock markets as a result of automated trading\n- Under-reporting of land values in agrarian fiscal regimes\n- Grade inflation in elite universities\n- Increase in product defect frequency following a reduction in inspections\n- Rising frequency of industrial errors at the end of work shifts\nHere is how Nancy Leveson describes systems causation in Engineering a Safer World: Systems Thinking Applied to Safety:\nSafety approaches based on systems theory consider accidents as arising from the interactions among system components and usually do not specify single causal variables or factors. Whereas industrial (occupational) safety models and event chain models focus on unsafe acts or conditions, classic system safety models instead look at what went wrong with the system’s operation or organization to allow the accident to take place. (KL 977)\nCharles Perrow offers a taxonomy of systems as a hierarchy of composition in Normal Accidents: Living with High-Risk Technologies:\nConsider a nuclear plant as the system. A part will be the first level — say a valve. This is the smallest component of the system that is likely to be identified in analyzing an accident. A functionally related collection of parts, as, for example, those that make up the steam generator, will be called a unit, the second level. An array of units, such as the steam generator and the water return system that includes the condensate polishers and associated motors, pumps, and piping, will make up a subsystem, in this case the secondary cooling system. This is the third level. A nuclear plan has around two dozen subsystems under this rough scheme. They all come together in the fourth level, the nuclear plant or system. Beyond this is the environment. (65)\nLarge socioeconomic systems like capitalism and collectivized socialism have system effects — chronic patterns of low productivity and corruption in the latter case, a tendency to inequality and immiseration in the former case. In each case the observed effect is the result of embedded features of property and labor in the two systems that result in specific kinds of outcomes. And an important dimension of social analysis is to uncover the ways in which ordinary actors pursuing ordinary goals within the context of the two systems, lead to quite different outcomes at the level of the “mode of production”. And these effects do not depend on there being a distinctive kind of actor in each system; in fact, one could interchange the actors and still find the same macro-level outcomes.\nHere is a preliminary effort at a definition for this concept in application to social organizations:\nA system effect is an outcome that derives from the embedded characteristics of incentive and opportunity within a social arrangement that lead normal actors to engage in activity leading to the hypothesized aggregate effect.\nOnce we see what the incentive and opportunity structures are, we can readily see why some fraction of actors modify their behavior in ways that lead to the outcome. In this respect the system is the salient causal factor rather than the specific properties of the actors — change the system properties and you will change the social outcome.\nWhen we refer to system effects we often have unintended consequences in mind — unintended both by the individual actors and the architects of the organization or practice. But this is not essential; we can also think of examples of organizational arrangements that were deliberately chosen or designed to bring about the given outcome. In particular, a given system effect may be intended by the designer and unintended by the individual actors. But when the outcomes in question are clearly dysfunctional or “catastrophic”, it is natural to assume that they are unintended. (This, however, is one of the specific areas of insight that comes out of the new institutionalism: the dysfunctional outcome may be favorable for some sets of actors even as they are unfavorable for the workings of the system as a whole.)\nAnother common assumption about system effects is that they are remarkably stable through changes of actors and efforts to reverse the given outcome. In this sense they are thought to be somewhat beyond the control of the individuals who make up the system. The only promising way of undoing the effect is to change the incentives and opportunities that bring it about. But to the extent that a given configuration has emerged along with supporting mechanisms protecting it from deformation, changing the configuration may be frustratingly difficult.\nSafety and its converse are often described as system effects. By this is often meant two things. First, there is the important insight that traditional accident analysis favors “unit failure” at the expense of more systemic factors. And second, there is the idea that accidents and failures often result from “tightly linked” features of systems, both social and technical, in which variation in one component of a system can have unexpected consequences for the operation of other components of the system. Charles Perrow describes the topic of loose and tight coupling in social systems in Normal Accidents; 89 ff.","Individual differences |\nMethods | Statistics | Clinical | Educational | Industrial | Professional items | World psychology |\nHuman performance can be affected by many factors such as age, circadian rhythms, state of mind, physical health, attitude, emotions, propensity for certain common mistakes, errors and cognitive biases, etc.\nHuman reliability is very important due to the contributions of humans to the resilience of systems and to possible adverse consequences of human errors or oversights, especially when the human is a crucial part of the large socio-technical systems as is common today. User-centered design and error-tolerant design are just two of many terms used to describe efforts to make technology better suited to operation by humans.\nHuman Reliability Analysis Techniques Edit\nA variety of methods exist for Human Reliability Analysis (HRA) (see Kirwan and Ainsworth, 1992; Kirwan, 1994). Two general classes of methods are those based on probabilistic risk assessment (PRA) and those based on a cognitive theory of control.\nOne method for analyzing human reliability is a straightforward extension of probabilistic risk assessment (PRA): in the same way that equipment can fail in a plant, so can a human operator commit errors. In both cases, an analysis (functional decomposition for equipment and task analysis for humans) would articulate a level of detail for which failure or error probabilities can be assigned. This basic idea is behind the Technique for Human Error Rate Prediction (THERP) (Swain & Guttman, 1983). THERP is intended to generate human error probabilities that would be incorporated into a PRA. The Accident Sequence Evaluation Program (ASEP) Human Reliability Procedure is a simplified form of THERP; an associated computational tool is Simplified Human Error Analysis Code (SHEAN) (Wilson, 1993). More recently, the US Nuclear Regulatory Commission has published the Standardized Plant Analysis Risk (SPAR) human reliability analysis method (SPAR-H) (Gertman et al, 2005).\nCognitive Control Based TechniquesEdit\nErik Hollnagel has developed this line of thought in his work on the Contextual Control Model (COCOM) (Hollnagel, 1993) and the Cognitive Reliability and Error Analysis Method (CREAM) (Hollnagel, 1998). COCOM models human performance as a set of control modes -- strategic (based on long-term planning), tactical (based on procedures), opportunistic (based on present context), and scrambled (random) -- and proposes a model of how transitions between these control modes occur. This model of control mode transition consists of a number of factors, including the human operator's estimate of the outcome of the action (success or failure), the time remaining to accomplish the action (adequate or inadequate), and the number of simultaneous goals of the human operator at that time. CREAM is a human reliability analysis method that is based on COCOM.\nRelated techniques in safety engineering and reliability engineering include Failure mode and effects analysis, Hazop, Fault tree, and SAPHIRE: Systems Analysis Programs for Hands-on Integrated Reliability Evaluations.\nHuman Error Edit\nHuman Error has been cited as a cause or contributing factor in disasters and accidents in industries as diverse as nuclear power (e.g., Three Mile Island accident), aviation (see Pilot error), space exploration (e.g., Space Shuttle Challenger Disaster), and medicine (see Medical error). It is also important to stress that \"human error\" mechanisms are the same as \"human performance\" mechanisms; performance later categorized as 'error' is done so in hindsight (Reason, 1991; Woods, 1990): thereofore actions later termed \"human error\" are actually part of the ordinary spectrum of human behaviour. Recently, human error has been reconceptualized as resiliency to emphasize the positive aspects that humans bring to the operation of technical systems (see Hollnagel, Woods and Leveson, 2006).\nCategories of Human ErrorEdit\nThere are many ways to categorize human error (see Jones, 1999).\n- exogenous versus endogenous (i.e., originating outside versus inside the individual) (Senders and Moray, 1991)\n- situation assessment versus response planning (e.g., Roth et al, 1994) and related distinctions in\n- By level of analysis; for example, perceptual (e.g., Optical illusions) versus cognitive versus communication versus organizational.\nThe cognitive study of human error is a very active research field, including work related to limits of memory and attention and also to decision making strategies such as the availability heuristic and other cognitive biases. Such heuristics and biases are strategies that are useful and often correct, but can lead to systematic patterns of error.\nOrganizational studies of error or dysfunction have included studies of safety culture. One technique for organizational analysis is the Management Oversight Risk Tree (MORT) (Kirwan and Ainsworth, 1992; also search for MORT on the FAA Human Factors Workbench.\nHuman Factors Analysis and Classification System (HFACS)Edit\nThe Human Factors Analysis and Classification System (HFACS) was developed initially as a framework to understand \"human error\" as a cause of aviation accidents (Shappell and Wiegmann, 2000; Wiegmann and Shappell, 2003). It is based on James Reason's Swiss cheese model of human error in complex systems. HFACS distinguishes between the \"active failures\" of unsafe acts, and \"latent failures\" of preconditions for unsafe acts, unsafe supervision, and organizational influences. These categories were developed empirically on the basis of many aviation accident reports.\nUnsafe acts are performed by the human operator \"on the front line\" (e.g., the pilot, the air traffic controller, the driver). Unsafe acts can be either errors (in perception, decision making or skill-based performance) or violations (routine or exceptional). The \"errors\" here are similar to the above discussion. Violations are the deliberate disregard for rules and procedures. As the name implies, routine violations are those that occur habitually and are usually tolerated by the organization or authority. Exceptional violations are unusual and often extreme. For example, driving 60 mph in a 55-mph zone speed limit is a routine violation, but driving 130 mph in the same zone is exceptional.\nThere are two types of preconditions for unsafe acts: those that relate to the human operator's internal state and those that relate to the human operator's practices or ways of working. Adverse internal states include those related to physiology (e.g., illness) and mental state (e.g., mentally fatigued, distracted). A third aspect of 'internal state' is really a mismatch between the operator's ability and the task demands; for example, the operator may be unable to make visual judgments or react quickly enough to support the task at hand. Poor operator practices are another type of precondition for unsafe acts. These include poor crew resource management (issues such as leadership and communication) and poor personal readiness practices (e.g., violating the crew rest requirements in aviation).\nFour types of unsafe supervision are: Inadequate supervision; Planned inappropriate operations; Failure to correct a known problem; and Supervisory violations.\nOrganizational influences include those related to resources management (e.g., inadequate human or financial resources), organizational climate (structures, policies, and culture), and organizational processes (such as procedures, schedules, oversight).\nSome researchers have argued that the dichotomy of human actions as \"correct\" or \"incorrect\" is a harmful oversimplification of a complex phenomena (see Hollnagel and Amalberti, 2001). A focus on the variability of human performance and how human operators (and organizations) can manage that variability may be a more fruitful approach. Furthermore, as noted above, the concept of \"resiliency\" highlights the positive roles that humans can play in complex systems.\n- Gertman, D. L. and Blackman, H. S. (2001). Human reliability and safety analysis data handbook, Wiley.\n- Gertman, D., Blackman, H., Marble, J., Byers, J. and Smith, C. (2005). The SPAR-H human reliability analysis method. NUREG/CR-6883. Idaho National Laboratory, prepared for U. S. Nuclear Regulatory Commission.\n- Hollnagel, E. (1993). Human reliability analysis: Context and control, Academic Press.\n- Hollnagel, E. (1998). Cognitive reliability and error analysis method: CREAM, Elsevier.\n- Hollnagel, E. and Amalberti, R. (2001). The Emperor’s New Clothes, or whatever happened to “human error”? Invited keynote presentation at 4th International Workshop on Human Error, Safety and System Development., Linköping, June 11-12, 2001.\n- Hollnagel, E., Woods, D. D., and Leveson, N. (Eds.) (2006). Resilience engineering: Concepts and precepts, Ashgate.\n- Jones, P. M. (1999). Human error and its amelioration. In Handbook of Systems Engineering and Management (A. P. Sage and W. B. Rouse, eds.), 687-702, Wiley.\n- Kirwan, B. (1994). A practical guide to human reliability assessment, Taylor & Francis.\n- Kirwan, B. and Ainsworth, L. (Eds.) (1992). A guide to task analysis, Taylor & Francis.\n- Norman, D. (1988). The psychology of everyday things, Basic Books.\n- Reason, J. (1990). Human error, Cambridge University Press.\n- Roth, E. et al (1994). An empirical investigation of operator performance in cognitive demanding simulated emergencies. NUREG/CR-6208, Westinghouse Science and Technology Center, Report prepared for Nuclear Regulatory Commission.\n- Sage, A. P. (1992). Systems engineering, Wiley.\n- Senders, J. and Moray, N. (1991). Human error: Cause, prediction, and reduction, Lawrence Erlbaum Associates.\n- Shappell, S. & Wiegmann, D. (2000). The human factors analysis and classification system - HFACS. DOT/FAA/AM-00/7, Office of Aviation Medicine, Federal Aviation Administration, Department of Transportation..\n- Swain, A. D., & Guttman, H. E. (1983). Handbook of human reliability analysis with emphasis on nuclear power plant applications., NUREG/CR-1278 (Washington D.C.).\n- Wiegmann, D. & Shappell, S. (2003). A human error approach to aviation accident analysis: The human factors analysis and classification system., Ashgate.\n- Wilson, J.R. (1993). SHEAN (Simplified Human Error Analysis code) and automated THERP, United States Department of Energy Technical Report Number WINCO--11908. \n- Woods, D. D. (1990). Modeling and predicting human error. In J. Elkind, S. Card, J. Hochberg, and B. Huey (Eds.), Human performance models for computer-aided engineering (248-274), Academic Press.\n- Dismukes, R. K., Berman, B. A., and Loukopoulos, L. D. (2007). The limits of expertise: Rethinking pilot error and the causes of airline accidents, Ashgate.\n- Forester, J., Kolaczkowski, A., Lois, E., and Kelly, D. (2006). Evaluation of human reliability analysis methods against good practices. NUREG-1842 Final Report, U. S. Nuclear Regulatory Commission. \n- Goodstein, L. P., Andersen, H. B., and Olsen, S. E. (Eds.) (1988). Tasks, errors, and mental models, Taylor and Francis.\n- Grabowski, M. and Roberts, K. H. (1996). Human and organizational error in large scale systems\n, IEEE Transactions on Systems, Man, and Cybernetics, Volume 26, No. 1, January 1996, 2-16.\n- Greenbaum, J. and Kyng, M. (Eds.) (1991). Design at work: Cooperative design of computer systems, Lawrence Erlbaum Associates.\n- Harrison, M. (2004). Human error analysis and reliability assessment, Workshop on Human Computer Interaction and Dependability, 46th IFIP Working Group 10.4 Meeting, Siena, Italy, July 3-7, 2004. \n- Hollnagel, E. (1991). The phenotype of erroneous actions: Implications for HCI design. In G. W. R. Weir and J. L. Alty (Eds.), Human-computer interaction and complex systems', Academic Press.\n- Hutchins, E. (1995). Cognition in the wild, MIT Press.\n- Kahneman, D., Slovic, P. and Tversky, A. (Eds.) (1982). Judgment under uncertainty: Heuristics and biases, Cambridge University Press.\n- Leveson, N. (1995). Safeware: System safety and computers, Addison-Wesley.\n- Morgan, G. (1986). Images of organization, Sage.\n- Mura, S. S. (1983). Licensing violations: Legitimate violations of Grice's conversational principle. In R. Craig and K. Tracy (Eds.), Conversational coherence: Form, structure, and strategy (101-115), Sage.\n- Perrow, C. (1984). Normal accidents: Living with high-risk technologies, Basic Books.\n- Rasmussen, J. (1983). Skills, rules, and knowledge: Signals, signs, and symbols and other distinctions in human performance models. IEEE Transactions on Systems, Man, and Cybernetics, SMC-13, 257-267.\n- Rasmussen, J. (1986). Information processing and human-machine interaction: An approach to cognitive engineering, Wiley.\n- Silverman, B. (1992). Critiquing human error: A knowledge-based human-computer collaboration approach, Academic Press.\n- Swets, J. (1996). Signal detection theory and ROC analysis in psychology and diagnostics: Collected papers, Lawrence Erlbaum Associates.\n- Tversky, A. and Kahneman, D. (1974). Judgment under uncertainty: Heuristics and biases. Science, 185, 1124-1131.\n- Vaughan, D. (1996). The Challenger launch decision: Risky technology, culture, and deviance at NASA, University of Chicago Press.\n- Wallace, B. and Ross, A. (2006). Beyond human error, CRC Press.\n- Woods, D. D., Johannesen, L., Cook, R., and Sarter, N. (1994). Behind human error: Cognitive systems, computers, and hindsight. CSERIAC SOAR Report 94-01, Crew Systems Ergonomics Information Analysis Center, Wright-Patterson Air Force Base, Ohio.\nStandards and Guidance DocumentsEdit\n- IEEE Standard 1082 (1997): IEEE Guide for Incorporating Human Action Reliability Analysis for Nuclear Power Generating Stations\n- Erik Hollnagel at the Cognitive Systems Engineering Laboratory at Linkoping University\n- Human Reliability Analysis at the US Sandia National Laboratories\n- Center for Human Reliability Studies at the US Oak Ridge National Laboratory\n- Flight Cognition Laboratory at NASA Ames Research Center\n- David Woods at the Cognitive Systems Engineering Laboratory at The Ohio State University\n|This page uses Creative Commons Licensed content from Wikipedia (view authors).|"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:1e5062bc-320c-42de-92ca-cd3cd1eabd7f>","<urn:uuid:3442e786-aada-4e11-a0c1-7fe6dc843b3c>"],"error":null}
{"question":"How do the recall investigations of Death Wish Coffee and Janes Pub Style chicken products compare in terms of their testing methods and confirmation of illnesses?","answer":"Both recalls involved laboratory testing, but with different outcomes and testing approaches. Death Wish Coffee conducted preventive testing for 14 weeks with Cornell University specialists, finding no actual contamination but identifying a potential risk in their manufacturing process. In contrast, the Janes Pub Style chicken investigation used DNA testing that positively matched Salmonella in the food samples to the genetic fingerprint of human illness cases. Regarding illnesses, Death Wish Coffee had no confirmed cases, while the Janes products were linked to 18 confirmed Salmonella cases across six Canadian provinces, with one possible death and 33% of cases requiring hospitalization.","context":["Less than two months after it hit retail shelves, Death Wish Coffee Co. is recalling “Nitro Cold Brew” beverage, ceasing production and changing its manufacturing process because it may have allowed deadly botulin toxin to develop in the product.\nThe Round Lake, NY, company launched online sales of the beverage on Feb. 2, billing it as the latest “World’s Strongest Coffee” in the Death Wish Coffee line of extra-caffeinated beverages. The “nitrogen-infused” coffee debuted in retail stores about six weeks ago.\nCoffee company officials told Food Safety News on Wednesday that they have been consulting with the Process Authority at Cornell University for the past three and a half months regarding the possibility that their processing could lead to the growth of Clostridium botulinum in the Nitro Cold Brew. The company did not, however, comment on what specifically spurred the internal investigation.\n“We strive to have the best product possible. We voluntarily sent the product to the specialists at Cornell as a safety net to protect our customers and make sure they were receiving a top notch nitro brew,” said Alyssa Hardy, content manager for Death Wish Coffee.\n“Mike Brown, our CEO and founder is extremely meticulous when it comes to the products we put out, so we have them tested. He decided to do the additional testing as a precautionary measure, and as it turns out, we’re glad we did.”\nRecall notices posted on the company website and the Food and Drug Administration website offered similar explanations. The recall of the 11-ounce cans of Nitro Cold Brew includes product sold at retail and via the internet.\n“We just started selling at stores about six weeks ago — there were 3,360 cans sent to stores — and all of the Death Wish Nitro product has been recalled at this time,” Hardy said.\n“Every single person who has purchased Death Wish Nitro since we launched it has been sent an email informing them of the recall, the risks and that they will receive their money back via a mailed check.”\nNo confirmed illnesses\nThe company’s recall notice says as of Wednesday, no confirmed illnesses had been reported in connection with the Nitro Bold Brew.\n“Consumers who have purchased Death Wish Nitro should not consume it and can either dispose of it or return the product to the location with proof of purchase for a full refund,” according to the recall notice.\nHardy said Death Wish has been testing its products for 14 weeks on an ongoing basis. She said the specialists have not found C. botulinum in any of the products. She did not specifically answer whether the pathogen has been found in the company’s production facilities.\nAlthough the weeks of testing haven’t showed any contamination or “degradation of quality” of the Nitro Cold Brew, CEO Brown decided to halt production and add a step to the manufacturing process.\n“In short, it looks like our process wasn’t perfect and we’re excited to revisit it with guidance from some of the most meticulous scientists in the world,” according to a statement on the Death Wish Coffee website.\n“… a process specialist has recommended that we add an additional step to our nitro cold brew production process.\n“Nitrogen-infused coffee is a fairly new process, in which at the moment, there are few federal standards and regulations through the Food and Drug Administration.”\nThe company statement said the risk of Clostridium botulinum is something that is present with any nitrogen-based products that are low acid foods commercialized in reduced oxygen packaging.\nAdvice to consumers\nAnyone who has consumed Death Wish Coffee brand Nitro Cold Brew and developed symptoms of botulism poisoning should immediately seek medical treatment. The toxins paralyzes muscles, including those used for breathing, which can quickly become life threatening. Most victims require hospitalization and many cannot breath without ventilator support.\nAlso, anyone who has consumed the beverage in the past couple of weeks should monitor themselves because it can take up to 10 days after exposure for the symptoms to develop. Symptoms can develop in as few as six hours, but usually show up 18 to 36 hours after exposure, according to the federal Centers for Disease Control and Prevention.\nFood contaminated with the bacteria that produces the botulin toxin does not look or smell bad. The contamination cannot be identified without laboratory testing.\nPeople with botulism may not show all of these symptoms at once. The symptoms of botulism poisoning in adults include:\n- double vision;\n- blurred vision;\n- drooping eyelids;\n- slurred speech;\n- difficulty swallowing;\n- a thick-feeling tongue;\n- dry mouth; and\n- muscle weakness.\n(To sign up for a free subscription to Food Safety News, click here.)© Food Safety News","Canadian health officials are urging consumers to check their homes for certain frozen “Janes Pub Style” breaded chicken because DNA tests have matched Salmonella in the products to victims of a nationwide outbreak, which already may have killed one person.\nConsumers should not eat the products, according to a Wednesday update from the Public Health Agency of Canada. The chicken burgers and chicken nuggets were distributed coast to coast.\nA similar warning went out Tuesday from the Canadian Food Inspection Agency in the form of a recall notice for the Janes brand frozen, uncooked, breaded “Pub Style” chicken burgers and popcorn chicken produced by Sofina Foods Inc.\nFive more people have been confirmed with Salmonella infections since the previous public update, bringing the total to 18 victims across six provinces, according Wednesday post. A third of the victims have had symptoms so severe that they required hospitalization.\n“One of the ill individuals has died; however, it has not been determined if Salmonella contributed to the cause of death,” the public health agency reported. “Individuals became sick between June and September of this year. The average age of cases is 41 years, with ages ranging between 0 to 85 years.\n“Several individuals involved in the outbreak reported eating ‘Janes Pub Style Chicken Burgers’ before their illness occurred. Food samples of Janes Pub Style Chicken Burgers, 800 g, with best before date 2018 MA 12, and Janes Pub Style Snacks Popcorn Chicken, 800 g, with best before date 2018 MA 15, tested positive for Salmonella Enteritidis.\n“The positive food samples had the same genetic fingerprint — using whole genome sequencing — as the cases of human illness reported in this outbreak.”\nInvestigators faced with multiple outbreaks\nThe 18 sick people are spread across six provinces in the following numbers British Columbia 1, Alberta 1, Ontario 10, Quebec 2, New Brunswick 2, and Nova Scotia 2. That marks a 38 percent increase since Canada’s federal health agency last reported on the outbreak on Sept. 28.\nAt that time, the health agency was not naming specific products, but warning the public on general points of food safety when storing, handling and preparing frozen, uncooked, breaded chicken. The agency was anticipating additional illnesses to be confirmed because of the lag time between when a person becomes ill and when a lab-confirmed test is reported to public health officials.\nAlthough the Sept. 28 public health alert stated there had not be a recall in relation to the ongoing outbreak, a recall notice posted by the Canadian Food Inspection Agency on July 12 alluded to the outbreak.\nThat recall involved President’s Choice brand “Pub Recipe Chicken Nuggets” from Loblaw Companies Ltd. The recall notice reported there had been illnesses associated with the product. But those 13 victims became ill from April through June.\n“This recall was triggered by findings by the Canadian Food Inspection Agency (CFIA) during its investigation into a foodborne illness outbreak. The CFIA is conducting a food safety investigation, which may lead to the recall of other products,” according to the July 12 President’s Choice recall notice.\nUltimately, sample of the product collected from a retailer tested positive for Salmonella Enteritidis and had the same genetic fingerprint as the cases of human illness reported in the outbreak earlier this year.\nThe public Health Agency of Canada posted a final outbreak report Aug. 25 on the outbreak traced to the President’s Choice brand Pub Recipe Chicken Nuggets.\nAs with the Janes brand products recalled this week, concern remains that consumers may still have unused portions of the President’s Choice branded products in their homes because the product’s best-before date is March 15, 2018.\nAdvice to consumers\nAnyone can become sick with a Salmonella infection, but infants, children, seniors and those with weakened immune systems are at higher risk of serious illness because their immune systems are more fragile, according to the Canadian health agency.\nMost people who become ill from a Salmonella infection will recover fully after a few days. It is possible for some people to be infected with the bacteria and not get sick or show any symptoms, but still be able to spread the infection to others.\nAnyone who has eaten any raw, frozen, breaded chicken products and developed symptoms of Salmonella infection should seek medical attention and tell their doctors about the possible exposure to the bacteria.\nSymptoms of a Salmonella infection, called salmonellosis, typically start 6 to 72 hours after exposure to Salmonella bacteria, but in some people it takes two weeks for symptoms to develop. Symptoms include fever, chills, diarrhea, abdominal cramps, headache, nausea and vomiting. These symptoms usually last for four to seven days.\nThe public health agency recommends the following precautions when handling and preparing raw or partially cooked frozen breaded chicken products:\n- Do not eat raw or undercooked frozen breaded chicken products. Cook all frozen breaded products to an internal temperature of at least 74 degrees C (165 degrees F) to ensure they are safe to eat.\n- Microwave cooking of frozen raw breaded poultry products including chicken nuggets, strips or burgers is not recommended due to uneven heating.\n- Always follow package cooking instructions, including products labelled Uncooked, Cook and Serve, Ready to Cook, and Oven Ready.\n- Wash your hands thoroughly with soap and warm water before and after handling frozen raw breaded chicken products.\n- Use a separate plate, cutting board and utensils when handling frozen raw breaded chicken products to prevent the spread of harmful bacteria.\n- Use a digital food thermometer to verify that frozen raw breaded chicken products have reached at least 74 degrees C (165 degrees F). Insert the digital food thermometer through the side of the product, all the way to the middle. Oven-safe meat thermometers that are designed for testing whole poultry and roasts during cooking are not suitable for testing nuggets, strips or burgers.\n(To sign up for a free subscription to Food Safety News, click here.)"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:2890b2f1-2e43-4a88-8902-211bc693d205>","<urn:uuid:0daf2999-7b9a-4580-8d1e-f8857674f36a>"],"error":null}
{"question":"我想了解tall fescue的nutritional benefits和establishment requirements是什么？Can you compare这两方面in detail?","answer":"Regarding nutritional benefits, tall fescue offers high digestibility, elevated soluble sugar concentrations, and lower lignin content. It has similar nutritive value to perennial ryegrass and can maintain high feed quality, though this declines when pasture mass exceeds 3000 kgDM/ha in spring and summer. As for establishment requirements, tall fescue is slow to establish and requires specific conditions: soil temperatures must be over 12°C (ideally sown in early autumn), sowing depth must be exactly 10mm (seedlings won't emerge beyond 20mm depth), and it should be sown at 25-30 kg/ha with clovers. Importantly, it should never be mixed with ryegrass as ryegrass will dominate and eliminate the fescue.","context":["To develop improved plant materials that enhance productivity of irrigated pastures in semi-arid growing regions.\n- Release new varieties with improved nutritional quality, palatability, and livestock utilization.\n- Develop improved plants that require reduced inputs of irrigation and fertilizer.\n- Discover improved selection protocols and methods for use in forage germplasm improvement.\n- Develop new genomic resources for use in evaluation and breeding.\n- Identify the role of plant endophytes in abiotic and biotic plant stress tolerance in semi-arid growing regions.\nLine-source irrigation for drought analysis\n- Improved grass and legume cultivars that have enhanced digestibility, elevated soluble sugar concentrations, and softer, more-palatable leaves to increase animal intake, gains, and health.\n- New grasses and legumes that can survive drought and efficiently utilize limited fertilizer, are compatible in grass/legume mixtures, and will conserve water resources.\n- Research will elucidate how plant spacing, machine harvesting versus livestock grazing, and hybrid vigor affect plant selection to make forage breeding more effective and efficient.\n- Molecular biology tools used in breeding and selection that will elucidate the genetic mechanisms behind increased digestibility, soluble sugar, and soft leaves.\n- Research will determine the extent and differences in plant genetic control versus symbiotic fungal endophyte effects upon drought and other stress tolerances.\nImproved Plant and Management Practices\n- High-yielding tall fescue cultivars with improved nutritional quality (e.g., higher digestibility and soluble sugars, and lower lignin).\n- Soft-leaved fescue germplasm to be used in breeding programs to improve livestock intake and utilization of tall fescue.\n- Tall fescue cultivars with increased drought tolerance that require less irrigation to maintain high yields.\n- Tall fescue cultivars that are more compatible with nitrogen-fixing legumes for improved economic and environmental sustainability.\n- Genetic mapping in tall fescue to understand traits of interest including soft leaves, higher digestibility, and drought stress.\n- Discovery and development of evaluation and selection methods that allow simulation of seeded stands and grazed pastures.\nHighly digestible tall fescue\n- Recent release of \"Cache\" meadow bromegrass (2004) with improved yield under reduced irrigation.\n- High-yielding, drought-tolerant meadow bromegrass cultivars for dryland (very limited irrigation) pastures.\n\"Cache\" meadow bromegrass\n- Recent release of \"Don\" yellow-flowered alfalfa (2008) with extreme persistence under reduced irrigation. \"Don\" also possesses a lower growth form that mixes well with grasses to provide nitrogen for grasses.\n- High-yielding, salt-tolerant, spreading-type (rhizomatous) alfalfa cultivars which are adapted to the intensive grazing.\n- Non-bloating birdsfoot trefoil cultivars that are high-yielding and persistent under intensive grazing in irrigated pastures.\n- Drought-tolerant, upright cultivars of cicer milkvetch and kura clover as legume components in pasture mixes.\n- Identify growth factors that influence compatibility of legume/grass mixtures and their interaction with the environment.\n- Identify and map genes controlling salt tolerance in legumes.\nNon-spreading (left) and spreading (right) alfalfa\nTrefoil grazing persistence study\n- Orchardgrass germplasm with improved winter survival for high-elevation, cold-temperate regions.\n- Identify hybrid vigor groups that lead to high-yielding, persistent orchardgrass hybrids and cultivars.\n- Develop novel orchardgrass germplasm that will be used for gene mapping and incorporation of improved stress tolerance and the forage production into current populations.\nOrchardgrass grazing study\n- Assess the feasibility and utility of warm-season grasses for grazing in areas with predominantly cool-season grass pastures.\n- Develop switchgrass cultivars adapted to the irrigated pastures of the semi-arid western U.S.\nWarm-season grasses in a temperate environment\n- Collect and evaluate the potential of new fungal endophytes to improve drought, salt, and insect tolerance in grasses.\n- Develop methods to transfer potentially beneficial endophytes into pasture grass species.\nSeed available from the Utah Crop Improvement Association 1-435-797-2082","- Tall fescue has greater summer growth than perennial ryegrass, with at least similar nutritive value.\n- Grazing of tall fescue needs to be more strictly managed than perennial ryegrass to achieve its potential nutritive value.\n- It is better adapted to hot and dry conditions than perennial ryegrass due to its deeper root system and higher temperature ceiling. This gives it a potential role in low rainfall regions.\n- It can grow in less-fertile soils, is tolerant to a wider range of pH and waterlogging conditions, and can achieve higher persistency than perennial ryegrass.\nTall fescue is likely to be most successfully integrated into a dairy system where a significant proportion of the farm (25% or more) is established in fescue rather than one or two paddocks.\nPaddocks that are summer dry, have suffered insect damage, or where ryegrass has failed to persist, are ideal candidates for renovation with tall fescue.\nSome cultivars are available with an endophyte (MaxP) which can enhance persistence of tall fescue where insect pests such as Argentine stem weevil or black beetle are present. MaxP produces the alkaloids peramine and loline and is safe for cattle.\nTall fescue is slow to establish compared to ryegrass, and is sensitive to soil temperature.\nSow in early autumn (February/early March) when soil temperatures are over 12°C. Lower soil temperatures (5-100C) will result in slower germination (28 days) and slower winter growth allowing weeds to smother the tall fescue.\nTall fescue is sensitive to sowing depth. The ideal depth is 10mm and when sown more than 20mm few seedlings will emerge.\nSow tall fescue as the main grass in a pasture seed mix at 25-30 kg/ha with clover(s).\nIt should never be sown in mixture with ryegrass as ryegrass will dominate and the fescue will disappear.\nFirst grazing should be when plants are 15-20cm in height and checked for pulling. For the first 2-3 grazings if should not be grazed lower than 7cm, preferably with light stock (calves).\nIt is recommended that the same post-grazing residual target be applied to tall fescue pastures as ryegrass pastures (e.g. 1500-1600 kgDM/ha or 7-8 clicks on the rising plate meter set on the formula “clicks” x 140 + 500).\nThe feed quality of tall fescue declines when pasture mass exceeds 3000 kgDM/ha in the spring and summer period, and it can reach this more quickly than ryegrass, and once feed quality is lost it is difficult to graze well.\nIt is not recommended that dairy farmers carry autumn-grown tall fescue through to the spring as it has a low palatability. It is better to graze early winter to provide fresh growth for the spring.\nThere is no risk of ryegrass staggers from tall fescue either endophyte free or with the MaxP endophyte.\nFacial eczema risk is typically lower than ryegrass, but invasion by ryegrass will increase the risk.\nDue to tall fescue typically having high levels of clover, bloat management needs attention, especially in the first spring after autumn establishment when clover can be dominant.\nThere are no additional metabolic issues, other than those experienced with ryegrass pastures, when grazing tall fescue with dairy cows.\nMilk production from fescue\nWhile DM yields of fescue are higher than ryegrass, its lower feed value means there is little advantage in potential milksolids production on many New Zealand dairy farms, or in wet summers.\n- Tall Fescue establishment and management (FarmFact 1-28)"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:ae41c247-3a07-480e-aee1-ae750a2cb88b>","<urn:uuid:a43a91c7-b879-4138-9ea0-acfc4e0a765f>"],"error":null}
{"question":"How do the roles and responsibilities of safety personnel compare between warehouse racking management and boiler operation systems, specifically regarding maintenance and inspection duties?","answer":"In warehouse racking management, a Person Responsible for Racking Safety (PRRS) is nominated by the health and safety team to ensure racking is used, inspected, and maintained according to regulations. They oversee regular inspections and ensure operator training in equipment use and limitations. For boiler operations, safety responsibilities are more distributed and regulated, involving state-certified inspectors and skilled technicians. Boiler maintenance requires tracking multiple data points like pressure, temperature, and water chemistry, plus performing various scheduled tasks (weekly, monthly, semi-annual, and annual) according to manufacturer specifications. While both systems require designated safety personnel, boiler operations have more stringent regulatory oversight and technical maintenance requirements.","context":["Minimising the possibility of damage to pallet racking is one of the most important challenges facing the storage industry and Nolwazi is very active in promoting the safe use of storage equipment. One of the most relevant documents for the safe use of pallet racking is the ‘SEMA Code of Practice for the Use of Static Pallet Racking SEMA 2010.’ This document has various requirements which will minimise damage to your racking in the warehouse environment.\nPerson Responsible for Racking Safety (PRRS)The warehouse health and safety management team should nominate a competent person to be responsible for racking safety. The PRRS is responsible for ensuring that the racking is used, inspected and maintained in accordance with the appropriate regulations and guidelines.\nOperator TrainingIt is also the warehouse health and safety management team’s responsibility to ensure that the operators are trained in the appropriate use and limitations of the storage equipment. It is the responsibility of the warehouse health and safety management to maintain the racking in reasonable condition. Comprehension and effective driver training will minimise the possibility of any accidents.\nRack ProtectionEnd frame protectors are recommended in warehouses where reach trucks or forklifts are operating. It is highly recommended that protectors for end frames between gangways (a raised platform or walkway, for pedestrians) aisle, and bridge bays are in place and maintained. Other racking protection requirements should be considered as identified by a risk assessment. The warehouse health and safety management team should be aware of the implications of retrofitting protection devices which reduce operating clearances and can, in some circumstances, lead to an increase in the amount of damage.\nInspection RequirementsRegular inspections of your pallet racking are required. The inspection should follow a hierarchical approach using three levels of inspection as follows;\n- Damage inspection by warehouse operatives\n- Weekly inspections as a visual check from ground level\n- Monthly or annual inspection by a ‘technical competent’ person or company\nMaintenanceAny damaged component, noted during inspection as requiring repair or replaced, should be taken out of use in accordance with SEMA guidelines and repaired or replaced by suitably trained personnel as required.\nTypical racking protection requirements are shown in Figures 1 and 2.\nFigure 1. Plan view of racking showing typical protection requirements\nFigure 2. Elevation of racking showing typical protection requirements at a bridge bay\nRacking ProtectionRack protectors should be regarded as a ‘last resort’ means of avoiding damage and other methods of damage prevention should be considered before taking the decision to use rack protectors. The protection of racking is not just dependant on physical rack protectors, but relies on many items including;\n- The specification\n- The design of the system\n- The installation\n- The defined responsibilities of the person responsible for racking safety\n- The training of the operatives\n- The inspection procedure\n- The maintenance procedure\n- The type of damage to be protected against\n- Whether other methods of protection are more appropriate\n- The type of protector required\n- Whether the protector will reduce clearances, potentially leading to more damage\n- Whether the protector may hide potentially serious damage\n- Whether the protector may lead to less reporting of damage\n- Whether the protector may result in operatives using the protector as a buffer","Boiler Efficiency and Safety Specialists\nWe repair, install and service all brands of boilers and burners. We provide preventative service agreements for annual and customized maintenance tailored to meet your needs. We have the most experienced technicians in the State of Oregon and are able to complete certified inspections meeting WA clean air act requirements.\nComplete boiler room services including:\nWe repair, install and service all brands of boilers and burners\n- A.O. Smith\n- Advanced Thermal Hydronics\n- AERCO Benchmark\n- Cleaver Brooks/Superior/B & W/Gabriel/Kewanee/Fulton and many others\nFireye/Siemens combustion controls\n- Gordon-Piatt/IC/Weishaupt/Beckett/Riello/Power Flame Oil and Gas Burners\n- Industrial Combustion Authorized Factory Burner Representative for the Pacific NW\n- HydroTherm KN\n- Power Flame\n- Ray Oil & Gas Burners\nParallel Positioning and O2 Trim -- Increase Efficiency and Optimize Fuel-to-Air Ratio\nParallel positioning controls help a burner to optimize its fuel-to-air ratio by using dedicated motorized actuators for the fuel and air valves. These parallel positioning controls directly tie into the electronic firing rate control, enabling more efficient and consistent performance. Adding O2 trim helps minimize excess air and keeps the burner from going excessively rich, which leads to inefficient combustion. Among the other benefits:\n- Parallel positioning saves up to 3% in fuel costs by eliminating hysteresis\n- O2 trim increases efficiency by up to 2%\n- Variable speed drive can help save as much as 40% in electrical costs\nParallel positioning (often referred to as “linkage-less” control) is a precise, repeatable method of controlling the flow of fuel and air through a modulating burner. The fuel control valve and air damper are each moved by their own dedicated stepper motor actuators, each of which is electronically controlled by an air-fuel ratio controller.\nOn the other hand, a “linkage” burner uses a single servo motor to modulate both the fuel valve and air damper which are mechanically connected together by a series of adjustable jackshafts and linkage arms. The difference is similar to how modern electronic fuel injection systems compare to carburetors in automotive engines.\nWhen air and fuel flow are parallel position systems, when these elements are accurately controlled, a well-designed burner can be precisely tuned to minimize leftover oxygen in the exhaust throughout the full turndown range. This leftover oxygen in the exhaust is referred to as “excess air” and is inversely proportional to combustion efficiency. That means that lower excess air means lower fuel consumption.\nThere are many factors that can affect the net fuel savings realized by switching to parallel positioning controls. A typical estimate would be 3%-5%, although savings of 15% or more are quite possible in certain circumstances. Linkage-less platforms like the Fireye Nexus, Weishaupt W-FM and Siemens LMV are generally known to be safer, more reliable, and require less maintenance over time than conventional burner controls.\nBoiler inspections, operation, and maintenance\nBoilers can be extremely dangerous if not properly maintained, the processes for inspecting them are rigorous and required by law by the state or insurance companies.\nThe State of Oregon requires that Operational Inspections be done by a State Boiler Inspector or Insurance Inspector. The frequency of inspections varies depending on the type of boiler or pressure vessel. N.C.C. is able to complete certified inspections to meet the state of Washington annual Clean Air Act testing and reporting requirements.\nBoiler inspections typically include all internal and external walls and surfaces which are inspected; looking for any signs of leaks, corrosion, overheating, or other structural issues within the boiler. All waterside areas of the boiler are inspected, including blow-down, water connections, and steam connections. All fireside conditions are inspected inside the boiler, including superheaters, deaerators, and economizers. All external trim and control devices are also inspected.\nBoiler Operation consists of many tasks to keep a boiler operating safely and efficiently. These tasks include tracking many data points, such as boiler pressure and temperature, boiler exhaust temperature, feedwater pressure and temperature, and boiler and water column blowdown timing. Continual checks should also be a part of any maintenance program, including burner flame for proper combustion, proper operation of control valves for all systems, feedwater tank or deaerator operation and level, water treatment systems, and taking water samples for comparison to recommended chemistry guidelines.\nTypical annual maintenance items\nThere are other equally important tasks to ensure proper boiler operation that should be performed on a weekly, monthly, semi-annual, and annual basis in accordance with the manufacturers and insurance recommendations. Failure to comply with the equipment manufacturer’s recommended maintenance schedule may result in equipment failure due to problem indicators being missed from lack of regular maintenance. Keeping a boiler operator’s log helps to ensure all data points are recorded, all checks are performed, and poor performance indicators are addressed.\nEvery Model and Manufacturer have different recommended annual maintenance procedures. Our highly experienced and skilled boiler technicians know the right maintenance and frequency for the various types of boilers and different brands. Whether it is checking for leaks or cleaning condensate traps, our experts have safety first and efficiency as their priorities."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:5cbf9587-22c3-4e51-95d6-97eefb47fce0>","<urn:uuid:5a9f80e8-6f35-4da4-876c-43693ef2f832>"],"error":null}
{"question":"How do UCSD's Superfund Program and Ohio Wesleyan's Environmental Science program compare in terms of their interdisciplinary approach to environmental education?","answer":"Both programs emphasize interdisciplinary environmental education but with different focuses. UCSD's Superfund Program integrates biology, chemistry, and environmental sciences through multiple graduate training programs including Chemistry & Biochemistry, Biological Sciences, Biomedical Sciences, and Neurosciences. The program trains PhD students to become environmental health researchers. Ohio Wesleyan's program takes a natural science approach within liberal arts, combining courses across disciplines like botany, zoology, geology, and chemistry. It includes both natural science requirements and social science/humanities components, preparing students for graduate studies with a whole-organism focus.","context":["The Training Core of the UC San Diego Superfund Program supports the interdisciplinary education of junior scientists to become the next generation of environmental researchers in the Environmental Health Sciences. It is a multidisciplinary program at the crossroads of biology, chemistry and environmental sciences which converge in our Superfund program. Our faculty members are affiliated with one or more of several graduate training programs: Chemistry &Biochemistry, Biological Sciences, Biomedical Sciences, and Neurosciences at UCSD or The Scripps Research Institute graduate program, each of which is independently organized and most are multidisciplinary and multi-departmental. Thus, while the supported graduate students may be based in dramatically different scientific disciplines, they share focused interest in environmental health sciences and participate in educational opportunities through meetings, seminar programs, the Community Engagement Core, the Research Translation Core, teaching opportunities, and required classes focused in molecular toxicology and toxicogenomics, ethics, statistics, and environmental health. Training students from a variety of graduate programs in interdisciplinary approaches to environmental sciences is novel and unique. Our program fosters predoctoral training that specifically addresses issues related to environmental health and such programs are critical for the preparation of young investigators to undertake the research so important to improving our environment and providing key information related to toxics in human health. Thus, the interdisciplinary training of Ph.D. graduate students within the extremely rich environment of the UCSD Superfund Program Center, will continue to create a cadre of dedicated, cutting-edge Environmental Health research leaders for the future.\nThe next generation of environmental health scientists will need to integrate a broader array of skill sets than any other in history. As our environmental challenges become greater, the cadre of scientific professionals required to analyze and solve them will need to be trained in state-of-the-art technologies in a wide variety of fields such as those represented by our Superfund laboratories at UCSD.\n|Patel, Niraj S; Doycheva, Iliana; Peterson, Michael R et al. (2015) Effect of weight loss on magnetic resonance imaging estimation of liver fat and volume in patients with nonalcoholic steatohepatitis. Clin Gastroenterol Hepatol 13:561-568.e1|\n|Seki, Ekihiro; Schwabe, Robert F (2015) Hepatic inflammation and fibrosis: functional links and key pathways. Hepatology 61:1066-79|\n|Fan, Weiwei; Evans, Ronald (2015) PPARs and ERRs: molecular mediators of mitochondrial metabolism. Curr Opin Cell Biol 33:49-54|\n|Yang, Ling; Roh, Yoon Seok; Song, Jingyi et al. (2014) Transforming growth factor beta signaling in hepatocytes participates in steatohepatitis through regulation of cell death and lipid metabolism in mice. Hepatology 59:483-95|\n|Kunz, Hans-Henning; Gierth, Markus; Herdean, Andrei et al. (2014) Plastidial transporters KEA1, -2, and -3 are essential for chloroplast osmoregulation, integrity, and pH regulation in Arabidopsis. Proc Natl Acad Sci U S A 111:7480-5|\n|Schnabl, Bernd; Brenner, David A (2014) Interactions between the intestinal microbiome and liver diseases. Gastroenterology 146:1513-24|\n|Maruo, Yoshihiro; Morioka, Yoriko; Fujito, Hiroshi et al. (2014) Bilirubin uridine diphosphate-glucuronosyltransferase variation is a genetic basis of breast milk jaundice. J Pediatr 165:36-41.e1|\n|Dowding, J M; Song, W; Bossy, K et al. (2014) Cerium oxide nanoparticles protect against A?-induced mitochondrial fragmentation and neuronal cell death. Cell Death Differ 21:1622-32|\n|Roybal, Lacey L; Hambarchyan, Arpi; Meadows, Jason D et al. (2014) Roles of binding elements, FOXL2 domains, and interactions with cJUN and SMADs in regulation of FSH?. Mol Endocrinol 28:1640-55|\n|Nakagawa, Hayato; Umemura, Atsushi; Taniguchi, Koji et al. (2014) ER stress cooperates with hypernutrition to trigger TNF-dependent spontaneous HCC development. Cancer Cell 26:331-43|\nShowing the most recent 10 out of 221 publications","Apr 24, 2019\nEnvironmental Science at Ohio Wesleyan is an interdisciplinary, natural science approach to the environment within the context of the liberal arts. Diverse courses provide you with an integrative, scientific understanding of the environment with an emphasis on approaches to addressing current environmental challenges.\nOnce you’ve taken your core classes, you can explore the environment through upper-level natural science courses in a wide range of disciplines. You’ll gain a whole-organism focus with courses that will prepare you for graduate studies in science. And every major will complete a capstone project.\n• Students will be able to explain how a wide range of interdisciplinary factors (social sciences, natural sciences and humanities) contribute to environmental issues.\n• Students will understand the relationship and connections between diverse natural sciences in the context of environmental issues. For example, what kinds of tools, research methods, and perspectives do the geological vs. the biological sciences contribute to our understanding of climate change?\n• Students will develop skills in (1) experimental design, (2) data analysis and interpretation, (3) using scientific instruments and techniques in the context of exploring and solving environmental problems. Examples of problems with an environmental dimension where natural science could be applied include environmental management, ecological restoration, monitoring and assessment, natural hazard response and mitigation, cultural conflicts, the built environment, ethical issues, human health, natural resource extraction, poverty, and war.\n• Students will develop skills in detecting and conceptualizing complex connections within and across disciplines in real-world environmental issues based on their experiences with engaged projects. For example, the contributions of botany, zoology, geology and chemistry in tandem with social science perspectives and interpersonal relations could be explored in a watershed restoration project. This implies a capacity to engage in real-world problem solving.\n• Students will be able to connect global environmental concerns to local places and communities and address environmental problems with natural science methodologies in a global context and from diverse cultural and geographic perspectives.\nCore Requirements (6.5 units) for Environmental Science major:\n- BOMI 233: Ecology and the Human Future\n- ENVS 110: Introduction to Environment & Sustainability\n- ENVS 198/498: Conversations: Toward a Sustainable Future (0.25 units)\n- BIOL 122 (+lab): Organisms and their Environment\n- CHEM 110 (+lab): General Chemistry I\n- GEOL 110: Physical and Environmental Geology\n- MATH 105: Basic Probability & Statistics or MATH 230: Applied Statistics or MATH 200.3: Biostatistics or PSYC 210: Quantitative Methods in Psychology\nIndependent Project (1 unit): A significant project developed in consultation with the Director of Environmental Science consisting of at least one unit of independent study (ENVS 490 or equivalent) or apprenticeship (ENVS 495 or equivalent). Project may be the outgrowth of travel learning courses, summer science research, theory into practice grant projects, internships, etc. Project will be refined and presented as part of ENVS 198/498 taken during the senior year.\nOne (1) social science / humanities unit from:\n- ECON 366: Environmental and Natural Resource Economics\n- GEOG 249: Weather and Climate\n- HIST 300.5: Introduction to Environmental History or HIST 350c Black Death\n- PG 280: Environmental Politics\n- PHIL 250: Environmental Ethics\nSix (6) additional natural science or physical geography classes selected from the options below. Students may chose Option 1 or Option 2:\nOption 1: One (1) introductory natural science unit from:\n- BIOL 120 (+lab): Cell Biology\n- BOMI 103 (+lab): Biology of Cultivated Plants\n- BOMI 125 (+lab): Introduction to Microbiology\n- CHEM 111 (+lab): General Chemistry II\n- GEOL 112: History of the Earth\n- MATH 110: Calculus I\n- MATH 111: Calculus II\n- PHYS 115 (+lab): Principles of Physics I\nPlus (5) of the upper level natural science/physical geography units below (no more than three from the same discipline) OR\nOption 2: Six (6) natural science/physical geography units (no more than three from the same discipline):\n- BOMI 252 (+lab): Biodiversity of Flowering Plants\n- BOMI 255 (+lab): Tropical Biology\n- BOMI 337 (+lab): Adaptive Biology of Plants\n- BOMI 344 (+lab): Plant Communities and Ecosystems\n- BOMI 355 (+lab, travel): Plant Responses to Global Change\n- CHEM 260 (+lab): Organic Chemistry I\n- CHEM 261 (+lab): Organic Chemistry II\n- CHEM 270 (+lab): Analytical Chemistry\n- GEOG 353: Cartography or GEOG 355: Geographic Information Systems (Social Science Credit)\n- GEOG 369: Remote Sensing of the Environment (Social Science Credit)\n- GEOG 375: Climate Change (Social Science Credit)\n- GEOL 270: Economic Geology\n- GEOL 275: Hydrogeology\n- GEOL 280: Volcanology\n- GEOL 285: Tectonics\n- GEOL 330 (+lab): Sedimentology and Stratigraphy\n- MATH 200.2: Mathematical Modeling of Climate Change\n- MATH 280: Differential Equations\n- ZOOL 311: Invertebrate Zoology\n- ZOOL 313: Entomology\n- ZOOL 341: Ornithology (+lab)\n- ZOOL 345: Marine Biology (travel)\n- ZOOL 347: Population and Community Ecology (+lab)\n- ZOOL 349: Island Biology (travel)\n- ZOOL 353: Conservation Biology (+lab)"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:88d083d9-6c06-4301-b0eb-3359c8fc0767>","<urn:uuid:2770fb82-84dd-43e3-ab1e-6a0d068daac8>"],"error":null}
{"question":"What is Japanese barberry's role at Dia:Beacon museum? And how does this plant affect natural environments in Colonial Forest regions?","answer":"At Dia:Beacon museum, Japanese barberry is used in tiers of planters adjacent to the Serra sculpture gallery, contributing to the artistic design by echoing the boxy cubes of hornbeams and Judd's wooden boxes. However, in natural environments, Japanese barberry is considered an exotic invasive shrub that can tolerate various site and soil conditions, creating dense stands that outcompete native trees and herbaceous plants, making it a problematic species in Colonial Forest regions.","context":["Robert Irwin's Gardens at Dia:BeaconArtist Robert Irwin designed the gardens at Dia:Beacon and the result is a beautiful echo of both the building—a former factory—and the art inside, including works by Richard Serra and Donald Judd.\nThis past weekend, I went to visit Dia:Beacon for the first time. The museum, which features art from the 1960s and onwards in a former box printing factory, is huge, with room after room devoted to monumental works of art that would be difficult to fit in many other museums.\nPhoto by: Adam Kuban.\nWork that might otherwise be lost in a conventional setting is part of a greater context at the Dia, with Donald Judd's fifteen plywood boxes—a play on variations on a theme—reminding me of Walter De Maria's Silver Meters, a long march of silver and gold plates down a narrow gallery at the front of the museum.\nI have always been awed by Richard Serra's Torqued Ellipses, made up of giant canyon wall-like pieces of curved steel, but when I saw the Ellipses after Joseph Beuys's Brazilian Fonds, Beuys's almost claustrophobic felt and metal sculptures, and the immense depths of Michael Heizer's North, East, South, and West, (above) I had a much a better sense of how Serra, Beuys, and Heizer were all part of the same 20th-century spectrum.\nYou can see the tops of the cherry trees in the background of the photo. Photo by: Claire Lui.\nThe gardens at Dia:Beacon were designed not by a landscape architect, but by artist Robert Irwin. After spending several hours inside Dia, I was able to see that the gardens were a direct extension of the art and the building itself. Four quadrants of hornbeams outside the museum are a living and breathing echo of the rhythm and cadences of the work inside—the squared-off shape of the trees, the sturdy repetition of the trunks, and the changing perception of light and shadows when I walked through the trees. Metal edges for the planters also were a reminder of the metal parts on Beuys's Brazilian Fonds, art pieces named for plants, but made of felt, and of the curved metal of Serra's Ellipses.\nThe long rolling lawn alongside the hornbeams is adjacent to a side garden of the museum, where cherry trees form two long rows—themselves forming lines parallel to the long interior galleries. On the west side of the museum, Japanese barberry* fills out tiers of planters that abut the huge gallery space for the Serra sculptures, once again a play on the boxy cubes of hornbeams and Judd's fifteen wooden boxes.\nPhoto by: Adam Kuban.\nThe walkway to the museum is lined with concrete pavers filled in with grass that has grown to cover most of the concrete in wide swaths, but then bursts in staccato diamonds along the edge of the walkway, forming a pattern of gray lattice work interrupted by the pointilist dots of grass. The idea of alternating opacity and clarity is repeated inside, where Irwin replaced panes of frosted glass with panes of clear glass, framing the verdant outdoors from indoors.\nIrwin said in an interview with the New York Times, ''I lived in a little town across the river for a year, and went there every day,'' Mr. Irwin said. ''I work experientially. I start in the space and walk through it a thousand times, just sort of running my hands over the whole thing,\" and the resulting landscape shows the touch of a man who lived with the building and its artwork every day and created a garden in the same spirit.\n*Though Japanese barberry is considered invasive, and from a gardening perspective, rather than a design perspective, perhaps not the best choice.\nSee more New York gardens","Nonnative invasive species are a threat across the entire country and our team of NWTF regional biologists, including Doug Little, Mitchel Blake and Cully McCurdy, reveal how dangerous invasives are in the northeast states from Maine to West Virginia.\nAn invasive species is a plant, fungus or animal that is not native to a specific location and has a tendency to spread and cause damage to the environment. Invasives affect the habitat of wild turkeys and other wildlife. They compete for resources and spell trouble for all native plants and animals by causing death or by degrading habitat.\n“Nonnatives can quickly take over old fields that otherwise would provide quality brood habitat for wildlife,” Little said. “For example, autumn olive and bush honeysuckle outcompete beneficial plant species that turkeys may forage on or around.”\nBiologists in the Colonial Forest states say invasive species are just as big of a concern as the lack of old field habitats and the decreasing percentage of forest land that is made up of mature forests. The deficiency of young forest cover brings down the diversity in this region, causing trouble for certain species and the environment.\nThe NWTF is taking the fight to invasives. On the Finger Lakes National Forest, the NWTF New York State Chapter and the U.S. Forest Service partnered on a project in the Curatola area to reduce invasive species. The land consists mainly of successional old forests, and a good majority of the unit is invaded by all forms of nonnative plants, prohibiting native shrubs and species from flourishing. Crews are working hard to eradicate nonnative species using chemical treatments.\nThis project and many others conducted by state or federal agencies are large-scale and not conducive to volunteer assistance. Little recommends that private landowners have professional foresters develop forest management plans to treat, control and eradicate invasive species on their land.\nIf you don’t have land of your own, becoming an NWTF member helps the organization and its partners with a wide variety of habitat enhancement projects, including the control of invasive species to improve habitat for wild turkeys and other species.\nSome of the most common and problematic species from the Colonial Forests include:\n- Multi-flora rose – Introduced for ornamental use and erosion control but forms dense thickets that invade pastures and crowds out native species.\n- Bush honeysuckle – This aggressive shrub is shade tolerant and forms dense thickets that block sunlight, preventing native plants from growing underneath.\n- Autumn olive – This woody, nonnative invader creates a monoculture that prohibits native plants from flourishing.\n- Japanese knotweed – Native to Asia, this noxious invasive weed spreads rapidly through areas overtaking native species, especially along waterways.\n- Glossy buckthorn – An invasive shrub of hardwood forests that spreads rapidly, shading out native shrubs and forest plants.\n- Japanese stiltgrass – This plant threatens native understory vegetation and spreads quickly through disturbed shaded areas, displacing native wetland and forest vegetation.\n- Mile-a-minute – First introduced in the 1930s, this weed produces a thick tangle of vines, blocking sunlight and killing the plants underneath.\n- Oriental bittersweet – Smothering native plants, this invasive species grows as a vine and often uproots trees due to its weight.\n- Japanese barberry – This exotic invasive shrub can tolerate a range of varying site and soil conditions, creating a dense stand that outcompetes native trees and herbaceous plants.\nIf you want to take action and help combat the spread of invasive species in your area, read the following guidelines:\n- Learn to properly identify invasives, research treatment methods and treat nonnative species on your property.\n- Don't drive vehicles, including ATVs and mowers, through patches of invasives, and clean equipment of vegetation and dirt before moving to a new area to avoid the spread of seeds.\n- Don't plant ornamental invasive species in your yard where they can spread to surrounding wild lands.\n- Thoroughly clean boats of vegetation and drain live wells when traveling between lakes.\nIf you’d like to learn more about nonnatives in your area, visit your state agency website for rules and regulations restricting the transport of species, recommended treatments and more."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:8ab5d950-1304-4204-8f4a-580f157ffefc>","<urn:uuid:ec283192-e97f-43a1-bbca-4316f3fe873c>"],"error":null}
{"question":"What are the immediate environmental impacts of mining on local ecosystems, and how does it affect water quality in surrounding areas?","answer":"Mining immediately disrupts ecosystems by causing habitat loss, deforestation, and erosion. It destroys natural landscapes and can wipe out wildlife populations. As for water quality, mining waste contains chemical compounds that contaminate water sources. Rainwater falling on mining sites becomes enriched with heavy metals like lead, aluminum, chromium, manganese and selenium. While companies construct filtration ponds, contaminated water still flows downstream. In severe cases, accidents like slurry pond failures can release millions of gallons of contaminated water, killing aquatic life and contaminating drinking water sources.","context":["Normally, he went on, \"those insects are going to fly back into the woods as adults, and everybody in the woods is going to eat them. And that happens in April and May, at the same time you have the breeding birds coming back, the same time the turtles and toads are starting to breed. Everything is coming back in around the stream because that's a tremendously valuable food source.\"\nBut a stream buried beneath a valley fill no longer supports such life, and the effects reverberate through the forest. A recent EPA study showed that mayflies—among the most fecund insects in the forest—had largely disappeared from waterways downstream from mountaintop mining sites. That might seem a small loss, but it's an early, critical break in the food chain that, sooner or later, will affect many other animals.\nMountaintop mining operations, ecologists say, fracture the natural spaces that enable dense webs of life to flourish, leaving smaller \"islands\" of unspoiled territory. Those become biologically impoverished as native plants and animals die and invasive species move in. In one study, EPA and U.S. Geological Survey scientists who analyzed satellite images of a 19-county area in West Virginia, eastern Kentucky and southwestern Virginia found that \"edge\" forests were replacing denser, greener \"interior\" forests far beyond the mountaintop mining-site borders, degrading ecosystems across a wider area than previously thought. Wildlife is in decline. For instance, cerulean warblers, migratory songbirds that favor Appalachian ridgelines for nesting sites, have dropped 82 percent over the past 40 years.\nThe mining industry maintains that former mining sites can be developed commercially. The law requires that the mining company restore the mountaintop's \"approximate original contour\" and that it revert to forestland or a \"higher and better use.\" A company can get an exemption from the rebuilding requirement if it shows that a flattened mountain may generate that higher value.\nTypically, mining companies bulldoze a site and plant it with a fast-growing Asian grass to prevent erosion. One former surface mine in West Virginia is now the site of a state prison; another is a golf course. But many reclaimed sites are now empty pasturelands. \"Miners have claimed that returning forestland to hay land, wildlife habitat or grassland with a few woody shrubs on it was ‘higher use,'\" says Jim Burger, a professor of forestry at Virginia Tech. \"But hay land and grassland is almost never used for that [economic] purpose, and even wildlife habitat has been abandoned.\"\nSome coal companies do rebuild mountains and replant forests—a painstaking process that takes up to 15 years. Rocky Hackworth, the superintendent of the Four Mile Mine in Kanawha County, West Virginia, took me on a tour of rebuilding efforts he oversees. We climbed into his pickup truck and rolled across the site, past an active mine where half a hillside had been scooped out. Then the twisting dirt road entered an area that was neither mine nor forest. Valley fills and new hilltops of crushed rock had been covered with topsoil or \"topsoil substitute\"—crushed shale that can support tree roots if loosely packed. Some slopes had grass and shrubs, others were thick with young sumacs, poplars, sugar maples, white pines and elms.\nThis type of reclamation requires a degree of stewardship many mine companies have not provided, and its long-term ecological impact isn't clear, especially given the stream disruptions caused by valley fills. And it still faces regulatory hurdles. \"The old mind-set is, we've got to control erosion first,\" Hackworth said. \"So that's why they want it walked real good, packed real good. You plant grass on it—which is better for controlling erosion, but it's worse for tree growth. It's a Catch-22.\"\nSome landowners have made stabs at creating wildlife habitats at reclaimed sites with pools of water. \"The small ponds are marketed to the regulatory agencies as wildlife habitat, and ducks and waterfowl do come in and use that water,\" said Orie Loucks, a retired professor of ecology at Miami University of Ohio who has studied the effects of mountaintop removal. \"It's somewhat enriched in acids, and, of course, a lot of toxic metals go into solution in the presence of [such] water. So it's not clear the habitat is very healthy for wildlife and it's not clear many people go up on these plateau areas to hunt ducks in the fall.\"\nMountaintop mining waste contains chemical compounds that otherwise remain sealed up in coal and rock. Rainwater falling on a valley fill becomes enriched with heavy metals such as lead, aluminum, chromium, manganese and selenium. Typically, coal companies construct filtration ponds to capture sediments and valley-fill runoff. But the water flowing out of these ponds isn't pristine, and some metals inevitably end up flowing downstream, contaminating water sources.\nMountaintop sites also create slurry ponds—artificial lakes that hold the byproducts of coal processing and that sometimes fail. In 2000, a slurry impoundment in Kentucky leaked into an underground mine and from there onto hillsides, where it enveloped yards and homes and spread into nearby creekbeds, killing fish and other aquatic life and contaminating drinking water. The EPA ranked the incident, involving more than 300 million gallons of coal slurry, one of the worst environmental disasters in the southeastern United States. After a months-long cleanup, federal and state agencies fined the impoundment owner, Martin County Coal, millions of dollars and ordered it to close and reclaim the site. Officials at the U.S. Mine Safety and Health Administration later conceded that their procedures for approving such sites had been lax.","Mining has several bad effects. It leaves behind a huge hole after mining is done. Secondly it damages natural beauty. A beautiful landscape which once existed is now a huge piece of dug up earth.\nEnvironmental Effects. Environmental issues can include erosion, formation of sinkholes, loss of biodiversity, and contamination of soil, groundwater and surface water by chemicals from mining processes. In some cases, additional forest logging is done in the vicinity of mines to create space for the storage of the created debris and soil.\nThe effects of mining in Africa have left large-scale devastation when companies do not honour their responsibility. Because mining areas are left in an unsustainable condition, plant species and wildlife are threatened and these areas are at risk of becoming lifeless wastelands.\nThe Impact and Effect of Illegal Mining (galamsey) towards the Socio-economic Development of Mining Communities: A Case Study of Kenyasi in the Brong Ahafo Region Adjei Samuel1, N.K.Oladejo1, I.A. Adetunde2, * 1University for Development Studies, Department of Mathematics, Navrongo. Ghana.\nSome of the major effects of mining on the environment are as follows: Minerals are the natural resources which play an important role in the economic development of the country. But the extraction and mining of these natural resources leads to some adverse effect on our environment as well.\nMar 09, 2017· The mining industry has the potential to disrupt ecosystems and wipe out wildlife populations in several different ways. Here's how mining affects the environment and wildlife. Habitat Loss; Mining can lead to the destruction of habitats in surrounding areas. The …\nModern mining is an industry that involves the exploration for and removal of minerals from the earth, economically and with minimum damage to the environment. Mining is important because minerals are major sources of energy as well as materials such as fertilizers and steel.\nApr 25, 2017· Mining is the extraction of minerals and other geological materials of economic value from deposits on the earth. Mining has the potential to have severely adverse effects on the environment including loss of biodiversity, erosion, contamination of surface water, ground water, and soil.\nSome gold can be found by panning in rivers; heavy gold will remain in the pan, whereas lighter rocks and minerals float out. This small-scale form of gold mining has little effect on the body of water, but the large-scale practice of mining gold from ore can have tremendous negative effects on water quality.\nMining can effect the earth because first, deforestation, and because mining requires large portions of land to be removed before they can start mining, lots of trees and plants are removed.\n1.1 PHASES OF A MINING PROJECT There are different phases of a mining project, beginning with mineral ore exploration and ending with the post-closure period. What follows are the typical phases of a proposed mining project. Each phase of mining is associated with different sets of environmental impacts. 1.1.1 Exploration\nFeb 07, 2018· The effects in such cases can be devastating for the environment. Be it due to ignorance of the regulations or just a freak accident, incidents like the Guyana spill of 1995 may occur again. This highlights the fact that issues like mining's effect on the environment are worth some serious deliberation.\nAug 26, 2010· Dust, radon and mercury impact miners' health. Dust, radon and mercury impact miners' health. ... Miners Face Health Risks, Even on Good Days ... mining …\nThe effects of mining coal on the environment. There are 2 ways to mine coal – Strip Mining and Underground Mining – both ways have their own impact to the environment and health. We know it but coal is such a cheap energy source that we don't want to let go of it. The negative effects of coal mining cannot be disputed:\nApr 21, 2019· The human health effects due to cyanide leach gold mining are not well documented, and this is no exception in Montana. The State of Montana has done no formal studies to specifically study mine-related health effects. Pegasus, the last mining company at Zortman-Landusky, started to fund a health study with the $1.7 million supplemental money from the 1996 settlement, but because …\nADVERTISEMENTS: Some of the major environmental effects of mining and processing of mineral resources are as follows: 1. Pollution 2. Destruction of Land 3. Subsidence 4. Noise 5. Energy 6. Impact on the Biological Environment 7. Long-term Supplies of Mineral Resources. Mining and processing of mineral resources normally have a considerable impact on land, water, […]\npositive and negative effects of mining on the environment. Mankind has been mining for precious metals since 42000 years ago and that's a staggeringly long time ago and that's exactly how long our species has been digging into the ground, to harvest its precious metals.\nDownload Coal Mining sounds ... 76 stock sound clips starting at $2. Download and buy high quality Coal Mining sound effects. BROWSE NOW >>>\nMining affects the environment by exposing radioactive elements, removing topsoil, increasing the risk of contamination of nearby ground and surface water sources, and acidification of …\nApr 20, 2015· Effects of Mining. Coal mining, the first step in the dirty lifecycle of coal, causes deforestation and releases toxic amounts of minerals and heavy metals into the soil and water. The effects of mining coal persists for years after coal is removed.\nJul 25, 2018· Environmental impacts from fossil fuel pollution are rapidly increasing in regions that have the highest concentrations of fuels. There are multiple effects of mining fossil fuels. Drilling and mining practices take a substantial toll on local water sources, biologic life and natural resources.\nPublished by the American Geosciences Institute Environmental Awareness Series. ... How can metal mining impact the environment? PDF version. Material adapted from: Hudson, T.L, Fox, F.D., and Plumlee, G.S. 1999. Metal Mining and the Environment, p. 7,20-27,31-35,38-39. Published by the American Geosciences Institute Environmental Awareness Series.\nMining operations usually create a negative environmental impact, both during the mining activity and after the mine has closed. Hence, most of the world's nations have passed regulations to decrease the impact. Work safety has long been a concern as well, and …\nEffects of mining on aquatic resources are both physical and chemical in nature. Most of earthmoving activities of mining occurred well before the enactment of laws designed to protect aquatic resources - particularly the 1977 Federal Water Pollution Control Act.\nThe former is known as underground mining, the latter as strip mining or mountaintop removal. Either process contributes a high level of damage to the environment: #12 Noise pollution. One of the most obvious (albeit perhaps least harmful) environmental effects of coal mining is noise pollution.\nMining has an adverse effect on soil quality. Soil degradation is the prime impact. Another impact is deforestation and loss of fauna and flora.\nThe impact of mining on the environment and the effects of mining techniques need to be more advanced with the utilization of modern equipment to be unintrusive to the environment. Economic growth is high on the agenda of leading countries, sustaining …\nMining is an inherently invasive process that can cause damage to a landscape in an area much larger than the mining site itself. The effects of this damage can continue years after a mine has shut down, including the addition to greenhouse gasses, death of flora and fauna, and erosion of land and habitat.\nNov 14, 2016· After mining is over, the land is left as barren land. The effects of mining sometimes vary depending on what is mined out, but these are some of the general effects you will see in all mine-areas. I'm not an expert when it comes to health impact on miners, but here are some of the things I know will affect them-\nJul 08, 2017· In coal mining, the extraction, crushing, and transport of coal can generate significant amounts of airborne respirable (extremely fine) coal dust. Dust less than 10 microns in size (cannot be seen with the eye). In non-coal mining, stone, and san...\nEnvironmental impacts of mining can occur at local, regional, and global scales through direct and indirect mining practices. Impacts can result in erosion, sinkholes, loss of biodiversity, or the contamination of soil, groundwater, and surface water by the chemicals emitted from mining processes. These processes also have an impact on the atmosphere from the emissions of carbon which have ...\nApr 04, 2017· The Dangerous Effects of Illegal Mining. April 4, 2017 Environmental Issues Written by Greentumble. Illegal mining has been ravaging our planet for. decades. Not only is illegal mining riskier from a safety perspective for those who choose to participate, but it encourages reckless behavior and leads to outcomes that have negative long-term ..."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:257f43b0-d11f-4107-b9cd-61e45f110d29>","<urn:uuid:11ce18f5-51f7-4dfa-a26b-c4a5796e1622>"],"error":null}
{"question":"Why both sport psychologists and mentors need to set clear expectations and boundaries with clients?","answer":"Both roles require setting clear expectations and boundaries, but for specific reasons in each field. For mentors, it's important to establish a structured agreement that isn't an unlimited time commitment - they need to define goals, schedule regular check-ins, set communication expectations, and determine when the mentorship will end. Similarly, sport psychologists need to establish clear parameters with their clients by assessing needs and abilities, creating tailored interventions, and setting up structured monitoring of performance and behavior. Both roles require maintaining professional boundaries while building trust - mentors avoid telling mentees what decisions to make, while sport psychologists must maintain professional relationships while helping clients deal with potentially stressful situations.","context":["Mentorship is an important part of being an experienced and influential leader. While mentorship often focuses on mentees’ value, mentorship is also an important and developmental experience for executive leaders. First, being a mentor keeps you close to the work of your juniors and clear on the challenges they have to address. Beyond staying relevant to how your organization and industry operate, mentorship is a premier way to build your personal influence as a leader. Read more about mentorship’s effect on building your influence.\nOf course, one of the largest draws to taking on the role of mentor is to give back to the business community and provide a way to coach and help younger career women find the path to their individual greatness. One of the biggest pushbacks on becoming a mentor is often the time commitment. It’s important to note that a proper mentorship agreement isn’t signing a blank check for your time with someone. Successful mentorship happens as a four-phase process and, yes, comes to an end.\nThe Four Phases of Mentorship\nPrepare Yourself: Take stock of your strengths, accomplishments, and the lessons you have learned over the years of your career. Consider your career experiences and how they can benefit a potential mentee. Because what you can offer will need to match the needs of your prospective mentee, what they want to learn, and the skills they want to develop as a mentee.\nFind Your Mentee: Your success as a mentor, especially becoming a great mentor, means you need to understand your potential mentee. Where are they in their career compared to your current position? While having a VP as a mentor sounds great on paper, if they haven’t been involved in the mentee’s current position in more than a decade, the lack of relevance can actually end up doing more harm than good. Learn your mentee’s goals and be certain that you have experience that can be beneficial in helping them achieve their personal goals.\nNurturing Your Mentee: This phase is the bread and butter of mentorship. Once you understand what you can offer a mentee and confirms it matches the mentee’s needs, you can begin working together, helping the mentee gain experience and find their success. Your relationship should be filled with scheduled check-ins that offer feedback, support, and constructive criticism. Another critical aspect of the mentor-mentee relationship is helping them celebrate the small victories along the way.\nClosing Your Mentorship: When should you end your mentorship? It’s important to set an expectation that your mentorship will end, and determine when and why it will come to a close. First, if your mentee achieves their goal(s) they came to you for help to achieve, it’s time to bring your mentorship to an end. Review your mentor check-ins. Are there still areas where you are providing new feedback that your mentee is implementing? If not, your relationship may be reaching the end of its usefulness. It may be time to extend the time between your check-ins or bring the relationship to a close.\nWhen ending your mentorship, make it an event. Express appreciation for your mentee with all the positives you have received from the mentorship. Consider all that you’ve learned, especially as a mentor. By reviewing your mentorship experience, you can learn more about yourself and how you can improve upon your methods for your next mentee.\nPreparing to Be a Great Mentor\nLike many things, thinking critically about your role and planning for your mentorship will set you and your future mentee up for success. The first step to preparing for a new mentorship is understanding why you want to be a mentor and what a potential mentee will gain from their relationship with you.\nIt’s not uncommon for a mentor to struggle with providing too much or too little guidance for your mentee. What’s most important is that you can see a clear way of helping a potential mentee. Focus on helping your mentee form and achieve their goals without impressing your goals and desires onto them. Think of the insight, context, and experience you have compared to a prospective mentee, as these are key areas where you can provide them much-needed guidance. Consider the ideas you can share, stories you can tell of your personal experience related to their goals and challenges, and how you can help a mentee dig deeper into the issues they are addressing.\nAnd, when considering all this during your preparation, if you can’t see a straightforward way to aid a potential mentee, don’t. Instead, think through your network and help the prospective mentee connect with someone you know who might be better suited to mentoring them, or at least helping them find a great mentor that matches their current needs.\nThe last step in preparation is taking a critical look at your availability. Do you have the emotional availability, physical availability, and time to serve as a mentor? It’s vital to ensure you have the resources to serve as a great mentor for your mentee. Creating a contact schedule can help guarantee you stay available as a mentor. Developing structured recurring check-ins and dedicating times when your mentee can reach out when they need it is beneficial.\nThe Mentoring Loop\nThe process for working with a mentee is deceptively simple. In three steps, as a mentor, you need to listen to your mentee, ask questions, and advise. Repeating this three-step process helps you have precise and insightful interactions with your mentee. Start by listening to your mentee. What challenges are they currently facing, and how do they relate to the goals you helped the mentee lay out for their mentorship? Ask clarifying questions to better understand the situation’s context, your mentee’s position, and what tactics your mentee has already attempted to improve the current situation. What was the result of those attempts?\nFinally, after listening to your mentee and doing your best to understand the current situation in a holistic sense, begin to advise your mentee. Make sure your mentee understands your advice and has the opportunity to ask questions about the advice and how they can implement it most effectively. It can be helpful to ask leading questions such as, “what hurdles can you imagine coming across if you try this?” These types of questions can start your mentee thinking proactively about their situation.\nSkills for Great Mentors\nHere is a list of skills you should add to your mentorship toolbox and refine over time.\nGreat Mentors Have Enthusiasm for Mentorship\nMany of us remember having a college course taught by a research professor. You could always tell they were not interested in teaching the class, and doing so was a burden. You don’t want to be that for a mentee. You must cultivate and show an earnest and eager desire to invest in others.\nGreat Mentors Define Their Goals & Relationship\nAs mentioned previously, it’s imperative to set the expectations for your mentorship journey with a mentee. These expectations include defining goals, a schedule for check-ins, defining communication expectations, and helping create mentorship goals you and your prospective mentee agree upon.\nGreat Mentors Are Positive Role Model\nTaking on the mantle of mentorship means placing yourself in a position where one or more people will look to you for advice on what they should do and how they should act. You must walk the walk. Another part of being a positive role model as a mentor is being vulnerable and sharing the bad with the good of your personal journey. Showcase your humility, share the mistakes you made along the way, and the negative experiences that helped define the leader you are today. Being human and sharing your experiences makes you a strong leader and helps build your personal influence.\nGreat Mentors Continue to Grow\nOne of the questions to consider when choosing a mentor noted in How to Find a Good Mentor, is to look for a mentor that continues to work on improving themself. What are you doing to improve yourself personally and professionally? For executive women, peer advisory like EWF’s Executive Peer Advisory Forum is a great way to continue growing and refining your abilities as a leader.\nGreat Mentors Are Relevant\nOften, the success of mentorship relies on the mentor’s experience, knowledge, and success to the mentee’s current situation, challenges, and goals. As a senior VP, you may be very excited to help and shape an entry-level employee’s success, but it’s likely that in the years since you were an entry-level employee that your experience was widely different from what it is today. If you are a decade or more removed from the mentee’s current situation, you can be of most help by finding a potential mentor with more relevant experience.\nGreat Mentors Offer the Right Advisement at the Right Time\nSometimes a great mentor needs to be tough. Offer constructive criticism that is direct and honest but always respectful. Then help build your mentee up by motivating and inspiring them to continue forward. An often overlooked part of mentorship is ensuring that you always recognize and validate your mentee’s accomplishments, be sure to celebrate their achievements and milestones while offering empathy and keeping your mentee accountable when they miss goals. Listen actively to your mentee, be curious, and show appreciation for their situation. Avoid coloring your mentorship by comparing their experiences as more or less severe than your own.\nGreat Mentors Teach Mentees to Fish\nGreat mentors don’t give out the answers or do the work for their mentees. Being a mentor is not about making decisions for another person’s career. Focus your mentorship on helping your mentee learn how to do for themselves. Help your mentee step back and ask why questions about the current challenge. Help expand their perception, identify the stakeholders, recognize their blind spots, and improve their emotional intelligence.\nGreat Mentors Don’t Tell Mentees What To Do\nAs a mentor, you want to focus on offering guidance. Provide your mentee with information and answer questions, but avoid telling your mentee what to do and what decisions to make. Avoid impressing your own biases and impulses on your mentee while helping them identify and account for their own when deciding on a course of action.\nFrom Great Mentor to Sponsor\nConsider the next step beyond mentorship by being a sponsor. A sponsor offers all the same work and benefits of mentorship with an additional layer of investment. As a sponsor, you advocate for your mentee/sponsee in public and private. This advocacy is critical because research shows that women, as a whole, are“over-mentored and under-sponsored. Use your political credibility and relationship capital to grant access to opportunities, people, and roles the mentee couldn’t get on their own. Sponsorship plays a critical role in addressing the “broken rung” obstacle women face in workplace gender parity.\nAddressing the Broken Rung with EWF International\nWe know that the biggest obstacle facing gender parity in the workplace is the reduced rate of women who successfully transition from individual contributors to management. Improved access to mentors, sponsors, and leadership development are prime targets for improving gender parity at the broken rung level. There is more you can do to help create the next generation of women business leaders. Sponsor a bright, young woman in EWF International’s Emerging Leaders program to gain the skills needed to move from individual contributor to leader.","Being physically fit isn't the only skill you need to work as a sport and exercise psychologist; you'll also need patience, a motivational attitude and the ability to cope in potentially stressful situations\nYour concern as a sport and exercise psychologist will be the behaviours, mental processes and wellbeing of individuals, teams and organisations involved in sport and exercise. Typically, you'll specialise in either sport or exercise, although some work across both fields.\nSpecialising in sport psychology you'll work with athletes and teams involved in sport from amateur to elite professional level, with the aim of helping them deal psychologically with the demands of the sport. You'll also help them improve their personal development and performance.\nWorking as an exercise psychologist you'll work with the general public to increase motivation and participation in exercise, encouraging a healthy lifestyle and advising on the psychosocial benefits that exercise can offer.\nAs a sport and exercise psychologist, you'll need to:\n- assess your clients' needs and abilities, and monitor sporting performance and behaviour\n- implement strategies to help clients overcome difficulties, improve performance or realise potential\n- work with a multidisciplinary team including other psychologists, nutritionists, GPs, coaches and physiologists\n- deliver counselling and/or workshops covering issues such as goal setting, visualisation and relaxation\n- conduct and apply research in sport or exercise psychology.\nIf you work in sport psychology, you'll need to:\n- work with a range of clients including individual athletes, teams, coaches and referees, from amateur to elite professional level across a range of sporting disciplines\n- develop tailored interventions to assist athletes in preparation for competition and to deal with the psychological demands of the sport\n- equip athletes with mental strategies to cope with and overcome setbacks or injuries\n- advise coaches how to improve squad cohesion or communication\n- deliver group workshops on areas such as self-analysis of performance or techniques to develop mental skills within the sport.\nIf you work in exercise psychology, you'll need to:\n- counsel clients who are ill, in poor physical or mental health, and who may benefit from participation in more regular exercise\n- advise individuals about the benefits, both physical and psychological, that can be derived from exercise\n- work with individuals and groups in a variety of settings including GP surgeries, employers' premises, the client's home, clinical settings and local fitness centres\n- devise, implement and evaluate exercise programmes based on the needs of the client\n- provide counselling and consultations to a cross-section of the public including people who are depressed, GP referrals, people in prison or groups of employees as part of a workplace-exercise programme.\n- Starting salaries range from £20,000 to £22,000.\n- Salaries for experienced sport and exercise psychologists typically range from £27,000 to £37,000.\n- Senior psychologists and heads of department can earn around £48,000 or more.\nFor up-to-date salary scales for further education (FE) and higher education (HE) positions, see the University and College Union (UCU). Experienced consultants working with top professional athletes can expect to charge up to £1,000 a day in consultancy fees. Salaries vary according to the type of employer. The salaries of those employed by professional clubs or national governing bodies tend to be higher than those of amateur ones.\nIncome figures are intended as a guide only.\nWorking hours for practitioners vary depending on the client and the nature of the sport. Although you would generally work office hours, evening or weekend work may also be required to fit in with training and sports competitions.\nHours within education or healthcare settings are mainly 9am to 5pm.\nWhat to expect\n- Work environments vary depending on the client and could include an office base within a university campus, GP surgery or hospital, or field settings such as the athletes' village at major sports events, the team training base or employers' premises.\n- Jobs are available with sports teams and organisations, as well as universities and colleges, throughout the UK and abroad. Sport psychology is well established in the USA.\n- Travel within a working day is common, particularly if working with sports professionals. You may form part of a support team travelling with a team or athlete to competitions and tournaments locally, nationally and internationally.\nTo qualify as a practising sport and exercise psychologist you'll need to complete:\n- a degree in psychology accredited by the British Psychological Society (BPS) leading to the Graduate Basis for Chartered Membership (GBC)\n- a BPS-accredited MSc in sport and exercise psychology\n- Stage 2 of the BPS Qualification in Sport and Exercise Psychology (QSEP) (two years of supervised practice).\nOnce you've completed Stage 2 of the BPS QSEP you will be eligible for registration with the Health & Care Professions Council (HCPC) and can use the title 'sport and exercise psychologist'.\nIf you have a degree in a subject other than psychology, you may be able to gain eligibility for GBC by taking a BPS-accredited conversion course, which usually takes one year full time or two years part time. For details of approved courses see BPS - Find an accredited course.\nEntry with an HND or foundation degree only is not possible.\nEntry on to a Masters course is competitive and you'll normally be expected to have at least a 2:1. Graduates with a 2:2 who also have a research-based higher qualification may be accepted. Before applying, check that your course is approved by the HCPC (see the HCPC register of approved programmes).\nOnce you've completed the Masters course, you'll also need two years' supervised practice before you can register with the HCPC.\nIf you want to lecture in sport and exercise psychology, you'll need to follow a career in research. A PhD in sport and exercise is usually required.\nYou'll need to show evidence of the following:\n- an interest in sport\n- excellent communication and interpersonal skills\n- active listening and reflection skills\n- patience and the ability to motivate others\n- flexibility in order to work in a range of settings with different clients\n- problem-solving skills\n- decision-making ability\n- commitment to research and continuing professional development (CPD)\n- ability to work under pressure and cope with stressful situations\n- a methodical approach to work\n- IT skills.\nIt's important to gain some experience in a sport or exercise environment and you can do this either through a placement, internship, vacation work, volunteering or shadowing position. You'll also need relevant experience before being accepted onto a Masters course.\nBASES (The British Association of Sport and Exercise Sciences) has lots of relevant information about work experience opportunities on its website. Also, you could try contacting your local sports club to ask about opportunities to help out. Until you're qualified it may not be possible to gain specific sport psychology experience, but you can still obtain useful experience in an area such as sports performance, coaching, fitness and exercise instruction, health promotion or PE teaching.\nFind out more about the different kinds of work experience and internships that are available.\nSome sport psychologists work as private consultants or work full time for professional sports teams or national governing bodies of sport.\nMost, however, combine consultancy work with teaching and research within universities or colleges throughout the UK and abroad, or work in other areas of psychology, for example clinical or occupational.\nSimilarly, exercise psychologists tend to combine consultancy with teaching and research. Your work might see you involved in GP exercise referral, cardiac rehabilitation schemes or work within the NHS or private healthcare providers.\nYou may get involved with setting up exercise and health programmes in prisons or for staff in the workplace as well as in psychiatric settings.\nLook for job vacancies at:\n- Jobs.ac.uk - for teaching and research posts.\n- NHS Jobs\n- NHS Scotland Recruitment\n- Jobs in Psychology\n- UK Sport\nIf you're looking to work in consultancy you'll need to establish contacts and build networks, even during your Masters course (if you do one), as referrals are often by word of mouth or facilitated through contacts with GP surgeries or other health professionals.\nAll practising sport and exercise psychologists are required to register with the HCPC (Health & Care Professions Council). Registration renewal takes place every two years and applicants for renewal must sign a professional declaration. Contact the HCPC for further information.\nIn order to stay registered with the HCPC, you must undertake and keep a record of continuing professional development (CPD). Activities may include attending courses, workshops and conferences run, for example, by the BPS.\nBPS provides learning and CPD opportunities through its Professional Development centre, and runs an annual learning and professional development programme.\nBASES runs a number of sport and exercise-related workshops and conferences, which you may find help keep your skills fresh.\nYou may decide to undertake further study, for example to broaden your expertise in other branches, such as clinical or health psychology. Or further research, by completing a PhD, especially if you're employed in a lecturing role. See the BPS website for a guide to postgraduate research degrees.\nOpportunities for advancement exist both within professional practice and through research within an academic environment.\nWithin the field of sport psychology, you may find the opportunity to advance to a private practice as a consultant or to move to senior positions within a professional club, with individual athletes or a governing body.\nElite athletes work for many years to reach such a level and expect the same level of expertise from their support staff. For this reason, it's usually necessary for a sport psychologist to have several years' experience before being able to work with top professional athletes.\nWithin the field of exercise psychology, there may be opportunities for advancement to a consultancy role or involvement in GP referral schemes. There is an increasing role for professional staff within the area of health promotion. The role of health psychologist within the NHS may also present opportunities for qualified exercise psychologists."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:8abefd1c-3b3a-4997-a2e3-1dee50a3490c>","<urn:uuid:59647632-86f6-4277-bf11-3bf3ef8db0ae>"],"error":null}
{"question":"How do the visual symptoms of scale insect infestations compare to aphid damage on plants?","answer":"Scale insects and aphids show distinct visual symptoms on plants. Scale insects appear as small bumps or wart-like growths on branches, causing yellowing leaves, premature leaf drop, twig dieback, and slower growth. They may also produce honeydew that creates sticky residue and leads to sooty mold growth. Aphids, while also producing honeydew, primarily attack soft growing parts and young seedlings. They rarely kill plants outright but can seriously damage flowers and fruit, leading to stunted growth and weakened plants. Additionally, aphids can quickly overwhelm a garden 'overnight' due to their rapid reproduction rate, with each adult capable of producing up to 80 offspring weekly.","context":["As you watch your trees grow or relax under their shade on sunny afternoons, it’s hard to imagine that something small like an insect could take one of those giant structures down.\nYet even very small pests can do quite a number on your trees if you don’t catch them early and manage them.\nTake scale insects, for instance, which are common pests of landscape trees and shrubs that can easily be overlooked. Yet they can be responsible for many worrisome symptoms.\nLet’s learn more about scale insects on trees, what common symptoms of an infestation look like, and how to manage them.\nWhat Is A Scale Insect?\nYou might be wondering what tree scale are and what they look like to ensure your trees are safe.\nScales are sap-sucking insects that feed on plants, including trees shrubs, and perennials. They don’t move much and if you look closely they seem to appear as warts or bumps rather than creepy-crawly insects. They often are colored so that they blend into the bark or leaf tissue of your trees, making them hard to spot.\nThere are generally two types of scale insects: soft and armored.\nSoft scales, such as magnolia scale, are small (¼ to ½ inch long) around with a waxy covering that is attached to the body. These insects extract sap from the tree and then excrete a sticky waste product, called “honeydew.”.\nArmored scales, such as pine needle scale, on the other hand, have waxy armor over their bodies giving them a harder exterior appearance.\nYou can find scale in all areas in the U.S.\nVisible Symptoms Of Tree Scale\nWhile you may not notice tree scale at first, you may see some of the signs and symptoms they leave behind.\nWith soft scale, the honeydew they excrete can be a nuisance. It can drip on patios, parked cars, walkways, and benches, becoming a sticky mess. It also attracts other pests like ants, flies, and wasps. Also, a fungus called sooty mold, can grow on this honeydew, prevent leaves from absorbing as much sunlight, and result in an unsightly appearance.\nArmored scale doesn’t produce honeydew, but you might see these other signs and symptoms that both types of scale can cause:\nBranches covered with small bumps, which are actually the insects themselves\n- Yellowing or brownish leaves\n- Premature leaf drop\n- Twig or branch dieback\n- Slower overall growth\nSimilar Tree Pests That Excrete ‘Honeydew’\nUnfortunately, honeydew isn’t unique to tree scale.\nIf you see a sticky substance that resembles honeydew, it might be scale, but it could also be one of the following:\n- Some leafhoppers\nThis is why a tree inspection by a certified arborist can help confirm which insect is causing the symptoms you’re seeing, so the control method can be more targeted and effective.\nTree Scale Management & Control\nTree scale are difficult to control, which is why catching the problem early can help optimize the timing of treatment for best pest management.\nBoth soft and armored scale are most susceptible when the nymphs first hatch from their eggs. Dormant oil applications can be a good first step in mitigating scale insect populations. Monitoring is key to optimally hit the appropriate treatment windows.\nAdditional steps as part of a year-round management program are usually necessary. For example, systemic insecticides can deliver lethal doses straight to the insects’ mouthparts through the very vascular system they feed within. Insect growth regulators (IGR) can also prevent new hatchlings from developing into their reproductive stage. Depending on the scale species and host plant, a certified arborist will prescribe different treatments in combination, and at precise times of year, to best manage the particular pest.","Aphids and Nitrogen\nAphids are one of the perennial pests that gardeners deal with, often with very mixed results. What works one year seems to fall flat on its face the next, with the reverse also being true. Aphids are tiny, soft bodied insects that have piercing, sucking mouthparts to feed on plant saps. They live in colonies, most often the underside of leaves and where the tender young growth is on a plant.\nBy themselves, aphids rarely outright kill a plant but they can inflict serious damage to both its flowers and fruit. It only takes a few aphids sucking on a young flower or fruit to weaken it or damage it beyond being edible. The sap they extract weakens the plant, stunting its growth and food production. The aphids can also be carriers or vectors of diseases or viruses, which they infect the plant with as they pierce the cell walls and extract the plant sap. Just a few seconds is all it takes for a virus to transfer from the mouth of an aphid to the plant.\nAphids will often arrive in a garden or on a plant by flying in. A small number will arrive and leave their wingless young who feed on the plant and immediately lay eggs, increasing the population quickly. The adults will then fly off and repeat this several times. This is why gardeners will often complain that the aphids overwhelmed the garden “overnight”. Each adult aphid can produce up to 80 offspring every week!\nSo what can be done about these insects? There are several methods of dealing with them, broken down into two main approaches – prevention and treatment. The prevention phase happens in the fall and early winter with soil testing and amendments that set the stage for success the next growing season. Treatment is just that – what to do when those pesky insects show up uninvited in your garden!\nAphids Love Nitrogen\nLet’s start at the beginning, shall we? Nitrogen is a big, big player in the aphid dance. Aphids love nitrogen, plain and simple. They are attracted to the soft growing parts of a plant that are high in nitrogen, as it is a major factor in plant growth. Aphids are also really attracted to young seedlings, since everything on a seedling is growing and there is lots of nitrogen to be had.\nNitrogen plays a very important part in a plants growth as well as the content of its sap. Soils with excessively high nitrogen create very fast growing plants. This leads to rapid cell wall growth, which are elongated, thinner and much easier for the piercing mouth-parts of an aphid to penetrate than normal. The plant sap will also be high in nitrogen and will be especially attractive for the aphid. It is very important to understand that high nitrogen sap will be lower in overall sugar content – also known as brix – because nitrogen forms amino acids and proteins – such as chlorophyll – but specifically needs magnesium, phosphorus and carbon to form sugars. This can’t happen if there is excess nitrogen, as there aren’t enough of the other elements to form those sugars. Once the soil is amended with the correct nutrients, the plant will increase the brix/sugar levels and the aphids will die from sucking high sugar content plant sap as it is deadly – aphids can’t digest the sugars with no pancreas and thus die.\nWhat all this means is that using high doses of chemical fertilizers will encourage aphids to call your garden home! Almost all commercial fertilizers are high in nitrogen and release their nutrients much faster than compost or other soil amendments, making aphid pressures much worse than they would normally be. They also contribute to lots of flowers with little fruit production or stunted and smaller than normal sized fruit.\nIt is interesting to note with soybean studies at Penn State University naturally occurring nitrogen-fixing bacteria – called rhizobia – provided a better form of naturally occurring nitrogen than the laboratory bred inoculated strains did. Plants growing in the naturally occurring rhizobia soils had markedly fewer aphids and stress than the inoculated ones. The amount of nitrogen provided by the lab developed rhizobia and the naturally occurring strains were the same, with the natural rhizobia having much lower aphid populations on the plants. They are continuing the study to see why this happens.\nThere is a concept at work here that we need to briefly discuss so that you can understand where we are going from here. The “Law of the Minimum” shows how interrelated many nutrients and elements are to healthy plants, not just the N,P and K that are listed on fertilizer bags.\nThe Law of the Minimum states,\n“Plant growth is determined by the scarcest, “limiting” nutrient; if even one of the many required nutrients is deficient, the plant will not grow and produce at its optimum.”\nPrevention and Preparation\nThe preventative method is to have a complete, comprehensive soil analysis done by a professional lab. I’ve mentioned these before, but they bear another – Crop Services International and Texas Plant and Soil Lab are great labs. They are thorough, friendly and will give you the info you need at a reasonable price. From this analysis, you will know exactly what soil nutrients, amendments and trace minerals are needed to eliminate a high nitrogen condition in your soil and plants, and by extension reducing the population and attraction of aphids.\nAnother, very simple method is to stop using commercially available chemical fertilizers! Without naming names, these come in a bag and sometimes have the word “miracle” attached to them. There are a number of brands that can be bought at any garden center or big box store in their garden section. Well-aged and decomposed compost – especially if you’ve worked with it how we discuss in our articles on compost – will give you a continuous supply of “soil food” to apply to your garden twice a year, in the early spring and again in the late fall. This compost with the amendments will slowly release the nutrients that the garden needs, along with attracting the biological elements in the soil that do the real work – earthworms, pillbugs, beneficial nematodes, fungi and all sorts of other hard-working critters that seriously improve the soil on a continuous basis.\nAnother aspect of prevention is cultural control, or removing the suitable environments where aphids can overwinter or establish an initial population to then swarm your garden. Standing weeds can be harbors of aphids, so remove them and remove dead plants from the garden in the fall. Don’t wait until the springtime to clean up the garden, as this can provide the perfect habitat for aphid eggs to be sheltered under. Inspect trees and bushes for aphid eggs in the fall, remove them by hand, vacuum or with a strong blast of water.\nNow that you’ve got the prevention and preparation covered, what can be done if and when the nasty aphids arrive? This is where the treatment portion comes in, and it would be wise to prepare for this as well. Early detection is very important for successful treatment, as if the aphids get a toe-hold, it will be much more difficult to rid your plants of several sizable populations instead of just one or two small ones. The incoming flights of adult aphids are random, so consistent inspection is best. This is very easy, just flip over the top, youngest leaves of several plants and look for the clusters of small aphids on the underside. Look around bud areas as well. If you see any aphids, they will be in small clusters or colonies and can be easily dealt with at this stage. Crush them by hand or prune the leaves or buds to remove them. Once you see the first small populations, go back and be very thorough with the rest of your plants, taking the time to examine them well. Trust me on this, the time spent now will save you much heartache, back ache, time and frustration in the very near future!\nAphids will excrete “honeydew” – a sweet, sticky substance – and is sometimes fed on by ants. This isn’t always the case; but in your inspections look for travel pathways of ants up and down the plant where the aphid colonies are. Sometimes the ants will lead you to the aphids that you would have otherwise missed.\nAnytime you see any aphids, it is a good practice to set out yellow sticky traps. Aphids are highly attracted to the color yellow, which lures these little monsters into the traps. They are available at most garden centers and will be in squares or strips. Place several of them in the area where you find the aphids, and put out a few more than you might think. They are good inexpensive insurance.\nIf the aphids have colonized more than about 5% of the bud and young leaves of a plant, or are on more than that amount of your total garden plants, then it’s time for the next round of action.\nBeyond physically removing the aphids, there are two approaches to treatment – biological and chemical controls. Biological controls use biology – predatory insects – to eat and control the aphids. Chemical controls are just what they sound like – using sprays of varying toxicity to reduce the aphids’ population.\nUsing the biological approach first combined with a non-toxic soapy spray is often the knockout punch needed for smaller aphid infestations. Releasing parasitic wasps that lay their eggs inside the aphids, ladybugs, lacewings, soldier beetles and the syrphid fly larvae are all highly effective if done in time, before the aphids’ population explodes. One of the best resources for biological controls is Arbico Organics. They will help you decide what species will work best for your garden situation, how many to release and how many times. Keep in mind that the goal is not to completely eliminate every single aphid, as then the beneficial and predatory insects won’t have a food source. The goal is to keep the aphids controlled, where they aren’t damaging the plants.\nRemember, weather can be on your side when dealing with aphids. Heat and high humidity can really knock them back as they are fairly fragile and die off in droves when temperatures are over 90°F.\nSoap sprays work by smothering the aphids by coating their skins. Start with a completely harmless soapy water spray like Dr. Bronner’s – using a tablespoon per half gallon in a hand sprayer. Make sure to apply the spray when beneficial insects aren’t around, as they will also be affected. From there, work up to an insecticidal soap like Safer Brand or horticultural oil such as neem oil. With both of these approaches, make sure to cover the underside of the infested leaves well for the smothering effect to work. These are contact controls, and depend on contact with the aphids to work, so they will need to be re-applied as often as needed until you’ve gotten control of the situation. This could mean once a day for a few days, then twice a week for a week or so, then tapering down to once a week. It might well take 2 – 3 weeks to really get a handle on persistent outbreaks, so be patient but persistent!\nMoving up the toxicity ladder, multi part sprays such as our Home Garden Bug Solution work very well, but need to be used carefully as they do have a high level of insect toxicity even though they are made with no petrochemical ingredients. If you do need to bring out these big guns, test in a small area to see the effects before spraying your entire garden!\nHopefully you now see that there are a number of ways to reduce and control the pesky aphids in your garden. It all starts with prevention and preparation with improving the soil and balancing the nutrients it needs. From there you now have several new tools in your “pest control” toolbox to help you manage aphids in your garden next growing season."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:a5291c31-f19d-4751-aa41-abb646c787d5>","<urn:uuid:a9f3b073-cdd9-4229-ab95-759f716a1044>"],"error":null}
{"question":"How do mason bees and honeybees differ in their pollination efficiency, particularly regarding the number of bees needed per acre and their ability to work in cold conditions?","answer":"Mason bees and honeybees have significant differences in their pollination efficiency. It takes fewer than 750 mason bees to pollinate an acre of orchard compared to 20,000-30,000 honeybees. Mason bees are more efficient in cold conditions as they can work at low light levels and temperatures, flying on cold spring days when honeybees prefer not to. Additionally, mason bees carry pollen on their hairy abdomen where it brushes off onto the anther and stigma of flowers, while honeybees carry pollen as a wet lump on the back of their legs. Studies by USDA researchers showed that mason bees produced higher average cherry production in cold areas compared to honeybees. However, honeybees remain crucial for agriculture, as they accomplish about 80% of crop insect pollination in the US, contributing to an estimated $9.7 billion in direct value to U.S. agriculture.","context":["Mason bee nests in the orchard of Robert Schreiber at Poysdorf, Austria, pictured during an International Fruit Tree Association tour\nGrowers should think not about species of beneficial insects, but develop strategies, such as insectaries, to benefit the whole community of natural enemies and pollinators, advises Paul Jepson, director of the Integrated Plant Protection Center at Oregon State University, Corvallis.\n“We have a wonderful array of organisms to exploit by these tactics,” he said during a recent workshop on habitat and conservation practices for beneficial insects on farms.\nSome natural enemies, such as beetles and spiders, are resident on the farm, and the goal is to not disrupt them. Others, such as hoverflies, lady beetles, and parasitic wasps, move from field to field and the objective is to attract them to the farm while they are moving about the landscape looking for something to forage on, he said.\nMatthew Shepherd, senior conservation associate with the Xerces Society for Invertebrate Conservation in Portland, Oregon, said bees are the most important pollinators because they actively transport and collect pollen, but other insects, including butterflies, moths, wasps, beetles, and flies, can serve as pollinators when they visit flowers to feed or lay eggs.\nThere are 4,000 bee species in North America, of which there are about 600 in Oregon and Washington’s Columbia Basin.\nSocial bees, such as honeybees and bumblebees, collect pollen and take it back to their nests for their offspring. They are very effective pollinators because they are consistently working the flowers in an area around their nest and tend to work on a single species of flower. They can visit up to 150 flowers in one foraging trip, and there’s little possibility that they will take the wrong pollen to a flower, whereas butterflies will fly from one species of plant to another. Honeybees are also a convenient pollinator because they can be transported into the orchard in hives and don’t need habitat.\nCommon wild native bees, which include the mason bee, mining bee, sweat bee, bumblebee, longhorn bee, and leafcutter bee, can also play a significant role in agriculture, said Shepherd. Solitary bees nest in the ground or in cracks or crevices in walls, or in wood. Areas with a range of flowers that are in bloom for much of the year will draw bees to the orchard. “The amount of the landscape that’s in natural areas has a big influence on whether you get pollinators in your crop or not.”\nMason bees make nests in reeds and natural holes, creating cells for their brood that are separated by mud dividers. To attract mason bees, growers can provide wooden bee homes with 5/16-inch diameter holes to serve as nesting tunnels. Straws can be inserted in the holes as liners so that they can be replaced each season. The bees also need a source of water in order to make mud.\nMace Vaughan, director of the Xerces Society’s agricultural pollinator conservation program, said that if a grower wants to manage mason bees as a pollinator, they need the same level of management as honeybees. The grower should know how to clean out the tunnels and get rid of the diseased cocoons in the winter.\nShepherd cited studies conducted by U.S. Department of Agriculture researchers in Utah that showed that average cherry production in cold areas was much higher in orchards where mason bees were used for pollination than where honeybees were used. One reason mason bees are so efficient is they are active at low light levels and low temperatures and will fly on cold spring days when honeybees prefer not to. Mason and honeybees also differ in the way they carry pollen. The mason bee, which looks similar to a housefly, has a hairy abdomen and carries pollen on the lower part of its body, where it brushes off onto the anther and stigma of the flowers. A honeybee, in contrast, carries pollen as a wet lump on the back of its legs.\nIt takes fewer than 750 mason bees to pollinate an acre of orchard versus 20,000 to 30,000 honeybees, Shepherd said.\nFor more information, check the Web site at www.xerces.org/bees.","Yellow Medicine County Bug Camp Beekeeping\nBeekeeping's benefit to agriculture\nThe U.S. Department of Agriculture has estimated that about 3.5 million acres of U.S. fruit, vegetable, oilseed and legume seed crops depend on insect pollination. Another 63 million acres derive some benefit from insect pollination. An estimated 80 percent of crop insect pollination is accomplished by honey bees.\nA 1989 Cornell University study concluded that the direct value of honey bee pollination to U.S. agriculture is $9.7 billion.\nAbout one-third of the total human diet is derived directly or indirectly from insect pollinated plants.\nThe production of most beef and dairy products consumed in the United States is dependent on insect-pollinated legumes (alfalfa, clover, lespedeza, etc.).\nHow many bees does it take?\nTo pollinate California's approximately 360,000 bearing acres of almonds, it is estimated that 250,000 colonies of honey bees must be borrowed from other states to add to the 500,000 colonies already in the state.\nThe practice of renting bees to pollinate crops has expanded rapidly. Most pollination services available to growers in the United States are provided by commercial beekeepers.\nHoney bee facts\nHoney bees are social insects, with a marked division of labor between the various types of bees in the colony. A colony of honey bees includes a queen, drones and workers.\nThe queen is the only sexually developed female in the hive. She is the largest bee in the colony. A two-day old larva is selected by the workers to be reared as the queen. She will emerge from her cell 11 days later to mate in flight with approximately 18 drone (male) bees. During this mating, she receives several million sperm cells, which last her entire life span of nearly two years. The queen bee can lay 9,000 eggs in one day.\nDrones are stout male bees which have no stingers. Drones do not collect food or pollen from flowers. Their sole purpose is to make with the queen. If the colony is short on food, drones are often kicked out of the hive.\nThe buzz about honey\nA hive of bees can make two pounds of honey a day. To create that much honey nectar from 100,000 flowers needs to be gathered. Talk about \"bee-ing\" active!\nA honey bee flies a distance equal to twice around the world in its lifetime!\nIf you want to know where a bee finds its nectar, just watch it dance. Bees explain nectar location to others in the colony with funny dance like performances. Honey bees are social insects. They communicate with each other by dancing. They do this to tell nest mates where pollen and nectar are available. There are two common dances. The round dance tells recruits that food is close by the hive. The waggle dance communicates specific information about the distance and direction of the food.\nBees love honey as much as people do. In fact, the whole process of making honey is a way of storing up food for the bee colony.\nBees may buzz a lot, but you won't hear honey bees complaining. They are totally deaf.\nA busy bee will produce just one-twelfth of a tablespoon of honey in a lifetime.\nAll honey will crystallize (develop sugar-like granules) in time. Honey will crystallize rapidly if placed in the refrigerator. Place one jar of honey in your refrigerator and one jar in a cabinet to compare. Don't worry. You can make the crystals disappear. Carefully place the jar in a pan of warm water. When heated, the honey will re-liquefy. Some honey (called creamed or spun) is finely crystallized. This makes the honey spreadable like butter.\nTalk like a beekeeper\nAnther: The area where pollen is developed and contained in a plant.\nApiarist: A beekeeper\nApiary: Where several bee colonies are kept in one place.\nBee bread: Mixture of pollen and honey fed to bees after they are three days old.\nBeehive: A place where a bee colony dwells. Beekeeping\nBeeswax: Secreted wax from the underside glands of the bee abdomen; bees mold the wax to form honeycomb.\nBrood: A group of bees born at the same time.\nBrood of chamber: Place in the hive where the queen lays her eggs.\nColony: A community of tens of thousands of worker bees, usually containing one queen, with or without drones.\nComb honey: Honey presented in its original wax comb.\nCrystallization: Honey is a supersaturated solution. Crystals will develop in honey when glucose crystallizes out of solution. Crystallization of honey is most rapid at 57 degrees F.\nDrone: A male bee.\nExtracted honey: Honey removed from the comb by a special machine called an extractor and sold in liquid or crystallized form.\nHive tool: A tool the beekeeper uses to pry the frames out of the hives.\nLarvae: A wingless, newly hatched bee.\nNectar: Sweet liquid from flowers that bees use to make honey.\nNurse bee: Bee that is in charge of caring for and feeding the larvae.\nPollination: Fertilization of a flower by a bee. The bee collects pollen from one flower on her legs and transfers it to other flowers when she lands on them.\nPupae: The stage of a bee's life between the larval stage and the adult.\nQueen: The female bee that lays all the eggs in the colony. There is only one queen bee in a colony.\nRoyal jelly: The food the larvae eat for the first three days of their lives.\nSmoker: Tool which sends out small amounts of smoke which calms the bees so the beekeeper can safely look at the hives.\nSuper: Where honey is stored.\nVenom: Poison produced by the bee that is injected into your skin when a bee stings you. This is what makes a bee sting hurt.\nWorker: A sterile female bee that performs special jobs in and around the hive.\nWhy Do Bees Sting?\nBees do not sting for the fun of it. A bee's sting is a way of protecting itself. She will only sting when she feels threatened or to protect her hive if she feels it is being invaded. If you don't bother her or make her feel like she or her hive is in danger, she won't bother (STING!) you.\nWhat does the beekeeper do?\nNot only are the bees hard at work year round (since they do not hibernate), but the beekeeper is \"busy as a bee\" as well. In the summer, he has to make sure there are plenty of flowers around the hive so the bees can collect the nectar to make honey. He also has t collect the honey after the supers fill up. In the fall, he extracts the honey and the supers. He feeds sugar water to the hive and wraps the hive to protect it from the winter cold. During the winger he makes sure the bees have sugar water to eat and that the hive is well insulated. When spring comes, he removes the winter wraps and moves the hive to a spot where spring flowers are blooming.\nNational Honey Board, 390 Lashley Street, Longmont, Co 80501\nPenn State SRUA, 324 Henning Bldg, University Park PA 10802, 814-863-7738"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:6bf72100-3237-4ce3-97a4-817b6807240e>","<urn:uuid:9729ea3e-985f-4051-9c40-0846b43dbd2d>"],"error":null}
{"question":"Why is skin care such an important part of lymphedema treatment? Really need to understand the science behind this!","answer":"Skin care is crucial in lymphedema treatment because swelling increases the extremity's diameter, which increases the diffusion distance that oxygen and nutrients must travel to reach the skin surface. This increased distance makes it harder for oxygen to reach the skin, resulting in slower wound healing. Since slower healing increases infection risk through open wounds, patients with lymphedema must be especially vigilant about keeping wounds clean and covered.","context":["By Riva Preil\nSo, how does physical therapy help treat lymphedema? Great question, I am glad you asked. The gold standard of care to treat lymphedema is COMPLETE DECONGESTIVE THERAPY (CDT). CDT consists of four components:\n- Manual Lymphatic Drainage (MLD)\n- Skin Care\nMLD was pioneered by Dr. Emil Vodder in the early 1900s, and it is series of various manual techniques (including “pump,” “rotaries,” “scoop,” “and pump-push,” to name several) which promote moving the excess lymph fluid from the distal extremities towards proximal, healthy open lymph channels. Another way to think about lymphedema due to lymph node removal is that there is congestion on one “road”. Therefore, the certified lymphedema therapist (CLT) seeks to open up and create new channels on uncongested “roads,” thereby creating detours for the lymph to return to the venous system through alternate routes. MLD allows the CLT to direct the lymph along healthy, open lymphatic pathways.\nIn addition, appropriate compression must be applied to promote the return of lymphatic fluid and to prevent it from improperly pooling in affected areas. Therefore, compression is an appropriate component of CDT. During the acute first phase of treatment, bandaging should ideally be worn 23 hours during the day and removed only for showering or bathing. CLT therapists bandage patients with short stretch bandages and they teach patients how to perform self-wrapping. During the chronic second phase of treatment, compression garments can be substituted for bandages during the day, either custom made or over the counter. Bandaging is continued for nighttime usage in most cases.\nNo physical therapy experience would be complete without some exercise, right? Exercises are important in treatment of lymphedema because the musculoskeletal system helps with venous return and promoting fluid movement by acting as a pump mechanism. That’s why ankle pumps (ankle dorsiflexion and plantarflexion) are recommended after ankle surgeries, in treatment of postsurgical edema (swelling). So too, similar upper extremity and lower extremity “pump-like” exercises help return lymphatic fluid to the venous system. Depending on the type of lymphedema (upper vs. lower extremity) several examples of lymphedema appropriate exercises include ankle pumps, knee and hip flexion and extension, shoulder flexion and abduction, elbow flexion and extension, and wrist flexion and extension.\nFinally, skin care is a huge component of CDT. The reason for this is that increased swelling of any extremity increases the DIAMETER of the extremity. This increases the distance that gases (ex. oxygen) and nutrients must travel to reach the surface of the skin. This is referred to as increased diffusion distance. Oxygen is a necessary component in skin healing after any injury, even simply a paper cut! It is harder for oxygen to travel long distances compared to short distances, therefore increased diffusion distance (as in the case of lymphedema) SLOWS the healing process. Slower healing rate increases the likelihood of developing an infection through the open wound. Therefore, it is crucial for all patients with lymphedema to be vigilant about skin care and inspection. Their wounds heal slower than those of other individuals, and it is important to minimize the chance of developing infection by keeping wounds clean and covered.\nSo there you have it! That was a brief overview of “How I Spent My Summer Vacation.” I could not think of a better way to spend my time this summer. Don’t get me wrong, a week in Italy or an Alaskan cruise are trips that I look forward to hopefully taking one day, but I am grateful that I had the opportunity to take this incredible course his past summer. This course is rarely offered in New York, so I had to jump on my chance when I saw it advertised. Thank you to Amy for supporting my continuing education and to my co-workers for helping to treat my patients while I was at the course. Italy and Alaska can wait for me, because my patients are more important!"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:660b8022-6dd3-4a91-ac0e-479d999c462c>"],"error":null}
{"question":"¿Cuáles son los 4 tipos tradicionales de movimiento según la física clásica? Necesito saberlo para mi clase 📚","answer":"Traditionally, there are four types of motion: 1) generation and corruption, 2) alteration, 3) augmentation and diminution, and 4) change of place. This classification comes from the classical understanding of kinesis (movement).","context":["Definitions for motionˈmoʊ ʃən\nThis page provides all possible meanings and translations of the word motion\nthe use of movements (especially of the hands) to communicate familiar or prearranged signals\na natural event that involves a change in the position or location of something\nmotion, movement, move, motility(noun)\na change of position that does not entail a change of location\n\"the reflex motion of his eyebrows revealed his surprise\"; \"movement is a sign of life\"; \"an impatient move of his hand\"; \"gastrointestinal motility\"\na state of change\n\"they were in a state of steady motion\"\na formal proposal for action made to a deliberative assembly for discussion and vote\n\"he made a motion to adjourn\"; \"she called for the question\"\nmotion, movement, move(noun)\nthe act of changing location from one place to another\n\"police controlled the motion of the crowd\"; \"the movement of people from the farms to the cities\"; \"his move put him directly in my path\"\napparent motion, motion, apparent movement, movement(verb)\nan optical illusion of motion produced by viewing a rapid succession of still pictures of a moving object\n\"the cinema relies on apparent motion\"; \"the succession of flashing lights gave an illusion of movement\"\ngesticulate, gesture, motion(verb)\nshow, express or direct through movement\n\"He gestured his desire to leave\"\nA state of progression from one place to another.\nA change of position with respect to time.\nA change from one place to another.\nA parliamentary action to propose something.\nThe motion to amend is now open for discussion.\nAn entertainment or show, especially a puppet show.\nfrom u03BAu03AFu03BDu03B7u03C3u03B9u03C2; any change. Traditionally of four types: generation and corruption, alteration, augmentation and diminution, and change of place.\nTo gesture indicating a desired movement.\nHe motioned for me to come closer.\nTo introduce a motion in parliamentary procedure.\nOrigin: From motion, mocion, motion, and their source, motio.\nthe act, process, or state of changing place or position; movement; the passing of a body from one place or position to another, whether voluntary or involuntary; -- opposed to rest\npower of, or capacity for, motion\ndirection of movement; course; tendency; as, the motion of the planets is from west to east\nchange in the relative position of the parts of anything; action of a machine with respect to the relative movement of its parts\nmovement of the mind, desires, or passions; mental act, or impulse to any action; internal activity\na proposal or suggestion looking to action or progress; esp., a formal proposal made in a deliberative assembly; as, a motion to adjourn\nan application made to a court or judge orally in open court. Its object is to obtain an order or rule directing some act to be done in favor of the applicant\nchange of pitch in successive sounds, whether in the same part or in groups of parts\na puppet show or puppet\nto make a significant movement or gesture, as with the hand; as, to motion to one to take a seat\nto make proposal; to offer plans\nto direct or invite by a motion, as of the hand or head; as, to motion one to a seat\nto propose; to move\nIn physics, motion is a change in position of an object with respect to time and its reference point. Motion is typically described in terms of displacement, velocity, acceleration, and time. Motion is observed by attaching a frame of reference to a body and measuring its change in position relative to another reference frame. A body which does not move is said to be at rest, motionless, immobile, stationary, or to have constant position. An object's motion cannot change unless it is acted upon by a force, as described by Newton's first law. An object's momentum is directly related to the object's mass and velocity, and the total momentum of all objects in a closed system does not change with time, as described by the law of conservation of momentum. As there is no absolute frame of reference, absolute motion cannot be determined. Thus, everything in the universe can be considered to be moving. More generally, the term motion signifies a continuous change in the configuration of a physical system. For example, one can talk about motion of a wave or a quantum particle where the configuration consists of probabilities of occupying specific positions.\nU.S. National Library of Medicine\nPhysical motion, i.e., a change in position of a body or subject as a result of an external force. It is distinguished from MOVEMENT, a process resulting from biological activity.\nBritish National Corpus\nSpoken Corpus Frequency\nRank popularity for the word 'motion' in Spoken Corpus Frequency: #2862\nWritten Corpus Frequency\nRank popularity for the word 'motion' in Written Corpus Frequency: #692\nRank popularity for the word 'motion' in Nouns Frequency: #883\nTranslations for motion\nFrom our Multilingual Translation Dictionary\n- κίνηση, πρότασηGreek\n- moción, movimientoSpanish\n- mouvement, motionFrench\n- gluasad, iarrtasScottish Gaelic\n- mouvmanHaitian Creole\n- javaslat, mozgás, indítványHungarian\n- movimento, mozione, mozioniItalian\n- 運動, 動き, 動議, 提案Japanese\n- 운동, 運動Korean\n- motio, motusLatin\n- pārvietošanās, kustībaLatvian\n- motie, bewegingDutch\n- movimento, deslocamento, moçãoPortuguese\n- предложение, движениеRussian\n- rörelse, motionSwedish\n- cử độngVietnamese\nGet even more translations for motion »\nFind a translation for the motion definition in other languages:\nSelect another language:"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:1ea86dcc-6e53-40e0-af37-86922d7f80f8>"],"error":null}
{"question":"How has the Golden Bean North America competition evolved into a significant event for coffee roasters, and what opportunities does it provide for industry professionals?","answer":"The Golden Bean North America roasters competition and conference has become a unique gathering that occurs only a few times each year where numerous specialty coffee roasters come together. The event serves multiple purposes beyond just competition. It provides an intense focus on evaluating the quality of products from roasters throughout the U.S. and Canada, while also creating opportunities for learning, socializing, and networking. The competition has shown significant prestige, as evidenced by achievements such as Mike Perry of Klatch Coffee's three-time Golden Bean top prize streak and the participation of accomplished professionals like the 2017 US Cup Tasters Champion Steve Cuevas, who became the Overall Champion Roaster.","context":["Only a few times each year will scores of talented specialty coffee roasters gather in a single room on U.S. soil. Among those opportunities, the Golden Bean North America roasters competition and conference is unique, in that it centers not only on an intense focus on the quality of products produced by roasters from throughout the U.S. and Canada, but also incorporates opportunities for learning, socializing and networking — a fit moment if ever there was one to reflect upon a roaster’s experience and the industry overall.\nAt the third annual Golden Bean North America event that took place in Portland, Ore., last week, Daily Coffee News took the opportunity to gather a six-by-six snapshot of the life and times of a roasting professional — six general questions asked of six randomly selected roasters in attendance.\nWe were fortunate enough to chat with:\n- Rio Prince of Corvallis Coffee Works, based out of Corvallis, Ore., winner of several bronze and silver medals in various categories both this year and last.\n- Golden Bean pourover/filter coffee Bronze Medal roaster Stacey Lynden, who’s been at it on Vancouver, BC-based Pallet Coffee‘s 15-kilo Joper for almost four years.\n- Mike Perry of Klatch Coffee, whose 21 years of roasting have culminated lately in a three-time Golden Bean top prize streak, among other distinctions.\n- Rhys Gilyeat, who’s rounding his first year roasting for Groundwork Coffee and used the company’s 2-kilo Primo sample roaster to snag multiple medals this year.\n- Will Andrews, roaster for Press Coffee out of Phoenix, Ariz., a Golden Bean gold medalist this year in the coveted Straight Espresso category.\n- Last but not least, 2017 US Cup Tasters Champion Steve Cuevas, the head roaster for Ukiah, Calif.’s Black Oak Coffee Roasters who has been manning a 15-kilo Probat and took home the title for Overall Champion Roaster at this year’s Golden Bean.\nFor our opening question, we asked:\nWhat do you think people mean when they describe roasting coffee as an art?\nWill: It’s this process of feeling the coffee out while it’s in the roaster, smelling it before, smelling it after. It feels more personal watching the coffee develop, watching it roast and smelling it rather than just watching lines go on Cropster. I like to start out with feeling it out as an art, and then once I feel like I know that coffee, then you delve into the science of it, why it tastes the way it does.\nMike: It’s a culinary art like cooking, because it’s about taste. That’s what you’re really trying to develop… You’re trying to find that taste, and taste is where the artistry comes in.\nRhys: There are factors that you want to hit at a certain point, but it really is up to the roaster to craft it and shape it and move it along the way. It’s just like being a painter; you have a certain image in mind, and then you fill in the blanks with your own strokes, while following your own trajectory. The artistry comes from you playing around with your machine and becoming intimate with it.\nSteve: That’s my favorite aspect about it. I’ve done a lot of different jobs — landscaping, carpentry. A lot of it I love because it’s unseen who does the work. You do landscaping, you move rock and earth to create something beautiful but you don’t see a signature on something. It’s the same thing with coffee, where I get to develop the flavor, you get to set the point of what it’s going to taste like, you get to help a team of people create a flavor, but you don’t have an ego about it. You don’t say, ‘This is my coffee.'”\nStacey: I think it’s that we’re putting effort into our craft; we’re not just trying to manipulate something. We’re trying to bring out the best qualities of that region, that bean, and you want to express the way that you feel about it as well, and share that with your consumer or your wholesale client or whoever else is coming along.\nRio: I think it’s the ability to taste a potential flavor aspect or aspects in the coffee that you can derive as a roaster just from experience… When you start getting a feel for, ‘What do I want out of this coffee? What are my choices?’ And then if you can pick, then you’ve got a pallet, and you’re painting a picture."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:812d4a18-c414-412d-9bad-2321ed19f06a>"],"error":null}
{"question":"Can you explain the main differences between colostrum and mature breast milk in terms of their composition, appearance, and timing of production?","answer":"Colostrum and mature breast milk differ in several key aspects. Colostrum, often called 'liquid gold' due to its deep yellow color, is the thick first milk produced during pregnancy and just after birth. It is rich in nutrients, antibodies for infection protection, and helps develop the baby's digestive system. It contains higher levels of vitamins A and E, protein, zinc, and potassium than mature milk. Mature milk, which begins production around days after birth, is bluish-white in color and has a higher content of fat and lactose. It contains about 88% water and provides all necessary nutrients, including over 20 amino acids needed for development of the brain, eyes, digestive system, and other tissues. It also uniquely contains alpha-lactalbumin, which aids in digestion and helps regulate milk synthesis.","context":["Added: Kacee Saulter - Date: 07.10.2021 05:31 - Views: 38887 - Clicks: 7319\nFederal government websites always use a. There are three phases of breast milk. Each one is vital in nourishing your baby. Ask WIC breastfeeding staff any questions you have about changes in your milk. This is the thick first milk your breasts make while you are pregnant and just after birth. Moms and doctors may refer to it as \"liquid gold\" for its deep yellow color and because it is so valuable for your baby. Colostrum is very rich in nutrients and has antibodies to protect your baby from infections. Colostrum also helps your baby's digestive system grow and work well.\nTransitional milk comes when mature breast milk gradually replaces colostrum. You will make transitional milk from days after delivery until up to 2 weeks after delivery. You may notice that your breasts become fuller and warmer and that your milk slowly changes to a bluish-white color. During this time, your breast milk changes to meet your baby's needs.\nNursing often, removing milk well, and relieving engorgement will help with milk production. About days after birth, you start making mature milk. Like each phase of breast milk, it has all the nutrients your baby needs. The amount of fat in mature milk changes as you feed your baby. Let your baby empty your first breast before switching to the other breast during a feeding. This will help your baby get the right mix of nutrients at each feeding. Planning to be apart from baby? Find tips for feeding baby with expressed milk. Many moms worry about low milk supply, even though most make exactly what baby needs.\nDepartment of Agriculture. The Phases of Breast Milk. Learn about the 3 phases of breast milk and why each is good for your baby. Phase 1: Colostrum This is the thick first milk your breasts make while you are pregnant and just after birth. Phase 2: Transitional milk Transitional milk comes when mature breast milk gradually replaces colostrum. Phase 3: Mature milk About days after birth, you start making mature milk.\nYour Breastfeeding Rights Know your rights to breastfeed in public and at work. Using Bottles with a Breastfed Baby Planning to be apart from baby? Breastfeeding Benefits Breastfeeding gives babies a healthy start—and is good for moms, too. . Breastfeeding Basics Learn how milk is made, when to nurse, how long babies nurse, and more. Low Milk Supply Many moms worry about low milk supply, even though most make exactly what baby needs. Steps and s of a Good Latch These tips help you get a good latch—and know if you have one.\nPumping and Hand Expression Basics New to milk expression? Finding a Breast Pump Here are options for finding a breast pump at low cost. Making Milk Expression Work for You Get tips and see how pumping can help solve your breastfeeding challenges. Footer Menu 4 Whitehouse.Adult lactation\nemail: [email protected] - phone:(187) 778-3040 x 8960\nCite This Item","Liquid Gold: The Amazing Nutritional Qualities of Breast Milk\nBreastmilk has often been called the “liquid gold” of infant nutrition. Unlike milk substitute (baby formula), the composition of human milk is not static and it adjusts to naturally meet your baby’s needs. For starters, your body will likely produce just enough colostrum in the first few days to fill your baby’s tummy, which is about the size of a marble. Over the next couple weeks, your milk supply will increase and the nutrients present in the milk will adjust to meet your baby’s needs.\nMilk production starts with colostrum. Although it may seem that those few drops of yellowish milk (probably less than 2 ounces over the course of an entire day) are not nearly enough to satisfy a growing newborn, this milk is some of the most nutrient-dense so it provides everything a baby needs in just enough volume to fill their tiny tummy. Colostrum contains high levels of immune factors, also called immunoglobulins, which help coat and seal the stomach lining that starts out permeable at birth, and helps prevent harmful pathogens from entering the digestive system. It also contains antioxidants and helps establish healthy bacteria for normal gut function while also acting as a laxative to help the baby clear meconium (the dark, tarry stool). Colostrum is higher in vitamins A and E, protein, zinc, and potassium than mature milk, which is higher in fat and lactose.\nTransitional and Mature Milk\nTransitional and mature milk have a high water content—almost 88%! Human breastmilk is all the fluid a baby needs, even in hot weather, to stay perfectly hydrated. Transitional milk is just as the name implies—the milk that transitions between colostrum and mature milk. This milk comes in around day 3-5 and lasts for about two weeks. By the time your baby is a month old, your breastmilk will be in the mature milk phase. Transitional and mature milk are usually lighter and whiter in color than colostrum, but the color of breastmilk can change based on any foods, medications, or vitamins you take, as well as the nutritional requirements needed to fight off illness for either you or your baby. Loaded with more than 20 amino acids, breastmilk provides nutrients necessary for the development of the brain, eyes, digestive system, nervous system, and other tissues in the body. Some amino acids may help induce sleep, which may be part of the reason babies love to fall asleep at the breast. A special feature of breastmilk that cannot be found in milk substitute is the presence of alpha-lactalbumin, which aids in digestion and helps regulate milk synthesis. Some studies even show it has cancer-fighting properties! There are also compounds, such as microRNAs that can help with gene expression and stop diseases from developing in the body.\nForemilk and Hindmilk\nForemilk is the milk that usually starts at the beginning of the feed. It tends to be thinner and is higher in carbohydrate content, providing a great source of hydration for the baby. The milk toward the end of emptying a breast is sometimes referred to as hindmilk and tends to be thicker and creamier because of its higher fat content. It is a good idea to empty your breast—whether it is in one complete feeding or by starting the baby on the breast that you left off on in the previous feeding—to help make sure the baby is getting all the nutrients from both the foremilk and the hindmilk, but over the course of a day, if the baby is nursing well, the overall milk consumption balances out so you don’t have to stress too much about what type of milk your baby is getting and when.\nEven after starting solid foods, babies get a majority of their calories from breastmilk, along with all the other important nutrients, so continuing to breastfeed for at least the first year of life is still the recommendation of the American Academy of Pediatrics. Every drop of breastmilk provides a little more of this nutritional liquid gold that helps keep babies healthy!\n*Nanobébé is thrilled to welcome guest bloggers. The views and opinions represented in these blog posts belong solely to the guest blogger and are not the legal responsibility of the company. The owner of this blog makes no representations as to the accuracy or completeness of the information provided by the guest blogger and will not be held liable for any errors or omissions of information nor for the availability of this information."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:3e7d9de3-3468-4cb7-bbef-59da693e2483>","<urn:uuid:db986587-3c26-40f1-b665-6cd192eb53dd>"],"error":null}
{"question":"What partnerships exist for food distribution programs, and what strategies are recommended to improve community nutrition access?","answer":"Food distribution programs rely on various partnerships, as seen with UC Merced's collaboration with United Way, Image Masters, and the Merced County food bank for their food pantry operations. The Health Plan of San Joaquin works with local food organizations and uses social workers to connect members to resources. To improve community nutrition access, recommended strategies include working with community- and faith-based organizations, businesses, and government leaders to increase healthy options at existing markets. Creating networks that include local farmers, food processors, and nonprofit organizations can help expand accessibility. Additionally, it's recommended to work with city officials and transportation departments to develop strategies for equal access to healthy food choices, and to schedule intervention activities alongside community social events to maximize participation.","context":["- Featured Businesses\n- Work Life\nWith school out for summer, food service programs continue to provide meals for needy students.\nLodi Unified School District’s Nutrition Services department served approximately 40,000 meals last summer for six weeks in June and July to students under the age of 18. Of these meals, 14,000 were provided to children not enrolled in summer school and at locations other than school campuses.\nAccording to Nancy Rostomily, Director of Nutrition Services for LUSD, the program’s goal is to provide healthy and nutritious meals to the area’s youth while school is out.\n“The food is prepared and served fresh by the Nutrition Services Department daily as part of the Federal Seamless Summer program that provides meals in low income areas during the summer,” Rostomily said. “The program combines features of the National School Lunch Program (NSLP) and the School Breakfast Program (SBP) that are offered during the school year.”\nThis summer’s program runs from June 5 through July 14.\nTo reach as many hungry children as possible, Lodi Unified reaches out beyond summer school students and partners with organizations such as the Salvation Army, YMCA, parks and recreation departments and the Boys and Girls Club.\n“Parents usually are looking for programs and activities for youth in the summer and this makes a perfect partnership. They provide an activity and we provide the food,” Rostomily said.\nHungry students are also enrolled at colleges and universities as well.\n“UC Merced’s food security program is linked to the UC system-wide food security goals to provide enough good, nutritional food for our students and to educate them around what that means, what that looks like for them,” said Vernette Doty, Associate Director for the Office of Student Life and Civic Leadership, University of California, Merced.\nAs part of the University of California’s Global Food Initiative, the focus is on students and their education of and access to food on campus.\nApproximately 200 to 300 students take advantage of the various food security services on campus each month.\n“These are low numbers relative to need. Students are not aware, that’s the main thing. Being able to increase our efforts to get the word out is going to help a lot,” Doty said.\nOff campus, Doty overseas a food pantry which is another vehicle for food distribution. The pantry distributes food on the third Friday of each month at Merced College and its goal is to enable anyone in need access to healthy food.\n“We started it specifically for our students but as a USDA distribution it has to be open to anyone, and so, gradually we began to see more and more community members come out to the distribution,” Doty said.\nCurrently, about 60 percent of the food is distributed to community members and 40 percent to students, a ratio that skews more towards non-students during the summer when the student population in the area drops.\n“To me it’s really important that everybody — students, community members, anyone — is treated exactly the same way, gets the same food, the whole thing,” Doty said.\nBetween 250 to 350 community members sign the USDA distribution sheet each month at the food pantry.\n“I can tell you that means between 500 and 700 people total when you count their families are benefiting from that distribution,” Doty said.\nThe food distributed through the food pantry is coming from the USDA primarily through the Merced County food bank, with occasional private donations from local gardens and farms. The UC Merced dining services also donates fresh produce to the distribution about three times a year.\nOther organizations help support the community food distribution through the food pantry.\n“We have a grant through United Way, so we partner with them,” Doty said. “We have also partnered with Image Masters. They donated some of our first round of reusable grocery bags and our food pantry tee shirts.”\nOn campus distribution is funded through the UC-wide Global Food Initiative with opportunities for students to access food about two times a month.\n“Those funds have to stay on campus and they go back to the students,” Doty said.\nHelping needy families find access to food is not limited to educational institutions. The Health Plan of San Joaquin (HPSJ), a not-for-profit health plan, does not provide food directly to its members but helps them find resources.\n“When food help is requested at any time during health care-related contacts with HPSJ staff, including our health navigators and customer service representatives, each inquiring member is given a warm hand-off to an HPSJ social worker,” said David Hurst, Vice President for External Affairs, HPSJ.\nThe social worker then identifies the most appropriate local food organizations from a resource list.\nThe health plan also works to get information quickly to its members via various forms of media about free school meals available in the area.\n“While health promotions are indirect, no child should go hungry in the summer because his family may not have heard about this tremendous resource, available in a school near them,” Hurst said.","- Grocery store availability. As the population density in urban and rural communities has decreased, large grocery stores have moved out. Often, only smaller stores are available to the community. These small corner grocery or convenience stores offer less variety of fresh foods and healthy items (Morland, 2002).\n- Quality of healthy foods. Urban and rural grocery stores operate at a lower volume and higher cost than large supermarkets. As a result, there is a reduced availability of fresh fruits, vegetables, meats and dairy products. In addition, the prices for the fresher foods are often higher (Zenk, 2005).\n- Access to nutrition education. Many nutrition concerns in rural America stem from the lack of proper education and available dietitians. In addition, schools may lack resources for adequate nutrition interventions (Tai-Seale, 2003).\n- Socioeconomic status. The socioeconomic status of individuals living in urban and rural communities might explain differences in healthy food intake (Drewnowski, 2004).The per capita income in rural areas is lower than in urban and suburban areas. Also, individuals living in these areas are more likely to live under the poverty level. For minorities, the disparity in income is even greater (NRHA, 2005).\n- Higher fat intake. Studies show that rural residents have a higher fat and calorie intake than others (Tai-Seale, 2003).\n- Sedentary lifestyles. Rural youth may spend more time watching television than their urban peers. This leads to an increase in snacking, increased desire for highly advertised foods, and less time participating in calorie-expending activities (Tai-Seale, 2003).\nStrategies to address these considerations\n- Improve availability of affordable, healthy food choices. Work with local community- and faith-based organizations, businesses, and governmental leaders to increase the healthy options available at existing markets and restaurants (Zenk, 2005). Some communities have offered grants, loans, and tax benefits to stimulate the development of neighborhood groceries, farmers’ markets, community gardens, and farm-to-cafeteria programs (IOM, 2005). A larger supermarket will carry more fresh items at lower prices (Zenk, 2005).\n- Improve access to healthy food choices. Creating a network that includes community groups, local government, nonprofit organizations, local farmers and food processors can help expand accessibility (IOM, 2005). It may be useful to work with city officials, urban planners, transportation departments, and faith-based organizations to develop city- or county-wide strategies related to land use planning and transportation so as to provide equal access to healthy food choices (Yancey, 2004).\n- Improve access to nutrition information. It may be useful to work with registered dietitians, school and worksite personnel, and community-based organizations and community members to ensure that the messages, content, format and placement of educational materials are appropriate for the population of interest. It may also be useful to work with healthcare providers in rural and urban settings to enhance their ability to convey appropriate nutritional counseling (Tai-Seale, 2003).\n- Use established settings. Strategies should maximize participation in the nutrition education intervention by having meetings or events at convenient locations and times (Bank-Wallace, 2002). It may be useful to schedule intervention activities with other church or community social events.\n- Convenient dissemination of nutrition information. Correspondence or web-based courses may prove useful in overcoming barriers to meeting places and times in rural areas (Tai-Seale, 2003).\n- Increase usage of supplemental food and nutrition information programs. Helping individuals get to the right place at the right time, and with necessary information, to enroll or participate in food and nutrition assistance programs can increase usage of food assistance programs (Strasser, 1991). Case management has also been suggested as a mechanism to help individuals overcome barriers and increase use of food assistance programs (Heslin, 2003).\n- School/Worksite Environment. Many successful interventions have focused on making more healthy choices available in schools and worksites as well as incorporating nutrition education and time for physical activity (Tai-Seale, 2003).\n- Involve the priority populations. It is important that individuals who are from the community of interest take an active role in planning, implementing and evaluating interventions (Bank-Wallace, 2002).\n- Engage community stakeholders. Leadership and active participation by community members, especially health care providers and community and religious leaders, can strengthen the credibility of and respect for the intervention (Bank-Wallace, 2002).\nPrint this window"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:1cc67421-84f9-42b1-a1da-50f4799346d3>","<urn:uuid:cd1cb9e4-296e-47b4-bdfe-6727a3fb0b35>"],"error":null}
{"question":"Could you explain the recommended formula for crafting an effective multi-paragraph business document? What are the key components and their purpose?","answer":"The formula for crafting an effective multi-paragraph business document consists of five main parts: 1) Begin with a first paragraph that introduces the main ideas to be conveyed. 2) Follow with paragraphs two through four that provide the content and support for the main ideas. 3) Each supporting paragraph should follow the basic formula of 4-7 sentences: an introductory sentence, 2-5 body sentences explaining the idea, and a concluding sentence. 4) End with a conclusion paragraph that summarizes the ideas presented and includes a decision or call to action. 5) While this 25-sentence structure (5 paragraphs x 5 sentences) isn't mandatory for all business correspondence, it provides effective guidelines for organizing thoughts and ideas, especially for those who are unfamiliar with business writing.","context":["This is a simple primer on written communications. Numerous books are available on writing for business and writing in general. I'll offer a piece of advice here: Writing for business is really not that different from any good writing.\nWhen it comes to a manual for effective writing, there is only one I need to recommend. Elements of Style by Strunk and White is considered the standard in the field. It covers basic writing rules and usage that, if adhered to, produce excellent results. Although many other books exist, I'll begin and end my recommendation here.\nWith that said, the sections that follow provide some guidelines to effective written communications.\nSome general guidelines for all forms of written communication are as follows:\n- Be brief— It's not about volume. The goal is well-understood and relevant content.\n- Make your point first, and then back it up— Those who write infrequently often begin by filling in details in an attempt to take the reader through their thought process. Don't! Instead, make the point and then fill in any necessary information. You will find in many cases that the point itself is sufficient.\n- Watch punctuation— A question is a question. Commas separate thoughts. It simply looks more professional and more intelligent to punctuate your work properly.\n- Break up your ideas— Use paragraphs to separate ideas. In the freeform world of e-mail, this rule is often abused. Long, unbroken paragraphs are difficult to read. In addition, use spacing between ideas to create a logical break for the reader.\nFew pieces of written correspondence have the impact of a well-written business letter.\nProperly address your letter. If you are unsure of the addressee, call the company or call the person who is receiving the correspondence.\nUse the person's name in the letter. This demonstrates your ongoing recognition of that person.\nCreate a logical flow in your letter. Use the tips for structure that are described in the section \"The Well-Crafted Page\" later in this chapter. This can help you organize your letter's ideas.\nKeep your intended recipient in mind as you write. Have a clear understanding of what this person is most interested in. Don't include tangential information that dilutes the primary message.\nMake a clear call to action or request. You are writing the letter to give information, request something, or spur some type of response. Make sure you let the person know what you expect as a result of your letter. Don't be vague.\nI have always found it surprising that individuals who normally write well-conceived and organized communications (letters or memos) throw all that out when it comes to e-mail.\nThis disregard takes the form of sloppy punctuation, lack of organization, a general disregard for capitalization, and other problems. For some reason, many believe that e-mail communications do not have to adhere to the normal rules that guide any of their other correspondence.\nIf that has been your approach, change it. With few exceptions, e-mail should follow most of the same guidelines that are used in other written correspondence. Just because it starts in electronic format does not mean it stays there.\nAlso, it is likely that your e-mail will be passed along to someone else—perhaps someone you do not know. Those scattered thoughts and run-on sentences might be some other individual's only exposure to you. Believe me, poor grammar in an e-mail can have an impact.\nIf your e-mail is a short clip of information, use the same guidelines as you would when creating a memo. If it is a longer correspondence, it should read—and be written—like a letter.\nI understand that there are exceptions. A quick yes/no type of clipped response can be appropriate. However, communications of any substance should not take on the roguish and unstructured format that is often present in e-mail correspondence.\nThe Well-Crafted Paragraph\nIf you have never been much of a writer, I am going to give you a basic formula for a well-crafted paragraph. Understand that I deviate from this formula often. It is a format to provide a guideline to help organize your writing. After you have learned this and practiced it well, you can depart from this form, too.\nYour standard paragraphs should have four to seven sentences. The introductory sentence introduces the main idea or topic. Two to five sentences form the body and should explain the idea introduced in sentence one. The final sentence concludes your thought by reiterating the original topic and the ideas presented in the body.\nCareer building is a long-term activity. It requires formulating a plan that involves coordinating skills and desires. After you've formed the plan, you can modify it based on your circumstances. However, a big picture mindset is required to ensure that changes in the plan are not based on reactions or compulsive behavior. Keeping such a perspective greatly enhances career development and opportunities.\nI'll admit it: The paragraph isn't great. But it does have a natural starting and ending point. The last sentence effectively summarizes the information presented. You would do well to follow this structure and pattern.\nBut what about the rest of your document? You now have a formula for writing a paragraph, but maybe you still can't put together a document.\nThe Well-Crafted Page\nFortunately, the formula for writing an effective paragraph can be used quite nicely to write a longer piece, too. In fact, the formula works well to organize thoughts in general.\nWhen writing a longer piece, such as a memo, use the following formula:\n- Paragraph one— Introduces the main idea or ideas to be conveyed in the memo.\n- Paragraphs two through four— Provide content for the ideas to be conveyed. Each paragraph, of course, still follows the simple guideline outlined earlier.\n- Conclusion paragraph— Summarizes the ideas presented and wraps up the document by issuing a decision or call to action on the ideas presented.\nI am not advocating that all business correspondence should be 25 sentences long (5 paragraphs x 5 sentences each). The formula simply provides guidelines for those who are unfamiliar or unpracticed in writing.\nYour writing will improve with use. And as it does, you will feel comfortable enough to move away from the formulas presented here. However, I still find myself following these same guidelines when words or ideas are hard to come by. They are highly effective."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"future"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:a08b9bef-34f8-46e4-a3a7-4f9bca623f8c>"],"error":null}
{"question":"Why is data quality important for machine learning models and how is it measured?","answer":"Data quality is crucial for machine learning and can be measured through several frameworks. These include Wang's four dimensions (Accuracy, Relevancy, Representation, Accessibility), Sherman's five C's (Clean, Consistent, Conformed, Current and Comprehensive), and Kahn's three categories (Conformance, Completeness and Plausibility). Kahn proposes evaluating these components through verification (checking intrinsic consistency) and validation (comparing values against external benchmarks). In machine learning specifically, data quality is essential for defining the search space and environment specification, which needs to be balanced with optimizing the learning method and model to achieve useful learning outcomes.","context":["At The Hyve, we have executed dozens of projects in the last few years to help customers make their data more FAIR in various ways: sometimes by assessing the FAIRness of data, sometimes by defining conventions for FAIRification, sometimes by building knowledge graphs. With such projects inevitably the question arises: What are the tangible benefits of implementing the FAIR principles? In this blog, I want to highlight one aspect in answer to that question: How FAIR data benefits machine learning applications or, in other words, the ‘machine actionability’ of data. But first, let’s quickly revisit what the FAIR principles have to say about machine actionability.\nRelation with the FAIR Principles\nWhich aspects of machine actionability of data are in the scope of the FAIR principles?\nThe latest ‘authoritative’ paper regarding the interpretation of the FAIR principles is from Jacobsen et al. 2020. It features the moniker introduced by Barend Mons et al.: FAIR requires that “the machine knows what we mean”. If we zoom in on the semantic aspects of machine actionability, the following principles are of particular importance:\n- F2: Rich metadata: machine-actionable metadata allow the machine to discover relevant data and services\n- I3: Qualified references to other data: allow the machine to detect and merge digital resources that relate to the same topic or entity, and to detect that a digital entity is compatible with an online service\n- Reusability: “Digital resources are sufficiently well described for both humans and computers, such that a machine is capable of deciding: (R1) if a digital resource should be reused (i.e., is it relevant to the task at-hand?); (R1.1) if a digital resource can be reused, and under what conditions (i.e., do I fulfill the conditions of reuse?); and (R1.2) who to credit if it is reused.”\nThe interpretation of reusability is especially interesting. The original definition of R1 is “(Meta)data are richly described with a plurality of accurate and relevant attributes”. The question ‘what does a machine need to decide whether a dataset is relevant to the task at-hand?’ is broader. Even if we go with the assumption that using attributes is sufficient to facilitate this decision making, the next question is: which attributes are needed for a given class of machine actors or for a particular purpose? To date, there seems to have been little work done to answer this question. R1 typically has few or no FAIR Maturity Indicators associated with it yet, so this would be a good avenue to further explore.\nWhich aspects of machine actionability of data are out of scope of the FAIR principles?\nThe FAIR principles do not cover aspects such as “ethics, privacy, reproducibility, data or software quality per se” (Mons et al. 2020). Some of these aspects are really crucial and inherent to machine actionability, in particular reproducibility and data quality. It seems a mistake that the FAIR principles would not explicitly include these aspects, and people have called this out in the past, even asking for a ‘new letter’, for example the ‘U’ of Utility (now covered in the explanation of R1). However, because the utility of data is so dependent on context, especially the intended use of the dataset, these topics may require guidelines and frameworks of their own. Now, let’s take a look at reproducibility and data quality in a bit more detail.\nIn the consensus report on reproducibility and replicability in science the American National Academies of Sciences, Engineering and Medicine defined reproducibility as “obtaining consistent computational results using the same input data, computational steps, methods, code, and conditions of analysis”, and replicability as “obtaining consistent results across studies aimed at answering the same scientific question, each of which has obtained its own data”.\nThis definition of reproducibility focuses on computational reproducibility. When it comes to measuring data from instruments, the field of metrology has developed a ‘vocabulary of metrology’ and has attempted to systematically identify uncertainty in measurement. In order to achieve replicability of for instance omics measurements, biological variability, uncertainty in measurement and computational reproducibility are all factors that contribute to the experimental results. Making these factors explicit in the metadata of the dataset – preferably by design and at the source – will greatly help data scientists that need to work with the datasets. So a major part of reproducibility is actually covered by the FAIR principles (R1.2, F2). Some of the projects we recently did focused on making these factors explicit in experimental datasets, for example using PROV-O and the OBO Foundry ontologies.\nData quality is a term that can be understood in many ways. In enterprise context, it often refers to master data management as defined by the ISO 8000 standards. In science, the quality of data is closely linked to the suitability of the data for (re)use for a particular purpose. There are many frameworks proposed in the literature for overall data quality, such as the four data quality dimensions (Accuracy, Relevancy, Representation, Accessibility) by Wang, the five C’s of Sherman (Clean, Consistent, Conformed, Current and Comprehensive), and the three categories from Kahn (Conformance, Completeness and Plausibility). Kahn also proposes two different modes to evaluate these components: verification (focusing on the intrinsic consistency, such as adherence to a format or specified value range) and validation (focusing on the alignment of values with respect to external benchmarks).\nIn the OHDSI community, we have documented in the Book of OHDSI how we understand these terms and how data quality, clinical validity, software validity and method validity all contribute to the eventual quality of the generated medical evidence. Again, for any specific intended use and context, some of these frameworks could be used to create computational representations of the data quality of a dataset, such as the one visualized in the OHDSI Data Quality Dashboard, which leverages the Kahn framework referenced above.\nIn machine learning, an important balance to maintain is that between adjusting the search space and environment specification (see e.g. Gym environment specifications in reinforcement learning) on the one hand and optimizing the learning method and model on the other. If the problem is not well defined the learning outcomes will likely not be useful. However, in practice defining the search space is a large part of the work of a data scientist, and another approach could be to formulate the data requirements for every model version. This would mean starting from the learning goals rather than from the existing data, and describing the requirements for data content, quantity and quality in a systematic way. These requirements can be used to identify or generate the datasets needed to develop the model. The FAIR principles are an excellent starting point to facilitate the creation of this feedback loop between data generators (data entry systems, lab equipment, data processing pipelines, et cetera) and data scientists. Popular open science machine learning websites such as PapersWithCode and OpenML demonstrate how powerful even a basic annotation of datasets, workflows and model runs can be.\nAs we have shown, almost all aspects of data utility for machine learning are covered explicitly or implicitly by the FAIR principles. The aspects of machine actionability of data can be ordered with respect to whether they are mostly intrinsic in the dataset (e.g. does the dataset have a globally unique and persistent identifier?), versus mostly dependent on the intended use. This ordering results in a ‘gradient’ which starts with elements that are explicit FAIR principles, but then can be augmented by domain-specific frameworks for data quality, reproducibility and fitness for purpose (see visualization below)."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:9edc1f86-0439-4fa3-be71-70dfd70bb9e4>"],"error":null}
{"question":"How do modern surveyors maintain their instruments with utmost care, and what historical challenges did early surveyors face in their work?","answer":"Modern surveyors must follow strict precautions in instrument care, including not lifting equipment by the telescope or circular plate, using proper carrying techniques, covering lenses when not in use, protecting from dust and sun, and ensuring proper storage. For compass instruments, they must prevent unnecessary swinging and lift off the pivot when not in use. Historical surveyors faced significant challenges including hostile territories with minimal army protection, magnetic anomalies affecting measurements, and difficult terrain. They used basic tools like magnetic compasses, transits, and Günter chains, working under harsh conditions that sometimes led to inaccurate or even fraudulent surveys.","context":["The surveyor is the person who is supposed to make precise measurement that will identify the boundary. Surveyors help in providing unique contours on the surface of the earth which will let the engineers to make maps and construction projects.\nWhat are the Duties of a Surveyor?\nThe following are the duties of a surveyor.\n- The surveyors have to measure the distance and the angles between specific points on the earth surface.\n- Based on reference points, certain important features points are located by traveling.\n- Detailed research is carried out on the records related to the land, survey, and the titles of the land.\n- The boundary lines are to be located by searching for the past boundary present in the site.\n- The surveying is conducted and the obtained results are recorded. Later they are verified for accuracy and corrections.\n- Based on the surveyed records plots, maps, the respective reports of the surveyed site is prepared.\n- The findings obtained from the surveying is presented to the clients and the respective government agencies.\n- The official land needs and water boundaries are established. These are established either for lease or deeds.\nWork Characteristics of Surveyor\nThe works performed by the surveyor can be divided into three different parts. They are:\n- Field Work\n- Office Work\n- Instruments Care and Adjustments\nField Work Conducted by Surveyors\nAs mentioned in the duties of surveyor, he has to determine the points and their respective distances and angles which have to be recorded in the form of field notes.\nThe operations involved in the field works are:\n- Initial establishment of benchmarks and all the stations as the reference. Based on these operations the horizontal and the vertical control is established.\n- The angles between the survey lines created have to be measured.\n- All the details of the survey are explained and located based on the stations and the lines between the stations. The details are the streams, buildings, milestones, streets and any other natural or man-made features present in the area surveyed.\n- For the constructions works related to buildings, culverts, sewers, bridges, and water supply schemes the lines have to set out and the grades are established by the surveyors.\n- Height or the elevation of certain points are determined by surveying by the surveyor. Or in either case, specific points have to be established in required elevations.\n- Carrying out topographic surveying. This is the surveying of contour of the given land which involves both vertical and horizontal controls.\n- Parallel lines and perpendicular lines have to be established.\n- The inaccessible points have to be measured.\n- To conduct survey past the obstacles. Many miscellaneous field works based on the trigonometric and the geometric principles have to be conducted.\n- Determination of the meridian, latitude, and longitude or to determine the local time by carrying out observations on the sun or a star.\nWhen dealing with field works, it is very important to understand the importance and the features of Field Notes.\nField notes are written notes that are prepared when the field work is carried out. There are situations when the whole precision of the survey and the field work becomes futile if the value recorded is wrong.\nHence the quality and competency of a surveyor’s work is more reflected in the field record when compared to the basic elements of surveying he performed.\nThe prepared field notes must be eligible, comprehensive, and Concise. The values and remarks have to be written in clean, clear plain letters and figures. There are certain basic rules to be keep while maintain a field note. They are:\n- As soon as the observations are made, the values are recorded in the field book\n- The pencil used must be sharp 2H or 3H. Never make use of a soft or ink pencil\n- It is recommended to have simple style of writing that must be consistent\n- Try to use liberal number of sketches\n- Each day of survey work must end up with a brief note. This must have the title of the work, the date of survey, the conditions of weather, the personnel involved, the list of the instruments and the equipment used.\n- It is recommended not to erase. If a mistake is made, rule through the mistake and the corrected value is marked above it.\n- The daily notes created must be signed.\nThe filed note consists of three parts:\n1. Numerical Values\nAll the recorded measurements like length, angles, staff readings, offsets are recorded in this column on the field book. All the figures that are significant must be recorded correctly. Values nearest to 0.01 must be recorded. A value observed as 342.30 must be written as same and not as 343.3.\nThe sketches will resemble the outlines, the topographic features, and the locations relative to each other. These are not made to the scale. If we plan to place the measurements on the sketches, correct belongings must be made while marking it. Always make a sketch which makes the interpretation easier. This must avoid maximum confusion. The sketches must be large, open as well as clear.\n3. Explanatory Notes\nAll those that cannot be conveyed through the values and the remarks must be written in the explanatory note section. These remarks will help to proceed the later works.\nOffice Works Done By surveyor\nDrafting, computing, and designing are the office works that have to be performed by the surveyor.\nThe drafting performed consists of the preparation of plans and the sections. These must be plotted to the measurement and to scale and prepare the topographic maps.\nThe computing process is done in two cases. Initially, it can be done for the plotting. Secondly, it can be constructed for the determination of the areas and the volumes.\nInstruments Care and Adjustments by Surveyors\nIf there is good experience in handling the equipment, the works can be carried out with great experience and precision. This knowledge on handling will make us understand the limitations of the instrument and can take proper care of these equipment.\nThere are many instruments like the level, theodolite, etc. that requires great care with delicate handling. There are many delicate parts in the instruments, if any of the parts get affected, it will bring total complaint and total loss.\nThe relative positions of the instruments must be checked while taking out the instrument and during its placement back after the use. Straining the parts of the instrument will result in the incorrect measurements.\nThe precautions that must be taken are:\n- The instrument must not be lifted by holding the telescope or the circular plate. We must hold the foot plate or the leveling base.\n- The movement of the instrument from one place to another must be carried out by placing it on the shoulder. All the clamps are set tightly.\n- When not in use, the lens must be covered.\n- Setting up the instrument on a smooth floor without any kind of precautions must be avoided.\n- The instrument must not be exposed to dust and dampness or direct sun rays. A waterproof cover can be used to cover and protect it.\n- Not leave the instrument without any guard.\n- The steel tape after use must be cleaned and kept dry. While measuring, vehicles are not allowed to move over it.\n- In case of compass instrument, the compass must not be allowed to swing without any need. This is lifted off the pivot when not in use.","Known as the world’s second oldest profession, surveying has been around since prehistoric times determining the advancement of civilization by the layout of multifaceted structures. In ancient times there was not the need for boundary surveying as it exists today, lands were shared by a common entity and individual land holdings were unheard of until more modern times. The surveyors of old were mostly doing the mathematics of measurement, laying out structures and developing base lines for which the buildings and infrastructure were determined.\nWith the pioneering spirit of homesteading, came the need for a more general layout of the lands as was previously adhered to in the Colonial United States, which was based more or less on European land holdings and grants and metes and bounds. The Land Ordinance of 1785 determined the system that Western States use today. The layout of the Public Land Survey System, PLSS, began in earnest in the mid and late 1800’s.\nThe surveyor of this era commonly used a magnetic instrument, a compass or, later, a transit, and a Günter chain, comprised of 100 links being 0.66 feet in length, making a chain 66 feet long. They would start their base lines based on celestial observations and survey the township grid based on the 80 chains to a mile, one square mile comprising 640 acres. Great admiration must be given to these early surveyors for their determination in surveying the townships through hostile lands with little or no assistance from the army for protection. Prior to 1900, corner monuments set at every half mile were stone, etched with a series of marks determining its relationship within the township. As in all aspects of life there were good and prudent surveyors as well as the not so prudent. The survey of townships was most generally let as a contract on a per mile basis, each township exterior being 24 miles in length with 48 corners and the interior subdivision approximately 60 miles with a total of 90 corners to be monumented.\nNow with this information you can easily perceive how the surveys became historically inaccurate or even fraudulent. The terrain, the magnetic anomalies, the hostile environs all played a part in the surveys being somewhat inaccurate or, in some cases, not even monumented at all. Re-tracement of these surveys is often a very arduous task, comprising of many hours on hand and knee scraping the ground for the physical evidence of the original survey, attempting to replicate the steps of that predecessor. Now a surveyor is a historian, keeping records and relying on the historic record in maintaining the integrity of the original land divisions.\nThat initial science of measurement has now turned into a mathematical miasma of relating measured observations to recorded or historic information and trying to maintain the integrity of the original land delineation. Draw a line on a piece of paper and have ten people measure it; you will have ten measurements that may or may not agree, to some extent. Now tell those same ten people what the length is and have them re-measure it, and deduce from that what is the true measurement of that line. Physically the line does not differ, but observations and perceptions differ, surveyors are the same. Several surveys over an expanse of time measuring between two monuments will differ, however slightly.\nCorners are accepted as true positions and relied upon by the owners. Property is exchanged and years later a new owner decides to improve his property and needs a survey. The new survey discovers the error in the original placement of a corner, and it is now brought to light. The true position of the corner is where it is, not where it should be as people had relied on its position in good faith. It has long been held by the courts, the true determiners of property lines, that physical evidence takes precedence over mathematical interpretation or measurement and that the original surveyor was never wrong."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:8dbfc198-ac98-46b0-a5c3-8b6a0d92cfff>","<urn:uuid:ae8cd4b6-8b49-467c-b254-55c0af692c7f>"],"error":null}
{"question":"How do Protestant Christian services and Jewish Shabbat services differ in their approach to music and singing during worship?","answer":"Protestant Christian and Jewish Shabbat services both incorporate music but in different ways. Protestant services typically include an opening set of music led by a worship leader, featuring hymns or songs, with either a worship team or organist/pianist performing. They also often include music during offering collection and after the sermon. In contrast, Jewish Shabbat services, particularly at places like Neveh Shalom, weave together ancient and modern melodies throughout the service, with special musical programs like SHIR! Musical Shabbat featuring a Cantor, Koleinu Choir, and musical ensemble. Both traditions use music to enhance worship, but Jewish services tend to integrate chanting and singing throughout the entire service, while Protestant services have more designated musical segments.","context":["- Religion and Philosophy»\n- Christianity, the Bible & Jesus\nHow to Design a Weekly Worship Service for Protestant Christians\nChurch services, whether on Sunday morning, Saturday evening or mid-week, are a vital time of fellowship and learning. Staying aigned with God's plan while balancing all the pieces that make a successful service requires good organization and communication.\n\"What then shall we say, brothers and sisters? When you come together, each of you has a hymn, or a word of instruction, a revelation, a tongue or an interpretation. Everything must be done so that the church may be built up....everything should be done in a fitting and orderly way.\"\nSeeking God First\nThe most important part of designing any service is asking for leading from God. Everyone, including both leaders and congregation members, need to be in prayer, with an attitude of submission for the Lord and a willingness to follow His call for that church.\nAll the other decisions must be built on that foundation!\nAfter that come the more practical considerations. Understanding who makes up the congregation as it presently is - older, families, younger attenders, etc, can help decide service times, elements and even the tone of services.\nEvery worship service consists of a series of elements. Many remain the same from week to week, but others may be utilized on particular Sundays to highlight events within or outside the church.\nYou'll need to have a good core of leaders who can help you find volunteers to fill each ministry slot for the service. Hopefully, each one who steps up to help will have a passion and gift for what they will be doing. Then, as it says in Romans, many parts will work together as one body!\n\"And He gave some as apostles, and some as prophets, and some as evangelists, and some as pastors and teachers, for the equipping of the saints for the work of service, to the building up of the body of Christ; until we all attain to the unity of the faith, and of the knowledge of the Son of God, to a mature man, to the measure of the stature which belongs to the fullness of Christ.\"\nLeaders Needed For A Service\nService Leader - this person opens the service, gives announcements and keeps the flow of the elements going smoothly. Sometimes they will lead a time of prayer and the offertory.\nWorship Leader - this person leads the music portion of the service, usually consisting of an opening set, an offertory piece, and possibly for communion.\nPastor/Speaker - this person prepares and delivers the sermon, and will most likely choose the scripture reading. They may deliver the Benediction, and offer up specific prayer concerns during the service.\nMajor Elements of A Worship Service\nNote: There is no one formula for a church service. Each church needs to seek out God's plan individually. This list is a starting point for ideas.\nThe opening time is meant to invite the congregation into worship. This is usually done by the day's Service Leader.\nThere is usually an opening set of some kind, to bring people deeper into worship. What kind of music (hymns, songs or some combination), and who will play them (a worship team or organist/pianist) will need to be agreed upon.\nHaving verses read, either by a service leader or another volunteer, that correspond with the sermon theme focuses people's hearts and minds. Offering (about 10 minutes)\nThe taking a monetary offering is based on scripture passages that talk about \"tithing\", or giving a tenth of what is earned to the church. This is usually done by the ushers. Often a piece of music is played during the collection.\nPrayer & Praise Time\nGiving church members the chance to ask for prayer or give a praise can bond the congregation together. The time allotment for this should be strictly followed, so the service doesn't run too far overtime.\nThe main section of teaching is most often done by the church Pastor, though it can be refreshing to occasionally bring in a guest speaker. Here, a scripture passage or Biblical truth is examined and explained.\nDoing one last song or hymn that also corresponds with the main theme punctuates the sermon.\nThis final blessing encourages the congregation to go out of the church into their week with renewed purpose. The Pastor or service leader does this.\nOn certain days additional elements might be added to the basics. They can bring excitement and interest to unique days, but must be inserted carefully to protect the flow of the service. Some of them include:\n- Special Music\n- Missions Moment\n- Personal Testimonies\n- Infant Dedication\n- Special Prayer concerns (missionaries, people leaving, etc.)\nWorship Ideas From Amazon\nHoliday and Other Special Services\nA church celebrating Easter and Christmas might include extra special music like a choir or brass ensemble, or additional scripture readings to tell the story of Jesus' birth or death and resurrection, or things like lighting candles or taking communion.\nSome churches even commemorate Thanksgiving: with a service and pie social, for instance. Patriotic days like July 4th and Veteran's Day, can include a salute to members in the congregation who've served in the armed forces.\nTo mark the \"Suffering Church\" awareness month in November, powerful videos or testimonies can remind us of our persecuted Christian brothers and sisters in countries where freedom of religion doesn't exist yet.\n\"Before the Service\" ministries\nBulletin Folders - During the week, the Pastor (or guest speaker) will decide what the elements will be for that Sunday's service. A bulletin template needs to be updated by the Pastor or perhaps the office assistant, then the final product must be copied, folded and put out for the greeters and ushers. This should be done by Friday.\nGreeters & Ushers - Greeters are an essential part of creating an atmosphere that brings people into worship. Their job is to meet members and visitors, either in the parking lot or at the door, and welcome them into the building. They make sure that everyone has a bulletin. Ushers are important, especially when the pews start to fill or if someone needs special help getting to a seat.\nVan Drivers - Every church community has people who for whatever reason can't drive themselves. Having a good team of drivers is invaluable for picking up those who wouldn't get to church otherwise.\nSunday School Teachers - Most churches offer some classes, at least for children and youth. This is a great way to offer people more learning opportunities. And most who come to Sunday School end up staying for the service. Equipping a team with training and materials helps everyone grow in knowledge.\n\"During the Service\" Ministries\nNursery Workers - It would be hard to hold effective church services without a nursery staff to watch the younger children. Parents are always appreciative when they know their kids will be well taken care of while they have a chance to worship. The workers all need to go through Good Shepherd or a similar kind of training, for everyone's protection, especially the children's.\n\"After the Service\" Ministries\nCoffee Hour Hosts - Lots of churches offer a social hour after the service ends. Many people like to have that relaxed time to connect with good friends, and to make new ones. To make it work, there should be a team that sets up beforehand, making coffee and hot water for tea, setting out some goodies to eat, and putting up tables and chairs. A group will be needed to clean up afterwards as well.\nPrayer Team - When someone is hurting, confused, or simply needing some support, they can be greatly encouraged by sitting for prayer with someone. Having a roster of people who are willing to listen and pray ensures that no one needs to leave the service feeling alone.","Friday and Saturday Shabbat Services\nNeveh Shalom offers several Shabbat service options to meet the needs of our diverse community. See the sections below for more information about each one.\nUpcoming Special Shabbat Events\nEvery Friday, 6:15 pm\n(except the 4th Friday of the month)\nJoin our clergy for a traditional Kabbalat Shabbat service where we weave together ancient and modern melodies as we celebrate Shabbat.\nSpecial Fridays with Ilene Safyan\nSelect Fridays, 6:15 pm\nBring your spirit and your voice for a special Kabbalat Shabbat. Accompanied by Ilene Safyan on guitar, this service is filled with beautiful music, singing and lots of participation.\nSHIR! A Musical Shabbat\nSecond Fridays, 6:15 pm\nImmerse yourself in the musical ruach of Kabbalat Shabbat and make it a truly uplifting experience. With Cantor Eyal Bitton, the Koleinu Choir and the Shir! Musical Ensemble.\nLiving Room Shabbat\n4th Friday of the month, 8:00 pm\nThis casual, laid-back service replaces the 6:15pm service once a month. Led by Rabbi Kosak, Cantor Bitton and musical friends, enjoy Shabbat in a relaxed setting.\nFourth Friday with Rabbi Eve\n(0-6 Years Old)\n4th Fridays, 5:15 pm\nRabbi Eve Posen leads this interactive home service that welcomes Shabbat through song and stories for families with children 0-6. Fourth Friday of each month at 5:15pm. RSVP: firstname.lastname@example.org.\nShabbat Morning Service\nEvery Saturday, 9:00 am\nTraditional weekly Shabbat morning service. Free Shabbat child care available at 9:30am by advance reservation only. Please call 503-246-8831 by noon on the preceding Wednesday to reserve space.\n2nd, 4th, and 5th Saturdays, 9:30am*\nA lay-led alternate Shabbat service followed by a Kiddush lunch and singing of z’mirot. Interested in leading a part of the service or reading Torah? Contact: email@example.com. *When there is no Bar/Bat Mitzvah or special Shabbat programming, Downstairs Minyan will join the main service at 9:00am.\nYouth and Family Services\nAll families are welcome to attend any of our regular services. In addition, we have a number of age specific Shabbat morning services for young families and youth. Click here for these services.\nMore About Shabbat\nShabbat (Hebrew for Sabbath) is the Jewish day of rest and symbolizes the day of rest after six days of creation. Shabbat starts at sundown Friday and ends at sundown Saturday. It’s traditionally a time to take a break from regular activities, to spend time with family and friends, and to attend synagogue. Observant Jews refrain from using electricity, driving cars, writing and working on Shabbat. Many Conservative Jews adapt these customs to fit their own level of observance.\nThe Shabbat morning service at our synagogue is similar to services at synagogues around the world. Neveh Shalom is part of Judaism’s Conservative movement (called Masorti outside North America), which one could say is “middle of the road.” Conservative Jews keep many traditional practices (such as observing Shabbat and keeping kosher) while adapting customs and rituals to reflect the times in which we live. Also helpful to keep in mind is that Neveh Shalom is a fully egalitarian synagogue, meaning men and women share the same roles and responsibilities.\nMost of the service is chanted Hebrew, with a few English sections, and there’s lots of singing throughout the service. If you’re not familiar with Hebrew texts, you may notice that the prayer book seems “backwards,” since Hebrew goes from right to left. The rabbi will call out page numbers as we move through the service. The prayer book (or siddur) is in Hebrew and has both English translations and some transliteration Hebrew words written in English) of the prayers.\nThere are several times during the service when the rabbi will ask the congregation to stand or sit; it’s pretty easy to follow along.\nOn Saturday mornings, there are four main parts to the service:\n- During Psukei D’zimra (around 9:00 to 9:30), we chant psalms and prayers that help warm up for the rest of the service.\n- The Shacharit section (around 9:30 to 10:00) includes the Sh’ma, a selection from the book of Deuteronomy that is a central part of every morning and evening Jewish prayer service, and the Amidah, a time for silent prayer and meditation.\n- The Torah Service will start around 10:00. It’s customary for Jews to kiss the Torah as it passes them. The Torah is divided into weekly portions, and the entire Torah is read during the course of the Jewish year. The same portion is read in synagogues around the world. Several family members and congregants will be honored with an aliyah and say blessings before each section of the Torah is read. After the Torah is read, the Haftarah, which this week is a selection from the book of Judges is read. After the Torah service, there is an explanation of the Torah portion (d’var Torah).\n- The final service of the morning is Musaf, the concluding service.\nIf there is a bar or bat mitzvah, the bat/bar mitzvah student will lead some parts of the service, and other parts will be led by clergy and lay people. For more information, a helpful website for basic knowledge about Judaism is My Jewish Learning."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:c31818bc-e32e-4d83-8873-cce0a63d65db>","<urn:uuid:9028d5fd-f3ff-4d26-ae5b-cb92649edd28>"],"error":null}
{"question":"What's the key difference between PowerPoint and paper prototypes when it comes to their effectiveness in UI design testing?","answer":"PowerPoint and paper prototypes serve different purposes in UI design testing. PowerPoint offers basic interactivity and the ability to easily import graphics, making it useful for showing what happens when buttons are pressed through hyperlinks and for creating more polished interface mockups. In contrast, paper prototypes, while lacking in realism and aesthetics, excel at being fast, cheap, and computer-less, making them particularly effective for testing specific interactions or competing concepts early in the design process. However, paper prototypes have the disadvantage of not being easily distributed, while PowerPoint prototypes can be more readily shared.","context":["Office isn’t only for documents: it’s a tool that can fit almost anywhere in your application development process.\nYou might not think of Office as a development tool, but it’s been at the heart of many development toolchains for a long time now. The two most popular tools aren’t the obvious ones either, with both Excel and PowerPoint fulfilling different roles in development, as part of a formal application development program and as a way of quickly building apps that solve urgent business needs, using users’ own programming skills.\nExcel: a secret programming environment\nExcel is a particularly interesting example of a development tool. Under the hood of what looks like a familiar spreadsheet is a set of programming languages that can be used in very sophisticated ways. As Simon Peyton Jones, a principal researcher at Microsoft Research, notes, it’s a tool that’s increasingly moving into the world of functional programming. Peyton Jones works in functional programming, where he’s perhaps best known as one of the main contributors to the development of Haskell, and for his work on the theory of lazy functional programming.\nHis MSR work has been very influential in the development of Excel’s programming environment, working to move its function model away from the ageing Visual Basic for Applications language and tooling. One key feature that’s come out of this research is the concept of dynamic arrays.\nWorking with data and formula\nWith dynamic arrays, a single formula can ‘spill’ into other cells, filling empty cells in your formula’s spill range with the results of a calculation — sorting a list and filtering out duplicates, for example. There’s new cell notation to help manage the dynamic nature of these new arrays, so you can work with the contents of an entire spill range with a single cell reference, simply by adding # to the cell reference of the first cell. To go with this new tool you get new functions, to help manage and test arrays, as well as the ability to work across rows and columns.\nClosely related to this is the addition of new data types to Excel, which are linked directly to external data. For example, you can define a cell as geographic data, which brings in lots of extra data that can be extracted either in functions or dragged straight into their own columns, without having to define a source for the data. That same data can be converted into map charts, giving you a framework for building more complex applications.\nDebugging with ExceLint\nAs Excel adds more and more functionality to its formulas, you’re going to need tools to help debug them. That’s where the ExceLint add-in comes into play. Available on GitHub, it’s a tool for debugging and auditing formulas across your workbooks. Once installed, it adds a new tab to the Excel ribbon, with an audit button that starts a guided audit of your spreadsheet.\nSuspected errors are highlighted in red, with closely related correct formulas in green. By comparing formulas like this, you’re given a hint to the shape of a solution, with the green highlights a proposed fix for a problem. It’s not saying that this is the correct formula to use, more that the formula should be of this form. You can then step through cells to find more problems.\nAnother option is a global view. This uses colour to give you an overview of the structure of the formulas in your spreadsheet, each block of colour being a set of related formulas. It’s still a work in progress, but there’s a lot of promise here, especially in giving you a visual way of finding errors in what can be very complex spreadsheets.\nPrototyping applications in PowerPoint\nWhile Excel excels in code, other aspects of the software development lifecycle are embodied in other Office applications. It’s important to get your application user experience right, and as early as possible to be sure that you’re delivering what your users want. That requires mocking up and sharing prototypes, even before you’ve written a line of code.\nOffice has long been a useful prototyping tool for user interfaces, using PowerPoint’s hyperlinks to show what happens when buttons are pressed. Mapping a link to a section of an image is easy, as is filling a deck with images illustrating the various states. You can start simply with application wireframes and add content as user interface elements are finalised.\nThere’s a long-standing problem with user interface prototypes, as their fidelity improves it’s easier to mistake them for a completed project. Even putting a small script-based mock behind a button can make it seem that there’s real code running your UI prototype. So how then to use Office tools to demonstrate application concepts?\nUsing Sketched Shapes to emphasise works-in-progress\nOne answer comes from the work of Microsoft Research’s Bill Buxton. In his book, Sketching User Experiences, he argues that sketches are a powerful tool for rapid prototyping of user experiences. A notebook and a pen are powerful tools for quickly putting together rough user interface concepts, and for sharing them with colleagues and users. That model became part of Visual Studio’s XAML tooling with the release of sketch-like user interface components for prototypes, where it would be impossible to mistake an experiment for shippable code.\nRecent releases of Office bring that same sketch concept to its drawing tools with its new Sketched Shapes feature. Currently shipping in Insider builds of Office, in Word, PowerPoint, and Excel, it’s a new outline option that replaces formerly regular object outlines with line styles which mimic hand-drawn lines. You can pick from a selection of different line styles to give a different feel to different elements, perhaps giving a rougher look to ideas that are still being discussed, and a smoother feel for those that look ready to be locked down for production.\nAdding sketches to your prototypes\nAdding a sketch look to a shape is easy enough: select the shape, and from the Shape Format tab in the ribbon choose Shape Outline to pick the line style you want to use. The same process can be used to go to a more formal line style when you want to stop showing sketches; so you can update a design without having to redraw it from scratch. It’s possible to set a sketch style as your default for a document, so all your drawings automatically get the same sketch look-and-feel.\nSketch styles can be applied to lines as well as Office’s standard library of shapes, using freeform shapes to draw directly on the screen. Another useful trick is to convert icons and other design elements to shapes using the Convert to Shape function, before applying a sketch line style to the object.\nOffice continues to evolve, and its partnerships with Microsoft Research are giving it more developer-friendly features, without affecting its role as a day-to-day productivity tool. There’s a lot for developers and development teams in the latest builds, and it’s well worth keeping more than a few of your development teams in Insider builds of the Office applications, so you can get access to these features as quickly as possible.","2. What we will cover…• Background• What is a prototype?• Why use prototypes?• A brief look at the toolkit• Prototyping principles\n3. What is the User Experience (UX)?user experience:n. the overall experience and satisfaction a user has when using aproduct or system\n4. A User-Centred Design (UCD) process Concept/Plan Design • Contextual Analysis • Wireframing • User Profiling/Persona • Prototyping development • IA Analysis • User Needs Analysis • Co-Design Workshops • Competitor Analysis Live Support Evaluate • Customer Surveys • Usability Testing • Analytics • Expert Evaluation • A/B Testing • Eye-tracking • Multi-variate Testing • Accessibility Audits\n5. Where are we…?• Background• What is a prototype?• Why use prototypes?• A brief look at the toolkit• Prototyping principles\n6. What is a prototype?“An approximation of a product (or system) or its components, in some form, for a definite purpose in its implementation” (Chua, Leong & Lim) “A visualisation of the requirements” (Arnowitz) “A representative model or simulation of the final system” (Warfel)\n7. What is a prototype?\n8. What is a prototype?Three dimensions: 1. Scope (Distinct aspect Entire product/service) 2. Form (Abstract Tangible) 3. Fidelity (Rough representation Exact representation)\n10. Where are we…?• Background• What is a prototype?• Why use prototypes?• A brief look at the toolkit• Prototyping principles\n11. The goal of prototyping “The goal of prototyping is to convince yourself and others of an idea” An idea has no value Unless it can be communicated! (Raskin)\n12. Why use prototypes?“My perspective is that the bulk of our industry is organized around the demonstratable myth that we know what we want at the start, and how to get it, and therefore build our process assuming that we will take an optimal, direct path to get there. Nonsense. The process must reflect that we dont know and acknowledge that the sooner we make errors and detect and fix them, the less (not more) the cost.” (Bill Buxton)\n13. Why use prototypes?Prototyping allows us to...  Brainstorm  Design  Create  Test  Communicate ...interaction design concepts and user interfaces, early in the design process and in a cost effective manner.\n14. Why use prototypes?Benefits  They help to generate ideas  They can communicate aspects of the design that cannot be adequately communicated by other artefacts  They increase understanding, add clarity and reduce misinterpretation  They can be updated quickly to reflect changes  They can enable quicker identification of mistakes and risks When applied early and often, the use of prototypes can save time and effort, reduce waste and ultimately save money\n15. Where are we…?• Background• What is a prototype?• Why use prototypes?• A brief look at the toolkit• Prototyping principles\n16. A look at some tools #1 Paper #2 Office tools #3 Vector drawing tools #4 Web based tools #5 Purpose-built prototyping tools #6 HTML\n17. #1 - PaperPaper, pen, scissors, tape & post-its (Blue Peter prototyping) Best use: To test specific interactions or competing concepts Advantages: Fast, cheap, computer-less, lack of realism/aesthetics Disadvantages: Not easily distributed, lack of realism/aesthetics\n18. #2 – Office toolsPowerPoint, Excel, Keynote Best use: To add basic interaction to flat designs, dashboards (Excel) Advantages: Cheap, easy to pick up, easy to import graphics, some interactivity, basic data/graph incorporation (Excel) Disadvantages: Largely linear, limited editing/drawing\n19. #3 – Vector drawing toolse.g. MS Visio, Omnigraffle, Adobe InDesign, etc. Best use: Medium/high-fidelity screen mock-ups Advantages: Use of stencils, precise layout, potential richer interactivity Disadvantages: More cost, interactivity requires coding knowledge\n20. #4 – Web-based toolse.g. Protoshare, Mockingbird, Protonotes Best use: For distributed teams Advantages: Online, collaborative, easily shared Disadvantages: Less rich interactions, no HTML export\n22. #5 – Prototyping toolse.g. Axure RP Pro, iRise, MockupScreens, Balsamiq, Fireworks Best use: More complete/complex models Advantages: Fast, rich interaction, collaboration, generate specs, HTML export Disadvantages: Costly, longer to pick up, not (always) reusable\n23. #5 – Prototyping toolsExample: Axure RP ProFeatures: Drag-and-drop Custom widgets and masters Rich interactions Multiple platform templates Mobile prototypes Collaboration/version-control Export to HTML Word/PDF specs\n24. #6 - HTMLFully-fledged web pages Best use: For finalising design decisions Advantages: Full interaction, expandable, easily transportable Disadvantages: Time and effort, requires expertise\n25. Where are we…?• Background• What is a prototype?• Why use prototypes?• A brief look at the toolkit• Prototyping principles\n26. Six Prototyping principles (Raskin)#1 Your first try will be wrong.• No matter how good you are, there is no substitute for trying it out• Budget for it• Design for it\n27. Six Prototyping principles (Raskin)#2 Aim to finish a usable artifact in a day• This helps you focus and scope• Do less• Don’t be afraid to start again\n28. Six Prototyping principles (Raskin)#3 You are making a touchable sketch• Do not fill in all the blanks• Focus on key contentelements• Remember the goal of the prototype\n29. Six Prototyping principles (Raskin)#4 You are iterating your understanding ofthe problem as well as your solution• Use the process to evaluate, validate and clarify your requirements• Be prepared to admit you were wrong!• Establish a tight feedback loop\n30. Six Prototyping principles (Raskin)#5 Borrow liberally• Don’t reinvent the wheel• Don’t waste time with the painting and decorating\n31. Six Prototyping principles (Raskin)#6 Tell a story with your prototype• Think about your personas• Think about your user’s journey• It isn’t just a set of features• Sell the idea!\n32. Other resources Prototyping Paper Prototyping Effective Prototyping Sketching User Todd Zaki Warfel Carolyn Snyder Jonathan Arnowitz Experiences Bill BuxtonA few web resources:• Prototyping Tools Review (http://goo.gl/QHI6m)• “Prototypically speaking” prototyping blog (http://softwareprototyping.net/)• Effective Prototyping site (http://www.effectiveprototyping.com/)\n33. Eye trackingQuestions?\n34. Thank You Stephen Denning Senior User Experience Consultant User Vision 55 North Castle Street Edinburgh EH2 3QA T: 0131 225 0850 E: email@example.com W: www.uservision.co.uk"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:8f30b5c3-6621-4e6c-b569-68c76892af09>","<urn:uuid:5d355df5-4a3a-417e-bb74-fe4977cfd5d1>"],"error":null}
{"question":"Which aircraft was more successful in actual combat - el famoso Flying Tigers' P-40 Warhawk or el ME-262 jet? How many enemy planes did each shoot down?","answer":"When comparing confirmed aerial victories, the Flying Tigers' P-40s achieved 297 confirmed enemy aircraft kills (plus 153 probable) in just 6 months with only 12 planes lost in air combat. The Me-262 pilots claimed a total of 542 allied kills, though the documents note these claims were often higher than actual downings. While both aircraft achieved significant success, the P-40's confirmed kill-to-loss ratio with the Flying Tigers appears more impressive, especially considering they were flying against overwhelming enemy numbers with conventional piston-engine technology.","context":["THE CURTISS P-40 WARHAWK\nThe P-40, with the distinctive shark's mouth painted on its nose, has always been one of the most recognizable fighters of World War II. Yet few realize that it is also one of the most controversial. The Curtis P-40 is thought by many to have been slow and obsolete from its inception. Its role in the defense of the Pacific during the early years of the war has been minimized. The Warhawk was, in fact, a much better fighter than most observers believe.\nThere are three main reasons for this misconception. For one, the P-40 was based on an older aircraft, the P-36. The forward section and the liquid cooled Allison V-12 engine (V-1710) were new, but from the firewall to tail it was exactly the same as the P-36. Because of this, the P-40 is thought to have been obsolescent from its inception. Its naval contemporary, the F4F Wildcat (which is described as being a better opponent for the Zero by World War II magazine) was based on a biplane design! Of course, few authorities mention that. The P-36 airframe wasn't obsolete, merely proven successful. It was actually very sturdy. Secondly, newer fighters, including the P-38, P-47, and P-51 overshadowed it. Finally, its faults (and it had some--all aircraft do) were exaggerated to the point that it seemed impossible for the P-40 to succeed against any enemy aircraft. Although it couldn't out maneuver the Zero (the Warhawk's main foe in the Pacific Theater), neither could the Spitfire, Hurricane, Lighting, Thunderbolt, Mustang, Wildcat, or Corsair, but that is never mentioned.\nThe P-40 concept began life in 1937. It was at about this time that the United States government began to understand the position they were in. The Nazis were preparing to crush Europe and the Japanese were taking bites out of China. U.S. leaders realized that neutrality wouldn't last, and that the need for arms was great. Because of that the Army Air Corps issued a specification for a new fighter that could be produced quickly.\nSeveral companies threw their hat in the ring with various aircraft designs. Curtiss offered their P-40 design; Lockheed came with their P-38 design, and Bell with the P-39. When Curtiss won the contract, the other companies created an uproar. They believed the P-40 was obsolescent, since much of it was based on an older design. As previously explained, from the firewall back it was the same as the previous P-36, but the engine and other vital components were new, which increased performance dramatically.\nThe Army wanted a sure-fire design. The twin-engine P-38 was too radical to be certain of success. It probably didn't help when the P-38 prototype crashed. One drawback to the P-38 was that it was not designed to be mass-produced. It was designed as a special purpose, high altitude interceptor. The P-38 simply could not be produced in sufficient quantity in time. Also, at the time, the Army did not foresee the need for great numbers of high altitude fighters that later became apparent. They thought a fighter's main role was ground support, and found the P-40 well suited for the job. Besides, if the Army had chosen the P-38, American pilots would have initially been outmatched in dogfights with the more nimble Japanese Zero, and probably taken as many losses as the P-40 did. In that case, I would be sticking up for the misunderstood Lockheed P-38!\nTwo myths about the P-40 were that it was slow and not maneuverable. Compared to later American and German aircraft with 400+ mph top speeds, a mere 345 mph at 15,000 feet (the top speed of the P-40C) doesn't seem that impressive. But remember, in 1940-41 the Warhawk's top speed essentially matched that of the Spitfire 1A (346 mph at 15,000 feet) and Bf-109E (348 mph at 14,560 feet), and surpassed the A6M-21 Zero (331 mph at 14,930 feet) and Hawker Hurricane II (327 mph at 18,000 feet).\nThe famous Mitsubishi Zero fighter was more maneuverable than the new Curtiss fighter was, and because of this people came to believe the P-40 couldn't out turn a hot air balloon. Of course they didn't take into account that the Zero was the most maneuverable fighter of the time. In fact, the P-40 proved in combat that it could out maneuver many of its rivals. Other positive attributes of the P-40 were good armor, firepower, roll rate, and dive speed, making it one of the best low altitude fighters of the war. Japanese pilots rated the Warhawk their most dangerous foe at low altitude. (They considered the P-38 Lightning best at high altitude and the F4U Corsair the best overall). The P-40's chief drawback was a slow climb rate, and climbing to escape an enemy was considered suicide.\nFollowing its acceptance by the Army Air Corps in 1940, the P-40 was quickly produced and sent to several American air bases. There, the pilots were glad to get new aircraft. The British also received P-40s and matched it against the legendary Messerschmitt Bf-109. The Tomahawk (as the British called the early P-40 models--they named the later models Kittyhawk) did well in combat with the famous German fighter. Although it was a bit slower and outclassed in rate of climb, its good dive speed, superior armor, maneuverability, and armament made the Tomahawk a force to be reckoned with. The Germans felt that it was a more dangerous opponent than the Hawker Hurricane. In fact, the British wanted to replace their old Hurricanes with new P-40s. Other countries that operated P-40s (early P-40, B, and C export models were referred to as the Hawk 81A by Curtiss; later P-40D, E, K, and M export models were known as the Hawk 87A) included Canada, Russia (the USSR), Brazil, New Zealand, Iraq, Egypt, Turkey, and China, among others.\nThe specifications for the numerous P-40E-1 of 1941 (also Hawk 87A-4 and Kittyhawk IA), which are similar to the entire series of P-40E, K, and M models, are as follows (taken from The Complete Book of Fighters by William Green and Gordon Swanborough): Max speed, 362 mph at 15,000 feet; Time to 5,000 feet, 2.4 minutes; Max range, 850 miles at 207 mph (with drop tank); Armament, six .50 inch (12.7mm) wing mounted machine guns; Empty weight, 6,900 pounds; Loaded weight, 8,400 pounds; Span, 37 feet 4 inches; Length, 31 feet 9 inches; Height, 12 feet 4 inches; Wing area, 236 square feet.\nOn December 7, 1941 a large Japanese air armada attacked the U.S. Navy base at Pearl Harbor, and all of the nearby airfields. This is where the P-40 first defended American soil. Unfortunately, few American fighters were able to get into the air; most were destroyed on the ground. Two pilots, George Welch and Kenneth Taylor, managed to take off and shot down several Japanese planes. From this time onward a great responsibility fell on the P-40 pilots' shoulders. In order for the U.S. to win the war, air superiority must be wrested from the Japanese. Otherwise, their bombers would be able to fly unopposed over Allied ships and territory. I've read descriptions of the Warhawk that would lead you to believe that it was the worst possible fighter for the job. Yet, it was able to do just that for the first two years of the Pacific war, and continued to help do so until the end of the war.\nThe P-40's first fame came at the hands of the now legendary Flying Tigers, a group of American mercenaries who volunteered to defend China against the Japanese. Well before the war reached America, President Franklin D. Roosevelt realized that the U.S. would eventually be drawn into it. Most Americans felt strongly about staying neutral, but Roosevelt wanted to aid the Allied powers against the fascist juggernaut. However, an election was coming and his toughest rival was a strong isolationist. Political expediency demanded that he promise continued neutrality (sadly, not the last time an American president would deceive the electorate). The A.V.G. (American Volunteer Group) was a way around this. The program was to send volunteers from the U.S. Army, Navy, and Marine air corps to China, along with U.S. fighters, and establish three squadrons to combat the threat from the rising sun. Colonel Claire Lee Chennault headed the operation. He pieced together a fighter group and shipped them to China with 99 (a crane mishap destroyed the 100th) new P-40 (actually export model Hawk 81A-2) aircraft originally intended for British use. Aircraft numbers 1 through 33 formed The Adams and Eves, 34 through 66 made up The Panda Bears and 67 to 99 created the most famous of the three squadrons, The Hells Angels.\nDuring the following weeks Chennault ran his crews ragged, training them to fly and maintain the P-40. He understood its shortcomings, mainly inferior turn rate and a slow rate of climb, and taught his men to maximize its effectiveness by using the P-40's superior top and diving speed to hit and run. He also preached fighting in pairs, rather than in Vics of three or singly, as Japanese doctrine taught. It took time, but by early December 1941 the A.V.G., with ferocious looking shark's jaws painted on their P-40's (an idea copied from the RAF's No. 112 squadron), was ready for action. By this time 33 planes had already been lost, mostly due to pilot error.\nTheir first big chance came on Dec. 8, a day after the surprise attack on Pearl Harbor. They attacked ground targets and engaged enemy aircraft in defense of the Burma Road, China's only supply line to the West. For the next 6 months this rag-tag band of volunteer pilots racked up an amazing record against overwhelming enemy numbers, earning the A.V.G. the nickname \"Flying Tigers\".\nOn July 4, 1942 the Flying Tiger's contracts were due to expire. After that, the U. S. Army intended to take over and make this back into a military group. Nearly all of the volunteers objected to this, including Col. Chennault, who tried to prevent it. But there was no reasoning with the Army; so on July 4th the group that had single-handedly defended the skies over China was disbanded. Most of the Flying Tigers went back to their old units. Only five pilots stayed with Chennault to help train the newcomers. The records show that after only 6 months of combat the Flying Tigers had shot down 297 enemy aircraft confirmed, and another 153 probable, for only 12 planes lost in air combat. But 57 of their original 99 P-40's were no longer air worthy due to combat damage, pilot error, maintenance problems, or just being worn out.\nThe dissolution of the A.V.G. was by no means the end of the P-40's involvement in China. The Army sent updated models of the P-40 and fresh pilots to form the China Air Task Force (CATF). Chennault (now a Brigadier General) tried his best to make it the most effective outfit possible. When the new pilots arrived in China, they found themselves in a difficult position. They were isolated in a remote theater, with the giant Himalayan Mountains to their backs, which allowed only a trickle of supplies to come through. Also, they were outnumbered more than 3 to 1 by the Japanese. The CATF's entire complement of Warhawks did not equal the number of planes assigned to just one of the Japanese airbases in the area. In nearly all of their engagements they found themselves heavily outnumbered. In one such engagement, 21 P-40s tangled with 45 Oscars. The result was 27 enemy aircraft shot down with no losses. It was one of the greatest victories enjoyed by the CATF. But eventually the China Air Task Force was defeated, not by Japanese planes, but by a lack of supplies, spare parts, and the spread of disease. At the end few of their P-40s were even capable of flying. Yet by the time it was disbanded, on the 19th of March 1943, the CATF had shot down 149 enemy aircraft with another 86 probable, for only 16 losses.\nRecords of success in combat like these were not isolated to the Pacific Theater. In Italy the 325 Fighter Group, commonly know as \"The Checker-Tailed Clan\" amassed one of the best kill to loss ratios of any fighter group in the European Theater. With a yellow and black checkerboard adorning the tail of their P-40s (and later P-47s and P-51s), they flew many sorties against more numerous German forces, and won most of the time. In 1943 the 325th won two major engagements. On July 1, 22 checker-tailed P-40s were making a fighter sweep over southern Italy when they were jumped by 40 Bf-109s. After an intense air battle, the result was half of the German aircraft shot down for the loss of a single P-40. There was a similar situation on the 30th of July, again over Italy, when 35 Bf-109s ambushed 20 P-40s. On this occasion, 21 German fighters were shot down, again for the loss of a single P-40. Because the pilots of the 325th were trained to maximize the P-40's strengths and minimize its weaknesses, it became a lethal opponent for the German fighters. The final record of \"The Checker-Tailed Clan's\" P-40s was 135 Axis planes shot down (96 were Bf-109s), for only 17 P-40s lost in combat.\nThese are just three examples of the Warhawks effectiveness in air-to-air combat. Consider the New Zealand Air Force's record: 99 confirmed kills and 14 possible, for only 20 P-40s lost in combat.\nStill, by 1944 the P-40 was being supplanted by newer fighters, and production of the final (and most numerous) version, the P-40N, ceased in December of that year. By 1945 all European Warhawk squadrons had switched to either the Mustang or Thunderbolt, and the only active P-40s were in the China-Burma-India Theater. Altogether, a staggering 13,738 P-40s were built during WW II, making it the third most numerous American fighter of the war (slightly behind the P-47 and P-51, but ahead of the P-38 and all others).\nAfter the war, the Truman Committee investigated why the P-40 was used until the end of the war, even when higher performance fighters were available. They concluded that the Army did not continue to order P-40s due to \"outside influence.\" Perhaps the obvious answer is the right one: the USAAF ordered so many P-40s because they did a heck of a good job!\nThe Second World War ended in September of 1945, but the P-40's career did not. Although the U.S. Army Air Force retired the P-40, it continued to serve with the air forces of smaller nations into the early 50s. Eventually, age and changing times got to the old war-horse and, like all of its piston-engine peers, it had to yield the skies to the jet age.\nIt is unfortunate that, even with all its victories, the P-40 appears on many people's \"worst fighters of WW II\" list. At best it is considered mediocre. Even though it served brilliantly, frequently fighting against the odds, and amassed an exceptional kill to loss ratio, it is still often thought of as a slow, unmanuverable, and obsolescent fighter. It fought in the Pacific against overwhelming enemy numbers, flying in some of the harshest conditions to be found on earth, and held the line for two years, until newer planes could be brought into service. After all that, it defended other (smaller) countries for several more years. Because of its achievements, I feel that the rugged, dependable, gravely misunderstood P-40 deserves to be considered one of the most successful fighters of World War II.\nIn these times of supersonic jets firing guided missiles at targets many miles away, the Warhawks of WW II, armed only with short range machine guns and their pilots' skill, seem almost quaint. Although collectors restore these old war birds, and a few lucky people at air shows occasionally get to see one of them fly, it will never be possible to restore the glory of the days when they were the Tigers of the sky.\nCopyright 2001 by Patrick Masell and Chuck Hawks. All rights reserved.","The Messerschmitt Me-262 was the world’s first operational jet-powered fighter aircraft. and also the world’s first mass-produced jet fighter. The first successful flight of a jet Me-262 occurred on the 18th of July, 1942.\nThe aircraft had two nicknames:\nSchwalbe (“Swallow”) for the fighter version, or Sturmvogel (“Storm Bird”) for the fighter-bomber version.\nDesign work started before World War II began, but engine problems, metallurgical problems and top-level interference kept the aircraft from operational status with the Luftwaffe until mid-1944.\nThe Me-262 was faster and more heavily-armed than any Allied fighter, including the British jet-powered Gloster Meteor.\nPilots of this aircraft claimed a total of 542 allied kills, though claims for the number are often higher than what was actually shot down.\nCaptured Me 262s were studied and flight tested by the major powers, and ultimately influenced the designs of a number of post-war aircraft such as the North American F-86 Sabre and Boeing B-47 Stratojet.\nGerman Scout Messerschmitt Me-262 A-Ia/U3 “Lady Jess IV”, captured by the Americans. In the background is visible a part of another Messerschmitt ME-262 [ Via] Underground manufacture of Me 262s [Bundesarchiv, Bild 141-2738 / CC-BY-SA 3.0] Captured by the British, Messerschmitt Me-262 at the airfield in Lubeck. In the background, on the right – a German Junkers Ju-88 [ Via] Technicians inspect a German jet fighter Messerschmitt Me-262V7, serial number 130303 at the airport in Germany after the surrender of Germany [ Via] Damaged German fighter Messerschmitt Me-262, captured by US Army in Salzburg. The engine fighter is set with the German anti-tank mine Tellermine 42. Probably this machine was prepared for demolition. Rauchen Verboten means “no smoking” [ Via] A pair of Messerschmitt Me-262A-1a, 1st Squadron 51th Bomber Squadron (1.KG51) on the sidelines of the route Munich – Salzburg [ Via] Test pilot and an engineer, Lieutenant Colonel Andrei Kochetkov conduct test flights jet aircraft Me-262 [ Via] Photo of the same Me-262 as above during the start [ Via] Me-262 is ready to fly [ Via]\nJet fighter Messerschmitt Me-262A-1a (III / EJG 2) [ Via] Me-262 A, circa 1944 [Bundesarchiv, Bild 141-2497 / CC-BY-SA 3.0] Me-262B-1a/U1 night fighter, Wrknr. 110306, with Neptun radar antenna on the nose and second seat for a radar operator [ Via] Pilots of the 44th Fighter Division (Jagdverband 44) and jet fighters Messerschmitt Me-262A-1a [ Via] Cockpit of the Me-262 [ Via] German experimental fighter Messerschmitt Me-262 A-1a / U4 (serial number 170083), captured by US troops at the factory in Augsburg. This one was equipped with Rheinmetall Mauser BK5 50mm gun 940 rounds per minute, 22 projectile ammunition) [ Via] German fighter jets Messerschmitt Me-262B-1a/U1. The first two visible aircraft have installed “Neptun” radar antenna FuG 218. Photo taken after the surrender of Germany [ Via] This airframe, Wrknr. 111711, was the first Me-262 to come into Allied hands when its test pilot defected in March 1945. It was subsequently lost in August 1946, the US test pilot parachuting to safety [ Via] US Staff Sergeant inspects a crashed German fighter Me-262A-1a bearing the number “22 White” from the 44th Fighter Group (Jagdverband 44, JV 44). The group is a special fighter unit and manned by the best fighter pilots of the Luftwaffe during the last months of World War II [ Via] A Jumo 004 engine is being investigated by Aircraft Engine Research Laboratory engineers of the National Advisory Committee for Aeronautics in 1946 [NASA – GPN-2000-000369] Destroyed by Allied bombing, jet fighters Messerschmitt Me-262 [ Via] American officers and dismantled Messerschmitt Me-262 at the airfield near Frankfurt. Note the shells of MK-108 gun next to the aircraft [ Via] American bomber B-24 “Liberator” (serial number 44-50838) of the 448th Bombardment Group, shot down by R4M missiles of a Messerschmitt Me-262. Only one member of the crew survived, he landed on the enemy territory and was captured [ Via] Photo of Luftwaffe Me-262 being shot down by USAF P-51 Mustang of the 8th Air Force, as seen from the P-51’s gun camera [ Via] Orthographically projected diagram of the Messerschmitt Me 262 [ Via]\nLength: 10.60 m (34 ft 9 in)\nWingspan: 12.60 m (41 ft 6 in)\nHeight: 3.50 m (11 ft 6 in)\nWing area: 21.7 m² (234 ft²)\nEmpty weight: 3,795 kg (8,366 lb)\nLoaded weight: 6,473 kg (14,272 lb)\nMax. takeoff weight: 7,130 kg (15,720 lb)\nPowerplant: 2 × Junkers Jumo 004 B-1 turbojets, 8.8 kN (1,980 lbf) each\nAspect ratio: 7.32\nMaximum speed: 900 km/h (559 mph)\nRange: 1,050 km (652 mi)\nService ceiling: 11,450 m (37,565 ft)\nRate of climb: 1,200 m/min (At max weight of 7,130 kg) (3,900 ft/min)\nGuns: 4 × 30 mm MK 108 cannon (A-2a: two cannon)\nRockets: 24 × 55 mm (2.2 in) R4M rockets\nBombs: 2 × 250 kg (550 lb) bombs or 2 × 500 kg (1,100 lb) bombs (A-2a variant)"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:310759ef-ddaa-437e-aab4-7c3aa716cc25>","<urn:uuid:52db243a-f182-4dc8-bddc-e33e307c07f3>"],"error":null}
{"question":"What are the key differences between Certified Financial Planners (CFP) and Enrolled Agents (EA) in terms of their educational requirements and professional focus?","answer":"CFP practitioners and EAs have distinct educational requirements and professional focuses. CFPs must complete a rigorous curriculum of study covering nearly 100 integrated financial planning topics, including investment management, income tax planning, retirement planning, employee benefits, estate planning, risk management, and insurance planning. They must also have 3 years of experience and pass a comprehensive 2-day exam. In contrast, EAs focus specifically on taxation, completing extensive study exclusively in tax matters and passing a three-part Special Enrollment Examination (SEE) covering individual taxation, business taxation, and representation practices. EAs generally have completed undergraduate studies in business management or accounting, with many holding master's degrees in taxation. Both professionals must maintain continuing education requirements - CFPs need 30 hours every 2 years, while EAs must complete ongoing education in federal tax updates, federal tax law, and professional ethics.","context":["FAQs 1. What is a Certified Financial Planner™ Practitioner?A CERTIFIED FINANCIAL PLANNER™ practitioner is recognized as the standard of excellence in personal financial planning. Becoming a CFP® certificant includes completing a rigorous curriculum of study, minimum of 3 years experience in the field, and pass a comprehensive 2 day exam. To maintain competency, a certificant is required to complete 30 hours of continuing education every 2 years. A Certified Financial Planner ™ Practitioner is bound by the CFP CODE OF ETHICS PRINCIPLES. The CFP Board requires it, personal integrity ensures it. Learn More2. What services do you offer?In a few words, comprehensive financial planning and objective investment advice. CFP® professionals must master nearly 100 integrated financial planning topics, ranging from:Investment ManagementIncome tax planningRetirement planningEmployee benefitsEstate planningRisk managementInsurance Planning3. Why should I hire an independent financial planner?As independent financial planner, I offer an objective approach. This enables me to put your interests ahead of mine without the influence of product or sale quotas. There are a broad spectrum of products and services available to investors today. You want someone you can trust who is able to help you select the appropriate strategies for you and your family. To avoid feeling overwhelmed with decisions regarding your planning or investments, you may benefit from having a personal financial coach to help you; freeing you up to do what you do best.4. Who is LPL Financial? LPL Financial is the largest Independent broker/dealer in the couintry as reported in Financial Planning Magazine June 1996--2018 based on total revenues. Currently, there are over 16,000+ advisors affiliated with LPL Financial. LPL Financial serves as custodian of client accounts and sends monthly/quarterly reporting statements to clients. LPL Financial is a Registered Investment Advisor, Member of FINRA and SIPC.5. What is the pricing of your service?All upfront consultations are free of charge and include an initial complementary financial plan. Additionally our objective is to help you manage your assets with the least fee possible. Depending on the strategies we utilize, we may not charge a fee at all to a quarterly advisory fee up to .30%. A full disclosure of any fees is provided prior to implementing any strategy.6. What is your investment philosophy?Our philosophy is to manage your assets in a way that works best for you. Key elements to this philosophy are to develop a diversified portfolio that matches your risk profile, investment objectives and goals. Monitoring your portfolio and keeping you informed will assure we are on the appropriate track together. However, there is no guarantee that a diversified portfolio will enhance overall returns or outperform a non-diversified portfolio. Diversification does not protect against market risk.7. What types of clients do you work with?We work with Individuals, families, and businesses that want to analyze and understand their financial situation, and develop a plan for the future. Also, clients tell us they hire a professional because they may not have the time, desire, or expertise to manage their investments.8. How much contact do you have with your clients?After an initial consultation, there may be several meetings in the first few months. During this time we want to get to know you and understand your situation. Throughout the year, we will meet for a mid year check up. At a minimum, you can expect an annual face to face meeting to review your entire plan, and make any needed adjustments. Although, life doesn’t always happen on a schedule. Never hesitate to call or email with any questions or concerns there may be for you or a member of your family.9. What licenses, credentials or other certifications do you have?CERTIFIED FINANCIAL PLANNER™My Securities registrations held with LPL Financial are: Series 7, 24, 63, 65.Insurance Licenses: Life and HealthFor a complete biography click on About Us.10. Are you accepting new clients?Yes. Most of our new clients are referred to us by our existing clients or by a trusted professional advisor, such as a CPA or attorney. To get started with your free consultation, please complete the information at Contact Us, or simply call me at 888-428-4483 to set an appointment.11. Who do I contact when I have a question?You receive personalized service. Call me directly at 888-428-4483 or email me at William.firstname.lastname@example.org. I will respond with in 24 hours, typically within a few hours. However, if you have an immediate need, LPL Financial customer service representatives can be reached at -1-800-558-7567, from 9:30 am to 7:30 pm Eastern Time.12. What do I need to bring for the first meeting?There is no preparation necessary for the first meeting. Most important is to get to know you and understand how we may be able to help. This will also give you an opportunity to get to know us better as well. However, if you know you have a specific question about something, please bring any statements or documents you would like us to review. Otherwise, for future meetings we will guide you on important items to bring with you.13. What’s in a financial plan?A holistic approach to the financial planning process integrates all areas of your financial life, including investment, tax, insurance, retirement, and estate planning. After a though analysis, strategies are designed to address any areas that need improvement. A summary of the analysis and recommendations are provided for your personal records.14. How do I keep track of my investments?Every client is provided a “My Financial Butler” private financial planning website. To see an example of the website you will have access to, Please click the following link or copy and paste into your web browser https://wealth.emaplan.com/ema/SignIn?ema%2flpl%2fwilkinswealth.THE USER NAME is: email@example.com and the Password is: Password1 (The password is case sensitive so please make the P a capital) If the system ask you any security questions the answer is always green Your private financial planning website can consolidate all of your financial information (even on accounts we don’t manage, such as your 401k, Fidelity, Vanguard, T.Rowe Price, Schwaab, Etrade, Scott Trade etc…) into one snapshot so you can see updated daily asset allocation, how much risk do you have across all your portfolios.You will be able to add your income, expenses, pensions etc… and generate a year by year cash flow projections to see how you are pursing your goals. This Cash Flow projection is in real time and will update with your asset values so you can see how you are pursing your goals at any point in time.You will be able to link your credit card and checking accounts to pull in all your spending transactions which will allow you to see how your money is being spent across all categories. It allows you to track refunds and unwarranted charges. You can also create a budget within the website that will match up to your spending transactions.You have access to an encrypted online vault where you can store digital copies of documents such as wills, trusts, insurance policies, photos, deeds, etc. for easy reference whenever you need to read through them. You can also upload photos & videos of personal property to have proof for insurance companies in case of loss.There is also an organizer tab that allows you to update your income, itemized expenses, assets, liabilities, insurance policies and more that allow you to play the What If Game? Any changes you make will automatically update your cash flow planning projections so you can see how your are pursing goals.There is also a financial planning checklist within the Organizer Tab to reference suggestions on planning and money saving ideas to help get your financial life in order.See Video For What Your Financial Butler Personal Financial Planning Website Can do.","UNDERSTANDING TAX PROFESSIONALS CREDENTIALS\nWhat are the differences between EAs and CPAs?\nEnrolled Agents are specialized tax professionals federally licensed by the U.S. Treasury Department.\nEnrolled Agent status is the highest credential the IRS awards.\nEAs are governed by the Internal Revenue Service Office of Professional Responsibility with strict adherence to the rules and regulations of the Treasury Department Circular 230, Title 31 Code of Federal Regulations, Subtitle A, Part 10 that are the Regulations Governing Practice Before the Internal Revenue Service.\nEAs generally have completed undergraduate studies in business management, accounting or other specialized fields. Many EAs have graduate-level master’s degrees in taxation.\nTo meet the enhanced education criteria necessary, EAs complete extensive study exclusively in taxation and pass a three-part Special Enrollment Examination (SEE).\nAll three parts of the Special Enrollment Examination includes questions in the areas of taxation and ethics:\n- Individuals Taxation\n- Business Taxation\n- Representation Practices and Procedures\nThe education necessary to pass the three-part examination questions includes:\n- Individuals Taxable Income and Tax-Exempt Income\n- Preliminary Work and Taxpayer Data\n- Individuals Assets\n- Deductions and Credits\n- Taxation and Advice\n- Estate and Trust Taxation\n- Gift Taxation\n- Foreign Income Reporting Requirements\n- Business Entities Formation (Sole Proprietorships, Partnerships, Corporations and S Corporations)\n- Business Financial Information\n- Business Income\n- Business Expenses\n- Business Deductions and Credits\n- Business Compensation For Officers and Employees\n- Worker Classification\n- Business Payroll Tax Liabilities\n- Business Inventory\n- Business Assets Depreciation and Dispositions,\n- Analysis of Financial Records,\n- Advisement For The Business Taxpayer\n- Exempt Organizations Qualifications\n- EO Maintaining Tax Exempt Status\n- EO Income and Expense Reporting Requirements\n- EO Unrelated Business Taxable Income\n- Retirement Plans Contributions\n- Retirement Plans Distributions\n- Retirement Plans Prohibited Transactions\n- Qualified and Non-qualified Plans\n- Representation Practices and Procedures\n- Due Diligence Requirements\n- Conflicts of Interest\n- Standards For Written Advice\n- Covered Opinions and Tax Return Positions\n- Sanctionable Acts\n- Information Shared With Taxpayer\n- Records Maintenance\n- Rules and Penalties\n- Identification of Tax Issues With Supporting Details\n- Supporting Documentation\n- Taxpayer Financial Situations\n- Potential Criminal Aspects\n- Competence and Expertise\n- Power of Attorney and Declaration of Representative\n- Representing Taxpayer In Audit/Examination\n- Representing Taxpayer Before Appeals\n- Representing Taxpayer In The Collection Process\n- Collection Alternatives and Settlement\n- Data Security and Protection of Taxpayer Information\nEnrolled Agents must also meet good moral and ethical character standards for licensing and renewal to continue their privilege of Unlimited Rights to Represent Taxpayers Before the Internal Revenue Service and maintain Continuing Professional Education specifically in areas of:\n- Federal Tax Updates\n- Federal Tax Law\n- Professional Ethics\nAn Enrolled Agent credential may also be earned as a former employee of the Internal Revenue Service.\nOnly Enrolled Agents, Certified Public Accountants and Attorneys have Unlimited Rights To Practice.\nThis means only these professionals are unrestricted as to which taxpayers they can represent, what types of tax matters they can advise or represent, and which IRS offices they can represent taxpayer clients before.\nClick on the link below to review the Internal Revenue Service information\nClick on the link below to review the U.S. Treasury Department Circular 230\nClick on the link below to review the EA Special Enrollment Exam SEE Part 1 Individuals Tax Topics\nClick on the link below to review the EA Special Enrollment Exam SEE Part 2 Businesses Tax Topics\nClick on the link below to review the EA Special Enrollment Exam SEE Part 3 Representation Practices and Procedures\nAlthough the letters CPA are commonly known by people, most members of the general public do not understand that these letters do not specifically mean the person has specialized current knowledge in taxation, any specialized current expertise in tax laws, tax planning, income tax returns preparation or tax case representation.\nCPAs have generally completed undergraduate study in accounting at a college or university which may include some general coursework in taxation. Some CPAs have graduate-level master’s degrees.\nCPAs must pass the four-part Uniform CPA Examination to be licensed by state boards of accountancy.\nOne part of the Uniform CPA Examination (Regulation -REG) that is ¼ of the examination includes questions in the areas of:\n- Professional Responsibilities\n- Federal Tax Procedures\n- Business Law\n- Federal Taxation of Property Transactions\n- Federal Taxation of Individuals\n- Federal Taxation of Entities\nTo meet State boards of accountancy license and renewal requirements, CPAs also are required to meet experience, good character standards, peer review and maintain specified levels of Continuing Professional Education.\nEach state’s requirements for Continuing Professional Education vary.\nThe State of Texas does not have any specified requirement for Continuing Professional Education in taxation.\nCPE in the areas of taxation is at the option of the CPA.\nAs listed in the State of Texas Administrative Code, Continuing Professional Education for a CPA may include technical courses in:\n- Management advisory service\n- Computer information technology\n- Regulatory ethics\n- or other areas of benefit to the licensee or their employer\nAs listed in the State of Texas Administrative Code, Continuing Professional Education for a CPA may include non-technical courses in:\n- Computer software\n- Behavioral ethics or science\n- Business management and organization\n- Advanced foreign languages\nTo determine if the CPA you are working with has current Continuing Professional Education and knowledge in the specialized area of taxation, it is in your best interest to specifically ask to see their Certificates of Continuing Professional Education courses in taxation including Annual Federal Tax Updates, Federal Taxation, Business Tax Matters, Individuals Tax Matters, Estates and Trusts Tax Matters.\nWe would be happy to show you ours.\nClick on the link below to review the continuing professional education requirements specified in the Texas Administrative Code\nClick on the link below to review the Internal Revenue Service information\nClick on the link below to review the Uniform CPA Exam parts topics\nWhat are the differences between a State Bar Licensed Attorney and a United States Tax Court Practitioner counsel?\nAttorneys have extensive education in various practice areas of law and have passed a rigorous bar examination administered by their state for admission to the State Bar.\nAttorneys may also be admitted to practice by other state or federal courts.\nThey have a general right to practice in their choice of law or specialty. They are unrestricted and can practice in a single area or multiple areas such as family law, real estate law, business law, civil law, criminal law, tax law, tax planning, tax preparation or a variety of other specialty areas.\nAttorneys who choose to represent taxpayers before the United States Tax Court are only required to pay a separate nominal fee to the court for admission to the U.S. Tax Court Bar if they hold an active State Bar license that is active and they are deemed in good standing.\nFor license renewal, attorneys similarly are required to participate in extensive Continuing Legal Education for credits that vary per state.\nUnited States Tax Court Practitioner counsels are admitted to the U.S. Tax Court Bar by means of passing a rigorous examination administered by United States Tax Court in Washington, DC bi-annually in even-numbered years.\nThe examination candidates eligible to sit for the examination are Enrolled Agents, Certified Public Accountants and other professional types.\nThe examination is a four-part, four hour written essay type examination covering topics of:\n- Tax Court Rules of Practice and Procedure\n- Substantive Tax Law\n- Federal Rules of Evidence\n- American Bar Association Model Rules of Professional Conduct\nEach examination part must be passed with a score of 70% or higher.\nThe examination is extremely challenging for even the most knowledgeable tax experts. The passing rate is extremely low.\n2000 16.67% 17 Passed of 102 Exam Candidates\n2002 14.89% 7 Passed of 47 Exam Candidates\n2004 5.56% 4 Passed of 72 Exam Candidates\n2006 10.34% 6 Passed of 58 Exam Candidates\n2008 14.81% 8 Passed of 54 Exam Candidates\n2010 9.64% 8 Passed of 83 Exam Candidates\n2012 14.29% 11 Passed of 77 Exam Candidates\n2014 18.25% 23 Passed of 126 Exam Candidates\n2016 13.45% 16 Passed of 119 Exam Candidates\nUpon passing an examination candidate must also meet strict moral, ethical and professional character expectations of the court and be sponsored by at least two persons admitted to practice as members of the U.S. Tax Court Bar.\nUnited States Tax Court Practitioner (USTCP) counsels must maintain extensive Continuing Professional Education in the areas of taxation and Continuing Legal Education in the areas of law.\nA United States Tax Court Practitioner (USTCP) counsel is limited to the practice and advisement of tax law and cannot practice or provide advisement in any other area of law.\nClick on the link below to review the criteria for the U.S. Tax Court Non-Attorney Examination\nClick on the link below to review the U.S. Tax Court Non-Attorney Examination Statistical Information Numbers Who Passed\nMy CPA (or EA) told me they can take my case to Tax Court, can they?\nThis is a significant problem.\nThe immediate answer is NO! Unless that CPA or EA has also been admitted to the United States Tax Court Bar.\nUnderstand that only State licensed attorneys who are also admitted to the U.S. Tax Court Bar or United States Tax Court Practitioner’s who are also admitted the U.S. Tax Court Bar can legally petition your case to Tax Court and directly represent you!\nThese professionals must sign the petition and list their U.S. Tax Court Bar Number.\nThis fact is of substantial importance for taxpayers!\nA tax professional with the designations as Unenrolled Preparer, a preparer with the former registration designation (RTRP), a preparer with current tax year approval (AFSP), an Enrolled Agent (EA) or a Certified Public Accountant (CPA) whom is NOT admitted to the U.S. Tax Court Bar CANNOT directly represent your tax controversy case before the judges of United States Tax Court.\nKnow that these designated professionals do not possess the qualifications or knowledge necessary of the procedural rules of the court, rules of evidence or rules of ethics to adequately represent a taxpayer before the court.\nKnow that any advisement received from any of these persons is the unauthorized practice of tax law.\nThey do not possess the recognized knowledge or qualifications to render proper advisement.\nThey do not have the authority under the law to sign a Tax Court Petition because they do not hold an active bar license in good standing with the court.\nAny Tax Court Petition prepared and filed to the U.S. Tax Court by these designated professionals will be docketed by the court as a Pro Se case. This is the Latin phrase meaning “for oneself” or “on one’s own behalf”.\nThis means that YOU are self-representing before the court.\nSadly, we hear of these circumstances quite often which is a direct compromise of the plaintiff taxpayer’s case when unlicensed persons attempt to advise innocent unknowing taxpayers. This is the Unauthorized Practice of law.\nWhen the unknowing taxpayer’s case actually is heard before the judges of the court, the CPA (or EA) cannot litigate the case, cannot confer with IRS counsel and cannot speak to the judge.\nKnow that generally the Commissioner of the IRS will have at least 2, and sometimes more, attorneys on the case.\nYou need the knowledge and experience of a properly licensed professional to advocate on your behalf.\nWhy do they say they can do something that they can’t really do?\nBecause you, the innocent taxpayer, doesn’t know any different. You have trusted that they have the knowledge and did not verify otherwise.\nThe second reason is that they are “banking” on the fact that the IRS Office of Appeals retains jurisdiction on Small Tax Cases up to the time of trial and retains jurisdiction on Regular Tax Cases for the first six months after the petitioned case is docketed for trial. During this period, the court may remand a case back to IRS Appeals that either was not previously subject to an Office of Appeals review or the court may remand a case back to IRS Appeals if the case administrative record is insufficient or incomplete.\nIn these circumstances, the CPA (or EA) is hoping that they can continue to advocate the case because under their active license they do have Unlimited Rights To Represent taxpayers before the IRS Office of Appeals.\nProblem is not all cases, or all matters of the case, can be resolved within the IRS Office of Appeals and subsequently will necessitate a trial.\nThe other substantial problem is that the CPA (or EA) has no qualified knowledge of the various procedural rules of the court, rules of evidence or trial mechanics, therefore they fail to properly plead all matters as required, fail to file proper motions and fail to complete many other case management requirements.\nIf you need a surgery, anyone can tell you they’ll save you money because they can purchase scalpels at any medical supply store, buy bandages, gauze and surgical tape. This does not make them a surgeon!\nDon’t be confused by the terms “Litigation Support”!\nAlthough the state’s boards of accountancy permit CPAs to advertise and promote “Litigation Support” this does not mean that they can practice law, tax law or represent any taxpayer before the U.S. Tax Court.\nLitigation support is the act of serving as an Expert Witness at trial on matters related to accountancy or the capability to provide assistive information relating to accountancy to a licensed attorney or USTCP.\nThe Taxpayer Advocate Service provides an Annual Report to Congress. This special report includes substantial matters relating to the IRS. In most years the TAS report shows that the IRS prevailed in over 80% of Pro-se cases tried before U.S. Tax Court.\nClick on the link below to read the TAS most recent report to Congress\nIf you are still not convinced that it’s a bad idea to have a CPA petition your case to U.S. Tax Court, we encourage you to click on the link below to read the “blunders” of a case in which the taxpayer was poorly advised from the day of the income tax return preparation to the day of trial and even during trial.\nTax Court Summary Opinion 2017-93\nMARY A. COLLIVER, Petitioner v. COMMISSIONER OF INTERNAL REVENUE, Respondent\nDocket No. 15307-16S. Filed December 26, 2017.\nWith all this being said, is it worth risking tens of thousands of dollars to the IRS to save a few hundred dollars on your tax returns preparation, tax representation case before the Internal Revenue Service or tax litigation before the United States Tax"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:30b44fb6-6f6a-4d00-9157-0c0d242f2f3c>","<urn:uuid:50d0c07f-db24-4f81-a3fe-36bdea2f6c1c>"],"error":null}
{"question":"How do C's assignment operators work, and what security features protect against their misuse?","answer":"C's assignment operators include the basic = operator and compound operators like +=, -=, *=, /=, %=, <<=, >>=, &=, ^=, and |=, which combine arithmetic or bitwise operations with assignment. To protect against potential misuse of these operators, modern software implements multiple security features. These include Control Flow Integrity (CFI) to prevent attackers from redirecting code execution, Stack Guards to detect stack manipulation, and Fortification which replaces risky functions with safer alternatives when possible. These safety measures help prevent attackers from exploiting vulnerabilities that could arise from improper assignment operations.","context":["As studied earlier, Operators are the symbols, which performs operations on various data items known as operands. For Example: in a\nb are operands and\n+ is an operator.\nNote:- To perform an operation, operators and operands are combined together forming an expression. For example, to perform an addition operation on operands a and b, the addition(+) operator is combined with the operands a and b forming an expression.\nDepending on the function performed, the C operators can be classified into various categories.\nArithmetic operators are used to perform arithmetic calculations.\nThey can work on any built-in data type of C except on boolean type.\nC provides various arithmetic operators that are,\n|+||Adds two operands.||A + B = 30|\n|−||Subtracts second operand from the first.||A − B = -10|\n|*||Multiplies both operands.||A * B = 200|\n|/||Divides numerator by de-numerator.||B / A = 2|\n|%||Modulus Operator and remainder of after an integer division.||B % A = 0|\n|++||Increment operator increases the integer value by one.||A++ = 11|\n|--||Decrement operator decreases the integer value by one.||A-- = 9|\nAddition is: 10\nRelational operators ar eused to find out the relationship between two operands.\nThe various relational operators provided by C are less then '<', less than or equal to '<=', greater than '>', greater than or equal to '>=', equal to '==' and not equal to '!=' operators.\n|==||Checks if the values of two operands are equal or not. If yes, then the condition becomes true.||(A == B) is not true.|\n|!=||Checks if the values of two operands are equal or not. If the values are not equal, then the condition becomes true.||(A != B) is true.|\n|>||Checks if the value of left operand is greater than the value of right operand. If yes, then the condition becomes true.||(A > B) is not true.|\n|<||Checks if the value of left operand is less than the value of right operand. If yes, then the condition becomes true.||(A < B) is true.|\n|>=||Checks if the value of left operand is greater than or equal to the value of right operand. If yes, then the condition becomes true.||(A >= B) is not true.|\n|<=||Checks if the value of left operand is less than or equal to the value of right operand. If yes, then the condition becomes true.||(A <= B) is true.|\na is less than b c and d both are equal a and b are not equal\nLogical operator is used to find out the relationship between two or more relationship expressions.\nThe C language logical operators allow a programmer to combine simple relational expression to form complex expressions by using logical not, and and or.\n|&&||Logical AND operator. If both the operands are non-zero, then the condition becomes true.||(A && B) is false.|\n|||||Logical OROperator. If any of the two operands is non-zero, then the condition becomes true.||(A || B) is true.|\n|!||Logical NOT Operator. It is used to reverse the logical state of its operand. If a condition is true, then Logical NOT operator will make it false.||!(A && B) is true.|\nTaking the logical And, when two expressions are added the resulting expression will be true only if both of the sub-expressions are true. Eg, (X>5) && (Y<15) will be only true if x greater than 5 and if y is less than 15. The second expression (Y,15) will not be evaluated if the first expression is false.\nA logical expression is evaluated from left to right as far as needed to determine the logical result. Eg. in the expression (Y<15)&&(X++>15), the value of X is incremented only if the value of Y is less than 15.\nThe logical or is represented by two vertical bars between two other expression as expression1 || expression2. The logical result of the entire expression will be true if either of the two expression surrounding the || is logically true. Expression involving the logical or are evaluated from left to right only when the result of the logical expression is known.\nLine 1 - Condition is true Line 2 - Condition is true Line 3 - Condition is not true Line 4 - Condition is true\nBitwise operators are used to perform bit-by-bit operations. This operator can be applied to all the primitive data types such as long, int, short, char and byte etc.\nVarious bitwise operators are bitwise AND(&), bitwise OR(!), bitwise exclusive OR(^) one's complement(~), shift left (<<), shift right(>>), shift right with zero fill (>>>).\nThe table given below lists the various bitwise operators.\n|&||Bitwise AND Operator copies a bit to the result if it exists in both operands.||(A & B) = 12, i.e., 0000 1100|\n||||Bitwise OR Operator copies a bit if it exists in either operand.||(A | B) = 61, i.e., 0011 1101|\n|^||Bitwise XOR Operator copies the bit if it is set in one operand but not both.||(A ^ B) = 49, i.e., 0011 0001|\n|~||Bitwise One's Complement Operator is unary and has the effect of 'flipping' bits.||(~A ) = ~(60), i.e,. -0111101|\n|<<||Bitwise Left Shift Operator. The left operands value is moved left by the number of bits specified by the right operand.||A << 2 = 240 i.e., 1111 0000|\n|>>||Bitwise Right Shift Operator. The left operands value is moved right by the number of bits specified by the right operand.||A >> 2 = 15 i.e., 0000 1111|\nLine 1 - Value of c is 12 Line 2 - Value of c is 61 Line 3 - Value of c is 49 Line 4 - Value of c is -61 Line 5 - Value of c is 240 Line 6 - Value of c is 15\nAssignment operators are used to assign the value of an expression to an identifier. The general form is,\nv op =exp;\nThe most commonly used assignment operator is the equal sign (=). Assignment expressions are written in the form identifier = expression.\nAn assignment operator assumes there is a variable name to its left and an expression to its right.\nThe expression may be arithmetic, relational, logical or another assignment. It takes the value of the expression and stores it in the variable.\nA=3 X=Y SUM =A+B\n|=||assigns values from right side operands to left side operand||a=b|\n|+=||adds right operand to the left operand and assign the result to left||a+=b is same as a=a+b|\n|-=||subtracts right operand from the left operand and assign the result to left operand||a-=b is same as a=a-b|\n|*=||mutiply left operand with the right operand and assign the result to left operand||a*=b is same as a=a*b|\n|/=||divides left operand with the right operand and assign the result to left operand||a/=b is same as a=a/b|\n|%=||calculate modulus using two operands and assign the result to left operand||a%=b is same as a=a%b|\n|<<=||Left shift AND assignment operator.||C <<= 2 is same as C = C << 2|\n|>>=||Right shift AND assignment operator.||C >>= 2 is same as C = C >> 2|\n|&=||Bitwise AND assignment operator.||C &= 2 is same as C = C & 2|\n|^=||Bitwise exclusive OR and assignment operator.||C ^= 2 is same as C = C ^ 2|\n||=||Bitwise inclusive OR and assignment operator.||C |= 2 is same as C = C | 2|\nThe conditional operators are also called ternary operators. A conditional operators forms an expression that will have one of two values, depending on the truth value of another expression.\nThe expression can be written in place of an if-else statement. A conditional expression is written in the form expression1 ? expression2:expression3 When evaluating a conditional expression, expression1 is evaluated first.\nIf expression1 is tru(i.e., if its value is nonzero), then expression2 is evaluated and this becomes the value of the conditional expression.\nEnter a signed numbe Absolute value = 22 Enter a signed number:23 Absolute value = 23\nC has two very usefull operators.\nThey are increment(++) and decrement(--) operators. The increment operator(++) adds 1 to the operator value contained in the variable.\nThe decrement operator(--) subtracts 1 from the value contained in the variable.\nIncrement operator can be used in two way pre-increment(++a) and post-increment(a++).\nSimilarly decrement operator can also be used in two way pre-increment(--a) and post-increment(a--).\n|A++||Add 1 to a variable agrer use.||int A=10,B;\n|+AA||Add 1 to a variable before use.||int A=10,B;\n|A--||Subtract 1 from a variable after use.||int A=10,B;\n|--A||Subtract 1 from a variable before use.||int A=10,B;\n10 10 11 10 11 11\nIn C programming the special operators are mostly used for memory related functions.\n|&||This is used to get the address of the variable.\nExample: &a will give address of a.\n|*||This is used as pointer to a variable.\nExample: *a where, * is pointer to the variable a.\n|Sizeof ()||This gives the size of the variable.\nExample: size of (char) will give us 1.\nIn this program, “&” symbol is used to get the address of the variable and “*” symbol is used to get the value of the variable that the pointer is pointing to. Please refer C – pointer topic to know more about pointers.\nvalue is: 20","The Main Question\n“How difficult is it for an attacker to find a new exploit for this software?”\nAttackers have limited resources, and just like anyone else, they don't like to waste their time doing something the hard way if an easier path is available. They have tricks and heuristics they use to assess the relative difficulty to exploit software, so that they can focus on the easy targets and low hanging fruit. Mudge has had a long career of doing just that (legally, and for research purposes), so he's developed his own personal toolkit of measurements to take when assessing software risk. By consulting with other luminaries of the security field who have extensive exploit development experience, we've been able to build up the list of static analysis metrics and features which we currently assess.\nOur ratings are based on a combination of measured data and expert analysis. The primary factors under consideration are safety features and function hygiene, with an emphasis on exploit mitigation and risk reduction.\nOur current score weightings heavily emphasize safety features and function hygiene. The safety features in particular are the easiest issues to fix, and are the most fundamental. When the software industry has gotten to a point where these are included as a matter of course, we will adjust our score weightings to place a greater emphasis on more subtle elements like complexity. The way we think of it is, if a house’s front door is unlocked, we’re not going to worry too much about getting shatter sensors installed in the windows. Vendors need to get the basics in place first, and then we’ll move our focus to the next thing which can move the overall quality of software forward. If you happen to have those basics in place in your own products, however, and want to know what we’ll be focusing on in the future, look to the complexity and dynamic testing measurements.\nFor Version 1 of our scoring system, we are focusing on Safety Features and Code Hygiene as dominant factors in our risk analysis rating. In the future we hope to factor in both Code Complexity scores and Crash Testing results.\nThere are three main categories of static analysis features that we look at for a software binary:\nScoring: A binary score is penalized by -20 for each missing feature below.\n|Feature||OS||Why It's Important|\n|Address Space Layout Randomization (ASLR)||All||Attackers will try to piece existing computer code together to make the system do what they want, like using words out of different magazines to write a ransom note. If each application's code is loaded at a predictable location, this is pretty easy to do, so modern operating systems randomize the locations of code segments. To follow our metaphor, if the attacker doesn't know which words are where, they can't write the message they want to write in their note.|\n|Data Execution Prevention (DEP)||All||DEP tries to make sure attacker introduced data is not treated as executable instructions by the computer. Some operating systems have separate application armoring settings for Stack DEP and Heap DEP, while others pretext both with a single setting. The stack and the heap are the two locations where data is stored by the application.|\n|Control Flow Integrity (CFI)||Windows||Control Flow Integrity (CFI) refers to any safety features that try to prevent attackers from redirecting the path of code execution. Many attacks involve hijacking software execution, so that the attacker can direct the computer to a different code path than originally intended. Sometimes this is to get their own code executed, and other times they're trying to cobble together a new effect from snippets of existing code, sort of like someone writing a ransom note by cutting out words from a magazine.|\n|Stack Guards||All||Buffer overflows, one of the most commonly exploited vulnerability types today, involve an attacker overwriting values on the stack so that they can change what code is being executed. Stack guards or canaries try to detect manipulations of the stack so that they can detect these styles of attacks in progress and prevent their success.|\n|Fortification||MacOS, Linux||When source fortification is enabled, the compiler replaces some risky functions with safer ones. The compiler can only do this if it can determine what all the parameters would be, so not all instances of a function get fortified, even when this feature is enabled. You can think of this like spell check and autocorrect - if it isn't clear what you meant to type, things get left as you originally typed them.|\n|Relocate Read-Only (RELRO)||Linux||RELRO stands for Relocate Read-Only, and is a mitigation of a Linux-specific vulnerability. Certain parts of the binary that are particularly sensitive need to be made read-only after they are built, but before the program starts running, so that an attacker can't use them to hijack operations.|\n|Safe Structured Exception Handling (SEH)||Windows||SEH stands for Structured Exception Handler. When an error occurs during the software's execution, the SEH gives the software its instructions on how to handle that error. Safe SEH makes sure that an attacker can't introduce changes to the SEH pointers and code.|\n|64-bit||All||Many safety features are more effective for 64 bit binaries, or are only available in their strongest form for 64 bit binaries, so we treat this as a separate safety feature.|\n|Function Ratings||Scoring||What Makes a Fuction this Type||Sample of Functions Checked For|\n|Good Functions||+10if good functions are present in the binary||These are much safer replacements for historically bad and risky functions. For example, “strlcpy” is the good version of strncpy and strcpy, which are in the risky and bad categories, respectively. Use of these functions is relatively rare, but indicates that the developers prioritize secure coding.||\n|Risky Functions||-5if risky functions are present in the binary||These are slightly safer functions than the bad functions, sometimes ones which were originally introduced specifically to fix flaws in those bad functions, but which are still tricky to use without introducing vulnerable bugs into code. More security-savvy programmers will generally use the “good” versions of these functions instead.||\n|Very Risky Functions||-15if very risky functions are present in the binary||These are functions which are difficult to use without introducing vulnerabilities, and/or which have been known to introduce buffer overflows or other vulnerable bugs into software. They can be used safely, but often aren’t, and there are safer alternatives available, so their use can be indicative of a lack of security awareness.||\n|Extremely Risky Functions||-25if extremely risky functions are present in the binary||These are the rare functions that have no place in commercial software, due to their extreme insecurity. Their use is fairly rare, but a big red flag.||\nThe most commonly accepted method of testing software today is crash testing, or fuzzing. Software is given malformed inputs, to see if and how it crashes. You can learn a lot about how something is built from how it breaks, and fuzzing provides a lot of insight into how robust or potentially exploitable software is.\nMost of the software we test is obtained the same way any consumer would get it – by buying it. This ensures that the software we test is the same as the software that end users have access to. We also get some software through publicly available sources, in the case of open source products, or where IoT firmware is available for download. Some IoT software we extract directly off of the IoT products, and in other cases we obtain software through partnerships with other non-profit and research entities. We never get software to test through private relationships with the vendors whose products we’re evaluating.\nOur security evaluations are performed using a combination of proprietary and publicly-available software analysis tools, employing both static and dynamic methods of analysis. These tools are used to examine the features described elsewhere on this page: code hygiene, build safety features, and crash testing.\nThe publicly-available tools in our toolchest are all well-known in the software analysis community. However, in the interest of encouraging vendors to go above-and-beyond the bare minimum, we do not publicly disclose what these tools are. Instead, we advise vendors to perform their own thorough evaluations of their products prior to release, using a wide variety of different analysis tools.\nThough we use publicly-available tools whenever possible, certain aspects of our mission have motivated us to develop some proprietary tools as well. These tools are designed to serve our organization's peculiar needs, most notably, the ability to perform cross-platform software analysis, and the ability to collect detailed debug and tracing data for use in our scientific efforts. Analytically, these tools perform functions which can largely be found in publicly-available alternatives. They also perform functions which are relevant to our scientific efforts, but which have not yet been incorporated into our general reporting."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:23802712-a779-44cb-acd4-e79b56db5787>","<urn:uuid:5222dd6f-9fda-4871-9aaa-e366e5f1ebc6>"],"error":null}