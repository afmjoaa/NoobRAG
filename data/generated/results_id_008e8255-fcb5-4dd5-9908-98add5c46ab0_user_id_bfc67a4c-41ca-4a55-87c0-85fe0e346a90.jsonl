{"question":"Could you compare how astronauts and blues harmonica players adapt their musical techniques to overcome physical challenges?","answer":"Astronauts must make specific adaptations due to zero gravity, like using floor loops or bungee cords to stay in place while playing instruments - as Ellen Ochoa did with her flute and other astronauts did with electronic keyboards. In contrast, blues harmonica players adapt their playing techniques using their diaphragm and epiglottis for effects like octaving, and must carefully control their breathing and mouth positioning to achieve specific sounds like bends and trills.","context":["Stories that Surprise and Inspire\nWhen musicians play a lively tune, they often find themselves spontaneously tapping their toes and moving about to the pulsing beat. But when Ellen Ochoa played her flute at work one day in 1993, she couldn’t be spontaneous at all. If she hadn’t made careful plans, she could have been blown about the room, just by playing one long note on her flute. That’s because she was an astronaut working on the U.S. Space Shuttle as it circled Earth more than a hundred miles out in space.\nGravity is so weak far out in space that astronauts—and any of their gear that isn’t fastened down—will float about inside a space craft. Blowing air into her flute could have created enough force to actually send Ochoa zipping about the space shuttle cabin. So, to keep herself in place as she played, she had to slip her feet into strong loops attached to the floor.\nDr. Ochoa, now the director of NASA’s Johnson Space Center, was the first U.S. astronaut to bring a flute on a space mission, but she wasn’t the first to make music in space. Nearly thirty years earlier, in December 1965, two astronauts onboard the Gemini 6 space craft played a musical joke on mission control officials down on Earth. Those astronauts—Walter M. Schirra, Jr., and Thomas P. Stafford—told mission control that they saw an unusual object near their spaceship, a satellite perhaps, moving from North to South. They said they would try to pick up some sound from this mysterious object. Then they used the harmonica and bells they had secretly brought with them on that December mission to surprise folks listening down below by playing “Jingle Bells.”\nIn recent years, other astronauts have brought musical instruments on space missions to help lift their spirits, especially those who spend many months on the International Space Station. Like Dr. Ochoa, these astronaut musicians have to make adjustments, such as using a bungee cord to attach an electronic piano keyboard to a pianist’s leg.\nSome astronauts have composed music in space, including Canadian Chris Hadfield. On May 6, 2013, he sang the song he wrote—called “I.S.S. (Is Somebody Singing)”—in a live TV broadcast from the space station as thousands of Canadian schoolchildren sang along with him down on Earth. Click here for a recording of that space-to-Earth performance\nLearning to play an instrument can be fun and, at times, frustrating. Amy Nathan's lively book helps young people cope with the difficulties involved in learning a new instrument and remaining dedicated to playing and practicing. Teens from renowned music programs - including the Juilliard School's Pre-College Program and Boston University's Tanglewood Institute - join pro musicians in offering practical answers to questions from what instrument to play to where the musical road may lead. For more information, click here.\nMLA 8 Citation\nNathan, Amy. \"Music That's Out of This World.\" Nonfiction Minute, iNK Think\nTank, 11 May 2018, www.nonfictionminute.org/the-nonfiction-minute/\nDo you ever feel spaced-out before you take a test? Yes or no, let’s go!\n1. TRUE or FALSE?\nIt’s possible for a spacecraft to fly from Earth to Venus, to Mars, back to Earth, then to Saturn, out to Pluto, back to Jupiter, and come home to Earth on one tank of fuel.\n2. It’s possible for a spacecraft to fly all over the solar system on one tank of fuel because of:\na. the sling-shot effect\nb. gravity assist\nd. all of the above\ne. none of the above\nThe sling-shot effect, also known as a swing-by or gravity assist, is used to accelerate a spacecraft. Acceleration means to change the speed and/or the direction of a moving body. A spacecraft that is speeding up, slowing down, or following a curved path is accelerating.\nGravity accelerates objects everywhere in the Universe. When you ride your bike up a hill it takes a lot of effort to make it to the top because the Earth is massive compared to you, and gravity pulls you toward its center. When you coast down the other side, gravity is your friend!\nSpacecraft can use the gravity of a planet to accelerate. Picture a spacecraft falling toward a planet. The spacecraft will crash unless it steers away.\n3. As a spacecraft accelerates toward a planet, the motion of the planet is also affected by the gravity exerted by:\na. the spacecraft\nb. the Sun\nc. cosmic rays\nd. both (a) and (b)\ne. both (b) and (c)\nf. all of the above\ng. none of the above\nAll bodies in space, no matter how big or small, exert gravity on each other. Planets stay in orbit around the sun because of gravity. A planet is also affected by the tiny mass of a spacecraft. Gravity assist was used to increase the speed of Voyager 1 by 36,000 mph on its swing by Jupiter, which sling shot it to Saturn. And Jupiter slowed down infinitesimally, at a rate of 12 inches per one trillion years.\n4. The person who discovered the math for using gravity assist to accelerate a spacecraft from planet to planet to planet…was:\na. Aristotle (384 B.C. to 322 B.C)\nb. Galileo (1564-1642)\nc. Sir Isaac Newton (1643-1727)\nd. Katherine Johnson (1918- )\ne. Michael Minovitch (1936- )\nEND OF TEST!\nDON’T STOP WORKING.\nGO TO THE LIBRARY TO FIND THE ANSWERS.\nIn this drawing a spacecraft gets an assist from Jupiter as it \"slingshots\" toward Saturn. Image courtesy of NASA/JPL\nVoyager 1 and Voyager 2 used gravity assist to fly by the outer planets. Image courtesy of NASA\nThe twin Voyagers have no people on board on their interstellar journey, but carry The Golden Record, which contains messages, music, and pictures from Earth. Image courtesy of NASA/Alexandra Siy\nIn case you didn't make it to the library: In 1961, UCLA graduate student Michael Minovitch used math and the new IBM 7090-7094 computers to invent gravity assist trajectories for space flight. Used with permission of Michael Minovitch\nAlexandra Siy's Voyager's Greatest Hits tells the story of the twin space probes that traveled to Jupiter, Saturn, Uranus, and Neptune, a journey beyond our solar system into interstellar space, where no probe has ventured before. Siy tells the fascinating story of how the Voyager probes work, where the probes have been and what they’ve seen, and what they carry on board.\nAlexandra Siy is also a member of Authors on Call. You can bring her to your classroom via interactive videoconferencing and learn more from her and ask her questions. To find out more go here.\nMLA 8 Citation\nSiy, Alexandra. \"Spaced Out.\" Nonfiction Minute, iNK Think Tank, 2 May 2018,\nNonfiction is the New Black\nIn 1983, shortly before she became America’s first female astronaut to participate in a mission, Sally Ride faced a press conference. Reporters raised questions they would never have asked a man. “Will the flight affect your reproductive organs?” one inquired. “Do you weep when things go wrong on the job?” queried another. A third wondered, “Will you wear makeup and a bra in space?” Tonight Show host Johnny Carson joked that the flight was delayed because Sally had to find a purse that matched her shoes.\nIt wasn’t just U.S. media. The Soviet Union had already sent two women into space. When one of them arrived at the space station, a male cosmonaut (the Soviet term for astronauts) said, “An apron is waiting for you in the kitchen.”\nBy this point, Sally had mastered parachute jumping, water survival, coping with weightlessness and the massive G-forces from a rocket launch, and other highly demanding skills. She flew jet planes. She had a Ph.D. degree in physics from Stanford, one of the nation’s top universities. She helped develop a robotic arm for use on the space shuttle. She was a nationally ranked tennis player who decided not to turn pro because she preferred science.\nThe general public seemed more accepting. On launch day at Florida’s Cape Canaveral, thousands of people wore “Ride, Sally, Ride!” T-shirts, from the lyrics of the pop song “Mustang Sally.”\nThe mission went flawlessly, and Sally flew again the following year. She was scheduled for a third flight in 1986, but it was scrubbed when the Challenger space shuttle blew up.\nSally left the space program soon afterward. She was passionate about encouraging young people—especially girls—to become involved in STEM (Science, Technology, Engineering, Math). Here are some of the things she did toward that achieving that goal.\nSadly, Sally Ride died of cancer in 2012 at the age of 61. Shortly afterward, President Barack Obama awarded her the Presidential Medal of Freedom. It is the nation’s highest civilian honor.\nTo find information on many of Jim Whiting's books, click here.\nMLA 8 Citation\nWhiting, Jim. \"'Ride, Sally, Ride!'\" Nonfiction Minute, iNK Think Tank, 14 May\nIn spring 1665 a college student named Isaac Newton studied natural philosophy, what we call “science.” Back then, a good student could learn everything to know about the natural world. But plague, the Black Death, came to England. Cambridge University closed. Isaac went home to Woolsthorpe.\nFor two years Isaac thought about his studies during four years at university. He’d always been thoughtful—not the best at games, making friends, or minding sheep. But everybody knew Isaac Newton liked to think. Folks told time by the sundial he’d drawn on a wall.\nHome at Woolsthorpe, Isaac’s learning about science and math bubbled up in his head like yeast rising in a loaf of bread.\nSo... Newton unplugged. His mind roamed like that of an artist or composer. He was driven by the need to create—not paintings or symphonies, but questions.\n“Why do things always fall down?”\n“Why does the earth move around the sun?\n“Why doesn’t the moon fall onto the earth?”\n“Does everything ‘up there” work like things work ‘down here?’”\nIsaac Newton answered his questions with three science rules, Newton’s Laws of Motion.\nAt Woolsthorpe, Newton grappled with the concept of moving objects. He worked out the math to find the area under curves. He called this math fluxions. Today we call this calculus, useful for launching rockets or tracking TV signals.\nOnce back at Cambridge, Newton said nothing until he read someone else’s paper on fluxions. Newton published a better paper. Soon he was Cambridge’s top math professor.\nIsaac Newton wondered another twenty years. He played with prisms in a dark room and theorized that white light comprises the visible spectrum of red, orange, yellow, green, blue, indigo, and violet. He practiced alchemy and chemistry, looking for the legendary philosopher’s stone to turn base metals to gold. In 1687, Newton published our most important science book, the Principia.\nIn the Principia, Newton showed how laws of gravity and motion work the same at great distances—far off in space, or in your classroom. We accept these ideas, but in 1687 many still had medieval beliefs that sun, moon, planets, and stars all traveled in their own crystal spheres.\nYes, Newton wondered about A LOT:\nSir Isaac Newton was an English mathematician, astronomer, theologian, author and physicist who is widely recognized as one of the most influential scientists of all time and a key figure in the scientific revolution. Based on a portrait by Godfrey Kneller, 1702, via Wikimedia Commons\nSir Isaac Newton's own first edition copy of his Philosophiae Naturalis Principia Mathematica with his handwritten corrections for the twentieth edition. Photograph Andrew Dunn via Wikimedia Commons\nTrinity College, the part of the University of Cambridge where Newton worked and lived. Library of Congress\nThis statue of the young Isaac Newton stands at the Oxford University Museum of Natural History. Look carefully around his feet for a hint on what he is wondering about. If you can’t figure it out, then read about Newton and gravity.\nFeaturing 21 hands-on projects that explore the scientific concepts Isaac Newton developed, Kerrie Logan Hollihan's Isaac Newton and Physics for Kids paints a rich portrait of the brilliant and complex man and provides readers with a hands-on understanding of astronomy, physics, and mathematics. A time line, excerpts from Newton's own writings, online resources, and a reading list enhance this unique activity book.\nMLA 8 Citation\nHollihan, Kerrie Logan. \"Isaac Newton's Wonder Years.\" Nonfiction Minute, iNK\nThink Tank, 21 Feb. 2018, www.nonfictionminute.org/the-nonfiction-minute/\nFor Vicki Cobb's BLOG (nonfiction book reviews, info on education, more), click here: Vicki's Blog\nThe NCSS-CBC Notable Social Studies Committee is pleased to inform you\nthat 30 People Who Changed the World has been selected for Notable Social Studies Trade Books for Young People 2018, a cooperative project of the National Council for the Social Studies (NCSS) & the Children’s Book Council","Funkin’ it up on the blues harmonica\nWhat shall we play now? Well, as the late great James Brown put it, whatever we play, it’s got to be funky! Wise words from a guy who learned harmonica as a kid; as well as the guitar, drums, piano, and of course, some hefty vocals.\nHarp players are – or should be – conscious of their scope for providing not only a horn line, but a whole horn section on the humble tin sandwich. By this I mean everything from a melody which might otherwise be delivered by a single brass instrument, to a fanfare or complete horn-style fill. It’s all there waiting to be mined. So let’s dig deeper…\nI Feel Good\nMany top harp players we’ve spoken to have said that listening to horn players had a massive influence on their own development. Gerry Portnoy said it, Magic Dick said it and Lee Oscar said it. And it’s widely documented that Little Walter himself was heavily influenced by the jump jive and jazz players of the day.\nHarp players can borrow so many ideas from their brass cousins. The themes, the fills, the attack, the phrasing and the timing. I’m thinking Water Melon Man, I’m thinking Mozart’s Horn Concerto No.4 in Eb Major, I’m thinking Last Post, I’m thinking The Peter Gun Theme and at the moment, of course, I’m thinking James Brown.\nCheck out this simple trick. Pick up a 10 hole major diatonic in any key and sound these draw notes in sequence: 2D 3D 4D 5D 6D Sound vaguely familiar?\nLike I knew that I would\nThe harp is telling us something here. It wants to get funky! You’ve just learned one of the key lines in James Brown’s I Feel Good (I Got You). It’s iconoc and it couldn’t be simpler. Now grab a G Major 10 hole diatonic and let’s carry on the journey.\nThe songs starts with the lyric Whooah! I feel good… And we instantly make like an air Saxophonist – thumb between our lips, humming the horn part that follows. Instead let’s grab our harmonica and make some noise, hitting the draw two (2D) and the full draw two bend (2D”):\n2D..2D” 2D..2D” 2D..2D” 2D\nLike I knew that I would garners the same horn response. Great! Now the lyric repeats while the song moves from the I chord (D major), up to the IV chord (G major). We have move with it and we need find the best fit to echo the next horn response:\n4B…3D’ 4B..3D’ 4B..3D’ 4B\nAn uncomfortable switch back and forth, but effective nonethless.\nFeel good, feel good, I got you\nNow we can bring in the howitzers. We all know the phrasing: Feel good – bomp, bomp, feel good – bomp, bomp, I got you! For bomp, bomp, read full-on octaving with a combined shunt from the diaphragm and epiglottis.\nI got you… and we’re back to where we started our tab:\n2D 3D 4D 5D 6D\nWhen I hold you in my arms\nThen we have to emulate that beautifully exposed line; one that could so easily be the start of a separate jazz number. It goes like this, starting with an octave jump:\n4D 5D..4D 3D’\n4D 5D..4D 3D’\nAnd don’t worry if the bend on draw three sounds slightly out at first. I don’t think the original hits the same note cleanly every time. Work it. Use you ears, your bending technique and your instincts to achieve the best match. Loosen up and work it some more.\nMy love won’t do you no harm\nNow we move into a chordal horn section vamp, with a soul flavour and a crafty little addition. Hit the vamp, then wait to add the fill. You’re head will be telling you all kind of othehr stuff, but this is how it goes. Remember the old rule, less is more:\nI know that I can do no wrong 2D..2D” 2D..2D” 1D\n1B-2B-3B-4B 1B-2B-3B-4B ….and pause\nAnd back to the 2D..2D” response covered above. All you now need to do is work the changes as you move through the song. Until the end. Whereupon you repeat the octave hits a couple of times before looping around the blues scale:\n2D 3D 4D 5D 6D 6B 5D 4D 4B 3D’….\nAnd for the big finish, perhaps a 4D~5D trill or a modulating, trumpet-style, blow bend 9B 10B..10B’~ Well done, the Grand Funkmeister would be proud of you! Granted, the original has a second harmonic brass line that adds that edge to the horn part, but blast your delivery through a good valve amp and no-one will know the difference. Enjoy!"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:ab669a3c-4e6a-45d0-a744-83ee73f0d40a>","<urn:uuid:f0d6c6fd-8f9e-4e8b-9c15-6886ccdce13e>"],"error":null}
{"question":"¿Cómo se comparan los efectos en la salud respiratoria de las emisiones volcánicas naturales frente a las emisiones de los incendios forestales?","answer":"Natural volcanic emissions and wildfire smoke both affect respiratory health but in different ways. Volcanic eruptions release pollutants like sulfur oxides and hydrogen sulfide, with effects that can be global in scale, as demonstrated by Mount Pinatubo's eruption which impacted global temperatures. Wildfire smoke, on the other hand, contains a specific mixture of fine particles (PM2.5) and hazardous gases that directly affect the respiratory and circulatory systems, leading to both short-term symptoms like wheezing and long-term health issues including asthma, bronchitis, and pneumonia, as well as potential impacts on heart disease and mental health.","context":["Wildfires and Lung Health Complications\nThe destruction wrought by wildfires is increasingly evident to the modern eye – one that observes the fire, its distant smoke, and the harmful microscopic particulate matter that is harmful to lungs when inhaled.\nAccording to research from Current Pulmonology Reports\n, smoke from wildfires contains a harmful mixture of fine particles (PM2.5) and hazardous gases that invade the respiratory and circulatory systems. Exposure to such particulate matter can result in a range of consequences. These consequences span from short-term irritation and wheezing to long-term declines in lung function. Diseases like asthma, bronchitis, and pneumonia become aggravated, and connections to heart disease and mental health add layers to this complex problem.\nWildfires, a natural occurrence often intensified by human activities and climate change, are a growing concern in North America, prompting government officials and health leaders to deliberate and propose ideas for adapting to and combatting such weather conditions.\nIn June 2023, North American cities were blanketed by thick smoke from Canadian wildfires. As reported in The Lancet\n, officials were quick to recognize and communicate the health risks. New York City's Health Commissioner, Ashwin Vasan, detailed how particulate matter from the smoke can affect those with existing health conditions, emphasizing the need to stay indoors and to wear masks if outdoor activity couldn’t be avoided.\nLast August, the U.S. Environmental Protection Agency (EPA) published guidelines urging health workers to anticipate smoke events and tailor disease management plans accordingly. Measures like stocking up on government-approved N95 respirators and creating personal contingency plans indicate our growing understanding of the issue.\nThe Lancet states that since 2001, exposure to wildfire smoke has increased, with projections showing a growing trend. But the response from governments, researchers, and community leaders demonstrates a determination to address the problem head-on. Kristie Ebi of the University of Washington insists on an \"all-of-society approach,\" promoting better management across various sectors and social groups –one that is open to the adoption of new tools and strategies for progress.\nAI as an aid in fighting wildfire-induced health risks\nAn expanding body of research demonstrates AI’s ability to competently aid doctors in detecting lung malignancies early – a feature which could be a game-changer in the fight against wildfire-induced lung health risks.\nAI’s potential in medicine has seen a rise, offering early detection and interventions in lung illnesses. For example, the deep learning algorithm, qCT-Lung\nfrom Qure.ai, demonstrates an example of AI’s capabilities in identifying pulmonary nodules, among other critical health markers.\nAccording to research from the journal Chest\n, during the COVID-19 crisis in India, qCT-Lung was used on chest CT scans, successfully detecting nodules and even identifying lung cancer cases. The synergy of human expertise and AI-driven insights marks a significant advancement in healthcare.\nPractitioners do not confine the integration of AI in healthcare to cancer detection. There is a growing interest in tailoring AI to address wildfire-induced lung diseases, specifically involving assisting healthcare professionals in early diagnosis and providing personalized care. Using real-time data from wildfire occurrences can further enhance the management of health impacts.\nResearch from PLOS ONE\nprobed the capacity of a deep learning algorithm in analyzing routine frontal chest radiographs (CXR) from adult patients. Researchers applied the algorithm to 874 frontal CXRs from 724 individuals, targeting pulmonary opacities, pleural effusions, and other chest irregularities.\nAround 42% of the analyzed CXRs exhibited no abnormalities, with the rest divided between single and multiple irregularities. Notably, the analysis by the deep learning model was on par with human evaluations, showing no substantial statistical difference in detecting all abnormalities, thus accentuating the technology’s adeptness in mirroring human precision.\nThe research also yielded insights into the effectiveness of DL, as measured by the area under the curve (AUC). Deep learning’s effectiveness ranged between 83% and 93%, closely paralleling the 69% to 92% range for human radiologists. Even more fascinating was that the lowest point noted in the study did not diminish the overall robustness of deep learning’s capabilities but highlighted specific areas for further refinement and research.\nMoreover, the finding that the presence of chest wall implanted devices could influence the accuracy of the DL algorithm offers an opportunity for enhancing the technology, tailoring it to accommodate even these complex scenarios.\nIntegrating AI into medical practice is no longer confined to theory, particularly in the critical domain of lung cancer screening. Further research from Chest\nsheds light on a promising collaboration between traditional radiological techniques and AI, offering an exciting pathway toward enhanced diagnostic accuracy.\nThis research sought to compare how radiologists use the standardized Lung-RADS classification to categorize lung nodules with how an AI algorithm calculates the malignancy risk for those same nodules. Chest researchers studied 210 adult patients, employing human expertise and machine learning to distinguish between malignant and benign nodules.\nThe AI algorithm ventured beyond mere imitation, assigning malignancy risk scores based on a diverse range of factors, such as the size and shape of the nodules, family history of lung cancer, and the patient's smoking history. The results were telling; AI-generated risk scores were notably higher for malignant nodules, clearly differentiating them from benign ones.\nThis comparison between traditional Lung-RADS classification and AI scores was not merely an academic exercise. The AI's ability to predict malignancy was comparable to that of the Lung-RADS system. These close results highlight the potential synergy between human intuition and algorithmic precision. The implications of this study are profound, suggesting that AI doesn't merely mimic human expertise but augments it. AI-generated malignancy risk scores demonstrate significant sensitivity in identifying cancerous nodules through low-dose CT scans for lung cancer screening.\nThis collaborative approach may signal a new era in early cancer detection, where technological innovation enhances human insight. In this merging of minds and machines, both are essential contributors to a potentially life-saving process.\nChest X-rays, renowned for their ability to delineate the anatomical structures of the chest, can identify abnormalities, but they might not be the ideal tool for detecting small particulate matter, especially if it's dispersed throughout the lungs. This technology primarily visualizes structures by differences in density, such as bones appearing white and air appearing black. Particulate matter may not present a significant density difference to be distinguished easily, especially if it's microscopic.\nHowever, alternative imaging modalities might offer greater insight. A CT scan, for example, provides more detailed images and can detect smaller changes in tissue density. In cases of pneumoconiosis\n—a disease frequently observed in those who have inhaled wildfire particulates—the inhaled particles can lead to fibrotic changes in the lung tissue, which can be visualized on a CT scan.\nIn these instances, the inhaled particles themselves might not be directly visible. However, the secondary effects they cause become the markers for their presence. And these include fibrosis, nodules, or other structural alterations. A skilled radiologist could infer the presence of particulate matter based on these characteristics with the augmentation of AI software such as qCT-Lung, enabling them to see more patients and prioritize critical cases.\nWildfires represent more than mere destruction; they highlight a nuanced health crisis that extends far beyond the visible landscape. The promising advances of AI provide a compelling answer, detecting, diagnosing, and potentially mitigating the respiratory impacts of wildfires.\nThe combination of human insight and technological innovation offers a promising future. AI's role in addressing the challenges posed by wildfires and lung health is a compelling testament to progress, showcasing an effective response and a vision of what might be.\nThe potential of AI in transforming the landscape of medicine and health is immense. This new chapter in diagnostics and treatment is not just a medical footnote but a significant stride toward improved care and understanding. As we grapple with the multifaceted effects of wildfires, technology offers a vision of hope and a path to a more responsive and empathetic healthcare system.","Search keywords : Bag filter VOC exhaust gas treatment Wet dust collector Dry dust collector Spray tower\nClassification of air pollution sources\nOrigin：Hefan Hit： Time：2020-03-17\nPollution source refers to the source of various pollution factors that cause environmental pollution. For example, a plant, site, or equipment that releases pollutants to the environment or releases harmful factors.\nI. Classification of pollution sources\nPollution sources can be divided into two categories, namely natural pollution sources and human pollution sources. For environmental science, the main object of research and control is man-made pollution sources.\n(I) Natural pollution sources\nNatural pollution sources Natural pollution sources refer to the sources of various pollutants caused by the movement of nature. For example, a volcanic eruption can emit a large amount of dust (volcanic ash), smog and chemical pollutants such as sulfur oxides and hydrogen sulfide; forest fires bring a large amount of carbon monoxide, carbon dioxide and incompletely combusted organic smog to the atmosphere; wave motion can Drops of saline water are thrown into the air, and the water evaporates to form salt grains; strong winds can bring sandy soil in desert areas into the air, and even re-sediment thousands of kilometers away; plant pollen is also a source of allergies to humans. Pollutants emitted by some natural pollution sources have a huge amount of adverse effects on the human environment. For example, in June 1991, a massive eruption of Mount Pinatubo in the Philippines erupted a large amount of ash and SO2 into the stratosphere. Pinatubo erupted about 18 million tons of SO2 into the stratosphere at one time, which is almost equal to the total amount of SO2 emitted by the United States in one year. After the volcanic eruption reached the stratosphere, it drifted from east to west around the equator and eventually covered the world. The SO2 gas is converted into extremely small sulfuric acid droplets, which generates a mist that reflects and scatters sunlight. Experts believe that this mist will reduce the average earth temperature by more than 0.3 ° C in 3 to 4 years. Some scientists also believe that volcanic particles may act like ice crystals over Antarctica, causing chemical reactions that destroy the ozone layer in densely populated mid-latitude regions. The environmental harm caused by natural pollution sources has attracted people's attention, but it is still difficult for human beings to control the main introduction of man-made pollution sources.\n(2) Anthropogenic pollution sources\nThe source of environmental pollution caused by human production and living activities is the man-made pollution source. The categories are summarized below.\nAccording to the nature of human activities, it is divided into industrial pollution sources (metal smelting, power generation, oil refining, mining, petrochemical, electroplating and other industrial and mining enterprises), agricultural pollution sources (using pesticides, chemical fertilizers, etc.), transportation pollution sources (modern vehicles, such as aircraft Exhaust gas from automobiles, ships, ships, etc.) and sources of domestic pollution (living coal such as heating and cooking).\nAccording to the nature of the polluted object, it is classified into sources of air pollution, sources of soil pollution, and sources of water pollution.\nAccording to the spatial distribution of pollution factors, quantile pollution sources (point distribution, such as the three wastes discharged by industrial and mining enterprises, towns, hospitals, scientific research institutions, etc.), surface pollution sources (also known as non-point pollution sources), and line pollution sources (shown as lines) Pollution sources such as exhaust gas from cars, trains, and other means of transportation on the main transportation lines also include noise from traffic workers). Point pollution sources and surface pollution sources are fixed pollution sources, while line pollution sources are mobile pollution sources.\nAccording to the physical and chemical properties of pollution factors, they are classified into organic pollution sources, inorganic pollution sources, mixed pollution sources (pollution sources that emit multiple pollutants at the same time), thermal pollution sources, noise pollution sources, radioactive pollution sources, and pathogen pollution sources.\nClassified according to the form of pollutants, it is divided into waste gas, waste water, solid waste, noise, and radioactive materials. This chapter focuses on anthropogenic sources of pollution in the atmosphere. Emphasis is on man-made pollution sources from fuel combustion and industrial processes, as well as sources of exhaust emissions from cars in transportation pollution sources"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:0cd57c12-9aba-4db3-89ee-a95ef3fd1f69>","<urn:uuid:41a4695b-9f81-4f0e-b5f3-ccca1d49ce34>"],"error":null}
{"question":"Could you explain how data integration architecture relates to third-party data sharing risks, and what security implications arise from both internal and external data movement?","answer":"Data Integration Architecture, a component of Data Architecture in data warehouses, must consider both internal data movement and external sharing risks. While data warehouses are designed to collect data from disparate sources and present integrated solutions, this integration creates security vulnerabilities, particularly with third-party partnerships. Recent incidents have shown that data can be compromised through business partners' systems, as demonstrated by cases like the Quest Diagnostics breach where over 12 million patient records were stolen through a contractor's IT system. The Data Movement Architecture must account for these risks, while also protecting against internal threats. Organizations need to implement comprehensive security measures that address both the architectural requirements for data integration and the potential security threats from both inside and outside the organization, as threats can come from various sources.","context":["This article originally appeared on the BeyeNETWORK.\nWhat is architecture? Architecture is the combination of the science and the art of designing and constructing physical structures. A wider definition often includes the design of the total built environment, from the macro level of the physical structure itself to the micro level of architectural or construction details.\nAn information system (IS) is the system of persons, data and activities that process the data and information in any organization, including manual and automated processes. A data warehouse is a subset of the information systems, whose purpose is to collect data from disparate sources and present it as integrated solutions to customers.\nData warehousing architecture is a complex subject. It is not a simple database on a server with a data model and processes to load and query data. Rather, it is the foundational layer for the business intelligence initiatives in the organization. A data warehouse is a program that will enable multiple projects. Its architecture and the blueprint that will drive its construction are critical to the success or the failure of the program and its projects.\nData warehouse architecture can be classified into the following areas:\n- Business Architecture\n- Technology Architecture\no Hardware Architecture\no Software Architecture\no Database Architecture\no Security Architecture\no IT Governance Architecture\n- Data Architecture\no Data Integration Architecture\no Data Movement Architecture\no Metadata Architecture\no Master Data Architecture\no Data Governance Architecture\n- Business Intelligence Architecture\no Data Visualization Architecture\no Data Querying Architecture\no Data Analysis Architecture\no BI Governance Architecture\nAs we list these major components, there are several interesting approaches to integrating the major components at the micro level in a given data warehouse. This brings us to another major component for a data warehouse program – methodology. While architecture describes what we need to build and lists the components, methodology represents the delivery mechanism of how to build the data warehouse and deliver the same.\nArchitecture and methodology need to work together for the overall success of any data warehouse program. While on this subject, architecture and methodology describe the technology and process legs of a three-legged stool. The other leg of this stool is the people or the data warehouse team.\nThere are several architectures available to choose for a data warehouse implementation. How do you select the right architecture for your organization? An easy technique will be to assess the components listed above in a weighted scorecard against the architectures and select the top two as your ideal choices.\nAs shown in this simplified approach, the four steps to selecting and implementing architectures can be easily confined to the process of defining the needs, assessing the choices, designing a solution on one or more selections and doing an easy prototype. This approach will definitely help you in the entire program, though you might choose to implement instead of prototype.\nAs we conclude this article, the fundamental goal of a data warehouse architecture is to present the blueprint and a road map to build a complex data processing and integrating infrastructure, while enabling the delivery process to control the build and deploy in a selected methodology framework.\nMy next article will discuss the different architectures across the multiple components by subject areas.","sdecoret - stock.adobe.com\nProtect against evolving data security threats\nAs data security threats evolve, knowing how to protect your data is more important than ever. Learn about the latest security threats and how to ward them off.\nCorporate data needs to be secure, private and protected. That's obvious advice, but the steps organizations should take to prevent data security threats and keep their data safe from hackers are much less apparent.\nThis article looks at some of the tactics -- both old and new -- hackers are using in their attempts to access your data. Spoiler alert: Protecting your organization from data security threats requires a comprehensive approach. Security isn't just orchestrated through a single security department, and it can't merely reside in a single layer of a protocol stack. Threats come from various sources, from both inside and outside an organization. Combating these threats requires a multilevel and multifaceted strategy that includes not just IT, but other departments, including HR, accounting and legal. Here's a look at some trends that pose the biggest threats to corporate data and the actions you can take to protect it.\nUndetected data security threats\nThese can be called exploits waiting to happen. Vulnerabilities that may already exist in your corporate systems can be used to compromise data privacy. For example, legacy systems might have built-in backdoor administrative passwords. These potential superuser identities might have access to all data, thus enabling the users to steal data without even having to hack a real user's credentials.\nProtecting your data means making a list of all third-party and in-house IT systems in use. Verify whether these systems have any superuser IDs. If so, confirm that the user ID's password isn't set to the system default and that it is either disabled, if not needed, or, if used, guarded by a strong password.\nFor everyone else, access control lists (ACLs) will be the primary tool for protecting data. ACLs specify data access rights, among them read-only, read/write, write-only or no access. These rights are then assigned to user profiles, which are, in turn, associated with users.\nIt is a thankless but necessary task to review your corporate ACL structures and certify ACL assignments reflect current corporate needs. Some lists might include all-access permission that was initialized when the current server environment was installed as a way to debug the system or to assist users with trouble accessing a particular file. ACLs don't expire. ACLs with global file access rights can effectively open a backdoor that can be used to compromise data -- even from within the corporate network. ACLs created for special groups or projects that are no longer active should be deleted.\nDespite an organization's best efforts, many breaches are essentially self-inflicted: Phishing attacks, propagated through emails cloaked with the look and feel of legitimate senders, are a major hazard today and likely will continue to be so in the years to come. These attacks become even more dangerous because users who click on these messages may use similar ID and password combinations for multiple systems, putting additional data in jeopardy. Even more insidious are newer, so-called whaling attacks, where the sender masquerades as the user's boss or some other known executive within the organization. Because the emails appear to be genuine business requests, recipients often comply.\nTried-and-true security options\nWhile addressing new and more subtle methods for compromising data, don't forget malware's fast-and-frontal attacks. Malware still exists and may come in the form of a SQL call or be carried in the code of a utility program downloaded by an employee. Regardless of its source, malware remains a force to be reckoned with when protecting your data.\nA basic, garden-variety, port-filtering firewall isn't sufficient protection against all data security threats. Deploying an intrusion detection and prevention system (IDS/IPS) that provides deep packet inspection will increase the effectiveness of an organization's security perimeter and reduce the severity of attacks that get through. Agent-based endpoint security management software capable of tracking malware signatures is another important tool. A multipronged approach -- one that includes user education -- is particularly important in battling ransomware, which is perhaps the most virulent form of malware. This is the ultimate attack on corporate data privacy and security. A successful ransomware attack can paralyze a system -- and, potentially, a company. The most common way for these attacks to occur is by getting someone to run infected software on the company's computers. The best way to eliminate that chance is to use a security strategy that relies on a cohesive -- and up-to-date -- set of endpoint, firewall and IDS/IPS tools.\nIn contrast to attacks that try to barge in via a firewall or email, a whole new breed of attacks can be described as low-and-slow intrusions. Instead of malware running on a computer, these attacks are funneled through low-level applications or devices, like surveillance cameras, and are deliberately programmed to avoid detection by exfiltrating data slowly over time.\nThese attacks use a variety of ways to harvest data. A Tolly Group evaluation of low-and-slow attacks in 2019 found that, in one case, malware had compromised the OS of a surveillance camera. The malware collected information about devices inside the company, while also still performing its video functions, periodically sending that data to an external website. Since many firewalls are configured to assume outgoing traffic is legitimate, the information was exfiltrated successfully.\nIn another example, exfiltration software was contained in an unregistered Google Chrome browser extension. Residing as part of the browser, it was able to gather data, which it then exfiltrated to an external website. Another incursion involved code that used the DNScat tunneling tool to take data files from the PC and send them to the attacker's website, which evades perimeter security in the process.\nTo deal with these kinds of threats when protecting your data, consider adding a new type of security system: a network detection and response (NDR) system. Unlike IDSes/IPSes that look for signatures, NDR systems use AI and machine learning to monitor network as they learn to understand normal traffic. An NDR system will detect anomalous traffic and alert corporate security teams to stop a low-and-slow exfiltration if it attempts to contact an external website to deliver stolen data.\nConsider DLP tools\nConsider ways to keep tabs on data. One way to do that is to use data loss protection (DLP) software. DLP generally relies on an agent that runs on every client device. The software runs in conjunction with a management server and uses templates that identify data that needs to be protected from removal. Typical examples include data strings, like Social Security or credit card numbers. Templates can also be created to flag certain keywords, such as trade secret or proprietary, to ensure documents containing these terms are protected. Generally, template-based DLP tools work best on structured data or files containing evident examples of confidential text. Newer DLP products use features that examine the movement of all data rather than examining specific data patterns. These tools don't intercept data; rather, they create a trail of evidence to enable security teams to remedy leaks by pinpointing who may have exposed the data and the type of information revealed.\nThe partnership data-security conundrum\nWhen data gets compromised, there is little solace in saying, \"It wasn't us. It was our partner.\" Keeping systems and data secured can be difficult when they are controlled by a partner. Indeed, the most serious problems organizations face may stem from lax security on the part of partners -- businesses or cloud storage providers -- with whom they share or deposit data.\nToday's commerce almost requires companies share important data files with their business partners. But those partnerships also include risks in terms of data security threats. More than 12 million patients who used Quest Diagnostics had some of their medical records stolen when hackers accessed a contractor's IT system between 2018 and 2019. The contractor, American Medical Collection Agency, was used by both Quest and LabCorp to handle billing collections. More recently, Amazon subsidiary Ring said some customer accounts were stolen by hackers who accessed an unidentified third-party service.\nEqually worrisome, cloud providers aren't immune from problems. The so-called Cloud Hopper hack, revealed at the end of 2019, gave hackers unfettered access to data from a myriad of clients. Security analysts have confirmed trade secrets and other intellectual property were contained in stolen files. The victim companies had assumed their cloud storage vendors had adequate security. Bottom line: Don't assume files handed over to SaaS vendors are safe. They may not be.\nProfitable data -- be it credit card records, Social Security numbers or any other transactional information -- will always be the target of motivated hackers. All they need is one way in. Keeping up with new hacking techniques and new safeguards, as well as constructing a strong security foundation, is the best way of protecting against data security threats.\nHow to secure data at rest, in use and in motion"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:9976dfdf-c07c-4c63-8934-9c35ee56abd6>","<urn:uuid:a57ddfde-aaee-45bc-96c9-d652a652f24d>"],"error":null}
{"question":"As a public transit enthusiast, I'd like to know how research influenced Cleveland's transportation system. What changes did the Greater Cleveland Regional Transit Authority make based on urban poverty studies?","answer":"The Greater Cleveland Regional Transit Authority adjusted its routes after a report from the Center for Urban Poverty and Community Development showed that Cleveland's inner-city residents couldn't access available jobs in the outer-ring suburbs via public transportation.","context":["Time and again, her scholarly findings have prompted concrete leadership changes—which in turn have improved the lives of the people she studies.\nFor example, when the Center for Urban Poverty and Community Development, which she directs, released a report showing Cleveland’s inner-city residents couldn’t get to available jobs in the outer-ring suburbs via public transportation, the Greater Cleveland Regional Transit Authority adjusted its routes. And when her research showed the major role disadvantaged neighborhoods play in people’s lives—leading to systemic issues such as lack of access to education, jobs and food as well as income inequality and health disparities—she founded the National Neighborhood Indicators Partnership, a 35-city collaboration that develops and supports the use of neighborhood research in local policymaking and community building.\nHer commitment and dedication to assisting others, to her academic field and to Case Western Reserve will be honored this week when she is named a Distinguished University Professor during fall convocation. The ceremony will be held Wednesday, Aug. 29, at 4:30 p.m. in Severance Hall.\nRecognizing the outstanding contributions of full-time, tenured professors with exceptional academic records of research, scholarship, teaching and service, the Distinguished University Professor title is the highest honor bestowed upon professors at Case Western Reserve University. Five faculty members will be given the permanent, honorific title during convocation and will be profiled throughout upcoming issues of The Daily.\nCoulton, the Lillian F. Harris Professor of Urban Social Research at the Mandel School of Applied Social Sciences, first became inspired to study urban poverty during the height of Martin Luther King’s Poor People’s Campaign. The movement raised her consciousness to the issues of persistent poverty—especially those of access to education, health and jobs—and led her into the field of social work. She earned a bachelor’s degree in sociology from Ohio Wesleyan University and a master’s in social work from The Ohio State University before earning a doctoral degree at Case Western Reserve.\nShe joined the Mandel School faculty in 1978, and 10 years later Coulton and the Mandel School received a grant from the Rockefeller Foundation and Cleveland Foundation to fund research into community conditions and how to improve the plight of those in distressed neighborhoods. This led to the creation of the Center for Urban Poverty and Development, which drew praise from leading scholars of the day and catapulted Coulton to the forefront of social work research.\nFrom there, Coulton helped develop the National Neighborhood Indicators Partnership, in which 35 cities across the country use the metrics collected on neighborhood quality to encourage systemic change. Having the research empowers people at all levels, from governmental organizations to grassroots associations, she noted.\n“Research can show how much neighborhoods matter, and by putting these metrics and tools in people’s hands we can help change their situations,” Coulton said. “We’ve done that here in this region, and I’m pleased I was a founder of the national partnership as well.”\nCoulton’s work has been published and referenced in numerous journals; in fact, her 1995 article describing the relationship between community factors and child maltreatment has been cited nearly 500 times. She’s also been recognized with numerous awards, including the Bruel Memorial Prize, awarded annually for the best article in Social Service Review. She was selected as the Aaron Rosen Endowed Lecturer at the Annual Conference for the Society for Social Work and Research in 2004 and inducted into the American Academy of Social Work and Social Welfare in 2010.\n“Professor Coulton is a stellar academician who possesses a record of outstanding accomplishments. She brings rigor, innovative methods and a multidisciplinary approach to addressing significant issues of concern to the profession and to society,” said Mandel School Dean Grover C. Gilmore. “She is also one of the most humble and generous faculty in the academic community. Her colleagues, students and staff are quick to praise her for a leadership style that encourages them to draw on their own knowledge and talents, which has resulted in a superior product and a willingness to work together harmoniously.”\nCoulton’s caring, encouraging nature is evident in the classroom, and it’s been recognized with the university’s John S. Diekhoff Award for Graduate Teaching.\nOne of her favorite aspects of her career, though, is mentoring students, which she’s done as an academic adviser and research supervisor to numerous graduate and doctoral students.\n“It’s a very special experience when you work with and mentor students who do great work here—but then go out to do even greater work when they leave,” she said.\nThe reward for Coulton comes through knowing not only has she helped change lives, but she’s helped develop others who can do the same.\nOther Distinguished University Professors have been announced already: James P. Anderson and Eva Kahana. The rest of the group will be profiled in tomorrow’s issue of The Daily. More information on past recipients can be found online."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:35653e71-feee-4084-9438-3ece08aea89c>"],"error":null}
{"question":"When was the last time Venus crossed in front of the sun and when will it happen next?","answer":"The last times Venus passed across the face of the sun (called a transit) were on June 8, 2004 and June 6, 2012. The next transit of Venus will occur on December 11, 2117.","context":["Venus Facts For Kids\nThe planet Venus is a little smaller than Earth and similar in mass and density, (what it’s made up of). It has a similar level of gravity too.\nIt has no rings round it, no moons and it spins backwards. What this means is that on Venus the sun rises in the West and sets in the East, which is the opposite to Earth.\nWho discovered Venus?\nWell no one really knows. It is now thought that ancient civilizations and the Mayan astronomers in 650 BC watched Venus very closely and from that creates a very accurate calendar.\nWhat is Venus?\nIt is the second closest planet to the sun and is the hottest planet – even hotter than Mercury which is the closest planet to the sun! Wow, that must be hot.\nThe reason for this is that Mercury can’t retain or keep heat, because it has no atmosphere.\nVenus has a very dense, rapidly spinning atmosphere made of carbon dioxide (CO²) and nitrogen (N²) with sulfur droplets in its clouds.\nRelated: Does Venus have rings?\nOnly trace amounts of water are found in the atmosphere. Its surface temperature is 864 degrees Fahrenheit (462 degrees Celsius).\nThis is hot enough to melt lead and there’s no way that anything can live on Venus at all.\nWhat does the word ‘Venus’ mean?\nVenus in most civilizations has been identified with love and war! Wow, two complete opposites.\nIt was named after the Roman goddess of love and beauty, apparently because of all the five planets known to the astronomers of old, Venus shone the brightest.\nThe reason that it shines so brightly is because it is close to Earth and the sunlight reflects on its clouds.\nWhat is it like on planet Venus?\nVenus is a very rocky planet with an iron core about 1,900 miles (3,000 kilometers) wide but has no magnetic field.\nIt spins too slowly to generate the kind of magnetic field Earth has. It has a solid surface, deep craters and volcanoes or volcanic centers bigger than 12 miles (20 kilometers) in diameter. Wow, that’s big.\nAmazingly, there are more volcanoes on Venus than on any other planet – about 1,600 and maybe even more. Not all of them are active though.\nThere are flow paths or channels on the surface running for hundreds of miles from erupted volcanoes.\nThe craters on Venus tend to be small, which are about 1.2 miles (2 kilometers wide). Any small meteor heading its way would burn up in the atmosphere before it could reach the surface.\nApparently about 300 to 500 million years ago there was massive volcanic activity on Venus which coated it in solidified basalt lava, which is as hard as rock.\nVenus is a very windy planet with winds reaching 450 miles per hour (724 kilometers per hour) in its middle cloud layer. These winds are much faster than Earth’s fastest tornado.\nIt wouldn’t be cool to be stuck in a wind storm on Venus, that’s for sure.\nIn the North Polar Region there is a huge highland area called Ishtar Terra which is about the size of Australia.\nIt has a mountain similar to Mount Everest on the eastern side, and a second area called Aphrodite Terra about the size of South America spreading across the equator for about 6,000 miles (10,000 kilometers).\nHas Venus been explored by humans?\nMore than 40 spacecraft have explored Venus, but it’s not easy to see through the thick atmosphere of Venus. NASA’s 1990 Magellan mission mapped 98 percent of Venus’s surface using radar.\nThere are other investigations that are carrying on from the Venus Express and Japan’s orbiter Akatsuki (Dawn) which is meant to arrive at Venus in 2015.\nThis will help us understand a whole heap more about this interesting planet.\nOther interesting facts about Venus\nVenus takes the equivalent of 243 Earth days to rotate once and the equivalent of 225 Earth days to orbit the sun.\nVenus is a rare planet in that it can sometimes be seen from Earth passing across the face of the sun, this is called a transit.\nTransits occurred in 1631, 1639, 1761, 1769, 1874, 1882, 8 June 2004, 6 June 2012 and the next transit will be on 11 December 2117, if anyone reading this is alive at the time!\nIn 2011, the pilot of an Air Canada flight saw Venus in its brightness. He thought it was an oncoming plane and took emergency procedures to avoid colliding."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:962ee27f-c532-4dbb-9a94-233cf0a5f96d>"],"error":null}
{"question":"How do the commercial applications of spinosad and spider venom peptides differ in agricultural and pest control settings?","answer":"Spinosad has achieved significant commercial success, having been derived from soil bacteria found at an abandoned rum distillery in the 1980s, and is now used in products like Censor larvicide for mosquito control in open or floodwater habitats. In contrast, spider venom peptides are newer to the market - Vestaron only began selling its first spider peptide-based biopesticide, Spear-T, recently for use against aphids, mites, thrips and whiteflies in greenhouse-grown vegetables and ornamental plants, with plans to expand to field crops in the coming years.","context":["Looking to fungi, spiders and other natural insect killers for less toxic alternatives to synthetic pesticides\nIn the 1940s, scientists discovered the insect-killing properties of synthetic chemical compounds, including DDT, chlordane and lindane. Such products were cheap to produce and very effective at protecting crops and fighting insect-borne disease. But by the 1960s, it was apparent that these chemicals — which can persist in nature long after initial use — were accumulating in the environment. What’s more, many were harmful to birds, fish, reptiles and mammals.\nMuch of this came to light through Rachel Carson’s revelatory book \"Silent Spring,\" published in 1962, but scientists already had begun to look for less toxic pest control alternatives. Researchers at University of California, Berkeley, and University of California, Riverside, defined in 1959 the principles of what later would be called integrated pest management (IPM) — an environmentally sensitive approach to controlling insects and other pests that relies on a combination of tools. Today, biopesticides are part of the IPM toolbox.\nBiopesticides are pesticides made from substances found in nature that offer a less toxic alternative to man-made chemicals. As a class they’ve been used for a long time, but most traditional biopesticides haven’t been able to compete with synthetic pesticides on cost and effectiveness. Researchers around the world are working to change that by manipulating chemicals found in plants, animals, bacteria and fungi. They’re discovering new ways to use nature’s arsenal to build better biopesticides and so reduce the need for less environmentally friendly pest control.\nA lot of the pioneering research around biopesticides focused on insect pheromones — chemicals that insects use to communicate with each other — says Jim Seiber, a professor emeritus of food science and environmental toxicology at University of California, Davis. The use of these chemicals to alter insect behavior when sprayed on crops or used as bait traps, for example, worked to some extent, but not nearly as well as synthetic insecticides. \"Most people like instant results,\" says Seiber. \"You could spray DDT and see the dead insects out there and count them. You couldn’t get those same results with biopesticides.\"\nAs a result, biopesticides became a niche market, today accounting for roughly 3 percent of the global annual market for pesticides. There have been a few commercial standouts, though. The popular insecticide spinosad, for instance, was derived from a kind of soil bacteria discovered at an abandoned rum distillery in the 1980s.\nBy the 1990s, scientists were documenting increasing problems with synthetics. Some had been used so extensively that insects were evolving resistance. Biochemist Glenn King turned to spiders for a potential solution.\n\"Spiders are professional insect killers,\" says King, a professor at the University of Queensland, Australia. Venom from spiders, scorpions, snakes and other predatory animals contain a mix of biologically active compounds called peptides. These small molecules kill prey in various ways. Some affect the nervous system. Others injure the cells and damage living tissues in other parts of the body. Some are specific to specific prey animals.\nThe idea was that, unlike manmade chemicals, the venom peptides wouldn’t leave residues that could persist in the environment for years, and because they targeted specific types of insects, they wouldn’t harm fish, reptiles, birds or mammals. King discovered a few molecules in the venom of the Australian Blue Mountains funnel web spider that could kill certain insects but wouldn’t harm other insects or vertebrates. In 2005, he founded a biotech company called Vestaron to turn these small molecules into biopesticides that farmers could use to protect their crops. The idea was that, unlike man-made chemicals, the venom peptides wouldn’t leave residues that could persist in the environment for years, and because they targeted specific types of insects, they wouldn’t harm fish, reptiles, birds or mammals.\nIn July, Michigan-based Vestaron began selling its first biopesticide derived from these peptides. Spear-T kills aphids, mites, thrips and whiteflies — common pests of greenhouse-grown veggies and ornamental plants. CEO Anna Rath says Vestaron plans to bring to market a family of peptide-based biopesticides in the next few years for use on field crops, too.\nThe company is also turning to bacteria to help expand where and how their products might be used, Rath says. It recently inserted a gene for producing a caterpillar-killing peptide derived from spider venom into a protein derived from a bacterium called Bacillus thuringiensis (Bt). When caterpillars chew on leaves that have been sprayed with the product, they’ll eat a little of the protein, which pokes tiny holes in their gut, helping to increase the penetration of the insecticidal peptide.\nBotanicals and fungi\nOther researchers are working to manipulate chemicals from plants and fungi to build better biopesticides.\nFor the past few decades, farmers around the world have employed essential oils, including neem oil — an extract from the Asian neem tree — as insecticidal botanicals. Essential oils, including neem, break down quickly in nature. This makes them environmentally friendly, but not very effective. Recently, scientists have started to experiment with encapsulating these botanicals in nanoparticles that could help to protect the active compounds from breaking down as quickly — an idea that’s been tried successfully with some medicines in the human body.\nFungi are another particularly promising source of biopesticides. \"They’re major disease-causing agents in nature,\" says Raymond John St. Leger, an entomologist at the University of Maryland. Many have evolved to parasitize specific insect hosts, devising gruesome methods for finishing them off. For instance, some ant fungi turn their hosts into zombies — infecting the ant’s brain and causing it to move to an area conducive to fungal growth before killing it and releasing fungal spores through the top of the ant’s head.\nThat natural specificity is a boon, because it means scientists can search for a fungus with a specific host range — for instance, one that kills bloodsucking flies but not bees or butterflies, says St. Leger.\nBut fungi work slowly, so St. Leger and University of Maryland Ph.D. candidate Brian Lovett are working on ways to genetically modify fungi for a quicker kill. They and other colleagues have inserted venom peptide genes from spiders and scorpions into fungi. They’ve taken their experiment to Africa, where they’re testing its efficacy in aerosol form on malaria-causing mosquitoes in an enclosure specially built to prevent accidental release.\nDespite the scientific excitement around new biopesticide technologies, most remain untested in the marketplace. When it comes to using in agricultural applications, \"in the end, all that matters is what farmers think,\" says King.\nNo researcher sees biopesticides as a perfect replacement for synthetics, for either farming or public health applications.\n\"Chemicals are going to continue playing a role for a long time to come,\" says St. Leger, who sees the recent innovation in biopesticides as creating more options for pest management, not a single solution. No one product will offer a long-term fix, because pests constantly are evolving resistance. These biopesticides are prone to resistance just like any other pesticide, so if they aren’t used appropriately and rotated with other forms of pest control, they — as synthetics before them — could contribute to harder-to-kill pests.","|Length of Control||\n|IRAC Mode of Action||\n5 NACHR Allosteric Activator\nThe Censor single-brood larvicide granule delivers the power of spinosad in a value-based, single brood formulation suited for mosquito control programs and/or habitats where OMRI® Listed status is not a requirement. It is especially effective in open or floodwater habitats and in cold water.\nCensor routinely delivers up to 7 days of control post application and can be applied as a pre-treatment tool. Applied Censor granules will activate and begin controlling mosquito larvae once exposed to standing water. Programs that require high-volume use of single brood, granule larvicides appreciate the value of Censor, backed by the proven performance of spinosad.\nBecause Censor uses spinosad as its active ingredient, the only IRAC Class 5 chemical for public health mosquito control, it is an excellent tool to suppress mosquito populations before they hatch, while helping avoid or fight resistance. Spinosad delivers a unique mode of action that works on all four stages of mosquito larvae. It has been tested to demonstrate efficacy on twenty of the most common vector and nuisance mosquito species, including Aedes, Anopheles and Culex.\nSpinosad, the active ingredient in Censor, was the first mosquito larvicide active ingredient registered under the EPA’s Reduced Risk program. This is due to its reduced risk to human health and non-target organisms when compared to other available alternatives. Spinosad is of low acute and chronic toxicity to a wide range of non-target species, including mammals, birds, terrestrial wildlife and fish.\nSpinosad degrades readily in the soil environment and is non-persistent. Exposure to sunlight accelerates the degradation process.\nBy design, larvicides are intended to be applied to standing water sources to prevent mosquito breeding for a specific period of time. As an active ingredient, spinosad degrades rapidly in water, with exposure to sunlight and presents a relatively low risk to beneficial and non-target insects compared to other broad-spectrum insecticides. Under laboratory conditions, spinosad is toxic to some aquatic invertebrates, primarily upon chronic exposure. Fortunately, the rapid degradation of spinosad in natural aquatic environments prevents the long-term exposure that would be needed for these effects to occur in real world operational conditions.\nWhen applied as indicated on the label for control of mosquito larvae, Censor will not endanger human or animal health. Spinosad is not toxic to mammals. Spinosad is not carcinogenic, not genotoxic and is not a reproductive or developmental toxin.\nCensor is a granular larvicide, and does not pose a bee or pollinator hazard given its formulation type and application method.\nCensor larvicide may be applied by ground, air or as a spot treatment to sites with active or potential mosquito breeding. Please refer to the product label for specific application and habitat guidance.\nFrequently Asked Questions\nWhat is the active ingredient in Censor® larvicide?\nSpinosad. It is a naturally derived active ingredient produced during fermentation by the soil organism, Saccharopolyspora spinosa. The natural metabolites produced during the fermentation process were termed “spinosyns.” Spinosad is the collective term for the two most prominent and most active compounds in the fermentation broth (spinosyn A and spinosyn D). Hence the name “Spinosad.”\nHow does the active ingredient in Censor larvicide control mosquito larvae?\nSpinosad has a novel mode of action; it alters the function of insect nicotinic acetylcholine receptors in a unique manner. Ultimately paralysis sets in upon ingestion and contact and the mosquito larvae don’t recover.\nWhat are the inert ingredients in Censor?\nAll inert components in domestic Natular formulations are included in EPA’s list of Minimal Risk Inert Ingredients. Inerts are non-synthetic (natural) or are synthetic components which do not contribute to mammalian or aquatic toxicity.\nIs Censor suitable for use in organic agriculture?\nNo. Censor is not listed by the Organic Materials Review Institute (OMRI) for use in and around organic agriculture.\nOur fundamental job is to discover and design the most effective mosquito control products of the future, and ensure the performance we can achieve in the lab translates to real-world operational excellence for customers.\nNeed support identifying the right potential products for your program? Our team of control consultants and customer care professionals are here to help you with any inquiry.\nLearn more about how we protect people and the planet, and find out what’s new with Clarke and the industries we serve. View our Blog\nClarke’s Field Science team spearheads the fieldwork and trials conducted to assist with developing, and registering new and existing mosquito control products under EPA and Good Laboratory Practices (GLP) regulations, including the launching and post-launch work of new products with…\nThe use of drones as a product deployment and surveillance tool within mosquito control programs has steadily increased over the past years. Earning an unmanned aerial vehicle (UAV) certification is essential to utilizing and incorporating drones within a mosquito control…\nWhile mosquito-borne diseases threaten human health worldwide, implementing an integrated mosquito management (IMM) system can go a long way toward mitigating those risks. Within the IMM network, the surveillance lever acts explicitly as a predecessor to larval or adult control…\nLet’s talk about what a more sustainable aquatic or mosquito control program looks like for you."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:53b1351b-14eb-451c-b8fa-80ad9f75b67e>","<urn:uuid:c99a4ff6-b867-4d3e-bbd7-38e828c3f20d>"],"error":null}
{"question":"What are the key geological differences between the Clun Forest in the UK and the San Joaquin Valley in California, particularly in terms of their age and formation process?","answer":"The Clun Forest and San Joaquin Valley have very different geological origins and ages. The Clun Forest area was formed around 420 million years ago during the Silurian age, when sandy silts and muds were deposited in a marine basin, eventually consolidating into red and grey shales, mudstones, and siltstones. In contrast, the San Joaquin Valley dates back more than 65 million years to the Mesozoic era and formed as a forearc basin through different processes. While the Clun Forest rocks are relatively weak and create steep-sided, shallow-domed hills, the San Joaquin Valley is a sediment-filled depression bounded by the California Coast Ranges and Sierra Nevadas, initially forming as an inland sea between two mountain ranges before being filled with sediment from the erosion of surrounding mountains.","context":["The catchment area of the Upper Clun is underlain by rocks of Silurian age, which 420 million years ago were deposited as sandy silts and muds, eroded from the surrounding land, into a marine basin. These sediments consolidated to give rise to red and grey shales, mudstones and siltstones (the Clun Forest and Cefn Einion formations respectively). The rocks are relatively weak and the hills have a characteristically steep-sided, shallow-domed shape with few outcrops. We can only see them in rare exposures or more easily (for the Cefn Einion siltstones) in the walls of the older buildings in Newcastle, Clun and the surrounding hills. To see a detailed geological map use the British Geological Survey’s viewer at BGS Map App.\nA free-draining acid brown soil has formed over the siltstones, which would have supported moorland and rough grazing in the pre-Roman era. Today the land is mostly grazed, although arable production is possible in some areas. Alluvial silts and glacial clays are somewhat richer but these are confined to the river valleys and dingles.\nThe area falls into Natural England’s Character Area 98 – ‘Clun and North West Herefordshire Hills’ – of which the key characteristics are described as:\n- Rolling, rounded upland hills divided by narrow valleys widening to the east.\n- Uniform landforms, similar to mid-Wales and contrasting with the rest of the Welsh Marches.\n- Small, wooded, enclosed upper valleys broadening to flat-bottomed intensively-farmed lower valleys.\n- Irregular field patterns in valleys and around settlements contrasting with large rectilinear fields on higher ground.\n- Large, recent conifer plantations, contrasting with remnant, ancient, semi-natural woodland.\nTo the west our area merges into the high moorland of Powys while further east the landscape becomes more domesticated. Hedges reach up to enclose most hill tops although patches of heathland and unenclosed upland grassland remain. On the ridge tops, the hedges are generally low with occasional trees in the more sheltered hollows. The unimproved pasture is sometimes in a patchwork with the smooth greens of improved grassland and occasional patches of arable land. There are panoramic views, which emphasise the area’s plateau origin, as well as long views down narrow, twisting valleys which widen eastwards.\nThe heads of the tributary valleys are narrow and deeply incised with woodland on the steepest slopes. Down-valley they widen to a narrow floodplain, widening at the confluence of the Folly Brook with the Clun, at Newcastle. On the lower slopes, there is a generally regular pattern of large fields cut through by small streams, often with drifts of streamside woodland. Around the farmsteads, and hamlets the field pattern becomes denser and more irregular. In the Clun valley itself, hedges are lower and intermittent and trees more infrequent.\nA changed habitat?\nHedgerows vary in density and management. In some of the upland areas gaps have been renewed or replaced with post and wire. On the floodplains also they are locally intermittent and sparse, with few hedgerow trees.\nThe improvement of pasture and some arable cropping onto higher ground has changed the historic pattern of cultivation in the valley bottoms and rough pasture on the hills. The deciduous woodlands can be adversely affected by grazing, thus reducing their capacity for natural regeneration and their nature-conservation interest.\nAlong stream banks, grazing can be detrimental and pollard management is poor. Riverside alders are suffering from fungal disease and are not being replaced at the rate of die-back. Agricultural land use is likely to be affecting water quality through soil erosion and sediment deposition, together with nutrient losses. The streams are also flashier, running faster in winter and suffering from low flows at times of low rainfall, because water is not retained in the landscape as much as it was – land has been drained, and peaty/boggy areas improved for grazing. See this leaflet for information on managing wetlands for wildlife – Managing wetlands for wildlife.\nSome of the coniferous woodlands are nearing the end of their first cycle. There is an opportunity for improving the landscape and amenity value of the woodland, with an increase in broadleaf woodland where appropriate, particularly along the riparian corridor.","Introduction to the\nGeology of the San Joaquin Valley\nThere are several aspects to the geology of the San Joaquin Valley. The story begins here with the Geologic History. Links to the other chapters of this story are on the menu to the left.\nThe San Joaquin Valley is a sediment-filled depression, called a basin, that is bound to the west by the California Coast Ranges, and to the east by the Sierra Nevadas. It is classified as a forearc basin, which basically means that it is a basin that formed in front of a mountain range.\nThe Valley dates back more than 65 million years ago to the Mesozoic, when subduction was taking place off the coast of California. However, the plate tectonic configuration of western North America changed during the Tertiary, and the ancient trench that once characterized offshore California was transformed into a zone of right-lateral strike-slip motion that we know today as the San Andreas fault. Nonetheless, the Valley still retains many features that characterized it prior to formation of the San Andreas transform.\nBecause the San Joaquin Valley is bound to the west by the California Coast Ranges, which represent a zone of folding and thrusting (i.e., an accretionary prism) associated with the ancient subduction zone, and bound to the east by the Sierra Nevadas, which represent the eroded roots of an ancient volcanic arc that was also associated with the subduction zone, some call the valley a remnant arc-trench gap.\nThe tectonic processes by which this arc-trench gap formed are complicated, as are the events by which the ancient trench became the San Andreas fault. These events are covered in more detail in the plate tectonics section of this website, and the four panels below show a snapshot of the key events.\nWhen the San Joaquin Valley first formed it was an inland sea between two mountain ranges. This configuration remained even after formation of the San Andreas fault (below). However, as the volcanic cover of the Sierras was eroded off, the resulting sediment was dumped into the Valley below. At the same time, The Coast Ranges were also being worn down and dumped into the valley. Thus, the inland sea was filled to create the continental basin we know today.\nThe maps above show the San Joaquin basin during the middle Miocene (left) and the late Miocene (right) when the principle source rocks were deposited. From these source rocks most of the oil in the valley is derived.\nWhen the basin was still an inland sea, diatoms and other plankton thrived in it, and when these organisms died they accumulated on the basin floor to create organic-rich shales that include the Eocene Kreyenhagen, and Miocene Monterey Formations. The integrated effects of heat and time then acted on the buried organic matter within these shales to create oil, and the detritus eroded from the Coast Ranges and the Sierra Nevadas provided reservoir rocks where the oil could accumulate.\nFour maps of the paleogeography of the San Joaquin Valley over geologic time are shown below. Clicking on any map will show a more detailed version.\nGeologic Map of the San Joaquin Valley\nclick on the map to view a more detailed version"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:1ab08801-51f6-4f08-9b2d-b12f3a49f83d>","<urn:uuid:72ba8560-260a-45d7-a83e-8852b1f7f186>"],"error":null}
{"question":"How do work environment characteristics compare between cybersecurity training programs and securities sales agent jobs? 環境の違いを教えてください。","answer":"The work environments have distinct characteristics. For securities sales agents, the environment is highly structured with 85% working indoors in environmentally controlled settings, 91% using email and telephone daily, and 73% having constant contact with others. They typically work more than 40 hours weekly in a highly competitive environment. For cybersecurity training programs, the environment is more flexible and education-focused, with courses taking 3 to 3.5 months to complete, offering various tracks including cybersecurity operations, security infrastructure, and technical support. These programs are designed to be accessible to transitioning military veterans and can be completed online.","context":["Cisco Talos, NetApp and Maryland’s state government announced an initiative to help military veterans in that state transition into civilian positions in cybersecurity. The hope is that it will address twin goals: To help the hundreds of thousands of discharged veterans flowing into the workplace each year; and to fill the growing workforce shortage when it comes to cybersecurity personnel.\n“Within the next few years, [Cisco estimates] we are facing the global reality of nearly 2 million unfilled cybersecurity jobs, which poses a threat to our national security, businesses and the local community,” said Christopher Marshall, a Navy veteran and director of cybersecurity research for Talos, in a blog on Tuesday. “At Cisco, and Talos in particular, we recognize the value veterans bring to the workplace…A major obstacle we face in that endeavor is how to support the military community as they transition to the civilian workforce at a rate of over 200,000 each year.”\nIn Maryland, the private firms are spearheading an effort, with the support of the governor’s office and state government, to provide the training needed to “develop the extrinsic skills to succeed in the cybersecurity workforce while capitalizing on the intrinsic values that hiring veterans brings to the commercial workforce,” according to Marshall.\nMore specifically, that means launching cyber-courses and certifications from various tech companies. The online courses will take three to three-and-a-half months to complete on average, with tracks available in cybersecurity operations; security infrastructure; software engineering; operations and technical support; security management and network security. From there, vets that complete the courses will be put on a certification and hiring path, depending on what skills they gain in the courses.\nCisco said that it will be soon be launching other programs that include a targeted employer matching program, and a proprietary matching engine to map military skills to newly earned certifications, linking veterans to the thousands of jobs posted by any one of Cisco’s 60,000 channel partners across the globe.\nCyberVets USA is partnership that’s been in place since 2016 – it’s made up of the U.S. Departments of Labor, Commerce and Veterans Affairs, various veteran service organizations (VSOs), state colleges and universities, certification group (ISC)2 and private firms including Amazon Web Services, Cisco Talos, Fortinet, NetApp, Palo Alto Networks and others. It offers free cyber-training and certifications to the military and veteran community in Maryland, North Carolina and Virginia, with additional states to be added in the next two years.\n“These men and women exhibit leadership, teamwork, inclusion, integrity, efficiency and (importantly) the ability to acquire technical prowess,” Marshall said. “These are highly desirable traits in any industry, especially one that is predicated on trust and a willingness to always learn and evolve.”\nGovernment Stepping Up\nThe effort is the latest government-backed initiative to address the growing skills gap in cybersecurity. In September, a bipartisan bill in the House of Representatives was introduced aimed at the cybersecurity workforce shortage, dubbed the Cyber Ready Workforce Act (H.R.6791). This would establish a grant program within the Department of Labor to support the creation, implementation and expansion of apprenticeship programs in cybersecurity. These efforts could include career counseling, mentorship, and assistance with transportation, housing and child-care costs. The bill is based on the state of Nevada’s recently introduced cybersecurity apprenticeship program [PDF].\nAlso, in July, the State of Georgia announced that it would invest $50 million to create a Georgia Cyber Innovation and Training Center in Augusta. The idea is that it will become the state’s centerpiece for cybersecurity research and development, and that it will include a statewide effort to train up cyber-workforce resources.\n“Cybersecurity threats will continue to present national security challenges for America in the 21st Century,” said Congressman Dan Donovan (R-N.Y.) when the House bill was introduced. “With these threats and the changing economic and technological landscape, America needs a workforce that can adequately advance our cybersecurity defense priorities.”\nDemand for Cyber-Jobs Deepens\nU.S. employers in the private and public sectors posted an estimated 313,735 job openings for cybersecurity workers between September 2017 and August 2018, according to just-released data from the CyberSeek jobs resource organization. That’s in addition to the 715,000-plus cybersecurity workers currently employed around the country.\n“Increasingly, governments and businesses are working to build better defense against cyberattacks, but training programs are simply not producing enough cybersecurity talent to keep up with demand and to keep data-driven enterprises safe,” said Matthew Sigelman, CEO of Burning Glass Technologies, which created CyberSeek in partnership with the IT industry association CompTIA and a grant from the National Institute of Standards and Technology.\nThe data, released Wednesday, shows that employers have fewer options when it comes to hiring cybersecurity professionals. Across all occupations in the U.S. economy there are currently 5.8 employed workers for every job opening, CyberSeek found; in contrast, the ratio of existing cybersecurity workers to the number of cybersecurity job openings is 2.3. That means employers have fewer opportunities to woo trained cybersecurity workers from other companies; so, providing cyber-training for current workers or attracting new talent to the sector are the main alternatives.\n“Efforts to address the shortage of cybersecurity workers are underway on many fronts, but progress has been frustratingly slow,” said Todd Thibodeaux, president and CEO, CompTIA. “The threats are real and growing, with the potential to impact the livelihood of any organization. Our current cybersecurity workforce is doing what it can to keep us protected. It’s critical for private-sector companies and public sector agencies to take the actions necessary to bring more people into the cybersecurity workforce, and to equip them with the appropriate education, training and certifications.”","Summary Report for:\n41-3031.01 - Sales Agents, Securities and Commodities\nBuy and sell securities in investment and trading firms and develop and implement financial plans for individuals, businesses, and organizations.\nSample of reported job titles: Account Executive, Financial Consultant, Financial Representative, Investment Advisor, Investment Consultant, Investment Executive, Investment Representative, Investment Specialist, Registered Representative, Stock Broker\nTasks | Technology Skills | Tools Used | Knowledge | Skills | Abilities | Work Activities | Detailed Work Activities | Work Context | Job Zone | Education | Credentials | Interests | Work Styles | Work Values | Related Occupations | Wages & Employment | Job Openings | Additional Information\n- Keep accurate records of transactions.\n- Complete sales order tickets and submit for processing of client-requested transactions.\n- Interview clients to determine clients' assets, liabilities, cash flow, insurance coverage, tax status, or financial objectives.\n- Discuss financial options with clients and keep them informed about transactions.\n- Develop financial plans, based on analysis of clients' financial status.\n- Review all securities transactions to ensure accuracy of information and conformance to governing agency regulations.\n- Inform and advise concerned parties regarding fluctuations or securities transactions affecting plans or accounts.\n- Review financial periodicals, stock and bond reports, business publications, or other material to identify potential investments for clients or to keep abreast of trends affecting market conditions.\n- Offer advice on the purchase or sale of particular securities.\n- Contact prospective customers to determine customer needs, present information, or explain available services.\n- Analyze market conditions to determine optimum times to execute securities transactions.\n- Explain stock market terms or trading practices to clients.\n- Prepare documents needed to implement plans selected by clients.\n- Calculate costs for billings or commissions.\n- Prepare financial reports to monitor client or corporate finances.\n- Supply the latest price quotes on any security, as well as information on the activities or financial positions of the corporations issuing these securities.\n- Identify potential clients, using advertising campaigns, mailing lists, or personal contacts.\n- Relay buy or sell orders to securities exchanges or to firm trading departments.\n- Read corporate reports and calculate ratios to determine best prospects for profit on stock purchases.\n- Compliance software — Regulatory agency compliance software\n- Customer relationship management CRM software — CSI Complex Systems ClientTrade\n- Data base user interface and query software — Data entry software ; FundCount Web\n- Desktop communications software — ADP/Vantra VOLTS; Imagine Software Imagine Trading System\n- Electronic mail software — Email software\n- Financial analysis software — AnalyzerXL; Bloomberg Professional; Thomson Reuters Tradeweb; Trading Blox (see all 12 examples)\n- Graphics or photo imaging software — Graphics software\n- Internet browser software — Web browser software\n- Office suite software — Microsoft Office\n- Presentation software — Microsoft PowerPoint\n- Spreadsheet software — Microsoft Excel\n- Word processing software — Microsoft Word\nHot Technology — a technology requirement frequently included in employer job postings.\n- Customer and Personal Service — Knowledge of principles and processes for providing customer and personal services. This includes customer needs assessment, meeting quality standards for services, and evaluation of customer satisfaction.\n- Economics and Accounting — Knowledge of economic and accounting principles and practices, the financial markets, banking and the analysis and reporting of financial data.\n- English Language — Knowledge of the structure and content of the English language including the meaning and spelling of words, rules of composition, and grammar.\n- Sales and Marketing — Knowledge of principles and methods for showing, promoting, and selling products or services. This includes marketing strategy and tactics, product demonstration, sales techniques, and sales control systems.\n- Mathematics — Knowledge of arithmetic, algebra, geometry, calculus, statistics, and their applications.\n- Computers and Electronics — Knowledge of circuit boards, processors, chips, electronic equipment, and computer hardware and software, including applications and programming.\n- Administration and Management — Knowledge of business and management principles involved in strategic planning, resource allocation, human resources modeling, leadership technique, production methods, and coordination of people and resources.\n- Law and Government — Knowledge of laws, legal codes, court procedures, precedents, government regulations, executive orders, agency rules, and the democratic political process.\n- Psychology — Knowledge of human behavior and performance; individual differences in ability, personality, and interests; learning and motivation; psychological research methods; and the assessment and treatment of behavioral and affective disorders.\n- Active Listening — Giving full attention to what other people are saying, taking time to understand the points being made, asking questions as appropriate, and not interrupting at inappropriate times.\n- Critical Thinking — Using logic and reasoning to identify the strengths and weaknesses of alternative solutions, conclusions or approaches to problems.\n- Persuasion — Persuading others to change their minds or behavior.\n- Reading Comprehension — Understanding written sentences and paragraphs in work related documents.\n- Speaking — Talking to others to convey information effectively.\n- Judgment and Decision Making — Considering the relative costs and benefits of potential actions to choose the most appropriate one.\n- Active Learning — Understanding the implications of new information for both current and future problem-solving and decision-making.\n- Writing — Communicating effectively in writing as appropriate for the needs of the audience.\n- Complex Problem Solving — Identifying complex problems and reviewing related information to develop and evaluate options and implement solutions.\n- Service Orientation — Actively looking for ways to help people.\n- Social Perceptiveness — Being aware of others' reactions and understanding why they react as they do.\n- Systems Analysis — Determining how a system should work and how changes in conditions, operations, and the environment will affect outcomes.\n- Time Management — Managing one's own time and the time of others.\n- Coordination — Adjusting actions in relation to others' actions.\n- Mathematics — Using mathematics to solve problems.\n- Monitoring — Monitoring/Assessing performance of yourself, other individuals, or organizations to make improvements or take corrective action.\n- Negotiation — Bringing others together and trying to reconcile differences.\n- Systems Evaluation — Identifying measures or indicators of system performance and the actions needed to improve or correct performance, relative to the goals of the system.\n- Oral Comprehension — The ability to listen to and understand information and ideas presented through spoken words and sentences.\n- Oral Expression — The ability to communicate information and ideas in speaking so others will understand.\n- Speech Clarity — The ability to speak clearly so others can understand you.\n- Written Comprehension — The ability to read and understand information and ideas presented in writing.\n- Deductive Reasoning — The ability to apply general rules to specific problems to produce answers that make sense.\n- Inductive Reasoning — The ability to combine pieces of information to form general rules or conclusions (includes finding a relationship among seemingly unrelated events).\n- Problem Sensitivity — The ability to tell when something is wrong or is likely to go wrong. It does not involve solving the problem, only recognizing there is a problem.\n- Speech Recognition — The ability to identify and understand the speech of another person.\n- Written Expression — The ability to communicate information and ideas in writing so others will understand.\n- Near Vision — The ability to see details at close range (within a few feet of the observer).\n- Category Flexibility — The ability to generate or use different sets of rules for combining or grouping things in different ways.\n- Fluency of Ideas — The ability to come up with a number of ideas about a topic (the number of ideas is important, not their quality, correctness, or creativity).\n- Information Ordering — The ability to arrange things or actions in a certain order or pattern according to a specific rule or set of rules (e.g., patterns of numbers, letters, words, pictures, mathematical operations).\n- Mathematical Reasoning — The ability to choose the right mathematical methods or formulas to solve a problem.\n- Number Facility — The ability to add, subtract, multiply, or divide quickly and correctly.\n- Flexibility of Closure — The ability to identify or detect a known pattern (a figure, object, word, or sound) that is hidden in other distracting material.\n- Originality — The ability to come up with unusual or clever ideas about a given topic or situation, or to develop creative ways to solve a problem.\n- Communicating with Persons Outside Organization — Communicating with people outside the organization, representing the organization to customers, the public, government, and other external sources. This information can be exchanged in person, in writing, or by telephone or e-mail.\n- Establishing and Maintaining Interpersonal Relationships — Developing constructive and cooperative working relationships with others, and maintaining them over time.\n- Making Decisions and Solving Problems — Analyzing information and evaluating results to choose the best solution and solve problems.\n- Analyzing Data or Information — Identifying the underlying principles, reasons, or facts of information by breaking down information or data into separate parts.\n- Evaluating Information to Determine Compliance with Standards — Using relevant information and individual judgment to determine whether events or processes comply with laws, regulations, or standards.\n- Getting Information — Observing, receiving, and otherwise obtaining information from all relevant sources.\n- Interacting With Computers — Using computers and computer systems (including hardware and software) to program, write software, set up functions, enter data, or process information.\n- Selling or Influencing Others — Convincing others to buy merchandise/goods or to otherwise change their minds or actions.\n- Organizing, Planning, and Prioritizing Work — Developing specific goals and plans to prioritize, organize, and accomplish your work.\n- Updating and Using Relevant Knowledge — Keeping up-to-date technically and applying new knowledge to your job.\n- Processing Information — Compiling, coding, categorizing, calculating, tabulating, auditing, or verifying information or data.\n- Interpreting the Meaning of Information for Others — Translating or explaining what information means and how it can be used.\n- Performing for or Working Directly with the Public — Performing for people or dealing directly with the public. This includes serving customers in restaurants and stores, and receiving clients or guests.\n- Judging the Qualities of Things, Services, or People — Assessing the value, importance, or quality of things or people.\n- Communicating with Supervisors, Peers, or Subordinates — Providing information to supervisors, co-workers, and subordinates by telephone, in written form, e-mail, or in person.\n- Identifying Objects, Actions, and Events — Identifying information by categorizing, estimating, recognizing differences or similarities, and detecting changes in circumstances or events.\n- Developing Objectives and Strategies — Establishing long-range objectives and specifying the strategies and actions to achieve them.\n- Documenting/Recording Information — Entering, transcribing, recording, storing, or maintaining information in written or electronic/magnetic form.\n- Scheduling Work and Activities — Scheduling events, programs, and activities, as well as the work of others.\n- Resolving Conflicts and Negotiating with Others — Handling complaints, settling disputes, and resolving grievances and conflicts, or otherwise negotiating with others.\n- Performing Administrative Activities — Performing day-to-day administrative tasks such as maintaining information files and processing paperwork.\nDetailed Work Activities\n- Maintain records of sales or other business transactions.\n- Prepare sales or other contracts.\n- Process sales or other transactions.\n- Gather customer or product information to determine customer needs.\n- Explain financial information to customers.\n- Customize financial products or services to meet customer needs.\n- Review accuracy of sales or other transactions.\n- Identify investment opportunities or strategies.\n- Monitor market conditions or trends.\n- Contact current or potential customers to promote products or services.\n- Explain technical product or service information to customers.\n- Analyze market conditions or trends.\n- Calculate costs of goods or services.\n- Prepare financial documents, reports, or budgets.\n- Estimate costs or terms of sales.\n- Identify potential customers.\n- Electronic Mail — 91% responded “Every day.”\n- Telephone — 91% responded “Every day.”\n- Contact With Others — 73% responded “Constant contact with others.”\n- Face-to-Face Discussions — 65% responded “Every day.”\n- Frequency of Decision Making — 52% responded “Every day.”\n- Indoors, Environmentally Controlled — 85% responded “Every day.”\n- Freedom to Make Decisions — 53% responded “A lot of freedom.”\n- Structured versus Unstructured Work — 57% responded “A lot of freedom.”\n- Impact of Decisions on Co-workers or Company Results — 52% responded “Important results.”\n- Deal With External Customers — 70% responded “Extremely important.”\n- Importance of Being Exact or Accurate — 50% responded “Extremely important.”\n- Spend Time Sitting — 44% responded “More than half the time.”\n- Level of Competition — 46% responded “Highly competitive.”\n- Time Pressure — 31% responded “Every day.”\n- Work With Work Group or Team — 35% responded “Extremely important.”\n- Duration of Typical Work Week — 40% responded “More than 40 hours.”\n- Letters and Memos — 55% responded “Once a week or more but not every day.”\n- Consequence of Error — 33% responded “Very serious.”\n|Title||Job Zone Four: Considerable Preparation Needed|\n|Education||Most of these occupations require a four-year bachelor's degree, but some do not.|\n|Related Experience||A considerable amount of work-related skill, knowledge, or experience is needed for these occupations. For example, an accountant must complete four years of college and work for several years in accounting to be considered qualified.|\n|Job Training||Employees in these occupations usually need several years of work-related experience, on-the-job training, and/or vocational training.|\n|Job Zone Examples||Many of these occupations involve coordinating, supervising, managing, or training others. Examples include accountants, sales managers, database administrators, graphic designers, chemists, art directors, and cost estimators.|\n|SVP Range||(7.0 to < 8.0)|\nInterest code: EC\n- Enterprising — Enterprising occupations frequently involve starting up and carrying out projects. These occupations can involve leading people and making many decisions. Sometimes they require risk taking and often deal with business.\n- Conventional — Conventional occupations frequently involve following set procedures and routines. These occupations can include working with data and details more than with ideas. Usually there is a clear line of authority to follow.\n- Integrity — Job requires being honest and ethical.\n- Stress Tolerance — Job requires accepting criticism and dealing calmly and effectively with high stress situations.\n- Initiative — Job requires a willingness to take on responsibilities and challenges.\n- Dependability — Job requires being reliable, responsible, and dependable, and fulfilling obligations.\n- Persistence — Job requires persistence in the face of obstacles.\n- Analytical Thinking — Job requires analyzing information and using logic to address work-related issues and problems.\n- Achievement/Effort — Job requires establishing and maintaining personally challenging achievement goals and exerting effort toward mastering tasks.\n- Attention to Detail — Job requires being careful about detail and thorough in completing work tasks.\n- Concern for Others — Job requires being sensitive to others' needs and feelings and being understanding and helpful on the job.\n- Self Control — Job requires maintaining composure, keeping emotions in check, controlling anger, and avoiding aggressive behavior, even in very difficult situations.\n- Independence — Job requires developing one's own ways of doing things, guiding oneself with little or no supervision, and depending on oneself to get things done.\n- Adaptability/Flexibility — Job requires being open to change (positive or negative) and to considerable variety in the workplace.\n- Cooperation — Job requires being pleasant with others on the job and displaying a good-natured, cooperative attitude.\n- Leadership — Job requires a willingness to lead, take charge, and offer opinions and direction.\n- Social Orientation — Job requires preferring to work with others rather than alone, and being personally connected with others on the job.\n- Innovation — Job requires creativity and alternative thinking to develop new ideas for and answers to work-related problems.\n- Achievement — Occupations that satisfy this work value are results oriented and allow employees to use their strongest abilities, giving them a feeling of accomplishment. Corresponding needs are Ability Utilization and Achievement.\n- Working Conditions — Occupations that satisfy this work value offer job security and good working conditions. Corresponding needs are Activity, Compensation, Independence, Security, Variety and Working Conditions.\n- Independence — Occupations that satisfy this work value allow employees to work on their own and make decisions. Corresponding needs are Creativity, Responsibility and Autonomy.\nWages & Employment Trends\nMedian wages data collected from Securities, Commodities, and Financial Services Sales Agents.\nEmployment data collected from Securities, Commodities, and Financial Services Sales Agents.\nIndustry data collected from Securities, Commodities, and Financial Services Sales Agents.\n|Median wages (2017)||$30.66 hourly, $63,780 annual|\n|Employment (2016)||376,000 employees|\n|Projected growth (2016-2026)||Average (5% to 9%)|\n|Projected job openings (2016-2026)||38,000|\n|Top industries (2016)|\nSource: Bureau of Labor Statistics 2017 wage data and 2016-2026 employment projections . \"Projected growth\" represents the estimated change in total employment over the projections period (2016-2026). \"Projected job openings\" represent openings due to growth and replacement.\nJob Openings on the Web\nSources of Additional Information\nDisclaimer: Sources are listed to provide additional information on related jobs, specialties, and/or industries. Links to non-DOL Internet sites are provided for your convenience and do not constitute an endorsement."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:68c1b428-dbef-45c1-8238-721b4b60451d>","<urn:uuid:2a530f8d-ec01-4612-a0de-25ffb1780210>"],"error":null}
{"question":"What are the main detection methods used to identify Sudan dyes in food products, and how do they compare in terms of effectiveness and practicality?","answer":"Several detection methods are available for identifying Sudan dyes in food products. These include HPLC with UV diode array detection (HPLC-DAD), enzyme-linked immunosorbent assay (ELISA), Raman spectroscopy, thin layer chromatography (TLC), near infrared (NIR) spectroscopy, and two-dimensional high-pressure liquid chromatography with solid phase extraction (2D-HPLC-SPE). While most traditional techniques are laborious and require extensive sample preparation, newer methods like SERS (Surface-enhanced Raman spectroscopy) provide rapid and reliable detection. TLC can effectively discriminate between natural colorants and Sudan dyes with a detection limit of 5mg/kg. NIR spectroscopy achieved better calibration results and lower detection limits than Raman spectroscopy for Sudan I in chili powders, while being rapid and requiring minimal sample preparation. The 2D-HPLC-SPE method with mass spectrometry provides good results for detecting Sudan I, II, III and IV in chili oil and curry paste.","context":["Making foods look fresh, edible and acceptable to consumers is common sense within the food industry. Who wants to buy pallid bakery products, wrinkled veggies or shriveled meat, for example? However, in using certain additives such as prohibited synthetic dyes, some unscrupulous producers take extreme measures to enhance their product, endangering consumer health in the process.\nSudan Dyes and Food Fraud\nSudan dyes are widely used in commercial manufacturing to impart a rich red, red-orange or yellow-orange color to plastics and textiles. They are cheap and easy to obtain; for this reason, they have also been used for enhancing presentation of spices such as chili powder, paprika and curry pastes. However, large amounts (mg per kg) are required for effect. These fat-soluble Azo dyes, more commonly known in laboratory staining protocols or as constituents of hair dye, are Group 3 carcinogens. They also have potential for genotoxic activity through formation of mutation-inducing DNA adducts, as well as allergenic activity. For these reasons, many countries ban their use in foods. Starting in 2003, the European Union took steps to eliminate foodstuffs containing Sudan dyes from entering the marketplace (see EU Regulations listed below).\nFood Fraud Detection\nThere are various methods available to food safety laboratories for detecting Sudan dye adulteration in foodstuffs, ranging from HPLC plus UV diode array detection (HPLC-DAD) to enzyme-linked immunosorbent assay (ELISA). Most of these techniques are laborious and time-consuming, requiring extensive sample preparation and analysis however, there are new methods in development.\nDiAnibal et al. (2012) reported using Raman spectroscopy for detecting Sudan I dye in adulterated paprika powder1. The researchers compared three types of Raman spectroscopy, including fourier-transformed (FT)-Raman using the Nicolet 5700 FT-IR (Thermo Scientific), analyzing prepared samples on aluminium foils. Their results showed that surface-enhanced Raman spectroscopy (SERS) gave the best results for rapid and reliable detection.\nIn 2013, Dar et al. explored thin layer chromatography (TLC) to detect Sudan III and IV in red chili powder, which they spiked with commercial dye standards2. After testing various solvents, they used acetonitrile to extract the samples prior to TLC. The researchers reported that this method discriminated effectively between natural colorants such as carotenoids and the spiked Sudan dye standards, with an effective visual limit of detection found at 5mg/kg.\nMeanwhile, Haughey et al. (2015) examined vibrational spectroscopy techniques, near infrared (NIR) spectroscopy and Raman spectroscopy to detect Sudan dyes in foodstuffs3. In their feasibility study, the Antaris II FT-NIR (Thermo Scientific) achieved lower limits of detection (LOD) and better calibration results for chili powders spiked with Sudan I standards analyzed than Raman spectroscopy. The authors did report however, that both techniques were rapid, easy to use and involved minimal or no sample preparation.\nAnother practical and efficient method uses two-dimensional high-pressure liquid chromatography in conjunction with solid phase extraction (2D-HPLC-SPE) prior to mass spectrometry4. Once the extraction is completed offline, separation and MS detection are automated using the Dionex Ultimate 3000 system controlled with Chromeleon software (both Thermo Scientific). Using a UV spectral standards library for reference, this method gives good results detecting Sudan I, II, II and IV in chili oil and curry paste.\nDynamic Detection for Evolution of Fraud\nIt is worth noting, however, that as detection methods for one fraudulent additive improve, unscrupulous producers will shift to using other adulterants, as shown in work by Ruf et al.5 In a 2012 paper, the researchers reported detecting Azo dye, Basic Red 46, masquerading as a Sudan dye in sumac spice.\nUsing standard methods for detection (HPLC-DAD), the team could not identify any Sudan dye content but were perplexed by the intense red staining retained in the fractionation column. It was only when they turned to mass spectrometry that the illegal dye was spotted. Ruf et al. analyzed their spice sample first by liquid chromatography coupled with tandem mass spectrometer (LC-MS/MS) TSQ Quantum Discovery, and then by high resolution LC-MS using an Orbitrap Exactive HCD mass spectrometer (both Thermo Scientific).\nOnce they collected spectral data, the researchers then turned to searching online databases for information on the structure isolated from the spice. Finally, they confirmed its identity by comparing spectral data with LC-MS analysis of a commercial hair product containing Basic Red 46.\nIn 2003, the European Commission (EC) released an emergency measures decision (2003/460/EC) regarding Sudan dye adulteration of hot chili and chili products, requiring producers to test raw materials, releasing another measure in 2005 due to high levels of adulteration and food fraud involving this dye group. The importance of the measures in maintaining food chain safety became apparent during 2005, when the United Kingdom recalled Worcester Sauce contaminated with Sudan I, tracing the source back to a 2003 shipment of chili powder.\nThe EC Decision 2005/402/EC implementing emergency measures regarding chili, chili products, curcuma and palm, requiring monitoring and reporting by member states. With the success of this measure and reduction in reported cases, European Commission Regulation (EC) No 669/2009 relaxed the monitoring requirements in 2009.\nVisit the Thermo Scientific Food and Beverage Community for further discussion on food safety\nLearn more about Raman spectroscopy in an on-demand webinar\n1. DiAnibal, C.V. et al. (2012) “Surface Enhanced Raman Spectroscopy (SERS) and multivariate analysis as a screening tool for detecting Sudan I dye in culinary spices“, Spectrochimica Acta Part A 87 (pp. 135– 141)\n2. Dar, M.M. et al. (2013) “Detection of Sudan Dyes in Red Chilli Powder by Thin Layer Chromatography“, 2:586 doi:10.4172/ scientificreports.586\n3. Haughey, S.A. “The feasibility of using near infrared and Raman spectroscopic techniques to detect fraudulent adulteration of chili powders with Sudan dye“, Food Control 48 (2015) 75e83\n4. Tukkeeree, S. and Rohrer, J. (2012) “Determination of Sudan Dyes I–IV in Curry Paste”, Thermo Scientific App Note http://www.dionex.com/en-us/webdocs/113854-AN1023-LC-2D-Sudan-Dyes-Curry-Paste-AN70212_E.pdf\n5. Ruf, J. et al. (2012) “Discovery and structural elucidation of the illegal azo dye Basic Red 46 in sumac spice“, Food Additives and Contaminants Part A 29(pp.897-907)\n- European Union Rapid Alerts System for Food and Feed (EU RASFF). (2012). Annual Report, 21\n- EUROPEAN COMMISSION DECISION 2003/460/EC on emergency measures regarding hot chili and chili products. Official Journal of the European Union, L154/114.\n- EUROPEAN COMMISSION DECISION 2005/402/EC on emergency measures regarding chili, chili products, curcuma and palm. Official Journal of the European Union, L135/34.\n- EUROPEAN COMMISSION REGULATION (EC) No 669/2009 implementing regulation (EC) No 882/2004 of the European Parliament and of the Council as regards the increased level of official controls on imports of certain feed and food of nonanimal origin and amending Decision 2006/504/EC. Official Journal of the European Union (EC 669/2009) L194/11."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:dbbb2d24-18b0-4fce-acc1-c9e06bcc226e>"],"error":null}
{"question":"I'm interested in learning about early aviation careers. What was Sir Keith Williamson's first role in the RAF?","answer":"Sir Keith Williamson joined the RAF as an apprentice and qualified as a radio fitter at Cranwell in Lincolnshire in 1948, before being selected for a flight cadetship which he passed in 1950.","context":["Sir Keith Williamson (OB 1938-41)\nWith thanks to Geoff Crome (OB 1939-47) who knew Keith from the age of 3/4.\nSir Keith was at Bancrofts School from 1938-41 and went on to have a distinguished career in the RAF.\nHe died age 90 on 2nd May 2018.\nObituary (Courtesy of The Times Newspaper)\nThe Falklands war was widely seen as a victory for the Royal Navy, but as commander-in-chief of the Royal Air Force during the conflict, Sir Keith Williamson felt that his men did not get the credit they deserved.\n“At first the navy wanted it all to themselves,” he recalled of the conflict, “which was unfortunate.” However, Strike Command landed the first blows with the bombing of the runway at Port Stanley airport by Vulcans flying from Ascension Island. It was the RAF’s longest operational raid.\n“We were showing them that we had the reach to strike Argentina,” he said. “The fact that it would have been politically unacceptable had nothing to do with it.” The operation led to the withdrawal of Argentine fighter defences from the islands to the mainland.\nHarriers later supplemented the Fleet Air Arm but, Williamson recalled: “The navy were very sensitive about us getting into the limelight at all, and one has to accept that in the middle of a war one doesn’t want to start raising interdepartmental rivalries. The major lesson from the Falklands war was the same as the lesson from the Korean War: that air power was decisive.”\nWith the Falklands retaken, Williamson, who joined the RAF as an apprentice, became the first of those who had entered by that route to be chief of the air staff. In 1985, after three years as CAS, he was expected to become chief of the defence staff (CDS) after Field Marshal Lord Bramall.\nMargaret Thatcher, however, chose that moment to break with “Buggins’ turn”, under which the three services held the appointment in rotation. The baton was handed to the first sea lord. For many years press and politicians had said the system should be scrapped; it was Williamson’s misfortune to be next in line when the government acted. It was suggested in some quarters that he had never looked right for the job. Naturally combative, he had won his spurs as an operational commander, not as a strategic thinker or “Whitehall warrior”.\nHe was fiercely loyal to the RAF and made no secret of his misgivings about the centralisation of power within the Ministry of Defence. With an energetic new defence secretary, Michael Heseltine, intent on building up the CDS at the expense of the separate chiefs of staff, Williamson’s temperament and left-of-centre views were out of fashion.\nHe was scathing about Heseltine’s reorganisation. “It had little to do with defence,” he said. “It had much more to do with Mr Michael Heseltine’s personal career, and I found that deeply offensive — and I still do.”\nKeith Alec Williamson was born in Leytonstone, east London, to Percy and Gertrude. His imagination was fired by the dogfights over Essex during the Battle of Britain. “I can’t remember a time after the declaration of war when I didn’t want to be a pilot in the RAF.”\nHe went to Bancroft’s School, Woodford Green, but when his school was bombed he was evacuated to Market Harborough, in Leicestershire, where he attended grammar school. He had joined the Air Training Corps and, just before his 17th birthday, he enlisted as an RAF apprentice. After qualifying as a radio fitter at Cranwell in Lincolnshire in 1948, he was selected for a flight cadetship, passing out in 1950. “I’m not sure I was ever going to be a good radio fitter,” he recalled wryly. He was posted to RAF Fassberg in Germany. “It was a particularly exciting time. We’d just recovered from the Berlin Airlift, the Korean War had started and the RAF was expanding.” They were flying five or six sorties a day. “The whole of Germany was our low-flying area.”\nAt one point a Lincoln was shot down on a border patrol, and he and his comrades were ordered to fly around the area, ready to engage. “To a young man, that just heightened the exhilaration.”\nHe next volunteered for service in Korea with the Royal Australian Air Force, for whom he flew a number of sorties in Meteors, which he described as “a terrible aeroplane . . . I didn’t feel I was hastening the end of the war.”\nIn 1953 he married Patricia Anne Watts, the daughter of a wing commander. They had two sons, Timothy and David, who lead private lives, and twin daughters, Anne and Susie. Susie died in 2015.\nWilliamson almost left the service, along with many of his colleagues, after the defence white paper of 1957, which ended national service and led to the disbanding of many RAF squadrons. Duncan Sandys, author of the white paper, “did more damage to this country’s ability to defend itself than any single person since Napoleon,” he said.\nWilliamson might have departed, but he was offered a flight commander’s post and thereafter rarely looked back. He commanded his first squadron at Leuchars in Fife (1966-68), flying supersonic Lightnings. He was awarded the Air Force Cross in 1968.\nIn 1978 he was put in charge of RAF Support Command at Brampton in Cambridgeshire. He sanctioned a TV documentary series about pilot training, Fighter Pilot, from which the RAF did not emerge with great credit. Yet as he later put it: “If you look in the mirror and you don’t like the image, you don’t blame the mirror.”\nIn 1980 he moved to Strike Command, and was in charge of the RAF during the Falklands war. Postwar concerns dominated the first half of his three-year reign as CAS. The hitherto neglected islands suddenly became a defence priority. Williamson took over in autumn 1982 and, after a visit to the islands, improved radar defences. He also insisted on basing Phantoms in the South Atlantic and persuaded his political masters to accept the need for a new airfield at Mount Pleasant.\nHe argued the case for the new European fighter aircraft and led the RAF challenge to Heseltine’s centralisation of policymaking. A number of his contemporaries felt he fought this dogfight too ferociously for the good of the service, as well for his own. In misjudging the strength of the tide against him, Williamson exposed the flaw which, in the opinion of his critics, made him unsuitable for the last great honour that was supposed to be awaiting him.\nHe retired to the country in 1985, to play golf, sail and watch rugby, on which he was eloquent and informed.\nIf there is a single message to be taken from his career, it is that we neglect our air capability at our peril. “Command of the air prevents defeat,” Williamson said, “as in every war since aeroplanes first appeared over the battlefields of Flanders. I believe that, bang for buck, you get far more value for money investing in the air.”\nSir Keith Williamson, GCB, AFC, Marshal of the RAF, was born on February 25, 1928. He died of undisclosed causes on May 2, 2018, aged 90"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:df41b6f2-4158-4fed-9b24-3e8502c3753d>"],"error":null}
{"question":"Between Coyotes and Arctic Wolves, which species shows more diversity in their hunting and dietary patterns throughout the year?","answer":"Coyotes demonstrate greater dietary diversity as opportunistic, generalist predators. They consume a wide variety of foods ranging from fruits and insects to large ungulates and livestock, adapting their diet based on availability and even exploiting human-provided food sources in urban areas. Arctic Wolves, while also predators, have a more specialized carnivorous diet primarily focused on musk oxen and Arctic hares, with some predation on caribou, seals, Arctic fox, and birds. They hunt in packs and are more dependent on large prey animals, particularly following caribou migrations in winter.","context":["|Scientific Name:||Canis latrans|\n|Species Authority:||Say, 1823|\n|Red List Category & Criteria:||Least Concern ver 3.1|\n|Assessor/s:||Gese, E.M., Bekoff, M., Andelt,W., Carbyn, L. & Knowlton, F.|\n|Reviewer/s:||Sillero-Zubiri, C. & Hoffmann, M. (Canid Red List Authority)|\nThe Coyote has a wide distribution throughout North America, Mexico and into Central America. They are abundant throughout their range and are increasing in distribution as humans continue to modify the landscape. The species is very versatile, especially in their ability to exploit human modified environments.\n|Range Description:||Coyotes were believed to have been restricted to the south-west and plains regions of the U.S. and Canada, and northern and central Mexico, prior to European settlement (Moore and Parker 1992). During the 19th century, coyotes are thought to have expanded north and west. With land conversion and removal of wolves after 1900, coyotes expanded into all of the U.S. and Mexico, southward into Central America, and northward into most of Canada and Alaska (Moore and Parker 1992).\nCoyotes continue to expand their distribution and occupy most areas between 8°N (Panama) and 70°N (northern Alaska). They are found throughout the continental United States and Alaska, almost all of Canada (except the far north-eastern regions), south through Mexico and into Central America (Bekoff 1982; Reid 1997; Bekoff and Gese 2003).\nNative:Belize; Canada; Costa Rica; El Salvador; Guatemala; Honduras; Mexico; Nicaragua; Panama; United States\n|Range Map:||Click here to open the map viewer and explore range.|\nCoyotes are abundant throughout their range and are increasing in distribution as humans continue to modify the landscape. Elimination of wolves may also have assisted Coyote expansion. Coyote density varies geographically with food and climate, and seasonally due to mortality and changes in pack structure and food abundance. Local control temporarily reduces numbers on a short-term basis, but Coyote populations generally are stable in most areas.\nCoyote densities in different geographic areas and seasons vary from 0.01–0.09 coyotes/km² in the winter in the Yukon (O'Donoghue et al. 1997) to 0.9/km² in the fall and 2.3/km² during the summer (post-whelping) in Texas (Knowlton 1972, Andelt 1985). Density in different geographic areas and seasons are listed in Sillero-Zubiri (2004).\n|Habitat and Ecology:||\nCoyotes utilize almost all available habitats including prairie, forest, desert, mountain and tropical ecosystems. The ability of coyotes to exploit human resources allows them to occupy urban areas. Water availability may limit Coyote distribution in some desert environments.\nCoyotes are opportunistic, generalist predators that eat a variety of food items, typically consuming items in relation to changes in availability. Coyotes eat foods ranging from fruit and insects to large ungulates and livestock. Livestock and wild ungulates may often be represented in coyote stomachs and scats as carrion, but predation on large ungulates (native and domestic) does occur (Andelt 1987). Predation by Coyotes on neonates of native ungulates can be high during fawning (Andelt 1987). Coyotes in suburban areas are adept at exploiting human-made food resources and will readily consume dog food or other human-related items.\nThere are no current threats to Coyote populations throughout their range. Local reductions are temporary and their range has been expanding. Conservation measures have not been needed to maintain viable populations. Coyotes adapt to human environs and occupy most habitats, including urban areas. Hybridization with dogs may be a threat near urban areas. Genetic contamination between dogs, Coyotes, and Grey Wolves may be occurring in north-eastern U.S. Hybridization between Coyotes and Red Wolves is problematic for Red Wolf recovery in south-eastern U.S.\nCoyote fur is still sought by trappers throughout its range, with harvest levels depending upon fur prices, local and state regulations, and traditional uses and practices. Many states and provinces consider Coyotes a furbearing species with varying regulations on method of take, bag limit, and seasons.\nThe species is not included on the CITES Appendices, and there is no legal protection of the species. Restrictions on harvest and method of harvest depend upon state or provincial regulations.\nThe Coyote occurs in almost all protected areas across its range.\nOccurrence in captivity\nOver 2,000 Coyotes occur in captivity in zoos, wildlife centres, etc., throughout their range. They readily reproduce in captivity and survival is high.\nGaps in knowledge\nSeveral gaps in knowledge still remain: coyote reproductive physiology and possible modes of fertility control; selective management of problem animals; effects of control; genetic differentiation from other canids (particularly the red wolf); development of non-lethal depredation techniques; interactions of coyotes and other predators; coyote-prey interactions; human-coyote interactions and conflicts at the urban interface; factors influencing prey selection; communication; adaptations in urban and rural environments; and interactions with threatened and endangered species.\n|Citation:||Gese, E.M., Bekoff, M., Andelt,W., Carbyn, L. & Knowlton, F. 2008. Canis latrans. In: IUCN 2013. IUCN Red List of Threatened Species. Version 2013.2. <www.iucnredlist.org>. Downloaded on 08 March 2014.|\n|Feedback:||If you see any errors or have any questions or suggestions on what is shown on this page, please fill in the feedback form so that we can correct or extend the information provided|","The Arctic Wolf is well known as the white wolf that is not afraid of humans. It is an amazing animal with adaptations that enable it to survive in the extreme weather of the Arctic regions. We have gathered a complete set of Arctic Wolf Facts For Kids that will help you in learning All About Arctic Wolves. You are going to learn about its scientific name, classification, appearance, physical description, lifespan, eyes, species, diet, hunting, habitat, range, climate, adaptations, behavior, sense of smell, wolf pack, life cycle, reproduction, baby, endangerment, population, predator, role in the ecosystem and many other interesting and fun facts about Arctic Wolves.\nArctic Wolf Facts For Kids\nWhat Is Arctic Wolf\n- Arctic wolf is a medium-sized subspecies of grey wolf indigenous to the Arctic regions of North America and Greenland.\n- It is also known as the polar wolf or white wolf.\n- Arctic wolf is distinguishable from the northwestern wolf due to its smaller size, whiter fur, larger carnassial teeth, and narrow cranium (braincase).\n- Like their southern relatives, Arctic wolves have an isolated habitat range and are not threatened due to habitat destruction or hunting.\n- The skull size of the Arctic wolf progressively reduced since 1930, which is possibly the result of the wolf-dog hybridization.\nArctic Wolf Scientific Name\n- The scientific name of the Arctic wolf is Canis lupus arctos.\nArctic Wolf Classification\n- The following is the scientific classification or taxonomy of Arctic wolf:\n|Sub-species||Canis lupus arctos|\nArctic Wolf Appearance\n- An Arctic wolf has a medium body size with 3.2 to 5.9 feet head-to-tail length.\n- It has a white coat color that resembles the snow.\n- It has two coats; a waterproof inner coat that keeps its body dry and a thick, furry outer coat that keeps its body warm.\n- It has a narrow skull, a short muzzle, and small ears.\n- An Arctic wolf has strong jaws. It has 42 sharp teeth specially designed for tearing flesh and crushing bones.\n- Arctic wolves have short legs and thickly padded paws, which allows them to walk easily on the snow and frozen grounds.\nArctic Wolf Physical Description – Arctic Wolf Characteristics\nArctic Wolf Size\n- Arctic wolves have body sizes from 3.2 to 5.9 feet (0.9 to 1.8 meters).\n- They have sexual dimorphism and male individuals are usually bigger than females.\nArctic Wolf Weight\n- The bodyweight of Arctic wolves ranges from 31.7 to 79.3 kg (70 to 175 pounds).\nArctic Wolf Height\n- The average standing height of Arctic wolves from head to toe is about 3 feet (0.9 meters).\n- At the shoulder region, their height may vary from 2 to 2.5 feet (0.6 to 0.7 meters).\nArctic Wolf Length\n- The body length (head to tail) of Arctic wolves ranges from 3.2 to 5.9 feet (0.9 to 1.8 meters).\nArctic Wolf Lifespan\n- The lifespan of Arctic wolves in the wild is about 7 to 10 years.\n- In captivity, they live longer for more than 18 years due to the proper availability of food and protection from natural hazards.\nArctic Wolf Eyes\n- Arctic wolves have blue color eyes at birth.\n- When they grow, the color of their eyes changes to yellow-golden or orange.\n- Adult full-grown individuals usually have golden-brown eyes.\nArctic Wolf Species\n- Arctic wolf belongs to the species of grey wolf (Canis lupus).\nWhat Do Arctic Wolves Eat – Arctic Wolf Diet\n- The Arctic wolf is a carnivore in nature and eats the flesh of other animals as its primary diet.\n- They usually hunt musk oxen and Arctic hare. However, they also eat caribou, moose, Arctic fox, lemmings, birds, seals, and beetles.\n- They are opportunistic and also sometimes scavenge on garbage and carrion.\n- Studies show that muskoxen are their primary prey. Because in the presence of abundant muskoxen, Arctic wolves have a higher reproduction rate as compared to the higher availability of Arctic hares.\n- Another study suggests that the consumption of the two species (muskoxen and Arctic hares) depends on the year and season and it is uncertain how much Arctic wolf relies on the two species.\n- Arctic wolves hunt in packs usually to prey on muskoxen or caribou.\n- The packs are also known for killing cubs of the polar bear.\n- During the winter, they will follow migrating caribou for a large distance.\n- In one meal, an Arctic wolf can eat about 9 kg (20 pounds) of meat.\n- Large hunts usually last for a few days. Between the meals, all members of the pack take turns to guard carcass against scavengers.\nWhat Do Arctic Wolves Hunt – Arctic Wolf Prey\n- The most common prey of Arctic wolves are:\n- Arctic hares\n- However, they also hunt other animal species like:\nWhere Do Arctic Wolves Live – Arctic Wolf Habitat\n- Arctic wolves are native to the Arctic region of North America and Greenland.\n- They spent their whole lives in the Arctic tundra biome.\n- A pack of Arctic wolves has a very large territory of about 1,000 square miles (2,500 square kilometers). The size of their territories depends on the availability of food.\n- Arctic wolves use caves or outcrops to take shelter, as it is difficult to dig the frozen snowy ground to create a den.\n- Many of the Arctic wolf populations live close to the sea to feed on seabirds and seals.\nArctic Wolf Habitat Map\nArctic Wolf Range- Arctic Wolf Location\n- The habitat range of the Arctic wolves occurs above the latitudes of 67° North.\n- The location of their habitat is:\n- Northern parts of Alaska in the United States\n- Queen Elizabeth Islands (from Melville Island to Ellesmere Island) in Northern Canada\nArctic Wolf Climate\n- Arctic wolf is one of the few mammals that tolerate extremely harsh conditions.\n- The climate of their habitat region is extremely cold throughout the air.\n- The temperature range in the region is widespread. The month of January is the coldest month of the year, in which the temperature lowers to an average of about -38℃ (-36.4 ℉). The warmest month is July, in which the average highest temperature is about 10℃ (50 ℉).\n- The daylight in their habitat also fluctuates highly with the season. Days in the winter are dark and without sunlight while the sun does not set in the summer days.\n- Most of their habitat region is covered with snow throughout the year.\n- The average precipitation is 250mm per year.\nWhat Other Animals Live With The Arctic Wolf\n- The following are some of the other animals that share the Arctic wolf’s habitat:\n- Arctic hares\n- Arctic fox\n- Many species of birds (such as Arctic tern, snow goose, and ptarmigan)\n- Lemmings and various other types of rodents\nArctic Wolf Adaptations\nThe following are the major structural and physiological adaptations of Arctic Wolves, which helps them to survive in the harsh Arctic climate:\nShort Muzzle and Ears\n- Arctic wolves have short muzzles and small ears as compared to other subspecies of grey wolves.\n- This feature reduces the surface area to volume ratio and prevents excessive heat loss through the mouth and ears.\nCoat Color and Thick Fur\n- Arctic wolves have white or light grey coat colors, which provides them with excellent camouflage.\n- They also have a double layer of fur; an inner layer of fine and soft hair and an outer layer of thick long hair.\n- The inner layer keeps their body insulated while the outer layer is waterproof and keeps them dry and warm.\n- Their fur grows denser during the cold season to provide extra insulation.\n- The paw structure of Arctic wolves is also well-adapted to their habitat.\n- Their paws have a covering of fur, which acts like snow shows.\n- The fur on their paws provides insulation as well as a strong grip when they walk on the snow and frozen slippery surfaces.\nCountercurrent Heat Exchange System\n- Arctic wolves have a special blood circulation mechanism in their paws that reduces heat loss, as their paws have direct contact with the cold snowy surface.\n- In this mechanism, the blood coming to the paws is used to heat the blood that is leaving.\n- In such a way, excessive heat loss is prevented while the paws are also saved from extreme coldness.\n- Such a mechanism is also found in the feet of penguins and the legs of Arctic foxes.\nThick Layer of Body Fat\n- Arctic wolves have a thick layer of body fat beneath their skin.\n- It provides insulation and also serves as a food reservoir in case of food scarcity.\nArctic Wolf Sense Of Smell\n- Wolves have about 100 times a greater sense of smell than humans. The smell receptive region in a wolf’s nose is about 14 times greater than a human nose.\n- Like other wolves, Arctic wolves also have a keen sense of smell.\n- Their scent recognition level is extremely higher, which they use for hunting potential prey.\nArctic Wolf Behavior\n- Arctic wolves are social and live in groups known as packs. The number of individuals in a pack varies from five to eight.\n- Arctic wolves have very large territories. A pack may have a territory of about 2,500 square kilometers (1,000 square miles) or more. Their territory’s size depends on the availability of food.\n- They communicate through signs, smell, behaviors, and sounds. They howl to communicate with other packs. A pack of Arctic wolves can compare its size with other packs by hearing the howling of other packs. Small packs move to another region to find food upon hearing the howls of bigger packs. In such a way, they avoid fighting with other packs.\n- Arctic wolves are unafraid of humans. In some regions, they can be persuaded to approach humans cautiously and curiously. This behavior is believed to be due to their very rare encounter with humans.\n- Arctic wolves migrate during the wintertime to avoid the complete darkness of their habitat. The 24 hours darkness of their habitat makes it hard to study their movement. So very little is known about their migration.\nArctic Wolf Pack\n- The group of Arctic wolves is known as a pack.\n- A pack has 5 to 8 individuals.\n- Every pack has a dominant male and female known as alpha male and female. They are the leaders of the pack.\n- Sometimes, a pack’s size reaches up to 20 individuals, however, it is uncommon.\n- Most of the packs are extended families and composed of a breeding pair and their pups.\n- A pack cooperatively hunts large animals.\n- All the pack members take care of the pups and protect them from predators and other threats.\n- Individuals of a pack use scent and urine to mark its territory’s borders.\nArctic Wolf Life Cycle\n- The life cycle of Arctic wolves starts when they attain sexual maturity at the age of 2 to 3 years.\n- A mature Arctic wolf leaves its birth pack and establishes its territory along with its mate or either stays within its birth pack.\n- If the size of its birth pack is large, it would have the opportunity of breeding. As in small packs, only the alpha members can reproduce.\n- Upon mating, a litter of 2 to 3 pups is born after a gestation period of 63 days.\n- The lifespan of Arctic wolves is 7 to 10 years in the wild and more than 18 years in captivity.\nArctic Wolf Reproduction\n- Alpha male and female breeds only in the pack of Arctic wolves. However, other mates also breed in large-sized packs.\n- They are known to remain with only one mate.\n- The breeding season of Arctic wolves is from January to March, in which the pairs mate.\n- After a gestation period of about 63 days, the females give birth to a litter of 2 to 3 pups.\n- Arctic wolves use outcrops, caves, or even shallow depressions as a den where the females give birth, as digging the frozen floor is so difficult for them. However, if the frozen snow is not too hard, they will dig a den.\n- The birth of pups occurs in late May to early June.\n- The pups stay with their mother for about 2 years.\nFemale Arctic Wolf\n- As Arctic wolves have sexual dimorphism, the females have a smaller size than males.\n- Female Arctic wolves reproduce, feed the pups with their milk, and take care of them until they become completely mature.\nArctic Wolf Babies – Baby Arctic Wolf – Arctic Wolf Pups\n- The babies of Arctic wolves are known as pups or whelps.\n- They are born after a gestation period of about 63 days.\n- At birth, the pup weighs about 2 to 3 pounds and has dark fur.\n- They are blind and deaf at birth and become able to see and hear after 12 to 14 days.\n- The pups have blue color eyes that become orange or yellow-golden when they are between 2 to 4 months old.\n- As an Arctic wolf is a mammal, the mother feeds the newborns with her milk.\n- After a few weeks, mothers bring small pieces of chewed food to the pups and they start nibbling it.\n- The pups stay in their birth den or cave for about 1.5 months along with their mother.\n- After 1.5 months, they start to join the pack in their activities and learn skills to survive.\n- The pups become full-grown at the age of about 8 months.\n- They stay with their mother for about 2 years.\n- Pups attain sexual maturity at the age of 2 to 3 years.\n- The pups stay generally with the same pack for many years.\nAre Arctic Wolves Endangered – Why Are Arctic Wolves Endangered\n- No, Arctic wolves are not endangered.\n- Due to their isolated habitat, they are not even threatened by hunting or habitat loss.\n- Climate change is the greatest threat to their survival. Extreme variations in the weather cause food scarcity for muskoxen and Arctic hares. It causes a decline in the populations of these species and so, in turn, leads to the reduction of the traditional food supply of the Arctic wolves.\nArctic Wolf Conservation Status\n- The conservation status of the Arctic wolf on the IUCN Red List as Least Concern.\nArctic Wolf Population\n- According to the Cool Antarctica Resources, the total population of Arctic wolves is estimated at 200,000 individuals.\nArctic Wolf Role In The Ecosystem\n- Like other wolves, Arctic wolves also play a vital role to maintain the health, structure, and balance of the ecosystem.\n- They keep the population of prey species in control.\n- Their presence affects the behavior and population of their prey and changes their foraging patterns.\n- In turn, it affects the plants and animal communities and often causes changes in the landscape.\n- That is why wolves are described as the keystone species.\nArctic Wolf Predators\n- Arctic wolf is itself the apex predator of its habitat.\n- The polar bear is the only natural predator of the Arctic wolf.\nArctic Wolf Interesting Facts – Fun Facts About Arctic Wolves\n- Arctic wolf is one of the rare examples of animal species that are relatively not threatened by humans. Due to the isolated habitat range, Arctic wolves are not disturbed by urbanization or habitat destruction.\n- An Arctic wolf has a special countercurrent heat exchange system in its feet, which prevents its feet from freezing while walking on the snow.\n- Arctic wolves are born with blue eyes that become brown or golden-brown when they are growing old.\n- Arctic wolves are fast and can run at the speed of 40 miles per hour when chasing prey.\n- Youngsters of the Arctic wolves eat partially digested food that their mother and other members of the pack regurgitate after hunting.\n- In a pack, only the highest-ranking members are allowed to reproduce. They are known as the Alpha male and females.\n- Like other wolves, packs of the Arctic wolves communicate through howling. They can compare the size of their pack with other packs’ sizes by hearing howlings.\n- Arctic wolves usually avoid conflicts with other packs and small packs simply move to other regions to find food."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:71fe3a7a-9cda-4f1f-a57a-f4987a0c4a1d>","<urn:uuid:7896e363-13ff-4314-aa6a-80a2f053c887>"],"error":null}
{"question":"How do the casualty figures from the Battle of Antietam compare to other major Civil War battles, and what made this particular battle so significant in American military history?","answer":"While the Battle of Antietam resulted in over 22,000 casualties, making it a very costly battle, other Civil War battles had higher total casualties - such as Gettysburg with over 50,000, Seven Days Battle with over 35,000, and Chickamauga with over 34,000. However, Antietam stands out as the bloodiest single day in American military history. The battle was particularly deadly due to the combination of advanced rifle technology that allowed for accurate long-range shooting, coupled with outdated battle line tactics and poor decision-making.","context":["- 1 Why did General Lee decide to invade Maryland in September 1862?\n- 2 What critical event happened on September 13 which provided McClellan Lee’s battle plan?\n- 3 What was Lee’s goal for the Battle of Antietam and the Maryland Campaign?\n- 4 What was Lee’s strategy during the Maryland Campaign?\n- 5 What was the bloodiest single day battle in the Civil War?\n- 6 Which state did General Lee hope to convince to join the Confederacy?\n- 7 What was the bloodiest day in history?\n- 8 What was the bloodiest single day?\n- 9 What high ranking union general is killed on the first day?\n- 10 Why was Antietam so deadly?\n- 11 Who Lost the Civil War?\n- 12 What did the Confederates call the battle at Sharpsburg Maryland?\n- 13 Could Lee have won at Gettysburg?\n- 14 Did the Confederacy invade the Union?\n- 15 What was the major cause of death during the Civil War?\nWhy did General Lee decide to invade Maryland in September 1862?\nLee invades Maryland! Why Lee Invaded Maryland: Lee’s greatest objective was that a victory in the North could possibly gain diplomatic recognition from Europe and bring England and France to the aid and assistance of the South.\nWhat critical event happened on September 13 which provided McClellan Lee’s battle plan?\nThe Battle of Antietam during the American Civil War proved indecisive. Confederate General Robert E. Lee’s men retreated to Virginia, and Union General George B. McClellan did not follow up his advantage and pursue them.\nWhat was Lee’s goal for the Battle of Antietam and the Maryland Campaign?\nRobert E. Lee’s Goals The invasion would enable Southern troops to commandeer supplies from Northern farmers. Lee also hoped that giving Northerners a taste of war would diminish their resolve, possibly bolstering the prospects of Peace Democrats in the upcoming 1862 Congressional elections.\nWhat was Lee’s strategy during the Maryland Campaign?\nHis goal was to reach the major Northern states of Maryland and Pennsylvania, and cut off the Baltimore & Ohio Railroad line that supplied Washington, D.C. His movements would threaten Washington and Baltimore, so as to “annoy and harass the enemy.” Several motives led to Lee’s decision to launch an invasion.\nWhat was the bloodiest single day battle in the Civil War?\nAntietam was the bloodiest one – day battle of the Civil War. But there were other battles, lasting more than one day, in which more men fell. The numbers below are total casualties for both sides.\nWhich state did General Lee hope to convince to join the Confederacy?\nHistorical Note: Robert E. Lee and his Army of Northern Virginia, after his decisive victory over the Union army at the Battle of Second Manassas, stands poised on the Virginia side of the Potomac River. Lee is eager to take the fight into the State of Maryland, a slave holding state. Lee is hopeful that the\nWhat was the bloodiest day in history?\nBattle of Antietam breaks out Beginning early on the morning of September 17, 1862, Confederate and Union troops in the Civil War clash near Maryland’s Antietam Creek in the bloodiest single day in American military history.\nWhat was the bloodiest single day?\nOn this morning 150 years ago, Union and Confederate troops clashed at the crossroads town of Sharpsburg, Md. The Battle of Antietam remains the bloodiest single day in American history. The battle left 23,000 men killed or wounded in the fields, woods and dirt roads, and it changed the course of the Civil War.\nWhat high ranking union general is killed on the first day?\nAlbert Sidney Johnston\n|General Albert Sidney Johnston|\n|Albert Sidney Johnston, c. 1861|\n|Born||February 2, 1803 Washington, Kentucky, US|\n|Died||April 6, 1862 (aged 59) Shiloh, Tennessee, CS|\n|Buried||Texas State Cemetery|\nWhy was Antietam so deadly?\nWhy was Antietam so deadly? Advanced technology, unwise tactics, and terrible decision-making. The combination of new rifles that could be shot with great accuracy from far away and old-fashioned battle lines led to unprecedented deaths in the Battle of Antietam (and in the Civil War in general).\nWho Lost the Civil War?\nAfter four bloody years of conflict, the United States defeated the Confederate States. In the end, the states that were in rebellion were readmitted to the United States, and the institution of slavery was abolished nation-wide. Fact #2: Abraham Lincoln was the President of the United States during the Civil War.\nWhat did the Confederates call the battle at Sharpsburg Maryland?\nThe Battle of Antietam, also called the Battle of Sharpsburg, occurred on September 17, 1862, at Antietam Creek near Sharpsburg, Maryland.\nCould Lee have won at Gettysburg?\nIn fact, Early claimed, Lee’s Army of Northern Virginia would have won the Battle of Gettysburg, the turning point in the Civil War, if his orders had been obeyed. So it was that “the sunrise attack order” of July 2, 1863, entered American history as a fact, and was treated as such for the next 100 years.\nDid the Confederacy invade the Union?\nThe Gettysburg campaign was a military invasion of Pennsylvania by the main Confederate army under General Robert E. Lee in summer 1863.\n|Date June 3 – July 24, 1863 Location Maryland, Pennsylvania, Virginia Result Union victory|\n|Commanders and leaders|\nWhat was the major cause of death during the Civil War?\nAmerican Civil War casualties are those soldiers, both Union and Confederate, who died, were wounded, went missing or were captured. Of those who died, by far the leading cause of death was disease. The exact number of dead will never be known with any certainty.","Civil War Casualties\nCasualties Numbers And Battle Death Statistics For the American Civil War\nThough the number of killed and wounded in the Civil War is not known precisely, most sources agree that the total number killed was between 640,000 and 700,000. (See article below)\nUnion Civil War Casualties\nCombat Deaths: Over 110,000\nOther Deaths*: Over 250,000\nConfederate Civil War Casualties\nCombat Deaths: Over 95,000\nOther Deaths*: Over 165,000\n(*Other Deaths include, among others: disease (by far the most common cause of death), accidents, drowning, heat stroke, suicide, murder, execution.)\nCivil War Casualties: The Bloodiest Battles\nBattle Of Gettysburg: Over 50,000 casualties\nSeven Days Battle: Over 35,000 casualties\nBattle Of Chickamauga: Over 34,000 casualties\nBattle Of Chancellorsville: Over 29,000 casualties\nBattle Of The Wilderness: Over 24,000 casualties\nBattle Of Antietam: Over 22,000 casualties\nSecond Battle Of Bull Run: Over 24,000 casualties\nBattle Of Shiloh: Over 23,000 casualties\nBattle Of Fredericksburg: Over 18,000 casualties\nCold Harbor: Over 18,000 casualties\nArticles Featuring Civil War Casualties From History Net Magazines\nWar by the numbers\nBy Harold HolzerEyebrows were conspicuously raised recently when a “demographic historian” from New York’s State University at Binghamton convincingly recalibrated the long-accepted Civil War death toll—boosting the grisly statistic by an astounding 20 percent.\nAccording to Dr. J. David Hacker, the traditional death toll of 620,000—which historians have accepted for more than a century—failed properly to account for several key factors, including the influx of immigrants into the armed forces, not to mention casualties among black women who found themselves victims of the onrush of war. Hacker employed a new range of statistical accounting to determine mortality, including a system called the “two-census method.” To measure deaths, he counts the number of 20- to 30-year-olds in the 1860 census, and the number of 30- to 40-year-olds who turn up in—or, more important, disappear from—the next count, 10 years later. The difference represents the number of young people who died in the intervening decade, and Hacker took an educated stab, based on a shrewd reading of regional loyalties, at determining how many of them likely perished on the battlefield and not home peacefully in bed.\nIt’s useful to keep in mind that the long-accepted 620,000 tally was the work of two energetic but amateur historians, William F. Fox and Thomas Leonard Livermore, Union veterans who read every pension record, battlefield report and muster roll they could put their hands on. Fox published his Regimental Losses in the American Civil War in 1889—and through their extraordinary research we learned that the average Federal soldier weighed 143.5 pounds.\nInevitably, the new death-counting process proved more complicated than even this. For one thing, apparently, the reunited country’s 1870 census was something of a hash, with a level of undercounting that made the complaints around our recent 2010 census seem mild by comparison. Hacker admits it also remains difficult to count civilians who died in wartime. And he’s still as intrigued as the rest of us by the challenge of counting the number of farm boys who died from sickness after exposure to germ-riddled, but essentially immune, urban soldiers. Union medical care, he further points out, was far superior to Confederate—and more Johnny Rebs might have died of disease than Billy Yanks. Deaths among African-American troops have long had a widely accepted numerical accounting, but these numbers, too, Hacker believes, deserve reconfiguring, though no one is quite sure how to do it.\nCaveats notwithstanding, Hacker bravely aimed at revising the total count, concluding the actual death toll for the Civil War amounted to between 650,000 and 850,000—and by prudently splitting the difference, proposed a new number: 750,000, as reported in America’s Civil War in March 2012. It also inspired a major New York Times story in April by Guy Gugliotta (whose new book, Freedom’s Cap, by the way, tells the extraordinary story of the U.S. Capitol and the coming of the rebellion). The scholarly journal Civil War History not only published the Hacker findings but trumpeted them, almost uncharacteristically, as “among the most consequential pieces ever to appear” between its covers.\nDrew Gilpin Faust was right. In her extraordinary book This Republic of Suffering, the historian and president of Harvard University reminded modern readers of post-war America’s obsession with Civil War death and memory. The rush to build cemeteries, monuments and memorials, together with the overwhelming responsibility merely to bury dead bodies, filled survivors with an abiding reverence for, and obsessive fascination with, those who sacrificed that the nation might live (and even those who gave their lives that it might die). Exhumations were common as survivors and widows struggled with competing notions of sacred ground. Soldiers cemeteries became part of the American culture—and not just at Gettysburg. Those old emotions remain raw. Mass mourning is never far from the surface of American culture, and statistics not only encourage scholarly debate but expose unhealed wounds.\nThe new Civil War death toll numbers have stirred the pot afresh. In reporting the new statistics, the Times, for example, took an unexpected pot shot at veteran historian James M. McPherson, one among countless scholars who have long accepted the earlier 620,000 number. The article called out the dean of the field for using that number “without citing the source in Battle Cry of Freedom, his Pulitzer-winning 1988 history of the war.” The fact that no one else has ever “sourced” the figures did not seem to matter in the new rush to up the gruesome ante.\nMcPherson, in turn, had a bone to pick with yet another great historian, Mark E. Neely, who once convincingly argued that the Civil War was not a total war in the 20th-century sense. McPherson commented that the revised numbers suggest that Neely was wrong after all—for what else but a total war could produce such staggering casualty figures?\nWhat is extraordinary about all this is that we still desperately want to know the truth—the whole truth, and nothing but the precise truth—about the toll of war. We may never find out for certain how many men and women, blacks and whites, native born and foreign born died to save the Union and destroy slavery. But as the new science and the new attention show—thanks to David Hacker, Guy Gugliotta, et al.—more than curiosity is at work here. Hacker put it modestly when he opined that “it is just a curiosity.” In a sobering afterthought, he wisely told Gugliotta and the Times: “It’s our duty to get it right.”\nHarold Holzer is chairman of the Abraham Lincoln Bicentennial Foundation.\nSmoke and fire filled the skies south of Petersburg in December 1864 as the Army of the Potomac's V Corps targeted the Weldon Railroad. During a raid along this vital supply line linking southeastern Virginia with North Carolina, liquor-fueled Federals …\nThe North's Unsung Sisters of Mercy\nBy Alice P. Stein\nA cadre of dedicated Northern women from all walks of life traveled to the charnel houses of the Civil War to care for the sick and wounded.\nThey came from …\nDesperate Stand at Chickamauga\nBy James B. Ronan II\nBrigadier General John King's disciplined brigade of Union Regulars found itself tested as never before at Chickamauga. For two bloody days, the Regulars dashed from one endangered spot to another, seeking …\nThe Civil War's deadliest weapons were not rapid-fire guns or giant cannon, but the simple rifle-musket and the humble minié ball.\nBY ALLAN W. HOWEY\nBy the time the smoke had cleared and the veterans headed back to …\nSavage Skirmish Near Sharpsburg\nBy Scott Hosier\nWith Robert E. Lee's wily Confederates waiting somewhere in the vicinity of Antietam Creek, Union General George McClellan ordered I Corps commander Joseph Hooker to advance and turn the Rebel flank. But McClellan, …\nWar's Last Cavalry Raid\nBy Chris Hartley\nEven as General Robert E. Lee was surrendering at Appomattox, a vengeful Union cavalry horde led by Maj. Gen. George Stoneman made Southern civilians pay dearly for the war. It was a last …\nThe hard-fighting 44th Georgia suffered some of the heaviest losses of any regiment in the Civil War.\nBy Gerald J. Smith\nOn March 10, 1862, companies of Georgians from Henry, Jasper, Clarke, Spalding, Clayton, Putnam, Fayette, Pike, Morgan, Henry and …\nTaking of Burnside Bridge\nBy John M. Priest\nWhile Union commander George McClellan fumed and the Battle of Antietam hung in the balance, a handful of Rebels held off Federal troops at \"Burnside Bridge.\"\nThe day–September 17, 1862–promised to be …"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:b22d44d6-5393-48c7-82d1-db42dec69ef9>","<urn:uuid:d7084b51-edd9-49b9-ab5a-cadd4db9e820>"],"error":null}
{"question":"How does social learning evolve in both traditional classroom settings and digital environments, and what are the associated data security challenges?","answer":"Social learning is evolving across both physical and digital spaces. In traditional settings, blogs enable communities to connect over common interests regardless of location, allowing educators and students to share knowledge and enhance teaching practices. The future of e-learning is increasingly social, with platforms enabling global connections and collaborative learning opportunities. However, these digital social learning environments face significant data security challenges. When students and teachers interact with these platforms, their data is often collected and shared through APIs across multiple services. Companies can amass large amounts of user information, often without explicit user consent. This data collection leaves users vulnerable to breaches, as demonstrated by recent incidents at major platforms. The complex web of interconnected services means that even simple educational activities can lead to widespread data sharing across numerous websites and applications.","context":["Blogs offer an excellent way for communities to come together over a common interest despite physical location. Whether they are in Alaska or Hawaii, Educators and students completing online degree programs for Education can take advantage of this shared pool of knowledge to find inspiration, enhance their teaching, and help students learn more. The following blog posts come from a variety of bloggers sharing their passion and insight.\nThese posts take a look at what makes a good teacher and will inspire you to appreciate what you do or maybe even improve your style as an educator.\n- What Makes a Good Teacher. This post from The Reading Workshop includes six important elements of effective teachers.\n- Top 5 Character Traits of Great Teachers. This list is actually a compilation from a reader survey and includes many great traits of quality teachers.\n- “Steal”. This teacher considers use of the word “steal” when discussing teaching ideas and urges teachers to embrace collaboration as a means to better teaching.\n- Reminders to teachers: Don’t get sick. This post, authored by W.R. Chandler from northern California, highlights what can happen with a poor-quality substitute teacher and offers suggestions for the next time you must be absent.\n- Are you trained or educated?. This thought-provoking post asks you to consider whether you are trained or educated as an educator.\n- Short Story: On Mercy Killing in the First Grade (or, how I stopped worrying and learned to appreciate punch lines). Read how this educator learned an important life lesson in the first grade.\n- Training. This post compares how teachers-in-training are taught to the education of students and wonders at the differing standards held for each.\n- Do Leaders Wear Jeans? Does What You Wear Show Who You Are?. This article offers a great approach to fitting in fashion-wise–even in jeans, and was written by Liz Strauss from Chicago, Illinois.\n- On being a passionate beginner. Learn why it is important to welcome failure and embrace the newness of situations to become a better teacher.\n- 5 Altruistic Values of Teaching. This post offers a great reminder of why many educators entered into the field of teaching.\nWorking with Students\nLearning about how students learn, perceive, and grow and discovering how you can help students succeed are the focus of these blogs.\n- Paul Potts, Susan Boyle, and the problem of undiscovered talent in schools. This brief post by Scott McLeod from Iowa urges educators to help recognized untapped potential in students.\n- How Much is a Field Trip Worth?. Examine the educational value of taking learning–and students–out of the classroom.\n- Home Ec Returns. Learn what Michael Mazenko says about the importance of teaching practical skills to high school students in this post. Mazenko is an AP English Teacher in Colorado.\n- Tutoring is like a GPS. This thoughtful post explores how teachers and tutors can work best to help students truly learn.\n- Why A Financial Education Should Not Be Postponed. Directed more specifically at parents, but an important reminder for educators as well, this post shares why children should learn about finances from an early age.\n- How DO We Learn Math?. This post explores the ways in which students learn math and how to incorporate these styles into effective learning.\n- The Library of Congress Pushes Primary Sources. Find out about this initiative at the Library of Congress and why teachers should support primary sources for students.\n- Ways of the Teacher-Feminist: Text, schema, and stereotypes, oh my!. This post looks at why it is important to teach diversity in the classroom.\n- When Parents do the Homework. This blog post offers a gentle reminder that the best way to get a student to do their own homework is by assigning child-appropriate tasks.\n- Knowledge is the Key. This teacher looks at the connection between knowledge and learning and shares some reading material that helped shape his approach as a teacher.\n- What do General Education Teachers Need To Know about Special Education. Learn about the importance of IEPs and how you can better serve your students by understanding what is written in them.\n- 52 Teachers, 52 Lessons: Week 14. See what this teacher has to say about picking your battles in the classroom in this blog post that is part of an ongoing series intended to offer helpful information for teachers.\nIdeas, Tips, and Resources for the Classroom\nFind great suggestions and resources you can use in the classroom with these posts.\n- The Best Places To Find Theatrical Movies On Science, Math, & History. Everyone loves movies, so learn how you can incorporate movies in your science, math, and history classes here.\n- Expressive Social Studies. Find a handful of methods for bringing history and social studies lessons to life.\n- Writing Tip #3: Pictures aren’t Just for Babies. Use this tip to encourage students to write more descriptively.\n- Essential Gardening Books for Kids. This list not only names some great gardening books to use in school, each is accompanied by an age level and what is special about the book.\n- More Classroom Ideas for Old Fashioned Index Cards. Get some creative and inexpensive ideas for using index cards in the classroom.\n- Serial Success: 6 Strategies for School. These six tips offer suggestions for making the most of the students’ time in class. These tips are geared toward older students.\n- Teaching Students to Ask the Right Kinds of Questions. Learn ways to encourage students to ask questions and ask well-thought out questions at that. The author of this blog is a professional teacher from South Carolina.\n- Finding Their Voices. This blogger attempts to define the tricky term of “voice” in a way that may help you share that knowledge with your students.\n- Look to Learn Launched. Discover this new resource that strives to help teachers promote media-rich learning opportunities in the classroom.\n- Teaching Antigone. This teacher shares an effective way to teaching Antigone to high school sophomores.\n- Reflections on Student Blogging. Get great tips and suggestions from this teacher on how to start blogging with your middle and high school students.\n- Educational Videos. This post introduces the PBS library of American History in Videos as well as tips on integrating videos in your lessons.\nThe Economy and Education\nThe recent economic downturn has affected education in many ways. Read these posts to see what others are saying about education and the economy.\n- Will There Be “Urban” Poverty in the Future? From the Inner-City to the Doughnut. This post takes a look at the connection between education, funding, and the spread of poverty out of inner cities.\n- The Impact of Market Norms on Education…. Bill Ferriter from North Carolina shares his thoughts on an educational system that has moved from one based on social norms to one based on market norms.\n- The Need for Breakfast Clubs. This call for a Canadian breakfast program emphasizes the need for a healthy breakfast each day to ensure the best start for students.\n- Part 1: What is happening with teaching jobs?. This is the first in a five-part series examining teaching jobs in America and what is happening to help prevent teacher layoffs.\n- A Dozen Ways to Save Dough. These twelve tips offer suggestions for administrators to consider when feeling the financial pinch.\n- The Union President’s Dilemma. Take a look at how the current financial crisis is impacting school districts and some of the tough choices that are being forced on educators.\n- Poverty and Education – The Challenge of Improving Schools. This post explores the issue of educating students faced with the challenges of living in poverty.\n- Education is harder to steal (and therefore also harder to tax) than physical wealth. Here’s one man’s perspective on why spending on education is a smart move.\n- Work-Study Will Get a Boost. Referencing a recent article in the New York Times, this post illustrates the importance of work-study programs for students.\n- Education Publisher’s Perspectives on the Economic Downturn – Panel on Education Technology. Read this post that encapsulates a panel Q&A of four learning publishers’ perspectives on the economy and education technology.\nEducational Issues and the Future of Education\nMany feel that the educational system is in need of reform. Others may not see that need so much, but recognize changes that are coming. Find out what people are saying about the current and future issues central to education.\n- Let’s just put them all in jail 24/7. This post offers a thoughtful counter-position to Arne Duncan’s statements that schools should be open 6 days a week for 11 or 12 months of the year.\n- Newton lectured to empty rooms. Not only does this post warn against the dangers of boring lectures, it also theorizes on why some lecturers prefer this manner of teaching.\n- International Tests of Mystery. Nancy Flanagan writes an intelligent post examining the differences behind various tests and what they measure and argues for investing in education of all students, regardless of their scores.\n- Jacks of All Topics, Masters of None. Based on a recent study that shows high school students who study a topic deeply perform better than those who do not, this post explores the downfalls of teaching to the test.\n- Curricula: a short essay about education. This post advocates restructuring education so that it is a more natural form of learning and teaching–with better results and less work to make it happen.\n- The Retention Myth. This post reflects on current thinking about retaining science and math teachers.\n- John Rawls is Twisting in His Grave. Read about the voucher system in connection with providing appropriate education for children with disabilities who cannot afford private education here.\n- Education Reform and the Freedom to Mod. Andy Carvin explores what education reform looks like to many teachers and parents.\n- Innovation And It’s Discontents. This post provides links to several other posts discussing innovation, teacher satisfaction, private capital in education reform, and more.\n- Schools Taking a Bite Out of Lunch Time. Find out why some schools are changing the way lunch time looks at the elementary level.\nEvery teacher knows that behavioral issues can disrupt class, especially teachers with a background or bachelor’s degree in Management. Read these posts to learn effective classroom management techniques to enhance what you already know.\n- Catch them Doing the Right Thing. This veteran teacher explains why this method of positive reinforcement works well to encourage effective learning and better classroom behavior.\n- Where Does Hard Work Come From?. Mr. McGuire ponders from where student motivation derives and opens the comments up to hear from readers.\n- My classroom management plan on Scribd. Check out the suggestions shared by this elementary teacher with her revised classroom management plan.\n- Losing It (In the Classroom). This post focuses on managing teacher anger in the classroom.\n- A Time for Timeout?. Specifically for those teaching younger students, this post explores time-outs and effective use of them.\n- Do It Now or Else. This post shows just why spending extra time for students with behavior problems is necessary.\n- America’s Next Top Student. This teacher talks about appropriate dress for students.\n- Lost My Cool. This teacher shares her frustration and how both she and her classroom came through the incident successfully.\n- Self Esteem: Part 1. Exploring the connection between low self-esteem and behavior issues, this post proposes that relying on teachers to raise self-esteem through feel-good tactics is not an effective method of dealing with these situations.\n- Que? – A Funny Experience I Had as a First-Year. This story will not only give you a chuckle, but may help put your classroom difficulties in perspective.\nTechnology is the topic among these blog posts–from blogging to Internet safety to making the most of your technology funds.\n- Chatting Up Internet Safety. This teacher explains how she used Chatzy with a 7th grade class to work on Internet safety and technology.\n- Keeping students cybersafe!. This blog post offers suggestions on how you can protect your students while enjoying blogs in the classroom.\n- Responsible blogging. Based on a collaborative effort between a teacher and his 10th grade students, this is the list of rules they developed for safe blogging.\n- Managing Comments and Posts On Student Blogs Using Google Reader. Sue Waters provides an easy way to keep up with student posts and includes a helpful tutorial on how to set it up with Google Reader.\n- Around the World with 80 Schools. Read this post for an inspirational way to use Skype in the classroom to connect your students with others around the world.\n- TECH & TE(A)CH. Explore how technology has become an important element in education and think about ways in which it can best be used.\n- Re-Visioning the Writing Classroom. This teacher used a computer lab opportunity to improve writing skills in a 5th grade class.\n- Project This! Technology Purchasing Priorities. Learn what pieces of technology should be purchased prior to others so that you can build your resources efficiently.\n- The future of e-learning is social learning. Find out what Jane Hart has to share about e-learning and social learning.\n- Taking It Global with TIGed. This program allows students to connect globally with others. Learn how you can incorporate the same technology and learning in your classroom.\n- Your Digital Dossier. Learn about digital portfolios and why they matter to you as an educator.\n- Copyright for Educators. This blog post includes an hour-long presentation on copyright issues specifically for teachers.\nPreparing Students for Life after High School\nWhether you are a high school teacher or counselor, if you are in a position to help students make the transition to life after high school, then you should check out these posts.\n- who says you have to go to college?. Advocating preparing students for life and successful careers doesn’t have to depend on college, this blogger explores the possibility of students who are not college material still succeeding.\n- Don’t Go to College. This article proposes an alternative method for those who choose not to go to college to showcase their skills and talents without the negative connotation of not having a degree.\n- Tech Education Opens Career Doors for Working Adults. This post describes how many adults can opt for tech education rather than college to succeed in having a promising career.\n- Book Review: 411 SAT Prep Series. Find out why this book is recommended as a great SAT prep for high school students.\n- 100 Terrific Tools and Resources to Find Your Perfect College. This blog post offers tons of resources for students looking for a college, including how to select, financial aid, and more.\n- Rejection tweeting. Take a glimpse at some students Tweets in reference to their college application rejections.\n- Why Earn an MBA? An MBA degree pays off!. If you have students who are the business school type, share this blog post with them when considering their future.\n- 23 Warning Signs of Scholarship Scams. Scholarships offer promise to those who may not be able to afford college, but help ensure your students are applying for legitimate scholarships with the help of this article.\n- Stanley Tate says; “Don’t Raise Tuition. Raise Hell!”. Think about the perspective presented here that institutes of higher learning are taking advantage of students when they raise tuition.\n- SCAMPER Your Way to Success. Learn this technique to help students prepare for college life.\n- How to Get off the College Wait List. This article offers suggestions to help students get off the wait list and accepted into a college of their choice.\nLinks to Resources\nThe following blog posts provide lists and links that offer plenty of additional resources on everything from online lectures to tools for using Twitter.\n- 100 Free Online Lectures that Will Make You a Better Teacher. This blog post shares online lectures that can inspire you as a teacher.\n- 50 Online Reference Sites for Teachers. This listing is a great resource for teachers looking for quality online resources for students and educators.\n- 122 For You: Cool Cat Teacher’s Favorite Apps, Software, and Sites. In order to give back to a community that has taught this teacher so much, she has compiled a list of all the apps, software, and sites that she finds helpful both professionally and personally.\n- Friday Five – Finding Ada (Role Models of Women in Tech). Get several sites that present women as role models in the field of technology from this blog post.\n- Get Smart: Receive Daily Learning Inspiration. This blog post tells you how to sign up to receive daily updates with information about learning resources, books, and more.\n- 100 Tips, Apps, and Resources for Teachers on Twitter. Whether you are just starting out or are an experienced Twitter user, these resources will offer something for you.\n- Top 100 Tools for the Twittering Teacher. Get 100 great tools to enhance your Twitter experience at school.\n- Digital Image Resources on the Deep Web. This blog post offers tons of links to resources offering quality digital images.\n- Student Tools – Let them fly!. This teacher has compiled a good list of web 2.0 tools students can use in the classroom to enhance their learning.","Researchers raise awareness about interconnectedness, privacy risks of online services\nIf you are a frequent user of technology, something like this probably happens to you every day: You search the web for articles on the nutrition needs of your new puppy and almost immediately ads from pet food companies start flooding your social accounts and web pages.\nPerhaps even more mysteriously, you check the daily news and read an article about the latest science behind obesity, and the next thing you know an offer from a national weight loss company appears in front of you, even though you didn't search, share or otherwise \"mark\" your experience with that article.\nWe laugh, feel violated, maybe even get a little indignant and spout off in a social media post, but research shows many of us accept these invasions of privacy in exchange for the convenience of technology. We may even appreciate some of this targeted messaging, like a movie recommendation or the link to a cheaper source for that pair of shoes we want.\nMeanwhile, companies are working to amass huge amounts of data about us, pretty much unchecked—at least for now—say researchers at the University of Michigan and Fordham Law School. The researchers, with support from AT&T, hope to help the public better understand the mechanism—so-called Application Programming Interfaces, or APIs—that allows data sharing that leaves us vulnerable to breaches like those recently making news at Facebook and Google.\nThomas Norton, executive director of Fordham Law's Center on Law and Information Policy, and Florian Schaub, assistant professor at the U-M School of Information, will present their report, \"APIs and Your Privacy,\" at the AT&T Policy Forum's Symposium on Application Programing Interfaces and Privacy 1-3 p.m. Feb. 5 in Washington, D.C.\n\"It seems no day goes by without hearing about a new data breach or unexpected sharing of personal data. APIs play an important role in many of those cases as well as in our digital economy,\" Schaub said. \"Our goal with this report is to help consumers and policy makers understand what APIs are, what APIs companies offer and what the privacy implications of APIs are.\"\nNorton said that in a broader sense, they hope the research \"will reveal how vast the data collection business model is and how it's often difficult for consumers to detect when their information is being collected. As has been often said: when it comes to free online services, people are the product. We hope our report helps readers internalize this concept.\"\nApplication Programing Interfaces allow software programs to communicate with one another. A website may have a feature or tool its developers want to make available to other websites or applications.\nIn their report, the authors use the example of a kayak rental website that, for the convenience of customers, links to a weather site. An API allows the customer to know the forecast for the day when interacting with the primary site, but it also gives the weather service an idea of who is visiting the kayak rental site.\nAPIs allow us to view a YouTube video through Facebook, watch an ad for another computer game in order to gain more \"lives\" in the one we're playing, and search a company website that uses Google technology.\nThey are used in mobile games, search engines, social media platforms, news and shopping websites, video and music streaming services, dating apps and mobile payment systems.\n\"Not everyone gets the chance to see just how complex the web is—any time you visit one website, you're likely communicating with dozens of other sites, too. And those extra, invisible connections create a lot of opportunities for data collection,\" said Allison McDonald, a co-author of the report and U-M doctoral candidate in computer science and engineering.\nFor their report and presentation, the researchers examined 11 prominent online services to demonstrate the role APIs play in gathering and distributing consumer information. These include the Candy Crush Saga mobile game, Google Search, Facebook, CNN.com, Netflix, Pandora, Amazon.com, Google Maps, the Tinder dating app, ESPN and the mobile payment app Venmo.\nThe researchers said a major concern is large companies like Facebook that gather so much information about people, making them \"juicy targets for hackers.\"\n\"Companies should not be allowed to collect information about us without our knowledge, for reasons we don't expect and without giving us a meaningful opportunity to opt out,\" Norton said.\nThe researchers also explain the various ways APIs are used, how breaches like Cambridge Analytica can occur and how companies like Facebook have responded.\n\"In the 10 or so months since Cambridge Analytica, we've seen even more reports about Facebook, Google and others sharing user data in a way and at a scale that's shocking and sometimes dangerous to consumers,\" McDonald said. \"I think this is a good indicator of how this ecosystem works and will continue to work.\n\"Companies will profit off of data sharing and obscure their arrangements until they're found out. The short of it: I'm unconvinced companies are tightening control on the whole. I think they're patching holes here and there when there's media attention. And that's a good reason for us to continue talking about this topic and exploring technical and policy solutions.\"\nSchaub said consumers can do a few things to better protect themselves, ranging from considering the business model of a service they use, what might be inferred from their data and behavior, and protecting themselves with privacy browser extensions.\n\"It's tough to be entirely invisible on the internet because the methods of collecting data from your online habits are changing as fast as the web is,\" McDonald said. \"But there are some things that make it harder for companies to track us and that keep us safer.\n\"Using an Adblocker like Adblock Plus or uBlock Origin means you'll be making fewer of those invisible connections while you browse. A 'tracker blocker' browser extension like Ghostery limits those connections, too.\""],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:9f76699e-ae18-4472-940a-200305c25e36>","<urn:uuid:8c29a3e8-f065-41e8-abca-86251d91e5b7>"],"error":null}
{"question":"Hi! I've been learning about blood diseases. Can you explain the difference between mesenteric infarction and polycythaemia vera in terms of their main symptoms and causes?","answer":"Mesenteric infarction and polycythaemia vera have distinct symptoms and causes. Mesenteric infarction is caused by sudden reduction of blood supply to the intestinal wall, with symptoms including epigastric pain, nausea, emesis and diffuse abdominal pain. It can be caused by various factors like thrombosis of mesenteric veins, volvulus, or arterial stenosis. In contrast, polycythaemia vera arises from a single haematopoietic progenitor cell with JAK-2 mutations (present in over 95% of patients), leading to excessive red cell proliferation. Its symptoms include headache, dizziness, tinnitus, visual disturbance, angina pectoris, intermittent claudication, pruritus and venous thrombosis. Physical signs include a plethoric complexion and hepatosplenomegaly.","context":["- Meeting abstract\n- Open Access\nBMC Geriatrics volume 9, Article number: A61 (2009)\nMesenteric infarction is a necrosis of the intestinal wall due to a sudden reduction of the blood supply.\nThe different aspects of intestinal ischemia will be reviewed after the presentation of three emblematic cases led to our attention.\nMaterials and methods\nA 70 years old man was admitted to our department for the onset of epigastric pain. The CT scan showed a thrombosis of the superior and inferior mesenteric veins and of the right branch of the portal vein with some branches for the segments VIII, VII, IV, and VI also involved.\nThe patient started anticoagulant therapy with intravenous sodic heparin and then switched to oral anticoagulants. Further investigations proved a heterozygosis G20210A for mutated prothrombin.\nWe report the case of a 63 years-old woman, with tracheostomy, gastrostomy and ciecostomy after a long recovery in intensive care unit. She was admitted for the onset of diffuse abdominal pain, with nausea, emesis and stipsis (despite of the ciecostomy). The clinical conditions worsened and she underwent laparotomic exploration in emergency. The occlusion was due to a volvulus of the small bowel and the necrosis was extended from the III jejunal loop to the last ileal loop. A latero-lateral ileo-trasversostomy was then performed.\nA 56 years-old man with a story of hypertension, chronic renal failure and consequent dialysis and parathyroidectomy for secondary hyperparathyroidism was admitted in our Department for the onset of recurrent diffuse abdominal pain during the dialytic treatment, displaying intestinal subocclusive crisis. The abdominal x-ray film showed severe calcifications of the aortic and visceral vessels walls. The color-Doppler of the digestive arteries showed a severe stenosis of the origin of the superior mesenteric artery (VPS 360 cm/sec). A transfemoral stenting of the stenosis of the superior mesenteric artery was then performed successfully.\nWithout radiologic or clinical evidence of transmural infarction of the bowel, the therapy for venous intestinal ischemia is conservative and based on anticoagulant medical therapy and this pathology arises frequently in a setting of unknown thrombophilia. We believe surgical thrombectomy must be considered only in case of infarction. The possibility of percutaneous treatment of mesenteric venous thrombosis is reported, through mechanic thrombectomy procedures or pharmacological fibrinolysis.\nIntestinal ischemia can be caused by mechanical obstruction of the mesentery as for a volvulus. Surgical intervention is mandatory and wide resections are frequent.\nThe third case shows an acute and reversible mesenteric syndrome, presumably due to a theft syndrome with the onset of abdominal pain during dialysis, solved with a minimally invasive procedure instead of surgical revascularization. This procedure on such patients is even harder for the extent of the abdominal arterial calcifications.\nThe variable etiology of the intestinal ischaemia is well related to the different therapeutic approaches (medical, surgical or endovascular). The situation of the intestinal walls can range from a suffering intestinal loop that requires revascularization to a definitive necrotic tract of bowel for which a resection becomes mandatory. Endovascular techniques seem to provide an efficient alternative to traditional surgery, especially for chronic or acute mesenteric ischemia when there is no evidence of infarction or necrosis.\nAbout this article\nCite this article\nToni, F., Patrizi, G. & Redler, A. Mesenteric infarction. BMC Geriatr 9, A61 (2009). https://doi.org/10.1186/1471-2318-9-S1-A61\n- Superior Mesenteric Artery\n- Secondary Hyperparathyroidism\n- Mesenteric Ischemia\n- Mechanic Thrombectomy","In these disorders there is uncontrolled clonal proliferation of one or more of the cell lines in the bone marrow, namely erythroid, myeloid and megakaryo-cyte lines. Myeloproliferative disorders include polycythaemia vera, essential thrombocythaemia, myelofibrosis (all of which have an mutation of the gene Janus kinase 2, JAK-2) and chronic myeloid leukaemia. These disorders are grouped together as there can be transition from one disease to another; for example, polycythaemia vera can lead to myelofibrosis, and they may also transform into acute leukaemia. They occur principally in middle-aged and elderly people. They differ from the acute leukaemias (also clonal proliferation of a single cell line), where the cells also do not differentiate normally but where there is progressive accumulation of immature cells.\nPolycythaemia is defined as an increase in Hb, PCV or RCC. These measure-ments are all concentrations and are therefore directly dependent on plasma volume as well as red blood cell mass. The production of red cells by the bone marrow is normally regulated by the hormone erythropoietin, which is produced in the kidney. The stimulus for erythropoietin production is tissue hypoxia.\nIn absolute polycythaemia there is an increase in the red cell mass. Primary polycythaemia is due to an acquired or inherited mutation leading to an abnormality within red blood progenitors. It includes polycythaemia vera\nFig. 5.4 The causes of polycythaemia.\nand rare familial variants. Secondary polycythaemia is caused by an erythropoietin response to chronic hypoxia or by an erythropoietin-secreting tumour (Fig. 5.4).\nRelative or apparent polycythaemia (Gaisbock’s syndrome) occurs in middle-aged obese men and is associated with smoking, increased alcohol intake and hypertension. The PCV is normal but plasma volume is decreased.\nPolycythaemia vera arises from a single haematopoietic progenitor cell and leads to excessive proliferation of red cells and a variable increase in platelets and myeloid cells. JAK-2 mutations, present in over 95% of patients, lead to constitutive activation of its tyrosine kinase activity, which plays a pivotal role in cell proliferation and survival.\nSymptoms and signs are the result of hypervolaemia and hyperviscosity. Typical symptoms include headache, dizziness, tinnitus, visual disturbance, angina pectoris, intermittent claudication, pruritus and venous thrombosis. Physical signs include a plethoric complexion and hepatosplenomegaly as a result of extramedullary haemopoiesis. Splenomegaly, if present, reliably distinguishes polycythaemia vera from secondary polycythaemia. There is an increased risk of haemorrhage as a result of friable haemostatic plugs, and gout caused by increased cell turnover and uric acid production.\nBlood count showing raised white cell and platelet counts is suggestive of polycythaemia vera as opposed to other causes of polycythaemia. The diag-nosis is made by demonstrating evidence of increased red cell volume (usually Hb >18.5 g/dL in men and >16.5 g/dL in women) and a gain of function mutation in JAK-2 (e.g. V617F) together with one of:\n■ Erythroid hyperplasia, with increased numbers of megakaryocytes and granulocytes on bone marrow examination\n■ Serum erythropoietin levels below normal\n■ Erythroid colony formation in vitro in the absence of exogenous erythropoietin\nThere is no cure, and treatment is given to maintain a normal blood count and to prevent the complications of the disease, particularly thrombosis and haemorrhage.\n■ Venesection to maintain PCV <0.45 L/L. Regular venesection (e.g. 3-monthly) may be all that is needed in many patients.\n■ Chemotherapy. Hydroxycarbamide (hydroxyurea) and busulfan are used to reduce the platelet count.\n■ Low-dose aspirin with the above treatments is used for patients with recurrent thrombotic episodes.\n■ Radioactive phosphorus (32P) is only given to patients over 70 years of age because of the increased risk of leukaemic conversion with its use.\n■ Allopurinol is given to decrease uric acid levels.\nSecondary polycythaemia presents with similar clinical features to primary polycythaemia, although the white cell and platelet counts are usually normal and the spleen is not enlarged. In patients with tumours the primary disease must be treated to lower the level of erythropoietin. In hypoxic patients, oxygen therapy (p. 520) may reduce the Hb, and a small-volume phlebotomy (400 mL) may help those with severe symptoms. Smokers should be advised to stop smoking.\nPatients have normal Hb levels and white cell count but elevated platelet count. Platelet size and function are abnormal, and presentation may be with bleeding or thrombosis. Differential diagnosis is from secondary causes of a\nTable 5.12 Differential diagnosis of a raised platelet count\nAutoimmune rheumatic disorders\nInílammatory bowel disease\nSplenectomy and functional hyposplenism\nraised platelet count and other myeloproliferative disorders (Table 5.12). There is no global gold standard diagnostic test but in general an otherwise well person with a platelet count of >1000 X 109/L will have essential thrombocythaemia. Busulfan, hydroxycarbamide (hydroxyurea), anagrelide or inter^eron alfa are used to reduce platelet production.\nMyelofibrosis is characterized by haemopoietic stem cell proliferation associated with marrow fibrosis (abnormal megakaryocyte precursors release fibroblast-stimulating factors, such as platelet-derived growth factor).\nThere is an insidious onset of weakness, weight loss and lethargy. Bleeding occurs in the thrombocytopenic patient. There is hepatomegaly and massive splenomegaly caused by extramedullary haemopoiesis.\n■ Blood count shows anaemia. The white cell and platelet counts are high initially, but fall with disease progression as a result of marrow fibrosis.\n■ Blood film examination shows a leucoerythroblastic picture (immature red cells caused by marrow infiltration) and ‘teardrop'-shaped red cells.\n■ Bone marrow is usually unobtainable by aspiration (‘dry tap'); trephine biopsy shows increased fibrosis.\n■ The Philadelphia chromosome is absent; this and the bone marrow appearance helps to distinguish myelofibrosis from chronic myeloid leukaemia, which may present similarly.\n■ Transfusions are given for anaemia and allopurinol to decrease serum uric acid levels.\n■ Hydroxycarbamide (hydroxyurea) or busulfan are used to reduce the raised white cell and platelet count.\n■ Splenic irradiation may be useful to reduce a large painful spleen.\n■ Splenectomy is performed if the spleen is very large and painful and the transfusion requirements are high.\nMyelodysplasia is a group of acquired bone marrow disorders caused by a defect in stem cells. There is progressive bone marrow failure, which tends to evolve into acute myeloid leukaemia. The myelodysplastic syndromes are predominantly diseases of the elderly, and are increasingly being diagnosed when a routine full blood count shows an unexplained macrocytosis, anaemia, thrombocytopenia or neutropenia. The diagnosis is made on the basis of characteristic blood film and bone marrow appearances. The Paradox of peripheral pancytopenia and a hypercellular bone marrow reflects premature cell loss by apoptosis.\nSupportive treatment (red cell and platelet transfusions) is given to elderly patients with symptomatic disease. For younger patients, intensive chemo-therapy (as used for acute myeloblastic leukaemia) or allogeneic BMT are used. Lenalidomide (a thalidomide analogue) is used in the treatment of early-stage disease.\n1. Ethics and communication\n2. Infectious diseases\n3. Gastroenterology and nutrition\n4. Liver, biliary tract and pancreatic disease\nLiver, biliary tract and pancreatic disease\nLIVER BIOCHEMISTRY AND LIVER FUNCTION TESTS\nSYMPTOMS AND SIGNS OF LIVER DISEASE\nNON - ALCOHOLIC FATTY LIVER DISEASE (NAFLD)\nCOMPLICATIONS AND EFFECTS OF CIRRHOSIS\nTYPES OF CHRONIC LIVER DISEASE AND CIRRHOSIS\nPRIMARY SCLEROSING CHOLANGITIS\nBUDD - CHIARI SYNDROME\nLIVER DISEASE IN PREGNANCY\nCARCINOMA OF THE PANCREAS\nNEUROENDOCRINE TUMOURS OF THE PANCREAS\n5. Haematological disease\nAssessment and treatment of suspected neutropenic sepsis\nINHERITED HAEMOLYTIC ANAEMIAS\nACQUIRED HAEMOLYTIC ANAEMIA\nTHE WHITE CELL\nHAEMOSTASIS AND THROMBOSIS\n6. Malignant disease\nCOMMON INVESTIGATIONS IN MUSCULOSKELETAL DISEASE\nCOMMON REGIONAL MUSCULOSKELETAL PROBLEMS\nTHE SERONEGATIVE SPONDYLOARTHROPATHIES\nClinical features, Investigations\nINFECTION OF JOINTS AND BONES\nAUTOIMMUNE RHEUMATIC DISEASES\nSYSTEMIC INFLAMMATORY VASCULITIS\nDISEASES OF BONE\n8. Water, electrolytes and acid–base balance\nWATER AND ELECTROLYTE REQUIREMENTS\nBODY FLUID COMPARTMENTS\nREGULATION OF BODY FLUID HOMEOSTASIS\nPLASMA OSMOLALITY AND DISORDERS OF SODIUM REGULATION\nDISORDERS OF POTASSIUM REGULATION\nDISORDERS OF MAGNESIUM REGULATION\nDISORDERS OF ACID - BASE BALANCE\n9. Renal disease\nINVESTIGATION OF RENAL DISEASE\nURINARY TRACT INFECTION\nHYPERTENSION AND THE KIDNEY\nRENAL CALCULI AND NEPHROCALCINOSIS\nURINARY TRACT OBSTRUCTION\nACUTE RENAL FAILURE/ACUTE KIDNEY INJURY\nCHRONIC KIDNEY DISEASE\nRENAL REPLACEMENT THERAPY\nCYSTIC RENAL DISEASE\nTUMOURS OF THE KIDNEY AND GENITOURINARY TRACT\nDISEASES OF THE PROSTATE GLAND\n10. Cardiovascular disease\nCOMMON PRESENTING SYMPTOMS OF HEART DISEASE\nINVESTIGATIONS IN CARDIAC DISEASE\nISCHAEMIC HEART DISEASE\nVALVULAR HEART DISEASE\nPULMONARY HEART DISEASE\nARTERIAL AND VENOUS DISEASE\nDRUGS FOR ARRHYTHMIAS\nDRUGS FOR HEART FAILURE\nDRUGS AFFECTING THE RENIN - ANGIOTENSIN SYSTEM\nNITRATES, CALCIUM - CHANNEL BLOCKERS AND POTASSIUM - CHANNEL ACTIVATORS\n11. Respiratory disease\n12. Intensive care medicine\n13. Drug therapy, poisoning, and alcohol misuse\n14. Endocrine disease\nPITUITARY HYPERSECRETION SYNDROMES\nTHE THYROID AXIS\nMALE REPRODUCTION AND SEX\nFEMALE REPRODUCTION AND SEX\nTHE GLUCOCORTICOID AXIS\nTHE THIRST AXIS\nDISORDERS OF CALCIUM METABOLISM\nDISORDERS OF PHOSPHATE CONCENTRATION\nENDOCRINOLOGY OF BLOOD PRESSURE CONTROL\nDISORDERS OF TEMPERATURE REGULATION\n15. Diabetes mellitus and other disorders of metabolism\n16. The special senses\nCOMMON NEUROLOGICAL SYMPTOMS\nCOORDINATION OF MOVEMENT\nTHE CRANIAL NERVES\nCOMMON INVESTIGATIONS IN NEUROLOGICAL DISEASE\nUNCONSCIOUSNESS AND COMA\nSTROKE AND CEREBROVASCULAR DISEASE\nEPILEPSY AND LOSS OF CONSCIOUSNESS\nNERVOUS SYSTEM INFECTION AND INFLAMMATION\nHEADACHE, MIGRAINE AND FACIAL PAIN\nSPINAL CORD DISEASE\nDEGENERATIVE NEURONAL DISEASES\nDISEASES OF THE PERIPHERAL NERVES"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:8ad7a984-8ab2-4c25-adbc-a559d8c9736f>","<urn:uuid:b35e83de-0917-43d9-a351-2cb10fa5f06b>"],"error":null}
{"question":"How do the stormwater management approaches at Duke University compare to New York City's initiatives, particularly in terms of their infrastructure and environmental impact?","answer":"Both institutions have comprehensive stormwater management systems, but with different focuses. Duke University maintains approximately 32 miles of storm pipes and drainage ditches, and is expanding its capabilities through an on-stream pond project that will serve both for stormwater treatment and as a cooling source for their Chilled Water system. They are specifically focusing on water reclamation and reducing potable water use. Meanwhile, NYC's DEP has developed a broader, research-based approach, studying stormwater programs from 34 communities worldwide to develop integrated solutions. The DEP's approach emphasizes regulatory compliance and flood mitigation, with their findings published in the 'Innovative and Integrated Stormwater Management' report to help other communities develop their own programs. Both systems demonstrate that stormwater management techniques vary based on specific community conditions.","context":["Facing a Changing Climate Report Documents Stormwater Management Efforts in Cities Across the Globe\nGroundbreaking Report will Serve as a Roadmap for Communities Looking for Stormwater Solutions\nPhotos of Stormwater Management Techniques are Available on DEP’s Flickr Page\nThe New York City Department of Environmental Protection (DEP), in partnership with the Water Research Foundation, recently released a new report entitled Innovative and Integrated Stormwater Management. The report is an accumulation of DEP’s examination of stormwater programs from across the country and around the globe, including vital data sharing between New York City and 34 communities. The conversations included lessons learned and the challenges associated with flood mitigation programs, integrated stormwater management initiatives and regulatory compliance. The Innovative and Integrated Stormwater Management report can be found here.\n“Managing stormwater and protecting public health means preparing our communities for a changing climate, but not every program will work for every municipality,” said DEP Acting Commissioner Vincent Sapienza. “By studying the efforts of cities across the country and around the globe, including the challenges they encountered and lessons learned, we believe this report can serve as a roadmap for communities looking for the right approach to better managing stormwater.”\nDEP examined national and international stormwater programs to enhance its understanding and refine New York City’s approach to a stormwater program that is both integrated and innovative. The Water Research Foundation is publishing and distributing this report on behalf of DEP as part of the focus on One Water. By sharing the Innovative and Integrated Stormwater Management report, the partners hope that other service providers can use it to cost-effectively improve the health of local water bodies and deliver related benefits to their communities.\n“It is incredibly generous of DEP to share this comprehensive report with the public,” said Rob Renner, CEO of the Water Research Foundation. “Many other communities can benefit from this guidance and the Water Research Foundation is thrilled to be part of this effort.”\nThe report topics encompass stormwater control measures recommended by the U.S. Environmental Protection Agency, as well as topics that are critical to stormwater program success or those that can result in progressive and innovative water quality solutions. Each chapter covers a stormwater program topic; the role stormwater has in the program; an introduction to the applicable U.S. federal regulations; and common high-interest factors that influence decision making when it comes to setting up and implementing the program. Within each chapter, case studies are presented for a sampling of the communities with progressive programs in the topic area. The effort also demonstrates that although many of the surveyed communities are actively implementing the stormwater control measures, their techniques vary based on the conditions that are specific to their community.\nAbout DEP: DEP provides approximately 1 billion gallons of high quality drinking water each day to more than 9 million residents. The watershed extends more than 125 miles from the city, comprising 19 reservoirs and three controlled lakes. Approximately 7,000 miles of water mains, tunnels and aqueducts deliver water throughout the five boroughs, and 7,500 miles of sewer lines and 96 pump stations take wastewater to 14 treatment plants. DEP has nearly 6,000 employees. In addition, DEP has a robust capital program, with a planned $20.7 billion in investments over the next 10 years that will create up to 3,000 construction-related jobs per year.\nAbout the Water Research Foundation (WRF): is a non-profit research cooperative that advances the science of water to protect public health and the environment. Governed by utilities, WRF delivers scientifically sound research solutions and knowledge to serve our subscribers and stakeholders in all areas of drinking water, wastewater, stormwater, and reuse. WRF has funded and managed more than 1,500 research studies from asset management to treatment, utility finance to resource management, conveyance systems to water quality. For more information go to waterrf.org.","Duke is organized as a district heating and cooling system. Buildings at Duke are heated by steam and cooled with chilled water, both of which are generated on-campus at central plants. Electricity from the electric utility company provides power to the campus.\nThe University also manages water and sewer on its campuses. The Durham campus is connected to Durham's municipal water system.\nAll utilities are managed by the Duke Utilities & Engineering Services (DUES) group within the Facilities Management Department. Content on this page refers to Duke's Durham campus only.\nThe Duke Utilities & Engineering Services group performs utility billing and administrative services, planning, operation, maintenance, and expansion of the University's infrastructure from the point of generation to the point to of delivery (building interface).\nEnergy needs at Duke are met by three major utilities:\n- Chilled Water\nFrom 2004 to 2012, Duke University developed the majority of a Central Chilled Water System that consists of two central plants (42,000 ton cooling capacity) and over 14 miles of distribution piping. After leaving each plant, chilled water runs to each building on campus, and then through various pipes and air handling equipment to cool the buildings. After leaving a building, the previously chilled water returns via closed loop to a chilled water plant for reuse. The system has what is known as a variable-primary arrangement, meaning that all pumping is done at the plant, requiring no additional pump equipment in buildings.\nThe centralized production and distribution of chilled water is the most efficient and economical method to cool Duke University's campus including the Schools of Medicine and Nursing and the Medical Center. The system provides chilled water for process equipment cooling and building air conditioning in a reliable and cost effective manner. Though chilled water is primarily thought of as an air-conditioning cooling source, the system operates year round to keep up with the demands of campus buildings.\nElectricity enters Duke's campus through a ring of five distribution substations, which are supplied with power from Duke Energy (the university and the utility company share no business connection other than name). Electrical power is distributed within the bounds of campus at 15,000 Volts.\nThe High Voltage team is responsible for the installation, operation, maintenance and distribution of high voltage electric power on campus. The group supports the University, Hospital and Medical Center to maintain the integrity of the electric system on campus, which consists of the five distribution substations, over 15 miles of duct banks and 62 miles of electrical cables.\nThis group also maintains the exterior lighting systems on campus, provides electric meter reading services, and maintains Duke's single traffic light.\nBuilding secondary lower voltage service and interior electrical systems are handled by the Building Maintenance Electrical Shop.\nSteam is the main heat source for buildings on campus.\nGenerated at two different campus plants, the boilers provide high-pressure steam to both East and West Campuses and the Medical Center. Both plants serve a common distribution system to increase system reliability. LIke the Chilled Water system, the Steam System operates year round to meet campus demands.\nSteam is used for space conditioning, hot water heating, hospital medical equipment sterilization, dining services, pool heating, dishwashing and other process uses. Duke Utilities and Engineering Services (DUES) maintains and operates approximately 35 miles of steam and condensate piping and associated manholes. DUES is also responsible for steam metering stations, pressure reducing stations, and condensate return systems, located within campus buildings.\nWest Campus Steam Plant\nDuke's West Campus Steam Plant was built in 1929 as part of Horace Trumbauer's original plan for Duke's West Campus. In February 2009, the plant ended an 80-year tradition of coal delivery by rail car and began its step toward reducing Duke's use of coal. In April of 2011 the West Campus Plant burned its last load of coal and began utilizing natural gas. This fuel change was an important step in the execution of the Duke Climate Action Plan. With the renovation that enabled the fuel change, the West Campus Steam Plant received significant upgrades in auxiliary equipment, including the addition of a condensing economizer. This device allows for significant energy recovery and maximized plant efficiency.\nEast Campus Steam Plant\nDuke's East Campus Steam Plant was built in 1929 and is located on the eastern end of Campus Drive, near Smith Warehouse. The 6,600-square-foot plant was used to heat Duke's buildings from 1929 to 1978. In May 2008 the Duke Board of Trustees approved funding to renovate the plant to add capacity to the steam system, provide for future growth and burn natural gas, which produces lower emissions and greenhouse gases than coal. Renovations were completed in August 2010.\nThe renovated plant features 14 smaller scale natural-gas boilers, which require less water and time to produce steam and also utilize propane as a backup fuel.\nDuke's civil infrastructure consists of a vast array of water, drainage, and sewer systems, as well as paved roads, walks and parking lots, all serving the academic, research, and residential buildings and lands of Duke.\nRoads, Sidewalks and Paths\nDuke maintains over 18 miles of private roads and 24 miles of sidewalks, along with associated paths throughout its academic and residential areas.\nWater, Sewer, & Storm Drainage\nDuke maintains approximately 34 miles of City Water and Sewer Lines. Duke's storm water runoff is collected in its storm drainage system. The storm drainage system consists of an extensive system of nearly 32 miles of storm pipes and drainage ditches.\nStorm Water Reclamation\nDuke Utilities and Engineering Services is currently expanding campus storm water treatment and reclamation capabilities through the installation of an on-stream pond. In addition to improving runoff water quality, this pond will serve as the primary cooling source for the Chilled Water system and will significantly reduce potable water use on campus. When complete, this 13-acre site will include research facilities for the academic community and amenities for recreation."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:9a623541-5cdc-47d0-b8c9-cc77a26c2dcd>","<urn:uuid:d36151db-8df8-48cc-bfdd-42d380969a17>"],"error":null}
{"question":"How have multivitamin supplement consumption patterns changed across different age groups in America, and what are the demographic factors that influence their use?","answer":"Research shows that more than one-third of Americans take MVMs, with usage patterns varying significantly by age. About 25% of young children take MVMs, while adolescents are least likely to use them. Usage increases with age during adulthood, reaching over 40% by age 71. Demographic factors strongly influence MVM use - women, elderly individuals, people with higher education and income, those with healthier diets and lifestyles, those with lower body weights, and residents of the western United States are the most frequent users. Conversely, smokers and certain ethnic and racial groups including African Americans, Hispanics, and Native Americans are less likely to take daily MVMs.","context":["What are multivitamin/mineral (MVM) dietary supplements?\nMultivitamin/mineral (MVM) supplements contain a combination of vitamins and minerals, and sometimes other ingredients as well. They go by many names, including multis and multiples or simply vitamins. The vitamins and minerals in MVMs have unique roles in the body. For more information about each one, see our dietary supplement fact sheets.\nWhat kinds of MVM supplements are available?\nThere are many types of MVMs in the marketplace. Manufacturers choose which vitamins, minerals, and other ingredients, as well as their amounts, to include in their products.\nAmong the most common MVMs are basic, once-daily products containing all or most vitamins and minerals, with the majority in amounts that are close to recommended amounts. Higher-potency MVMs often come in packs of two or more pills to take each day. Manufacturers promote other MVMs for special purposes, such as better performance or energy, weight control, or improved immunity. These products usually contain herbal and other ingredients (such as echinacea and glucosamine) in addition to vitamins and minerals.\nThe recommended amounts of nutrients people should get vary by age and gender and are known as Recommended Dietary Allowances (RDAs) and Adequate Intakes (AIs). One value for each nutrient, known as the Daily Value (DV), is selected for the labels of dietary supplements and foods. A DV is often, but not always, similar to one's RDA or AI for that nutrient. The label provides the %DV so that you can see how much (what percentage) a serving of the product contributes to reaching the DV.\nWho takes MVM supplements?\nResearch has shown that more than one-third of Americans take MVMs. About one in four young children takes an MVM, but adolescents are least likely to take them. Use increases with age during adulthood so that by age 71 years, more than 40% take an MVM.\nWomen; the elderly; people with more education, more income, healthier diets and lifestyles, and lower body weights; and people in the western United States use MVMs most often. Smokers and members of certain ethnic and racial groups (such as African Americans, Hispanics, and Native Americans) are less likely to take a daily MVM.\nWhat are some effects of MVMs on health?\nPeople take MVMs for many reasons. Here are some examples of what research has shown about using them to increase nutrient intakes, promote health, and reduce the risk of disease.\nIncrease nutrient intakes\nTaking an MVM increases nutrient intakes and helps people get the recommended amounts of vitamins and minerals when they cannot or do not meet these needs from food alone. But taking an MVM can also raise the chances of getting too much of some nutrients, like iron, vitamin A, zinc, niacin, and folic acid, especially when a person uses more than a basic, once-daily product.\nSome people take an MVM as a form of dietary or nutritional \"insurance.\" Ironically, people who take MVMs tend to consume more vitamins and minerals from food than those who don't. Also, the people least likely to get enough nutrients from diet alone who might benefit from MVMs are the least likely to take them.\nHealth promotion and chronic disease prevention\nFor people with certain health problems, specific MVMs might be helpful. For example, a study showed that a particular high-dose formula of several vitamins and minerals slowed vision loss in some people with age-related macular degeneration. Although a few studies show that MVMs might reduce the overall risk of cancer in certain men, most research shows that healthy people who take an MVM do not have a lower chance of getting cancer, heart disease, or diabetes. Based on current research, it's not possible to recommend for or against the use of MVMs to stay healthier longer.\nOne reason we know so little about whether MVMs have health benefits is that studies often use different products, making it hard to compare their results to find patterns. Many MVMs are available, and manufacturers can change their composition at will. It is therefore difficult for researchers to study whether a specific combination of vitamins and minerals affects health. Also, people with healthier diets and lifestyles are more likely to take dietary supplements, making it hard to identify any benefits from the MVMs.\nShould I take an MVM?\nMVMs cannot take the place of eating a variety of foods that are important to a healthy diet. Foods provide more than vitamins and minerals. They also have fiber and other ingredients that may have positive health effects. But people who don't get enough vitamins and minerals from food alone, are on low-calorie diets, have a poor appetite, or avoid certain foods (such as strict vegetarians and vegans) might consider taking an MVM. Health care providers might also recommend MVMs to patients with certain medical problems.\nSome people might benefit from taking certain nutrients found in MVMs. For example:\n- Women who might become pregnant should get 400 mcg/day of folic acid from fortified foods and/or dietary supplements to reduce the risk of birth defects of the brain and spine in their newborn babies.\n- Pregnant women should take an iron supplement as recommended by their health care provider. A prenatal MVM is likely to provide iron.\n- Breastfed and partially breastfed infants should receive vitamin D supplements of 400 IU/day, as should non-breastfed infants who drink less than about 1 quart per day of vitamin D-fortified formula or milk.\n- In postmenopausal women, calcium and vitamin D supplements may increase bone strength and reduce the risk of fractures.\n- People over age 50 should get recommended amounts of vitamin B12 from fortified foods and/or dietary supplements because they might not absorb enough of the B12 that is naturally found in food.\nCan MVMs be harmful?\nTaking a basic MVM is unlikely to pose any risks to health. But if you consume fortified foods and drinks (such as cereals or beverages with added vitamins and minerals) or take other dietary supplements, make sure that the MVM you take doesn't cause your intake of any vitamin or mineral to go above the upper levels. (Use the Online DRI tool to learn the upper level of each nutrient.)\nPay particular attention to the amounts of vitamin A, beta-carotene (which the body can convert to vitamin A), and iron in the MVM.\n- Women who get too much vitamin A during pregnancy can increase the risk of birth defects in their babies. This risk does not apply to beta-carotene, however. Smokers, and perhaps former smokers, should avoid MVMs with large amounts of beta-carotene and vitamin A because these ingredients might increase the risk of developing lung cancer.\n- Adult men and postmenopausal women should avoid taking MVMs that contain 18 mg or more of iron unless their doctor has told them that they have iron deficiency or inadequacy. When the body takes in much more iron than it can eliminate, the iron can collect in body tissues and organs, such as the liver and heart, and damage them. Iron supplements are a leading cause of poisoning in children under age 6, so keep any products containing iron (such as children's chewable MVMs or adults' iron supplements) out of children's reach.\nAre there any interactions with MVMs that I should know about?\nMVMs with recommended intake levels of nutrients don't usually interact with medications, with one important exception. If you take medicine to reduce blood clotting, such as warfarin (Coumadin® and other brand names), talk to your health care provider before taking any MVM or dietary supplement with vitamin K. Vitamin K lowers the drug's effectiveness and doctors base the medicine dose partly on the amount of vitamin K you usually consume in foods and supplements.\nWhich kind of MVM should I choose?\nTalk to a health care provider to help you figure out whether you should take an MVM and, if so, which one is best for you. Consider basic MVMs whose amounts of most or all vitamins and minerals do not go above the DVs. These MVMs usually have low amounts of calcium and magnesium, so some people might need to take one or both minerals separately. Make sure that the product does not have too much vitamin A and iron.\nAlso consider choosing an MVM designed for your age, sex, and other factors (like pregnancy). MVMs for men often contain little or no iron, for example. MVMs for seniors usually provide more calcium and vitamins D and B12 and less iron than MVMs for younger adults. Prenatal MVMs for pregnant women often provide vitamin A as beta-carotene.\nMVMs and healthful eating\nPeople should get most of their nutrients from food, advises the federal government's Dietary Guidelines for Americans. Foods contain vitamins, minerals, dietary fiber and other substances that benefit health. In some cases, fortified foods and dietary supplements may provide nutrients that otherwise may be consumed in less-than-recommended amounts. For more information about building a healthy diet, refer to the Dietary Guidelines for Americans and the U.S. Department of Agriculture's MyPlate.\nWhere can I find out more about MVMs?\n- For general information on MVMs:\n- For information on recommended intakes of vitamins and minerals:\n- For more advice on buying dietary supplements:\n- For information about building a healthy diet:\nThis fact sheet by the Office of Dietary Supplements provides information that should not take the place of medical advice. We encourage you to talk to your healthcare providers (doctor, registered dietitian, pharmacist, etc.) about your interest in, questions about, or use of dietary supplements and what may be best for your overall health. Any mention in this publication of a specific brand name is not an endorsement of the product.\nUpdated: February 17, 2016"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:2681394a-36b6-439b-8e92-dc1013b2fefc>"],"error":null}
{"question":"As global seas rise, what are the projected impacts on coastal infrastructure in West Africa compared to the existing damage patterns in countries like Bangladesh? Could existing early warning systems from South Asia be adapted for African contexts?","answer":"In West Africa, particularly in the Democratic Republic of Congo, rising sea levels have already caused the loss of 15 meters of coastline over the past 10-year period, destroying colonial-era infrastructure and threatening agricultural lands through salt water intrusion. The situation threatens to create climate refugees and strain national resources. In Bangladesh, there are already established systems to address such challenges, including comprehensive coastal inundation forecasting models and early warning systems that integrate multiple hazard factors. These existing systems from South Asia could potentially be adapted for African contexts, as demonstrated by projects like the CIFDP (Coastal Inundation Forecasting Demonstration Project) which has been successfully implemented across different regions including Bangladesh, Fiji, and the Dominican Republic.","context":["Oceans of fortune, oceans of peril\nClimate change on Africa’s west coast\nBy Clotilde Goeman\nOn Africa’s West Coast, the ocean is everything. For thousands of years, its bounty has provided food for families, employment for fisherfolk, remarkable sunsets that attract tourists, ports that carry goods and build economic resilience, and coastal barriers that buffer the earth, cleanse the ocean and create a more sustainable ecosystem. The ocean is hearth and home. But changes in the climate are resulting in rising sea levels, degraded fish stocks, coastal degradation and more. Making this both an ocean of fortune and an ocean of peril.\nTaking it from a broad optic, the coastal zone of West Africa represents a major source of revenues for its communities. In some countries, like Senegal, 66% of the population lives in coastal areas. In addition, due to high population growth and the decreased productivity of agricultural lands in the coastal zones – caused in particular by an increased salinization of the soils – coastal communities are under ever-greater pressure and are increasingly dependent on ocean resources for their survival.\nThis results in intensive unsustainable fishing close to the coastal zones where fish reproduce, as well as sand displacement resulting from the ever-growing fleets of fishing boats that land on the shore. Mangrove and other natural buffers are cut down for firewood, and the construction sector mines the sand to build new houses for a growing population. In short, there are stresses on every side of the ecosystem and economy.\nIn addition to the unsustainable human-induced pressures depleting these resources (especially fish populations), climate change is putting an additional stress on livelihoods in coastal communities.\nThe Democratic Republic of the Congo (DRC) is witnessing unprecedented rates of erosion on its 40km sliver of coast. Recent estimates indicate that 15 meters of coastline has already been lost over the past 10 years. On a mission there this March, the connection between local communities, poverty traps and rising seas became readily apparent.\nIn certain areas, large colonial homes that were built 100 years ago are now slipping into the sea. Erosion here happens due to a combination of factors, including the low topography of the coastal area, the gritty nature of the rock, but most importantly the increased impact of sea level rise. This erosion means infrastructure and valuable assets can be wiped out, and salt water can intrude on agricultural lands, cutting economic productivity for this least developed country.\nOn a global level, climate change impacts are at the heart of international discussions, afternoon talk shows and everyday conversations. While the reality of climate change is a consensual fact, doubts are being raised with greater force and greater resonance every day. Unfortunately, in this global conversation, all too often the voices of vulnerable coastal communities are left unheard. In Africa myriad stresses – from war to famine to disease to resource depletion – pull at resources and threaten peaceful and prosperous achievement of the 2030 Agenda for Sustainable Development, with little attention paid to the impacts sea-level rise will have on economic development.\nOn my last mission to DRC, a local villager from the coastal town of Nsiamfumu, Muanda, underlined the true impact rising seas and coastal degradation are having on the community. “Our ancestors brought us in coastal areas to exploit the resources the ocean offers, but with the increasing impacts of climate change and the threats it poses on our families and our livelihoods, we are forced to retreat inland and leave our ancestor’s lands.” These modern-day climate refuges will strain national resources and face new perils on their inland journey.\nAmong the various projects UNDP supports to address the challenges posed by climate change in Africa’s coastal areas, the Strengthening Resilience of Muanda’s communities from coastal erosion in the Democratic Republic of Congo project is providing a comprehensive and sustainable approach to support vulnerable communities in their efforts to adapt to climate change. Financed through the Global Environment Facility’s Least Developed Countries Fund (GEF-LDCF) with US$5.3 million in grant funding, the project will be a centerpiece in the nation’s efforts to protect its coasts. By collecting climate data and translating it into usable and understandable information, the project will enable decision-makers to plan and budget for climate change and provide communities with the necessary climate information and early warnings they need to prepare in case of extreme weather.\nLocal communities will be directly supported by investments in coastal defence, including the introduction of pilot adaptation measures to stabilize the cliffs at Muanda and secure fisherman docking and landing operations at Nsiamfumu. In addition, the project will create alternative Income Generating Activities (IGA) for women and youth organizations to reduce pressure on coastal resources and uplift the economic status of these at-risk groups. More specifically, processing of fish, fruit-based products, poultry, mushroom farming, honey production, artisanal craft, oyster cultures in mangrove areas and resilient small-scale farming practices will be promoted to provide an incentive to sustainably divert from unsustainable practices.\nIn the end, the ocean has always abided. Whether it will be an ocean of fortune, that protects and nurtures the vulnerable people of DRC, or an ocean of peril, where rising tides force migration, perpetuate poverty traps and diminish stability-building efforts, remains to be seen. But with sustained support, and active engagement with at-risk coastal communities, the chances of building an ocean of fortune remain high.\nAbout the Author\nClotilde Goeman is a Regional Technical Advisor for Climate Change Adaptation and International Waters in Central and West Africa for UNDP. Follow her on twitter @clotilde_goeman.","Mr. Fakhruddin is a hydrologist by training, and has 13 years of experience specializing in hydrology, disaster and climate risk management. His recent work involves generation and application of medium and long range ensembles probabilistic forecasting techniques for the Region using numerical weather and climate modelling (WRF, ECMWF, APCC, etc) and demonstration of location specific early warning system for coastal hazards (i.e. cyclone, storm surge). His key areas of expertise are hydrological modeling, early warning and dissemination system design, emergency communication, incident command system, climate change adaptation and integrated water resources management. Fakhruddin graduated in civil engineering and has Master in Water Engineering and Management from AIT, Thailand. He has a PGD in Integrated Water Resource Management from United Nations University, Canada and ongoing PhD in Water Engineering and Management. He is a certified Instructor of ICS from United States Forest Service (USFS).\nFakhruddin has working experiences in climate risk management and disaster management field more than 15 countries ( i.e. Bangladesh, Bhutan, China, Cambodia, Fiji, India, Iran, Indonesia, Myanmar, Maldives, Nepal, Pakistan, Philippines, Sri Lanka, Seychelles, Samoa, Sirree Leon, Thailand, USA, Uganda, Vietnam, etc).\nFakhruddin currently working as a System Developer for Coastal Inundation Forecasting Demonstration Project (CIFDP) of World Meteorological Organization (WMO). Before joining WMO, he also worked for University Corporation for Atmospheric Research (UCAR), USA, The United States Agency for International Development (USAID), Regional Integrated Multi-Hazard Early Warning System (RIMES), Asian Disaster Preparedness Center (ADPC), International Federation of Red Cross and Red Crescent Societies (IFRC), Sri Lanka, US Forest Service (USFS), OXFAM-Hong Kong, AECOM International, USA, CEGIS and FFWC of BWDB, Bangladesh.\n• Since July 2013: Independent Consultant, Coastal Inundation Forecasting, World Meteorological Organization (WMO), Switzerland\n• September 2013: Climate Resilience and Infrastructure Specialist, USAID ADAPT Asia-Pacific\n• July-August 2013: Independent Consultant, Global – Support for African and Latin American Resilience to Climate Change (ARCC), USAID/Tetra Tech ARD, USA\n• May 2012- June 2013: Technical Advisor, UCAR – University Corporation for Atmospheric Research. Bolder, USA\n• May 2010-July 2013: Team Leader- Hydrology, Regional Integrated Multi-Hazard Early Warning Service (RIMES) for Asia and Africa, Bangkok, Thailand\n• Sep 2007- May 2010: Senior Technical Specialist, Asian Disaster Preparedness Center (ADPC), Bangkok, Thailand\n• Nov 2005- Aug 2007: Technical Specialist, USAID Indian Ocean Tsunami Warning System (US IOTWS) Program, Thailand, Indonesia, Sri Lanka\n• Jun 2002-Oct 2005: Hydrologist, Center for Environment and Geographic Information Services (CEGIS), Bangladesh\nWMO funded ‘Coastal Inundation Forecasting Demonstration Project (CIFDP)’- System designed and guidance for developing coastal inundation forecasting model integrating cyclone, storm surge, wave and hydrodynamic models for Bangladesh, Fiji and Domenic Republic. July 2013- to dates.\nEU-7 supported project “RISC-KIT” to develop a case study in Bangladesh on coastal inundation modeling. Feb 2014 – to date\nDRDM-UNDP funded ‘Establishment of an Early Warning and Disaster Management System in Seychelles’ – Designed an end-to-end multi-hazard early warning system and command center operations. Developed and tested warning dissemination and notification procedures from national to local levels in Seychelles –Sep 2007- Feb 2008.\nUSAID funded ‘African and Latin American Resilience to Change (ARCC) Program’- Conducting assessment for opportunities to improve the efficiency and sustainability of groundwater and climate change in West Africa. July 2013.\nUN ESCAP funded ‘Reducing risks of tsunami, storm surges, large waves and other natural hazards in low elevation coastal zones, Bangladesh, India, Maldives, Myanmar, Sri Lanka and Thailand’ – Designed and developed emergency operation center (EOC), trained local governments on emergency management and cyclone early warning system. 2011-2012.\nRIMES funded ‘Climate Risk Management for Infrastructure’- Designed and developed training modules on climate change predictions and impact on urban infrastructure, engineering assessment. 2012- to dates\nUSAID funded ” SHOUHARDO-I/II” program of CARE Bangladesh- Developed medium and long range ensembles probabilistic flood forecasting technology for community level applications. May 2008- to May 2013.\nUSAID funded ” Nobo Jiban” program of Save the Children. Enhanced cyclone risk management in Bangladesh, established volunteers for local level rainfall observation, developed storm surge model for local inundations. Sep 2011- to dates.\nUNDP supported ‘Climate Risk Management Technical Assistance Support Project” (CRM-TASP), Armenia, Bangladesh, Bhutan, India, Maldives, Mongolia, Mozambique, Nepal, Pakistan, Papua New Guinea, and Timor-Leste – Developed country assessment report on CRM, provided training on climate risk management and designed projects proposal for countries CRM. Sep 2010-Nov 2011.\nUSAID funded ‘Lesson learned and good practice assessment for USAID OFDA on hydro-meteorological hazards’- conducted survey and assessment on good practices on climate change adaptation and disaster risk management in Asia. Sep 2012- May 2013.\nUSAID funded ‘Climate Forecast Applications’ in Philippines and Indonesia- developed interpretation methods of seasonal climate forecasts for agriculture risk management and trained farmers groups on climate risk management.\nDANIDA funded ‘Enhancing Adaptive Capacity to Climate Change in Bhutan’- Developed flood forecasting model for Wangchu river basin, trained NHMS staffs on flood risk management. 2008-2009.\nUSAID funded ‘US Indian Ocean Tsunami Warning System Project’ Maldives, Sri Lanka, Thailand, Indonesia and India- Designed concept of operations for tsunami warning center operations; established and trained DMO staffs on incident command system for emergency response; developed tsunami alert notification system; conducted policy and institutional assessment for early warning and disaster management. Nov 2005-Sep 2007.\nUNESCAP funded ‘End-to-end early warning of tsunamis and other natural hazards for disaster preparedness and mitigation in the Indian Ocean and Southeast Asia: Phase 1’, Maldives, Myanmar and Sri Lanka –designed and tested through full scale simulation exercises of standard operation procedures for warning dissemination and response. Trained local governments on emergency response management. Sep 2007-Sep 2009.\nUNDP funded ‘Comprehensive Disaster Management Program’ in Bangladesh (CDMP)’- Trained national and local governments on Incident Command System for disaster management and conducted communication mapping and planning at community levels to support disaster management information network. 2008-2009.\nUN ISDR funded ‘Enabling Environment for Integrating Disaster Risk Reduction and Climate Change Adaptation in South Asia, Bangladesh, India, Myanmar and Thailand’ – Conducted assessment of institutional and policy map for DRR and CCA, linkages between DRR and CCA for enabling environment and identified good practice in South Asia. 2009-2010.\nIFRC funded ‘Community based early warning and national disaster management project’ in Sri Lanka. Conducted evaluation of projects achievements and future recommendations for Sri Lankan Red Cross. Nov 2011- Feb 2012.\nDANIDA funded ‘Support to Strengthening National Capacity for Flood Risk Reduction and Adaptation to Climate Change Project’- Developed flood forecasting model and flood inundation scenarios using 1d and 2d MIKE series modelling. 2009-2010.\nBill & Melinda Gates Foundation funded ‘Standard Operation Procedures (SOP) for BRAC’s Emergency Response’ in Bangladesh’- Developed and tested standard operation procedures and conducted full scale simulation exercises for multiple hazards and developed M&E framework for SOP. 2010-2012.\nADPC funded ‘Regional Training on End-to-End Early Warning System’- designed and trained participants on early warning system. 2009.\nRIMES funded ‘Regional Training on Climate Risk Management’- designed and trained participants on climate risk management. 2011.\nITU funded ‘Emergency Telecommunication System, Bangladesh, Sri Lanka and Maldives’– Conducted assessment on emergency telecommunication for disaster management. 2005.\nUNDP funded ‘Sustainable Environmental Management Programme (SEMP)’- Developed models to access impacts of sea level rise on land use suitability and adaptation options in coastal zones of Bangladesh. 2004.\nUNDP funded ‘National Adaptation Program for Action (NAPA), Bangladesh’ – Conducted assessment for water, coastal areas, natural disaster and health. 2004.\nCARE funded ‘Cyclone Shelter Management System, Bangladesh’ – Conducted survey to collect database on cyclone shelter information and data analysis for assessment. 2003-2004.\nUSAID funded ‘Community Based Flood Information System (CFIS)’, Bangladesh – Analysed hydrological data and hydrological model development. Conducted field survey and community capacity building. 2003-2004.\nEKN funded ‘Analytical Framework for IWRM, Bangladesh – designed analytical framework for water resources management. 2002.\nScience Committee Member, Integrated Research on Disaster Risk (IRDR), ICSU (2013-2016)\nMay 2013: AGU Bakner Award for outstanding paper on ensembles flood forecasting application at the community level.\nJan 2013: Young Scientist Fellowship of APEC Climate Center (APCC), Korea for generation of seasonal flood outlooks.\nOctober 11: Best Poster Award ” Decision Support System for Flood Risk Management” at World Climate Research Program (WCRP), National Center for Atmospheric Research (NCAR), Denver, USA\nSep 10 to May 12: Asian Institute of Technology (AIT) & Norwegian Ministry of Foreign Affairs (NMFA) Scholarship to pursue M.Sc. Engineering Study at AIT, Thailand.\nAug 05 to Nov’06: United Nations University Scholarship for the PGD program at Asian Institute of Technology (AIT), Thailand\nOct 2004: Best Paper Award ” Analytical Framework for Water Resources Management” by Water, Engineering and Development Center (WEDC)\n• 2014, Fakhruddin, S.H.M. and Yanin Chivakidakarn. A More Comprehensive Instrument to Assess Institutional Capacity: A Case Study for Early Warning and Disaster Management in Thailand. International Journal of Disaster Risk Reduction\n• 2014, Fakhruddin, S.H.M. Medium Range Flood Forecasting for Community Application. Reducing Disaster: Early Warning Systems for Climate Change. Springer\n• 2013, Fakhruddin, S.H.M. Using science for disaster risk reduction: report of the ISDR scientific and technical advisory group, 2013\n• 2013, Babel, M.S., Fakhruddin, S.H.M., and A. Kawasaki. A Decision Support Framework for Flood Risk Assessment: An Application to the Brahmaputra River in Bangladesh. Floods: From Risk to Opportunity. IAHS Publication no. 357-2013\n• 2013, Fakhruddin, S.H.M. and Scira Menoni. Coping with Coastal Risk and Vulnerabilities in Bangladesh. International Journal of Disaster Risk Reduction (In-Press)\n• 2013, Fakhruddin, S.H.M., Akiyuki Kawasaki, Mukand S. Babel, Development of Medium Range Ensembles Flood Forecasts for Community Level Applications: Case Study in Kaijuri Union, Bangladesh, Springer ( In press)\n• 2011, Fakhruddin, S.H.M., “Long Lead Flood Forecast Application to Benefit Society: Experiences of 2007-08 Bangladesh Floods”, -Acuqa Alta, TuTech Publishers/TuTech Verlag Hamburg, Germany\n• 2011, Michael H. Glantz, Gregory Pierce, Fakhruddin S.H.M., Fernando Briones, “Resilient Adaptation: Adjusting to Climate Changes as if the Future Matters -Illustrations from the Asia-Pacific”- Cambridge UP volume\n• 2007, Fakhruddin S.H.M., and Kelman, I. “Tsunamis”, pp. 146-150 in M.H. Glantz (ed.), Heads Up! Early Warning Systems for Climate, Water and Weather, Tsinghua University Press, Beijing and Climate Affairs LLC, Boulder, Colorado, USA\n• 2006, Fakhruddin S.H.M., “Gender, Water and Sanitation- Case Studies On Best Practices”, Published under United Nations, Office of the Special Adviser on Gender Issues and Advancement of Women, Department of Economic and Social Affairs, United Nations Headquarters, USA (http://www.un.org/womenwatch/osagi/pdf/GenderWaterSanitation/English%20full.pdf)\nEditor/ Editorial Board Member/ Book Review\n• 2011, Reviewer, ” Developments in Integrated Flood and Riverbank Erosion Risk Management for the Braided Brahmaputra in India and Bangladesh” Natural Hazard, Springer Journal\n• 2009, World Disaster Report 2009: Early warning and early action: an essential partnership to prevent disasters, published by the IFRC\n• 2008- “Tsunami Relief and Preparedness in the Indian Ocean”, The Global Leaflet, News From International Programs of the Forest Service, No. 7, February 2008: Disaster\n• 2007,- “Concept of Operation for Tsunami Warning Center Guidebook”, US Indian Ocean Tsunami Warning System Program\n• 26-30 April 2013, Terrestrial Water Cycle OBservation and Modelling from Space, China Academy of Science, Beijing, China\n• 09-20 November 2012, United Nations University, Japan: Dynamic Climate Downscaling method and flood impacts\n• 14- 18 May 2007, UNESCO, The Abdus Salam International Centre for Theoretical Physics, Trieste, Italy: Physics of Tsunami, Hazard Assessment: Methods and Disaster Risk Management\n• April 2007, Wild land Fire Training Center, McClellan, CA, USA, Advance Incident Command Course (ICS-420)\n• September 2006, River Flow 2006, Lisbon, Portugal Master Classes on Fluvial Hydraulics"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:d0be20eb-45e9-4c47-a79e-3b932eaf7b34>","<urn:uuid:40d1d056-1c90-442e-a9fd-217ff8c634ed>"],"error":null}
{"question":"How do California's service leadership initiatives in education relate to its emerging clean technology sector?","answer":"California's educational institutions, exemplified by Foothill College's service leadership projects, are preparing students for the clean technology sector through initiatives like sustainability research, environmental awareness programs, and hands-on projects in engineering and environmental conservation. This educational focus aligns with California's growing cleantech industry, which is driven by state mandates requiring 100% renewable electricity by 2045 and goals for 1.5 million zero-emission vehicles by 2025. The state leads in energy efficiency jobs, creating a direct connection between educational service leadership and emerging career opportunities in the low-carbon economy.","context":["Foothill College 60th Anniversary\nChanging Lives Through Service for 60 Years\nSince the opening of our doors, Foothill College students, staff and faculty have dedicated themselves to leadership through service. As we celebrate our 60th anniversary, we also celebrate 60 service leadership projects organized by our own Foothill College Owls.\nWhat is Service Leadership?\nService leadership allows our students to develop and hone their leadership skills through projects benefitting their fellow Owls and the greater community. Foothill’s #60for60 highlights projects shaping students into emotionally intelligent leaders and global citizens. These projects increase our students’ critical thinking and teamwork skills.\nTo commemorate our 60th anniversary, we're sharing service leadership projects led by students to better the lives of many.\nService Leadership Project Photos\n#60for60 Service Leadership Projects\n- Spring 2017 Design and creation of the Dreamers' Mural\n- Summer 2017 Global Brigade club travels to Central America for sixth trip\n- Summer 2017 Football Team distributes food and clothing to homeless\n- Fall 2017 International Students volunteer at Second Harvest Food Bank\n- Fall 2017 Art Department creates chalk murals at Los Altos Fall Festival\n- Fall 2017 We Care Club raises money for victims of Hurricane Harvey\n- Fall 2017 We Care Club packs Thanksgiving meals for low-income families\n- Fall 2017 Vet Tech Students collect donations for rescued NCal animals\n- Fall 2017 Students learn about Service Leadership through lens of business\n- Fall 2017 Students in SOSC 79 and CNSL 86 serve up support at shelter\n- Fall 2017 Students volunteer with City of Los Altos as archivists and historians\n- Fall 2017 ENGR 10 Students plan how to reduce water bottle waste on campus\n- Fall 2017 Students revive the school newspaper, The Script\n- Fall 2017 Photography Students Bring Pop-Up Portrait Studio to Cuba\n- Fall 2017 Holiday Book Drive for Foster Youth — Sponsored by all Senates and Administrators districtwide\n- Fall 2017 Jingle & Mingle Toys for Tots Drive — Foothill Repertory Dance Company\n- Fall 2017 Secret Santa Gift Card Drive — Sponsored by students from SOSC 79 Intro to Community Engagement\n- Fall 2017 Engineering students conduct parking study for sustainability\n- Winter 2018 We for She Club joins the Women's March in San Jose\n- Winter 2018 Astronomy students help the public view the lunar eclipse\n- Winter 2018 Business Innovation Challenge — Students across campus presented their business ideas to local professionals\n- Winter 2018 Foothill's Got Talent — Funds from this talent show will help pay for students to travel to the Philippines to build wells and compost piles\n- Winter 2018 Start Environmental Club to increase awareness of composting for community gardens/horticulture on campus\n- Winter 2018 Volunteering for the Wildlife Center, raising awareness of wildfires and wildlife rehabilitation\n- Winter 2018 Modular spacesuit design customizable for women\n- Winter 2018 Anatomic Side Markers Research: Are these essential radiologic technology tools harmful to the community?\n- Winter 2018 Understanding proper sanitation in the radiologic technology lab\n- Winter 2018 Social media campaign to create greater awareness of services available for Foothill students\n- Winter 2018 Art in Action — send out crafts materials to schools to make art accessible to many children\n- Spring 2018 6th Annual Oral Cancer Awareness Walk\n- Spring 2018 Gun control walkout\n- Spring 2018 Global Gala — Funds from tickets sales benefit scholarships for refugees\n- Spring 2018 Project Doggybags — put up free doggy bag dispensers to reduce waste on children play areas\n- Spring 2018 Cub Camp Project — Engagement of Cub Scout camp youth through STEM, sports and arts\n- Spring 2018 Oral hygiene and Parkinson's Disease — educational presentation on oral health\n- Spring 2018 Service learning with Sunday Friends, to break the cycle of poverty\n- Spring 2018 Pacifica Beach Clean Up with We Care Club\n- Spring 2018 Big Wave Project\n- Spring 2018 Goat Structure for Vet Tech Department - Structure was designed by Vet Tech students and the help of the Horticulture Department for the animals of the department\n- Spring 2018 How Climate Change in Bangladesh Affects Women and Girls by Foothill student Kareeda Kabir (featured in Teen Vogue)\n- Ongoing Physics Show\n- Ongoing Fund the Future Club provides microcredit to people in developing countries\n- Ongoing Community Dental Health Outreach Program — Oral health education for developmentally delayed children and teens, older adults, and other populations\n- Ongoing Geospatial Informational Systems Technology students intern with the VTA to address sustainability issues\n- Ongoing Vet tech donation drives\n- Ongoing Dance class students dazzle local nursing homes with their dance moves\n- Ongoing Powerful Beyond Measure: working to develop strong female leaders by creating a cross-cultural experience between high school girls from the U.S. and Africa\n- Ongoing Let's Play Math: Professor Young Hee and the Math Club make math fun for kids in the community.\n- Ongoing Foothill Vet Tech Students Team up with San Jose Animal Care Center for Vets for Healthy Pets\n- Research & Service Leadership Symposium Virtopsy: integrating forensic science with radiologic technology\n- Research & Service Leadership Symposium Integrated research on overcoming test anxiety at Foothill\n- Research & Service Leadership Symposium Anthropological work to lower ethnocentrism and increase cultural relativistic skills\n- Research & Service Leadership Symposium How Foothill athletes experience racism\n- Research & Service Leadership Symposium Islamophobia awareness and education\n- Research & Service Leadership Symposium Smile for the CAM: Integrative Medicine as a Remedy for Eurocentrism in International Public Health Policy\n- Research & Service Leadership Symposium Creating community interest in social sciences\n- Research & Service Leadership Symposium Concrete Rose Collaborative: Helping community members affected by gentrification\n- Research & Service Leadership Symposium Habitat Conservation Plan for Santa Clara County Parks & Recreation\n- Research & Service Leadership Symposium Actions to make Foothill College plastic-free\n- Research & Service Leadership Symposium What Types of People Do Science? Research and Service Learning to Challenge Stereotypes of Scientists in Class\n- Summer 2018 KCI Saturday Maker's Space with Middle School Students\n- Summer 2018 Enactus Club builds wells in the Philippines\nWhy is this objective important?\nThe skills needed to succeed have become more focused on leadership, non-technical skills such as emotional intelligence, growth mindset, teamwork, critical thinking, lifelong learning, and oral and written communications.\nThe experience was amazing. I encourage people to give back to the community because it's humbling and fun.\nEmployers have consistently ranked non-technical skills as critical for hiring and advancement in the workforce. Our students also need to have a strong sense of community and build community as they navigate work and life.\nBy participating in service leadership projects, students feel more engaged and develop a stronger sense of community.\nResearch and Service Leadership Symposium\nThese #60for60 Service Leadership Projects list above are examples of community and social activism projects that were submitted for the first Research and Service Leadership Symposium on May 17, 2018. LEARN MORE\nFor submissions or questions, contact Vanessa Smith, Interim Director of Marketing and Public Relations, at firstname.lastname@example.org.","Risk & Reward: 5 California Industries Being Reshaped by Climate Change\nCalifornians are feeling the effects of climate change: wildfires, record heat waves, and even atmospheric rivers have begun to wreaked havoc on the Golden State.\nIt’s increasingly clear that certain sectors of the world’s fifth largest economy are emerging as ground zero for the costs of climate change – and the potential costs are staggering.\nThis year alone, nearly half a million homes in California face high or very high wildfire risk, at a potential cost of $268 billion.\nBut at the same time, there are industries — new and existing — that may flourish in this changing landscape. With that contrast in mind, here’s a look at five sectors being reshaped by climate change:AT RISK Food & Wine | Agriculture Faces Water Challenges\nThe changing climate will likely increase the need for fresh water, but reduce the supply. California is the top food producer in the U.S., so this is a serious concern for its agriculture industry.\nWith nearly 90% of California’s crops grown on farms that are entirely irrigated, climate change may force farmers to reduce cultivated acreage or shift from water-intensive crops. For example, over the next 75 years, the land capable of consistently growing high-quality wine grapes is likely to shrink by more than 50%.\nMy colleague Adam Beak has experience on both sides of the wine business. Based in Napa, he’s helped lead wineries as a CFO and owner himself and brings this unique perspective to his work as managing director of our wine & beverage group.\n“Water access and water supply is a top concern for California’s wine industry and the impact of global warming on rainfall and water availability is a serious concern for the long term.”\nAdapting to climate change by growing different varietals is an option for California’s farmers, including those in the wine industry. However, there may be fewer ways to adapt crops such as almonds, which was valued at $5.6 billion in 2017, because a gallon of water is required to grow a single nut.Tourism: Snowfall is an Achilles’ Heel for Winter Sports\nSnow sports are one of the state’s biggest draws. An average snow year in California represents $1.6 billion in economic value from millions of skier visits.\nBut higher temperatures result in more rain and less snow. As the climate warms, California’s snowpack will diminish, shortening the ski season, reducing both in-state and out-of-state visitors and severely affecting the range of businesses that service the snow sports industry and provide jobs.Real Estate: Fire and Water are a Risky Combination\nSome studies say climate change is already impacting real estate markets, with U.S. properties exposed to sea-level rise selling at a 7% discount compared with those with less exposure.\nBy the end of the century, rising sea levels, combined with more serious storms and erosion, will put an estimated $150 billion worth of California’s iconic coastal real estate at risk of flooding. Wildfire is another risk. California’s Camp Fire was the single largest insurance event of 2018.\nIf the risk becomes high enough, insurers may be pushed out of certain California regions, affecting the market for moderately priced homes.\nCalifornia is the top state in the nation for generating electricity from renewable resources.\nThese industries are well-positioned for growth in part because the state’s renewable portfolio standard requires 33% of retail electricity to come from renewables by 2020, 60% by 2030, and 100% by 2045.\nIn addition, in 2018, traditional energy jobs in the U.S. – those related to non-renewables – declined while those related to energy efficiency grew. Energy efficiency-related jobs involve work on buildings, appliances, and transportation to reduce energy use or to improve “smart grid” technology. They remain the highest source of employment within the energy sector and show the highest growth rate nationally. The state with the most energy efficiency jobs? California.Cleantech: State’s Zero-Emissions Goals Shoot for the Moon\nA low-carbon economy will be fueled by the clean energy, bio-engineering, and electric vehicle industries among others.\nCalifornia’s largest source of GHG emissions comes from the transportation sector. With the policy support of a plan to put 1.5 million zero-emission vehicles on the road by 2025, car manufacturers are incentivized to build better and cheaper electric vehicles that appeal to more buyers.\nPossibly the biggest opportunity for would-be climate disruptors are so called “moonshot” technologies. Many of these focus on removing CO2 from the atmosphere and either transforming it into a useful raw material or simply storing it.\nCalifornia companies – whether in Silicon Valley, Los Angeles, or other regional innovation hubs – that can leverage technology innovation to address climate change, will be able to capitalize on the carbon-free economy of the future.\nThe potential annual cost of climate change to the U.S. economy has been placed at $224 billion by the end of this century. As with almost all upheavals, there will be winners and there will be losers.\nWill California’s climate economics impact your industry in the future?"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:280a7c64-2059-4e79-a7fd-5adffa2084b8>","<urn:uuid:192b8f0b-ab82-4a8f-b0fa-68ed9734517e>"],"error":null}
{"question":"As a historical architecture researcher, I'm curious about the architectural evolution of St. Joseph's Hospital. Could you explain how the building developed over time, including its major additions and design features?","answer":"St. Joseph's Hospital was built in multiple stages, starting in 1876 on Collinson Street (now Fairfield Road). The hospital expanded with additional beds - 48 beds in 1888 and 67 more in 1897, bringing the total to 150. The most significant addition came in 1908, designed by Thomas Hooper and C. Elwood Watkins, which faced Humboldt Street. This 1908 addition features Neoclassical Edwardian style elements, including paired Ionic columns at the front entrance porch, a balcony with stone and cement balustrade, multi-hipped roof with gabled front extension, stone corbelling under eaves, a circular light in the gable peak, and a slender turret with Christian cross at peak. The building also includes semi-recessed second storey bays arcaded by pilasters and a historic chapel on the northeast of the structure.","context":["Former St. Joseph's Hospital\nSt. Joseph's Hospital\nSt. Joseph's Apartments\nLinks and documents\nListed on the Canadian Register:\nStatement of Significance\nDescription of Historic Place\nSt. Joseph's Hospital is a four-and-one-half storey plus basement stone Neoclassical Edwardian structure located across the street from St. Ann's Academy. The designation also includes interior features.\nSt. Joseph's Hospital, built in 1876 with additions in 1888, 1897, and 1908, is valued for its association with the Sisters of St. Ann, its connection with the health care of Victoria's citizens, its architecture and architect, and its grounds.\nSt. Joseph's Hospital has value for its association with one of British Columbia's major pioneering religious orders, and for its role in the provision of health care in the region. It is a prominent reminder of the importance of the Sisters of St. Ann to the physical well-being of Victoria's citizens.\nFounded in Quebec in 1848, the Sisters of St. Ann represent the strong presence of French-Canadian missionaries in British Columbia's formative history. From the Pacific Northwest headquarters of the order at St. Ann's Academy, directly across Humboldt Street from this building, they gradually expanded their missionary work in Victoria to include a school, a convent, a novitiate program, and nursing services to better serve the educational, medical, and spiritual needs of the population for over a century.\nArchitecturally, the historic place was designed and built in stages, but it is the 1908 addition by Thomas Hooper and C. Elwood Watkins that is most prominent in today's configuration. The first phase of St. Joseph's Hospital was built on Collinson Street (now renamed Fairfield Road) in 1876 under the leadership of Sister Mary Providence and Dr. John Sebastian Helmcken. In 1888, an additional 48 beds were added, and in 1897, another 67, bringing the total to 150. The 1908 addition was designed to face Humboldt Street. With its Edwardian solidity and strong Neoclassical features, it serves as a foil to the Hooper wing at St. Ann's Academy and further reinforces the significance of the Order in Victoria's development. A historic chapel on the northeast of the structure has been preserved and provides a visual link to the religious past. Several interior features have been retained, further enhancing the historic importance of this building. It has been rehabilitated for use as seniors' housing with units for transient accommodation.\nIn addition, the grounds contribute to Victoria's natural environment. The building is surrounded by a manicured lawn with mature plantings that create a park-like setting. The presence of this greenspace so close to downtown Victoria, coupled with the St. Ann's Academy grounds and Beacon Hill Park just to the south, provides a counterpoint to the hard urban landscape of the city core.\nSource: City of Victoria Planning Department\nKey elements that define the heritage character of St. Joseph's Hospital include:\n- its monumental scale\n- characteristics of the Neoclassical Edwardian style, such as the front entrance porch with paired Ionic columns, balcony above with stone and cement balustrade, multi-hipped roof with gabled front extension, stone corbelling under eaves, circular light in gable peak, slender turret with Christian cross at peak, front extension over porch\n- semi-recessed second storey bays arcaded by pilasters\n- form and pattern of fenestration\n- design and detailing of the 1908 addition, attributable to Thomas Hooper and C. Elwood Watkins\n- spatial configuration, form and interior finishes of the chapel\n- main entrance and foyer located on the south side of the main floor\n- entrance with oak doors\n- mosaic tile floors located on the east and west sides of the main floor\n- wooden doors and sidelights located at the entrances to the porches on the south side of the third and fourth floors\n- its proximity to St. Ann's Academy across Humboldt Street\n- its proximity to Beacon Hill Park\n- stone wall on Humboldt Street frontage\n- well-established landscaping including lawns, mature trees and other plants\n- evidence of historical religious uses of the building as seen in the chapel\nLocal Governments (BC)\nLocal Government Act, s.967\nTheme - Category and Type\n- Building Social and Community Life\n- Religious Institutions\nFunction - Category and Type\n- Multiple Dwelling\n- Health and Research\n- Hospital or Other Health Care Institution\nArchitect / Designer\nLocation of Supporting Documentation\nCity of Victoria Planning Department\nCross-Reference to Collection"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:28696eae-d70c-4013-80a8-ce96f99d7ef6>"],"error":null}
{"question":"Could you explain the proper felting technique for ensuring silk hankies stay securely attached in a nuno felting project?","answer":"For secure attachment of silk hankies, thorough felting is crucial. The wool must completely penetrate all layers of silk to keep it in place. It's recommended to let your scarf rest overnight after the initial felting and then felt it again the next day. The hankies need to be well felted or they will pull away from the scarf. When choosing wool colors, you can use either contrasting or matching colors with the hankies - contrasting colors can create a marbling effect if felted enough.","context":["Felt Wool Instructions, nuno felting, how to felt\nHow to play around with Silk Hankies by Nancy Ballesteros\nSilk hankies traditionally have been underutilised as a source of silk by felt makers. Generally people are not sure how to tackle handling them. In the following instructions we will attempt to de-mystify the hanky, and inspire you with a couple of simple scarf instructions.\nPlaying around with Silk HankiesWhat’s so special about silk hankies? Each silk hanky is made from a degummed cocoon. The caterpillars spin each cocoon as one long continuous strand of silk which is then opened out and stretched over a square wooden frame (resembling the shape of a handkerchief). One cocoon is as gossamer as a spider’s web. I generally never use just one hanky as it would easily get lost in the wool. If you look at the edge of the hankies, each thick edge is one cocoon (see Figure 1). Layer upon layer has been stacked on top of one another.\n|Figure 1 - each thick edge is one cocoon|\nFor general felting purposes pull away several layers at a time – it is quite hard to count the layers (so don’t), simply pull off just enough so you can still see through to your hand. It will be MUCH easier to work with the hankies if you iron them before and after splitting off layers. Different visual/textural effects can be achieved by using slightly more or slightly less, play around to see what you prefer.\nRemember SILK HAS NO FELTING PROPERTIES; therefore, you must always use wool with it to act as the “glue”. When used in conjunction with fine silk fabrics it doesn’t really matter in which order you lay down the silk hanky, the wool and the fabric. The wool will generally always be able to work its way through to felt the silk hanky. Hankies seem to always have thick and thin areas. It’s a good idea to add extra wool to the underneath edges of the hankies since this is usually a thicker area. Try to thin out any other thick areas of the hankies or be sure and add extra wool. A word of warning, if you use too many hankies the silk will be too thick for the wool to penetrate and it will not felt properly. Remember shrinkage is a big part of the felting process; the silk hankies will become denser when felted.\nIt is important to decide which colour of wool to place behind the hankies. A very different effect is achieved using a contrasting colour of wool to the hankies verses using the same colour. Experiment to see what look you prefer. Personally I use a strong contrast between the wool and the hankies resulting in a lovely marbling effect - if you felt your scarf enough. The hankies need to be well felted or else they will pull away from the scarf.\n|Close-up of well felted silk hankies, the red is the wool coming through the silk.|\nIt is extremely important to felt your hankies well. I recommend that you let your scarf rest overnight after the initial felting and felt it again the next day. The wool needs to completely penetrate all layers of the silk to keep it in place.\nConsider your overall design. The wool can just be laid underneath where the hankies are placed OR the wool can be laid over the entire fabric and the hankies placed anywhere on top of the wool. Personally I like to leave gaps so that I can see through to the fabric in places. Be sure and be very light handed with the wool, it won’t take much (roughly 10gms (.35 ounce)) per scarf – for those without scales that is about ½ metre of wool tops. Laying the fibres only in one direction is fine OR you can lay a light cross-layer to help shrink the scarf in both directions.\nIron, iron, iron – if you are feeling frustrated by the hankies catching on everything, or they just look a bit messy – IRON THEM! You can also try a bit of talcum powder on your hands to help stop the hankies from catching on any rough spots. Thin disposable gloves are also an option."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:7a3ee5d3-8ff1-4459-b9c2-396586efa7ac>"],"error":null}
{"question":"What are the similarities between preventing thalassemia complications and avoiding sickle cell crises?","answer":"While thalassemia cannot be prevented since it is inherited, and sickle cell crises prevention focuses on avoiding triggers, both conditions require careful medical management. For sickle cell crisis prevention, patients need to stay hydrated, avoid cold temperatures, exercise with care, get enough sleep, and manage stress. Thalassemia patients require regular blood transfusions and iron chelation therapy to prevent complications like organ damage, and may need spleen removal or bone marrow transplants in severe cases.","context":["(Mediterranean Anemia; Cooley’s Anemia; Thalassemia Major; Thalassemia Minor)\nThalassemia – Definition\nThalassemia is an inherited disorder. It leads to the decreased production and increased destruction of red blood cells. Hemoglobin in the red blood cells carry oxygen for all organs in the body. The loss of red blood cells results in low hemoglobin. This leads to anemia. The decreased oxygen will impair the ability to maintain normal functions.\nHemoglobin is made of two separate amino acid chains. They are alpha and beta. Thalassemias are categorized by the specific chain and number of genes affected:\nAlpha thalassemia — the alpha chain is affected\n- Silent carrier — one gene affected\n- Thalassemia trait — two genes affected\n- Hemoglobin H disease — three genes affected\n- Alpha hydrops fetalis — four genes affected, most severe form, results in fetal or newborn death\nBeta thalassemia — the beta chain is affected\n- Thalassemia minor — one abnormal gene\n- Thalassemia major (Cooley’s anemia) — two abnormal genes\nThalassemia – Causes\nThis condition is caused by genetic material known as genes. Genes are inherited from the biological parents. There are four alpha genes and two beta genes.\nThe disease can develop if only one parent has abnormal genes. If only one gene is inherited, the person will be a carrier of the disease. They will have mild or no symptoms.\nThalassemia – Risk Factors\nRisk factors that increase your chance of Thalessemia include:\n- Geographic location of ancestors, including:\n- Alpha thalassemias — Southeast Asia, Malaysia, and Southern China\n- Alpha hydrops fetalis — Southeast Asian, Chinese, and Filipino ancestry\n- Beta thalassemias — Africa, areas surrounding the Mediterranean Sea, and Southeast Asia\n- A family history of the disorder\nThalassemia – Symptoms\nSymptoms most often begin within 3-6 months of birth. Symptoms may include:\n- Anemia, which may be mild, moderate, or severe\n- Enlarged spleen\n- Fatigue (tiredness)\n- Reduced appetite\n- Enlarged and fragile bones, including:\n- Thickening and roughening of facial bones\n- Bones that break easily\n- Teeth that don’t line up properly\n- Growth problems\n- Increased susceptibility to infection\n- Skin paler than usual\n- Hormone problems such as:\n- Delayed or absent puberty\n- Thyroid problems\n- Heart failure\n- Shortness of breath\n- Liver problems\nThis form usually causes milder forms of the disease. It has with varying degrees of anemia.\nThis form can be asymptomatic or be a mild form of disease. The mild form is known as thalassemia intermedia. This form rarely needs extensive medical care.\nBeta Thalassemia Major (Cooley’s Anemia)\nThis version usually has symptoms within the first two years of life. Children are pale and listless. They often have poor appetites. They grow slowly and often develop jaundice (yellowing of skin). It is a serious disease. It requires regular blood transfusions and extensive medical care.\nWithout treatment, the spleen, liver, and heart soon become very enlarged. Bones become thin and brittle. Abnormal deposits of iron in body organs can lead to organ failure. This is called secondary hemochromatosis. It most often affects the heart, liver, and pancreas. Heart failure and infection are the leading causes of death among children with untreated thalassemia major.\nThalassemia – Diagnosis\nThe doctor will ask about your symptoms and medical history. A physical exam will be done. Blood tests may include:\n- Complete blood count — a count of the different types of blood cells\n- Blood smear\n- Hemoglobin electrophoresis\n- Quantitative hemoglobin analysis\n- Iron levels\nThalassemia – Treatment\nTreatment may include:\nYou may require blood transfusions. These are done to replace abnormal red blood cells with healthy new ones.\nIron Chelation Therapy\nExcess iron can accumulate in the body after repeated blood transfusions. Too much iron can damage the heart, liver, and other vital organs. A drug call deferoxamine (Desferal) can be given to bind to excess iron in the body. It is then carried out through the urine. This drug is given through the skin or by vein using a small infusion pump.\nThere is a newer drug called deferasirox (Exjade). It can be given as a drink, mixed with water or juice.\nThis surgery is done to remove the spleen. It may help reduce the number of blood transfusions that are needed.\nBone Marrow Transplant\nHealthy stem cells from a donor’s bone marrow are injected into your vein. The new cells travel through the bloodstream to the bone cavities. There they can produce new blood cells, including red blood cells. This is usually only done in severe cases. A compatible sibling donor is required.\nThalassemia – Prevention\nThis disease is inherited. Blood tests and family genetic studies will show if you are a carrier. A genetic counselor can discuss risks of passing on the disease. They can also give you information on testing.","What is a sickle cell crisis?\nA sickle cell crisis is a painful episode that may begin suddenly in a person who has sickle cell disease.\nA sickle cell crisis occurs when sickle-shaped red blood cells clump together and block small blood vessels that carry blood to certain organs, muscles, and bones. This causes mild to severe pain. The pain can last from hours to days. \"Painful event\" and \"painful crisis\" are other terms used to describe these episodes. Some people who have sickle cell disease have many painful events, while others have few or none.\nThe pain can happen in any part of the body. But the most common areas include the:\n- Bones of the spine.\n- Bones in the arms and legs.\nHow is it treated?\nTreatment depends on the level of pain and how long it lasts. Sometimes, nonprescription, or over-the-counter, pain relievers such as ibuprofen can help. Other times, a person needs stronger pain relief medicine that is prescribed or given by a doctor. Some painful episodes may need IV therapy for fluids and powerful pain medicines, such as morphine, to ease the pain.\nYou can prepare for a crisis in advance by creating a pain management plan with your doctor. This plan should include not only the types of medicines you can take at home but also other actions you can take at home to relieve pain. Also, your plan helps you know when to call your doctor or go to a hospital.\nFor more information, see Sickle Cell Disease: Pain Management.\nHow can you prevent a crisis?\nIt isn't always possible to know what sets off a crisis, but triggers include dehydration, cold temperatures, infection, stress, and low oxygen intake.\nYou can help prevent a crisis by:\n- Drinking plenty of fluids, especially before, during, and after exercise or strenuous activity and when you have a fever or infection. Drink enough so that your urine is light yellow or clear like water.\n- Dressing warmly in cold weather. Try to avoid situations where you might become cold. Exposure to cold air, wind, and water may cause a crisis by triggering red blood cell sickling in exposed areas of the body.\n- Exercise with care. If you exercise strenuously, rest when you feel tired, and drink plenty of fluids to prevent dehydration. Dehydration and reduced oxygen levels in a person's blood resulting from strenuous exercise may cause red blood cells to sickle.\n- Get plenty of sleep.\n- Try to reduce and manage stress in your life.\nFor more information, see Sickle Cell Disease: Staying Healthy.\nOther Works Consulted\n- Meremikwu MM, Okomo U (2011). Sickle cell disease, search date March 2010. BMJ Clinical Evidence. Available online: http://www.clinicalevidence.com.\n- National Heart, Lung, and Blood Institute, National Institutes of Health (2002). The Management of Sickle Cell Disease (NIH Publication No. 02-2117). Available online: http://www.nhlbi.nih.gov/health/prof/blood/sickle/.\nCurrent as of: March 28, 2019\nAuthor: Healthwise Staff\nMedical Review: E. Gregory Thompson MD - Internal Medicine\nAdam Husney MD - Family Medicine\nMartin J. Gabica MD - Family Medicine\nMartin Steinberg MD - Hematology"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:195f9a42-7960-496f-a4a8-1b7fe6f8a8c4>","<urn:uuid:6288870c-0903-4fc4-b268-8e9bf6a3b421>"],"error":null}
{"question":"How do brand journalism ventures operate in the communications field, and what resources are available for professionals seeking careers in this emerging area?","answer":"Brand journalism ventures must carefully navigate between journalism and PR, focusing on disclosure of inherent conflicts and building credibility through quality reporting rather than operating as disguised PR. They need to be transparent about their funding relationships and demonstrate independence by covering parent organizations objectively. For professionals interested in this field, resources include industry platforms like AdAge.com and ADWEEK for current news, professional development through organizations like the PR Council, and career guidance through resources like Candid Career which provides informational video interviews with industry professionals. The Occupational Outlook Handbook also offers information about career paths, salary, and training in these emerging communications roles.","context":["Career Field - Communications\n- Career Profiles and Guides\n- Advertising, Marketing, Public Relations\n- Journalism, Literary, Publishing\n- Translation & Interpretation\n- Occupational Outlook Handbook - Information about career, salary, training and employment projections for many different career fields, including arts and entertainment.\n- Candid Career - Provides 1000's of informational video interviews with industry professionals covering a comprehensive collection of careers..\n- Firsthand - Insider information, video profiles, downloadable e-books, and more, covering major careers and industries. Access through Handshake.\n- AdAge.com - Current news and industry information about marketing, media and advertising.\n- ADWEEK - Current news and industry information for people working in advertising.\n- American Marketing Association (AMA) - National professional group. Local chapter in San Francisco.\n- PR Council - Industry news, events and issues, directory of PR firms, case studies, “PR Quickstart” Web-based industry training program designed for those new to PR, and other resources.\n- Public Relations, Advertising & Marketing - What's the Difference? - Quick rundown of these three fields, how they’re related and the differences between them.\n- Alumni Career Chat: Journalism - YouTube video of interview with Cal alum and journalist, Will Kane. >>\n- Bookjobs.com - About Publishing - Overview of the publishing industry; descriptions of the types of work performed within publishing.\n- JournalismJobs.com - Career articles and other helpful information for people interested in journalism and media.\n- MPA/Association of Magazine Media - Offers Industry news, career resources and professional development event information for the magazine media industry.\n- Poets and Writers - Resources for literary writers' professional development including a directory of places which host readings, funding, publishing advice and more.\n- Writer's Digest - Tips and information on how to get published in magazines and books.\nTranslation & Interpretation\n- Masters/Certification Programs:\nMiddlebury Institute of International Studies at Monterey\nSan Francisco State University (College Extended Learning)\nThe American Translators Association\nUC Berkeley Extension\n- The American Association of Language Specialists\n- The Translator's Home Companion\nParticipating in these groups can be a valuable avenue for learning about an occupation or field. They are an excellent resource for networking, and their websites often have information about careers.\nCal Student Organizations\nStudent organizations related to this field include:\n- Daily Californian\n- Undergraduate Marketing Association\n- Other Student Organizations - List of UC Berkeley student organizations registered with the LEAD Center.\nMajor professional organizations include:\n- Advertising Educational Foundation\n- American Society of Journalists and Authors\n- Bay Area Editors' Forum\n- International Association of Business Communicators (IABC)\n- Media Alliance - Media resource and advocacy center for media workers, non-profit organizations, and social justice activists.\n- National Sports Marketing Network Bay Area Chapter\nAffinity group organizations:\n- Asian American Journalists Association\n- Association for Women in Communications\n- National Association of Black Journalists\n- National Association of Hispanic Journalists\n- Public Relations Society of America (PRSA) and Public Relations Student Society of America (PRSSA) (student arm of PRSA)\n- Radio Television Digital News Association\n- Society of Professional Journalists\n- Society for Technical Communication\n- Writers Guild of America, West\nTo find other professional associations, go to:","Journalism funded by interested organizations\n- What ethics issues arise when your news venture is created by a company or government?\n- How do you disclose inherent conflicts in your coverage?\n- How do you cover your parent organization and the issues that matter to it?\nIncreasingly, organizations that have not traditionally been involved in journalism are launching operations that produce news, features and commentary in the same field where the business or government organization operates. For instance, a hospital corporation may create a health news service bearing its name, or a camera company may launch a news brand to write about photography. These ventures are sometimes called “content marketing” or “brand journalism.” They are new enough that their operations, structures and ethics are all evolving.\n(This module concerns news ventures entirely launched by companies and governments. For donations to existing news organizations from outside funders, or news ventures launched mainly for the purpose of philanthropy, see the module “Accepting money from donors and foundations.”)\nOne decision the company or government needs to make is whether it truly is launching a journalism operation or simply wants to practice a new form of public relations. If it’s a public relations operation, it should operate by PR ethics codes, such as the Public Relations Society of America Code of Ethics or a PR code covering the nation where you live. But if you’re trying to launch a true journalism operation, you should consider how to minimize and disclose the inherent conflicts. Perhaps it’s a hybrid that should draw from both. If you can’t accept the fundamentals in Step 1, it’s probably best not to describe your work as journalism.\nA journalism operation with roots in a company or non-profit organization in the same field might gain credibility if it can develop an independent — or at least arm’s-length — funding relationship. But generally the whole operation is funded by a particular entity, and the best way to maintain integrity is to disclose the relationship and try to build credibility by producing excellent journalism rather than PR in disguise. Masquerading as an independent news organization will almost surely fail and damage your credibility severely when you are exposed.\nCredibility will be a key challenge for a brand-journalism or content-marketing operation. Both in discussions with executives of the parent organization and in the daily practice of journalism, you should explore creative approaches to achieve some measure of independence, to build credibility despite (and because of) your affiliation and to disclose conflicts and connections.\nCovering your funding organization\nIf your newsroom covers the field where the parent organization operates, you will eventually need to cover news that involves the funding organization. This is a good test of whether you truly are practicing journalism or PR. If you are able to independently report criticism of your parent organization and examine its performance, that’s a good indication that you’re building a journalism organization. If you go into damage control and focus primarily on reporting the viewpoint of the parent organization, you need to accept that you’re a PR organization and operate accordingly.\nThe main author of this section is Steve Buttry of Louisiana State University.\nSee also the “Reporting on Your Own Organization” section in this project."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:9fa64b85-f760-40c6-8f4e-235a9523926f>","<urn:uuid:0fd73786-fbd9-462e-b337-eed234a5b239>"],"error":null}
{"question":"How do the programming methods differ between Carmanah and CR Control Systems' LED navigation lights?","answer":"Both systems use infrared remote control programming, but they implement it differently. Carmanah's LED lights use an infrared programming device that can change flash patterns, intensity levels, activate/deactivate the light, and check battery levels. CR Control Systems' lanterns use a standard TV remote control as the programming device, with the lantern's own light serving as a response mechanism to complete the communication system. Their system also includes additional diagnostic commands for reading battery voltage, reporting flash pattern numbers, and flash intensity settings.","context":["Delaware River & Bay Lighthouse Foundation\n|A Carmanah Model 701 LED beacon attached to a buoy in Rohoboth Bay.|\nBeacons for our 21st century lighted aids to navigation will never rival the sparkling beauty of the classical lens but they sure do spawn intrigue and sport fascinating digital technology. The old saying, \"good things come in small packages,\" might aptly describe the latest advancement in LED optics created for the United States Coast Guard by Carmanah Technologies, British Columbia, Canada.\nThough the days of solar powered aids utilizing the 100 year-old technology of incandescent lighting have long been numbered, even recent LED (light emitting diode) advances could not shake the need for traditional external solar components - until now. Whereas current solar arrangements utilizing a 12-volt incandescent lighting system must be equipped with an external battery, solar panel and cable connections not to mention internal components such as lamps, lampchanger and flasher mechanism, the new 700 Series of Carmanah LEDs eliminates this cumbersome arrangement.\n|The optic, battery & solar panels are all-inclusive in the Carmanah 700 Series.|\nThe Carmanah 700 Series LEDs are a compact, durable and self-contained optic designed to operate flawlessly once installed, with no additional servicing or maintenance necessary for approximately 5 years before battery replacement is required. In addition, the LED optic has a life span of up to 100,000 hours or eleven years. By deploying one 12-lb self-contained Carmanah LED, Coast Guard servicing personnel eliminate the arduous task of having to hoist heavy 75-lb photovoltaic batteries, solar panels and fragile light source components up tall light towers and across rough terrain where many lighted aids are established.\nThe standard 155-mm red, green, and white acrylic optics that recreational boaters are accustomed to seeing on many buoys, piers and light towers will slowly be phased out as time moves forward and replaced by optics such as the Carmanah 700 Series LEDs. Though technological advances in lighted aids to navigation won't stop here, it is fascinating to watch the non-stop progression of improving our nation's guiding lights. Even the definition of automation as we know it in the world of AtoN seems inadequate to describe how lighted aids are advancing to achieve previously unheard of capabilities.\n|(L to R) MK2 Rich Wasilius, BM1 Jeremy McConnell, & SN Chad Stevenson use an infrared remote control to program the Carmanah LED light.|\nTwo factors that stand out most in the latest LEDs manufactured by Carmanah. One, the optic's reliability is nearly impeccable and two, the LED being a compact, all-inclusive unit versus its predecessors with all their external components, is a 21st century AtoN marvel that is best described with one word - wow!\nCarmanah LED Facts...\n- The Carmanah 700 Series is currently the only LED-based marine lights approved by the U.S. Coast Guard for use in the U.S. Aids to Navigation System\n- Distance of visibility up to 3 miles\n- Over 200 programmable flash patterns\n- Completely self-contained and sealed against environmental conditions\n- Quick, non-technical installation\n- An infrared programming device in the form of a remote control can change the flash pattern, intensity level, activate / deactivate and check battery level for current capacity\n- Replaceable battery packs\n- The Carmanah LED will charge under nearly all weather conditions\n- The Carmanah LED Model 701 weighs 11.5 pounds and the Model 702 weighs 17 pounds\n- The Carmanah LED Model 701 can operate under a full charge for up to 300 hours and the Model 702 can operate under a full charge for up to 600 hours.\n|FN Greg Panas holds up a buoy with a Carmanah Model 701 LED light.||Senior Chief Dennis Dever holds a Carmanah Model 702 LED light.|\nThe article and all photos were reproduced with the permission of the author, Mr. Bob Trapani, Jr. of the Delaware River & Bay Lighthouse Foundation. Visit www.delawarebaylights.com for more information.","CR Control Systems produces marine lanterns for navigation purposes. Their lanterns can be mounted on floating buoys or marine structures, and provide bright LED lights that flash in unique patterns as required by the customer’s application. The lights can be powered by batteries that are charged with solar panels for remote and floating buoy installations. As a small company, they do not have permanent engineering staff, so they came to MSRK Design to provide updates and extensions to their product lines.\nSome of the completed and ongoing projects are:\n• Develop circuitry and firmware to enable programming of their lanterns via a standard TV remote control, and the ability to retrieve diagnostic information from the installed device.\n• Replace an obsolete microcontroller with a more capable device and reduce the cost of the overall design.\n• Develop circuitry and firmware to protect the battery from overcharging by the solar panel.\n• Replace an obsolete LED driver with a newer device that is less expensive and more capable.\n• Develop circuitry and firmware to provide RF synchronization of the flashing pattern between several lanterns.\n• Respond to unique customer issues and special installation requirements.\nAn Ongoing Engineering Arrangement:\nWith the number of projects that needed to be pursued and a limited budget for engineering, MSRK Design agreed to act as a part-time engineering staff to CR Controls. Instead of quoting and invoicing each project individually, a monthly retainer is paid to MSRK Design. This allows CR Controls to set and change priorities for the projects, and pay a fixed cost per month. MSRK Design can seamlessly move from project to project, and work on several projects simultaneously so that progress is being made on all of them, instead of sequentially. CR Controls gets the benefit of having multiple experienced engineers available as needed, without paying for even one fulltime employee.\nIR Remote Control Technology:\nThe marine lanterns have hundreds of flash pattern codes that need to be programmed on a unit-by-unit basis. The programming may need to be changed while installed, so it is desirable to not open the buoy or attach cables to the unit. Infrared technology is inexpensive and adequate for signaling the lantern, and the lantern itself can respond with flashes of light to complete the communication system.\nMSRK Design selected the IR sensor, added the circuitry to the schematic, and specified the pcb layout for the hardware. MSRK Design worked with CR Controls to invent a programming protocol that used a standard TV remote control as the sending unit and the lantern light as the response. Then MSRK Design selected the specific Cypress PSoC controller and developed the firmware to implement the IR Remote Control System.\nIn addition, MSRK Design added other commands for diagnostic information, such as reading the battery voltage, reporting the flash pattern number, and the flash intensity setting. These features enable the customer to periodically check on the unit’s operation from a nearby boat.\nCost Reduction, Obsolete Component Replacement:\nThe original design used a PSoC from Cypress to control the lantern. The cost for these parts drop every year for a while, but then begin to rise as other customers change their designs to use newer parts and the overall volume drops. The original design had an external EEPROM to store the information for the hundreds of flash patterns, and an external dip switch to select the desired pattern.\nSince the IR Remote Control project enabled a new way to program the flash pattern, we took the opportunity to select a PSoC with a larger internal EEPROM and eliminated both the external EEPROM and the external dip switch. The resulting design was not only more capable, but significantly less expensive than the original.\nA similar opportunity for cost reduction occurred with the LED driver. The original component was obsolete and becoming more expensive. Alternatives were explored and a newer part was selected that is both better and less expensive.\nSolar Charging Technology:\nThe LED lanterns used in CR Control System’s buoys are very efficient, yet offer visibility up to 6 miles. The solar panels included in the lanterns need to be large enough to charge the batteries even on cloudy days. But if they are too large they can overcharge the batteries on sunny days, which can damage the batteries. So MSRK Design was retained to develop circuitry that monitors and manages the solar panel and the battery charging process.\nRF Synchronization of Lanterns:\nIf multiple lanterns are deployed in close proximity, it is desirable to synchronize their flash patterns so that at a far distance the flash pattern is distinguishable. The original design used wires between the lanterns to synchronize their flashes; this is difficult to install and limits the distance between lanterns.\nMSRK Design was asked to develop an RF synchronization option for the lanterns that could be more easily deployed. Various RF technologies were explored, including GPS, and an architecture for an inexpensive system was developed.\nCR Control Systems was able to update its product line without hiring a fulltime engineer. The newer products are substantially better, offer more features, and are less expensive to produce. And CR Control Systems is in a position to continue to respond to market demands by having a trusted group of engineers at its disposal.\nAccording to Bob Seidler, President of CR Control Systems,\n“As a small company we can’t afford to retain full time engineering staff for our various needs which include optics, electronics hardware and software and mechanical design. We contract with experts in each field to build long term relationships. When specific projects come up, we use our network of consultants to help us conceptualize, develop and design to completion our products. When we determined we needed electronics hardware and software services, we contacted and evaluated about seven different consulting firms. We found in our conversations and communications that MSRK “got it” when it came to understanding our technical as well as our financial limitations. We were able to break the project down into smaller sections by prioritizing our actual needs and came up with a timing and budget plan that works for us. We couldn’t be more pleased with the communications, the attentiveness, and the expertise of the MSRK staff. Their product support has been impressive. We wish we had found them sooner.”"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:bbbd28fc-1e01-4cf4-b2cc-f5a84e43101f>","<urn:uuid:e936383b-50a9-49b0-8acb-ef522d3dde02>"],"error":null}
{"question":"What are the linguistic differences between American and British English spelling conventions, and how can these cultural distinctions impact international business communications?","answer":"American English uses streamlined spellings like 'honor,' while British English maintains traditional spellings like 'honour.' This spelling difference reflects broader cultural variations in communication styles that can affect international business. The impact of such cultural differences is significant - for instance, Microsoft lost millions of dollars due to poor translations and cultural insensitivities in their software across different regions. Cultural misunderstandings in business communication can lead to substantial financial losses and missed opportunities, making it essential for companies to be aware of and adapt to linguistic and cultural variations when operating internationally.","context":["It’s no secret that Americans and Britons spell words differently. American English features many streamlined or simplified spellings that some Americans feel more accurately represents the way the language is actually spoken.\nThese spelling differences are reflected in the words honor and honour, which are two variations of the same word. One is the American spelling, while the other is British. Continue reading to find out if you should be using honour or honor in your writing, depending on for whom you are writing.\nWhat is the Difference Between Honor and Honour?\nIn this article, I will compare honor vs. honour. I will use each spelling in a sentence, and, at the end, I will show you a useful trick to determine whether you should choose honor or honour.\nWhen to Use Honor\nAs a noun, honor means respectability, virtue, or a code of conduct derived from a high value placed on the pursuit of these concepts. Some people and societies have meticulous, detailed codes of honor that involve strict rules for behavior and treatment of others. In other cases, honor is understood in a more general sense of uprightness and honesty.\nHere are some examples of honor used in a sentence as a noun,\n- It is my great honor to announce this year’s winner of the Tri-Cities Goat Breeding Competition, Dr. Meenakshi Agarwal.\n- David got a black eye while fighting for Kady’s honor.\nAs a verb, honor means to hold in high esteem.\n- Sam honored his grandparents by placing flowers on their gravestones each year.\n- I will accept no more requests for time off until the end of the month, but I will honor requests for time off that have already been submitted.\n- On a night when retired coach Steve Spurrier was honored along with his 1996 national championship team, No. 25 Florida looked nothing like a potential Southeastern Conference title contender. –The New York Times\nWhen to Use Honour\nWhat does honour mean? Honour is the British English spelling of the same word. It can be used in all of the same contexts as honor.\n- The Christian Bible commands young believers to “honour thy father and thy mother.”\n- The Serb joined Chelsea in 2008 and has won every domestic and European club honour during his eight years in London. –The Telegraph\nTrick to Remember the Difference\nHere is a helpful trick to remember honour vs. honor.\nHonor is the preferred spelling of this word in American English. In British English, honour is standard. Therefore, you should use honor with American audiences and honour with British audiences.\nYou can remember to use honour for British audiences since honour is spelled with a U, which is also the first letter of United Kingdom. This shared U should make it easy to remember than British audiences prefer honour.\nIs it honor or honour? Honor and honour are two spellings of the same word, which can be a noun or a verb.\nAs a noun, honor means respectability, virture, or a code of conduct valuing these concepts. As a verb, honor means to show esteem.\n- Honor is the preferred spelling in American English to American audiences.\n- Honour is the preferred spelling British English to British audiences.\nIt’s easy to remember that honour is the British spelling, since honour and United Kingdom are both spelled with a U.\nIf you need additional guidance, you can always refer back to this article.","Article: International Business — The Cost of Not Being Prepared\nWhat’s the cost of not being familiar with a foreign culture your company is engaging in?\nHow about a billion dollars? After all, that’s how much American mega-investor Kirk Kerkorian sued DaimlerChrysler for after their German chairman, Jürgen Schrempp, had bragged in a Financial Times interview that the merger between the two companies, officially promoted as a ‘merger of equals’, was really no more than a takeover. The case is still in court, but a similar class-action suit by other investors has already been settled by the company for $300 million. Technically, the issue was a legal one. Practically, however, what got DaimlerChrysler into trouble was that Schrempp lacked the cultural sensitivity and experience to realize that in the US, one simply won’t get away with that kind of two-faced behavior. The same act would expectably have much less dramatic consequences in his home country. In fact, it didn’t have any.\nMicrosoft reported losing several millions of dollars in India, the Arab world, and in South America because of cultural mistakes in some versions of their Windows program. Incorrect maps, poor translations that introduced offensive language, and other inappropriate material offended locals and in some cases led to government action. The company had to recall the affected versions, replacing huge quantities of its software packages. A spokesman admitted that \"some of our employees, however bright they may be, have only a hazy idea about the rest of the world\". As a consequence, Microsoft now sends their staff to dedicated training classes.\nA large high-tech corporation lost more than $10 million in development costs and missed market opportunities when they set up two of their international teams, one in Israel and one in Japan, to directly compete with each other in the same project, developing an important new product. What the division’s manager was not aware of was that in many cultures, such an approach sends a message to the team that it is incompetent and cannot be trusted. Rather than serving as a motivator as it might have in the U.S., the decision led to low morale, increased turnover, and poor results in both countries. The project had to be stopped and re-initiated.\nFortunately, most cross-cultural blunders are less severe, or at least less costly, than in these\nexamples. Nevertheless, the list still goes on and on about how businesses waste money and miss opportunities\nbecause of a lack of international experience or preparation.\nWhat Goes Wrong\nThere are three fundamental ways in which international business interactions and engagements fail or become more costly than they ought to be:\nFailure to cross the culture gap. The interaction falls apart because the parties involved are unable to bridge the culture gap between them. Many negotiations end at this stage. “They asked way too much”, “they expected us to accept the short end of the stick”, or “you just couldn’t trust these guys – they never lived up to their promises” are statements one might hear at the end of such failed attempts. Most of the time, these can be traced back to poor mutual understanding and faulty initial assumptions rather than bad intentions on either side.\n“Coopetition”. The cross-cultural interaction limps along, but the parties involved fail to communicate effectively and to build sufficient trust between them. As a result, the competitive element outweighs the cooperative one, introducing issues over contracts terms, intellectual property, budgets and payments, and so on. This case is both more common and more devastating than the previous one. Rather than adding value to a company’s global business strategy, such an engagement can become a major distraction from its key objectives and cause a lot of damage.\nLimited collaboration. The parties involved establish reasonable ways\nto communicate and interact. However, they never fully trust each other. In many foreign cultures, people\nwill not make any major business commitments unless a strong business relationship has been established and they feel\nthat the partner can be fully trusted. Americans may be more at ease here because its culture encourages a\ncompetitiveness that maintains an element of rivalry business partners are used to. Dealing with foreign partners\nthus represents a bigger challenge if the goal is to achieve extensive collaboration.\nSix elements can be identified that make or break the success of a global business interaction. All of them are ultimately linked back to people’s cross-cultural understanding (or lack thereof).\n#1 Strategic Objectives\nMany international business interactions suffer from poorly defined objectives. Strategy, goals and approach all need to be set with the target culture(s) in mind. Only if strategic objectives and tactics are well aligned with the other culture’s values, strengths, and preferences can a long-term gain be realized.\nLike any other aspect of running a business, success in cross-cultural interactions requires properly planning the approach. Strategic objectives need to be translated into a plan of action that defines steps, timing, roles, and responsibilities. That plan must also take into account the specific preferences and sensitivities of the targeted culture. Ad-hoc approaches in foreign countries have a very limited chance of success.\nNegotiating in a different cultural context is one of the toughest challenges in international business. What is effective and what is considered inappropriate varies greatly between countries. At the same time, the stakes are usually high and mistakes costly. Sending one’s best and most skilled negotiators won’t help much unless they are well-prepared. If they lack a thorough understanding of the other culture, the company may be in for a business disaster.\n( For more on this subject, see International Negotiation: How Do I Get Ready? )\nOnce a cross-cultural engagement is under way, visionary leadership becomes pivotal. Leaders will need to consistently demonstrate that they are serious about the engagement and willing to work through the cultural differences. That takes a strong commitment as well as the skills needed to identify sensitive areas and to act appropriately to build and maintain trust. Executives or middle managers who maintain an “us-versus-them” attitude can cause huge damage. Extensive communication, both within the own camp and with the foreign side, is also essential and requires constant leadership attention.\nThe importance of relationship and trust building triggers a need for proper facilitation throughout the engagement. While early in the interactions senior leaders often drive the progress, they may have to become less involved once the engagement is under way. At that point, it becomes essential that a facilitator be assigned who continues to build the relationship. Sending an expatriate who lives in the foreign country can be very effective, but only if he or she is sensitive and well familiar with the specific culture. Companies not paying attention to this aspect frequently find their employees inadvertently triggering confrontations that hurt the business relationship.\n#6 Team Preparation\nWell-defined strategy and good leadership are not enough to make global business interactions successful. Getting\nbuy-in from all team members involved is also essential. Without proper preparation for the engagement, cooperation\nwill likely be poor and concerns may prevail. The objective has to be to get both sides into the right mindset,\nopening up to the engagement as an opportunity rather than viewing it a threat. Again, it will be very important\nto understand and address any cultural differences. Aspects such as how to motivate a team can differ significantly\nand may dictate a new approach in a foreign culture.\nAs Globalization accelerates business around the world, companies are realizing that proper preparation for international business is a mandatory step that has a strong positive impact on the bottom line. Effective communication and trust building are the primary factors in making a foreign engagement successful. They are influenced by several elements that take careful planning and orchestration. While this requires significant efforts, it is critical to the business success, and the tradeoff between costs and benefits is clearly favorable.\n|Printable PDF version||written by Lothar Katz|\n( Copyright 2004-2015, Leadership CrossroadsTM )"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:0dbe6ce8-c49a-4281-ba65-547b69fa230f>","<urn:uuid:14c0a04b-d804-465e-ab9d-768ebf6debc0>"],"error":null}
{"question":"What are the genetic predictors of cardiovascular disease, and which lifestyle modifications can counteract genetic risks?","answer":"Several genetic variants affect cardiovascular disease risk, including those influencing LDL cholesterol, HDL cholesterol, and triglyceride levels, as well as variants in the 9p21 chromosome region. These genetic factors are associated with increased arterial wall thickness and carotid plaque. However, lifestyle modifications, particularly a diet rich in raw vegetables, fruits, and berries, can effectively counter these genetic risks. While the study examined other lifestyle factors such as physical activity levels and smoking, diet emerged as the most significant factor in mitigating genetic risk for heart attacks and cardiovascular disease.","context":["Researchers from the US and The Netherlands have found ways to earlier and better prediction of diseases such as diabetes, atherosclerosis, and heart disease through studying the genetic make-up of different varieties of lipids in blood plasma.\nStudying the genetic make-up of different varieties of lipids (fatty molecules) in the blood plasma of an individual can lead to a better and earlier prediction of diseases such as diabetes, atherosclerosis, and heart disease, two researchers told the annual conference of the European Society of Human Genetics on May 30. In the first study, Dr. Joanne Curran from the Texas Biomedical Research Institute, San Antonio, USA, told the conference that lipidomic profiling will become a more reliable early indicator of individuals likely to develop diabetes than the more commonly used predictors such as blood glucose and insulin levels.\nDr. Curran and colleagues from the US and Australia measured 356 different lipid varieties from about 1100 Mexican American members of large extended families who were part of the San Antonio Family Heart Study. The Mexican American population is at high risk of diabetes with about 25% of this population ultimately becoming diabetic. At the start of the research, 861 of the individuals studied did not have diabetes. However, over the 10 year follow-up examined in the study, 110 individuals did develop the disease.\nThe scientists were able to isolate 128 different varieties of lipids that predicted the progression to diabetes by measuring the the lipidomic profiles of each individual at multiple timepoints during the follow-up period. “The single best predictor we found was a novel component called dihydroceramide (dhCer). This was substantially increased in people with diabetes. It is also heritable, and appears to be an independent risk factor unconnected to blood sugar and insulin levels,” says Dr. Curran.\nAfter uncovering the link between dhCer and diabetes, the team searched the genome to find locations that harboured genes that influence dhCer levels. They identified a region on chromosome 3 that appeared to contain a gene with substantial importance for the production of dhCer. “Through whole genome sequencing, we are now attempting to identify this causal gene in the hope that it will be informative in the understanding of the pathogenesis of diabetes, and also suggest new avenues for treatment,” Dr. Curran says.\nIn the future, the researchers say, measurement of dhCer levels could become routine in the prediction of individuals likely to become diabetic. One of the difficulties of the current predictive methods is that they do not function until a patient is near to developing the disease. Being able to identify those at risk at the earliest stage would mean that individuals have plenty of time to make the lifestyle changes that could help them avoid the disease — through a change in diet, or increasing physical activity, for example.\n“Currently one in ten US adults suffers from diabetes and recently the Centers for Disease Control has predicted that this will increase to one in three by 2050,” says Dr. Curran. “We are optimistic that our discovery will lead to new treatments, but in the short-term the importance of finding out at an early stage whether any individual is likely to develop it cannot be overstated. A test based on dhCer levels will help to avoid the serious health effects that diabetes has in its own right, such as kidney failure, amputations, and blindness. It is, of course, also a risk for cardiovascular disease, so the health burden of this condition is enormous,” she concludes.\nIn the second study, Dr. Sara Willems, from the Erasmus Medical Centre, Rotterdam, The Netherlands, described to the conference research carried out on the influence of common genetic lipid variants on atherosclerosis and related heart disease. “A recent genome-wide meta-analysis of more than 100,000 individuals identified a large number of genetic variants associated with levels of LDL (bad) cholesterol, HDL (good) cholesterol and triglycerides. These molecules are, at increased levels of LDL and triglycerides and decreased levels of HDL, important risk factors for cardiovascular disease,” says Dr. Willems.\nThe researchers used risk scores from these genetic variants to test the hypothesis that their cumulative effects were associated with cardiovascular disease. For this purpose they used genetic data from more than 8000 individuals from the population-based Rotterdam Study and more than 2000 individuals participating in the Dutch family-based Erasmus Rucphen Family study.\nThey found an association between the LDL risk score and arterial wall thickness, and a strong association of this risk score with carotid plaque. These conditions can cause arterial blockage which leads to stroke. The same risk score was also associated with coronary heart disease.\n“Our findings show that an accumulation of common genetic variants with small effects on lipid levels can have a significant effect on clinical and sub-clinical outcomes,” says Dr. Aaron Isaacs, who led the project. “In the future, as our knowledge of genetic variation increases, effective pre-clinical genetic screening tools may be able to enhance the prediction and prevention of diseases such as cardiovascular disease.”\nNew genetic variants influencing lipid levels are being identified all the time, the researchers say. “As new variants are discovered, we would like to be able to continue to test them, both singly and combined, for association with cardiovascular disease. The cost of these diseases to individuals, families, society and healthcare systems is immense,” says Dr. Willems.\n“Cardiovascular disease is the main cause of death in Europe, killing over 4 million people per year. It also represents 23% of the total disease burden (illness and death) across the continent. Managing cholesterol levels is important for prevention. This can be done early in life by effective treatment. We hope that our study, showing that common genetic variants play an important role in the occurrence of cardiovascular disease, marks a starting point for early prediction and prevention and may thus reduce the burden of disease,” she concludes.","The Daily Express reports that a “wonder diet cures heart disease” and goes on to say that “a simple diet packed with fruit and raw vegetables is the key to beating heart disease.”\nThe news report is based on a large study that looked at how certain genetic variations known to increase a person’s risk of heart attack and cardiovascular disease (CVD) are influenced by lifestyle factors, such as diet, physical activity levels and smoking.\nThe study found that some of the effects of these genetic variations could be countered by a diet high in raw vegetables, fruits and berries. Raw vegetables seemed to have particularly important effects. The researchers found similar effects when looking at the risk of CVD and diet in a different group.\nThis well conducted study’s findings indicate that people with specific genetic risk factors for heart attack can reduce their risk through a diet high in fresh fruit and vegetables. It does have some limitations in that it relied on people accurately recalling their food intake and assessed only one area of genetic variation. Despite these however, the findings appear to be robust. As about 50% of the ethnic groups tested in this study carried one of the four risk variants, the application of these findings to the general population is likely to be high.\nWhere did the story come from?\nThe research was led by researchers from McGill University in Canada in collaboration with a number of researchers from other universities around the world. It was funded by a grant from the Heart and Stroke Foundation of Ontario and other grants associated with the collaborating researchers.\nThe study was published in the peer-reviewed medical journal Public Library of Science (PLoS) Medicine .\nGenerally, this study was reported accurately in the media although some headlines may have exaggerated the significance of these findings. For instance, the Daily Express’ headline says, ‘Wonder diet cures heart disease’. However, although the study found this diet to be of benefit for heart disease, the findings do not signify a cure.\nWhat kind of research was this?\nThis study investigated how diet could influence a person’s risk of heart attack and CVD when they possessed particular genetic variations that increased their risk.\nThis was a gene-environment association study using participants enrolled in the INTERHEART study, a global retrospective case-control study that investigated potential risk factors for heart attack.\nCardiovascular diseases (CVDs) affect the heart and blood vessels and are a leading cause of illness and death in most developed countries. Lifestyle factors such as diet, exercise and smoking, as well as genetic factors, influence a person’s risk of developing CVD. Recent studies have identified several genetic variations that are associated with an increased risk of CVD. One of these areas is in a region of the chromosome (the structure DNA is packed into in each cell) called 9p21.\nThis study looked at how environmental factors were linked to variations in the 9p21 region and how the interaction of these influenced a person’s risk of heart attack and CVD.\nWhat did the research involve?\nThe researchers assessed four different genetic variations (called single nucleotide polymorphisms SNPs) within the 9p21 chromosome region of DNA. They compared the genetic information of 3,820 participants who had had a non-fatal heart attack, with that of 4,294 healthy controls. All the participants were enrolled in the INTERHEART study and were from five ethnicities: Europeans, South Asians, Chinese, Latin Americans and Arabs. This made up 27% of the total people enrolled in the INTERHEART study.\nThe main analysis of the INTERHEART data was in two parts. In the first part, the researchers looked at the effect of the four SNPs on the risk of heart attack. In the second, they looked at how this risk was influenced by environmental factors such as smoking, activity level and diet.\nDiet was assessed by a short food frequency questionnaire of 19 food items. These were then grouped into three dietary categories the researchers called oriental (soy sauce, tofu, pickled foods, green leafy vegetables, eggs and low sugar), western (eggs, meats, fried and salty foods, sugar, nuts and desserts), and prudent (raw vegetables, fruits, green leafy vegetables, nuts, desserts and dairy products). For the prudent diet, fresh vegetables, fruits and berries made up the largest components of the score.\nThe researchers aimed to validate their findings from the INTERHEART study in a large group of people that were enrolled in a different study that looked at CVD. This second group were enrolled in a prospective study, called the FINRISK study, containing information on 19,129 Finnish individuals, in which there were 1,014 cases of CVD. The analysis of the FINRISK participants used different methods to assess participants diet than the INTERHEART study.\nThe statistical analysis was appropriate for this type of study.\nWhat were the basic results?\n- All four specific variations of the SNPs that were tested increased the risk of heart attack by about a fifth compared to other variations of the SNP (odds ratios ranged from 1.18 to 1.20 for all individuals from all ethnicities combined).The amount that the risk increased for each of the SNPs was affected by ethnicity with South Asians at the highest risk.\n- An individual’s risk of heart attack was affected by whether or not they had a prudent diet pattern and which variant of the SNP they had. For example, those that carried a specific SNP variant called rs2383206, and who ate a diet poor in fruit and vegetables, had a higher risk of heart attack than those without the variant. However, carriers of rs2383206 who ate a prudent diet, had the same heart attack risk as those without the risk variant.\n- The influence of the prudent diet diminished when the researchers removed the influence of raw vegetable intake. This did not occur when other elements of the prudent diet were removed. This suggested raw vegetable intake is a key component of the influence.\n- The more key components of a prudent diet that SNP risk carriers ate, the lower their risk of heart attack compared to those who ate less of this diet.\n- The effect of the SNPs on the risk of heart attack was not influenced by physical activity level or smoking.\n- Similar dietary interactions were seen to influence the risk of CVD in the FINRISK study.\nHow did the researchers interpret the results?\nThe authors say they have demonstrated that different variants of 9p21 SNPs have a consistent effect on the risk of heart attack and CVD in people whose diet has only a low ‘prudent diet score’. The risk diminished the higher an individual’s prudent diet score was.\nThey say that although it is now known exactly how this association works, they believe that their ‘results support the public health recommendation to consume more than five servings of fruits or vegetables as a way to promote good health.’\nThis large gene-environment association study provides new insight into the influence of diet on mitigating the increased risk of heart attack associated with specific variations in the 9p21 chromosome region.\nThe study has some limitations, the main being that the data from the two populations studied (INTERHEART and FINRISK) was collected and analysed in different ways. INTERHEART looked at the effect on the risk of heart attack, while FINRISK looked at the risk of CVD.\nThe SNPs were analysed differently. The cardiovascular outcomes were made using different criteria (heart attack versus CVD) and there were differences in how the diets were measured. As such, it is not possible to be sure a prudent diet has the same effect on the risk of heart attack as it does on the risk of CVD. Further studies focusing on the specific elements of diet and their influence of CVD would be needed to confirm this.\nIn addition, the results were dependent on the participants completing the food frequency questionnaires themselves. All dietary studies that use questionnaires are limited by the participants’ ability to recall the food they have eaten accurately. Although these researchers used careful methods to try and eliminate this bias, it is still possible that some inaccuracy was introduced.\nThere are likely to be many genetic variations and environmental factors that determine an individual’s risk of having a heart attack or developing other CVDs. This study looked at just one area of genetic variation and, while this is an important finding, there will be many others that also contribute to an individual’s overall risk. It is not known whether diet would affect these other areas of genetic variation in the same way as demonstrated in this study.\nWhile these findings have some limitations, and ideally would be confirmed in further studies, they do correspond with what is already known about eating a healthy, balanced diet containing fruit and vegetables in order to promote good health.\nOverall, this was a good study that showed people with specific genetic risk factors for heart attack may reduce their risk through a diet high in fresh fruit and vegetables to that similar to someone without the genetic risk factors. In this study, about 50% of the ethnic groups tested carried one of the four risk variants, and so the application of this study to the general population is likely to be high.\nThis research supports the well-established recommendations to consume more than five servings of fruits or vegetables as a way to promote good health."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:e96c0bde-020b-44e6-b58a-d79f4cca852a>","<urn:uuid:02566a27-095c-400f-8f43-17ec8dfa2816>"],"error":null}
{"question":"As someone interested in spacecraft technology, could you compare how SMART-1's ion engine propulsion system differed from GRAIL's final mission operations in terms of fuel usage and end-of-mission procedures?","answer":"SMART-1 and GRAIL had distinct approaches to fuel usage and mission endings. SMART-1 used a solar-electric propulsion system (ion engine) that consumed only 59 kilograms of xenon propellant out of 82 kilograms during its journey, with the extra fuel allowing for orbit adjustments and potential mission extensions. In contrast, the GRAIL spacecraft (Ebb and Flow) were intentionally crashed into a lunar mountain near the north pole when they were nearly out of conventional fuel, impacting at 3,750 mph to avoid unintentional crashes near important sites like Apollo landing locations.","context":["Smart Way to the Moon\n|SMART ion engine.\nSMART-1 is successfully making its first orbit of the Moon, a significant milestone for the first of Europe's Small Missions for Advanced Research in Technology (SMART) spacecraft.\nA complex package of tests on new technologies was successfully performed during the cruise to the Moon, while the spacecraft was getting ready for the scientific investigations which will come next. These technologies pave the way for future planetary missions.\nSMART-1 reached its closest point to the lunar surface so far - its first 'perilune' - at an altitude of about 5000 kilometers at 18:48 Central European Time (CET, or 12:48 ET US) on 15 November.\n|Electrical solar energy for SMART probe.\nJust hours before that, at 06:24 CET, SMART-1's solar-electric propulsion system (or 'ion engine') was started up and is now being fired for the delicate maneuver that will stabilise the spacecraft in lunar orbit.\nDuring this crucial phase, the engine will run almost continuously for the next four days, and then for a series of shorter burns, allowing SMART-1 to reach its final operational orbit by making ever-decreasing loops around the Moon. By about mid-January, SMART-1 will be orbiting the Moon at altitudes between 300 kilometers (over the lunar south pole) and 3000 kilometers (over the lunar north pole), beginning its scientific observations.\nThe main purpose of the first part of the SMART-1 mission, concluding with the arrival at the Moon, was to demonstrate new spacecraft technologies. In particular, the solar-electric propulsion system was tested over a long spiralling trip to the Moon of more than 84 million kilometers. This is a distance comparable to an interplanetary cruise.\n|Lunar Clementine mission shows the South Pole of the Moon. The permanently shadowed region center showed earlier evidence of meteor cratering and ice never exposed to direct sunlight, but Arecibo radar reveals dust.\nCredit: NASA/DOD Clementine\nFor the first time ever, gravity-assist maneuvers, which use the gravitational pull of the approaching Moon, were performed by an electrically propelled spacecraft. The success of this test is important to the prospects for future interplanetary missions using ion engines.\nSMART-1 has demonstrated new techniques for eventually achieving autonomous spacecraft navigation. The OBAN experiment tested navigation software on ground computers to determine the exact position and velocity of the spacecraft using images of celestial objects taken by the AMIE camera on SMART-1 as references. Once used on board future spacecraft, the technique demonstrated by OBAN will allow spacecraft to know where they are in space and how fast they are moving, limiting the need for intervention by ground control teams.\n|Moon occulting Venus, the morning star, taken by the lunar probe, Clementine.\nSMART-1 also carried out deep-space communication tests, with the KaTE and RSIS experiments, consisting of testing radio transmissions at very high frequencies compared to traditional radio frequencies. Such transmissions will allow the transfer of ever-increasing volumes of scientific data from future spacecraft. With the Laser Link experiment, SMART-1 tested the feasibility of pointing a laser beam from Earth at a spacecraft moving at deep-space distances for future communication purposes.\nDuring the cruise, to prepare for the lunar science phase, SMART-1 made preliminary tests on four miniaturized instruments, which are being used for the first time in space: the AMIE camera, which has already imaged Earth, the Moon and two total lunar eclipses from space, the D-CIXS and XSM X-ray instruments, and the SIR infrared spectrometer.\nIn all, SMART-1 clocked up 332 orbits around Earth. It fired its engine 289 times during the cruise phase, operating for a total of about 3700 hours. Only 59 kilograms of xenon propellant were used (out of 82 kilograms). Overall, the engine performed extremely well, enabling the spacecraft to reach the Moon two months earlier than expected.\nThe extra fuel available also allowed the mission designers to significantly reduce the altitude of the final orbit around the Moon. This closer approach to the surface will be even more favourable for the science observations that start in January. The extra fuel will also be used to boost the spacecraft back into a stable orbit, after six months of operations around the Moon, in June, if the scientific mission is extended.\nSMART-1 will be looking at the darker parts of the Moon's south pole for the first time. It will be mapping the so-called Peak of Eternal Light, an eerie mountaintop that is permanently bathed in sunlight, while all around are dark craters never touched by the Sun. These craters are believed to harbour water-ice in the lunar soil.\nSMART-1 will also help scientists to confirm if ice is present at the lunar poles, where the temperature never rises above -170°C. Any water on the lunar surface would be very helpful in the creation of permanent bases on the Moon.\nAmong important milestones in moon exploration over the last decade, the successful HITEN (formerly called MUSES-A) Japanese mission was launched in 1990, to perform a sophisticated Earth-Moon circumnavigation, including Earth atmospheric breaking. HITEN was finally directed to impact on 10 April 1993 near Stevinus crater on the southeast part of the lunar side. This hyper-velocity planetary impact could be observed from Earth.\nBetween February and May 1994, the American Clementine orbiter observed the Moon with visible, and infrared imagers and a laser ranger which mapped practically the entire lunar surface with a 200 m resolution. Its most remarkable discovery was the strong mineralogical and tectonic large-scale inhomogeneity of the Moon. This explains why, even with the Apollo and Luna samples taken from near-side equatorial areas, we do not know the global Moon.\nClementine also discovered the South Pole Aitken basin on the far side, the largest impact basin (2500 km) in the Solar System. Clementine confirmed earlier evidence of permanently shadowed polar craters. A bistatic radar experiment looked at the possible presence of ice in certain polar craters. Clementine's first objective had been to demonstrate new technologies for the United States Strategic Defense Initiative Organisation but lunar and planetary science benefited greatly.\nLunar Prospector followed, launched in January 1998. The low-cost NASA Discovery mission had been designed to provide answers to long-standing questions about the Moon, its resources, structure and origins. Lunar Prospector's 18-month mission reaped very valuable scientific data, mapping thorium and potassium radioactive elements and iron. In March 1998 the Lunar Prospector team confirmed the existence of surface hydrogen, with an enhancement at the lunar poles interpreted as water ice. The spacecraft was then deliberately crashed into the Moon's South Pole, in the hope, a vain one, that this water projected into space would be observed and analyzed from Earth.\nRelated Web Pages\nLong, Strange Trips\nMoon Meteor Truly Extraterrestrial\nMoon Written in Stone\nSMART-1: Chips Off the Terrestrial Block\nTreasures from the Lunar Attic\nEnd of an Era, Dawn of Another?\nMaking the Moon","NASA’s twin GRAIL (Gravity Recovery and Interior Laboratory) spacecraft have spent the last year mapping the moon’s hidden heart.\nToday at 4:28 p.m. CST they’ll be intentionally crashed into a mountaintop near the lunar north pole. You might think this a poor use of a half-billion of your tax dollars, but the mission has been an astounding success.\nNow that the paired probes are nearly out of fuel, mission controllers have directed them to the far north of the moon to avoid an unintentional crash near something important like an Apollo landing site.\nNicknamed Ebb and Flow, the GRAIL probes have been orbiting in tandem around the moon for nearly a year, peeling back the moon’s internal layers by mapping variations in its gravity field. As they fly over areas of greater and lesser gravity caused by both visible features, such as mountains and craters, and masses hidden beneath the lunar surface, the distance between the two spacecraft changes slightly. By measuring these slight variations in separation and combining that with accurate time and position information, scientists created the color-coded gravity map above.\nThe lunar twins discovered the moon’s crust is 21 to 27 miles thick or 6-12 miles thinner than previously thought. It also contains the same amount of aluminum as found in Earth’s crust.\nBoth findings confirm the moon’s origin via the “giant impact hypothesis.” It’s believed the moon formed from materials launched into orbit when Earth was struck by a Mars-sized planet in the chaotic days of planet formation over 4 billion years ago.\n“With this crustal thickness, the bulk composition of the moon is similar to that of Earth. This supports models where the moon is derived from Earth materials that were ejected during a giant impact event early in solar system history,” said GRAIL co-investigator Mark Wieczorek.\nGRAIL’s gravity map reveals familiar lunar features like craters, mountains and lava plains, but with its unique ability to probe beneath the surface, the probes discovered a thoroughly fractured and pulverized lunar crust riddled with subsurface “linear gravity anomalies” more than a 100 miles long.\nImpacting meteoroids and asteroids common during planetary formation 4.5 billion years ago created these deep cracks in the moon’s crust that later filled with lava and solidified into dense pathways of rock.\nSays Jeff Andrews-Hanna, a GRAIL guest scientist with the Colorado School of Mines: “These linear gravity anomalies indicate the presence of dikes, or long, thin, vertical bodies of solidified magma in the subsurface. The dikes are among the oldest features on\nthe moon, and understanding them will tell us about its early history.”\nWhile plate tectonics has recycled Earth’s early crust, other planets may still retain similar deep fractures attesting to the bombardment experienced during their formation long ago. Maria Zuber, the MIT geoscientist leading the GRAIL mission, thinks it possible that the cracks could have served as refuges for microscopic life on a planet like Mars when its climate changed from wet to the current cold and dry. That’s quite a wonderful thought really – that bombardment during the formation of the moons and planets created sanctuaries for life.\nWith the two probes now nearly out of fuel and orbiting only as high above the lunar surface as a jetliner, they’re on a collision course with an unnamed mountain near the crater Anaxagoras. The scene will be hidden in shadow at the time of impact. The first to go will be Ebb at 4:28:40 p.m. (CST) with Flow following 20 seconds later.\nSlamming into rock at 3,750 mph, each probe will create a faint flash of light upon impact that might be visible through a telescope. Chances are slim, so don’t expect a big bang, but if you’re game to try, you can use the maps here to hone in on the impact location. Who knows, maybe you’ll see a tiny star-like point of light in the darkness at the appointed time. Observers on the East Coast are favored since the flashes will occur when the moon’s in a dark sky. For Duluth, Minn. and around the Midwest the sun will just be setting at that time.\nNASA will provide live commentary of the scheduled lunar surface impacts beginning at 4 p.m. CST. Check out NASA TV or USTREAM to watch and listen. The Lunar Reconnaissance Orbiter (LRO) has already taken photos of the site before the crash and will photograph it soon after to record Ebb and Flow’s impact craters.\nVisit the GRAIL site for more information about the mission.\nUPDATE: Both craft crashed into the mountain at their appointed times. NASA plans to petition the International Astronomical Union (IAU) to name the impact site(s) in honor of Sally Ride, GRAIL team member and the first American woman in space. Ride died earlier this year."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:247cc463-2387-40a6-afd9-4894ccc14df7>","<urn:uuid:5692f746-5d54-4e7a-87e9-cf5dabd7c99f>"],"error":null}
{"question":"How has astrophotography equipment evolved over time, and what are the modern techniques for finding celestial objects in the sky?","answer":"Astrophotography equipment has evolved from early basic cameras to modern sophisticated systems including video equipment, CCD cameras, digital single lens cameras, and computer software for image processing. In terms of finding celestial objects, modern astronomers can use computerized mounts with object databases for automatic targeting. However, for manual targeting, astronomers use a technique called Star Hopping, which involves starting from a bright, easily visible object and moving from star to star using finder charts until reaching the target. This requires understanding the finderscope's limiting magnitude and field of view, and often involves using software like Cartes du Ciel to generate appropriate charts.","context":["If there’s one step that I personally am guilty of skimping on, it’s this one. I generally have an idea of what I’d like to capture on a given night, but often get distracted by all the other lovely possibilities in the sky, and star simply snapping 20 or so frames of random objects. Although this has led to a few happy successes, for the most part I end up with mediocre images. Now there’s nothing intrinsically wrong with this; if nothing else, it’s good practice at imaging a variety of objects, and helps me to hone my craft. But if you want your images on the covers of magazines, you need to take the time and effort to get as much of the best quality data you can manage. And that requires careful planning.\nYou see, each object is different, and requires a different configuration. Planets are small and bright, so you might want to boost the focal length of your telescope with a barlow lens and snap hundreds or thousands of frames at very short exposure lengths. Emission nebulae can be quite large and faint, so you might choose to replace the barlow lens with a focal reducer and shoot very long exposures. For any combination of equipment configuration and viewing conditions, you’re going to need to capture a full set of calibration images (darks and flats), and that takes time to do properly. If you decide to swap targets, you’re either going to have to go with your existing configuration and shoot with the wrong settings, or change the configuration and have to start calibration all over. One option gives you less than perfect images, the other wastes a large slice of your evening. So lesson one from this section has to be “Chose which single object you’re going to capture, and stick with that decision”.\nWhen to observe\nHaving chosen an object, you need to choose a date. As any astronomer knows, all the stars, galaxies and other objects on the celestial sphere rotate around the Earth once every 23 hours and 56 minutes. This means that every objects rises four minutes earlier each night. Or to put it another way, if you look for that object every single night at the exact same time, you’ll find it has moved East by four minutes of Right Ascension every night. Over the course of a full year, those minutes add up to one full rotation around the sky, which is why some constellations are visible in Summer, and others only in Winter. So if you want to image, say, the great nebula in Orion, but it’s the middle of June, then you’re fresh out of luck. It won’t be visible in the evening skies for many months. Therefore: choose a date.\nWhen I studied observational astronomy at the University of South Africa, we were taught to calculate the Sidereal time, and compare that to an object’s Right Ascension to determine whether it would be above the horizon or not. I was never particularly good at this, so I prefer to simply fire up my copy of Stellarium, search for the object, set the time to somewhere around 9pm (when I’m most likely to observe), and then advance the date till my object is nice and high above the horizon. Specifically the North or Western horizons, because my observing site has bad sky glow to the South and East, from neighbouring towns. However you do this, remember to think about rising and setting times, and learn to modify these to suit your local environment. If you’re intending to spend 6 hours imaging (not unusual for the serious guys), that means you have to start more than six hours before the object sets. I’ve made this mistake before, and it’s extremely frustrating to realise, half way through an imaging session, that you’re going to have to pack up because your target is about to vanish behind a distant hill top. Or the roof of your own house.\nStalking your target\nIf you have a nice computerised mount, with a good database of objects, and if the electronics haven’t failed, then you can skip this section. Just enter the name of the object into the hand controller and enjoy the electronic grinding noises while your telescope robotically heaves itself into position. But if you favour a more hands-on approach, and are imaging anything that you can’t easily see with the naked eye, you’ll need some star charts. If you’re shooting a widefield view, with just standard camera lenses, then the sorts of finder charts one finds in magazines will be fine since you dont have to aim too precisely. But to find a faint object through a telescopes narrow field of view, without electronic assistance, the only way to get around is by using a technique called Star Hopping.\nStar hopping is simply a matter of pointing the telescope at some bright, easily found object, and then hopping from star to star till you reach your destination. It takes a little practice, though, and a lot of preparation. You’ll need to know the limiting magnitude of your finderscope (that’s the faintest stars that it can see), and the size of its field of view. Start with a chart that shows stars down to the limiting magnitude of your finderscope, and find the target object. Then, find some bright object nearby that you can use as a starting point. This bright object has to be something that you can find with your naked eye, and get into view through the finder without too much effort, otherwise you’re going to have trouble getting started. Then you simply plot a course from star to star, leading to your target. Remember that no hop should be further than the field of view of your finderscope, else you might get lost trying to make those big leaps. And similarly, the stars between hops need to be bright enough that you can see them without straining. If you want to go old school, use a piece of tracing paper over a printed starchart to draw your own finder chart, showing the route. Oh and… remember that telescope views are usually upside down, or mirrored left-to-right, or both, so you’ll have to turn the completed chart appropriately to match your view. Alternatively, use a software package like Cartes du Ciel (recently renamed to Sky Map, after the French author realised how many english speakers use his excellent software) to generate these charts on a computer.\nOnce you’ve got all this done, and you’re out in the dark with the target acquired, you’ll still need to take a bit more time to take test shots and ensure that it is nicely centered on the camera sensor. Once you’re satisfied that it’s exactly where you want it in your test shots, and you’ve double-checked that everything is still sharply focussed, then you can mark the target as “Acquired”, and move on to actually taking some pictures.\nThere are always outside factors that you can’t do much about, such as that the weather, or light pollution, or an unsympathetic spouse. But you can usually find ways to plan around these obstacles. Many individual sources of light pollution are turned off late at night, and if this is the case in your neighbourhood, you can try imaging in the early morning instead of late evening. Or you could plan a weekend away in a nice dark spot away from the city lights. You can deal with weather by keeping a careful eye on weather reports and getting to know your local climate. Where I live, we have consistently dry and cloudless winter evenings, making for excellent viewing, but Summer is humid and rainy and cloudy. I know it’s not worth planning more than a day or two ahead in Summer because I cannot rely on the weather, so I worry less and play more. Winter, by contrast, is for serious viewing campaigns, because I know that I can image through the night and then leave my gear set up outside to continue the following evening without worrying about rain or dew. And if your spouse is unhappy at losing you to the stars yet again, try to get him or her involved so they can join you outside, or perhaps find the time to give them extra undivided attention inbetween your viewing sessions.\nYour equipment can and will find interesting and totally original ways to interrupt your imaging sessions. Whether it’s as embarassing as a simple flat battery that you forgot to replace, a forgotten finderscope that you forgot to pack before a weekend excursion, a data cable that didn’t have enough slack and is now being dragged from its socket by your drive motors, equipment failure is frustrating and embarassing and usually fatal to your plans. I’ve experienced every one of these examples at least once, and have learned from them, but youe best bet here is to draw up checklists. If you know that your camera battery will last for 300 shots, make sure that you have a spare, and that they’re both fully charged before heading out to capture 400 frames, for example. The more time you spend thinking about ways things can fail, and the more actual failures you’ve lived through, the longer your list will get and the safer you’ll be.\nBut it’s not just the machinery you need to look after. Are you dressed warmly enough? Are you going to be out late enough to need some coffee to keep you awake? Are you going to get hungry? Do you have a table for your laptop, and something to sit on, or will you need to squat on the dew-soaked ground for hours, balancing a laptop on your knees? These considerations all seem minor when the Sun is busy setting and you’re impatient to get started. They don’t seem minor at all when it’s past midnight and you’re looking at a few more hours before you can head back to shelter. These little details are not luxuries!\nOf course, you can get by without following any of the advice on this page. Many forms of astrophotography can be done quite quickly, with no preparation at all. Eyepiece projection especially is a spur-of-the-moment thing, whipping out a cameraphone and pressing it to the eyepiece of a telescope. But if you want to start taking serious images that will impress other photographers, if you’re spending money on serious equipment and want your results to be worth it, you need to take the time to plan each shot carefully.","Author : Lowell Bradford\nAstrophotography is the process of taking photos of objects in space. Whether photographing a celestial object visible with the naked eye, such as the moon or a group of stars, or astronomers photographing space with the Hubble telescope, photographing space is astrophotography. The practice of astrophotography does not date as far back as some other sciences, simply because it depends on photography. Photography didn’t become a viable invention until the early 19th century. The first case of astrophotography took place in 1840 when the moon was photographed for the first time. Over a century later, astrophotography is the method used to capture the world’s most phenomenal images of space. Astrophotography is available to everyone. From professional astronomers to the backyard skygazer, all can enjoy the wonders of astrophotography.\nOverview of Astrophotography\nThere are several methods used in astrophotography including various techniques, camera and video equipment, and various telescopes. Understanding the type of equipment used in astrophotography will help ensure you select the best methods for taking photos. The first thing to understand is that astrophotography is different from standard photography. Those accustomed to taking standard photos may find photographing celestial objects is more difficult than anticipated. Lighting, shadows, atmospheric changes, and the distance of heavenly objects must be taken into account to ensure the best photos are captured.\nTelescopes should be adjusted to accommodate for the rotation of the earth. This is accomplished by setting equipment to rotate in an opposite manner from earth. Telescope mounts are an important device used in astrophotography that ensures photos captured are precisely timed and accurately track heavenly objects. Preventing tracking errors is vital to successful astrophotography and there are modern-day breakthroughs in computer science that helps make that happen.\nAstrophotography is one of the oldest forms of science-based photography. Beginning in the 19th century, early astrophotography consisted of photographing the moon, stars, eclipses, and nebulas. The procedure requires long exposure and early images were known for being quite blurry. It was not uncommon for telescopes to lose power, focus, or direction during a prolonged photo shoot.\nThe earliest astrophotography photos captured were of the moon, followed by the sun and stars. It wasn’t until the early 20th century that astrophotography began to become an important scientific research method. The invention of refracting telescopes enabled more powerful imaging to be captured. As the 20th century progressed, new telescopes such as the Hale, Samuel Oschin, and Hubble revolutionized the art of professional astrophotography. By the late 20th century, new telescopes and equipment allowed for some of the most awe-inspiring space photos to be captured for the first time in history.\nWhile astrophotography is an important tool used in scientific applications, it is also a popular hobby. Advances in video equipment, standard, and digital cameras have enabled the backyard enthusiast to capture amazing photos. Computer software makes it possible to take photos that appear as though a professional took them. From the novice to the advanced hobbyist, everyone can enjoy the art and science of astrophotography.\nA variety of media devices and equipment setups are used. Examples include video and webcams, CCD, over-the-counter cameras, single lens and digital single lens cameras. Computer software may be used to adjust cameras and telescopes to zoom in on certain objects then take pictures. Photos taken may be edited in image processing software, for a clearer picture. Please consult the following links for additional information on astrophotography."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:0efd6ac8-92c7-4431-8255-4c1d9f22a0e7>","<urn:uuid:98662f17-7243-44e6-8b93-f249dc488e6a>"],"error":null}
{"question":"How do British honey bee conservation efforts engage the public, and what are the technical approaches to varroa mite control in Germany?","answer":"British conservation efforts focus on public engagement through the BBKA's campaign, which found that 58% of British adults would help if they knew how, and 83% of children aged 5-10 believe in environmental protection. They provide practical guidance including planting flowering trees, creating vegetable patches, and building bee habitats. In Germany, the technical approach to varroa control follows a strict schedule: cutting drone brood and making splits from April-June, followed by three Formic acid treatments in July-September, and an Oxalic acid treatment in December. Their breeding program focuses on two resistance parameters: pink-killing brood testing and monitoring varroa population development throughout the season.","context":["Help save our honey bees is the rallying cry of a new campaign aimed at getting the next generation involved and featuring one of honey's biggest fans\n\"A day without a friend is like a pot without a single drop of honey left inside\", said Winnie-the-Pooh of his favourite afternoon snack. Who better then to front a new initiative from the British Beekeepers Association (BBKA) aimed at encouraging children to help save our diminishing British honey bees?\nThis launch comes as a response to new research commissioned by the BBKA confirming a decline of 14.5% in England's honey bee colonies over the 2014-2015 winter. But while these figures are disheartening - particularly given the vital role that we all know bees play within our delicate ecosystem - encouragingly, both children and adults alike have expressed a desire to help but a lack of understanding of just how to go about that. 58% of British adults pledged that they would do more to help if only they knew how. While 83% of British children aged 5-10 were found to believe it is important to look after our environment.\nDavid Aston, President of The British Beekeepers Association said: \"While many people are aware of the plight of the honey bee, there are many that don't know what they can do to help. We hope that by supporting our 'Friends of the Honey Bee' initiative, families across the country can get involved, making a practical contribution and supporting bee health research.\"\nHelp is at hand\nThursday 25 June sees the launch of a brand new 'bee-friendly' guide, inspired by Winnie-the-Pooh and friends which is available in printed form as part of a BBKA 'Friends of the Honey Bee' member pack or downloadable from Friends of the Honey Bee. The guide aims to inform families of the numerous ways in which they can help support the continuation of our honey bee colonies for generations to come.\n10 simple steps to help save honey bees are included, ranging from planting up a window box and creating a vegetable patch to building bee habitats and becoming a beekeeper. Each step is illustrated with a full colour image of Winnie and friends taking action to save honey bees and - perhaps more importantly for Winnie himself - the future supply of fresh, local, British honey.\nDie hard fans of everyone's favourite Hundred Acre Wood-dwelling bear need fear not as the illustrations have been beautifully created by illustrator Mark Burgess, who coloured the original E.H. Shepherd black and white art.\nThe new guide includes a brand new short story and a list of ten simple activities to support British honey bees, as recommended by Winnie-the-Pooh (with a little help from the BBKA).\nWinnie-the-Pooh's 10 simple steps to help save the honey bees\n1. Plant your own window box2. Create your own vegetable patch or tub3. Plant a flowering tree in your garden4. Make some 'seed balls' and throw them into the wild5. Use arts and crafts to educate the younger generation on the importance of bees e.g. finger puppets, painting bee pictures, knitting bees etc6. Learn to become a beekeeper yourself by attending a course through the British Beekeepers Association7. Bake together at home using local honey8. Build bee habitats9. Volunteer for your local beekeeper association or visit your local apiary10. Don't panic, if you see a local honey bee swarm, contact your local beekeeping association","Varroa resistance breeding is now for us a feasible and promising way. The beekeeping system of Ed & Dee Lusby, with small cell bees, represents for us a basis which allows the bees to actively defend themselves against the Varroa mites. The additional use of selective breeding, in accordance with the experience of Harbo, gives us the opportunity to bring Varroa tolerance breeding in Europe to success what was not prosperous the last 40 years. The investigations of our small cell hives on VSH (Varroa Sensitive Hygiene) shows us the mechanisms how the bees actively defend against the Varroa. Now it is up to us in the working group to put it into reality, because we are now in the fortunate position to have all the necessary basics. We will establish a working group of resistance breeding in our group, and all who wish to cooperate are cordially invited to attend.\nThe other day I received Imkerfreund 12-2013 in my postbox. In this issue of the German bee magazine I found an interview with the scientist leading a varroa breeding project involving beekeepers. It’s a project that has made some progress.\nA German strategy\nGermany has had varroa for about 40 years. And they mostly try to breed pure Carniolan bees (carnica) and pure Buckfast with the help of isolated mating stations (and insemination). The main varroa treatment concept is the following in the south of Germany.\n- April-June Cut dronebrood and make splits\n- July After harvest in July treatment with Formic acid\n- August Four weeks later the second treatment with Formic\n- September The third treatment with Formic if mite downfall after treatment no 2 is too high\n- December Treatment with Oxalic acid, trickling of sugarsolution with oxalic\nTheir breeding program has two resistance parameters: pinkilling a square decimeter of brood (similar to freeze killed brood) and measuring the development of the varroa population during the season. Besides traits like low swarming, good temper and a honey crop. Their progress: being able to skip one of the summer treatments with Formic acid (as far as I understood the German article).\nAn American strategy\nUSA has had varroa for about 25 years. Few try to breed pure race bees like the Germans. Ineminated and naturally mated queens are used. The main varroa treatment concept for smaller operations are aimed at organic treatment (involving formic acid, thymol, drone cutting, hops) and treatment free (involving small cell size, topbar hives). Bigger operations most often trust different syntetic miticides, formic, thymol and hops.) Americans are more focused on treshold counts of mites for the time to treat.\nThe Russian bees from the very east of Russia, the Primorski area, is one quite succesful undertaking started and developed by the USDA. A number of beekeepers have formed an association to take care of the 18 lines of these bees. They are treated much less than ordinary bees against varroa. With these bees you have to watch out for swarming.\nThe VSH-bees are influencing most of the other efforts of breeding resistant bees to varroa mites. These bees are a mixture of all kind of bees. Here you find most of the traditional beekeepers that don’t use any treatment against varroa mites, for example John Harbo, http://www.harbobeeco.com/breeder-queens/ , and Adam Finkelstein, http://www.vpqueenbees.com/breeding.html\nBeekeepers in different states are forming breeding organisations funded by the governement, like this one: http://mysare.sare.org/mySARE/ProjectReport.aspx?do=viewRept&pn=FNE12-737&y=2012&t=1\nThere are those in Europe forming good projects and those already not treating against varroa either, but with this article I want to get attention to how little progress is made with the strategy mentioned above in Germany and how great achievement is made with the VSH-breeding, focused at the start on one trait: VSH. Then on the other. Something has to be learnt here.\nActually resistant bees were achieved already after very few years, by Harbo and Harris in 1995-98 in USA. They started with survivor colonies of any type of bee they were given. The only one parameter they focused on first was VSH: Varroa Sensitive Hygiene. See other articles here in the VSH category. This development of resistant bees have to be done in all kind of regions differing in climatic and other local characteristics, mites and pathogens for example.\nIt’s time for Europe to learn the lesson and form breeding projects together with beekeepers where ’central’ areas are formed fairly isolated from other bees. All bee colonies in these ’small starter’ areas are part of the program. An effective way of using funding money from European Union would be to support such projects.\na response from our colleague Rüdiger:\nAs a German I have of course to answer to Eriks contribution “Breeding for Varroa resistance: Germany versus USA”…:-).\nWhen commenting about activities in the varroa resistance breeding area I guess it’s better to compare Europe versus US. Otherwise it would be too bad for Germany…\nI think the main drawback for Europe compared to US is that a funded continuous breeding program is missing. The US seems to have at least 3 – Minnesota Hygienic Stock (MNHYG), Russian Honey Bee program (RHB) and VSH program, which all seem to have shown valuable outcomes. Moreover, the organic beekeeping community in the US, e.g. Ed and Dee Lusby, Michael Bush, Dennis Murrel and others have been innovative and could establish treatment free beekeeping since many years. And this could be achieved with local bee races or no complicated bee breeding scheme!!! Their impact with small cells, comb distance, not contaminated bee wax etc. is not only logic and inspiring, it works as stated above.\nEurope did of course some funded scientific investigation of Varroa and could contribute to the understanding of infestation mechanism in the 90-ies, e.g. grooming behaviour (Bienefeld, Aumeier, Thakur etc.) or VSH (Rosenkranz, Vandame). However, efforts seem to be sporadic and as already mentioned not continuous, to yield in resistant queens that are distributed via the beekeeping community.\nBesides, beekeeping organizations here I can only comment on Germany with the AgT (Arbeitsgemeinsschaft für Toleranzforschung) http://www.toleranzzucht.de/en/breeding-programme/, try to connect and coordinate different breeders in order to achieve bees that combine favourable and varroa resistant traits. However, improvement ratios seem to be small up to now.\nBut in my eyes Europe could contribute significantly by activities of bee breeders. The idea to use already varroa resistant bees for breeding was first established by Erik Österlund (1989) and John Keyfuss (1993), who cross African bees into A.mellifera mellifera/Buckfast. John uses a Tunisian bee (Apis mellifera intermissa) and Erik Apis mellifera monticola from Kenya. The resulting Elgon bee is since a bee that needs less or even no varroa treatment. The same is true for Keyfuss bees and he gain merits by bringing this topic into broad public interest with his “World varroa challenge”.\nThis approach was copied by Rinderer (RHB), who used Russian bees that lived since 200-250 years with varroa mites and hence, should have developed resistance traits. The same idea was practically followed by P. Jungels (Buckfast – Primorski mixes) and J. Koller (pure Primorski) (Primorski synonyme for russian bees) in Europe, who contributed significantly by providing varroatolerant queens to the European bee keepers.\nA guy that use local (carnelian) bees for his breeding program is Alois Wallner from Austria http://www.voralpenhonig.at/, who bred since 1990 for bees that groom and kill varroa mites by removing their legs. The result is now a bee that kill nearly every mite (varroa killer factor 100). Additionally his bees express VSH behaviour and hence, bees need only few or no treatment with formic acid.\nIn my opinion one brave European guy need to be mentioned as well which is Juhani Lunden in Finland http://www.saunalahti.fi/lunden/varroakertomus.htm He managed in a brute force approach to breed varroaresistant bees, which are not treated since 2009. He used a strong selective pressure to achieve his goal and hence, other traits as gentleness or honey crop might be compromised.\nSo taken together, these efforts need to put on a strong base in Europe as well and both, the spread of “resistant genes” by suitable queens and by suitable programs need to be pushed and furthermore the usage of organic beekeeping principles that result in treatment free bees should be distributed. That includes the courage of not using treatments to outselect non optimal strains. Here the community in Europe is already on the way see http://resistantbees.com/ (Germany and Spain), but Europe should definitely speed up and learn the positives from the US. This is especially true for the scientific sector and funded EC programs."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:8b291e49-d938-4258-9cd7-fed7216e26aa>","<urn:uuid:004e58ad-14c5-41f4-9670-3c967b3716b5>"],"error":null}
{"question":"¿Cuál es la diferencia principal between dog flu vaccine and cat rabies vaccine in terms of their effectiveness y prevención?","answer":"The dog flu vaccine does not guarantee complete prevention of the virus but helps in lessening symptoms. In contrast, the rabies vaccine for cats is considered a core vaccine that can prevent the deadly infection of the brain and nervous system. The rabies vaccine is typically given as a single dose between 12-16 weeks of age in cats, while the dog flu vaccine is considered a non-core vaccine that may be recommended based on lifestyle factors and exposure risk.","context":["Vaccines have been the talk of the town (and country…and the world!) for some time now. This vaccine discussion does not only apply to humans but to canines, as well.\nDog influenza, also known as the dog flu, is a sickness that all pet owners would like their beloved pups to avoid. But you may be wondering, “does my dog actually need to be vaccinated or is it a waste of time?”\nThis article is going to address the side effects of dog flu vaccine, including their symptoms, and what the latest research reveals. Before diving into these points, here is a little history of the dog flu itself. After all, knowing the roots of a disease can help give insight into how to potentially prevent it.\nWhere Did Dog Influenza Originate? Why Does That Matter?\nThe origin of dog influenza is cited as being initiated from a virus, known as H3N8, that had begun in horses and, somewhere along the way, transitioned into dogs. Also, a different type of virus is thought to have sparked another round of flu – the H3N2 from a bird virus that then transferred to dogs as well.\nDog flu, whether from birds or horses, developed and was transferred in very modern history. Because the viruses are so new, it means that dogs have not had time to develop an immunity towards them. That’s why, if exposed, 80% of dogs that come into contact with a dog flu virus will contract the virus and show symptoms.\nHow is Dog Flu Spread Now?\nSince the virus has transferred over to dogs, the flu spreads much more rapidly. The virus is mostly contracted through airborne particles (shared through canine coughing, barking or other activities that would transfer saliva particles), touch (with other dog bowls, dog park equipment) or transference from a human petting one dog and then their own dog. Even a friendly lick could spread the virus.\nThe dog flu, due to the method of contraction, is most prevalent in close communities. For example, if you live in a busy city and go to a bustling dog park, put your dog in a doggy daycare with other canines or go on long walks around a lake with lots of other pups, that may put your dog more at risk. However, if you live in a remote area where they wander the countryside or have a lot of space that is more isolated, they may be less in need of the vaccine.\nHow and When are Dog Flu Vaccines Recommended?\nSome vaccines are considered the most vital, core vaccines while others are just recommended, but not necessarily in the “most, must-have recommended”. According to the American Animal Hospital Association’s Canine Task Force, the core vaccines are considered to be:\nJust behind these almost-always-recommended vaccines are the “non-core” vaccines, which are:\nThe Dog Flu Vaccine (or Canine Influenza) is indeed in the top 8 vaccines (lower half “non-core”) recommended by a trusted task force that has the purpose of sharing vaccination recommendations in both the United States and in Canada.\nWhen Deciding Whether to Vaccinate, Consider This\nWhen deciding on whether to get the vaccine or not, there are different perspectives to consider. After all, there are many viewpoints on this that should be taken into consideration.\nOne factor to consider, if wary, is if there have been reports of outbreaks in your area. If you’re on the fence about vaccinating and hear there have been cases near you, this could potentially help add a layer of protection.\nPerhaps the main concern is potential side effects your dog could experience. Let’s address that now.\nPossible Dog Flu Vaccine Side Effects\nThere can be side effects to getting vaccinated, however, most side effects are found to be a rare occurrence, according to established veterinary clinics like Harlingen Veterinary Clinic. Some of the potential side effects you may witness would include:\n- Loss of appetite\n- Digestion issues\n- Experiencing discomfort or a swelling of the area around where the injection took place\n- Skin rash\nIt’s important to note that these side effects are more likely to have a greater effect on smaller canines.\nIf the side effects of the vaccine have not worn away within three days, then it would be a good idea to consult with your trusted veterinary professional.\nDoes the Dog Flu Vaccine Halt the Virus?\nUnfortunately, the vaccine doesn’t guarantee the virus will be completely prevented, however, it is most likely to help in lessening symptoms.\nYour Next Step\nEvery situation and dog is unique, so you’ll want to make sure you get a unique diagnosis. Consult with your vet professional to discuss the options and find the right one for you and your pup.\n“Canine Flu: What Should You Be Afraid Of?” Dogs Naturally Magazine, 19 Oct. 2018, Accessed 9 Dec 2018. www.dogsnaturallymagazine.com/canine-flu-what-should-you-be-afraid-of/.\n“Canine Influenza and Dog Flu Shots – Get the Facts!” Porte Veterinary Hospital, 19 Dec. 2017, Accessed 9 Dec 2018. www.porteveterinary.com/2017/12/19/canine-influenza-dog-flu-shots-get-facts/.\nStorie, Will. “Dog Flu Vaccine Becomes Increasingly Available, But Does Your Dog Need It?” BarkPost, 15 Jan. 2016, Accessed 9 Dec 2018. www.barkpost.com/discover/dog-flu-vaccine/.\n“Dog Vaccinations.” Harlingen Veterinary Clinic, 16 May 2017, Accessed 9 Dec 2018. www.harlingenveterinaryclinic.com/services/dogs/dog-vaccinations.\n“AAHA Canine Vaccination Task Force.” AAHA, Accessed 9 Dec. 2018. www.aaha.org/guidelines/canine_vaccination_guidelines.aspx.","Vaccinations and Your CatPosted 5/3/2015 Kittens are vulnerable to any number of illnesses and disorders. Their vulnerability can vary depending on whether they’re raised with multiple pets in the house or whether they’re allowed to romp outdoors.\nThe best defense, as they say, is a good offense. Vaccinations can prevent some of the most serious and common illnesses that can afflict your cat. Some of the preventable diseases are highly contagious. Some are even deadly.\nIn some cases, vaccinations may not only be recommended, they may be required by law. Your veterinarian will know the local requirements, but in general, you can expect your veterinarian to recommend the following regimen of vaccinations.\nFeline Distemper. Is a condition caused by the Panleukopenia Virus. The Feline Panleukopenia Virus causes symptoms of fever, vomiting, diarrhea, and seizures. Because Panluekopenia Virus is usually fatal in young unvaccinated kittens, this vaccine is one among four core vaccines recommended by veterinarians.\nFeline Herpesvirus-1 and Feline Calicivirus. These vaccines can be delivered with the same dose as the distemper vaccine. Also referred to as core vaccines, these medications are designed to ward off two upper-respiratory infections in cats.\nRabies. This disorder is an infection of your cat’s brain and nervous system. Disease-carrying mammals such as bats or skunks can infect your cat through a bite. All mammals, including humans, can be infected with this disease. Rabies is considered a core vaccine.\nFeline leukemia. Though not among the core vaccines, this is frequently recommended, particularly for cats that spend a lot of time outside, as well as for kittens. The virus can impair your cat’s immune system and cause certain types of cancer. It can certainly be fatal.\nThose are among the more common vaccines recommended, but they are by no means the only ones that are available. Most are delivered through an injection, but some can be administered nasally. Your veterinarian can recommend which vaccines your cat should receive, or may be required to receive, depending on the laws in your region.\nWhen should vaccines be given?\nThe schedule for delivering these potentially life-saving medicines may vary with the particular vaccine. In general, kittens start vaccinations when they’re 6 to 8 weeks old, with different vaccines (or doses of the same vaccine) administered at intervals of three or four weeks until they are about 16 weeks old.\nRabies vaccines are typically a single dose given between 12 and 16 weeks. Revaccinations typically begin after your cat’s first year.\nUnvaccinated adult cats, or kittens older than 16 weeks, typically receive their shots in two doses three or four weeks apart.\nBe sure to be proactive about protecting your cat’s health by staying on top of its vaccination schedule, with the help of your veterinarian. That includes scheduling well-pet exams.\nTips to Take Away\nBe prepared to start your kitten’s vaccination regimen as early as 6 weeks of age.\nVaccinating your cat can prevent many serious and potentially fatal diseases.\nWhile some vaccines are recommended, the law in your area may require others.\nLearn more about the role Royal Canin’s Feline Health Nutrition can play in helping your kitten maintain a healthy lifestyle post-vaccination.\nVaccines & Vaccination Schedule for Cats & Kittens—Drs. Foster and Smith Pet Education\nVaccination Guidelines—American Associatin of Feline Practitioners\nWhat vaccines does my kitten or cat need and how often?—Banfield Pet Hospital\nHead Cold in Cats—PetMD\nFeline Calicivirus Infection in Cats—PetMD\nCoronavirus in Cats—PetMD\nDistemper in Cats—PetMD\nRabies in Cats—PetMD"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:081e4ba6-cc30-4ced-8f12-e8c890e18bc5>","<urn:uuid:bbfd22db-440c-4c7b-ae47-525633d1dd51>"],"error":null}
{"question":"How do you properly use a pipe wrench for plumbing work? Could you outline the basic steps?","answer":"To use a pipe wrench, follow these steps: 1) Place the pipe between the hook jaw and heel jaw of the wrench 2) Adjust the nut to get the best possible grip 3) Move the handle to tighten or loosen. The pipe wrench works by compressing the pipe to prevent slipping and offers better grip and fastening compared to other wrenches.","context":["A pipe wrench or spud wrench is a common tool that can be seen in any handyman or DIY enthusiast’s arsenal. But how do they differ?\nIn short, both have similar applications and almost the same level of output. But pipe wrenches are a bit more convenient than spud wrenches because they have stronger grips and more torque!\nLet’s get to the details!\nSpud wrenches have one adjustable head on one side and a tapered spike on another. The head is mainly used to tighten or loosen nuts and spike lines up the bolts.\nWhere to use a spud wrench?\nInitially they were used to twist and tighten small nuts of toilet pipes and those nuts were called spuds. Since 2010, the modern toilets stopped requiring these nuts.\nHowever, spud wrenches have made their own space in other sectors. For example, construction workers now use them to connect the beams together.\nHow? Steel beams have holes at the end to connect with another beam. The workers put the spiked end in two holes of two different beams to align and then they attach them together with a bolt.\nPipefitter and steamfitter also use this type of wrench in similar ways.\nHow a spud wrench works\nWatch the video to understand how to work with a spud wrench.\nTypes of spud wrenches\n1. Construction Wrenches\na) Adjustable or Crescent Spud Wrench\nOne side of their head is fixed and another side is fixable according to the nut’s size (8 to 11 inches)\nHowever, twisting bolts with them is difficult compared to the fixed-head wrenches.\nb) Plumber’s Spud Wrench\nYou need them for toilet spuds and sink pipe bolts because these bolts fit perfectly with this type.\nAlso, they are less adaptable among the wrenches.\ni) Internal spud wrench\nThey twist the bolt from inside and are adjustable according to the size of the bolt.\nii) External spud wrench\nThey are useful for tightening and holding big nuts, grasp pipes and bolts. Just like the internal ones, they are expandable too.\niii) Adjustable Plumbing Spud Wrench\nThis versatile tool can work with a huge range of pipes and bolts because of being easily adjustable.\nBut the only problem is their grip is not as strong as their fixed-headed counterparts.\nc) Offset Spud Wrench\nLook at them and you will find their thinner spiked tail. This means they are able to line up smaller bolts than others.\nSimilarly, their grip on bolts are also stronger. In fact, their curved heads help speed up the fastening or loosening process of the bolt manifold.\nHowever, they won’t work as flat as pancakes on square shaped bolts like they will do on round headed bolts.\n2. Radiator Spud Wrench\nThe length of this wrench is 1.5 to 2 inches and it performs with radiator valves only.\n3. Ratchet Spud Wrench\nDouble socket ratchet spud wrench\nThey have two sides open to adjust the bolts and the grip is strong. With 13 inches length and 2.20 to 2.4 pounds of weight, they work real fast.\nSingle head ratchet spud wrench\nAs the name suggests, they have one side open to adjust bolts and small bolts can be adjusted with this type only.\nHow to Use a Spud Wrench\nStep 1: Place the wrench around the bolt\nStep 2: If the grip is loose, place a coin in between the wrench and the bolt.\nStep 3: Now move the wrench accordingly (to tighten or to loosen)\nPipe Wrench or Monkey Wrench or Stillson Wrench\nLike a ratchet wrench, they only grip when you turn the wrench in one direction. The hook jaw and the heel jaw create the grip on the pipe.\nWhere to use them?\nThey are used for pipes e.g. black iron or other metal pipes or galvanized steel.\nThere are different types of these wrenches for six different types of pipes.\nFor instance, the end pipe wrench is for end pipes while the offset wrench is perfect for difficult angles and narrow spaces. On the other hand, the compound leverage wrench takes care of the seized joints.\nTypes of pipe wrenches\nThey have unique heads (made of rubber/ leather/ chain or other strong fabrics) but the usual handlebars though operating them is easy.\nJust wrap the pipe with the belt of the wrench, adjust them and then turn the handlebar.\nWatch this video if you need more instructions:\nThey have an offset of 90-degree angle. The jaws are different though they look like an end pipe wrench.\nJust to inform you, the bigger size can be upto 48 inches.\nEnd pipe wrench\nTheir angular jaw is specially designed to have a grip on the tough end pipes in narrow spaces.\nStraight pipe wrench\n8 to 24 inches long, this is the most-favorites of plumbers. The design is quite simple with a parallel head and handle to ensure maximum grip on the pipes.\nCompound leverage wrench\nCompound leverage wrenches were invented to provide the maximum torque and leverage needed to work with seized joints.\nYou probably know that the seized joints are often damaged by cold weather or age.\nThis powerful wrench can create 14 times more pressure on the pipe than a standard pipe wrench and is specially designed for quick recovery.\nHow to Use a Pipe Wrench?\nThere are five types of pipe wrenches that are slightly different from each other with different angles of hook and heel jaw.\nAmong them, the strap wrench has a completely different-looking head and we’ve mentioned earlier how to operate them\nStep 1: The thumb rule is, place the pipe in between hook and heel jaw\nStep 2: Adjust the nut to get the best possible grip\nStep 3: Move the handle to tighten or loosen\nPros and Cons\nPipe Wrench Pros\n- Compress the pipe to block slipping\n- Good to go with plumbing works\n- Better grip than spud wrench\n- Similarly fastens better too\nPipe Wrench Cons\n- Can’t line up bolts\nSpud wrench Pros\n- Good for tightening the bolts\n- Preferable for auto mechanic work\n- Helps in lining up the bolt with their spike ends\nSpud wrench Cons\n- Weaker grip over the nuts\nWhich One Should You Take for Your Next Project?\nBoth of them have similar applications and they work just fine under similar situations.\nFor example, their adjustable heads work effectively on a socket, bolts, and pipe fitting. In fact, pipe and spud wrenches have similar size charts for sockets.\nHowever, pipe wrenches will be a bit more useful than spud wrenches with their stronger grip and more torque.\nLeave a Comment"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:4a013183-4a2e-4250-b02c-aef07ea46996>"],"error":null}
{"question":"I need a quick list of automation testing tools that can handle complex system testing like SoCs use.","answer":"Several automation testing tools can handle complex system testing. From the hardware perspective, SoC integration verification tools focus on connectivity, IP integration, and system management. For software testing, tools like UFT (Unified Functional Testing) offers comprehensive testing solutions with features like cross-platform compatibility and distributed testing. TestComplete provides automated testing for desktop, web, and mobile apps with capabilities for GUI testing, scripted testing, and test visualization. The IBM Rational Functional Tester specifically focuses on regression testing and automated functional testing, supporting data-driven testing and GUI testing. These tools, while different from SoC verification tools, provide various automated testing capabilities for complex systems.","context":["As system-on-chip (SoC) designs have become ever more complex, verification engineers have risen in importance to become an integral part of a product’s success. With more SoCs being designed, there is a growing class of verification engineers who are woefully under-appreciated in terms of the complexity of the job they have to do and the lack of tools made available to them. These are the engineers given the responsibility for integration verification as well as system verification and validation.\nThis article defines the unique problems that SoC verification engineers face in their jobs and outlines an approach that provides a level of automation for them similar to that enjoyed by block-level verification teams. It also discusses longer-term implications of this approach within the overall SoC development flow and demonstrates that a higher level of abstraction is necessary for efficient and effective verification.The unique SoC verification problem\nHigh-end chips may contain 200 or more discrete IP blocks. Many of these blocks come about as part of the system design process, where functional needs are identified and those that do not offer product differentiation are fulfilled by an existing IP block wherever possible. This leaves the in-house design engineers to concentrate on the most differentiated functions: the high-value parts of the SoC.\nWhat about the integration of these blocks? IP blocks can come from many sources, including internal design teams, development partners, EDA vendors and third-party IP providers. All these blocks need to be integrated with new blocks being designed for use within the particular SoC.\nSome blocks may have been designed for reuse, which means that they likely will have several modes of operation or ways in which they can be used or configured, with only a few of these options actually used. Some are parameterizable and some may have been modified to fit the particular set of design requirements unique to the current design. Blocks may be at different levels of stability and quality. They may or may not come with testbenches designed to demonstrate that they function correctly in a standalone environment. Most testbenches are of little help when it comes to stitching blocks together and verifying that they can collectively perform a useful function.\nThis is the nightmare that the SoC integration verification team must face.\nOne of the reasons why SoC design is based on the divide-and-conquer approach is that it is too difficult for any one person to fully understand the total functionality. The same is true for verification because the task should be divided such that each aspect of verification can be performed by different people, possibly with different skill sets.\nRedundancy between these verification tasks must be minimized. For example, there is no point in having integration engineers repeat verification that has already been performed at the block level. Each block has been verified in a standalone manner and thus, when integration verification is performed, the team should not attempt to do this task again.\nInstead, the verification should focus on ascertaining that the blocks have been connected correctly and that collectively they can perform the necessary system-level functions. In addition, there are certain types of functionality that only become apparent and verifiable at the full-SoC level –– the ability to support concurrency, power and clock management, for example, and performance issues such as throughput and latency.\nSoC integration engineers must perform three levels of verification, in addition to the block-level verification generally performed by different groups. Naturally this testing must be done at the “bare metal” hardware level, with no production operating software or drivers. However this article uses the metaphors of “drivers,\" “applications” and “performance” borrowed from operating systems.\nThe first level is the driver level where the integration verification team concentrates on the ability of blocks to effectively communicate with each other. This communication is most commonly between a processor and a peripheral, but could also involve DMA engines, memory subsystems or other infrastructure blocks. This level establishes that the necessary communication paths are functional.\nThe second level, which builds on the first, is the application level. Here the focus is on verifying complete paths that represent application use cases for the SoC. Verification engineers ascertain whether the SoC can perform the necessary functions, whether multiple paths can be exercised concurrently, and whether there performance bottlenecks.\nThe third level is the functionality that only exists at the system level, such as power and clock management. While pieces of the functionality are distributed among the blocks, the main controllers reside at the full-SoC level.\nCollectively, these three verification levels make up the integration engineer’s responsibility, and information necessary to perform each task is somewhat different. The verification approaches are also different from those of block-level verification.Defining SoC verification tasks\nWhile there is variability in the way that companies approach SoC integration verification, they probably include many of the tasks in the following list, each designed to answer a particular set of questions. These tasks represent a refinement of the previous three levels of integration verification:\n- Connectivity. Can each processor access all of the memories and other peripherals within the system? Does everything get reset to the right state? Are the fields within register words mapped correctly? Is the memory map correct? This is part of driver-level verification.\n- IP integration. Can a processor read and write to all of the registers of an IP block? Can DMA move data in and out of each block and are the correct interrupts generated on completion? If there are any GPIO pins on the chip, are they multiplexed correctly? This is also part of driver-level verification.\n- Use cases. Can data move through the system correctly to perform defined tasks? This may start with single tasks and extend to multiple concurrent tasks. This is part of application-level verification.\n- Performance testing. Under worst-case conditions, does the system perform with no loss of fidelity, dropped data, or other conditions that may cause unexpected degradation of system functions? This is another part of application-level verification.\n- Stress testing. When multiple random tasks are initiated, can the system handle concurrency, maintain data coherence, deal with multiple interrupts, and retain data integrity? This is the closest verification category to techniques used at the block level. This type of verification may also be used for power and thermal analysis of the chip. This is a further part of application-level verification.\n- System management. Do system-level features such as clock and power management function as intended? Do changes of configuration at this level affect the results of any of the other verification tasks? Typical scenarios trigger transitions in the system power states and execute IP operations to verify that blocks are behaving correctly under these conditions. This is system-level verification.\nWhile this list represents some of the tasks expected during integration verification, most verification teams do not move beyond connectivity and basic integration verification. As shown in Figure 1, such teams address only the tip of the iceberg, leaving the more advanced SoC verification tasks underwater and unexecuted.\nFigure 1: Many SoC verification tasks remain underwater.Lack of specialized tools\nSoC integration verification is usually performed with the same techniques used for block-level verification. Historically, verification was performed using a suite of directed tests, each specifically hand-written to verify a particular feature or aspect of the block.\nOver the past decade, many companies have adopted constrained-random verification using languages such as SystemVerilog or e. This allows multiple tests to be created from a set of verification models and enables aspects of the block to be tested that may not have been explicitly considered. Only when constrained-random stimulus fails to verify specific or required cases are directed tests written.\nHowever, as size and complexity of designs grow and as engineers attempt to use constrained-random techniques for SoC integration verification, they are finding it to be less successful. Simply put, constrained-random is not effective at exercising a specific function.\nIn addition, constrained-random stimulus is a “shotgun” approach where it is difficult to get focus on any particular aspect of a design. Verifying the complete functional space requires redundancy, which translates to an inefficient process.\nThe inefficiency and ineffectiveness of the constrained-random approach is contributing to the SoC verification process growing out of control.\nOne problem is that sequential constraints or guidelines are difficult to describe in the existing testbench languages. The Universal Verification Methodology (UVM) standard provides guidelines for describing sequences and layering these into higher-level sequences. However, it is difficult to control dependencies between sequences, limiting the effectiveness of SoC-level testbenches. In addition, the UVM does not address embedded processors within the SoC; testbench languages were designed for transaction-based verification of hardware, and not the control of multiple pieces of concurrent software.\nFurther, it is difficult to fully exercise a large design by manipulating only its inputs, whether via hand-written directed tests or constrained-random stimulus. Software running on the embedded processors plays a critical part in the functionality of the SoC. This argues for a new method that encompasses both the code running on the SoC’s embedded processors and the testbench connecting to its inputs and outputs. This method must be able to handle multi-threaded processes running on multiple processors within the SoC and coordinate multiple concurrent activities within the chip and with the testbench.\nIdeally, the new approach should be able to span all the SoC integration verification tasks shown in Figure 1, from the tip of the iceberg to its deepest point.","Automation testing is a testing procedure that can dramatically reduce the time needed to implement the complete life cycle testing of a piece of software.\nThere are plenty of testing tools available and we have compiled 15 of the best options for functional testing to help you to choose. The list below has both open source and licensed options for automation testing. Most licensed applications offer free trial versions so you can try before you buy.\nFree & Open Source Automated Testing Tools List\nSelenium is widely regarded as the top option among all the automation tools for testing. It works over multiple operating systems and several browsers and complements many programming languages and frameworks for automation testing. Selenium helps users write browser-centric test scripts, which can be used in various environments. When using scripts with Selenium, they can help with easy bug finding, exploratory testing, and regression testing.\nThe Cucumber testing tool is an open-source application designed for behavior driven development. It automates acceptance testing by running examples that exhibit the way the software behaves. It functions as an up-to-date document with specifications and tests. The tool features cross-platform OS support as well as additional languages like Java and .NET, even though it is scripted in Ruby.\nThe Watir testing tool is scripted by Ruby for automating web applications. The name is pronounced as ‘water’, and it can test any language-based online application. It features cross-browser testing and works with business-centric development tools like Cucumber, Test/Unit, and RSpec. The application can test buttons in web pages, links, forms, and more.\nCommercial Automated Testing Tools List\nThe Unified Functional Testing or UFT tool is one of the best options for functional testing. Known formerly as QuickTest Professional or QTP, it uniquely brings the developers and testers under one roof, giving quality testing solutions that are also cost-effective and user-friendly. UFT is best known for cross-browser and multi-platform compatibility, multiple testing features, optimized and distributed testing and image recognition. It is a licensed tool that comes with a two-month trial.\nRanorex is a complete GUI testing application. It is pretty flexible and works on almost all devices. Apart from GUI features, it also recognizes super smart objects and any changes that occur in UI. Ranorex has extra features such as reusable code modules, quick bug detection, smooth integration with more tools, and easy test recording and editing.\nTestComplete is another application that helps automate testing in desktop, web and smartphone apps. It has features like GUI testing, scripted testing, test visualization, test recording and playback. It supports programming languages like Python, VBScript, Java, C++, C#, and Delphi.\nThe Katalon Studio application is an open-source tool for automation that supports both web and mobile. It also works with top tools like Appium and Selenium. Therefore, it can improve the testing framework through the object spy feature, object repository, user-friendly IDE and browser plugin. The scripting language for this tool is Groovy, and it will support an external Java library. Interestingly, Selenium scripts written in Java can be used with this application. The Katalon Studio tool integrates well with continuous integration systems like TeamCity, Jenkins, and Bamboo.\nIBM Rational Functional Tester\nThe IBM Rational Functional tester focuses mainly on regression testing and automated functional testing. It lets you perform data-driven testing and GUI testing. Automated testing in this tool is based on ScriptAssure technology that aids testing efficiency and offers script maintenance. The tool lets you support many web and terminal emulator-centric apps.\nAppium testing is mainly concerned with mobile applications. It’s an open source tool that supports the automation of native apps, mobile and web applications, and hybrid applications for both Android and iOS. Appium works via automation provided by the vendor and is based on server architecture. The tool is easy to install and use, and it has recently gained a lot of support from the market.\nVisual Studio – Coded UI Test\nThe automated tests with Coded UI mainly drive testing through the user interface of the application under test. It includes functional testing aspects of UI controls and generates code in VB/C#. It is closely associated with Microsoft’s Application Lifecycle Management processes and other Microsoft tools and technologies.\nThe eggplant tool is licensed and is primarily used for GUI testing. It also supports other forms of testing such as performance, stress and load testing. It works on the image-based approach, not object-based, as most of the other tools. A single script enables testing on many platforms like Mac, Windows, Solaris, and Linux.\nSahi is a tester-centric tool that works on cross-platforms. It has advanced features such as Smart accessory identification, no ajax timeout issues, powerful scripting, is open to any recording and playback, end-to-end reporting, and an Excel framework. Use the trial version from the link above and then upgrade to Sahi Pro."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:1cf575ea-b1db-42d1-99a4-615286eda089>","<urn:uuid:455b4ccb-3ed5-4340-ba53-626e5d3d55e2>"],"error":null}
{"question":"What are differences between CMOS and CCD cameras in terms of image quality and power consumption?","answer":"CMOS and CCD cameras have distinct characteristics. In terms of image quality, CCD sensors offer better performance with a 100% fill factor, producing brighter and sharper images, especially in low-light conditions. They have lower noise factor and higher color fidelity. CMOS cameras, while offering good quality in bright conditions like sunny days, are less sensitive to light due to having electronics integrated into the sensor surface. Regarding power consumption, CMOS cameras are more efficient as they have fewer components and typically use 9V, while CCD cameras draw more power and typically use 12V.","context":["When you are setting up a machine vision system, your choice of camera will depend on the objects that you want to inspect, the necessary speed, lighting and temperature, and available space. And not to forget—the system costs.\nMachine vision vs. Surveillance\nFor most applications in factory automation or the medical field you will need a machine vision camera. A machine vision camera captures image data and sends it uncompressed to the PC. This is the reason why pictures look less “pretty” than the ones from cell phones. In consumer cameras the image data gets compressed and smoothed out which looks good, but doesn’t provide the quality needed for flaw detection and code reading.\nNetwork cameras or IP (Internet Protocol) cameras record video and compress it. Their advantage is their robustness and resistance to vibration and temperature spikes. They are also tolerant to poor lighting conditions and direct sunlight. IP cameras are mainly used in surveillance and in Intelligent Traffic Systems (ITS) applications, for example, for road tolling and red light detection.\nArea scan vs. Line scan\nIf you have a high-speed application with a conveyor belt, you will need a line scan camera. These cameras use a single line of pixels (sometimes two or three lines) to capture the image data. They can check the printing quality of newspapers at a speed up to 60 mph, quickly sort letters and packages in logistics, inspect food for damages. They also control the quality of plastic films, steel, textiles, wafers and electronics.\nIf you need in-depth inspection, area scan cameras are your choice. They have a rectangular sensor consisting of several lines of pixels and capture the whole image at the same time. Area scan cameras are used in quality assurance systems, code reading, and for pick and place processes in robotics. They also get integrated in microscopes, dental scanners, and other medical devices.\nMonochrome vs. Color\nMonochrome cameras are mostly a better choice if the application does not require a color analysis. Because they don’t need a color filter, they are more sensitive than color cameras and deliver more detailed images.\nMost of the color machine vision cameras use the Bayer matrix to capture color data. Each pixel has a color filter, half of them green and a quarter red and blue each. The debayering algorithm uses the information from adjoining pixels to determine the color of each pixel. So a 2×2 debayering reads the information from three adjoining pixels and 5×5 debayering reads the information from 24 adjoining pixels. So if you need a color camera, the bigger the debayering number, the better.\nCMOS vs. CCD\nIn CMOS cameras, the electronics that convert the light into electronic signals are integrated into the surface of the sensor. This makes the transfer of the data especially quick. CMOS sensors are less expensive, don’t have blooming or smear and have a higher dynamic range. That allows them for example to capture both a high-lit license plate and the shadowed person in the car in one and the same image.\nBecause the CCD sensors do not have conversion electronics on the sensor’s surface, they can capture more light and so have a lower noise factor, high fill factor, and higher color fidelity. These properties make CCD cameras a good choice for low-light and low-speed applications like astronomy.\nGlobal vs. Rolling Shutter\nIf you want to avoid distortions at high speeds and are not too concerned about price, then global shutter is the optimal choice. With the global shutter the entire sensor surface gets exposed to light at once. It’s great for high-speed applications, such as traffic and transportation, or logistics.\nThe rolling shutter reads the image line-by-line. The captured lines are then recomposed into one image. If the object is moving fast or the lighting conditions are bad, the image gets distorted. However, adjusting the exposure time and implementing flash, you can minimize the distortion. Rolling shutter is less expensive and is available only on CMOS based cameras.\nFrame rate is the number of images that the sensor can capture and transmit per second. The human brain detects approximately 14 to 16 images per second; the frame rate of a movie is usually 24 fps. For fast-moving applications like inspections of newspapers the camera needs to “shoot” in milliseconds. On the other end there are microscopic applications which require low frame rates comparable to the ones of the human eye.\nA simple formula is used to determine which resolution is required for your application: Resolution = (Object Size / Detail size)².\nLet’s say you want to determine the eye color of a person who is two meters tall.\n(2 m ÷ 1 mm)² = 4,000,000 pixels = 4 MP\nAn example of such a resolution would be 2048 px x 2048 px.\nIf your camera is far from the PC or if you need multiple cameras in one system, you’ll need GigE. It allows cable lengths up to 100 m and makes multi-camera integration easy. However, USB 3.0 gives you more than three times the bandwidth (350 MB/s versus 100 MB/s), is plug-and-play and has power and data via one cable. It can work with cable lengths only up to 8 m. If you need the highest possible bandwidth—up to 850 MB/s—then Camera Link is the best choice. Camera Link requires a frame grabber and so the system costs are higher.\nToday the most popular compact camera size is around 30 mm each side. The miniaturization continues and so there is a new class of bare board cameras without housing and only 6 mm thin. These cameras are good for size-restricted and cost-effective applications in the embedded field. These require a different infrastructure: computer on chip instead of PCs, ARM processor architecture instead of x86; Linux, Windows IoT or Android instead of Windows. Some setups can use the USB 3.0 interface, but MIPI and LVDS-based interfaces allow more flexibility and compactness.","Staff Patrol offers two types of cameras, CMOS and CCD. It is important to understand the difference because cost and qualify are a big consideration in choosing a camera system. Each of our camera systems indicate the type of system used.\nThis type of image sensor is relatively inexpensive and offers a good quality image. The cost of this system is usually less and the overall system can be very economical. While CMOS sensors excel in the capture of outdoor pictures on sunny days, they suffer in low light conditions. Their sensitivity to light is decreased because part of the sensor is covered with circuitry that filters out noise and performs other functions. The amount of space on a sensor devoted to collecting light is called the pixel’s fill factor. The design of CMOS cameras creates a much lower fill factor and therefore they are less sensitive. CMOS cameras do have the advantage when it come to power consumption. Because there are less components in the camera board, it draws less power and will run longer if used on a battery. For general purpose viewing in good light, they are a good choice. If there are budgetary constraints, it’s possible to get more features in the overall system using CMOS cameras.\nCCD cameras have a 100% fill factor and therefore offer a brighter and sharper image. Like most most electronics, this increased quality does add to the manufacturing costs. CCD cameras produce a higher quality picture mainly because the image sensor is used strictly for collecting the image and all of the other necessary processing is handled by other components on the camera board. CCD cameras on the other hand have more components and draw more power. This may be a consideration when using battery power.\nThese images are representative of the differences you can expect between the two styles of cameras. Individual lighting and other conditions will effect the actual image.\nCMOS sensors offer average image quality in good light, have lower power requirements, and are substancially less expensive. CCD sensors offer excellent images in most lighting conditions, have higher power requirements, and are typically more expensive. Evaluate your needs in a surveillance system and we can offer you the best in either category.\nGlossary of Video Terms\nAGC – Automatic Gain Control is an electronic system found in many types of devices. Its purpose is to control the gain of a system in order to maintain adequate performance over a range of input signal levels. Or an electronic circuit that tries to keep the video signal at a constant level 1 (Ivolt peak-peak). Useful on cameras working at low light levels .\nAI – Auto Iris is an electronic circuit that acts as an iris on CCD cameras by electronically shuttering the CCD sensor. Or an automatic method of varying the size of a lens opening in response to changes in scene illumination.\nAWB – Auto White Balance is an electronic process used in video cameras to retain true colors. It is performed electronically on the basis of a white object in the picture.\nCCD – Charged Coupled Device: It is analog technology. The CCD camera has a higher resolution than CMOS. The camera also functions better in low light. A CCD camera drains a little bit more power than the CMOS cameras. It typically uses 12v instead of 9v. CMOS cameras are smaller than a CCD camera and works longer with the 9v battery.\nCMOS – Complementary Metal Oxide Semiconductor: A lower resolution camera compared to a CCD model. The advantage of a CMOS camera is that it uses lower operation current.\nHAD CCD – Hole Accumulation Diode is a type of CCD sensor with a layer designed to accumulate holes (in the electronic sense), thus reducing noise level.\nS to N Ratio – Signal to Noise Ratio is simply the ratio of the signal power and noise power, expressed in decibels (dB). Or a measure of noise on a video signal. It is represented in Decibels as the level of the video signal compared to the level of noise present on that signal. The higher the signal to noise ratio the better.\nBLC – Balance Light Control is a method to compensate for bright spots in a picture. It is also important to consider whether there are bright spots in the picture such as car headlights which can make identification of the vehicle registration or model impossible. This can also be a major problem where it is necessary to identify a person who is moving from bright daylight into artificial light. This could result in the subject becoming an unidentifiable silhouette.\nOSD – On Screen Display is a method of displaying set-up information or instructions on to a display monitor.\nResolution – Resolution measures the cameras ability to reproduce an image. The higher the resolution, the better the picture quality.\nLUX – LUX is the measurement of low light needed for the camera to view and record properly.\nFPS – Frames Per Second is the number of still frames (pictures) that give the illusion of motion, which appear in a single second of time. 3O fps is considered “Real Time”. So for Real Time viewing of your cameras, you need at least 3O fps for each camera.\nIR – Infrared, IR LEDS are used on Day/Night cameras which allow the camera to see in the dark.\nFocal Length – the distance from the surface of a lens and its focal point.\nFrequently Asked Questions\nHow do hidden cameras work? A small board camera is built into an everyday item. The camera can be wired which means it is connected to the DVR or VCR using a cable. The camera can also be wireless, in this case the camera transmits a signal to a receiver that is connected to the DVR or VCR.\nCan I get audio in my hidden camera? – No you cannot. According to United States federal laws, audio should not be used in a surreptitious manner. One example of surreptitious interception is audio in a hidden camera. This includes pinhole board cameras and all covert or hidden cameras; i.e., a clock radio. Audio in a hidden camera or board camera is only available to law enforcement agencies. Title 18, Section 2512.\nWhat is the difference between a wired and a wireless camera? – Wired cameras have a video cable that runs from the camera to your recording or viewing device such as a DVR, VCR or monitor. Wireless cameras have a built-in transmitter that sends the video signal to a receiver. The receiver connects to your recording or viewing device.\nHow far can a wireless hidden camera transmit? – Standard wireless hidden cameras can transmit up to 1000 feet and high-powered wireless hidden cameras can transmit up to 2000 feet.\nHow many wireless cameras can you have in one location? – You can have up to four wireless cameras in one location. You can view all cameras at once using four receivers or you can use one receiver and switch to each camera. You will only be able to view one camera at a time if you only use one receiver. If you want to install multiple wireless cameras in one location it’s best to order them at the same time so that we can put them on different channels.\nWill a cordless phone interfere with wireless cameras? – Cordless phones that operate on the 2.4 GHz frequency will cause interference with 2.4 GHz cameras. Interference should be minimal and usually occurs if the phone is between the camera and receiver.\nWhat is a Quad? – A Quad splits your monitor into 4 sections allowing you to view 4 cameras at once.\nWhat is a 2.4 GHz wireless frequency? – A frequency is used to transmit a signal or data like video. 2.4GHz is the specific wireless frequency that our transmitters use to send video signal to a receiver. All of our wireless cameras operate on the 2.4 GHz frequency. Range varies from 200′ to 700′ depending on environmental conditions. These units are FCC approved.\nAre the transmitters FCC approved? – The transmitters used in all of our wireless systems are FCC and Industry Canada Certified.\nWhat is CCD? – CCD is used in professional cameras because of it’s high resolution quality and it’s ability to record in low-light situations. CCD is basically a small silicon chip that receives light and turns it into voltage variations which makes up an image, it’s usually measured in inches with 1/3″ CCD being the standard. They are higher priced but are great for cameras that may require vision in near darkness.\nWill a Quad allow four cameras to record at the same time? – Yes you can record all four cameras at the same time. You also can record using a switching monitor, which will record the camera that is showing at that moment.\nDo the receivers work through walls up to two feet thick? – Yes as long as there isn’t excessive amount of metal in the wall.\nDo the plug and play connections require extra wire to run the signal back to the VCR or TV? – Yes, you need to buy the length of cable you need we offer various lengths up to 100′.\nWhat is the difference between a DVR and a VCR? – A Digital Video Recorder (DVR) system records high-resolution digital images to a hard disk drive (HDD) and eliminates the requirement of maintaining VHS tapes. Since the video images are stored digitally, the image quality will not degrade overtime, as would a VHS tape when recorded over multiple times. The time-saving search capabilities of a DVR will enable the user to locate the desired video clips via user defined parameters (camera, time, date, etc.) versus the fast forward and rewind functions of a VCR. A DVR can be accessed remotely from anywhere in the world using the Internet.\nHow many hours will a DVR record? – The amount of time a DVR will record for is based on the size of the DVRs hard drive, the number of cameras recording and the number of frames per second ‘ it is recording at. One camera recording in Real Time uses 1 GB per day. So a 16 camera system will use 16 GB in one day.\nDo I need a VCR from you or can I use my own VCR? – You can use your own VCR for recording. However, the VCR will have to be recording all the time. Most VCRs can only record for about 10 hours but we have special VCRs that can record up to 1280 hours.\nWhat is a DVR card? – DVR Cards enable the user to convert their computer into a Digital Video Recorder. The DVR Card(s) is typically installed in an available PCI slot of a computer. DVR cards are bundled with video surveillance software, which allows the user to record and display multiple cameras simultaneously from the camera site or a remote location.\nWhat is a Plug and Play connection camera? – It is an RCA Video plug and a power plug on the camera for easy connection to VCR or TV. This is done by running the RCA (Aux) line into the VIDEO IN of the VCR or TV. The TV or VCR must be set on the correct channel to view the VIDEO IN picture.\nHow many cameras can I hook up to one TV? – You can hook up as many cameras as your TV has inputs. Most TVs have 2 inputs but when using a quad you can hook up four cameras.\nDo you need a VCR to record or will the cameras record? – You must have a VCR or DVR if you want to record.\nBetween what temperatures is it safe for cameras to operate in? – It is safe for B/W cameras to operate between -10F to +122F. It is safe for color cameras to operate between -10F to +104F. It is safe to store cameras between -22F to +158F.\nWhat is the operating voltage of a camera? – Our cameras range from 9 volts (CMOS) to 12volts, and also 24 volt professional models.\nWhat is a varifocal lens? – A varifocal lens is one where the focal length of the lens can be varied. This is a fancy way of saying it is a zoom lens. Most varifocal lenses have, and in almost all circumstances should have, an auto iris feature.\nWhat is a fixed lens? – A fixed focal length lens cannot zoom. The focus is fixed. A fixed focal length lens usually allows more light to pass through the lens at a given focal length than a varifocal, or zoom lens. This can be important in low light situations."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:0dbeb2ae-e0af-4396-a1aa-86b6942b2b76>","<urn:uuid:4692ba3c-3e59-4d3a-a353-3dfc96f504cb>"],"error":null}
{"question":"What are the physical requirements for kayaking expeditions, and what types of insurance coverage should be considered for outdoor adventure activities?","answer":"Kayaking expeditions require participants to be reasonably fit, though not necessarily extremely experienced. Advanced beginners are acceptable, and age isn't a limiting factor, with many participants in their seventies completing trips. Groups paddle together at the slowest paddler's pace, averaging 4km per hour in mild headwinds. Regarding insurance, adventure activities require specific coverage - regular travel insurance may not be sufficient. Activities like kayaking need to be explicitly covered in the policy. Insurance should include comprehensive coverage for medical evacuation, which can cost up to $100,000 without insurance. Travelers must verify their policy covers their specific activities and understand the extent of air ambulance coverage before departure.","context":["What can we expect on a typical day?\nOur expeditions are vigorous but not excessively rugged. However, they are not for totally inexperienced paddlers. We would like you to have done wet exits and if you haven't we can do that the first day. An advanced beginner would be fine. On our 9 night trips some days we will be covering rather long distances on the water so we expect all participants to be reasonably fit. Our 5 night trips are more relaxed.\nAge should not be a factor. Our groups often include paddlers in their seventies. We paddle together as a group and our speed is determined by the slowest paddler. Typically we average 4 km. per hour when headwinds are mild. This is flat water paddling with a very little current.\nWe get on the water early enough each day to avoid headwinds that usually pick up around noon. Early starts also get us to our daily destination early enough to stretch our legs and visit the area. The guides appreciate help getting boats and equipment on and off the water. Our kayaks are very stable, seaworthy expedition single and tandem models.\nWhere will we be staying and eating?\nWe stay in small family operated hotels and quintas where our neighbors are Portuguese. Breakfast will be in the hotels. Lunches will be prepared by our guides or eaten in local restaurants. We will enjoy regional Portuguese cuisine in restaurants for evening meals. All our meals will be from local grown products mostly bought at that days market. In several cases the meals will be prepared with products grown in fields right next to the restaurant. We will do our best to cater to dietary restrictions but we must be informed when you sign on for the expedition.\nClothing for our trip\nFoot wear: one pair of lightweight walking shoes and a pair of sandals and whatever you wear in the kayak which could be the same sandals.Flip flops are not acceptable as kayak footwear. There are more accidents getting into and out of kayaks than at any other kayaking moment. Flip flops tend to slip when wet and we can't risk slipping.\nKayaking wear; Some of these items you might not need but I prefer to err on the safe side. All this must be quick drying (i.e.) not cotton. Shorts or bathing suit suitable for wearing in any bar or restaurant, paddling jacket/rain and wind proof. Paddling pants if you tend to get cold easily. Long sleeved shirt, some sort of head protection from the sun, sunglasses with strap. A buff is something I find very practical for sun protection.\nOff water wear: Long pants, skirt, dress, t-shirts and other clothes you might like but do travel as light as possible.\nWhat can I read to prepare myself for this trip?\nCork Boat John Pollack 2004\nMr Pollack takes us on a dream fulfilling journey down the same route we\nfollow on the Douro River. The last part of the book is very relevant\nto our expedition. In any case it’s an enjoyable read. Amazon has this\nin both paperback and Kindle versions.\nHow do you suggest I travel to and from the pick up site?\nThe first Global Village Martin Page 2002\nMr Page has written what could be the most enjoyable book on the\nhistory of Portugal. The paperback edition is available for about $25 in\nPortugal. Amazon has a new paperback edition for $242.48 (outrageous\nprice). Amazon also has a kindle version for $12.94\nAll our expeditions will start and finish in Porto (OPO airport code) we suggest you fly into either Lisbon or Porto and leave from either Oporto or Lisbon. If you fly into Lisbon you can either fly or take the train to Porto. The train connections between Lisbon and Porto are very good.\nGeneral travel information links:\nPortugal travel information:\nA general search engine providing links\nOffical tourist site for all of Portugal\nOffical site for Porto tourist information\nThe Portuguese airline site\nSite for all of Portuguese commercial airports. Gives names of all airlines serving the airport.\nOn-line news service\nTemperature conversations from Celsius to Fahrenheit\nCheck the time difference in Portugal or Spain. Portugal is on GMT but Spain is one hour ahead.\nCurrency exchange rates\nWhat is included in the expedition package\nExperienced professional guides.\nIndividual Expedition kayaks, paddles, life jackets, wet skirts and paddling jackets.\nTandem kayaks are available on request.\nSupport vehicle for entire expedition. All our luggage is carried from accommodation to accommodation. You will not need to carry anything in the kayaks except a little money and your camera.\nA small dry bag for personal items needed while on the water.\nTransfers to the launch sites.\nAll meals from dinner on the first day to our farewell dinner on the last night. House wine or bottled water is included. You may select fine wines from the wine list.\nBed and breakfast or quinta accommodations in double bed rooms for the nights.\nBottled drinking mineral water.\nPersonal incidental purchases (souvenirs etc.)\nEntrance fees for museums\nTips other than meals and hotels\nTravel to the pick up sites (flights, trains etc.)\nTravel to train station on first day is not included but of course the train is included.\nWhat if I get sick or have an accident?\nKayaking has some inherent risks. Sickness and accidents can spoil an otherwise enjoyable time and really wreck a budget. Therefore, you must have a comprehensive travel insurance policy. You must be covered against loss of baggage, illness, accidents, trip cancellation and the very unlikely necessity, of emergency medical evacuation. Please revise the claim process and inform us of the contact information when you sign on this expedition.\nHow much money should I bring?\nRemember that you must pay the balance of your trip cost in cash. We are usually working in a cash economy where we must pay everything in cash or suffer much more elevated costs. This is the reason we need to be paid in cash. If you absolutely must pay by bank transfer we will provide our bank details and you will have to pay a 23% vat that will be added on your invoice.\nIf you are traveling from outside the Euro Zone it is usually best to buy Euros before you arrive in Europe. In our travel information links, you will find sites for ATM machines that accept VISA and Master Cards. Generally the ATM machines have a daily limit of 300 Euros. Most international airports have exchange banks. Traveler checks are not a good idea as they must be changed at banks and the process is very time consuming.\nAfter you pay the fees for the expedition in cash, that includes B&B and all regularly scheduled meals, you will only need money for personal purchases such as gifts and tips. Tips at restaurants are included. If you have enjoyed your tour the guides greatly appreciate your tips. When I'm asked about amount for tipping staff I say 10% of the tour price and this should be given to Rui and he will divide the total amount between the guides.\nWhat will the weather be like?\nDays in Portugal this time of year are usually sunny and comfortably warm and are similar to Atlanta, Georgia. 23 C (73 F) daytime, 14C (57F) nighttime. Evenings are cool but not cold. Rain is unpredictable but not common. A fleece sweater is suitable for cooler evenings. However, you should also be prepared for any cooler than usual weather. Having a windbreaker handy is always a good idea.\nThe electrical current in Portugal is 220 V 60 HZ. Most telephone, camera and computer chargers are prepared for 100-240V and 50-60HZ. If this is the case for your electronics then all you need to do is have a European round two prong plug adapter. If you plan to use your telephone you will want to ask your provider to set you up for roaming. Be sure to ask about the charges and be aware that voice connection is not the same as data connection. If you plan to use your smart phone for email you might find the charges are very high. A way to avoid this is to disable your phone date connection and only use Wi-Fi where it is available along our route. Most days we have Wi-Fi but not always.\nFRIEND US ON: facebook.com/DouroKayakExpeditions","Far too many people stuck overseas are reaching out to the world for money through crowd funding. Some even might say that crowd funding for rescue is being viewed as ‘insurance’ coverage in case anything goes wrong, especially when the emergency involves air ambulance transport across borders. While the world community must be appreciated for showing concern and extending a helping hand, international travellers too must take precautions to avoid giving rise to such situations in the first place. Here, we touch upon certain details pertaining to travel insurance and air ambulance coverage that people travelling abroad must keep in mind.\nAdventure, Insurance and Air Ambulance – the Three Main Factors\nMost travel-related injuries occur when adventure activities go wrong. However, adventure, most often, is the motivating factor for international travellers. Even though most of the travellers understand the importance of insurance and go for one, certain nuances are sometimes ignored. For instance, if you are headed for a skiing adventure in the Swiss Alps, you must make certain that the travel insurance covers that sport in particular. Also, one needs to know whether the coverage is for local care or air ambulance evacuation too, and if the evacuation is covered, then to what extent.\nTravellers Must Discuss Health Concerns with their Travel Agents\nWhile online research is an excellent place to start, it should not be considered a substitute to the insight that a travel agent can provide. Discuss different insurances, the activity you plan to indulge in, the coverage and air ambulance evacuation. Go for the right balance of travel cost and health care assurance. While measures like crowd funding may rescue you, the hassles of raising money and the worries that come along, can be quite harrowing. The effort taken to research and purchase travel insurance goes a long way to bring you a tension-free adventure.\nTravel insurance is the safety net that you need when travelling abroad, especially in countries where the quality of health care is not the same as the one you enjoy in your home country. In such situations, air ambulance can become an absolute necessity. Most people assume that travel insurances automatically ensure an air ambulance to their home countries if they felt there is a necessity and that it is the patients’ choice to get the treatment wherever they desire; however, these beliefs are far from true. The insurance company can actually deny a claim and for the patient to get treatment in the visiting country. The company would weigh the cost advantage in situations like these.\nChecks Air Ambulance Cover Before Purchasing the Travel Insurance\nExplore as many travel insurance options as you can before zeroing down on a specific one. Reveal upfront the kind of activities you plan to indulge in and check out the premiums. If you have any pre-existing illness, make that known clearly and check out the premiums. The fear of higher premiums must not deter you from making this revelation as you can get into a huge financial mess owing to this mistake later.\nKnow Your Coverage Well\nAlso, some activities are considered very high risk and certain travel insurance policies might not cover them. If you’re going to be a part of some extreme adventure event, check out the facilities that the organizers offer and also whether the insurance company is willing to cover the mishaps that occur during such activities.\nThere have been several instances in the recent past where the travelers have not bothered to look closely enough whether the insurance company covers air ambulance services and later have had to resort to crowd funding to afford international air ambulance services. A little homework can avoid such issues and make your travel more secure and safe.\nIf you are going on an extreme sports trip, the chances of needing medical care and air ambulance cover are naturally more. So, you need to question yourself, “Is mere travel insurance enough?” In fact, there are special travel insurance designed just for the purpose and you must look into them. However, before that, you must know certain facts.\nClassifying Extreme Sports before Looking at Air Ambulance Cover\nGoing for a swim in calm waters or ruffling some waves by the beach does not count as extreme sports. Extreme sports are those where there is a high likelihood of mishaps, even after safety precautions, and can warrant emergency medical treatments including air ambulance evacuations. Some of these are:\n- Deep sea scuba diving\n- River rafting\n- Rock climbing\n- High altitude mountaineering\n- Bungee jumping\n- High-speed or cross-country biking\nAir Ambulance Cover and Insurance are no Substitute for Reasonable Precautions\nMost extreme sporting companies will not even entertain your request unless you have adequate extreme sports insurance. They might also demand that you have enough extreme sporting experience and practice behind you. The reason is simple; the eventualities in such sports can even mean death or disability. Having adequate insurance and air ambulance cover increases your chances of survival drastically. You are assured of immediate medical care in any foreign countries. One must note in this context that most countries outside the US demand assurance of payments before the treatment is initiated.\nChoosing the Air Ambulance Cover and Insurance\nHere are a few points that you need to consider:\n- Your sports insurance might cover air ambulance evacuations. If that’s the case you don’t have to worry at all. However, it is vital to look whether there are limitations to this cover.\n- Compare the costs. Sometimes buying a separate air ambulance cover is more economically viable. So check whether the travel insurance company is charging you exorbitant rates.\n- Compare between various travel-insurance providers and compare the prices and the kind of facilities being provided. You would not only need air ambulance evacuation cover, but also coverage for medical consultation, hospitalization, and treatment.\n- If you are going as a couple or a group, look for discounts. Never hesitate speaking to the company’s representative.\nYou need to fill your forms for extreme sports travel insurance clearly and mention the kind of activities that you would be involved in to avoid confusion later.\nThe holiday season is here! This is the time of the year when most people head to exotic locations all over the world. Travel insurance is today part of every individual’s holiday planning. You choose to cover minor risks such as losing your belongings that essentially have no financial consequences. You even buy flight insurance to cover your death incase the airplane crashes within the short duration that you travel in it. But have you thought about international air ambulance evacuation coverage? Missing this vital coverage could result in bankruptcy or at least leave you with a huge financial burden.\nWhy Purchase International Air Ambulance Evacuation Cover?\nThe exotic places that you travel to might not have the same standards of medical care that your home country provides. According to one estimate only 5 out of every 100,000 tourists ever need international air ambulance evacuation, but when you consider that over 60 million American citizens travelled abroad last year, the number does not seem so small all of a sudden. The need for international air ambulance evacuation actually seems extremely essential.\nInternational Air Ambulance Evacuation Cover is Not as Expensive as You Imagine\nOne can buy international air ambulance evacuation cover through travel insurance for about $200 to 300. Seems expensive? Think again. Getting back through a commercial airliner with a nurse to accompany you can alone cost about $25,000. A full fledged international air ambulance evacuation can cost you up to $100,000. Also, keep in mind that Medicare does not cover you outside the United States. When you consider the financial risk, spending additional couple hundred dollars for your own safety and wellbeing does not seem all that expensive. Frequent overseas travelers can avail international air ambulance evacuation cover for roughly the same amount through annual plans. With a little thought and adequate cover, you can enjoy this holiday season without any worries at the back of your mind."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:f45bb7da-5fab-4695-9f18-07bb0e5a64a7>","<urn:uuid:10e5df45-fdaa-420d-b69c-078433a023b7>"],"error":null}
{"question":"What similarities exist between urban renewal strategies aimed at highway removal and Murfreesboro's approach to historic preservation in downtown development?","answer":"Both approaches emphasize preserving community character while promoting economic development. Urban highway removal advocates focus on restoring existing urban centers and reconfiguring spaces into diverse districts, while preserving built legacy. Similarly, Murfreesboro's development plan specifically preserves the historic church sanctuary and bell tower from 1888 while incorporating new mixed-use development. Both strategies aim to create vibrant communities - urban planners through removing concrete barriers and restoring street grids, and Murfreesboro through balanced development that maintains historic elements while adding modern amenities like hotels, residential units, and retail spaces.","context":["October 21, 2017; New York Times\nHas the time come to radically revise how we think of cities? The fabric of urban and suburban life has been woven with thick strands of highways that connect and separate communities and neighborhoods. Some urban planners see this as the right moment to rip them out and rebuild cities with less of an eye toward their ability to move traffic and more emphasis on quality of life for their residents.\nAccording to the Congress for the New Urbanism, these urban planners envision “the restoration of existing urban centers and towns within coherent metropolitan regions, the reconfiguration of sprawling suburbs into communities of real neighborhoods and diverse districts, the conservation of natural environments, and the preservation of our built legacy.”\nSteven Kurtz, writing in the New York Times about the place of highways in this new urban landscape, says, “If it sounds counterintuitive, if not crazy, to tear down a highway that still carries thousands of cars and trucks each day, there are a number of case studies to point to.” Supporting the removal of freeways also has an economic rationale that is quite attractive with tight city, state, and national budgets:\nPerhaps the greatest argument that removal advocates have is that so much of this infrastructure is nearing the end of its life span. In this era of tight budgets and political gridlock, it may be cheaper for local and state governments to remove a freeway rather than repair or build a new one.\nBy removing lanes of concrete, city planners hope to encourage the development of new and improved mixed-use communities that will attract new residents and spur economic growth. As an example, proponents point to the removal in 2002 of a length of highway leading into central Milwaukee that was once thought critical. John Norquist, Milwaukee’s mayor at the time and the spearhead for this effort, told the Times, “The bill to demolish the Park East and restore the street grid was around $30 million, significantly less than the $80 to $100 million estimated cost to rebuild the 40-year-old freeway.” Norquist pointed to “the rising land values and the slow-but-steady development along the 26-acre corridor in the years since—and the lack of a traffic apocalypse—as signs of success.”\nAngelo Trichilo, deputy chief engineer for the New York State Transportation Department, imagines “an urban boulevard that allows all modes of transportation—pedestrians, bicyclists, and cars—to use that facility” once offered by the multi-lane highways.\nMaking large, urban communities take on more of a small-town feel sounds appealing, but will it actually solve any of the difficult problems our cities face? In many cases, our highway systems provided mechanisms to allow white communities to wall off black and poor neighborhoods and to allow whites to flee cities for the suburbs. Removing these barriers might create a way to undo some of the harmful impacts of segregation against which urban schools and police departments still struggle.\nHowever, there’s also the danger that these efforts will accelerate gentrification, bringing younger, whiter, and wealthier residents back into central cities at the expense of longtime older, poorer, and less mobile populations. Ensuring enough affordable housing is available in our urban centers would only be made more difficult by an uncontrolled process of renewal. And without highways to connect suburbs to cities, there is a real danger that “undesirables” will be further isolated and disconnected from economic and social opportunity.\nRick Cohen, writing for NPQ, saw the downside of uncontrolled gentrification in Chattanooga, Tennessee, as “the deepening of income and racial inequities in the midst of metropolitan progress.” For policymakers and urban planners, the challenge will be to balance the interests of all residents and not just follow the allure of the potential economic benefits from gentrification. Can they hear and value the voices of those with limited financial and political clout? The answer may be seen where the rubber currently meets the road.—Marty Levine","(MURFREESBORO) On Friday (6/28/2019) the City of Murfreesboro finalized plans to sell city-owned land and develop an uptown hotel, condos and retail-office space. The old First Methodist bell tower and sanctuary will be preserved in the $1.8-million project. The land was sold to Nick Patel and One East College, LLC from Detroit.\nThe development, on property known as the former First United Methodist Church location, will incorporate office, residential, retail, restaurant, and hotel elements in a design consistent with the historic character of the site.\nAs you recall, in 2016 the City of Murfreesboro purchased the square block of downtown land from Franklin Synergy Bank for $1.55-milion. This is the approach the city did many years earlier, when it purchased large amounts of farm land to develop into what is now The Avenue, St. Thomas Rutherford Hospital, and all of what is now know as The Gateway Project.\nThis approach allows the city to be in control of what moves into a major development, along with controlling the design.\nCity Manager Craig Tindall told WGNS, \"The Development will preserve and incorporate the sanctuary and bell tower of the historic church. The project will include a hotel, an important amenity for the downtown area and Middle Tennessee State University. It will also include a publicly accessible parking garage that will increase the availability of downtown parking.\"\nBack on October 18, 2018, the Murfreesboro City Council voted to approve the purchase and sale agreement with developer One East College, LLC for redevelopment of the city block.\nOne East College, LLC is led by Nick Patel of Detroit, Michigan, Suri Ramanna, Nashville chairman and CEO of TRC Construction Services Inc., and Dhvanit Patel, a Tampa real estate developer. These developers have been studying the engineering and other technical aspects of the proposed development. The development group completed their purchase of the property Friday for $1.8 million. After closing, the development group will begin working with the City and the community to plan and design a project that will enhance the City's downtown and serve as a catalyst for further redevelopment efforts. Jeff Reed of Murfreesboro firm Hudson Reed and McCreary, PLLC is representing the developers.\nThe 2.48-acre City block bounded by Church, Lytle, College and Spring Streets, includes a church sanctuary and iconic bell tower and office buildings previously owned by Franklin Synergy Bank. The property sale agreement also includes a 0.62-acre City-owned parking lot at the corner of Lytle and Spring Streets to complete the block.\nMurfreesboro Mayor Shane McFarland noted, \"We are excited about closing a finalized agreement with One East College, LLC for redevelopment of this historic location in our City. We believe this mixed-use development balances both the need for downtown economic development and preservation of this historic amenity important to Murfreesboro's character and tourism.\"\nRedevelopment of the former Franklin Synergy site is expected to revitalize downtown Murfreesboro by adding significant commercial, retail, and residential properties into the downtown area. Over the long-term, the mixed-use development will substantially increase the assessed value of the site, which will enhance City and County property tax revenues.\nThe hotel is expected to include 110 rooms. The residential portion of the development anticipates approximately 55 residential units with nearby access to office and retail space with a restaurant and public parking garage. The four to six-story structure development will include more than one building with up to 200,000 square feet.\nIn 2016, the City reached an agreement with Franklin Synergy Bank for the acquisition of 1.87-acres of property on the corner of East College and Church Street for $ 1.55 million. At that time, promoting and preserving the downtown area, including mixed-use development had been advocated as part of the City's 20-year comprehensive plan, Murfreesboro 2035. Main Street: Murfreesboro/Rutherford favored preserving at least some of the historic structure, which dates to 1888.\nTindall added, \"Our purchase of the property in 2016 gave City administrators and elected officials greater input into how this important historic location is developed. We believe this project has the opportunity to create jobs, capital and energy into the City's downtown.\"\nAlthough the church sanctuary is not listed on the National Register of Historic Places, nor is it located in the City's historic district, City and community leaders sought to keep the bell tower preserved as part of the historic downtown landscape. The former First United Methodist Church building served the congregation there until 2003.\nThe Murfreesboro 2035 comprehensive plan recommended greater investment in the downtown area to leverage its full potential for mixed use, residential and office space, and entertainment. Preserving historic elements of downtown is seen as a lucrative aspect of future redevelopment and maintaining Murfreesboro's character."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:993dc43f-7bd9-4e16-8c34-4558066950ec>","<urn:uuid:4d0c8b28-6ab3-47fe-b179-1932821bf0cb>"],"error":null}
{"question":"How can reduced tillage practices benefit organic soybean farming, and what are the potential climate-related challenges these farming systems might face in the future?","answer":"Reduced tillage practices in organic soybean farming offer several benefits, including 50% less soil disturbance compared to tillage-based systems, which leads to improvements in water quality and soil conservation. These systems also have lower input costs. However, these farming systems may face significant climate-related challenges in the future. The combination of land degradation and climate change is predicted to reduce global crop yields by an average of 10%, and up to 50% in some regions by 2050. Climate change impacts include accelerated soil erosion on degraded lands, increased risk of forest fires, and changes in the distribution of invasive species, pests, and pathogens.","context":["UNIVERSITY PARK, Pa. – Organic soybean producers using no-till and reduced-tillage production methods that incorporate cover crops can achieve similar yields at competitive costs compared to tillage-based production.\nThat’s the conclusion of a new study by researchers at Pennsylvania State University. The findings are significant because they may contribute to increased sustainable domestic production of organic soybeans.\nThe experiment focused on finding ways to reduce the intensity or frequency of tillage or soil disturbance in organic field-crop production systems. It was conducted on certified-organic land at Penn State’s Russell Larson Agricultural Research Center. We compared tillage-based soybean production preceded by a cover crop mixture interseeded into corn. Reduced-tillage soybean production was preceded by a roller-crimped cereal-rye cover crop sown after corn silage.\nThe reduced-tillage soybean sequence resulted in 50 percent less soil disturbance compared to the tillage-based soybean sequence across study years. That offers promising substantial gains in water quality and soil conservation. Budget comparisons showed the reduced-tillage soybean sequence resulted in fewer input costs than the tillage-based soybean sequence. But the reduced-tillage system was about $46 per acre less profitable because of slightly less average yields.\nOrganic grain producers are interested in reducing tillage to conserve soil and reduce labor and fuel costs. We examined agronomic and economic tradeoffs associated with alternative strategies for reducing tillage frequency and intensity in a cover crop-soybean sequence within a corn–soybean-spelt organic-cropping system.\nPeople are also reading…\nWeeds are a serious problem for organic growers of field crops because growers are unable to kill them with herbicides. We found that weed biomass didn’t differ between soybean-production strategies. That matters because tillage and cultivation are the primary methods used by organic producers to reduce weeds and other pests.\nTillage-based soybean production marginally increased grain yield by fewer than three bushels per acre compared with the reduced-tillage soybean system.\nThe study is the latest in a 15-year-long line of organic no-till research conducted in the college of agricultural sciences at Penn State and led by William Curran, professor emeritus of weed science. Although he retired last year he also participated in the study. Organic no-till field-crop research continues at Penn State with the direction of Wallace and entomologist Mary Barbercheck.\nFinding ways to allow more domestic production of organic soybeans is a huge issue. More than 70 percent of the organic soybeans fed to organically produced poultry in the United States are imported. The organic soybeans come primarily from Turkey, India and Argentina.\nThere have been many cases of fraudulent imports. That has depressed premiums that U.S. producers receive because we’re being flooded with these imports.\nThe U.S. Department of Agriculture’s National Institute of Food and Agriculture through the Organic Agriculture Research and Extension Initiative grant program partially supported the research. The study recently was published in “Renewable Agriculture and Food Systems.” Visit cambridge.org/core/journals and search for \"agronomic-and-economic-tradeoffs-between-alternative-cover-crop-and-organic-soybean-sequences\" for more information.\nJohn Wallace is an assistant professor of weed science at Pennsylvania State University. He led the research project on using cover crops in organic-soybean production.","Author: Claire Stam\nPosted on: Euractiv.com| March 27th, 2018\nLand degradation caused by human activities undermines the well-being of at least 3.2 billion people, costs more than 10% of annual global GDP in lost ecosystem services and endangers food security, warn a hundred experts from 45 countries in a three-year assessment report published yesterday (26 March).\n“By 2050, the combination of land degradation and climate change is predicted to reduce global crop yields by an average of 10%, and by up to 50% in some regions. In the future, most degradation will occur in Central and South America, sub-Saharan Africa and Asia – the areas with the most land still remaining that are suitable for agriculture,” the report reads.\nProduced by the Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services (IPBES) under the auspices of the United Nations, the organisation describes the report as “the world’s first comprehensive evidence-based assessment of land degradation and restoration”.\nThe experts explain that ongoing land degradation worldwide is caused by high consumption lifestyles in more developed economies, combined with rising consumption in developing and emerging economies.\nTo meet the needs for this “high consumption lifestyle”, crop and grazing lands are being expanded into native vegetation, leading to unsustainable agricultural and forestry practices and, in specific areas, an increase in urban expansion, infrastructure development and extractive industry, they explain.\nUnless urgent and concerted action is taken, “land degradation will worsen in the face of population growth, unprecedented consumption, an increasingly globalised economy and climate change”, the experts warn.\nThe report also finds that land degradation is a major contributor to climate change, with deforestation alone contributing about 10% of all human-induced greenhouse gas emissions.\nAnother major driver of the changing climate has been the release of carbon previously stored in the soil, with land degradation between 2000 and 2009 responsible for annual global emissions of up to 4.4 billion tons of CO2, it says.\n“Land degradation is a major contributor to climate change, while climate change can exacerbate the impacts of land degradation and reduce the viability of some options for avoiding, reducing and reversing land degradation”, the experts explain.\nAs a result of more extreme weather, major impacts are to expected, the experts warn: “These include, among others, accelerated soil erosion on degraded lands, increased risk of forest fires and changes in the distribution of invasive species, pests, and pathogens”.\nSustainable land management\nBut the situation is not irreversible, they say. The solution lays in sustainable land management, meaning reforestation, changes in agricultural practices which would lead to fewer pesticides as well as long-term costs and benefits calculation.\nAccording to the experts, “short-term gains from unsustainable land management often turn into long-term losses, making the initial avoidance of land degradation an optimal and cost-effective strategy”.\nThey argue that on average, the benefits of restoration are 10 times higher than the costs, with a number of ripple effects: “While challenging, the benefits of restoration include, but are not limited to, increased employment, increased business spending, improved gender equity, increased local investment in education and improved livelihoods.”\nBut time is scarce, they warn: “An urgent step change in effort is needed to prevent irreversible land degradation and accelerate the implementation of restoration measures”\nRead at: https://www.euractiv.com/section/agriculture-food/news/land-degradation-threatens-wellbeing-of-two-fifths-of-humanity-major-report-warns/\nAuthor: Claire Stam"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:e62e50ef-7f37-4949-8e0a-6e45db17410d>","<urn:uuid:78f21bf0-5b41-4805-a060-342cfb08c56d>"],"error":null}
{"question":"What are benefits and risks of electronic financial records management?","answer":"Electronic financial records management offers benefits through organized Finance Management Systems (FMS) that enable efficient accounting, financial reporting, and internal controls. However, it also brings significant cyber risks including potential data breaches that can lead to privacy notification expenses, forensic investigation costs, and business interruption. Examples show damages can be substantial - one case involved $1.8 million in lost data during a system upgrade, while others faced legal expenses from security breaches and unauthorized information access.","context":["How important are good financial strategies and processes for your business? Of course they are very important. The goal of the Finance Polices and Procedure Manual is to assist you with key aspects of financial operations that include regulatory compliance, improving performance (through well-defined processes) and implementing best practices into operational areas like Raising Capital and Treasury Management.\nThe Finance field is broad, complex, and dynamic, so that no document can claim to be absolute in capturing every possible issue, policy, or procedure and remain current. The concepts discussed in the manual cover the common, basic elements of a Finance Management System.\nThis section provides a training introduction and overview of the basic concepts of Finance, and more:\n- The Balanced Scorecard\nFINANCE AND REGULATORY BASICS\n- Securities Legislation of the 1930’s\n- The Sarbanes-Oxley Act\nFINANCE AND INTERNAL CONTROL\n- COSO and Internal Control\n- Why Internal Control?\n- A Control System that Focuses on Improvement and Success\n- The Plan-Do-Check-Act Approach to Well-Defined Processes\n- An Internal Control System Can Drive Improvement\n- A Disciplined Approach\nUNDERSTANDING KEY FINANCIAL PROCESSES\n- Capital Needs Analysis\n- Operating Cash Flow or EBITDA\n- Making Investment Decisions\n- Return on Invested Capital (ROIC)\n- Cost of Capital\n- Managing Working Capital\n- Accounts Payable\n- Accounts Receivable\n- Inventory Management\nOTHER FINANCIAL PROCESSES\n- Financial Statements\n- Administration and Auditing\nAdd To Cart\nFinance Policies Procedures Manual\nUse the Financial Policies and Procedures Manual to quickly establish consistent financial statement treatment, treasury management, strong financial auditing and internal controls to manage capital. Implement consistent standards based on standard best practices that help you comply with regulatory requirements. Download your set of easily editable Microsoft Word documents.\nFinance is a technical field. A number of terms are used throughout the Financial Policies and Procedures Manual. Those financial terms commonly used within a Finance Department are defined in this glossary.\nARTICLES OF INCORPORATION\nA document (also called a charter) filed with a U.S. state by a corporation’s founders, describing the purpose, place of business, and other details of a corporation.\nA management system organized around balancing four distinct business perspectives: financial, customer, internal, and innovation/learning; Balanced Scorecards seek to balance short and long term objectives, financial and non-financial measures, lagging and leading indicators, and internal and external perspectives.\nBOARD OF DIRECTORS\nIndividuals elected by a corporation’s shareholders to oversee the management of the corporation. The president reports to the Board of Directors.\nThe period or phase of a business usually defined by its potential for growth and profitability. For example, businesses are not typically profitable in their initial phase but have tremendous growth potential. On the other hand, while mature businesses may no longer be capable of rapid growth rates, they are often more profitable than when they were new.\nThe rules that govern the internal affairs or actions of a corporation. Bylaws generally include procedures for holding meetings and electing the board of directors and officers. The bylaws also set out the duties and powers of a corporation’s officers.\nA business taxed as a separate entity: a business taxed under Subchapter C of the Internal Revenue Code and legally distinct from its owners.\nCHIEF FINANCIAL OFFICER (CFO)\nThe senior manager of the Finance Department. The CFO reports to the Company’s Chief Executive Officer (CEO).\nA practice by which the mundane and non-controversial board action items are organized in a group and passed with one motion.\nDisaster Recovery Plan to ensure that the effects of an extended disruption (e.g., natural disaster) are minimized and the organization is able to maintain or quickly resume mission-critical functions.\nDocument that provides information or direction for performance of work within the scope of the FMS. Characteristics of control include such things as revision number (letter), signatures indicating review and approval, and controlled distribution.\nInformation and its supporting medium; the medium may be paper, magnetic, electronic, optical, photograph, or a sample of the Company’s product.\nDOWNTIME (OR “DOWN TIME”)\nDuration of an equipment or system stoppage, scheduled or unscheduled, measured from the moment of failure to the moment at which normal operations resume.\nDocument originating outside the Company (e.g., customer drawings, industry and/or international standards, suppliers’ equipment maintenance manuals, or references) that provides information or direction for performing activities within the scope of the FMS.\nFINANCE MANAGEMENT SYSTEM (FMS)\nSystem of managing the Company’s financial resources, including accounting and financial reporting, internal controls, budgeting, collecting accounts receivable, risk management, and insurance. The FMS for a small business includes how you are financing the business and how you manage its money. Ordered, well-documented system of policies, processes, and procedures, designed to assure compliance, conformity, and security; demonstrate a system of internal controls; and promote continual improvement of financial processes.\nClearly expressed and specific objectives (i.e. expressed in numbers, percentages, ratios) of the Company as they relate to its financial operations and financial structure.\nPrinted, typed, or electronic document with blank spaces for insertion of required or requested information (e.g., tax form, order form). Document or web form with spaces in which to write; business document that typically contains some predefined data and designated, labeled areas for filling in data.\nDocument of internal origin (developed entirely by or completed by the Company) that provides information or direction for the performance of activities within the scope of the FMS. Examples include, but are not limited to, FMS procedures.\nIndicator of past performance, such as actual sales over a period.\nIndicator of future performance (e.g., an increase in interest rates is often a leading indicator of reduced consumer spending).\nConsists of the CEO, CFO, and Finance department managers, at a minimum.\nProcess or series of acts involved in a particular form of work; detailed elements of a process used to produce a specified result. Procedures are different from work instructions. Procedures document a process. Lower level detail of activities or tasks are documented within a work instruction.\nThe minimum number of people who must be present at a stockholders’ meeting, physically or by proxy, in order for a decision made at such a meeting to be binding. The quorum requirements should be stated in the Company’s bylaws.\nAnything retained to provide and preserve permanent evidence of or information about an event (e.g., document, photograph, nonconforming product sample).\nAn official document representing an action or intent of action on the part of the Board of Directors of a corporation.\nRETURN ON INVESTMENT (ROI)\nProfit or loss resulting from an investment transaction, usually expressed as an annual percentage. Return On Investment (ROI), calculated by dividing the expected results of committed resources by the resources committed to achieve the results. (ROI = Results / Resources.)\nAn acronym that describes important attributes for objectives: Specific, Measurable, Attainable, Relevant, and Time-Bound or SMART.\nSTOCKHOLDER (OR SHAREHOLDER)\nOne who owns shares of stock in a corporation (also called stockholder). Ownership includes the right to declared dividends and to vote on certain company matters, including electing the Board of Directors.\nGenerally, a group of Company officers (e.g., chief executive officer (CEO), Chief Financial Officer (CFO)) with primary responsibility for decisions and activities affecting the Company in the long term (e.g., strategic planning); may also be referred to as “senior level management”.\nDocument that is not a part of the controlled document system. Uncontrolled documents may not be used to provide work direction or information necessary for the performance of work.\nFor a step in a process, the instruction(s) on how to perform that step, sufficiently detailed to allow an inexperienced person to perform the work. See Difference Between Process, Procedures and Work Instructions.","What is Cyber Risk?\nCyber Risk is first and third-party risk associated with e-business, the Internet, networks and informational assets.\nWho is at risk?\n- Business Owners who operate a website.\n- Business Owners who are concerned with their clients’ and employee’s information being compromised.\n- Business Owners who are concerned about copyright/trademark infringement.\n- Business Owners who are aware of the risks associated with computer hackers, viruses and other damaging computer programs.\n- Business Owners who understand the importance of upholding and preserving their professional reputation should an incident occur.\n- Business Owners who keep electronic records of clients names, addresses, phone numbers, social security numbers, credit card numbers and other sensitive information.\n- Business Owners who accept credit card payments.\n- Business Owners who may have employees that could compromise sensitive customer information or do something illegal to make some money.\n- Business Owners who use laptops, Blackberries or other portable devices that store client information.\nThird Party Liability\n- Disclosure Injury - Including lawsuits alleging unauthorized access to or dissemination of the plaintiff’s private information. (Can be extended to outsourced data processing and data storage services.)\n- Content Injury – Including suits arising from intellectual property infringement, trademark infringement, and copyright infringement.\n- Reputational Injury – Including suits alleging disparagement of products or services, libel, slander, defamation, and invasion of privacy.\n- Conduit Injury – Including suits arising from system security failures that result in harm to third-party systems.\n- Impaired-Access Injury – Including suits, civil fines and penalties arising from system security failure resulting in your customer’s systems being unavailable to its customers.\nFirst Party Cyber Crime Expenses\n- Privacy Notification Expenses – Including printing, drafting, postage, call center costs and advertisements, cost of credit-monitoring services, credit freezes and fraud alerts for affected customers (even when state law doesn’t require notification). Estimated at $30 per person.\n- Forensic Costs – Costs to determining how the breach occurred.\n- Crisis Management and Reward Expenses – Including the cost of public relations consultants to maintain the reputation of the business.\n- E-Business Interruption – Including first-dollar extra expense.\n- E-Theft and E-Communication Loss – Extended to networks outside of your company’s system.\n- E-Threat or Cyber Extortion - Including the cost of a professional negotiator and ransom payment to stop cyber attacks caused by malicious hackers.\n- E-Vandalism Expenses – Even when the vandalism is caused by an employee.\nA manufacturer hosted a site banner for a key vendor. The manufacturer was unaware that the vendor's slogan was similar to a slogan of a company based in France. The manufacturer was dragged into an international trademark infringement lawsuit. Claim Value: $700,000\nA chain of luxury hotels was expanding it's operations worldwide. They needed to upgrade their billing system to accomodate various currencies and tax rates. The chosen vendor upgrading the existing system, meeting all time requirements and milestones. However, during the final phase of installation, one of the installers accidentally erased $ 1.8 million of crucial data. As a result the customer sued the software installation company for the losses they incured. Claim Value: $1.8 Million\nDuring a national trade convention, the CFO of a prominent company read from a media kit about its products and those of competitors, including defamatory comments about the executive officers of a competitor. The competitor sued for libel and slander for $1.5 Million\nA bookseller created a Web site to promote itself. The Web site included passages from books. The publisher and author of one of the books quoted on the Web site sued the bookseller, alleging copyright infrinement and theft of intellectual property. The case settled for approximately $60,000. The bookseller incurred defense costss close to $35,000.\nA software development company was sued by one of its best customers after using the company's cost-estimating program. The custommer claimed that a defect in the software caused them to underbid several projects. After a lengthy investigation, the software was found free of any defect, and it was user error that caused them to underbid. The customer dropped the case after considerable legal expenses were incurred by the software developer."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:093c7c84-d31c-4d27-98a0-365a8a64bb01>","<urn:uuid:9240e847-a688-419f-945f-e0ba9a3a6134>"],"error":null}
{"question":"As someone studying marine biology, I'm curious about how the Gulf Stream's physical properties affect marine life compared to the deeper ocean currents it connects to?","answer":"The Gulf Stream's physical dynamics strongly influence marine life at the surface, while changes in deeper currents can affect the entire ocean ecosystem. At the surface, the Gulf Stream creates eddies and meanders that drive exchanges between shallow shelf waters and the open ocean, affecting nutrient distribution and phytoplankton growth. Satellite data shows that where the Gulf Stream ejects warm, nutrient-poor waters northward, there are lower concentrations of phytoplankton, while areas where it entrains shelf waters show high phytoplankton concentrations. As for the deeper currents in the meridional overturning circulation, any disruption could lead to decreased downwelling that would deliver less oxygen to the deep ocean, while reduced upwelling would bring fewer nutrients up from the bottom, potentially devastating ocean ecosystems.","context":["Tracking the Gulf Stream with satellite data\n2020 and 2021\nUsing satellite data from multiple satellite instruments to track the Gulf Stream.\n05 May 2023\n07 June 2021\nBy Vinca Rosmorduc (CLS), Ben Loveday (Innoflair), Hayley Evers-King (EUMETSAT) and Annamaria Luongo (SpaceTec Partners)\nThe Gulf Stream is one of the most famous ocean currents. First mapped by Benjamin Franklin in 1769, modern maps showing the full extent and complexity of this major current have been made by satellites since the 1990s. Radar altimetry satellites, such as Jason-3, Sentinel-3A and -B, and soon Sentinel-6 Michael Freilich, observe the sea surface height over all the oceans. Over the Gulf Stream, they observe a sea surface 'cliff', meandering with small bumps and troughs detaching and, sometimes, attaching to it.\nKnowing the ocean topography in two dimensions enables analysis of the surface geostrophic currents, i.e., the ocean circulation generated by the balance between the horizontal pressure gradient forces exerted by water masses and the effect of acceleration due to the Earth’s rotation (Figure 1). Outside the equatorial band, the geostrophic currents are computed from the gradient of the surface slopes, which are observed by altimetry satellites. The satellite data are provided by EUMETSAT to the Copernicus Marine Environment Monitoring Service (Global ocean gridded L4 Sea Surface Heights and derived variables) and the Copernicus Climate Change Service (Sea level daily gridded data from satellite observations for the global ocean from 1993 to present), which provide the sea surface heights and geostrophic currents to users (Figure 2).\nThe impact of the Gulf Stream can be observed beyond its signature in sea surface height. The transport of water around the global oceans by its currents, is intimately connected to the transport of heat. As such, observing such currents from the yearly to decadal timescales gives information about impacts on seasonal weather (see this companion case study on how weather satellites observe the Gulf Stream — Gulf Stream visible off Virginia coast), and over decades a shift has been observed which may be linked to climate change (Kelly and Dong, 2004, The relationship of western boundary current heat transport and storage to mid-latitude ocean-atmosphere interaction, in Earth’s Climate: The Ocean-Atmosphere Interaction).\nThe physical environment of currents like the Gulf Stream also strongly influence life in the oceans. The interplay between strong western boundary currents and shelf-edge bathymetry often results in the production of eddies and meanders. These drive exchanges between the shallow shelf waters and those of the open ocean, influencing the provision of nutrients and light for the most basic levels of life in the ocean — the phytoplankton.\nFigure 3 and Figure 4 show respective sea surface temperature (SST) and blue/green false colour images over a section of the Gulf Stream in May 2021, as retrieved from measurements made by the SLSTR and OLCI instruments on Sentinel-3A. The false colour indicates areas of higher (green) and lower (blue) concentrations of phytoplankton. The similar patterns seen in both the SST and the false colour image show how influential the physical dynamics of the current are on the growth of phytoplankton. Where the Gulf Stream ejects eddies of warm, nutrient poor waters to the north, on to the shelf, we see an associated patch of blue, low phytoplankton waters. Where the Gulf Stream entrains shelf waters, we see the tell-tale imprint in the green, high phytoplankton patches that have been dragged to the south.\nThe plant-like phytoplankton are the basis of nearly all marine life, providing food for many dependent animals throughout the marine chain, and ultimately, for us. They also play a critical role in the carbon and oxygen cycles of the Earth, influencing both atmospheric composition and the chemical composition of the oceans.\nConstellations of satellite missions such as Jason-3, Sentinel-3 and Sentinel-6 mean that we can observe the complex interactions between the physics and biology of our oceans, in places like the Gulf Stream and beyond.\nCurrent heat - January 2005 (AVISO)\nSurface Currents in the Atlantic Ocean - The Gulf Stream (Rosenstiel School of Marine and Atmospheric Science)","By Edwin Schiele\nOcean surface currents redistribute heat around the world and have a profound effect on the world’s climate. Nowhere is this clearer than in the North Atlantic Ocean. The Gulf Stream and the North Atlantic Current ferry huge volumes of warm salty tropical water north to the Greenland coast and to the Nordic Seas. Heat radiating off of this water helps keep the countries of northwest Europe, which are at the same latitude as Labrador and Greenland, relatively comfortable places to live.\nMany scientists, however, are warning that the North Atlantic might cool down, perhaps by the turn of the century. Paradoxically, global warming would be to blame. Rising temperatures may trigger events that could not only slow the supply of tropical water flowing north, it could disrupt the entire ocean circulation pattern.\nThis scenario has led to wild talk of the start of a new ice age, a notion that climate scientists universally dismiss. Still the impact on the world’s climate could be profound. Scientists are therefore scrambling to gather data on ocean circulation and the forces that drive it.\nOcean circulation is comprised of a global network of interconnected currents, counter-currents, deepwater currents, and turbulent eddies. From this complex circulation, an underlying transport pattern emerges. Water cycles from surface currents to deepwater currents then back to the surface again in what scientists liken to a giant conveyor belt. Scientists call this global conveyor belt the meridional overturning circulation.\nThere are two major forces driving the meridional overturning circulation. First there is the wind. The wind, in combination with the Earth’s rotation, generates the gyres that circle the major ocean basins. Turbulent swirling packets of water called eddies, many of which are hundreds of kilometers in diameter, spin out of these wind-driven currents and carry the water trapped inside them to other parts of the ocean.\nThe second force is tied to differences in the density of water. Temperature and salinity independently affect water’s density. The colder and saltier the water, the denser it becomes. As water becomes denser, it sinks.\nThis is where the Atlantic Ocean plays a pivotal role. Again, the Gulf Stream and the North Atlantic Current carry warm salty tropical water up into the Labrador and Greenland Seas. Frigid Arctic winds cool this water, increasing its density. The water then sinks, feeding deepwater currents. This same density driven creation of deepwater also takes place in the frigid Ross and Weddell Seas off the coast of Antarctica, and to a lesser extent in the salty Mediterranean Sea.\nScientists call this density-driven component of the meridional overturning circulation the thermohaline circulation; thermo meaning heat and saline meaning salt. Without this density-driven process, deepwater currents would no longer be created. The global conveyor belt would grind to a halt.\nScientists are using observations and models to trace the complex pathways of the meridional overturning circulation and determine its strength. It’s an overwhelming task. Maps charting the circulation’s course are still evolving. Deeper currents and upwelling in particular are extremely difficult to measure. But some patterns are becoming clearer.\nStarting off the Greenland coast, the newly created deepwater slowly drifts south along the western margin of the Atlantic basin. It then crosses the equator and mixes with the deepwater currents circling Antarctica. Models suggest that some of this water resurfaces in this area. Much of it, however, spreads north into Indian and Pacific Oceans where it mixes with warmer water and resurfaces.\nTo close the loop of the conveyor belt, surface water flows from the Pacific and Indian Oceans back into the South Atlantic then heads north. Some cold water enters the South Atlantic from the Pacific around the southern tip of South America. The Agulhas Current in the Indian Ocean is another important source. This fast-moving current, the Indian Ocean’s equivalent of the Gulf Stream, flows down the southeast coast of Africa and past the tip of South Africa then takes a sharp turn to the east. Large eddies called Agulhas Rings spin off this bend and carry huge bundles of warm salty Indian Ocean water west into the South Atlantic. Currents carry much of this Indian Ocean water north to the equator where the sun heats it further. Eventually this water enters the Caribbean and is swept into the Gulf Stream.\nScientists believe that these Agulhas Rings are critical sources of the salty water that drives the formation of deep water up north. Eddies spinning out of the Mediterranean Sea and net evaporation in the tropical Atlantic also contribute salty water.\nDespite its enormous scope, the meridional overturning circulation is vulnerable. Places where deepwater currents are created comprise less than one percent of the ocean’s surface area. Should the temperature or salinity in these limited areas change, the creation of deep water could slow or even stop.\nThere is strong evidence that such a shutdown has happened in the past, drastically altering the world’s climate in just a matter of years. Eleven thousand years ago, ice age glaciers were retreating. In central Canada, an immense glacial lake called Lake Agassiz occupied an area larger than all the Great Lakes. Suddenly the dams holding Lake Agassiz collapsed. The contents of the entire lake rushed into the North Atlantic by way of the St. Lawrence River. This massive infusion of fresh water diluted the polar seas to the point where the water was no longer dense enough to sink. The meridional overturning circulation likely ground to a stop. Called the Younger Dryas, this one thousand year period saw the temporary return of the ice age.\nWe may soon face a similar although far less drastic situation. Scientists are predicting that rising temperatures will melt the Greenland ice sheet. Models suggest that the resulting influx of fresh melt water into the polar sea could weaken the meridional overturning circulation, although not as drastically as the events thought to have triggered the Younger Dryas period. Still it could slow enough to reduce the flow of warm tropical water north into the polar seas. Temperatures over northwestern Europe could drop as much five degrees Celsius.\nPredictably, talk of such a scenario has led to some big misconceptions. First, a slowdown or even a stoppage of the meridional overturning circulation would NOT spell the end to the Gulf Stream. Wind and large-scale turbulence drive the bulk of the Atlantic Subtropical Gyre, of which the Gulf Stream is a part. The Gulf Stream would, however, draw significantly less water from the tropics.\nSecond, unlike during the Younger Dryas, a weakening of the meridional overturning circulation will NOT trigger another ice age. Rising temperatures due to global warming would offset most of the temperature drop. Armadas of icebergs floating off the New Jersey coast are just Hollywood fantasies.\nBut even in the absence of these most extreme scenarios, any disruption of the meridional overturning circulation can have far-reaching consequences. Models and paleoclimate data suggest that as less warm water flows north across the equator, the southern oceans will warm. The thermal equator (band of highest temperatures) would therefore likely shift south. The tropical rain belts would follow, altering rainfall patterns. Decreased downwelling would deliver less oxygen to the deep ocean, and decreased upwelling would carry fewer nutrients up from the bottom, potentially devastating ocean ecosystems.\nMonitoring the meridional overturning circulation and identifying changes in the thermohaline circulation is daunting. To separate real trends in ocean circulation from natural variability, scientists require huge volumes of data gathered over a long period of time. A global network of surface and deep profiling ARGO drifters that measure currents, water temperature, and salinity form the backbone of this effort. Moored buoys measure the southbound deepwater currents at strategic locations in the Atlantic. Satellites measure wind, sea surface temperatures, and sea surface height, and programs such as OSCAR calculate surface currents based on these measurements. In 2010, a new satellite, Aquarius, will begin to measure surface salinity throughout the ocean. These observations already form the foundation for global ocean circulation and climate models that are helping scientists predict how the oceans and climate will respond as the Earth warms."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:4bfe2888-bfa3-45a4-b5c9-1ca4028a1070>","<urn:uuid:fea69aba-bfd8-4845-9bab-d7e44bcba03d>"],"error":null}
{"question":"As a software developer, I need to document system states - should I use state machine diagrams or flowcharts, and what's the key distinction?","answer":"State machine diagrams and flowcharts serve different purposes in system documentation. State machine diagrams are specifically designed to represent the behavior of an application by showing how an object moves through different states during its lifespan until program termination. Flowcharts, on the other hand, show the step-by-step flow of processes or algorithms. A key distinction is that state machine diagrams require the system to be composed of a finite number of states and focus on triggered state transitions, while flowcharts represent the sequential flow of activities or decisions regardless of state.","context":["Table of Contents\n1. Object-Oriented Analysis and Design.\nWhat Will You Learn? Is it Useful?\nThe Most Important Learning Goal?\nWhat is Analysis and Design?\nWhat is Object-Oriented Analysis and Design?\nA Short Example.\nWhat is the UML?\nVisual Modeling is a Good Thing.\n2. Iterative, Evolutionary, and Agile.\nWhat is the UP? Are Other Methods Complementary?\nWhat is Iterative and Evolutionary Development?\nWhat About the Waterfall Lifecycle?\nHow to do Iterative and Evolutionary Analysis and Design?\nWhat is Risk-Driven and Client-Driven Iterative Planning?\nWhat are Agile Methods and Attitudes?\nWhat is Agile Modeling?\nWhat is an Agile UP?\nAre There Other Critical UP Practices?\nWhat are the UP Phases?\nWhat are the UP Disciplines?\nHow to Customize the Process? The UP Development Case.\nYou Know You Didn't Understand Iterative Development or the UP When.\n3. Case Studies.\nWhat is and isn't Covered in the Case Studies?\nCase Study Strategy: Iterative Development + Iterative Learning.\nCase One: The NextGen POS System.\nCase Two: The Monopoly Game System.\n4. Inception is Not the Requirements Phase.\nWhat is Inception?\nHow Long is Inception?\nWhat Artifacts May Start in Inception?\nYou Know You Didn't Understand Inception When...\nHow Much UML During Inception?\n5. Evolutionary Requirements.\nEvolutionary vs. Waterfall Requirements.\nWhat are Skillful Means to Find Requirements?\nWhat are the Types and Categories of Requirements?\nHow are Requirements Organized in UP Artifacts?\nDoes the Book Contain Examples of These Artifacts?\n6. Use Cases.\nDefinition: What are Actors, Scenarios, and Use Cases?\nUse Cases and the Use-Case Model.\nMotivation: Why Use Cases?\nDefinition: Are Use Cases Functional Requirements?\nDefinition: What are Three Kinds of Actors?\nNotation: What are Three Common Use Case Formats?\nExample: Process Sale, Fully Dressed Style.\nWhat do the Sections Mean?\nNotation: Are There Other Formats? A Two-Column Variation.\nGuideline: Write in an Essential UI-Free Style.\nGuideline: Write Terse Use Cases.\nGuideline: Write Black-Box Use Cases.\nGuideline: Take an Actor and Actor-Goal Perspective.\nGuideline: How to Find Use Cases.\nGuideline: What Tests Can Help Find Useful Use Cases?\nApplying UML: Use Case Diagrams.\nApplying UML: Activity Diagrams.\nMotivation: Other Benefits of Use Cases? Requirements in Context.\nExample: Monopoly Game.\nProcess: How to Work With Use Cases in Iterative Methods?\n7. Other Requirements.\nHow Complete are these Examples?\nGuideline: Should We Analyze These Thoroughly During Inception?\nGuideline: Should These Artifacts be at the Project Website?\nNextGen Example: (Partial) Supplementary Specification.\nCommentary: Supplementary Specification.\nNextGen Example: (Partial) Vision.\nNextGen Example: A (Partial) Glossary.\nCommentary: Glossary (Data Dictionary).\nNextGen Example: Business Rules (Domain Rules).\nCommentary: Domain Rules.\nProcess: Evolutionary Requirements in Iterative Methods.\nIII. ELABORATION ITERATION 1-BASICS.\n8. Iteration 1-Basics.\nIteration 1 Requirements and Emphasis: Core OOA/D Skills.\nProcess: Inception and Elaboration.\nProcess: Planning the Next Iteration.\n9. Domain Models.\nWhat is a Domain Model?\nMotivation: Why Create a Domain Model?\nGuideline: How to Create a Domain Model?\nGuideline: How to Find Conceptual Classes?\nExample: Find and Draw Conceptual Classes.\nGuideline: Agile Modeling-Sketching a Class Diagram.\nGuideline: Agile Modeling-Maintain the Model in a Tool?\nGuideline: Report Objects-Include 'Receipt' in the Model?\nGuideline: Think Like a Mapmaker; Use Domain Terms.\nGuideline: How to Model the Unreal World?\nGuideline: A Common Mistake with Attributes vs. Classes.\nGuideline: When to Model with 'Description' Classes?\nExample: Associations in the Domain Models.\nExample: Attributes in the Domain Models.\nConclusion: Is the Domain Model Correct?\nProcess: Iterative and Evolutionary Domain Modeling.\n10. System Sequence Diagrams.\nExample: NextGen SSD.\nWhat are System Sequence Diagrams?\nMotivation: Why Draw an SSD?\nApplying UML: Sequence Diagrams.\nWhat is the Relationship Between SSDs and Use Cases?\nHow to Name System Events and Operations?\nHow to Model SSDs Involving Other External Systems?\nWhat SSD Information to Place in the Glossary?\nExample: Monopoly SSD.\nProcess: Iterative and Evolutionary SSDs.\nHistory and Recommended Resources.\n11. Operation Contracts.\nDefinition: What are the Sections of a Contract?\nDefinition: What is a System Operation?\nExample: enterItem Postconditions.\nGuideline: Should We Update the Domain Model?\nGuideline: When Are Contracts Useful?\nGuideline: How to Create and Write Contracts.\nExample: NextGen POS Contracts.\nExample: Monopoly Contracts.\nApplying UML: Operations, Contracts, and the OCL.\nProcess: Operation Contracts Within the UP.\n12. Requirements to Design-Iteratively.\nIteratively Do the Right Thing, Do the Thing Right.\nProvoking Early Change.\nDidn't All That Analysis and Modeling Take Weeks To Do?\n13. Logical Architecture and UML Package Diagrams.\nWhat is the Logical Architecture? And Layers?\nWhat Layers are the Focus in the Case Studies?\nWhat is Software Architecture?\nApplying UML: Package Diagrams.\nGuideline: Design with Layers.\nGuideline: The Model-View Separation Principle.\nWhat's the Connection Between SSDs, System Operations, and Layers?\nExample: NextGen Logical Architecture and Package Diagram.\nExample: Monopoly Logical Architecture?\n14. On to Object Design.\nAgile Modeling and Lightweight UML Drawing.\nUML CASE Tools.\nHow Much Time Spent Drawing UML Before Coding?\nDesigning Objects: What are Static and Dynamic Modeling?\nThe Importance of Object Design Skill over UML Notation Skill.\nOther Object Design Techniques: CRC Cards.\n15. UML Interaction Diagrams.\nSequence and Communication Diagrams.\nNovice UML Modelers Don't Pay Enough Attention to Interaction Diagrams!\nCommon UML Interaction Diagram Notation.\nBasic Sequence Diagram Notation.\nBasic Communication Diagram Notation.\n16. UML Class Diagrams.\nApplying UML: Common Class Diagram Notation.\nDefinition: Design Class Diagram.\nWays to Show UML Attributes: Attribute Text and Association Lines.\nNote Symbols: Notes, Comments, Constraints, and Method Bodies.\nOperations and Methods.\nStereotypes, Profiles, and Tags.\nUML Properties and Property Strings.\nGeneralization, Abstract Classes, Abstract Operations.\nComposition Over Aggregation.\nTemplate Classes and Interfaces.\nWhat's the Relationship Between Interaction and Class Diagrams?\n17. GRASP: Designing Objects with Responsibilities.\nUML versus Design Principles.\nObject Design: Example Inputs, Activities, and Outputs.\nResponsibilities and Responsibility-Driven Design.\nGRASP: A Methodical Approach to Basic OO Design.\nWhat's the Connection Between Responsibilities, GRASP, and UML Diagrams?\nWhat are Patterns?\nWhere are We Now?\nA Short Example of Object Design with GRASP.\nApplying GRASP to Object Design.\nInformation Expert (or Expert).\n18. Object Design Examples with GRASP.\nWhat is a Use Case Realization?\nUse Case Realizations for the NextGen Iteration.\nUse Case Realizations for the Monopoly Iteration.\nProcess: Iterative and Evolutionary Object Design.\n19. Designing for Visibility.\nVisibility Between Objects.\nWhat is Visibility?\n20. Mapping Designs to Code.\nProgramming and Iterative, Evolutionary Development.\nMapping Designs to Code.\nCreating Class Definitions from DCDs.\nCreating Methods from Interaction Diagrams.\nCollection Classes in Code.\nExceptions and Error Handling.\nDefining the Sale.makeLineItem Method.\nOrder of Implementation.\nTest-Driven or Test-First Development.\nSummary of Mapping Designs to Code.\nIntroduction to the NextGen POS Program Solution.\nIntroduction to the Monopoly Program Solution.\n21. Test-Driven Development and Refactoring.\nIV. ELABORATION ITERATION 2-MORE PATTERNS.\n22. UML Tools and UML as Blueprint.\nForward, Reverse, and Round-Trip Engineering.\nWhat is a Common Report of Valuable Features?\nWhat to Look For in a Tool?\nIf Sketching UML, How to Update the Diagrams After Coding?\n23. Quick Analysis Update.\nCase Study: NextGen POS.\nCase Study: Monopoly.\n24. Iteration 2-More Patterns.\nFrom Iteration 1 to 2.\nIteration-2 Requirements and Emphasis: Object Design and Patterns.\n25. GRASP: More Objects with Responsibilities.\n26. Applying GoF Design Patterns.\nSome GRASP Principles as a Generalization of Other Patterns.\n\"Analysis\" Discoveries During Design: Domain Model.\nConclusion of the External Services with Varying Interfaces Problem.\nComposite (GoF) and Other Design Principles.\nObserver/Publish-Subscribe/Delegation Event Model (GoF).\nV. ELABORATION ITERATION 3-INTERMEDIATE TOPICS.\n27. Iteration 3-Intermediate Topics.\n28. UML Activity Diagrams and Modeling.\nHow to Apply Activity Diagrams?\nMore UML Activity Diagram Notation.\nExample: NextGen Activity Diagram.\nProcess: Activity Diagrams in the UP.\n29. UML State Machine Diagrams and Modeling.\nDefinitions: Events, States, and Transitions.\nHow to Apply State Machine Diagrams?\nMore UML State Machine Diagram Notation.\nExample: UI Navigation Modeling with State Machines.\nExample: NextGen Use Case State Machine Diagram.\nProcess: State Machine Diagrams in the UP.\n30. Relating Use Cases.\nThe include Relationship.\nTerminology: Concrete, Abstract, Base, and Addition Use Cases.\nThe extend Relationship.\nThe generalize Relationship.\nUse Case Diagrams.\n31. More SSDs and Contracts.\n32. Domain Model Refinement.\nNew Concepts for the NextGen Domain Model.\nDefining Conceptual Superclasses and Subclasses.\nWhen to Define a Conceptual Subclass?\nWhen to Define a Conceptual Superclass?\nNextGen POS Conceptual Class Hierarchies.\nAbstract Conceptual Classes.\nModeling Changing States.\nClass Hierarchies and Inheritance in Software.\nAggregation and Composition.\nTime Intervals and Product Prices-Fixing an Iteration 1 \"Error\".\nAssociation Role Names.\nRoles as Concepts versus Roles in Associations.\nUsing Packages to Organize the Domain Model.\nExample: Monopoly Domain Model Refinements.\n33. Architectural Analysis.\nProcess: When Do We Start Architectural Analysis?\nDefinition: Variation and Evolution Points.\nCommon Steps in Architectural Analysis.\nThe Science: Identification and Analysis of Architectural Factors.\nExample: Partial NextGen POS Architectural Factor Table.\nThe Art: Resolution of Architectural Factors.\nSummary of Themes in Architectural Analysis.\nProcess: Iterative Architecture in the UP.\n34. Logical Architecture Refinement.\nExample: NextGen Logical Architecture.\nCollaborations with the Layers Pattern.\nOther Layer Pattern Issues.\nModel-View Separation and \"Upward\" Communication.\n35. More Object Design with GoF Patterns.\nExample: NextGen POS.\nFailover to Local Services; Performance with Local Caching.\nFailover to Local Services with a Proxy (GoF).\nDesigning for Non-Functional or Quality Requirements.\nAccessing External Physical Devices with Adapters.\nAbstract Factory (GoF) for Families of Related Objects.\nHandling Payments with Polymorphism and Do It Myself.\n36. Package Design.\nPackage Organization Guidelines.\n37. UML Deployment and Component Diagrams.\n38. Designing a Persistence Framework with Patterns.\nThe Problem: Persistent Objects.\nThe Solution: A Persistence Service from a Persistence Framework.\nRequirements for the Persistence Service and Framework.\nPattern: Representing Objects as Tables.\nUML Data Modeling Profile.\nPattern: Object Identifier.\nAccessing a Persistence Service with a Façade.\nMapping Objects: Database Mapper or Database Broker Pattern.\nFramework Design with the Template Method Pattern.\nMaterialization with the Template Method Pattern.\nConfiguring Mappers with a MapperFactory.\nPattern: Cache Management.\nConsolidating and Hiding SQL Statements in One Class.\nTransactional States and the State Pattern.\nDesigning a Transaction with the Command Pattern.\nLazy Materialization with a Virtual Proxy.\nHow to Represent Relationships in Tables.\nPersistentObject Superclass and Separation of Concerns.\n39. Documenting Architecture: UML & the N+1 View Model.\nThe SAD and Its Architectural Views.\nNotation: The Structure of a SAD.\nExample: A NextGen POS SAD.\nExample: A Jakarta Struts SAD.\nProcess: Iterative Architectural Documentation.\nVI. SPECIAL TOPICS\n40. More on Iterative Development and Agile Project Management.\nHow to Plan an Iteration?\nAdaptive versus Predictive Planning.\nPhase and Iteration Plans.\nHow to Plan Iterations with Use Cases and Scenarios?\nThe (In)Validity of Early Estimates.\nOrganizing Project Artifacts.\nYou Know You Didn't Understand Iterative Planning When...\nPurchase Info ?\nWith CourseSmart eTextbooks and eResources, you save up to 60% off the price of new print textbooks, and can switch between studying online or offline to suit your needs.\nOnce you have purchased your eTextbooks and added them to your CourseSmart bookshelf, you can access them anytime, anywhere.\n$39.99 | ISBN-13: 978-0-13-708174-5","Get started for free! 3) Edraw Max Edraw Max is a UML builder software that helps you to make diagrams using ready-made symbols and templates. UML is a way of visualizing a software program using a collection of diagrams. A decision asks a question. If using UML it is an Activity Diagram. By installing the PlantUML as a VS Code extension, you can edit it right inside the VSCode software as in the example gif animation below. UML is a standard language for specifying, visualizing, constructing, and documenting the artifacts of software systems. State diagrams require that the system described is composed of a finite number of states; sometimes, this is indeed the case, while at other times this is a reasonable abstraction.Many forms of state diagrams exist, which differ slightly and have different semantics You can create a flowchart from scratch, or simply start from a flowchart template available in our flowchart software. It’s a general-purpose modeling language to standardize a way to visualize the architecture of software systems. Even if you drew an activity diagram in exam, the answer was not correct unfortunately. It is an open-source tool and syntax for creating a UML diagram. If using something else, then something else is being used. Statechart diagram is used to capture the dynamic aspect of a system. When comparing Lucidchart vs Diagrams.net, the Slant community recommends Lucidchart for most people. Although flow chart is used in almost all the fields of education and engineering data flow diagram is mainly used is the software industry. Furthermore, this book describes the fundamental programming and testing techniques for successful agile solution delivery. On the other hand, use case diagram is a behavioral UML diagram type and frequently used to analyze various systems. The flowchart and diagramming tools are used for everything from basic workflow diagrams to complex network diagrams, organization charts, BPMN (Business Process Model and Notation), UML … It was developed by Grady Booch, Ivar Jacobson and James Rumbaugh at Rational Software in 1994–1995. This flowchart uses decision shapes intensively in representing alternative flows. OMG is continuously making efforts to create a truly industry standard. Best For: Lucidchart makes it easy for anyone to visualize a complex idea, process, or system! Flowchart templates, smart connectors and intuitive interface to create flowcharts. This is a hospital flowchart example that shows how clinical cases shall be processed. Example: State diagrams versus flowcharts. The most popular chart designs can be crafted within the program, including Flowcharts, UML, ER diagrams, data flow diagrams, mind map diagrams, and more. UML stands for Unified Modeling Language. Flowchart Example – Simple Algorithms. UML Diagram What is a UML Diagram? Lucidchart is ranked 1st while Diagrams.net is ranked 2nd 1. Flowchart vs Data Flow Diagram (DFD) The terms flowchart and data flow diagram (DFD) are related to software engineering describing the path of process or data step by step. It also represents algorithms and different workflows. Whether you're trying to read a flowchart or creating a flowchart, knowing the most common flowchart symbols and conventions is going to make it a lot easier. I think you are correct here. The purpose of the flowchart is mainly to design, manage and to analyze different programs and processes conducted in various areas. Many exporting options, preset color themes and real-time collaboration for teams. Data flow diagram (DFD) is the part of the SSADM method (Structured Systems Analysis and Design Methodology), intended for analysis and information systems projection. The lifespan of an object remains until the program is terminated. State machine vs. Flowchart ; Why State Machine Diagram? Flowchart is a diagram which is made step by step using different shapes and sizes of arrows which show their connection. For instance, if there is no response on a case for 10 minutes after it is created then an email is triggered to the Case manager. Flowchart is one of the most widely-used diagrams that represents an algorithm, workflow or process, showing the steps as boxes of various kinds, and their order by connecting them with arrows. Here, we've got the four flowchart symbols you've got to know, plus a rundown on some more intermediate process symbols if you're looking for extra credit. UML - Activity Diagrams - Activity diagram is another important diagram in UML to describe the dynamic aspects of the system. A state diagram is a type of diagram used in computer science and related fields to describe the behavior of systems. People often confuse state diagrams with flowcharts. Developing software is a complex process, and it is difficult to write the program of the entire system directly. BPMN has evolved the simple concept of a flowchart decision block into different types of gateways (shapes). The arrows flowing from the decision shape are usually labeled with Yes, No or True, False. Starting Price: $7.95/month/user. $5.95/month/user. 4 Basic Flowchart Symbols. SmartDraw serves anyone who needs to make a diagram, from contractors and architects, to business executives and project managers. In the question \"What are the best online and free Visio replacements?\" Create easy-to-understand visuals with confidence. The following is a basic overview, with descriptions and meanings, of the most common flowchart symbols - also commonly called flowchart shapes, flow diagram symbols or process mapping symbols, depending upon what type of diagram you're creating.The table below lists the flowchart symbol drawing, the name of the flowchart symbol in Microsoft Office (with aliases in … The activity can be described as an operation of the system. Not the same, flowchart is a simplied version of activity diagram in UML. The main difference between DFD and Flowchart is that DFD is a graphical diagram that represents the data flow of a system while flowchart is a graphical diagram that represents the sequence of steps to solve a problem.. example is the Unified Modelling Language (UML) where the flowchart is used under the name Activity Diagram. Lucidchart (Web) - Best online flowchart maker for collaboration and Microsoft Visio compatibility. Textografo (Web) - Best text-based flowchart maker for quickly turning outlines into flowcharts. Popular among software engineers to document software architecture, class diagrams are a type of structure diagram because they describe what must be present in the system being modeled. Diagrams are a great means for us to connect with information and process its significance; they help in communicating relationships and abstract information and enable us to visualize concepts.. UML was created by the Object Management Group (OMG) and UML 1.0 specification draft was proposed to the OMG in January 1997. Draw.io (Web, Windows, Mac, Linux, ChromeOS): Best free flowchart … The 8 Best Flowchart and Diagramming Tools. The ultimate tool for diagramming. One of the more popular types in UML is the class diagram. Data flow diagrams are intended for graphical representation of data flows in the information system and for analysis of data processing during the structural projection. Data flow diagram often used in connection with … Learn about UML, BPMN, ArchiMate, Flowchart, Mind Map, ERD, DFD, SWOT, PEST, Value Chain and more. EdrawMax is an advanced all-in-one diagramming tool for creating professional flowcharts, org charts, mind maps, network diagrams, UML diagrams, floor plans, electrical diagrams, science illustrations, and more. Learn from diagram examples and start creating your diagrams online. Therefore, it is necessary to model the system … The notation has evolved from the work of Grady Booch, James Rumbaugh, Ivar Jacobson, and the Rational Software Corporation to be used for object-oriented design, but it has since been extended to cover a wider variety of software engineering projects. The Unified Modeling Language (UML) can help you model systems in various ways. Microsoft Visio (Windows, Web) - Best Windows flowchart software for Office power users. Quickly create visual representations of a process or organization by making a diagram with ClickCharts. Sometimes diagrams are drawn which are combinations of flow charts, Petri nets and state transition diagrams, the IEC 61131-3 Sequential Flow Chart being typical in … Also, if your application has a state machine, drawing a (flowchart-like) state machine diagram is essential. State machine diagrams are used to represent the behavior of an application. UML State machine diagram and activity diagram are both behavioral diagrams but have different emphases.Activity diagram is flow of functions without trigger (event) mechanism, state machine is consist of triggered states.. A flowchart can also be used in visualizing algorithms, regardless of its complexity. With this tool, you can create a wide variety of UMLs, from a simple flowchart to a complex diagram. Online flowchart software to easily visualize your workflows. A UML Activity Diagram is a type of \"flowchart\".. in UML, with bells and whistles.There is no such thing as a \"UML Flowchart\". The Object Primer 3rd Edition: Agile Model Driven Development with UML 2 is an important reference book for agile modelers, describing how to develop 35 types of agile models including all 13 UML 2 diagrams. A data flow diagram (DFD) explains the relationship among different components of a system by showing the flow of data within it. These have been used in the support system example for the response time & communication process. You Will Love This Easy-To-Use Diagram Software. Flowchart Example – Medical Service. Later in 1997, it was adopted as an industry standard. ... Activity diagram is basically a flowchart to represent the flow from one activity to another activity. An object goes through various states during its lifespan. For example, in the shape below, if it is cloudy, you follow the arrow down near the word Yes.If it is not cloudy, you follow the arrow to the right near the word No. (Source: Visual Paradigm Tutorials) State Machine Diagram vs Activity Diagram UML State machine diagram and activity diagram are both behavioral diagrams but have different emphases. Get Started! The figure below shows a comparison of a state diagram with a flowchart. Features: You can create a flowchart, mind map, UML, electrical diagrams, network diagrams, etc. It is more common to use class-dependency diagrams like UML, to describe your architecture at a much higher level. It allows you to import your drawings to file formats such as PDF, PPT, Word, HTML, etc. Choose from dozens of premade templates, starter diagrams, and … When comparing Diagrams.net vs PlantUml, the Slant community recommends Diagrams.net for most people.In the question“What are the best flowchart / diagram softwares?”Diagrams.net is ranked 1st while PlantUml is ranked 18th. The answer to the question determines which arrow you follow out of the decision shape. Markdownish syntax for generating flowcharts, sequence diagrams, class diagrams, gantt charts and git graphs.\nNikon D750 Lenses, Char-broil Tru-infrared 2-in-1 Electric Smoker & Roaster, Casio Ct-x5000 Tone List, How To Disbud Chrysanthemums, Sparkling Cider Punch Non Alcoholic, Web Application & Software Architecture 101 Pdf, Msi We75 Review, Tick Symbol In Word, Project Initiation Document Template Pmbok, Alh Login Portal,"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:682aba86-05d0-4a71-b96c-0e68aae200c8>","<urn:uuid:6ff66721-c8b6-4155-8280-5e2451dbfc29>"],"error":null}
{"question":"How do the challenges and benefits of frozen farmed fish compare to those of fresh wild-caught fish in terms of quality and sustainability?","answer":"Frozen fish, when caught, processed, and frozen immediately during transportation, can actually be fresher than wild-caught fish that spent up to 10 days on a boat, plus additional processing and shipping time, potentially being 10-14 days old by the time it reaches consumers. Wild-caught fish that isn't properly handled can taste fishy due to age. While many consumers are conditioned to view wild and fresh as superior, both wild fish and farmed fish can be unsustainable depending on their sourcing. Wild salmon populations are severely threatened, with sockeye salmon at risk of extinction by 2050 at current latitudes, while responsible aquaculture can provide a sustainable alternative when properly managed.","context":["by Lindsay Abrams, News Deeply\nTo feed a growing population, sustainable seafood advocate Ned Bell is encouraging fellow chefs and seafood consumers to look beyond ‘fresh’ and ‘wild.’\nNed Bell has no shortage of opinions about seafood. He grew up fishing Pacific salmon off the west coast of Canada and now evangelizes for the future of fish as the founder of Chefs for Oceans and the executive chef at Ocean Wise, a nonprofit organization based at the Vancouver Aquarium that develops criteria for sustainable seafood and lends its logo to vendors and restaurants that meet its standards.\nBell has worked in the tradition of activist chefs who aim to change minds by first filling stomachs. “I have to focus on what is within my wheelhouse,” he told Oceans Deeply, after meeting at the recent Sustainable Brands Vancouver Conference. “I feed people.” He means that literally, but sees his cookbooks, videos and other attempts at public engagement, such as appearances on various TV cooking shows, as another form of “feeding” us – in this case, inspiration for a more sustainable future.\n“I joke, if I put something tasty in your belly, you’re going to like me, and if you like me, you’re going to listen to my message,” Bell said. “I want to make sure that when I’m sharing a message, that it comes from a place of fact and experience, not just opinion.”\nBell spoke to Oceans Deeply about the travel and conversations that led him to embrace the seafood we’ve been conditioned to suspect – farmed fish and frozen fish chief among them – and how he hopes to change the global conversation around sustainable eating one plate at a time.\nOceans Deeply: Can you share some of your thinking in terms of what brought you around to seeing aquaculture as a good, sustainable option?\nNed Bell: Aquaculture is the future. Was it a come-around? Yes, absolutely. I grew up on Vancouver Island, where fish farming was a four-letter word. But that’s just because we didn’t know better. It’s just because we listened to the rhetoric, we listened to the environmental crusaders, we listened to the headline news and unfortunately we didn’t spend enough time with the fish farmers ourselves, challenging them to dive into technology and improve what they were doing and where they were doing it. And underneath our noses, they have been doing that.\nIs aquaculture perfect? Absolutely not. But we don’t eat wild animals. Most of us eat animals that we raise for our consumption. Most people don’t even know what they’re eating half the time, let alone whether it’s farmed or wild. They just see fresh fish and they think, “Oh, it must be good.”\nOceans Deeply: As a chef, how do you think about working to change that narrative?\nBell: I think if I could choose one thing that I’m going to focus on in the next decade, it’s to educate my peers. It’s not about promoting the next cool, really popular, unsung hero seafood. No. It’s about getting them to appreciate what responsible aquaculture means, and how to celebrate it. Feeding an ever-growing population needs to be something we’re focused on, and also getting clean, healthy protein in the mouths of people.\nFarmed fish is scary for many people, so they just end up staying away. Unless it says “wild,” they for some reason forget that wild fish can also be unsustainable Certainly if it says “frozen,” there’s, “Oh, my gosh, there’s no way that frozen fish can be of high quality.” This means it’s old. Well, that couldn’t be further from the truth. Fish caught, processed and then frozen immediately during its transportation, when you take it home and thaw it under cold water, is going to be way fresher than a fish that lived on the boat for up to 10 days, got to the port, got processed and then was shipped on to your grocery store and could be upwards of 10 to 14 days old by the time you bring it home. No wonder the fish you grew up eating tasted fishy, because it was.\nOceans Deeply: When you say “responsible aquaculture,” what does that look like?\nBell: From my perspective as the Ocean Wise executive chef, the fish that we deem responsible meet our criteria. We have a number of criteria that we base on science, the where and how of raising fish, so whether it’s closed containment, land-based, what they’re eating or what type of fish is being grown.\nI feel as though we’re only a few steps, and maybe a few technological improvements away from having most farmed fish be pretty darn “sustainable.” We’re going to fix the feed problem. We’re going to fix the siting of farms – where they are. We’re going to fix the species problem – what we’re growing. We get good at things if people are willing to support that journey. I’m literally having for lunch today some wild Pacific albacore that I torched last night at home. Aren’t I lucky to be able to eat that for lunch? Yeah. But if it was land-based coho salmon grown a couple hours from here, that would be amazing. If it was a species grown in net pens somewhere where I knew that it wasn’t having impact on the other Pacific species of fish or an ecosystem, I would support that.\nOceans Deeply: If we don’t think a lot about where the seafood comes from, we probably think even less about whom it’s coming from. What did your visit to Vietnam teach you about that aspect of sustainability?\nBell: We import 80 percent of the seafood we consume in North America. Why is that? Well, because it’s cheaper. By far, the most consumed seafood in North America is shrimp. So I wanted to go to Vietnam to go see the good, the bad and the opportunity of the shrimp farming industry in a country where we import a lot of those shrimp into my country.\nWhenever I go to a place where an industry has an impact on the community, I’m always amazed when you peel back the layers. When I went to Vietnam, I definitely saw some bad. I saw some of the low-end of the shrimp farming industry and the commodity shrimp farming production of large-scale, cheap product for some retailers and/or restaurant groups in North America that need inexpensive product. But then I also saw some really great sustainable stories that really focused on the family and the community. One of the farmers said, “You know, I live on top of a gold mine. All I have to do is take care of these organic black tiger shrimp, and they sustain me, my family and my community four, five, six times a year,” depending on his crop.\nSo here’s a guy who directly understood that the environment and the ecosystem that he is nurturing is giving him his livelihood, and I think we forget that when we’re all the way back across the ocean in North America, going to our local premium grocery store and buying an organic black tiger shrimp. We forget that by choosing sustainable, by choosing Ocean Wise-recommended, or by choosing Seafood Watch green, whatever the equivalent is depending on where you live, you are having an impact on a community, or fisher folk or a family.\nOceans Deeply: When you talk about clean, healthy food, does that mean you see that as something that can come out of aquaculture as well?\nBy being welcomed into the conversation, they’re going to share with me almost everything I need to know to make my own mind up. Of course, they might skew it slightly. You never know the full truth of everything, but I’ve spent 10 years now of my life in this space of healthy lakes, oceans and rivers and sustainability, and I’ve just started to feel confident enough to start championing the good stories that are out there.\nThis interview has been condensed and edited for clarity. This story was produced as a result of a partnership with the Sustainable Brands Conference.\nBell: Yes. And I’m not saying that all aquaculture is good. We have work to do. But I know that I will have a larger impact on the industry by working with them as opposed to working against them.","Mr. Speaker, we have today in this Parliament a historic opportunity to act on the key recommendations of the Cohen Commission to protect wild salmon and the wild salmon economy and to innovate and take action on coastal job creation.\nThe importance of wild Pacific salmon cannot be overstated. They are the foundation of indigenous culture in British Columbia. They are the foundation of our coastal ecology, and they established British Columbia's settlement pattern.\nSalmon support a $102-million west coast commercial fishery employing 1,400 people. They support a $326-million west coast recreational fishery employing 8,400 people. They fuel a $783-million west coast wilderness tourism industry employing 26,000 full time, and roughly 40,000 in total. Yet wild salmon are at risk globally. Due to climate change and the increased prevalence of salmon farms along migratory routes, salmon populations on the west coast are at serious risk.\nWorldwide, since 1975, oceans have absorbed 90% of the excess heat from global climate change. Worldwide, fisheries, as a result, could lose $10 billion of their annual revenue because of climate change.\nSince salmon farms proliferated on our coast in the nineties, Fraser sockeye populations have crashed. In 2009, the salmon run on the Fraser River saw only 1.4 million fish, a drastic low in spawning returns from typical levels, which are usually 20 million to 30 million. Sockeye salmon at our latitude are threatened with extinction by 2050, and potentially all species of salmon are threatened with extinction by 2100, if we do not act. This would affect everything, moving up the food chain, including resident killer whales.\nWrites Diana Hardacker from the riding I represent, “I teach students at the Nanaimo River Fish Hatchery about the important irreplaceable role that Pacific Salmon play in the health of our ecosystem, and our health. Any threat to that, namely disease from Atlantic salmon, is unacceptable.”\nI agree. Open-net fish farms are a further threat to Pacific wild salmon. They are located in key migratory areas for wild salmon, and there is evidence that they are harming wild salmon. Feces and waste feed damage the ecosystem near fish farms. They promote the spread of disease and allow sea lice to flourish.\nImagine being a wee salmon minnow running the gauntlet of net-pen fish farms on the migratory route. They emerge bristling with sea lice. Salmon have enough to contend with between ocean, river, lake, and four years out in the ocean without this impossible burden of sea lice. If and when viruses spread to wild salmon, which are already under threat from sea lice, the results could be even more catastrophic.\nI salute the work of Alexandra Morton. She is a heroine on our coast for standing up for wild salmon and ringing the alarm on science and the threat from salmon farms.\nWe need to transition to closed containment. West coast salmon, wild salmon, are under threat from sea lice, pollutants, and diseases coming from open-net fish farms. We have see this happen already. Norway, Chile, Scotland, and now B.C. have all had problems with their wild salmon fishery as a result of the contamination from open-pen fish farms. We cannot afford the declining wild salmon population, and we cannot afford the aquaculture collapse.\nI heard about this from Julie Smith, who wrote to me, “As someone who was a commercial fisher for over 25 years and now has lost my job because of the decline of fish returns, this is something I feel very strongly about. Please support this, it is important.”\nI urge the government to do the right thing and transition this industry to safe closed containment technology.\nNew Democrats called for a judicial inquiry into the sockeye collapse of the Fraser salmon run, and then we championed the implementation of the recommendations resulting from the Cohen Commission. The new Liberal government has promised full implementation of Cohen's recommendations, yet 18 of its 20 deadlines have passed already without any action.\nCohen said that the Government of Canada should remove from the Department of Fisheries and Oceans' mandate the promotion of salmon farming as an industry and farmed salmon as a product.\nNever was the need more clear to remove that conflict of interest than when we heard the DFO parliamentary secretary in debate last month proclaim the spotless record of the aquaculture industry. It is just not fair.\nWe heard strong words from Cohen on the precautionary principle and the possible link between open-net salmon farming and the decline in wild salmon.\nWe are arguing in the bill today that closed containment salmon farms are the solution for the west coast. They would create jobs while protecting wild salmon. We already have 70 licensed closed containment fin farms in British Columbia already, so the technology is proven. They would keep the farm environment on the farm, and the wild environment wild. They would protect wild salmon from parasites like sea lice. On-land fish farms can better control the water temperature and water quality, maximizing the efficiency of growth, which is good for the salmon farming business.\nIt is time to innovate. The rate of change in this industry is tremendous and Norway and Denmark are already generating very good results. This is proven technology and we are already making great strides across Canada in closed containment Atlantic salmon production with Sustainable Blue in Nova Scotia and Kuterra leading the way in B.C.\nWith many of my colleagues from three different parties in the House, we had the chance to visit Kuterra. It has been in operation since 2013 and is fully owned by the 'Namgis First Nation on northern Vancouver Island. It is designed to produce 450 tonnes per year of antibiotic and hormone-free, non-GMO Atlantic salmon. All the water is recycled and cleaned every hour and 99% of that water is reused. The solid waste, which in traditional fish farms is dumped into the ocean, is filtered, captured, and composted in Kuterra's on-land farm. The ammonia in the water is converted to nitrate and can be used for aquaponics.\nI cannot say enough how inspiring it was to go to this facility to see local people innovating, working together, creating local jobs, using local feed, and generating those on-the-ground results that can really inspire further success.\nWe have heard again and again from the proponents at Kuterra what a tremendous advantage British Columbia has. We already have fish farms, technicians, processing plants, everything to give us an advantage now over the United States. Yet the Danish company Langsand Laks is starting a 30,000 tonne on-land closed containment Atlantic salmon farm in Florida. Canada could get ahead of this sustainable, hi-tech wave by innovating now to protect wild salmon.\nMy colleague, the member of Parliament for Port Moody—Coquitlam, has such a strong record of standing up for wild salmon. He has flown across Georgia Strait, across the Salish Sea, and down the Fraser River. He has been a fantastic proponent and advocate, along with NGOs like the Georgia Strait Alliance. His bill before us today would strengthen the Fisheries Act by requiring west coast salmon farms to move from open-net harmful pens to safe closed containment systems within five years.\nIt would require the Minister of Fisheries, Oceans and the Canadian Coast Guard to plan the transition of the salmon farming industry on the west coast to closed containment in a way that moves those jobs that support the local economy. It is a win-win for the environment and local employment.\nWhile 150 first nation bands oppose open-net salmon farms, there is tremendous support for this on-land salmon farm bill. The Union of B.C. Indian Chiefs, the First Nations Fisheries Alliance, the Canadian Wildlife Federation, the B.C. Wildlife Federation, and hundreds of others are in support. Moreover, almost 1,200 of my constituents have written to me to say that they want to see Parliament support the bill.\nI will end with a plea to Parliament to recognize the sacred duty we have, the responsibility we have, to make things right for both the wild economy and local jobs. We can support a just transition to on-land closed containment salmon farms in B.C., proving that Canada can innovate, create jobs, protect wild salmon, and protect B.C.'s coastal economy."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:fcb3e767-5203-44d7-ac6d-f54555e009b2>","<urn:uuid:6cbf4537-e431-44d8-9947-4c88ade0cdae>"],"error":null}
{"question":"What are the potential consequences and impacts of dog attacks on sheep farms?","answer":"Dog attacks on sheep farms can have severe consequences: 1) Pregnant ewes can suffer miscarriages 2) Sheep can sustain serious injuries or death from dog attacks 3) Even small dogs can cause devastating injuries to sheep 4) Surviving sheep often develop long-term psychological effects, becoming wary and panicky around dogs, which makes handling them difficult 5) Farmers face expensive veterinary treatments 6) In some cases, farmers may have no choice but to shoot dogs that worry livestock 7) The attacks can be particularly devastating for small holdings where sheep are treated almost like pets.","context":["Spring time is of course one of the most important times of the year for sheep farmers, spending long hours tending pregnant ewes and vulnerable new lambs. That is why it is more important than ever to ensure that dog owners avoid livestock and always keep their dogs under control and in sight – even better would be to keep dogs on leads at all times – when in or close to fields with pregnant ewes and new born lambs.\nWhile reported cases of livestock worrying in the Cairngorms National Park are low it is nonetheless a serious worry for the Park’s farmers and crofters with dog fouling being another issue of concern. Parasites found in some dog faeces can result in the abortions of cattle and death in sheep.\nOutdoor access officers at the Cairngorms National Park Authority (CNPA) are urging dog owners to put dogs on leads where there is livestock, regardless of whether the dog is well trained and usually walks to heel, and to ensure all poo is picked up and disposed of appropriately.\nAdam Streeter Smith, Outdoor Access Officer at the CNPA said: “Every springtime we encourage dog owners to consider how and where they walk their dog to ensure a less stressful lambing time for our farmers and crofters. Dogs should never be off a lead where there could be pregnant ewes and newborn lambs. Our daily dog walks are good for our physical and mental health – and please continue to enjoy them – but please do so with the utmost care.”\nThe Scottish Outdoor Access Code and current legislation are very clear when it comes to the responsibilities that dog owners have when walking in or close to farmland and a dog caught amongst sheep could mean a hefty fine for the owner and the dog being destroyed.\nCNPA board member and farmer Eleanor Mackintosh said: “Pregnant ewes and newborn lambs are extremely vulnerable and nervous at this time of year and should not be worried by dogs that have been allowed to run loose. The consequences can be devastating from miscarriage to injuries as a direct result of chasing or attacks by dogs that are not in control. No-one wants to have to witness the damage than can result from dogs that are allowed to run amok.”\nNFU Scotland’s Head of Policy Team, Gemma Cooper said: “As we prepare to emerge from lockdown at a time that coincides with lambing and calving, it is more imperative than ever that dog owners ensure that their pets are controlled in the countryside.\n“We continue to see the devastating impacts of dog attacks on livestock and this crime is completely unacceptable. Unfortunately, we know of a number of cases where farmers have been left with no choice but to shoot dogs that have worried livestock. Any dog, including the most placid family pet, can inflict horrific damage to animals such as sheep. Particularly during lambing season, dogs must not be taken into fields of pregnant ewes or fields where there are young lambs.\n“Given that livestock attacks and dog fouling are two of the biggest issues that farmers and landowners face through the irresponsible actions of dog owners, the Union has welcomed the landmark Protection of Livestock Bill, as it makes its way through the Scottish Parliament. Having now passed Stage Two in the parliamentary process, the Bill will significantly increase penalties and powers for investigation in cases where livestock have been attacked by dogs.”\nMark and Alison run a small holding in rural Aberdeenshire. They have 22 pure bred Texel sheep in an 18-acre field near to their home.\nOne November afternoon Alison received a call from her mother-in-law to say that one of their neighbour’s dogs was in the field with their sheep. Mark and Alison raced to the field where they found a scene of devastation. Numerous sheep had blood on their faces, two were lying on the ground and one could be seen to have a significant injury to its face.\nThe dog, a small Border terrier, was now under control and tied to a fence post. Its owner, one of Mark and Alison’s neighbours, was visibly upset and immediately admitted her dog was responsible for what had happened.\n“There was so much damage. I couldn’t believe a small dog could have caused such awful injuries,” said Alison. “Eight of our sheep had bites to their faces, some were laid out flat on the ground. Another had a more significant bite and had a gaping hole in its face. It was so upsetting. We had to get the vet immediately.”\nFollowing a course of expensive treatment, the sheep with the most severe injury had to be put to sleep.\n“She couldn’t be saved, she was left with a huge hole in her face, and her injuries were too extensive. Because we only have 22 sheep she was like a pet, always coming up to us looking for attention. It’s been really heart-breaking.”\nThere have been long term practical implications for the couple as Mark explains: “As well as physical scars the sheep have been left really wary and they panic whenever they see dogs. This makes it very hard for us to handle them.”\nPolice Scotland encourages farmers and landowners to engage with dog walkers and to put signs up on gateways and on key roads and paths alerting them to the presence of sheep and lambs in their fields, and to report all incidents of livestock worrying to police on 101 or 999 in an emergency.\nThe Scottish Outdoor Access Code provides comprehensive advice for dog walkers and all aspects of accessing the Scottish countryside."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:93205d26-2ffe-4379-8b26-9064711d15e2>"],"error":null}
{"question":"How do traditional maps used for hiking compare to specialized orienteering maps in terms of their scale and detail level, and what navigation methods are used differently between Badge events and general orienteering practices?","answer":"Orienteering maps are created on smaller scales than traditional maps, providing more detail and enabling more precise navigation. They uniquely show vegetation boundaries in yellow and man-made objects in black, while including detailed rock and water features, fences, power lines, and picnic tables. In Badge events, which are held throughout the UK about four times a year in the southwest, competitors navigate courses according to their age/sex with varying technical and physical difficulty. General orienteering practices involve using attack points (obvious land features like boulders or riverbends) for navigation and maintaining a pace count to measure distance in the woods.","context":["Types of Orienteering events\nBOK runs numerous events through out the year. In order that these events may be properly planned and organised it is necessary for Klub members to undertake these tasks. For this purpose the klub is divided into teams to carry out such duties as Car Parking, Start and Finish. In this way no one need help more than about twice a year.\nBOK events are divided into several categories.\nEvents are either:\n|Score||These take place over a fixed period of time and competitors score points according to the number of control locations visited.|\n|Relay||Teams of competitors run their courses in sequence usually with a mass start.|\n|Cross Country||Competitors start at interval and have to visit certain control locations in a given order. The fastest is the winner. The majority of events are cross country and are categorised as follows. Informal or Local, Usually of a non-competitive nature, often aimed at a particular orienteering skill.|\n|Colour Coded or District|\n|Badge or Regional||Badge events are held through out the UK and there are about four a year in the south west. Competitors run on courses according to their age/sex, the technical and physical difficulty of the course varies accordingly. There are 'A' & 'B' (hard & easy) classes for Juniors and Long & Short courses for Seniors. Badges are awarded according to the standard achieved. BOK organises one Badge or a National event each year, the BOK Trot.|\n|National||There are eight National events held each year throughout Britain with one in the south west. These are similar to Badge events but of a higher standard of difficulty.|\n|The J.K. (Jan Kjellström) International Festival||This annual event is organised in turn by the regions and therefore take place in various parts of the country. Everyone can take part. The courses are of similar standard to the National event.|\n|British Championships||This takes place annually in various regions of the UK. You must be a full BOF member to take part.|\nThese are more navigationally challenging than day events. National and Regional Night Championships take place each year. BOK members also compete in the Western Night League; a series of score events held in and around Bristol.\nBecause of the inherent risks involved with night orienteering, the klub requires that all competitors must carry a whistle and a backup light. The organiser may also require runners to wear an article of high visibility and/or reflective clothing, and, if the weather justifies it, wear or carry a waterproof hooded jacket.\n|String Course||A String course has now become a popular feature at many events. Provided for young children with very little knowledge or skill of orienteering. They follow the string, and using a specially prepared map, visit controls on the way. Although it is not intended to be of a competitive nature the youngsters are usually timed and enjoy emulating the older competitors.|\nSaturday events normally take place in the afternoon, except for pre-entry events when competitors are informed of their start times. At Sunday events registration usually opens at 9.30am, with start times typically from 10.30am to 12.30pm. These times do vary, especially for pre-entry events. It is always advisable to check with the event contact before travelling.\nIf you are interested in sharing transport to an event you should contact the klub member who arranges sharing or a member in your area. A membership list will be sent to you when you join the klub. Many members share on a regular basis.\nDuring the summer months Informal events are organised for you to practice orienteering skills, with tuition if required.\nWeekly training runs are also held for the more active followed by an informal chat in a pub. You do not have to do the run to attend.\nEach summer there is a camping weekend when there are training sessions as well as social events.\nDetails of all these are in BOKNews and on this web site.","Most people tend to avoid wandering into the woods or unfamiliar terrain, but that's part of the fun of orienteering.\nOrienteering is a collection of sports that requires navigational skills and tools, such as a map and compass, to navigate from point to point.\nSometimes orienteering is done quickly as part of large competitions, but it can also be done at a more leisurely pace to explore a new area and hone one's navigational skills.\nOn a recent trip to Georgia's Sweetwater Creek State Park, Jason Henline, a former U.S. Army intelligence analyst, showed us how to navigate to several unidentified man-made objects on our orienteering map, where we discovered everything from crumbling cabins to rusty old trucks.\nWhether you want to improve your map-reading and navigation skills for sport or survival, here are some tips, tricks and explainers to get you started.\nAlways let someone know where you're going and when you plan to be back.\n\"This is 'Outdoorsy 101' stuff obviously, but it's less of an issue when you are sticking to established paths,\" Henline said. \"The whole idea of orienteering is to travel overland, so informing someone that you are going to go walk around in the woods for a while and might get lost is a good idea.\"\nThis video above does a nice job of summing up the basics of using a map and compass to navigate in the woods.\nUnderstand your map\nOrienteering maps are created on smaller scales than traditional maps, increasing the amount of detail and allowing for more precise navigation. They're topographic maps that include not only the land's natural features, but also detailed rock and water features and objects like fences, power lines and picnic tables.\nSuch maps also have unique elements like portraying vegetation boundaries with yellow or marking man-made objects in black. Before heading into the woods, familiarize yourself with the basics of reading such a map.\nYour map should mimic your environment exactly\nYour compass will naturally always point north, but make sure your map does as well. For example, if you're in the woods looking north, hold your map so it too faces north. If you turn around and face south, flip your map around so that north on the map still points north.\nThis method ensures that the landscape you're looking at is mirrored on the map.\nOne simple way to keep track of where you are on the map is to keep your thumb on your current location. As you move through terrain, move your thumb as well so that you can always pinpoint precisely where you are.\nKnow your pace count\nOne way to measure distance is pace count. A pace is equal to your natural step, and to use this method, you need to know how many paces it takes you to walk 100 meters.\nTo determine your pace count, walk a measured 100-meter course and count every other step you take. Then turn around and rewalk the distance, counting again. Take the average of those numbers and you have your pace count.\nMake sure the terrain you use to count your steps is similar to the terrain you'll be hiking, and try to mimic the conditions and gear as much as possible.\n\"Don't go out with an empty backpack and tennis shoes if you're planning a major overnight hike in boots later because the two pace counts will vary quite a bit,\" Henline said.\nOnce you have your count, you can use it to gauge distance in the woods, and there are many ways to keep track of your count as you trek.\n\"Distances in the woods are really tricky,\" Henline said. \"You may feel like you've been walking a long time but only covered a short distance, on account of difficult terrain, a heavy pack, or losing track of time as you contemplate your existence in nature ... not that I've done that.\"\nAn attack point is an obvious land feature, such as a large boulder or a riverbend, that can be used to help you navigate.\nFor example, if your destination is in the middle of similar-looking terrain, it can be difficult to locate. Instead of wandering, identify a unique marker as your attack point and navigate to it. From there, measure the distance to your destination and map out how many meters you have to go.\nAlways have a backup plan\nBefore you set out, establish a panic azimuth, which is simply a direction in which to walk if you start to panic and need to get out of there.\n\"The idea is that you are hard-pressed to find a section of woods without a nearby road in today's America,\" Henline said. \"Sure, you can find vast expanses in various national parks and the Alaskan tundra, but at your local state park, you're probably going to be bordered by a road or natural boundary in some direction.\"\n\"Use it to your advantage. Find a road that goes in a relatively straight line that borders the area you're planning to navigate within. Figure out which direction it will be from your planned orienteering course, and use that direction as your panic azimuth.\""],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:43325dae-d247-42b9-b716-d9387802cc69>","<urn:uuid:9df7d252-42ed-4356-b5e6-f0f29358691f>"],"error":null}
{"question":"As a physics student studying thermal dynamics, I'm curious about how ice formation differs in lakes versus refrigerators, and what causes the humming noise during freezing?","answer":"Ice formation differs significantly between lakes and refrigerators. In lakes, freezing occurs only from the top surface down because cold water at 4°C sinks, preventing freezing until the whole body reaches 4°C. In contrast, in a refrigerator, freezing happens from all outer surfaces inward. The humming noise during freezing is caused by the refrigerator's operation - while a gentle humming is normal, excessive noise typically indicates either a wearing fan motor or ice buildup interfering with fan blades. This ice buildup can occur when the defrost system malfunctions due to issues like a burned-out heater, faulty sensor, or clogged defrost drain.","context":["|frozen ice in cup|\nWe expected a flat surface as one would see on the surface of a frozen lake. The questions we had were 1) Why did this happen? 2) How can we make a flat surface from the process?\nThe difference from the freezing of water in a ceramic mug in a freezer and that in a lake is that the mug was exposed to -20F environment from all directions, while the lake water got cold only from top surface. So the freezing process should be different.\nIn a lake the freezing process is best illustrated by the following figures:\nSince cold water at 4C has smallest volume and highest density, it will sink to the bottom, so the body of lake water can not freeze until the whole body of water reached 4C - not more natural convection from freezing water. At this time as surface water continue to cool to 0C, the freezing water density will decrease, and the surface will continue to cool down without sinking. So in a lake the ice forms at the surface and moving downward. The reason we typically not seeing bulging ice could be a) the lake bed will deform due to the increase of the volume of the freezing process; 2) formed ice can tolerate small volume increase from water right beneath it turning to ice.\nFreezing in the mug in a freezer is quite different. It is expected that the water would start to freeze from the water in contact with the mug as well as the top of the water surface. So one would expect that the water would freeze from its outer surface inward. During the freezing process, there should be a stage when there is a layer of ice on the outer surface of the water in the mug and its inner body is still liquid. At this stage the ice surface should be flat - despite the fact that when water becomes ice it increases its volume because ice can tolerate small volume change. As more and more ice formed in the center, the volume change will be larger, as mug is much stiffer than ice, the only direction the increased volume can move is toward the top surface of the ice - which would crack to release the confinement to the center and allow the volume increase to occur.\nWe conducted a series tests to verify the reasoning, and we found that the frozen surface was flat at ~ 1 hour time, and there is a liquid center in the frozen water. When we left the mug in the freezer for 2 hours or longer, the water in the mug was fully frozen and a bulge formed with evidence of cracking at the top surface - exactly as what was observed in the first test. As we figured out the reason we found the answer to our second question as well.\nWe also observed that the flat surface in a partially frozen mug was not exactly flat. There was a very shallow wave shape to the surface. Upon observation, it is believed to be caused by the steady humming (vibration) of the refrigerator.\nThis process enabled us one more time to remind Nicholas that science fair project is for learning how to conduct scientific research, and is to explore and find reasons for unexpected observations. Figuring out unexpected observations led to the solution of this small problem and many great scientific discoveries.\nNote: If you are observant, curious and tend to connect dots. You might ask why ice cube made in the freezer has flat surface. This becomes an interesting brain exercise in light of our observation.","Noises coming from a refrigerator aren’t always bad news, but you shouldn’t ignore them. That’s especially true with humming sounds, which can be easy to overlook. A gentle humming is normal for fridges, but it’s a problem if the sound is loud and continuous.\nA Samsung fridge will make excessive humming noises when one of its fan motors is wearing out. The fan might still work, but the noise indicates the motor will fail soon. Besides that, there’s likely an ice buildup getting in the way of spinning fan blades. That will happen when the defrost heater, thermostat, or sensor is faulty or when the defrost drain is clogged.\nThe following sections in this guide will help you understand the root causes of this problem and how you can solve them.\nWhy Is My Samsung Fridge Making A Humming Noise?\nIt’s normal for all refrigerators to generate a gentle humming noise as part of their regular operation. However, excessively loud and frequent humming noises from inside are caused by two problems: an issue with the fan or an ice buildup behind the compartment’s rear panel.\nHere’s a closer look at both of those possibilities:\n1. Bad Fan Motor\nYour Samsung refrigerator relies on several fans to distribute air and keep itself cool. Those fans fall into two categories: evaporator and condenser fans.\nEvaporator fans are located internally, and their purpose is to distribute cold air throughout the appliance’s compartments.\nMeanwhile, the condenser fan is external and blows air through the hot condenser coils. Doing so helps the refrigerator get rid of heat to continue cooling the compartments from within.\nBoth kinds of fans consist of an electric motor that spins fan blades, and those parts work almost silently. Unfortunately, the fan motor can cause excessively loud humming noise when defective or worn out.\nAs long as the fans still turn, they’ll help the refrigerator function correctly. But the noise will cause discomfort, so you’ll still want to troubleshoot and fix the problem.\nThe Solution: Firstly, you must identify the fan humming louder than it should. Keep in mind that there could be more than one fan affected simultaneously, so be sure to listen to them carefully.\nNext, the fan must be inspected to ensure the problem is the motor and not an ice buildup (which you’ll read about later). If the motor is to blame, it’s best to replace it with a new one.\nConnect with an Appliance Repair Tech\nClick here to use the chatbox to speak with one of our technicians.\nNo in-home service calls. No appointments.\nEven though the fan motor might still run, the loud humming suggests it will stop working sometime soon.\n2. Burned Out Defrost Heater\nAs you read earlier, a bad fan motor is the first explanation for your Samsung fridge humming loudly. The second explanation is a significant buildup of ice inside the unit.\nThat buildup will grow larger until it gets in the way of the evaporator fan blades. When that happens, the spinning fan blades will hit the ice and cause the loud humming noise you’re hearing.\nStill, you must remember that the ice buildup is a symptom, not the root cause. Instead, several root causes exist, starting with a burned-out defrost heater.\nThe defrost heater is concealed behind the fridge compartment’s rear panel. The appliance activates the heater several times a day to melt away any frost buildup before it can turn into solid ice.\nHowever, a burned-out defrost heater won’t generate any heat. As a result, an ice buildup will gradually form behind that same panel. As described above, it’ll continue growing until the ice reaches the evaporator fans.\nThe Solution: Diagnosing a burned-out defrost heater is straightforward. You only need to look at it to see if there are any burn marks. Unfortunately, the challenge is accessing the heater concealed behind the refrigerator’s rear panel.\nThe ice buildup might prevent you from removing the panel. If that’s the case, you must shut the fridge off and give it time to defrost naturally until the panel is removable.\nOnce you get the panel off, you can detach the heater and inspect it closely for burn marks. When you confirm that it’s no longer functional, you can purchase a replacement to take its place.\nA failed heater might not seem like such a big deal, given that it doesn’t affect the appliance’s cooling. But if you don’t replace it quickly, the ice buildup will return, and you’ll experience more problems again.\n3. Faulty Defrost Sensor or Thermostat\nSuppose you check the defrost heater and find nothing wrong with it. In that case, the ice buildup likely happened because the refrigerator wasn’t turning the heater on to begin with.\nThat can happen due to a faulty defrost sensor or thermostat. Both those components work closely to control how the defrost heater functions.\nFor example, when they sense that the temperature is too low near the evaporator (i.e. there’s ice building up nearby), they’ll trigger the defrost heater to turn on and melt away any minor buildups.\nThen, if the defrost heater becomes too hot, the same components will shut it off to prevent overheating.\nThe Solution: The defrost thermostat and sensor are parts you cannot service or repair, so you must replace them if they’ve gone bad. But that replacement is the easy part.\nThe challenging bit is knowing whether or not either component has failed. You can do that by testing them for electrical continuity with a\n4. Frozen Defrost Drain\nEven if all defrost components are functioning correctly, an ice buildup can still form if the defrost drain is frozen. That drain is at the bottom of the compartment, collecting all water that melts due to the defrosting cycles.\nUnfortunately, that drain is prone to freezing over itself. When that happens, the excess water accumulates around it and turns back into ice.\nAs you can imagine, that ice buildup worsens with time until one of the evaporator fans is affected. Again, that will cause the fans to create a loud humming noise as its blades hit the ice continuously.\nThe Solution: Firstly, you must melt the ice trapped inside the defrost drain. You can do that by gently pouring hot water around the drain to raise its temperature and clear a way for the water to flow through.\nIf that doesn’t help, you must shut the fridge off and open its doors. That’s a slower but more thorough way to defrost all ice buildups inside the appliance.\nThen, you must clean away any dirt and debris trapped inside the drain. A small brush will be handy for that purpose.\nRemember that your refrigerator will make some humming noises throughout the day. However, you shouldn’t be able to hear those noises when you’re away from the unit. Nevertheless, excessive humming should never be ignored, as it’s a clear sign of a problem.\nThe first likely reason is that a fan motor is wearing out and must be replaced soon. Besides that, the problem is likely caused by an ice buildup getting in the way of spinning fan blades.\nThe ice builds up when the defrost system can’t work correctly due to a burned-out heater, faulty sensor or thermostat, or a frozen and clogged defrost drain."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:dd79a333-b484-443e-b5f2-0c3e0b5c72f7>","<urn:uuid:96eba891-7f5b-4b19-a186-f83f5bbf152e>"],"error":null}
{"question":"Could you explain how the breathing-related symptoms compare between Griffon Bruxellois dogs with BOAS (Brachycephalic Obstructive Airway Syndrome) and children with Chiari malformation type I? I'm trying to understand the respiratory challenges in both conditions!","answer":"Both conditions can cause breathing difficulties, but they manifest differently. In Griffon Bruxellois dogs with BOAS, the breathing problems are caused by their short nose and wide round head (brachycephalic features), resulting in noisy breathing during moderate exercise. For children with Chiari malformation type I, the breathing issues typically manifest as periods of not breathing during sleep (sleep apnea), which may require sleep studies for monitoring. In Griffons, dogs with noisy breathing should not be bred to avoid passing the condition to puppies, while in children with Chiari malformation, sleep apnea is monitored and may require additional treatment.","context":["Breeding Griffon Bruxellois is for the knowledgeable breeder. There are health issues to breed out and structural concerns to keep the breed away from. We’ve asked Jessica Darkle from the Griffon Bruxellois Club of Great Britain to write this introduction to Brussels Griffon breeding.\nA small, square, solid dog with bags of character and personality. They are monkeyish in appearance with a short muzzle, lustrous eyes and an intelligent expression. They are very comical and like to play the clown if they can. They are extremely loyal and affectionate to their family and some may be a little reserved with strangers.\nThe First World War and Second World War proved to be a disastrous time for the Griffon Bruxellois breed. War time is difficult on any dog breed, and the recovering numbers after the First World War were set back by increased vigilance in breeding away from faults such as webbed toes.\nBy the end of the Second World War, Belgium had almost no native Griffon Bruxellois left, and it was only through the vigilance of dedicated breeders (in the U.K. particularly) that the breed survived at all.Wikipedia\nWhen we talk about the Griffon, we must establish the differences between the Brussels Griffon (Griffon Bruxellois), the Belgian Griffon (Griffon Belge), and the Petit Brabançon.\nIn the UK we recognize only 2 varieties — the Griffon Belge is only a separate variety in other countries such as the United States.\nBoth the Griffon Bruxellois and Petit Brabançon are identical in the head, body shape, and size and come in 3 recognized colors. Red (various shades), Black and Tan and Black. The Griffon Bruxellois has a rough coat and grows a beard and mustache. The Petit Brabançon is smooth coated on the head and body and therefore much more easy to care for.\nBoth varieties are equally lovable and easy to live with.\nGrooming Of Griffon Dogs\nThe Griffon Bruxellois has a rough coat and grows a beard and mustache. It needs to be hand stripped or clipped to keep the coat tidy. Hand-stripping is quite hard work but can be mastered by most people. This is how the dog’s coats are prepared for the show ring. Hand-stripping helps to maintain the harshness and color of the coat. Or you can use a professional dog groomer.\nSome people just clip the dog’s coat but you loose some of the typical appearances if you do this. In between stripping the dogs should be combed through 2-3 times a week to remove dead coat and untangle the beard and mustache, which are longer than the coat on the body.\nThe Petit Brabançon is smooth coated on the head and body and therefore much more easy to care for. It needs only minimal grooming. Brushing with a soft bristle brush or hound glove will be enough to remove dead hair and keep a healthy shine on the coat.\nThe dogs need not be bathed unless they have managed to find something horrid to roll in.\nSome dogs get a skin fold over the nose as they mature, you will need to clean in between the folds on a daily basis with something that does not irritate the skin or eyes. Baby wipes can be good for this.\nEars should be cleaned with a proprietary dog ear cleaner when necessary.\nTeeth can be brushed with a small toothbrush and canine toothpaste.\nNails should be trimmed once or twice a month. Less, if they do wear down naturally with walking on a hard surface.\nHealth Concerns When Breeding Griffon Bruxellois\nThe Griffon is a tough, healthy little dog. But once they reach adulthood, like all dogs, they can suffer from a number of problems.\nSyringomyelia is a chronic progressive disease in which longitudinal cavities form in the cervical region of the spinal cord. This characteristically results in wasting of the muscles in the hands and a loss of sensation.\nAll dogs used for breeding must be MRI scanned and KC/BVA graded to determine whether or not they have Syringomyelia before they are bred from. All breeders must follow the published guidelines before mating two individuals.\nSyringomyelia is a neurological problem that is seen in a number of Toy breeds. The Club has been actively participating in research to try to eliminate this disease.\nThe Griffon can suffer from Patellar luxation (slipping kneecaps), this complaint is common to many small breeds.\nThe Griffon club has a Patella grading scheme. Dogs are graded by a vet from 0 (normal knees) to 4 (permanently dislocated knees). Dogs graded 3 and 4 should not be bred from unless the dislocation is due to injury only.\nAll Griffons should be eye tested for the presence of inherited and congenital eye disease. They should be tested before being bred and then at least every 3 years. There is a KC/BVA scheme that all breeders should take part in. Always ask to see the parent’s certificates before you buy a puppy.\nBrachycephalic Obstructive Airway Syndrome (BOAS)\nThe Griffon is a brachycephalic breed which means it has a short nose and a wide round head. This can cause breathing problems if too extreme.\nAll Griffons should be able to tolerate moderate exercise without having noisy breathing. Dogs with noisy breathing should not be bred from as they will pass this on to their puppies.\nUsing Inbreeding and Linebreeding\nInbreeding is something that should only be done by or on the advice of experienced breeders. Inbreeding can concentrate on the good things as well as the bad. Most good breeders will know all the dogs for many generations behind their dogs.\nExperienced Griffon breeders will know where it is safe to have a relative appearing on multiple occasions in a pedigree and where it is not. Very close inbreeding may be used occasionally to try to achieve a specific aim.\nMany British breeders will use a tool called Mate Select to work out the coefficient of inbreeding for any given mating and means that everyone can get an accurate measure of inbreeding. This helps them to be aware of the risks as well as the benefits.\nBreeding Practices When Working With Griffons\nMost breeders do take part in some health testing. If you are interested in a puppy the level of health testing should be one of the first questions you ask. All good breeders will be happy to show you their certificates and discuss any concerns you have. If they do not, simply do not buy a puppy from them, especially a puppy or dog that will be part of your breeding stock.\nThe Griffon is not a breed that can be puppy farmed so there are relatively few breeders compared to most Toy breeds. The majority of Griffon breeders rear their puppies in the home under good conditions. Always visit the puppies at the breeder’s house and ask to see the puppies with their Mother.\nIn the United Kingdom, the Kennel Club Accredited Breeder Scheme sets out minimum standards for breeding Griffons and the rearing of puppies, breeders who belong to this scheme have been checked by the Kennel Club as reaching these standards. These breeders also have to give full documentation and information with every puppy they sell, they have to undertake lifelong support for the new owner.\nThere are also many good breeders who are not members of any scheme.\nAlso, expect to be interrogated by the breeder. All good breeders take the utmost care to ensure that you can provide a suitable home and that you and the puppy will be the best possible match before letting one of their precious puppies go to a new home.\nPlease do not be offended by any question breeders may ask you, they are just trying to make sure that the puppy will have a permanent loving home.\nThe Griffon is a specialist breed. It is not a breed for the inexperienced. Take the time to learn as much as possible before embarking on breeding a litter.\nLitter sizes range from 1-6 puppies and need a lot of care for the first few weeks. It is not possible to breed them for profit.\nMake use of the combined knowledge of the breed clubs who are always happy to help and advise and make sure you use all the Kennel club’s “Mate Select” tools.\nKeep the health of the breed to the fore at all times and always put your dogs first.\nThe breed clubs are working hard to find a way to eliminate any health problems from the breed. Breeders are becoming more open to and aware of new technology and ways of breeding Griffon Bruxellois. I hope that in 20 years the Griffon will be just as popular and even more healthy.","What is a Chiari malformation?\nA Chiari malformation is a problem in which a part of the brain at the rear of the skull bulges through a normal opening (foramen magnum) in the skull where it joins the spinal canal. This puts pressure on parts of the brain and spinal cord, and can cause mild to severe symptoms. In most cases, the problem is present at birth (congenital).\nThere are several types of Chiari malformations, but type I (CM-I) is the most common type. In type I, part of the cerebellum bulges through the foramen magnum. This type is most often congenital (also called primary CM-I), but is often not found until a child is a teen or young adult. In rare cases, this type may also develop later in life. This is known as acquired or secondary CM-I.\nWhat causes a Chiari malformation type I?\nThe exact cause of a congenital Chiari malformation type I is not known. A problem during fetal growth may cause the defect. It may be caused by contact with harmful substances during pregnancy. Or it may be linked with genetic problems that run in families.\nAn acquired Chiari malformation type I happens to a person after birth. It is caused by excess leaking of spinal fluid from the lower back (lumbar) or chest (thoracic) areas of the spine. This can happen because of an injury, contact with harmful substances, or an infection.\nWhat are the symptoms of a Chiari malformation type I?\nYour child may not have any symptoms. Or symptoms may develop slowly over time. Most children don't have symptoms until they are teens or young adults.\nThe most common symptoms are headaches or pain in the back of the head or neck. The headaches and pain are made worse by coughing, laughing, or sneezing.\nYour child may also have other symptoms of a Chiari malformation type I, such as:\nHoarseness or trouble speaking\nRapid, back and forth eye movements (nystagmus)\nPeriods of not breathing during sleep (sleep apnea)\nWeakness or abnormal movements\nTrouble with balance\nAbnormal shape of the spine (scoliosis)\nYour child may also have a pocket of fluid in the spinal cord or brain stem. This is called a syrinx. A syrinx can cause trouble walking or pain in the arms or legs.\nHow is a Chiari malformation type I diagnosed?\nIn a child with no symptoms, the defect may be found when imaging tests are done for other reasons. For a child with symptoms, the health care provider will ask about your child's medical history and give your child a physical exam. He or she may refer your child to a specialist.\nImaging tests are done to detect a Chiari malformation type I. Your child may have 1 or more of these tests:\nMRI. This test is the one most often used to diagnose Chiari malformations. It uses large magnets and a computer to make detailed pictures of the inside of the body.\nCT scan. This test uses a series of X-rays and a computer to create detailed pictures of the inside of the body. A CT scan is more detailed than a regular X-ray.\nHow is a Chiari malformation type I treated?\nYour child may be treated by neurologists and neurosurgeons. These are experts in brain and spinal cord problems. Treatment will depend on your child’s symptoms, age, and general health. It will also depend on how severe the condition is.\nWith no symptoms. Your child’s health may be watched closely. This may include frequent physical exams and MRI tests.\nWith symptoms. Your child's health care provider may prescribe medicines to reduce pain. Or he or she may advise decompression surgery. This is done to relieve pressure on the brain, or to restore the flow of spinal fluid.\nWith few or no symptoms, but a syrinx. Your child's health care provider may suggest close monitoring of the defect with a special type of MRI called cine phase contrast. This helps look for blocked spinal fluid flow. Your child may need surgery, based on the MRI results or if symptoms get worse.\nWith signs of sleep apnea. Your child may need a sleep study. In this test, your child will be monitored during sleep to look for problems. A sleep study can also help the health care provider decide on additional treatment.\nWhat are the complications of a Chiari malformation type I?\nComplications of a Chiari malformation type I can include:\nDevelopment of syrinx\nPermanent damage to muscles or nerves\nCarefully watching for changes in your child’s health can help prevent complications. This helps to make sure that treatment is done early.\nHow is a Chiari malformation type I managed?\nIt is hard for health care providers to predict how a Chiari malformation type I will affect a child’s long-term health. Your child may not have any changes caused by the defect. Or he or she may have worsening neurological problems. Your child’s health will be closely watched. This will include with frequent physical exams and imaging tests such as MRI. There is ongoing research as to how to best manage Chiari malformations.\nWhen should I call my child's health care provider?\nCall your child's health care provider if you notice any changes in your child. Be sure to call if you notice problems with:\nWalking or moving\nKey points about Chiari malformation type I\nWith a Chiari malformation, the lower part of the brain dips down through a normal opening at the bottom of the skull. In some cases, more brain tissue also dips down through this opening.\nIn most cases, the problem is present at birth (congenital).\nThere are several types of Chiari malformations. Type I is the most common type.\nYour child may not have symptoms. If symptoms occur, the most common ones are headaches or pain in the back of the head or neck. The headaches and pain are made worse by coughing, laughing, or sneezing.\nYour child may also have a pocket of fluid in the spinal cord or brain stem. This is called a syrinx.\nImaging tests are done to detect a Chiari malformation type I. Your child may have an MRI or a CT scan."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:122b1ffa-8844-4847-96be-20aff9cf1c3b>","<urn:uuid:ea61f1f2-749f-4232-aa5a-b62f58c5015b>"],"error":null}
{"question":"Je suis interessé in logic theory - what's the connection entre historical logic developments et modern computer applications?","answer":"The historical development of logic has directly shaped modern computing. Aristotle first established bivalent (binary) logic, which influenced thought through the Enlightenment. In the 19th century, George Boole developed algebraic structures based on binary logic. Later, Jan Łukasiewicz developed three-valued logic in 1920, and Emil Post introduced additional truth degrees in 1921. These foundations became crucial for computer science - the first programming language was essentially created by logician Gottlob Frege, and theoretical computer science is based on concepts from logicians like Alonzo Church and Alan Turing. Today, logic continues to be fundamental in artificial intelligence, software engineering, and the semantic web.","context":["In logic, a finite-valued logic (also finitely many-valued logic) is a propositional calculus in which truth values are discrete. Traditionally, in Aristotle's logic, the bivalent logic, also known as binary logic was the norm, as the law of the excluded middle precluded more than two possible values (i.e., \"true\" and \"false\") for any proposition. Modern three-valued logic (ternary logic) allows for an additional possible truth value (i.e. \"undecided\").\nThe term finitely many-valued logic is typically used to describe many-valued logic having three or more, but not infinite, truth values. The term finite-valued logic encompasses both finitely many-valued logic and bivalent logic. Fuzzy logics, which allow for degrees of values between \"true\" and \"false\"), are typically not considered forms of finite-valued logic. However, finite-valued logic can be applied in Boolean-valued modeling, description logics, and defuzzification of fuzzy logic. A finite-valued logic is decidable (sure to determine outcomes of the logic when it is applied to propositions) if and only if it has a computational semantics.\nAristotle's collected works regarding logic, known as the Organon, describe bivalent logic primarily, though Aristotle's views may have allowed for propositions that are not actually true or false. The Organon influenced philosophers and mathematicians throughout the Enlightenment. George Boole developed an algebraic structure and an algorithmic probability theory based on bivalent logic in the 19th century.\nJan Łukasiewicz developed a system of three-valued logic in 1920. Emil Leon Post introduced further truth degrees in 1921.\nStephen Cole Kleene and Ulrich Blau expanded the three-valued logic system of Łukasiewicz, for computer applications and for natural language analyses, respectively. Nuel Belnap and J. Michael Dunn developed a four-valued logic for computer applications in 1977. Since the mid-1970s, various procedures for providing arbitrary finite-valued logics have been developed.\nIn linguistics, finite-valued logic is used to treat presuppositions as product systems with ordered pairs of truth degrees, or truth tables. This enables assumptions built into verbal or written statements to be associated with varying degrees of truth values in the course of natural-language processing.\nIn the study of formal languages, finite-valued logic has shown that encapsulating a truth predicate in a language can render the language inconsistent. Saul Kripke has built on work pioneered by Alfred Tarski to demonstrate that such a truth predicate can be modeled using three-valued logic.\nPhilosophical questions, including the Sorites paradox, have been considered based on a finite-valued logic known as fuzzy plurivaluationism. The Sorites paradox suggests that if adding a grain of sand to something that is not a heap cannot create a heap, then a heap of sand cannot be created. A logical model of a heap in which there are as many truth degrees as grains of sand tends to refute that suggestion.\nIn electronics design, a logical model of the stable states of a circuit, in which there are as many truth degrees as there are states, serves as a model for finite-valued switching. Three-valued operators can be realized in integrated circuits.\nIn fuzzy logic, typically applied for approximate reasoning, a finitely-valued logic can represent propositions that may acquire values within a finite set.\nIn mathematics, logical matrices having multiple truth degrees are used to model systems of axioms.\nBiophysical indications suggest that in the brain, synaptic charge injections occur in finite steps, and that neuron arrangements can be modeled based on the probability distribution of a finitely valued random variable.\nIn the study of logic itself, finite-valued logic has served as an aid to understand the nature and existence of infinite-valued logic. Kurt Gödel attempted to comprehend the human ability for logical intuition in terms of finite-valued logic before concluding that the ability is based on infinite-valued logic.","Logic in computer science\n- Theoretical foundations and analysis\n- Use of computer technology to aid logicians\n- Use of concepts from logic for computer applications\nTheoretical foundations and analysis\nThe most essential foundations for computer science are grounded in logic and set theory. The logician Gottlob Frege who defined the first propositional calculus essentially created the first programming language. The language he defined has all the formal requirements for a powerful computer programming and specification language. The theory of computation is based on concepts defined by logicians and mathematicians such as Alonzo Church and Alan Turing. In addition some other major areas of theoretical overlap between logic and computer science are:\n- Godel's incompleteness theorem proves that any logical system powerful enough to characterize arithmetic will contain statements that can neither be proven true nor false within that system. This has direct application to theoretical issues relating to the feasibility of proving the completeness and correctness of software.\n- The Frame problem is a basic problem which must be overcome when using first order logic to represent the goals and state of an artificial intelligence agent.\n- The Curry-Howard correspondence is a proof about the relation between logical systems and software. This theory established the theoretical foundation for viewing a computer program as a formal logical statement that could be proven to be correct and consistent.\nComputers to Assist Logicians\nOne of the first applications to use the term Artificial Intelligence was the Logic Theorist system developed by Allen Newell, J.C. Shaw, and Herbert Simon in 1956. One of the things that a Logician does is to take a set of statements in Logic and deduce the conclusions (additional statements) that must be true by the laws of logic. For example If given a logical system that states \"All humans are mortal\" and \"Socrates is human\" a valid conclusion is \"Socrates is mortal\". Of course this is a trivial example. In actual logical systems the statements can be numerous and complex. It was realized early on that this kind of analysis could be significantly aided by the use of computers. The Logic Theorist validated the theoretical work of Bertrand Russell and Alfred North Whitehead in their influential work on mathematical logic called Principia Mathematica. In addition subsequent systems have been utilized by logicians to validate and discover new logical theorems and proofs.\nLogic applications for computers\nThere has always been a strong influence from mathematical logic on the field of Artificial Intelligence (AI). From the beginning of the field it was realized that technology to automate logical inferences could have great potential to solve problems and draw conclusions from facts. Ron Brachman has described First Order Logic (FOL) as metric by which all AI knowledge representation formalism should be evaluated. There is no more general or powerful known method for describing and analyzing information than FOL. The reason FOL itself is simply not used as a computer language is that it is actually too expressive, in the sense that FOL can easily express statements that no computer, no matter how powerful, could ever solve. For this reason every form of knowledge representation is in some sense a trade off between expressivity and computability. The more expressive the language is, the closer it is to FOL, the more likely it is to be slower and prone to an infinite loop.\nFor example, IF THEN rules used in Expert Systems are a very limited subset of FOL. Rather than arbitrary formulas with the full range of logical operators the starting point is simply what logicians refer to as Modus Ponens. As a result the computability of rule based systems can be quite good, especially if they take advantage of optimization algorithms and compilation.\nAnother major area of research for logical theory was software engineering. Research projects such as the Knowledge-Based Software Assistant and Programmer's Apprentice programs applied logical theory to validate the correctness of software specifications. They also used them to transform the specifications into efficient code on diverse platforms and to prove the equivalence between the implementation and the specification. This formal transformation driven approach is often far more effort than traditional software development. However, in specific domains with appropriate formalisms and reusable templates the approach has proven viable for commercial products. The appropriate domains are usually those such as weapons systems, security systems, and real time financial systems where failure of the system has excessively high human or financial cost. An example of such a domain is Very Large Scale Integrated (VLSI) Design—the process for designing the chips used for the CPU's and other critical components of digital devices. An error in a chip is catastrophic. Unlike software chips can't be patched or updated. As a result there is commercial justification for using formal methods to prove that the implementation corresponds to the specification.\nAnother important application of logic to computer technology has been in the area of Frame languages and automatic classifiers. Frame languages such as KL-ONE have a rigid semantics. Definitions in KL-ONE can be directly mapped to set theory and the predicate calculus. This allows specialized theorem provers called classifiers to analyze the various declarations between sets, subsets, and relations in a given model. In this way the model can be validated and any inconsistent definitions flagged. The classifier can also infer new information, for example define new sets based on existing information and change the definition of existing sets based on new data. The level of flexibility is ideal for handling the ever changing world of the Internet. Classifier technology is built on top of languages such as the Web Ontology Language to allow a logical semantic level on to the existing Internet. This layer of is called the Semantic web.\n- Lewis, Harry R.; Christos H. Papadimitriou (1981). Elements of the Theory of Computation. Englewood Cliffs, New Jersey: Prentice-Hall. ISBN 0-13-273417-6.\n- Davis, Martin. \"Influences of Mathematical Logic on Computer Science\". In Rolf Herken. The Universal Turing Machine. Springer Verlag. Retrieved 26 December 2013.\n- Hofstadter, Douglas R.. Gödel, Escher, Bach: An Eternal Golden Braid. Basic Books. ISBN 978-0465026562.\n- McCarthy, J; P.J. Hayes (1969). \"Some philosophical problems from the standpoint of artificial intelligence\". Machine Intelligence 4: 463–502.\n- Newell, Allen; J.C. Shaw and H.C. Simon (1963). \"Empirical explorations with the logic theory machine\". In Ed Feigenbaum. Computers and Thought. McGraw Hill. pp. 109–133. ISBN 978-0262560924.\n- Levesque, Hector; Ronald Brachman (1985). \"A Fundamental Tradeoff in Knowledge Representation and Reasoning\". In Ronald Brachman and Hector J. Levesque. Reading in Knowledge Representation. Morgan Kaufmann. p. 49. ISBN 0-934613-01-X.\nThe good news in reducing KR service to theorem proving is that we now have a very clear, very specific notion of what the KR system should do; the bad new is that it is also clear that the services can not be provided... deciding whether or not a sentence in FOL is a theorem... is unsolvable.\n- Forgy, Charles (1982). \"Rete: A Fast Algorithm for the Many Pattern/Many Object Pattern Match Problem*\" (PDF). Artificial Intelligence 19: 17–37. doi:10.1016/0004-3702(82)90020-0. Retrieved 25 December 2013.\n- Rich, Charles; Richard C. Waters (November 1987). \"The Programmer's Apprentice Project: A Research Overview\" (PDF). IEE Expert Special Issue on the Interactions between Expert Systems and Software Engineering. Retrieved 26 December 2013.\n- Stavridou, Victoria (1993). Formal Methods in Circuit Design. Press Syndicate of the University of Cambridge. ISBN 0-521-443369. Retrieved 26 December 2013.\n- MacGregor, Robert (June 1991). \"Using a description classifier to enhance knowledge representation\". IEEE Expert 6 (3). Retrieved 10 November 2013.\n- Berners-Lee, Tim; James Hendler and Ora Lassila (May 17, 2001). \"The Semantic Web A new form of Web content that is meaningful to computers will unleash a revolution of new possibilities\". Scientific American.\n- Colin Stirling (1992). \"Modal and Temporal Logics\". In S. Abramsky, D. M. Gabbay, T. S. E. Maibaum, editors,. Handbook of Logic in Computer Science II. Oxford University Press. pp. 477–563. ISBN 0-19-853761-1.\n- Ben-Ari, Mordechai (2003). Mathematical Logic for Computer Science (2nd ed.). Springer-Verlag. ISBN 1-85233-319-7.\n- Huth, Michael; Ryan, Mark (2004). Logic in Computer Science: Modelling and Reasoning about Systems (2nd ed.). Cambridge University Press. ISBN 0-521-54310-X.\n- Burris, Stanley N. (1997). Logic for Mathematics and Computer Science. Prentice Hall. ISBN 0-13-285974-2.\n- Article on Logic and Artificial Intelligence at the Stanford Encyclopedia of Philosophy.\n- IEEE Symposium on Logic in Computer Science (LICS)\n- Alwen Tiu, Introduction to logic video recording of a lecture at ANU Logic Summer School '09 (aimed mostly at computer scientists)"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:e7bd9f8a-2c6f-491e-89ca-e23407d74745>","<urn:uuid:402bf2d1-b382-422e-b345-1e39ec2e36eb>"],"error":null}
{"question":"What economic value do properly maintained trees provide, and what are the risks of improper maintenance techniques like topping?","answer":"Properly maintained trees can increase property values by 5-20% compared to non-landscaped homes and add considerable value as an investment when well-cared for. However, improper maintenance techniques like topping create serious risks - it stresses trees, causes decay, creates hazards, and makes trees ugly. Topping is considered perhaps the most harmful tree pruning practice known, as it involves indiscriminately cutting branches to stubs or lateral branches too small to assume the terminal role. While often done to reduce tree size, topping actually makes trees more hazardous in the long term.","context":["Overstory #182 - Remember to touch trees\nSince 1998, Dr. Alex L. Shigo has contributed four articles to The Overstory (editions 68, 69, 70, and 132). Earlier this month, Dr. Shigo passed away at the age of 75. Dr. Shigo led a revolution in the way we think about arboriculture. The following is a selection of passages from Dr. Shigo's prolific writings. See \"About the Author\" below for a brief biography of Dr. Shigo and for information about purchasing his publications.\nfrom Tree Basics (p. 4)\nA brief overview of some unique features of trees\nTrees are the tallest, most massive, longest-lived organisms ever to grow on earth.\nTrees, like other plants, cannot move. However, trees, unlike other plants, are big, woody, and perennial, which means they are easy targets for constant wounding.\nTrees are super survivors mainly because they grow in ways that give them defense systems that are highly effective against infections from wounds.\nTrees have the capacity to adjust rapidly to changes that threaten their survival.\nAnimals move to get food, water, and shelter. They move to avoid destructive agents. When animals are injured and infected, processes of restoration and repair start. Animals heal after wounding.\nWhen trees are injured and infected, processes of boundary formation start. Trees do not restore or repair wood that is injured and infected. In this sense, trees do not heal. Instead, trees compartmentalize wound infections.\nCompartmentalization is the tree's defense process after injuries where boundaries form that resist the spread of infections. The boundaries also protect systems involving water, air, energy storage, and mechanical support. In a sense, the boundaries are like an inside bark.\nTree Basics (p. 30)\nTrees provide their associates with food, water, shelter, and home, nesting, and roosting sites.\nHere are some of the benefits the associates provide for trees\n- Facilitate absorption of water and elements - fungi, (mycorrhizae).\n- Break down organic and inorganic materials - bacteria, fungi, nsects, animals.\n- Aerate soils - worms, insects, fungi, animals.\n- Fertilize - droppings from worms, insects, and other animals.\n- Detoxify harmful substances - bacteria and fungi.\n- Help adjust pH - bacteria, fungi.\n- Convert nitrogen in air to a usable form (fix nitrogen) - bacteria and actinomycetes.\n- Protect roots against pathogens - bacteria, fungi, (mycorrhizae).\n- Hold water - actinomycetes, bacteria, (cell coatings).\n- Regulate slow-release fertilizers - bacteria.\n- Resist decay - anaerobic bacteria (wetwood), non-decay-causing fungi (discolored wood).\n- Disseminate seeds - birds, animals, insects.\n- Pollinate flowers - insects, animals, especially birds and bats.\n- Facilitate branch shedding - rot-causing fungi.\n- Protection against wound infection by decay-causing fungi - bacteria, non-decay-causing fungi.\nfrom A New Tree Biology (p. 161)\nSurvival means to remain alive under conditions that have the potential to kill. Trees, as we know them today have been evolving on this earth for over 200 million years. They have survived the killing forces of countless pathogens and the ravages of environmental extremes. Somehow, some trees have remained alive under all types of conditions that had the potential to kill. But, trees did not accomplish this by acting as individuals. Trees connected or interacted with a great number of other living things, and together they survived. The power of interactions and connections kept many living things alive. Indeed, trees evolved in groups. They had group protection and group defense. They were protected and defended by their neighbors and associates, and trees protected and defended them. A tight circle or web of connections was the way the individuals within the group survived.\nNow the connections are being broken. The heart of the survival system is being threatened. The most deadly words to the survival system are \"suddenly and repeated.\" Given enough time most members of the large circle adapted to adverse conditions. But, when adverse conditions repeat faster than adaptation can occur, then the entire system is threatened.\nTrees never knew a stump until axes and saws came into the forest. Trees never knew complete removal of trunks, machine compaction of soils, sudden changes in water drainage patterns due to roads, pollution, and disruption of niches for soil organisms. The list goes on. These actions have come suddenly. They are being repeated.\nfrom A New Tree Biology (p. 458)\nA hundred times, at least, I have heard it said, \"....the client wants their trees topped, and we (the arborist talking) give them what they want and what they will pay for. We must make a profit, or we will not be in business long. And, believe me, the next company that comes along will take the topping job.\" A dilemma begins to happen. If the arborist does not do the requested topping, somebody else will and the trees still will be injured, so the first arborist often feels that he might as well do the injurious job and get the profit. Some arborists will refuse to top trees. I accept the fact that there is a dilemma. And, I believe that like wound dressings, topping will never go away, but I also believe that there are ways to greatly reduce this injurious practice and still make profit, or even more profit.\nFirst, some order is needed on the subject. Let us not confuse topping-the internodal removal of a leader trunk - with early pruning or training of young trees, pollarding, bonsai pruning, crown reduction by cutting at crotches (many names for this drop crotch pruning, dehorning, etc.). Topping is done internodal; proper crown reduction is done at nodes, or at crotches. So the first separation must be nodes - good, internodes - bad.\nfrom 5 minute Tree Care (p. 1)\nFor all people who care about trees, but do not have the time to read long articles.\nCORRECT 5 simple primary tree problems and you will prevent many costly secondary problems caused by insects, diseases, heat and cold extremes, and drought. You will also reduce the chances of your trees becoming hazards.\nFive major problems and their solutions\n- SELECT HEALTHY TREES. Do not buy or plant trees that have roots crushed or crowded in a bag or container.\n- PLANT PROPERLY Do not plant too deep.\n- PLANT THE RIGHT TREE IN THE RIGHT PLACE. Do not plant large-maturing trees near buildings or power lines.\n- PRUNE BRANCHES CORRECTLY. Do not remove branch collars or leave stubs.\n- PRUNE TREES CORRECTLY. Do not top trees.\nfrom Modern Aboriculture (p. xvi)\nModern arboriculture means\n- The right tree in the right place.\n- Building designs that give trees space to grow.\n- Beautiful trees growing in clusters.\n- Healthy trees growing below grade.\n- Young trees with space to grow and with proper early pruning.\n- The target is removed, not the tree.\n- No sprouts from a correct pruning cut.\n- Early training regulates size and shape of trees.\n- The sidewalk is cut, not the tree or its roots.\n- Proper care for old trees, and respect for their dignity.\n- Planting trees at the proper depth.\n- People touching trees and learning how they work, before they work on them.\n- Treatments that destroy defense systems must be stopped.\n- Treatments that cause serious internal injuries must be stopped.\n- Treatments that start other problems must be stopped.\n- Treatments that injure and kill transplanted trees must be stopped.\nfrom A New Tree Biology Dictionary (p. 26)\nConcentrations and survival\nWhat keeps you alive, can kill you. No water or no salt will kill you, and too much water or too much salt will kill you. It is not water and salt that are essential for survival. It is the proper concentrations of each that are essential. Concentrations of all the factors essential for survival are always changing in nature. Constant adaptation is needed to survive in such an environment. The static state does not exist, and what may be good today could be bad tomorrow for a tree. A cavity full of water will be bad for the decay-causing organisms and good for the tree. As the water evaporates, a point of wood moisture will be reached that will be very good for the wood-decaying organisms. Then they will grow rapidly, within the boundaries set by the tree. When the moisture concentration falls below a certain level, the wood-decaying organisms do not grow further. Moisture, temperature, and all the essential elements are constantly changing, and conditions that are too extreme for best growth of the tree or the pathogens, are also always changing. Vibrations in concentrations of essential survival factors are ways natural systems constantly rid themselves of the weak individuals. We must be very careful not to disrupt the natural fluctuations by adding too much water, and too much fertilizer, or by disturbing the tree at a critical time in the vibration period. This is why we need to understand tree biology. Too many times our honest and loving attempts to help are really actions that hurt the plant.\nfrom New Tree Health (p. 1)\nTREES grow taller, live longer, and become more massive than any other living thing because trees are perennial woody plants.\nWOOD gives trees superior mechanical support, which is the trees' unique feature.\nROT destroys the trees' unique feature.\nPREVENT WOUNDS THAT LEAD TO ROT lawnmowers, cars, fire, construction, climbing spikes, improper pruning, topping, deep injection and implant holes, and the list goes on and on!\nShigo on Trees (back cover)\nTrees have dignity, too.\nThere comes a time when trees in cities, parks, and near homes should be removed and new ones planted.\nWhen possible, plant trees in groups or clusters.\nLEARN about trees and their associates so that you can help make better decisions for their long-term, high-quality survival.\nfrom Tree Hazards (p. 1)\nA tree hurts, too!\nMost tree hazards do not just happen. They are usually started by mistreatments by people. When a hazardous tree breaks, it may hurt not only people, but the tree hurts, too in the sense of wounds or even death.\nMOST TREE HAZARDS CAN BE PREVENTED by regular checkups and proper treatments by tree professionals - arborists.\nfrom Tree Hazards (p. 3)\nCAUTION! Before more trees and people are injured and killed, we must STOP doing some old injurious tree practices and START doing some new beneficial ones. The list is long.\nHere are a few examples\n- Removing tops of upright leader stems on big trees - TOPPING.\n- Removing tips of large branches on big trees - TIPPING.\n- Removing branch collars when pruning- FLUSH CUTTING.\n- Planting trees that grow big, under power lines or in small spaces.\n- Planting trees that have many low branches with tight crotches.\n- Wounding trees, especially during construction.\n- Crowding trees with roads, walkways, and buildings.\n- Planting the wrong tree in the wrong place.\n- Developing a tree hazard prevention plan with arborists.\n- Checking trees for health and safety at least once a year.\n- Consulting arborists for advice before construction starts.\n- Learning more about trees read A NEW TREE BIOLOGY.\n- Making decisions based on an understanding of tree biology.\n- Talking to elected officials about realistic tree support.\n- Recognizing early signs of problems; consult arborists.\nfrom 100 Tree Myths (various pages)\nOld arboriculture is based on the heartrot concept where the tree is considered a passive organism and that wood is dead. Modern arboriculture is based on the concept of compartmentalization where the tree is considered an active, responding organism, and that wood does have many living cells among the dead cells.\nMany trees tolerate injurious treatments. This does not mean that such treatments are good for trees.\nEngineers are straight lines. Biologists are circles. More round cluster plantings of trees are needed in our straight cities!\nSEE, not just look\nACT, not just wait\nLISTEN, not just hear\nTOUCH, not just watch\nFrom Modern Arboriculture (p. v)\nModern arboriculture is about the tree system How it grows, how it defends itself, and how it eventually dies. I hope you will give trees and their associates - the tree system - a fair chance. Learn about them. Touch them.\nThese passages were excerpted with the kind permission of Dr. Shigo's daughter and co-publisher Judy Shigo Smith from\nShigo, A.L. 1986. A New Tree Biology Dictionary. Shigo, A.L. 1989. A New Tree Biology, 2nd Ed. Shigo, A.L. 1991. Modern Arboriculture. Shigo, A.L. 1993. 100 Tree Myths. Shigo, A.L. undated. New Tree Health. Shigo, A.L. undated. 5 Minute Tree Care. Shigo, A.L. undated. Shigo on Trees. Shigo, A.L. undated. Tree Hazards.\nAll above publications are available from the publisher\nAbout the author\nAlex L. Shigo was chief scientist with the US Forest Service, and known by many as \"the father of modern arboriculture\". He is recognized internationally for the development of expanded interpretations of decay based on compartmentalization and microbial succession. His research includes over 15,000 longitudinal tree dissections with a chainsaw. He has published over 15 textbooks used in many universities worldwide, and hundreds of other publications. He received numerous honors and awards. Dr. Shigo passed away October 6, 2006.\nTo purchase Dr. Shigo's publications or more information contact\nRelated editions to The Overstory\n- The Overstory #181--Dispelling Misperceptions About Trees\n- The Overstory #144--How trees stand up\n- The Overstory #143--Dendrology\n- The Overstory #132--How Trees Survive\n- The Overstory #92--Trees and Their Energy Transactions\n- The Overstory #70--Troubles in the Rhizosphere\n- The Overstory #69--Some Tree Basics\n- The Overstory #68--Twelve Tree Myths","Why Hire An Arborist?\nAn arborist is a specialist in the care of individual trees. Arborists are knowledgeable about the needs of trees, and are trained and equipped to provide proper care. Hiring an arborist is a decision that should not be taken lightly. Proper tree care is an investment which can lead to substantial returns. Well-cared-for trees are attractive and can add considerable value to your property. Poorly maintained trees can be a significant liability. Pruning or removing trees, especially large trees, can be dangerous work. Tree work should only be done by those trained and equipped to work safely in trees.\nSelecting the Right Arborist for the Job\nThere are a variety of things to consider when selecting an arborist which include:\n- Membership in professional organizations such as the International Society of Arboriculture (ISA), the Kansas Arborist Association (KAA) and the Tree Care Industry Association (TCIA) demonstrates a willingness on the part of the arborist to stay up-to-date on the latest techniques and information.\n- Check in the phone directory yellow pages for those arborists who advertise as ISA Certified Arborists or display the official logo of the ISA Certified Arborist. Certified arborists are experienced professionals who have passed an extensive examination covering all aspects of tree care.\n- Ask for proof of insurance and then phone the insurance company if you are not satisfied. A reputable arborist will have personal and property damage insurance as well as workers compensation insurance. Many homeowners have had to pay out large amounts of money for damages caused by an uninsured individual claiming to be a tree expert. You could be held responsible for damages and injuries that occur as a result of the job.\n- Some governmental agencies require contractors to apply for permits and/or to apply for a license before they are able to work. Be sure they comply with any local, state, provincial or national law that governs their work.\n- Ask for references to find out where the company has done work similar to the work you are requesting. Don't hesitate to check references or visit other worksites where the company or individual has done tree work. Remember, tree care is a substantial, long-lasting investment; you would not buy a car without a test drive!\n- Be wary of individuals who go door-to-door and offer bargains for performing tree work. Most reputable companies are too occupied to solicit work in this manner. Improper tree care can take many years to correct itself and in some cases never corrects itself. Are you willing to take that risk with your valuable investment?\n- Good arborists will only perform accepted practices. For example, practices such as topping a tree, removing an excessive amount of live wood, using climbing spikes on trees which are not being removed, and removing or disfiguring living trees without just cause, are unnecessary.\n- Don't always accept the low bid. You should examine the credentials and the written specification of the firms who submitted bids and determine the best combination of price, work to be done, skill and professionalism to protect your substantial investment.\nWhat is a Certified Arborist?\nAn arborist by definition is an individual who is trained in the art and science of planting, caring for and maintaining individual trees. ISA Arborist Certification is a non-governmental, voluntary process by which individuals can document their base of knowledge. It operates without mandate of law and is an internal, self-regulating device administered by the International Society of Arboriculture. Certification provides a measurable assessment of an individual's knowledge and competence required to provide proper tree care.\n- Certification is not a measure of standards of practice. Certification can attest to the tree knowledge of an individual, but cannot guarantee or ensure quality performance.\n- Certified arborists are individuals who have achieved a level of knowledge in the art and science of tree care through at least three years experience and who have passed a comprehensive examination developed by some of the nation's leading experts on tree care. Certified arborists must also continue their education to maintain their certification. Therefore, they should be up-to-date on the latest techniques in arboriculture.What are the Benefits of Trees?\n- Most trees and shrubs in cities or communities are planted to provide beauty or shade. These are two excellent reasons for their use. Woody plants also serve many other purposes, and it often is helpful to consider these other functions when selecting a tree or shrub for the landscape. The benefits of trees can be grouped into the following catagories:\n- Communal Benefits. Trees bring natural elements and wildlife habitats into urban surroundings; all of which incerease the quality of life for residents of the community\n- Environmental Benefits. Trees alter the environment in which we live by moderating climate, improving air quality, conserving water, and harboring wildlife.\n- Economic Benefits. Property values of landscapted homes are 5-20% higher than those of non-landscaped homes.\nTree Hazard Checklist?\nConsider these questions:\n- Are there large dead branches in the tree?\n- Are there detached branches hanging in the tree?\n- Does the tree have cavities or rotten wood along the trunk or in major branches?\n- Are mushrooms present at the base of the tree?\n- Are there cracks or splits in the trunk or where branches are attached?\n- Have any branches fallen from the tree?\n- Has the trunk developed a strong lean?\n- Have the roots been broken off, injured, or damaged by lowering the soil level, installing pavement, repairing sidewalks, or digging trenches?\n- Has the site recently been changed by construction, raising the soil level, or installing lawns?\n- Has the tree been topped or otherwise heavily pruned?\n- Dead or Dying tree?\nWhy Topping Hurts Trees\nTopping is perhaps the most harmful tree pruning practice known. Topping is the indiscriminate cutting of tree branches to stubs or lateral branches that are not large enough to assume the terminal role. The most common reason given for topping is to reduce the size of a tree. Home owners often feel that their trees have become too large for their property. In fact, topping will make a tree more hazardous in the long term.\n- Topping stresses trees\n- Topping causes decay\n- Topping creates hazards\n- Topping makes trees ugly"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:bdb63ff5-16eb-4c95-8a9f-fa7f8a787c4d>","<urn:uuid:efb04c10-394d-46a7-99cf-5bedc3cd932b>"],"error":null}
{"question":"What were common reasons why Métis people enlisted in Canadian military during World War I and II, and what traditional skills made them successful as soldiers?","answer":"Métis people enlisted for various reasons including patriotism, escaping poverty from their road allowance or northern bush communities, and seeking adventure. Their traditional skills such as trapping, hunting, scouting and guiding were in high demand by the military and made them very successful soldiers.","context":["During the First World War, the Second World War and the Korean War, Métis from across the Métis Nation enlisted in the Canadian military in large numbers. Unfortunately, we don’t have exact enlistment figures for these Métis soldiers because they were not allowed to self-identify as Métis. This especially bothered Claude Adams of St. Louis, Sask., who was a member of the 1st Special Service Force, a joint Canadian-American force known colloquially as the Devil’s Brigade. In military records, soldiers were categorized as either Treaty Indians (First Nations) or by their European ethnicities, with no category for Métis.\nThe Métis enlisted in these wars for a variety of reasons, including patriotism. By enlisting, many escaped grinding poverty in their road allowance or northern bush communities, and many young enlisters were also following a sense of adventure. Traditional Métis skills such as trapping, hunting, scouting and guiding were in high demand by the military and made the Métis very successful soldiers. While their men went to war, many Métis women worked on traplines or did other work to support their families.\nMany Métis who volunteered during the First and Second World Wars were descendants of Métis who fought for Louis Riel and Gabriel Dumont in 1885. Gabriel Dumont’s nephews and great-nephews served, as did Louis Riel’s, including Louis Philippe Riel, a renowned sniper in the First World War who killed 30 Germans before he himself was killed on Jan. 15, 1916. Patrice Fleury, a Métis veteran of 1885, lost a son in the Second World War, at which point he wrote:\n. . . today there are no more staunch upholders of the rights of Canada than the Métis, and many are those of the younger generation who have laid down their lives for King and Country during the late war.\nLloyd Hamilton, the great-grandson (through adoption) of Gabriel Dumont, served in the Canadian Army during the Korean War. On one occasion, Hamilton worked with an American soldier to save 80 Korean children in an orphanage who were caught in the Korean Demilitarized Zone. For this action, Hamilton received a medal from the United Nations.\nFollowing the Second World War, many Métis veterans would become political leaders and activists, particularly within the Métis Society of Saskatchewan, founding Indian and Métis Friendship Centres and Legion branches. Métis veterans were often unable to obtain their veterans’ benefits after their military service, and their struggle for restitution continues to this day.\nVarious organizations within the Métis Nation built the National Métis Veterans Monument to honour Métis veterans from across the Métis Nation Homeland. Located in Batoche, Sask., the monument includes the names of almost all the Métis veterans who have served Canada until the present.\nThe Métis during the War of 1812\nDuring the War of 1812, Indigenous peoples, including the Métis, played key roles as allies of the British and Upper and Lower Canadians. Almost all Indigenous peoples in the Great Lakes basin and in Upper and Lower Canada were loyal to the British Crown and fought with British regulars and the British North American colonial militias to resist the American invaders. British and colonial authorities actively encouraged alliances with Indigenous peoples since the Americans vastly outnumbered the British American colonists.\nThe Métis were loyal to the Crown during the War of 1812, and they used their First Nations, familial, military and fur trade relationships to organize resistance to the American invasion of Upper and Lower Canada. Some Métis helped repel American invaders in key battles such as those at Detroit, Queenston Heights, Lundy’s Lane, Fort Mackinac, Frenchtown (Raisin River), and Prairie du Chien. Métis warriors served in the North West Company’s Corps of Canadian Voyageurs, William Caldwell’s Western Rangers, the Michigan Fencibles and the Mississippi Volunteers. Many of the Métis who fought in the War of 1812 either had roots in what is now Western Canada or would soon move there following the war’s conclusion in 1814. The direct descendants of these Métis warriors include the Dease, Ducharme, Fisher, McGillivray, Nolin, Pambrun, Ranville, Revard, Rolette and Sayers families.\nOne of the fortunate results of this bloody war has been a 200-year peace between the United States and the British North American colonies, and their successor state, Canada. However, this long peace also meant that British and Canadian officials no longer needed Indigenous peoples as allies. As a result, oppression soon followed and included the Province of Canada’s stridently racist Indigenous policies in the 1850s, which were later consolidated in the 1876 Indian Act.\nfrom Amazon.ca or Chapters.Indigo.ca or contact your favourite bookseller or educational wholesaler"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:b6e460b7-35e7-4ec3-8819-28116f5e0c1c>"],"error":null}
{"question":"I'm planning to set up a large aquarium - what's the minimum tank size needed for Bala sharks, and what are their natural habitats like?","answer":"Bala sharks require a very large tank - a minimum of 120 gallons for proper care, with recommendations suggesting even larger tanks of 70-100 gallons if keeping multiple fish. As for their natural habitat, they originate from Southeast Asia where they live in medium to large-sized rivers and lakes, particularly in regions like Thailand, Borneo, Sumatra, and the Malayan peninsula. However, they have become rare or extinct in many of their native areas due to factors like overfishing, river damming, and pollution. Currently, most Bala sharks in the aquarium trade are commercially farm-bred.","context":["Bala sharks are a very popular species among tropical fish enthusiasts, but many aquarium owners do not realize just how large these fish can get. In the pet store, Bala sharks are sold when they are very young and only measure one or two inches. At maturity, however, the Bala shark has the capacity to grow up to 12 or 14 inches in length. The Bala shark does not even reach sexual maturity until it is around four inches (10 or 15 cm) in size, so Bala shark breeding can be difficult for the average aquarium enthusiast. If you are interested in breeding your Bala sharks, you may want to learn the basics about breeding requirements for this species first.\nAbout Bala Sharks\nAlso known as the silver shark, Bala sharks are not actually a species of shark - they are a species of freshwater fish whose sleek body shapes resembles that of a shark. These fish generally have slender, silver-colored bodies with black-tipped fins, and they belong to the same family as danios, carps, and minnows. Bala sharks are a hardy variety of tropical fish, which makes them popular among aquarium enthusiasts, but they do require a great deal of space as they grow larger.\nA mature Bala shark can grow up to 14 inches in length so they require very large tanks\n-- it is recommended that they be kept in an aquarium no smaller than 70 gallons in capacity. If you plan to keep several bala sharks, or keep them with other fish, you may want to go with a 100-gallon tank just to be safe. Even though they can grow to be quite large, the Bala shark is a docile species that does well in a community tank, even with smaller species like danios and tetras. These fish enjoy a tank decorated with live plants, but it is also recommended that you leave open areas for free swimming.\nBala Shark Breeding Requirements\nUnlike some species of fish, like guppies, that are able to mate when they are still quite young, the Bala shark does not reach sexual maturity until it is around 4 inches (between 10 and 15 cm), in size. It is very difficult to tell a male from a female Bala shark so it is generally a good idea to purchase five or six of them from the pet store and to observe them for signs of mating behavior. Male Bala sharks tend to grow a little larger than females, and the female is said to have a slightly more rounded abdomen. It may take some time and practice for you to be able to tell a male from a female Bala shark – you may also be able to ask an experienced breeder or the professionals at the aquarium supply store for advice.\nTo make preparations for Bala shark breeding\n, set up a separate 55 gallon or larger breeding tank, and keep it at a temperature between 72 and 82 degrees Fahrenheit. Do not crowd the breeding tank with plants but place some around the outside of the tank and a few in the center to provide hiding places for the fish while also allowing for free swimming space. If you plan to use the breeding tank to raise the fry once they are hatched, you may want to keep the bottom of the tank bare. A bare-bottomed tank is easier to clean and it will also make your fry more visible. To keep the water in the tank clean, install a sponge filter in one corner of the tank. A sponge filter provides basic mechanical filtration as well as biological filtration without creating enough suction to put newly hatched fry in danger.\nBala Shark Mating Process\nIt has not been scientifically proven that the \"dance\" ,male and female Bala sharks have been known to exhibit is directly linked to the mating process, but breeders of Bala sharks have made the connection between the occurrence of this dance and the imminent production of eggs. After producing eggs, the female will scatter them around the tank for the male to fertilize externally. To facilitate this type of external fertilization, it is important to have adequate filtration in place in your breeding tank, to provide a current that will spread the male Bala shark's sperm to all of the eggs laid by the female.\nOnce the eggs have been laid and fertilized, the male and female Bala sharks pay no further attention to them. In the wild, Bala sharks travel to separate breeding grounds to spawn, and they do not provide any protection for their own young. Bala sharks are prone to eat the eggs and newly hatched fry, however, so you may want to move the fry to a different tank after hatching or simply remove the mature Bala sharks from the tank. In order to protect the Bala shark fry, you will need to keep an eye on the breeding process so you can remove the adults as soon as they are finished spawning.\nAfter hatching, feed the fry a varied diet of fine foods like newly hatched brine shrimp and commercial fry food\n. They will grow quickly at first and you may soon find it necessary to split them among several tanks or to raise them in an indoor pool where they can have adequate space to thrive. Ideally, you should feed the baby fish small amounts of food several times a day rather than a large amount once or twice. If you feed the fish too much at once, all of the uneaten food will simply sink to the bottom of the tank and accumulate – eventually this could lead to a decrease in water quality in your tank.\nBreeding Bala sharks is not easily accomplished by the average aquarium owner, because these fish require so much more space than the average aquarium enthusiast is able to provide. Professional breeders are able to create environments specifically designed to cater to the needs of Bala sharks. If successfully cared for and bred, however, these fish are a joy to keep. They can even be quite useful in the aquarium by \"predicting\" outbreaks of ich. Bala sharks have been known to rub their scales against rocks in the aquarium prior to an outbreak of ich.\nKeeping Bala sharks in the home aquarium can be a challenge due to their space requirements. For the aquarium hobbyist who can provide the space these fish need, however, they can be a delight to keep. If you are looking for a delightful, docile fish for your community tank and if you have adequate space available, the Bala shark is an excellent choice. These fish are also a good specimen to breed, as long as you are able to provide for their basic needs.","A school of Bala Shark can make a dramatic addition to a large aquarium tank. These Southeast Asian fish are not true sharks, but their shark-like appearance and size make them quite an intriguing pet. Here are the facts on the species and how to care for them.\nCommon Names: Bala Shark, Hangus, Malaysian Shark, Silver Bala, Silver Shark, Tricolor Shark, Tri-color Shark Minnow\nScientific Name: Balantiocheilus melanopterus\nAdult Size: 13 inches (35 cm)\nLife Expectancy: 10 years\n|Social||Peaceful, but may eat small fish|\n|Tank Level||All levels|\n|Minimum Tank Size||120 gallon|\n|Diet||Omnivore, accepts all foods|\nEgglayer, not bred in home aquaria\n|Care||Easy to intermediate|\n|Hardness||to 10 dGH|\n|Temperature||72–82 F (22–28 C)|\nOrigin and Distribution\nBala Sharks originate from Southeast Asia in medium to large-sized rivers, as well as lakes. At one time they were found in Thailand, Borneo, Sumatra, and the Malayan peninsula. However, they have become rare in many areas that they originally inhabited and are believed to be completely extinct in some regions.\nThe cause of this drastic reduction of Bala Sharks in their native lands is still under debate. Some believe they were overfished for the aquarium industry, others believe damming of the rivers is to blame, while still others believe pollution is the root cause. All these events quite likely factored into the demise of this fish that at one time was quite prolific in the area.\nRegardless of the reason, there is no question that the Bala Shark is rarely found in its original native haunts. In fact, since 1996 it has been on the IUCN Red List of Threatened Species. Currently, this species is commercially farm bred in the Far East, using hormones to promote spawning. Almost all specimens sold in the aquarium trade are captive bred.\nColorings and Markings\nThis species is known by a number of names, all of which have something in common: the word shark. Even though the Bala Shark is not a true shark, it has a large triangular-shaped dorsal fin and torpedo-shaped body that gives it a distinctly shark-like appearance. That’s where the similarity to real sharks ends, as this species is far more peaceful than the average ocean-going shark (and is a freshwater fish while sharks are ocean-dwellers).\nA member of the Cyprinid family, Balantiocheilos melanopterus has a shiny metallic silver body with well-defined scales, large eyes, and a deeply forked yellow-tinged teal. The dorsal caudal, pelvic, and anal fins are all edged in deep black. This tri-color scheme of silver, yellow, and black gives rise to another of its common names, the Tri-Color Shark.\nPerhaps the most important feature of this fish is its adult size. Usually sold as young juveniles in pet shops, they are only a mere three to four inches, giving potential owners the impression they are suitable for most tanks. What isn’t apparent at this point is the fact that this fish can grow to a foot or more in size, making it suitable for only a very large aquarium. This is particularly true because Bala Sharks are schooling fish that must be kept with others of its own kind. Bala Sharks are active fish that startle easily and will leap out of the tank if it does not have a good cover.\nDue to their peaceful nature, juvenile Bala Sharks do well in a community aquarium. As they mature, however, they will eventually outgrow most tanks, and they continue to grow for several years.\nAlso, be aware that many pet shops will not take large fish back, so take that into consideration before bringing one home. Ask the shop if it makes trade-in arrangements. If all else fails, check out public locations, such as medical offices or other businesses that have large aquariums that could use large fish. The one option that should never be used is to dump a Bala Shark, or any nonindigenous fish, into local waterways. Unwanted fish should be euthanized before opting to dump them outdoors.\nJuvenile Bala Sharks can be kept with a wide variety of fish due to their generally peaceful nature. As they grow larger, though, they will sometimes eat small fish, particularly sleek fish, such as the Neon Tetra. They also tend to stress shy or slow-moving fish, due to their constant vigorous activity in the tank. Balas should not be kept with inverts, such as snails and shrimp, as these are part of their normal diet in nature.\nBalas should be kept in schools, preferably four or more. When kept alone, they tend to be quite timid and skittish. If kept with only one or two others of its own kind, a dominant fish may emerge and bully the others. Therefore, it is wise to keep a larger school of Balas. If the tank is large enough, adult Bala Sharks can be kept with other medium- to large-sized robust fish.\nHabitat and Care\nAs previously mentioned, the key factor in the Bala environment is tank size. An aquarium of 125 gallons is needed to keep a school of adult Bala Sharks. Since they are active swimmers, a longer tank is recommended. In warm climates, ponds are also an option for this species, but they should only be kept outdoors in locations where it is warm year-round.\nWater should be soft to medium-hard, slightly acidic, and 72–82 F (22–28 C). They are sensitive to water conditions, particularly low water temperatures, and are susceptible to white spot when temperatures drop too low.\nThe tank should be fitted with a good cover, as this species jumps when startled. Decorate with large robust plants around the periphery of the tank, leaving plenty of open swimming space in the center of the tank. Floating plants are also suitable to deter the fish from jumping out. Provide smooth rocks and driftwood to round out the décor. Filtration should be robust to ensure good water movement and high oxygen levels in the water column.\nThis species is an omnivore that is not fussy about what it eats. Bala Sharks accept flake foods, pellets, and freeze-dried and frozen foods. They also voraciously accept live foods, including Daphnia, bloodworms, brine shrimp, mosquito larvae, and tubifex. Vegetables should be included in their diet, and they readily consume fresh veggies such as spinach and peas, as well as fresh fruits.\nMost of the time, there are no obvious external differences between the sexes. However, during the spawning season the female develops a noticeable rounder underbelly than the male.\nBreeding the Bala Shark\nBala Sharks have not been successfully bred in home aquaria, although occasional undocumented reports surface. The primary issue is tank size, as well as the lack of information regarding the required conditions for breeding. Interestingly enough, this species is bred commercially. In fact, most specimens for sale are commercially bred in the Far East. However, these commercial breeders employ the use of hormones to induce spawning, so natural spawning conditions are still largely undefined.\nMore Pet Fish Breeds and Further Research\nIf you’re interested in similar breeds, check out:\nOtherwise, check out all of our other freshwater pet fish breed profiles."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:834caad6-2322-48e0-951c-07afbb78a0f8>","<urn:uuid:1e4268c4-a3ef-4192-956c-c60287949533>"],"error":null}
{"question":"As someone worried about power outages, which place faced worse consequences from solar storms - Quebec in 1989 or Mars in recent times?","answer":"The impacts were quite different in nature. In Quebec, the 1989 solar-induced geomagnetic storm left 6 million people without power for nine hours. On Mars, the recent September 2012 solar storm didn't affect power infrastructure (since there isn't any), but it created more severe radiation effects - NASA's Curiosity rover detected surface radiation levels over double the highest intensity ever measured since 2012, which would have been dangerous enough to require any human explorers to seek immediate shelter. While both events were significant, they posed different types of threats - infrastructure disruption on Earth versus radiation hazards on Mars.","context":["NASA missions detect massive aurora and radiation spike on Mars\nEarlier this month, NASA probes in orbit and on the Martian surface detected an incredibly bright global aurora as a powerful solar storm struck Mars. The aurora was accompanied by a powerful dose of radiation that would have posed a significant danger to humans exploring the Red Planet.\nBetween September 12 and 13, NASA’s MAVEN spacecraft observed the Martian atmosphere as a magnificent ultraviolet aurora 25 times brighter than any seen by the probe since entering Mars’ orbit in 2014 illuminated the planet.\nThe September 11 solar storm, known as a coronal mass ejection (CME), that prompted the aurora was so powerful that it was detected on Earth, despite the fact that our planet was on the opposite side of the Sun during the event.\nA CME occurs when magnetic fields on the Sun explosively realign, throwing a colossal cloud of magnetized particles, also known as plasma, into space. This mass of plasma travels at millions of miles per hour and, upon colliding with a planet’s magnetic field, can trigger a geomagnetic storm, during which particles trapped in a planet’s atmosphere are released.\nThe release of the particles causes a reaction in the atmosphere, which triggers a release of photons that can create a breathtaking aurora. MAVEN observed the Martian aurora in ultraviolet light.\n“When a solar storm hits the Martian atmosphere, it can trigger auroras that light up the whole planet in ultraviolet light,” says Sonal Jain of the University of Colorado Boulder’s Laboratory for Atmospheric and Space Physics, and member of MAVEN’s Imaging Ultraviolet Spectrograph instrument team. “The recent one lit up Mars like a light bulb. An aurora on Mars can envelope the entire planet because Mars has no strong magnetic field like Earth’s to concentrate the aurora near polar regions. The energetic particles from the Sun also can be absorbed by the upper atmosphere, increasing its temperature and causing it to swell up.”\nWhile MAVEN and a number of other spacecraft watched the fireworks from orbit, NASA’s Curiosity rover recorded a more sinister aspect of the solar storm from the Red Planet’s surface. As the solar storm struck Mars, the rover’s Radiation Assessment Director (RAD) tracked the amount of radiation reaching its surface.\nSolar storms are known to significantly heighten the amount of radiation penetrating the Martian atmosphere, and at the peak of the storm the RAD instrument detected surface radiation levels over double the intensity of any that it had ever detected since touching down on the Martian surface in 2012.\nNASA is striving towards a long-term goal of putting humankind on Mars, and exposure to cosmic radiation is one of the key health issues to be faced by astronauts during any potential interplanetary mission. The agency and its partners will have to understand and cope with these radiation strikes if they hope to keep their astronauts healthy as they explore the surface of the Red Planet.\nHad explorers been present on the planet at the time of such a storm, they would have been advised to seek shelter.\n“If you were outdoors on a Mars walk and learned that an event like this was imminent, you would definitely want to take shelter, just as you would if you were on a space walk outside the International Space Station,” says RAD Principal Investigator Don Hassler of the Southwest Research Institute’s Boulder, Colorado, office. “To protect our astronauts on Mars in the future, we need to continue to provide this type of space weather monitoring there.”\nNASA believes that the observations made in the wake of the September 11 solar storm will lead to a greater understanding of how Mars’ original atmosphere was lost to space, leaving it a dry, barren world. It will also inform scientists’ understanding of the current Martian environment, and the threats posed to future explorers by powerful space weather events.\nthanks: Anthony Wood\nDedicated by:Kavignar Thanigai.","Thursday , March 06, 2014 - 12:45 PM\nOn a cool September night in 1859, campers in Colorado were roused from sleep by a “light so bright that one could easily read common print,” as one newspaper described it. Some of them, confused, got up and began making breakfast.\nFarther east, thousands of New Yorkers were rushing onto their roofs and sidewalks to gaze up at the heavens. The sky was glowing, ribboned in yellow, white and crimson.\nAt the time, it was a dazzling display of nature. Yet if the same thing happened today, it would be an utter catastrophe.\nThe auroras of 1859, known as the “Carrington Event,” came after the sun unleashed a large coronal mass ejection, a burst of charged plasma aimed directly at the Earth. When the particles hit our magnetosphere, they triggered a fierce geomagnetic storm that lit up the sky and frazzled communication wires around the world. Telegraphs in Philadelphia were spitting out “fantastical and unreadable messages,” one paper reported, with some systems unusable for many hours.\nToday, electric utilities, telecommunications providers and the insurance industry are grappling with a scary possibility. A solar storm on the scale of that in 1859 would wreak havoc on power grids, pipelines and satellites. In the worst case, it could leave 20 million to 40 million people in the Northeast without power — possibly for years — as utilities struggled to replace thousands of fried transformers stretching from Washington to Boston. Chaos and riots might ensue.\nThat’s not a lurid sci-fi fantasy, but rather a sober new assessment by Lloyd’s of London, the world’s oldest insurance market. The report notes that a smaller solar-induced geomagnetic storm in 1989 left 6 million people in Quebec without power for nine hours.\n“We’re much more dependent on electricity now than we were in 1859,” explains Neil Smith, an emerging-risks researcher at Lloyd’s and co-author of the report. “The same event today could have a huge financial impact” — pegged at up to $2.6 trillion for an especially severe storm. (To put that in context, Hurricane Sandy caused about $68 billion in damage.)\nThe possibility of apocalypse has piqued scientific interest in solar storms in recent years. But researchers are now realizing that space weather can cause all sorts of lesser mischief, such as disorienting GPS satellites or severing contact between polar flights and air-traffic control.\nSo, in recent years, scores of businesses and government agencies are starting to take space weather more seriously. Electric-grid operators are devising plans to reroute currents through their systems to brace for solar storms. Airlines such as Delta have developed plans to reroute flights in the case of emergency. The U.S. military has begun to realize that space-weather blips can disrupt communication in the heat of battle.\nBut preparing for disruptions isn’t easy. Just as interest in space weather is surging, the United States is facing the loss of key monitoring satellites in the coming years because budget cuts mean that aging systems aren’t being replaced. And scientists are rushing to plug troubling gaps in their knowledge about these storms.\nThe problem is far from theoretical. Last month, at a conference on space weather in Silver Spring, Md., Daniel Baker of the University of Colorado announced that the sun had unleashed another large coronal mass ejection in July 2012 that traveled at speeds comparable to the Carrington Event of 1859. It missed the Earth by a week.\n“Had that storm occurred a week earlier, it would have been a direct hit,” Baker said. “And we’d probably be having a very different conversation about this today.”\nWhen it comes to space weather, the foremost concern is what a solar-induced geomagnetic storm might do to electric grids around the world.\nAt certain points in the sun’s cycle, as sunspots appear and flares erupt, the sun will eject part of its outer atmosphere, a cloud of fast-moving charged particles. If one of these coronal mass ejections hits the Earth’s magnetic field in just the right way, it can induce strong ground currents that travel through power lines, oil pipelines and telecom cables.\nA truly severe geomagnetic storm could create currents powerful enough to overload electric grids and damage a significant number of high-voltage transformers, which can take a long time to repair or replace. That could leave millions without power for months or years.\n“That’s a key vulnerability,” Smith says. “If you had a really big solar event, there just aren’t enough replacement transformers available. It can take up to 12 months to build new ones.”\nAs it turns out, most utilities don’t keep lots of spares around. The largest transformers, which convert the electricity in high-voltage lines to lower voltages, are custom-built, can cost millions of dollars and weigh up to 400 tons. Procuring a new one is a complex process that involves lining up the necessary copper and steel supplies, working with a long chain of manufacturers and arranging specialized transport. So, the Lloyd’s report notes, if even 20 transformers in the Northeast were knocked out, the logistical challenges would be “extremely concerning.”\nSmith notes that the Northeast, with its aging power grid and peculiar geologic features, is especially at risk. Suffice it to say, it’s not fun to think about what would happen to the region if 40 million people had to go without power indefinitely.\nTake Pittsburgh: One 2004 assessment from Carnegie Mellon University found that a large number of the city’s services were simply unprepared for an extended blackout. Half the city would lose water after three days if the city’s electrical pumps couldn’t be revived. Grocery stores, gas stations and cellphone networks would be knocked out. Police stations would go dark. Traffic lights would blink out. Most hospitals have backup systems in place, but emergency rooms would be strained if, say, the air conditioning went out during a hot summer.\n“The absence of such fundamental services could lead to major and widespread social unrest, riots and theft,” the Lloyd’s report warns.\nIn theory, power utilities could try to take precautions if they had advance notice of a major solar storm headed our way. Using existing satellites, the National Weather Service’s Space Weather Prediction Center in Boulder, Colo., can detect an incoming event that’s about 30 minutes away.\nGrid operators would have to react quickly. For example, PJM Interconnection operates a huge swath of the U.S. power grid from Illinois to the District of Columbia, serving 60 million people. After receiving a storm warming, human operators would re-dispatch electricity to reduce the flow of current from west to east. That would minimize the grid’s vulnerability to ground currents, Frank Koza, the executive director of operations support at PJM, said at the June space weather conference.\nFor a modest solar storm, Koza said, PJM’s operators could respond if voltages started to drop anywhere in the system. (Pepco, which delivers electricity to 778,000 homes and businesses in the District of Columbia and Maryland, is a member of PJM.)\nBut there’s a limit to how much these strategies can help. “The one we’re really concerned about is extreme space weather, a Carrington-level event,” Koza said. “What would happen in that scenario? I would have to tell you we don’t really know.”\nFor bigger storms, there are technologies that could harden the grid, such as capacitors that can help block the flow of ground currents induced by a geomagnetic event. In Quebec, the Canadian government has spent about $1.2 billion on these technologies since the 1989 blackout.\nOne problem, says Chris Beck of the Electric Infrastructure Security Council, is that many of these technologies are expensive and could make the current grid slightly less efficient in its day-to-day operations.\n“We’ve designed our power lines to work efficiently under perfect conditions — long transmission lines, high voltages,” Beck says. Unfortunately, those characteristics make the grid particularly vulnerable to a solar storm. So there’s a trade-off.\nRecently, the federal government decided to take a more serious look at the issue. Last fall, the Federal Energy Regulatory Commission issued an order that will eventually require grid operators to prepare both operational and technological responses to a space weather event.\nKoza said he expects most grid operators to have response plans in the next year or two, but “engineered mitigation” could be another two to four years away.\nInsurance companies, meanwhile, are trying to figure out how to get a handle on the risk from a solar storm. Will a major one come around once every 150 years? More often than that? “We’re hoping we might one day be able to cover these risks,” says Smith of Lloyd’s, “but we’ll need to be able to quantify them more accurately.”\nPolicymakers have also started getting involved. For a long time, conservatives such as Newt Gingrich were mostly interested in the risks to electric grids posed by a nuclear weapon that exploded in the atmosphere and induced ground currents. In June, Gingrich spoke to members of the Electromagnetic Pulse Caucus in the House, a group of 16 Republicans and two Democrats, about this possibility. “This could be the kind of catastrophe that ends civilization,” Gingrich said, “and that’s not an exaggeration.”\nNow that the Cold War has ended, however, many of these Cassandras have switched over to warning about solar storms, which can have a similar effect, albeit on a smaller scale. Rep. Trent Franks, R-Ariz., a founder of the EMP Caucus, has pushed a bill to protect against both “natural and man-made EMP events.” And in public, he tends to put more emphasis on solar storms.\n“We’re starting to see more awareness there,” Beck says, “although we’re not quite to the point where we’re actually putting solutions in place.”\nSetting aside apocalyptic blackouts, solar storms and space weather can create all sorts of hiccups in the global economy that scientists are only just beginning to understand.\nCase in point: During the Battle of Takur Ghar in Afghanistan in 2002, a U.S. helicopter team was sent in to pick up a team of Navy SEALs. The SEALs sent a message to the helicopter warning the team not to land, but for some reason, it was never received. The helicopter landed under intense fire and four Americans were killed — an event dramatized in Sean Naylor’s bestselling account of Operation Anaconda, “Not a Good Day to Die.”\nSome scientists now suspect that space weather could have been to blame for the incident.\nAt the space weather conference in June, Michael Kelly of the Johns Hopkins University Applied Physics Laboratory presented early evidence that a form of space weather known as “scintillation” can cause disturbances in the ionosphere and disrupt local radio communications. Researchers are working to model this phenomenon more accurately.\nAirlines, too, have to take outbursts from the sun into account. Delta runs a number of commercial flights over the poles, such as routes between Detroit and Beijing and between Atlanta and Tokyo. But if they get a last-minute warning from the Space Weather Prediction Center of a geomagnetic storm, the planes often have to divert their routes away from the poles or risk losing radio contact with the ground. These diversions can cost thousands of dollars, Delta officials noted, so better predictions would help a great deal.\nAnd those concerns only scratch the surface.\nJoseph Kunches, a scientist at the Space Weather Prediction Center, says we’re still learning about activities that could be disrupted by solar weather. Satellite communications can go astray. Pipelines can corrode from ground currents. Even human space travel faces a threat.\n“Radiation is a big issue for space travel — particularly once you get away from the Earth’s magnetic field,” he says. Astronauts working outside the Earth’s protective shield can be particularly vulnerable to bursts of solar radiation, which can have harmful health effects. That means that if we ever want to wander around in space, it would be helpful to have a better grasp of space weather.\n“In 1972, there was actually a huge eruption that fortuitously fell between two of the Apollo flights, so the radiation didn’t hurt anybody,” Kunches says. “But it’s a problem.”\nAnd there are still plenty of unknowns. Kunches and other experts pointed to the potential impact of solar eruptions on GPS technology. Certain storms could degrade the signal as it makes its way from the satellite to the ground. GPS is built into so much of the modern economy — from navigation to geophysical exploration by oil and gas companies — that any interference with GPS signals could be quite costly.\n“I call it the cyber-electric cocoon we’ve built around the Earth,” says Baker, who heads the Space Physics Research Laboratory at the University of Colorado. “There are all these relationships that most people don’t even have a clue are there, and we’re still trying to understand everything that’s at risk.”\nOne big problem that businesses are having in preparing for a space weather attack is that they’re still not sure, exactly, what to prepare for. Should we expect a Carrington event? Something like the Quebec storm in 1989? And how often do these actually come along? After all, it’s hard to know how often we should expect catastrophic events.\n“Until we know that, the industry will be limited in its response,” said Koza of PJM Interconnection.\nAnd there’s plenty more that space scientists are still trying to grasp. It’s difficult to predict, for instance, whether a solar outburst will actually create a storm when it hits Earth. A great deal depends on how a coronal mass ejection interacts with other solar winds as it moves toward us. Kunches likens it to knowing that a hurricane is coming, but not being able to measure its barometric pressure.\nIt would also be helpful to have more spacecraft studying the sun and giving us advance warning of storms. But, if anything, the Earth’s alert systems are about to get worse, not better.\nRight now, the United States has four space satellites situated between the Earth and the sun, which can provide roughly 30 minutes’ warning of a major solar eruption. But these satellites are all reaching the end of their planned lives (and fuel tanks), and there’s only one replacement satellite scheduled to launch in 2014.\nAt the space conference in June, various speakers discussed ways to improve our ability to watch the sun. One engineer described fantastical plans for a satellite with a 100-square-meter “sail” that would use be steered and pushed by the sun’s photons in order to get closer and closer to the star without getting sucked in by gravity. A solar-sail satellite could, in theory, give us twice as much warning to prepare for a space storm.\nBut so far, these plans are all theoretical. “There’s a real need for a truly operational, 24-hour-a-day, seven-day-a-week space weather observatory,” Baker says. “But right now, we don’t see that coming from policymakers or the agencies that would have to step up.”\nThat means we may have to hope for a bit of luck in the years ahead. Solar activity tends to follow an 11-year cycle, with the most intense events often occurring near the peaks of the solar maximum — which, NASA says, could well arrive in late 2013, although it’s difficult to predict for sure.\nThat doesn’t mean the big one will hit then (for one thing, sunspot activity has been rather muted of late). But it does lend some urgency to the problem.\n“We’re really on an unknown timeline here,” Beck says. “One of these could happen at any time.”\nSign up for e-mail news updates."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:e4f21a68-1aaa-41cd-a293-021700c68af5>","<urn:uuid:2ab97540-2eb5-48ff-ac41-f886205d30ba>"],"error":null}
{"question":"Which requires more careful attention to obstacles during installation: laying tile flooring or cutting storm-damaged trees?","answer":"Cutting storm-damaged trees requires more careful attention to obstacles, as the hazards are potentially lethal. While tile installation requires working around obstacles like cabinets, vanities, and doorways that increase time and cost, tree cutting involves life-threatening hazards like widow-maker branches that can suddenly drop, live power lines that can kill, dangerous root balls that can suddenly spring upright, and compressed branches that can strike out with lethal force. Additionally, tree cutting requires constant awareness of escape routes and overhead threats, while tile installation obstacles primarily affect aesthetics and installation time rather than posing safety risks.","context":["In the first minutes after a heavy storm passes, the high-pitched growl of chain saws begins to echo across the landscape. “They are tremendously powerful tools,” says Charlie Hoffman, a certified arborist and account manager with outdoor power supplier Golden Eagle Distributing Corp., Trevor, Wis.\nThe scene after a heavy storm or tornado is one of chaos—jumbled branches, tangled wires and sheet metal wrapped around trees stripped of bark. There’s a crushed pickup, a roof torn from the barn and a 100-year-old oak laying on the house. The desire to do something, to do anything, is overpowering. Cutting trees looks like a good first step to recovery. Chain saws are the tools of choice.\nA sharp saw makes quick work of storm-damaged trees. The chain rips wood at 5,000 feet per minute, or 83 feet per second, at full throttle. More than 600 cutting teeth pass by a single point in 1 second. Raked across an unprotected leg, the damage caused in that second (the time it takes to say “600 cutting teeth”) is sickeningly catastrophic. A chain saw–caused gash is followed by an average of 110 stitches, according to the U.S. Product Safety Commission.\n“They are tremendously powerful tools, safely used,” Hoffman says, emphasizing “power,” once again. “But unfortunately, chain saws are treated with the same respect as a household power drill,” he adds. “Statistically, you’re not going to die [from a cut]. But you are going to be maimed.” Hoffman has been employed by chain saw manufacturer Echo, Inc., for a half-dozen years. “Having a situational awareness is the most important step,” Hoffman says. “Look for the dangers.”\nStress Loaded. Storm-damaged trees are bundles of conflicting pressures that, when wrongly relieved by an ill-placed cut, strike out with the lethality of a gunshot. Chain saw work accounts for 40,000 injuries a year, most to the upper leg area and hands. Forty to 50 people die in tree-cutting accidents each year.\nIt’s too easy to become a statistic. Cut without safety gear, ignore overhead hazards, toss the safety manual, fail to account for the unstoppable roll of a trunk weighing several tons, and grave injury is a moment away. The average medical cost for a chain saw cut injury is $12,000. Husqvarna sells a chain saw safety apparel kit—helmet, ear protection, chaps, gloves and protective gloves—for $149 on Amazon.com. Safety is a cheap investment.\nMark Chisholm urges patience. He is a third-generation certified arborist, working this day 60 feet off the ground among the limbs of a historic tulip poplar tree. The tree’s owner has employed Chisholm’s family-owned Aspen Tree Expert Co., in New Jersey, to do the work. Chisholm is a member of Team STIHL and is a competitive tree-climbing champion. STIHL manufactures chain saws.\nHazards Above. “First thing,” he says of the first few minutes after the storm, “look up. Mature trees are a real hazard.” Look for branches dangling up in the trees. They are called “widow makers” for good reason. They can drop suddenly to the ground, killing or injuring anyone below.\nSecond, look around for power lines, Chisholm says. Is the power line to the house or barn still attached? If not, where are they? How about lines running along the road or overhead, across the property? Live wires kill.\nThird, make an assessment of the damage. What work can you safely do? What do you want to do but probably shouldn’t? Chisholm says the seasonal saw operator ought to work from ground level only. “Don’t mess with trees on the house,” he says. Standing on a wet roof with a running chain saw, cutting branches that may spring into your face, is just poor practice. It is better, he says, to lift trees from a house by a capable skid steer loader or a tractor with forks or grapples. The dumbest idea is cutting branches while standing unsecured to the tree on an extension ladder. It is the stuff of YouTube lore, of chain sawers falling from ladders toppled by a falling branch. The result is often less humorous, as the operator tumbles to the ground, running chain saw in hand.\nProfessional cutters take the time to understand the pressures of twist and compression. Here’s what they see.\n-A tree bent sharply by another tree can suddenly spring up if the object holding it down is removed.\n-Cutting limbs without understanding their directional forces is hazardous. Keep your body to the left of the chain saw’s guide out of the plane of the chain rotation.\n-Root balls are especially dangerous. If the operator cuts through the trunk too close to the ball, the remaining trunk section and root ball can suddenly sit upright.\n-A skilled operator learns to see the potential for a phenomenon known as barber chairing, when either wind, the lean of the tree or the branch load causes a quick, rising split through the trunk of a prematurely falling tree.\nSplinters longer than the average man is tall fly high into the air. The barber chair gets its name from the end result. The seat of the barber chair is the partial cut made by the operator through the trunk. The back of the chair is the vertical piece left as the tree splits up the middle.\n“You need to take the time to look at a tree,” says Tim Ard, of Forest Applications Training, in Rome, Ga. “Move from side to side. Look at the tree and where it might go [if it’s not already on the ground].”\n*Begin At The End. The best way to begin cutting a tree already lying on the ground is to start at the butt end of the tree and work forward—while being mindful of the weight of the root ball if it is pulled out of the ground. Try not to pass by limbs as the work progresses toward the crown of the tree—unless they are supporting the trunk, Ard says.\nNever cut above your shoulder, always cut with two hands and never cut without an escape route. The escape route should be clear to at least 15 feet from the tree and at 45-degree angles from the work. The goal is to put the tree all the way onto the ground, the trunk cut into manageable sections.\nThere should be no more than one cutter working on a tree until the tree is fully on the ground. “Try to keep the [branch] pullers away until the tree is cut. No one should be holding or pulling anything while the saw is running,” Ard says. But it is a very good idea to have an observer working with the chain sawer. He is someone who watches for overhead hazards the operator may not see.\n*In Your Face. Kickback is the rapid, upward motion of the saw toward the operator’s face after it has struck an obstruction. It happens in the blink of an eye and accounts for a third of chain saw injuries. Hitting an obstruction with the top quarter of the running chain saw or with the nose of the saw are two ways kickback can occur.\nChain saws are equipped with brakes that stop the chain during kickback. A mechanical brake activates as the saw rises and the operator’s hand comes into contact with the front hand guard—which is part of the brake lever—pushing it forward.\nChain saw manufacturers Echo, STIHL and Husqvarna mount inertia braking systems onto their saws. This feature stops the chain under high levels of upward acceleration, even before the mechanical hand brake is activated.\nMore recently, Husqvarna has added a third, rear-mounted brake positioned just above the rear hand guard that also activates the brake as the saw rises upward. STIHL’s throttle lockout lever activates a third chain-braking feature on its saws when the right hand is completely removed from the rear handle. â¦�\nRules of the Saw\nTim Ard, owner of Forest Applications Training, offers a comprehensive list of articles describing best practices for chain saw use. Find them at his web page (www.forestapps.com) under “Tim’s Tips.” One article discusses good felling techniques. Here are Ard’s tips for felling:\n-Identify Potential Hazards. Look up into the crown of the tree and identify dead limbs and hanging branches. Assess the area into which the tree will fall for hazards. Clear ground debris, live saplings and shrubs. Once the tree is on the ground, pause to look for hazards that may have developed in nearby trees.\n-Determine Side or Weighted Lean. It is desirable, but not always possible, to cut from the good side of the tree. The bad side of the tree is the side to which it leans or is weighted. While standing back from the tree along the fall line, the operator should look up into the tree and draw a mental circle around the outermost limbs of the tree’s canopy. From the center of this circle, drop an imaginary plumb line to the ground. The distance from this spot on the ground to the center of the tree’s trunk gives a good estimate of the amount of weighted side lean.\n-Determine Your Escape Route. Always pick a “retreat route” away from a falling tree. This escape route should be clear of ground debris and should be opposite of the direction of fall at a 45-degree angle.\n-Determine the Hinge Size. To control the fall of a tree, a hinge is necessary. The hinge is the narrow strip left between the open-face notch and the back cut. As a general rule of thumb, the width of the hinge should be equal to 10% of the diameter of the tree at 4½ feet above the ground. It should have equal width across the stump. For example, a 15-inch tree should have a hinge 1.5 inches thick. The length of the hinge is also important in guiding the direction of fall. The general rule of thumb here is to set up a hinge length that is 80% of a tree’s diameter at 4½ feet from the ground. A 15-inch tree would need a 12-inch hinge.\n- Establish Your Cutting Plan. After the notch and hinge are set up, the back cut should be level with the open-face notch. Always remember to finish the back cut on the good side of the tree. If this is not possible, be aware of hazards that could cause safety problems.\nFor more details on safely cutting down trees, go to our tree-safety video series at bit.ly/1GBeKgj\n© Copyright 2015 DTN/The Progressive Farmer. All rights reserved.","Tile installation costs start at $4 to $6 per square foot based on tile type – ceramic, porcelain or stone. Peak rate is about $15 per square foot, and $8.50 is average.\nThe least expensive jobs require little preparation and use large tiles. The cost of tile installation rises as complexity increases and tile size gets smaller.\nIn this tile cost breakdown we are going to focus solely on the installation cost for tile – the labor charges only – and look at the many variables that can affect price. We’ll show you how to calculate a reliable estimate for your own project before you start getting quotes from local contractors.\nTo Get Started\nEach cost factor and how it affects price is explained below.\nDetermining Fair Cost to Install a Tile Floor\nTile installation is a competitive business. Tile setters know that you’ll likely get estimates from several installers before hiring one.\nMost tile contractors need a net a profit of about 15 percent on installation costs to stay in business. They take one of two approaches:\n- Price jobs competitively (12 to 18 percent profit) and try to stay busy.\n- Price jobs for a 25 to 30 percent profit, get less work, but make similar total income.\nThe second approach only works with excellent sales skills and/or a reputation for superior work worth paying a premium for.\nHigh costs: If you get an estimate significantly higher than the others, but the company’s reputation doesn’t stand out, they are probably price gouging.\nLow costs: Estimates on the low side might indicate that the company has a bad reputation, doesn’t pay fair wages, isn’t insured or plans to find a way to cut corners and costs.\nBefore we get to tile installation cost factors, let’s examine costs you might not have considered. All potential costs are summarized in a table below.\nFloor Leveling Cost\nRepairs might be required to the existing floor before tile or a suitable subfloor can be added.\nLow spots in any flooring type are eliminated with self-leveling underlayment mix. A latex primer-sealer is rolled onto the low area to improve adhesion. If the deck is OSB or plywood, joints must be filled with caulk first.\nPrimer sealer costs about $14 per quart. The leveling underlayment price is $35 per 50lb bag. One each should be enough to cover low spots in areas up to 150 square feet, based on the depth of the hollows.\nLabor cost is most of the total expense.\n- Self-leveling underlayment material & installation cost: $4.85-$6.50 per square foot of affected area\nFloor Sheathing Repair Cost\nThe sheathing is the material fastened to the floor joists, usually plywood or OSB. Sections of soft, water-damaged material must be replaced.\nOSB and plywood for flooring range in cost from $22 to $26 per 4×8 sheet, so $0.80 or less per square foot. Glue and screws or nails are a small expense. This is another labor-intensive job.\n- OSB and plywood subfloor repair cost: $3.50-$5.00 per square foot of affected area\nUnderlayment Installation Cost\nA concrete floor in “perfect” condition might not need a specialty subfloor for tile. Very flat vinyl that is fully glued rather than perimeter glued might also work.\nAll other materials require the installation of underlayment.\nThere are three common subfloor materials to consider.\nSelf-leveling underlayment, discussed above, is the cheapest material. When poured over the entire surface, tile can then be fixed directly to it. Most concrete brands like Quickrete and TEC make self-leveling underlayment. When the entire floor is covered, rather than just low spots, cost drops to about $4.25 per square foot.\nCement board underlayment (CBU), or backer board, is the most popular underlayment. Installers are familiar with it and know how to properly use it. Hardiebacker, PermaBase and US Gypsum are common brands. Sold in 3×5 sheets, cost is $0.60 to $1.25 per square foot depending on whether you use ¼” or ½” board. Installed cost with fasteners and labor is $3.25 to $4.75 per square foot.\nUncoupling membrane Schluter-DITRA has the advantage of being waterproof. It is also the thinnest, ideal for use where you don’t want height buildup. DITRA (1/8”) and DITRA XL (5/16”) cost $1.35 to $2.15 per square foot for the material. Installed cost is $3.00 to $4.00.\n- Tile underlayment material & installation cost: $3.00-$4.75 per square foot\nOur guide to Schluter-DITRA has comprehensive information about this unique material. It includes benefits, uses and a comparison to CBU that will help you select the right material for your project.\nTile Installation Cost Factors\nAn experienced estimator will quickly survey the area to be tiled and the material to be used. They’ll make measurements, crunch the numbers, and produce a written tile installation price estimate.\nTile installation labor cost starts at $4.00 per square foot and rises to $15.00 from there.\nObvious factors that might affect the tiling estimates you get include:\n- The reputation of the installer, as noted\n- Cost of living in your area\n- Availability of experienced installers\nAdditional factors affecting where on that price range your estimate will fall are next.\nSize of the job – Small jobs like a half-bath floor cost more per square foot than large jobs like a kitchen with an eating area. This is because each job has fixed costs such as fuel cost, paid travel time to and from the site and paid time to set up and take down equipment. The bigger the job, the more the fixed costs are spread out, so cost per square foot goes down.\nSize of the tile – Small tiles take longer to install. Floors made up of small tiles require more spacers and grouting. More pieces must be handled to cover the same space. As a result, installation cost is higher for small tiles and drops as tile size goes up. Here’s more on tile shapes and sizes.\nRoom sizes – Cutting tile to fit against walls and around jambs is time consuming, driving up cost. There are more linear feet of wall and more door jambs in multiple small rooms than in one large room.\nObstacles – Walls and doorways aren’t the only obstacles that increase time and cost. The same is true of vanities, cabinets, hearths, stairways and more. If the area being tiled is something like an open laundry area with one drain, cost will be lower than for a bathroom with multiple obstacles.\nTransitions – A transition is required where two separate flooring types meet. If not included in the estimate, the transition piece might be itemized at about $2-$4 per lineal foot for large transitions or $15 to $20 apiece at doorways. Chezck out our guide to transitions strips.\nStairs – Most tile setters don’t recommend putting tile on stairs unless they are concrete or have been framed for tile. Standard wood-framed stairs flex. Flexing cracks the grout.\nIf the stairs are constructed to hold tile, expect labor to be priced at a premium of up to 100% due to the large about of tile trimming needed.\nMaterial – Ceramic is easiest to cut, so prices start at about $4/sq.ft. Porcelain tile is harder, and cost begins at about $5. Stone tile is the most difficult to cut and costly in terms of saw blades needed, so installation starts at about $6.\nDesign – Standard tile layout comes with a standard cost. Diagonal designs boost cost by 15 to 20 percent. Mixing small and large tiles or adding a border of another type tile increases price too. Truly custom designs, like a dramatic sunburst in an entryway or crest at the foot of stairs, come with top cost.\nTo Get Started\nTile Installation Cost Extremes and Averages\nNear the beginning of this article, we said tile installation costs range from $4 to $15 per square foot, with $8.50 being average.\nAverage: Examples of projects that might be priced near the $8.50 per square foot average are:\n- A master bathroom or foyer with just a few obstacles\n- A large kitchen with cabinets, an island, breakfast bar peninsula and multiple floor grates to work around\nLow cost: Your cost will be on the low end if you’re covering 300 square feet or more in large tile and there are few obstacles. Having several competing tile contractors near to you can limit cost too.\nHigh cost: Two factors that drive cost toward the high end fastest are installations with a large amount of trimming and custom tile designs.\nExtra Costs in Tile Flooring Installation Estimates\nEmptying rooms and removing old flooring are necessary tasks when preparing for new tile.\nRemoving old flooring – Tile on concrete is very difficult and costly to remove. Carpet and pad are easy to remove, as is perimeter-glued vinyl. Ask you tile installer about fully glued vinyl in good condition. It’s possible the new subfloor and tile can be installed over it. Depending on the material, here are associated costs.\n- Old flooring removal cost: $0.20-$0.40 per square foot for most materials and up to $1.50 for tile on concrete.\n- Disposal of used flooring: Up to $0.25 per square foot, possibly more for tile.\nMoving furniture and appliances – The number and weight of things moved determines cost.\n- Moving furniture and appliances: $25-$125 per room\n- Disconnecting and reconnecting gas or water lines: $15-$30 per appliance\nCost Summary Table\nThis table summarizes the tile installation labor costs discussed above with cost factors and extras.\n|Floor Leveling||$4.85-$6.50/sq.ft.||Amount of material needed, size of low areas|\n|Sheathing repair||$3.50-$5.00/sq.ft.||Material used, degree of damage|\n|Underlayment Installation||$3.00-$4.75/sq.ft.||Material used, amount of trimming|\n|Basic tile flooring installation||$4.00-$6.00/sq.ft.||See Tile Installation Cost Factors above|\n|Transitions||$2.00-$4.00/linear ft.||Material used, length and shape of transition|\n|Moving furniture/appliances||$25-$125 per room||Number, weight and delicacy of pieces|\n|Appliance connections||$15-$30||Type (electric, gas, water), weight|\n|Old flooring removal||20-35 cents/sq.ft.||Type of flooring, how it is fastened|\n|Old flooring disposal||20 cents to $1.50/sq.ft.||Local disposal costs and availability of free recycling|\nSaving Money on Tile Installation Costs\nExisting homes: You can reduce cost in two ways. First, by removing the old flooring. Most flooring, even laminate and newer vinyl materials, can be recycled at no cost. That’s a cheap choice when a center that will take the material is nearby.\nThe second cost-saving step is to empty rooms and handle appliance disconnects and re-connects. Leave it to the tile contractor if you’re unsure of how to do it safely and correctly.\nNew construction: If you’re planning a home or addition, the floorplan you chooses can affect flooring installation cost for all materials.\nFlooring installation costs are lower in open plans with minimal obstacles to trim around.\nAny home: Finally, homeowners that shop around for the best price save the most money. We recommend getting written estimates from three or four tile contractors.\nLet them know that you are getting several bids. Unless they are so busy that they don’t need the work, most will write competitive estimates.\nProtect yourself in the process by being wary of lowball offers and what’s behind them. If the work is done improperly, appearance of the floor will suffer, and it will cost you money in future repairs that could have been avoided.\nBe sure any company you hire has a reputation for skilled tile setters, excellent work and fair prices.\nTo Get Started"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:e62485aa-426c-448a-917d-3e993aba2a44>","<urn:uuid:264f83f7-66c0-4129-8adc-d81605da6d9f>"],"error":null}
{"question":"How does the visual pathway process light information from the upper and lower visual fields?","answer":"Light from the upper and lower visual fields is processed through specific pathways. Fibers from upper retinal quadrants project to the medial lateral geniculate body (LGB) and represent the lower visual field, while fibers from lower retinal quadrants project to the lateral LGB and represent the upper visual field. These then divide into dorsal and ventral bundles. The dorsal bundle projects via the parietal lobe to the upper wall of calcarine cortex (cuneus), while the ventral bundle, also known as Meyer's loop, projects via the temporal lobe to the lower wall of calcarine cortex (lingual gyrus).","context":["AKA: Rhinencephalon (contribute to sense of smell): consists of: Olfactory Epithelium, Olfactory Bulbs, Olfactory Striae, Olfactory cortex and additional areas of brain. Functions in special sense of smell or olfaction.\nSubstances that are smelled (odorants) release small quantities of gas, oil. esters (fragrant compounds) acids, etc. into air which reach nasal mucosa\nOlfactory System: Neurosensory Cells\nFirst order, bipolar sensory neurons. In olfactory epithelium projected peripheral processes into nasal mucosa; peripheral processes act as receptors (unlike other special sensory nerves that have separate receptors) that are stimulated by odorants.\nSensations are transmitted over central processes of neurosensory cells to form olfactory nerves (CN1)\nOlfactory Nerve Fibers\nOlfactory nerve fibers ascend in small bundles (fasciuli) across cribiform plate of ethmoid bone to synapse on mitral and tufted cells (2nd order neurons) in olfactory bulb\nComprised of axons from mitral and tufted cells as well as anterior olfactory nucleus. CNS: we are in tracts now. Project centrally to make up olfactory tracts. Splits into lateral, medial and intermediate striae.\nprojects to anterior olfactory nucleus and to ipsilateral pyriform cortex in uncus (primary olfactory cortex). Pyriform fibers project to: mediodorsal thalamic nucleus for consious analysis of odor, 3rd order.\nCome from lateral striae. Entorhinal area (secondary olfactory cortex) to parahippocampal gyrus and orbitofrontal cortex: associating smell with previous experience\nArises from anterior olfactory nucleus and projects to contralateral olfactory bulb via anterior commissure; mediates emotional/autonomic response to odors\nRods (low intensity vision, monochromatic) and cones (sharp visual acuity, color) of retina compose receptors of visual pathway; light falling on these receptors is transducer into retinal potentials\nVisual Information Passes through...\nVisual info passes forward in the retina (back of eye through internal layer, which is more forward) to the bipolar cells which are the primary sensory (1st order neuron) neurons in visual pathway.\n1st order neuron synapse with?\nThese cells synapse with the ganglion cells (2nd order neurons) of the retina whose axons converge near the center of the retina to form the optic nerves\nLocated predominately in the central retina which contains the macula lutea (area where light is striking), a small circular area lateral to the optic disk\nLocated within the macula lute; inner retinal layer almost absent in fovea forming a pit or foveola. Allows for max. amt of light to reach fovea which is focal point of central vision.\nThey predominate in rest of retina farther from fovea and participate in peripheral vision (peripheral portion of retina)\nOptic nerves travel medially from fovea to converge at optic disk (aka papilla or optic nerve head) where they exit from retina. Blind spot area where nerve exit from retina: because no photoreceptors are present in optic disc area\nOptic nerve fibers become myelinated as they emerge from optic disc.\nOptic nerves pass from eyeball to leave the orbits through the optic canals; each optic nerve enters the middle cranial fossa and joins together to form the optic chasm (optic cross).\nAt Optic Chiasm\nnerve fibers from medai half of each retina decussate, while the lateral fibers from each retina remain uncrossed or direct.\nOptic fibers continue without interruption behind the chasm as two diverging optic tracts that go to left and right lateral geniculate bodies.\nCells in the LGB are 3rd order sensory neurons; their axons give rise to fibers that form the geniculocalcarine tracts or optic radiations to primary cortex of the occipital lobe (region surrounding calcarine fissure)\nHow does light fall on the retina? How does it reflect back?\nVisual fields have central and peripheral regions. Central field is projected onto macula (area of most accurate vision. straight back). Peripheral field is projected onto paramacular (medial) and peripheral portions of retina\nEach eye is divided into nasal (medial) and temporal (lateral) halves which contain upper and lower quadrants. Light rays from temporal visual field project to nasal half of retina; rays from nasal visual field project to temporal half of retina.\nVisual fields are projected onto each retina in a reversed and inverted pattern, follow point-to-point projection to occipital cortex. When we get to retina, it will follow same pathway back from this point on\nOrigin of light rays\nLight rays from upper quadrant strike lower retina, rays from lower quadrant strike upper retina.\nEach LGB receives projections from the homonymous (right or left) halves of the field for both eyes. At this point we have \"full\" fields. 1 temporal field and 1 nasal field.\nUpper and Lower Quadrants\nFibers from upper retinal quadrants project to medial LGB. Lower visual field.\nFibers from lower retinal quadrants project to lateral LGB. Upper visual field.\nThese divide into dorsal and ventral bundles.\nprojects via parietal lobe to upper wall of calcimine cortex (cuneus). Takes upper - medial LGB projects over.\nAKA: loop of Meyer. Projects via temporal lobe to lower wall of calcimine cortex (lingual gyrus). Take lower. Lateral LGB projects over.\nCalcarine Cortex = Striate Cortex\nFollow all the way back\nHomonymous Visual Field Detects:\n1. Similar regions of visual field defects for each eye\n2. Results from lesion in visual pathway distal to optic chiasm\nHeteronymous Visual Field Detects:\nNot as frequent\n1. Two different parts of visual field impaired (eg. bitemporal hemianopsia - tunnel vision)\n2. Due to lesions at level of optic chiasm\nWant to see the other 35 Flashcards in Neuro: Week 7?JOIN TODAY FOR FREE!"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:f6ea4a35-142e-4149-a550-47764c3e7a2b>"],"error":null}